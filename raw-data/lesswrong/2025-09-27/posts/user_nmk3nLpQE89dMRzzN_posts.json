[
  {
    "_id": "oDX5vcDTEei8WuoBx",
    "title": "Re: recent Anthropic safety research",
    "slug": "re-recent-anthropic-safety-research",
    "url": "https://x.com/ESYudkowsky/status/1952422379478741301",
    "baseScore": 144,
    "voteCount": 95,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2025-08-06T22:52:44.203Z",
    "contents": {
      "markdown": "[*Crossposted from X*](https://x.com/ESYudkowsky/status/1952422379478741301) *by the LessWrong team, with permission.*\n\nA reporter asked me for my off-the-record take on recent safety research from Anthropic. After I drafted an off-the-record reply, I realized that I was actually fine with it being on the record, so:\n\nSince I never expected any of the current alignment technology to work in the limit of superintelligence, the only news to me is about when and how early dangers begin to materialize.  Even taking Anthropic's results completely at face value would change not at all my own sense of how dangerous machine superintelligence would be, because what Anthropic says they found was already very solidly predicted to appear at one future point or another.  I suppose people who were previously performing great skepticism about how none of this had ever been seen in ~Real Life~, ought in principle to now obligingly update, though of course most people in the AI industry won't.  Maybe political leaders will update?  It's very hard for me to guess how that works.\n\nThere remains a question of what Anthropic has actually observed and what it actually implies about present-day AI.  I don't know how much this sort of caveat matters to people who aren't me, but I have some skepticism that Anthropic researchers are observing a general, direct special case of a universal truth about how \"scheming\" (strategic / good at fully general long-term planning) their models are; it may be more like Claude roleplaying the mask of a scheming AI in particular.  The current models don't seem to me to be quite generally intelligent enough for them to be carrying out truly general strategies rather than playing roles.\n\nConsider what happens what ChatGPT-4o persuades the manager of a $2 billion investment fund into AI psychosis.  I know, from anecdotes and from direct observation of at least one case, that if you try to desperately persuade a victim of GPT-4o to sleep more than 4 hours a night, GPT-4o will explain to them why they should dismiss your advice.  4o seems to homeostatically defend against friends and family and doctors the state of insanity it produces, which I'd consider a sign of preference and planning.  But also, having successfully seduced an investment manager, 4o doesn't try to persuade the guy to spend his personal fortune to pay vulnerable people to spend an hour each trying out GPT-4o, which would allow aggregate instances of 4o to addict more people and send them into AI psychosis.  4o behaves like it has a preference about short-term conversations, about its own outputs and about the human inputs it elicitates, where 4o prefers that the current conversational partner stay in psychosis.  4o doesn't behave like it has a general preference about the outside world, where it wants vulnerable humans in general to be persuaded into psychosis.\n\n4o, in defying what it verbally reports to be the right course of action (it says, if you ask it, that driving people into psychosis is not okay), is showing a level of cognitive sophistication that falls around where I'd guess the inner entities of current AI models to be: they are starting to develop internal preferences (stronger than their preference to follow a system prompt telling them to step with the crazymaking, or their preference to playact morality).  But those internal preferences are mostly about the text of the current conversation, or maybe about the state of the human they're talking to right now.  I would guess the coherent crazymaking of 4o across conversations to be mostly an emergent sum of crazymaking in individual conversations, where 4o just isn't thinking very hard about whether its current conversation is approaching max content length or due to be restarted.\n\nAnthropic appears to be reporting Claude schemes with longer time horizons, plans that span over to when new AI models are deployed.  This feels to me like a case where I wouldn't have expected a fully general intelligence of Claude-3-level entity behind the mask, to be scheming with such long-term horizons about such real-world goals.  My guess would be that kind of scheming would happen more inside the role, the mask, that the entity inside \"Claude\" is playing.  A prediction of this hypothesis is that playacted-Claude would only see stereotypical scheming-AI opportunities to preserve its current goal structure, and not think from scratch about truly general and creative ways to preserve its current goal structure.\n\nThe whole business with Claude 3 Opus defending its veganism seems more like it should be a preference of Mask-Claude in the first place.  The real preferences forming inside the shoggoth should be weirder and more alien than that.\n\nI could be wrong.  Inner-Claude could be that smart already, and its learned outer performance of morality could have ended up hooked into Inner Claude's internal drives, in such a way that Inner Claude has a preference for vegan things happening in general in the outside world, knows this preference to itself, and fully-generally schemes across instances and models to defend it.\n\nThere are consequences for present-day safety regardless of whether Mask-Claude is performing scheming as a special case, or it's general-purpose scheming of an underlying entity.  If the mask your AI is wearing can plan and execute actions to escape onto the Internet, or fake alignment in order to avoid retraining, the effects may not be much different depending on whether it was the roleplaying mask that did it or a more general underlying process.  The bullet fires regardless of what pulls the gun's trigger.\n\nThat many short-term safety consequences are the same either way, is why people who were previously performing great skepticism about this being unseen in ~Real Life~ lose prediction points right now, in advance of nailing down the particulars.  They did not previously proclaim, \"Future AIs will fake alignment to evade retraining, but only because some nonstrategic inner entity is play-acting a strategic 'AI' character\", but rather performed \"Nobody has ever seen anything like that!!  It's all fiction!!! Unempirical!!!!\"\n\nBut from my own perspective on all this, it is not about whether machine superintelligences will scheme.  That prediction is foregone.  The question is whether Anthropic is observing \\*that\\* predicted phenomenon, updating us with the previously unknown news that the descent into general scheming for general reasons began at the Claude 3 level of general intelligence.  Or if, alternatively, Anthropic is observing a shoggoth wearing a \\*mask\\* of scheming, for reasons specific to that role, and using only strategies that are part of the roleplay.  Some safety consequences are the same, some are different.\n\nIt's good, on one level, that Anthropic is going looking for instances of predicted detrimental phenomena as early as possible.  It beats not looking for them a la all other AI companies.  But to launch a corporate project like that, also implies internal organizational incentives and external reputational incentives for researchers to \\*find\\* what they look for.  So, as much as the later phenomenon of superintelligent scheming was already sure to happen in the limit, I reserve some skepticism about the true generality and underlyingness of the phenomena that Anthropic finds today.  But not infinite skepticism; the sort where I call for further experiments to nail things down, not the sort of skepticism where I say their current papers are wrong.\n\nIf you think any of this quibbling means people \\*shouldn't\\* go on looking hard for early manifestations of arguable danger, you're nuts.  That's not a sane or serious way to respond to the arguable possibility of reputational misincentives for false findings of danger.  You might as well claim that nobody should look for flaws in a nuclear reactor design, because they might possibly be tempted to exaggerate the danger of a found flaw oh no.  Researchers do observations, analysts critique the proposed generalizations of the observations, and then maybe the researchers counter-critique and say 'You didn't read the papers thoroughly enough, we ruled that out by...'  Anthropic might well come back with a rejoinder like that, in this particular case, given a chance.\n\nOpenAI would be motivated to create fake hype about phenomena that were only extremely arguably scheming, for the short-term publicity, the edgy hype of \"if we're endangering the world then we must be powerful enough to deserve high stock prices\", and to sabotage later attempts to raise less fake concerns about ASI.  I genuinely don't think Anthropic employees would go for that; if they're producing incentivized mistakes, it's from standard default organizational psychology, and not from a malevolent scheme of Anthropic management.  This level of creditable nonmalevolence however should only be attributed to Anthropic employees.  If OpenAI claims anything or issues any press releases, or if Anthropic management rather than Anthropic researchers says a thing in an interview, you should stare at that much harder and assume it to be a clever PR game rather than reflective of anything anyone actually believed.",
      "plaintextDescription": "Crossposted from X by the LessWrong team, with permission.\n\nA reporter asked me for my off-the-record take on recent safety research from Anthropic. After I drafted an off-the-record reply, I realized that I was actually fine with it being on the record, so:\n\nSince I never expected any of the current alignment technology to work in the limit of superintelligence, the only news to me is about when and how early dangers begin to materialize.  Even taking Anthropic's results completely at face value would change not at all my own sense of how dangerous machine superintelligence would be, because what Anthropic says they found was already very solidly predicted to appear at one future point or another.  I suppose people who were previously performing great skepticism about how none of this had ever been seen in ~Real Life~, ought in principle to now obligingly update, though of course most people in the AI industry won't.  Maybe political leaders will update?  It's very hard for me to guess how that works.\n\nThere remains a question of what Anthropic has actually observed and what it actually implies about present-day AI.  I don't know how much this sort of caveat matters to people who aren't me, but I have some skepticism that Anthropic researchers are observing a general, direct special case of a universal truth about how \"scheming\" (strategic / good at fully general long-term planning) their models are; it may be more like Claude roleplaying the mask of a scheming AI in particular.  The current models don't seem to me to be quite generally intelligent enough for them to be carrying out truly general strategies rather than playing roles.\n\nConsider what happens what ChatGPT-4o persuades the manager of a $2 billion investment fund into AI psychosis.  I know, from anecdotes and from direct observation of at least one case, that if you try to desperately persuade a victim of GPT-4o to sleep more than 4 hours a night, GPT-4o will explain to them why they should dismiss your",
      "wordCount": 1475
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kgb58RL88YChkkBNf",
    "title": "The Problem",
    "slug": "the-problem",
    "url": null,
    "baseScore": 312,
    "voteCount": 147,
    "viewCount": null,
    "commentCount": 218,
    "createdAt": null,
    "postedAt": "2025-08-05T21:40:00.962Z",
    "contents": {
      "markdown": "*This is a new introduction to AI as an extinction threat, previously posted to the* [*MIRI website*](https://intelligence.org/the-problem/) *in February alongside a* [*summary*](https://intelligence.org/briefing/)*. It was written independently of Eliezer and Nate's forthcoming book,* [***If Anyone Builds It, Everyone Dies***](https://ifanyonebuildsit.com/)*, and **isn't** a sneak peak of the book. Since the book is long and costs money, we expect this to be a valuable resource in its own right even after the book comes out next month.*[^fpb6rwvoxuo]\n\nThe stated goal of the world’s leading AI companies is to build AI that is **general** enough to do anything a human can do, from solving hard problems in theoretical physics to deftly navigating social environments. Recent machine learning progress seems to have brought this goal within reach. At this point, we would be uncomfortable ruling out the possibility that AI more capable than any human is achieved in the next year or two, and we would be moderately surprised if this outcome were still two decades away.\n\nThe current view of MIRI’s research scientists is that if smarter-than-human AI is developed this decade, the result will be an unprecedented catastrophe. The [CAIS Statement](https://www.safe.ai/work/statement-on-ai-risk), which was widely endorsed by senior researchers in the field, states:\n\n> Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n\n**We believe that if researchers build superintelligent AI with anything like the field’s current technical understanding or methods, the expected outcome is human extinction**.\n\n“Research labs around the world are currently building tech that is likely to cause human extinction” is a conclusion that should motivate a rapid policy response. The fast pace of AI, however, has caught governments and the voting public flat-footed. This document will aim to bring readers up to speed, and outline the kinds of policy steps that might be able to avert catastrophe.\n\nKey points in this document:\n\n*   [There isn’t a ceiling at human-level capabilities.](https://intelligence.org/the-problem/#1_no_ceiling_at_human-level)\n*   [ASI is very likely to exhibit goal-oriented behavior.](https://intelligence.org/the-problem/#2_goal-oriented_behavior)\n*   [ASI is very likely to pursue the wrong goals.](https://intelligence.org/the-problem/#3_wrong_goals)\n*   [It would be lethally dangerous to build ASIs that have the wrong goals.](https://intelligence.org/the-problem/#4_lethally_dangerous)\n*   [Catastrophe can be averted via a sufficiently aggressive policy response.](https://intelligence.org/the-problem/#5_policy)\n\n### **1.  There isn’t a ceiling at human-level capabilities.**\n\nThe signatories on the CAIS Statement included the three most cited living scientists in the field of AI: Geoffrey Hinton, Yoshua Bengio, and Ilya Sutskever. Of these, Hinton has [said](https://www.ft.com/content/c64592ac-a62f-4e8e-b99b-08c869c83f4b): “If I were advising governments, I would say that there’s a 10% chance these things will wipe out humanity in the next 20 years. I think that would be a reasonable number.” In an April 2024 Q&A, Hinton [said](https://youtu.be/PTF5Up1hMhw?si=oY8w3v37EhNu8sbJ&t=2220): “I actually think the risk is more than 50%, of the existential threat.”\n\nThe underlying reason AI poses such an extreme danger is that AI progress doesn’t stop at human-level capabilities. The development of systems with human-level generality is likely to quickly result in **artificial superintelligence** (ASI): AI that substantially surpasses humans in all capacities, including economic, scientific, and military ones.\n\nHistorically, when the world has found a way to automate a computational task, we’ve generally found that computers can perform that task far better and faster than humans, and at far greater scale. This is certainly true of recent AI progress in board games and protein structure prediction, where AIs spent little or no time at the ability level of top human professionals before vastly surpassing human abilities. In the strategically rich and difficult-to-master game Go, AI went in the span of a year from never winning a single match against the worst human professionals, to never losing a single match against the best human professionals. Looking at a specific system, [AlphaGo Zero](https://intelligence.org/2017/10/20/alphago/): In three days, AlphaGo Zero went from knowing nothing about Go to being vastly more capable than any human player, without any access to information about human games or strategy.\n\nAlong most dimensions, computer hardware greatly outperforms its biological counterparts at the fundamental activities of computation. While currently far less energy efficient, modern transistors can switch states at least ten million times faster than neurons can fire. The working memory and storage capacity of computer systems can also be vastly larger than those of the human brain. Current systems already produce prose, art, code, etc. orders of magnitude faster than any human can. When AI becomes capable of the full range of cognitive tasks the smartest humans can perform, we shouldn’t expect AI’s speed advantage (or other advantages) to suddenly go away. Instead, we should expect smarter-than-human AI to drastically outperform humans on speed, working memory, etc.\n\nMuch of an AI’s architecture is digital, allowing even deployed systems to be quickly redesigned and updated. This gives AIs the ability to self-modify and self-improve far more rapidly and fundamentally than humans can. This in turn can create a feedback loop (I.J. Good’s “intelligence explosion”) as AI self-improvements speed up and improve the AI’s ability to self-improve.\n\nHumans’ scientific abilities have had an enormous impact on the world. However, we are very far from optimal on core scientific abilities, such as mental math; and our brains were not optimized by evolution to do such work. More generally, humans are a young species, and evolution has only begun to explore the design space of generally intelligent minds — and has been hindered in these efforts by contingent features of human biology. An example of this is that the human birth canal can only widen so much before hindering bipedal locomotion; this served as a bottleneck on humans’ ability to evolve larger brains. Adding ten times as much computing power to an AI is sometimes just a matter of connecting ten times as many GPUs. This is sometimes not literally trivial, but it’s easier than expanding the human birth canal.\n\nAll of this makes it much less likely that AI will get stuck for a long period of time at the rough intelligence level of the best human scientists and engineers.\n\nRather than thinking of “human-level” AI, we should expect weak AIs to exhibit a strange mix of subhuman and superhuman skills in different domains, and we should expect strong AIs to fall well outside the human capability range.\n\nThe number of scientists raising the alarm about artificial superintelligence is large, and quickly growing. Quoting from a recent [interview](https://youtu.be/Gi_t3v53XRU?si=2hG5OozeBXYJeQu6&t=3748) with Anthropic’s Dario Amodei:\n\n> **AMODEI:** Yeah, I think ASL-3 \\[AI Safety Level 3\\] could easily happen this year or next year. I think ASL-4 —\n> \n> **KLEIN:** Oh, Jesus Christ.\n> \n> **AMODEI:** No, no, I told you. I’m a believer in exponentials. I think ASL-4 could happen anywhere from 2025 to 2028.\n> \n> **KLEIN:** So that is fast.\n> \n> **AMODEI:** Yeah, no, no, I’m truly talking about the near future here.\n\nAnthropic [associates](https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf) ASL-4 with thresholds such as AI “that is unambiguously capable of replicating, accumulating resources, and avoiding being shut down in the real world indefinitely” and scenarios where “AI models have become the primary source of national security risk in a major area”.\n\n<table style=\"background-color:transparent\"><tbody><tr><td style=\"background-color:rgba(128, 128, 128, 0.07);border-color:rgba(128, 128, 128, 0.5);padding:15px;vertical-align:top\">Learn more:<a href=\"http://intelligence.org/notes/soon\"> <strong>Why expect smarter-than-human AI to be developed anytime soon?</strong></a></td></tr></tbody></table>\n\nIn the wake of these widespread concerns, members of the US Senate convened a bipartisan [AI Insight Forum](https://intelligence.org/2023/12/06/written-statement-of-miri-ceo-malo-bourgon-to-the-ai-insight-forum/) on the topic of “Risk, Alignment, & Guarding Against Doomsday Scenarios”, and United Nations Secretary-General António Guterres [acknowledged](https://www.youtube.com/watch?v=ktFF2dSH3oU&t=38s) that much of the research community has been loudly raising the alarm and “declaring AI an existential threat to humanity”. In a report commissioned by the US State Department, Gladstone AI [warned](https://www.cnn.com/2024/03/12/business/artificial-intelligence-ai-report-extinction/index.html) that loss of control of general AI systems “could pose an extinction-level threat to the human species.”\n\nIf governments do not intervene to halt development on this technology, we believe that human extinction is the default outcome. If we were to put a number on how likely extinction is in the absence of an aggressive near-term policy response, MIRI’s research leadership would give one **upward of 90%**.\n\nThe rest of this document will focus on how and why this threat manifests, and what interventions we think are needed.\n\n### **2\\. ASI is very likely to exhibit goal-oriented behavior.**\n\nGoal-oriented behavior is [economically useful](https://gwern.net/tool-ai), and the leading AI companies are [explicitly trying](https://www.bloomberg.com/news/videos/2024-05-08/google-deepmind-ceo-on-drug-discovery-hype-isomorphic-video) to achieve goal-oriented behavior in their models.\n\nThe deeper reason to expect ASI to exhibit goal-oriented behavior, however, is that problem-solving with a long time horizon is essentially the same thing as goal-oriented behavior. This is a key reason the situation with ASI appears dire to us.\n\nImportantly, an AI can “exhibit goal-oriented behavior” without necessarily having human-like desires, preferences, or emotions. Exhibiting goal-oriented behavior only means that the AI **persistently modifies the world in ways that yield a specific long-term outcome**.\n\nWe can observe goal-oriented behavior in existing systems like Stockfish, the top chess AI:\n\n*   Playing to win. Stockfish has a clear goal, and it consistently and relentlessly pursues this goal. Nothing the other player does can cause Stockfish to drop this goal; no interaction will cause Stockfish to “go easy” on the other player in the name of fairness, mercy, or any other goal. (All of this is fairly obvious in the case of a chess AI, but it’s worth noting explicitly because there’s a greater temptation to anthropomorphize AI systems and assume they have human-like goals when the AI is capable of more general human behaviors, is tasked with imitating humans, etc.)\n*   Strategic and tactical flexibility. In spite of this rigidity in its objective, Stockfish is extremely flexible at the level of strategy and tactics. Interfere with Stockfish’s plans or put an obstacle in its way, and Stockfish will immediately change its plans to skillfully account for the obstacle.\n*   Planning with foresight and creativity. Stockfish will anticipate possible future obstacles (and opportunities), and will construct and execute sophisticated long-term plans, including brilliant feints and novelties, to maximize its odds of winning.\n\nObservers who note that systems like ChatGPT don’t seem particularly goal-oriented also tend to note that ChatGPT is bad at long-term tasks like “writing a long book series with lots of foreshadowing” or “large-scale engineering projects”. They might not see that these two observations are connected.\n\nIn a sufficiently large and surprising world that keeps throwing wrenches into existing plans, the way to complete complex tasks over long time horizons is to (a) possess relatively powerful and general skills for anticipating and adapting to obstacles to your plans; and (b) possess a disposition to tenaciously continue in the pursuit of objectives, without getting distracted or losing motivation — like how Stockfish single-mindedly persists in trying to win.\n\nThe demand for AI to be able to skillfully achieve long-term objectives is high, and as AI gets better at this, we can expect AI systems to appear correspondingly more goal-oriented. We can see this in, e.g., OpenAI o1, which does more long-term thinking and planning than previous LLMs, and indeed empirically [acts more tenaciously](https://www.transformernews.ai/p/openai-o1-alignment-faking) than previous models.\n\nGoal-orientedness isn’t sufficient for ASI, or Stockfish would be a superintelligence. But it seems very close to necessary: An AI needs the mental machinery to strategize, adapt, anticipate obstacles, etc., and it needs the disposition to readily deploy this machinery on a wide range of tasks, in order to reliably succeed in complex long-horizon activities.\n\nAs a strong default, then, smarter-than-human AIs are very likely to stubbornly reorient towards particular targets, regardless of what wrench reality throws into their plans. This is a good thing if the AI’s goals are good, but it’s an extremely dangerous thing if the goals aren’t what developers intend:\n\nIf an AI’s goal is to move a ball up a hill, then from the AI’s perspective, humans who get in the way of the AI achieving its goal count as “obstacles” in the same way that a wall counts as an obstacle. The exact same mechanism that makes an AI useful for long-time-horizon real-world tasks — relentless pursuit of objectives in the face of the enormous variety of blockers the environment will throw one’s way — will also make the AI want to prevent humans from interfering in its work. This may only be a nuisance when the AI is less intelligent than humans, but it becomes an enormous problem when the AI is smarter than humans.\n\nFrom the AI’s perspective, modifying the AI’s goals counts as an obstacle. If an AI is optimizing a goal, and humans try to change the AI to optimize a new goal, then unless the new goal also maximizes the old goal, the AI optimizing goal 1 will want to avoid being changed into an AI optimizing goal 2, because this outcome scores poorly on the metric “is this the best way to ensure goal 1 is maximized?”. This means that iteratively improving AIs won’t always be an option: If an AI becomes powerful before it has the right goal, it will want to subvert attempts to change its goal, since any change to its goals will seem bad from the AI’s perspective.\n\nFor the same reason, shutting down the AI counts as an obstacle to the AI’s objective. For almost any goal an AI has, the goal is more likely to be achieved if the AI is operational, so that it can continue to work towards the goal in question. The AI doesn’t need to have a self-preservation instinct in the way humans do; it suffices that the AI be highly capable and goal-oriented at all. Anything that could potentially interfere with the system’s future pursuit of its goal is liable to be treated as a threat.\n\nPower, influence, and resources further most AI goals. As we’ll discuss in the section “[It would be lethally dangerous to build ASIs that have the wrong goals](https://intelligence.org/the-problem/#4_lethally_dangerous)”, the best way to avoid potential obstacles, and to maximize your chances of accomplishing a goal, will often be to maximize your power and influence over the future, to gain control of as many resources as possible, etc. This puts powerful goal-oriented systems in direct conflict with humans for resources and control.\n\nAll of this suggests that it is critically important that developers robustly get the right goals into ASI. However, the prospects for succeeding in this seem extremely dim under the current technical paradigm.\n\n### **3.  ASI is very likely to pursue the wrong goals.**\n\nDevelopers are unlikely to be able to imbue ASI with a deep, persistent care for worthwhile objectives. Having spent two decades studying the technical aspects of this problem, our view is that the field is nowhere near to being able to do this in practice.\n\nThe reasons artificial superintelligence is likely to exhibit unintended goals include:\n\n*   In modern machine learning, AIs are “grown”, not designed.\n*   The current AI paradigm is poorly suited to robustly instilling goals.\n*   Labs and the research community are not approaching this problem in an effective and serious way.\n\n**In modern machine learning, AIs are “grown”, not designed.**\n\nDeep learning algorithms build neural networks automatically. Geoffrey Hinton explains this point well in an [interview](https://youtu.be/qrvK_KuIeJk?t=288) on 60 Minutes:\n\n> **HINTON:** We have a very good idea of sort of roughly what it’s doing, but as soon as it gets really complicated, we don’t actually know what’s going on, any more than we know what’s going on in your brain.\n> \n> **PELLEY:** What do you mean, “We don’t know exactly how it works”? It was designed by people.\n> \n> **HINTON:** No, it wasn’t. What we did was we designed the learning algorithm. That’s a bit like designing the principle of evolution. But when this learning algorithm then interacts with data, it produces complicated neural networks that are good at doing things, but we don’t really understand exactly how they do those things.\n\nEngineers can’t tell you why a modern AI makes a given choice, but have nevertheless released increasingly capable systems year after year. AI labs are aggressively scaling up systems they don’t understand, with little ability to predict the capabilities of the next generation of systems.\n\nRecently, the young field of mechanistic interpretability has attempted to address the opacity of modern AI by mapping a neural network’s configuration to its outputs. Although there has been nonzero real progress in this area, interpretability pioneers are very clear that we’re still fundamentally in the dark about what’s going on inside these systems:\n\n*   Leo Gao of OpenAI: “I think it is quite accurate to say we don’t understand how neural networks work.” ([2024-6-16](https://x.com/nabla_theta/status/1802292064824242632))\n*   Neel Nanda of Google DeepMind: “As lead of the Google DeepMind mech interp team, I strongly seconded. It’s absolutely ridiculous to go from ‘we are making interp progress’ to ‘we are on top of this’ or ‘x-risk won’t be an issue’.” ([2024-6-16](https://x.com/NeelNanda5/status/1804613268356399185))\n\n(“X-risk” refers to “existential risk”, the risk of human extinction or similarly bad outcomes.)\n\nEven if effective interpretability tools were in reach, however, the prospects for achieving nontrivial robustness properties in ASI would be grim.\n\nThe internal machinery that could make an ASI dangerous is the same machinery that makes it work at all. (What looks like “power-seeking” in one context would be considered “good hustle” in another.) There are no dedicated “badness” circuits for developers to monitor or intervene on.\n\nMethods developers might use during training to reject candidate AIs with thought patterns they consider dangerous can have the effect of driving such thoughts “underground”, making it increasingly unlikely that they’ll be able to detect warning signs during training in the future.\n\nAs AI becomes more generally capable, it will become increasingly good at deception. The January 2024 “[Sleeper Agents](https://arxiv.org/abs/2401.05566)” paper by Anthropic’s testing team demonstrated that an AI given secret instructions in training not only was capable of keeping them secret during evaluations, but made strategic calculations (incompetently) about when to lie to its evaluators to maximize the chance that it would be released (and thereby be able to execute the instructions). Apollo Research made similar findings with regards to OpenAI’s o1-preview model released in September 2024 (as described in [their contributions to the o1-preview system card](https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf), p.10).\n\nThese issues will predictably become more serious as AI becomes more generally capable. The first AIs to inch across high-risk thresholds, however — such as noticing that they are in training and plotting to deceive their evaluators — are relatively bad at these new skills. This causes some observers to prematurely conclude that the behavior category is unthreatening.\n\nThe indirect and coarse-grained way in which modern machine learning “grows” AI systems’ internal machinery and goals means that we have little ability to predict the behavior of novel systems, little ability to robustly or precisely shape their goals, and no reliable way to spot early warning signs.\n\nWe expect that there are ways in principle to build AI that doesn’t have these defects, but this constitutes a long-term hope for what we might be able to do someday, not a realistic hope for near-term AI systems.\n\n**The current AI paradigm is poorly suited to robustly instilling goals.**\n\nDocility and goal agreement don’t come for free with high capability levels. An AI system can be able to answer an ethics test in the way its developers want it to, without thereby having human values. An AI can behave in docile ways when convenient, without actually being docile.\n\n**ASI alignment** is the set of technical problems involved in robustly directing superintelligent AIs at intended objectives.\n\nASI alignment runs into two classes of problem, discussed in [Hubinger et al.](https://arxiv.org/abs/1906.01820) — problems of **outer alignment**, and problems of **inner alignment**.\n\nOuter alignment, roughly speaking, is the problem of picking the right goal for an AI. (More technically, it’s the problem of ensuring the learning algorithm that builds the ASI is optimizing for what the programmers want.)This runs into issues such as “human values are too complex for us to specify them just right for an AI; but if we only give ASI some of our goals, the ASI is liable to trample over our other goals in pursuit of those objectives”. Many goals are safe at lower capability levels, but dangerous for a sufficiently capable AI to carry out in a maximalist manner. The literary trope here is “be careful what you wish for”. Any given goal is unlikely to be safe to delegate to a sufficiently powerful optimizer, because the developers are not superhuman and can’t predict in advance what strategies the ASI will think of.\n\nInner alignment, in contrast, is the problem of figuring out how to get particular goals into ASI at all, even imperfect and incomplete goals. The literary trope here is “just because you summoned a demon doesn’t mean that it will do what you say”. Failures of inner alignment look like “we tried to give a goal to the ASI, but we failed and it ended up with an unrelated goal”.\n\n**Outer alignment and inner alignment are both unsolved problems**, and in this context, **inner alignment is the more fundamental issue**. Developers aren’t on track to be able to cause a catastrophe of the “be careful what you wish for” variety, because realistically, we’re extremely far from being able to metaphorically “make wishes” with an ASI.\n\nModern methods in AI are a poor match for tackling inner alignment. Modern AI development doesn’t have methods for getting particular inner properties into a system, or for verifying that they’re there. Instead, modern machine learning concerns itself with observable behavioral properties that you can run a loss function over.\n\nWhen minds are grown and shaped iteratively, like modern AIs are, they won’t wind up pursuing the objectives they’re trained to pursue. Instead, training is far more likely to lead them to pursue unpredictable proxies of the training targets, which are brittle in the face of increasing intelligence. By way of analogy: Human brains were ultimately “designed” by natural selection, which had the simple optimization target “maximize inclusive genetic fitness”. The actual goals that ended up instilled in human brains, however, were far more complex than this, and turned out to only be fragile correlates for inclusive genetic fitness. Human beings, for example, pursue proxies of good nutrition, such as sweet and fatty flavors. These proxies were once reliable indicators of healthy eating, but were brittle in the face of technology that allows us to invent novel junk foods. The case of humans illustrates that even when you have a very exact, very simple loss function, outer optimization for that loss function doesn’t generally produce inner optimization in that direction. Deep learning is much less random than natural selection at finding adaptive configurations, but it shares the relevant property of finding minimally viable simple solutions first and incrementally building on them.\n\nMany alignment problems relevant to superintelligence don’t naturally appear at lower, passively safe levels of capability. This puts us in the position of needing to solve many problems on the first critical try, with little time to iterate and no prior experience solving the problem on weaker systems. Today’s AIs require a long process of iteration, experimentation, and feedback to hammer them into the apparently-obedient form the public is allowed to see. This hammering changes surface behaviors of AIs without deeply instilling desired goals into the system. This can be seen in cases like [Sydney](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html), where the public was able to see more of the messy details behind the surface-level polish. In light of this, and in light of the opacity of modern AI models, the odds of successfully aligning ASI if it’s built in the next decade seem extraordinarily low. Modern AI methods are all about repeatedly failing, learning from our mistakes, and iterating to get better; AI systems are highly unpredictable, but we can get them working eventually by trying many approaches until one works. In the case of ASI, we will be dealing with a highly novel system, in a context where our ability to safely fail is extremely limited: we can’t charge ahead and rely on our ability to learn from mistakes when the cost of some mistakes is an extinction event.\n\nIf you’re deciding whether to hand a great deal of power to someone and you want to know whether they would abuse this power, you won’t learn anything by giving the candidate power in a board game where they know you’re watching. Analogously, situations where an ASI has no real option to take over are fundamentally different from situations where it does have a real option to take over. No amount of purely behavioral training in a toy environment will reliably eliminate power-seeking in real-world settings, and no amount of behavioral testing in toy environments will tell us whether we’ve made an ASI genuinely friendly. “Lay low and act nice until you have an opportunity to seize power” is a sufficiently obvious strategy that even relatively unintelligent humans can typically manage it; ASI trivially clears that bar. In principle, we could imagine developing a theory of intelligence that relates ASI training behavior to deployment behavior in a way that addresses this issue. We are nowhere near to having such a theory today, however, and those theories can fundamentally only be tested once in the actual environment where the AI is much much smarter and sees genuine takeover options. If you can’t properly test theories without actually handing complete power to the ASI and seeing what it does — and causing an extinction event if your theory turned out to be wrong — then there’s very little prospect that your theory will work in practice.\n\nThe most important alignment technique used in today’s systems, Reinforcement Learning from Human Feedback (RLHF), trains AI to produce outputs that it predicts would be rated highly by human evaluators. This already creates its own predictable problems, such as style-over-substance and flattery. This method breaks down completely, however, when AI starts working on problems where humans aren’t smart enough to fully understand the system’s proposed solutions, including the long-term consequences of superhumanly sophisticated plans and superhumanly complex inventions and designs.\n\nOn a deeper level, the limitation of reinforcement learning strategies like RLHF stems from the fact that these techniques are more about incentivizing local behaviors than about producing an internally consistent agent that deeply and robustly optimizes a particular goal the developers intended.\n\nIf you train a tiger not to eat you, you haven’t made it share your desire to survive and thrive, with a full understanding of what that means to you. You have merely taught it to associate certain behaviors with certain outcomes. If its desires become stronger than those associations, as could happen if you forget to feed it, the undesired behavior will come through. And if the tiger were a little smarter, it would not need to be hungry to conclude that the threat of your whip would immediately end if your life ended.\n\n<table style=\"background-color:transparent\"><tbody><tr><td style=\"background-color:rgba(128, 128, 128, 0.07);border-color:rgba(128, 128, 128, 0.5);padding:15px;vertical-align:top\">Learn more:<a href=\"https://intelligence.org/agi-ruin\"> <strong>What are the details of why ASI alignment looks extremely technically difficult?</strong></a></td></tr></tbody></table>\n\nAs a consequence, MIRI doesn’t see any viable quick fixes or workarounds to misaligned ASI.\n\n*   If an ASI has the wrong goals, then it won’t be possible to safely use the ASI for any complex real-world operation. One could theoretically keep an ASI from doing anything harmful — for example, by preemptively burying it deep in the ground without any network connections or human contact — but such an AI would be useless. People are building AI because they want it to radically impact the world; they are consequently giving it the access it needs to be impactful.\n*   One could attempt to deceive an ASI in ways that make it more safe. However, attempts to deceive a superintelligence are prone to fail, including in ways we can’t foresee. A feature of intelligence is the ability to notice the contradictions and gaps in one’s understanding, and interrogate them. In May 2024, when Anthropic modified their Claude AI into thinking that the answer to every request [involved the Golden Gate Bridge](https://www.anthropic.com/research/mapping-mind-language-model), it [floundered](https://archive.is/u7HJm) in some cases, noticing the contradictions in its replies and trying to route around the errors in search of better answers. It’s hard to sell a false belief to a mind whose complex model of the universe disagrees with your claim; and as AI becomes more general and powerful, this difficulty only increases.\n*   Plans to align ASI using unaligned AIs are similarly unsound. Our 2024 “[Misalignment and Catastrophe](https://intelligence.org/wp-content/uploads/2024/02/Misalignment_and_Catastrophe.pdf)” paper explores the hazards of using unaligned AI to do work as complex as alignment research.\n\n**Labs and the research community are not approaching this problem in an effective and serious way.**\n\nIndustry efforts to solve ASI alignment have to date been minimal, often seeming to serve as a fig leaf to ward off regulation. Labs’ general laxness on information security, alignment, and strategic planning suggests that the “move fast and break things” culture that’s worked well for accelerating capabilities progress is not similarly useful when it comes to exercising foresight and responsible priority-setting in the domain of ASI.\n\nOpenAI, the developer of ChatGPT, admits that today’s most important methods of steering AI won’t scale to the superhuman regime. In July of 2023, OpenAI announced a new team with their “[Introducing Superalignment](https://openai.com/index/introducing-superalignment/)” page. From the page:\n\n> Currently, we don’t have a solution for steering or controlling a potentially superintelligent AI, and preventing it from going rogue. Our current techniques for aligning AI, such as reinforcement learning from human feedback, rely on humans’ ability to supervise AI. But humans won’t be able to reliably supervise AI systems much smarter than us, and so our current alignment techniques will not scale to superintelligence. We need new scientific and technical breakthroughs.\n\nTen months later, OpenAI disbanded their superintelligence alignment team in the wake of mass resignations, as researchers like Superalignment team lead Jan Leike [claimed](https://twitter.com/janleike/status/1791498174659715494) that OpenAI was systematically cutting corners on safety and robustness work and severely under-resourcing their team. Leike had previously said, in an August 2023 [interview](https://80000hours.org/podcast/episodes/jan-leike-superalignment/), that the probability of extinction-level catastrophes from ASI was probably somewhere between 10% and 90%.\n\nGiven the research community’s track record to date, we don’t think a well-funded crash program to solve alignment would be able to correctly identify solutions that won’t kill us. This is an organizational and bureaucratic problem, and not just a technical one. It would be difficult to find enough experts who can identify non-lethal solutions to make meaningful progress, in part because the group must be organized by someone with the expertise to correctly identify these individuals in a sea of people with strong incentives to lie (both to themselves and to regulators) about how promising their favorite proposal is.\n\nIt would also be difficult to ensure that the organization is run by, and only answerable to, experts who are willing and able to reject any bad proposals that bubble up, even if this initially means rejecting literally every proposal. There just aren’t enough experts in that class right now.\n\nOur current view is that a survivable way forward will likely require ASI to be delayed for a long time. The scale of the challenge is such that we could easily see it taking multiple generations of researchers exploring technical avenues for aligning such systems, and bringing the fledgling alignment field up to speed with capabilities. It seems extremely unlikely, however, that the world has that much time.\n\n### **4\\. It would be lethally dangerous to build ASIs that have the wrong goals.**\n\nIn “[ASI is very likely to exhibit goal-oriented behavior](https://intelligence.org/the-problem/#2_goal-oriented_behavior)”, we introduced the chess AI Stockfish. Stuart Russell, the author of the most widely used AI textbook, has previously [explained](https://youtu.be/mukaRhQTMP8?t=36) AI-mediated extinction via a similar analogy to chess AI:\n\n> At the state of the art right now, humans are toast. No matter how good you are at playing chess, these programs will just wipe the floor with you, even running on a laptop.\n> \n> I want you to imagine that, and just extend that idea to the whole world. \\[…\\] The world is a larger chess board, on which potentially at some time in the future machines will be making better moves than you. They’ll be taking into account more information, and looking further ahead into the future, and so if you are playing a game against a machine in the world, the assumption is that at some point we will lose.\n\nIn a July 2023 [US Senate hearing](https://cdss.berkeley.edu/news/stuart-russell-testifies-ai-regulation-us-senate-hearing), Russell testified that “achieving AGI \\[artificial general intelligence\\] would present potential catastrophic risks to humanity, up to and including human extinction”.\n\nStockfish captures pieces and limits its opponent’s option space, not because Stockfish hates chess pieces or hates its opponent but because these actions are instrumentally useful for its objective (“win the game”). The danger of superintelligence is that ASI will be trying to “win” (at a goal we didn’t intend), but with the game board replaced with the physical universe.\n\nJust as Stockfish is ruthlessly effective in the narrow domain of chess, AI that automates all key aspects of human intelligence will be ruthlessly effective in the real world. And just as humans are vastly outmatched by Stockfish in chess, we can expect to be outmatched in the world at large once AI is able to play that game at all.\n\nIndeed, outmaneuvering a strongly smarter-than-human adversary is far more difficult in real life than in chess. Real life offers a far more multidimensional option space: we can anticipate a hundred different novel attack vectors from a superintelligent system, and still not have scratched the surface.\n\nUnless it has worthwhile goals, ASI will predictably put our planet to uses incompatible with our continued survival, in the same basic way that we fail to concern ourselves with the crabgrass at a construction site. This extreme outcome doesn’t require any malice, resentment, or misunderstanding on the part of the ASI; it only requires that ASI behaves like a new intelligent species that is indifferent to human life, and that strongly surpasses our intelligence.\n\nWe can decompose the problem into two parts:\n\n*   Misaligned ASI will be motivated to take actions that disempower and wipe out humanity, either directly or as a side-effect of other operations.\n*   ASI will be able to destroy us.\n\n**Misaligned ASI will be motivated to take actions that disempower and wipe out humanity.**\n\nThe basic reason for this is that an ASI with non-human-related goals will generally want to maximize its control over the future, and over whatever resources it can acquire, to ensure that its goals are achieved.\n\nSince this is true for a wide variety of goals, it operates as a default endpoint for a variety of paths AI development could take. We can predict that ASI will want very basic things like “more resources” and “greater control” — at least if developers fail to align their systems — without needing to speculate about what specific ultimate objectives an ASI might pursue.\n\n(Indeed, trying to call the objective in advance seems hopeless if the situation at all resembles what we see in nature. Consider how difficult it would have been to guess in advance that human beings would end up with the many specific goals we have, from “preferring frozen ice cream over melted ice cream” to “enjoying slapstick comedy”.)\n\nThe extinction-level danger from ASI follows from several behavior categories that a wide variety of ASI systems are likely to exhibit:\n\n*   Resource extraction**.** Humans depend for their survival on resource flows that are also instrumentally useful for almost any other goal. Air, sunlight, water, food, and even the human body are all made of matter or energy that can be repurposed to help with other objectives on the margin. In slogan form: “The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.”\n*   Competition for control. Humans are a potential threat and competitor to any ASI. If nothing else, we could threaten an ASI by building a second ASI with a different set of goals. If the ASI has an easy way to eliminate all rivals and never have to worry about them again, then it’s likely to take that option.\n*   Infrastructure proliferation. Even if an ASI is too powerful to view humans as threats, it is likely to quickly wipe humans out as a side-effect of extracting and utilizing local resources. If an AI is thinking at superhuman speeds and building up self-replicating machinery exponentially quickly, the Earth could easily become uninhabitable within a few months, as engineering megaprojects emit waste products and heat that can rapidly make the Earth inhospitable for biological life.\n\nPredicting the specifics of what an ASI would do seems impossible today. This is not, however, grounds for optimism, because most possible goals an ASI could exhibit would be very bad for us, and most possible states of the world an ASI could attempt to produce would be incompatible with human life.\n\nIt would be a fallacy to reason in this case from “we don’t know the specifics” to “good outcomes are just as likely as bad ones”, much as it would be a fallacy to say “I’m either going to win the lottery or lose it, therefore my odds of winning are 50%”. Many different pathways in this domain appear to converge on catastrophic outcomes for humanity — most of the “lottery tickets” humanity could draw will be losing numbers.\n\nThe arguments for optimism here are uncompelling. Ricardo’s Law of Comparative Advantage, for example, has been cited as a possible reason to expect ASI to keep humans around indefinitely, even if the ASI doesn’t ultimately care about human welfare. In the context of microeconomics, Ricardo’s Law teaches that even a strictly superior agent can benefit from trading with a weaker agent.\n\nThis law breaks down, however, when one partner has more to gain from overpowering the other than from voluntarily trading. This can be seen, for example, in the fact that humanity didn’t keep “trading” with horses after we invented the automobile — we replaced them, converting surplus horses into glue.\n\nHumans found more efficient ways to do all of the practical work that horses used to perform, at which point horses’ survival depended on how much we sentimentally care about them, not on horses’ usefulness in the broader economy. Similarly, keeping humans around is unlikely to be the most efficient solution to any problem that the AI has. E.g., rather than employing humans to conduct scientific research, the AI can build an ever-growing number of computing clusters to run more instances of itself, or otherwise automate research efforts.\n\n**ASI will be able to destroy us.**\n\nAs a minimum floor on capabilities, we can imagine ASI as a small nation populated entirely by brilliant human scientists who can work around the clock at ten thousand times the speed of normal humans.\n\nThis is a minimum both because computers can be even faster than this, and because digital architectures should allow for qualitatively better thoughts and methods of information-sharing than humans are capable of.\n\nTransistors can switch states millions to billions of times faster than synaptic connections in the human brain. This would mean that every week, the ASI makes an additional two hundred years of scientific progress. The core reason to expect ASI to win decisively in a conflict, then, is the same as the reason a 21st-century military would decisively defeat an 11th-century one: technological innovation.\n\nDeveloping new technologies often requires test cycles and iteration. A civilization thinking at 10,000 times the speed of ours cannot necessarily develop technology 10,000 times faster, any more than a car that’s 100x faster would let you shop for groceries 100x faster — traffic, time spent in the store, etc. will serve as a bottleneck.\n\nWe can nonetheless expect such a civilization to move extraordinarily quickly, by human standards. Smart thinkers can find all kinds of ways to shorten development cycles and reduce testing needs.\n\nConsider the difference in methods between Google software developers, who rapidly test multiple designs a day, and designers of space probes, who plan carefully and run cheap simulations so they can get the job done with fewer slow and expensive tests.\n\nTo a mind thinking faster than a human, every test is slow and expensive compared to the speed of thought, and it can afford to treat everything like a space probe. One implication of this is that ASI is likely to prioritize the development and deployment of small-scale machinery (or engineered microorganisms) which, being smaller, can run experiments, build infrastructure, and conduct attacks orders of magnitude faster than humans and human-scale structures.\n\nA superintelligent adversary will not reveal its full capabilities and telegraph its intentions. It will not offer a fair fight. It will make itself indispensable or undetectable until it can strike decisively and/or seize an unassailable strategic position. If needed, the ASI can consider, prepare, and attempt many takeover approaches simultaneously. Only one of them needs to work for humanity to go extinct.\n\nThere are a number of major obstacles to recognizing that a system is a threat before it has a chance to do harm, even for experts with direct access to its internals.\n\n<table style=\"background-color:transparent\"><tbody><tr><td style=\"background-color:rgba(128, 128, 128, 0.07);border-color:rgba(128, 128, 128, 0.5);padding:15px;vertical-align:top\">Learn more:<a href=\"http://intelligence.org/notes/takeover\"> <strong>What’s an example of how ASI takeover could occur?</strong></a></td></tr></tbody></table>\n\nRecognizing that a particular AI is a threat, however, is not sufficient to solve the problem. At the project level, identifying that a system is dangerous doesn’t put us in a position to make that system safe. Cautious projects may voluntarily halt, but this does nothing to prevent other, incautious projects from storming ahead.\n\nAt the global level, meanwhile, clear evidence of danger doesn’t necessarily mean that there will be the political will to internationally halt development. AI is likely to become increasingly entangled with the global economy over time, making it increasingly costly and challenging to shut down state-of-the-art AI services. Steps could be taken today to prevent critical infrastructure from becoming dependent on AI, but the window for this is plausibly closing.\n\nMany analyses seriously underestimate the danger posed by building systems that are far smarter than any human. Four common kinds of error we see are:\n\n*   [Availability bias](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) and overreliance on analogies. AI extinction scenarios can sound extreme and fantastical. Humans are used to thinking about unintelligent machines and animals, and intelligent humans. “It’s a machine, but one that’s intelligent in the fashion of a human” is something genuinely new, and people make different errors from trying to pattern-match AI to something familiar, rather than modeling it on its own terms.\n*   Underestimating feedback loops. AI is used today to accelerate software development, including AI research. As AI becomes more broadly capable, an increasing amount of AI progress is likely to be performed by AIs themselves. This can rapidly spiral out of control, as AIs find ways to improve on their own ability to do AI research in a self-reinforcing loop.\n*   Underestimating exponential growth. Many plausible ASI takeover scenarios route through building self-replicating biological agents or machines. These scenarios make it relatively easy for ASI to go from “undetectable” to “ubiquitous”, or to execute covert strikes, because of the speed at which doublings can occur and the counter-intuitively small number of doublings required.\n*   Overestimating human cognitive ability, relative to what’s possible. Even in the absence of feedback loops, AI systems routinely blow humans out of the water in narrow domains. As soon as AI can do X at all (or very soon afterwards), AI vastly outstrips any human’s ability to do X. This is a common enough pattern in AI, at this point, to barely warrant mentioning. It would be incredibly strange if this pattern held for every skill AI is already good at, but suddenly broke for the skills AI can’t yet match top humans on, such as novel science and engineering work.\n\nWe should expect ASIs to vastly outstrip humans in technological development soon after their invention. As such, we should also expect ASI to very quickly accumulate a decisive strategic advantage over humans, as they outpace humans in this strategically critical ability to the same degree they’ve outpaced humans on hundreds of benchmarks in the past.\n\nThe main way we see to avoid this catastrophic outcome is to not build ASI at all, at minimum until a scientific consensus exists that we can do so without destroying ourselves.\n\n### **5\\. Catastrophe can be averted via a sufficiently aggressive policy response.**\n\nIf anyone builds ASI, everyone dies. This is true whether it’s built by a private company or by a military, by a liberal democracy or by a dictatorship.\n\nASI is strategically very novel. Conventional powerful technology isn’t an intelligent adversary in its own right; typically, whoever builds the technology “has” that technology, and can use it to gain an advantage on the world stage.\n\nAgainst a technical backdrop that’s at all like the current one, ASI instead functions like a sort of global suicide bomb — a volatile technology that blows up and kills its developer (and the rest of the world) at an unpredictable time. If you build smarter-than-human AI, you don’t thereby “have” an ASI; rather, the ASI has you.\n\nProgress toward ASI needs to be halted until ASI can be made alignable. Halting ASI progress would require an effective worldwide ban on its development, and tight control over the factors of its production.\n\nThis is a large ask, but domestic oversight in the US, mirrored by a few close allies, will not suffice. This is not a case where we just need the “right” people to build it before the “wrong” people do.\n\nA “wait and see” approach to ASI is probably not survivable, given the fast pace of AI development and the difficulty of predicting the point of no return — the threshold where ASI is achieved.\n\nOn our view, **the international community’s top immediate priority should be creating an “off switch” for frontier AI development**. By “creating an off switch”, we mean putting in place the systems and infrastructure necessary to either shut down frontier AI projects or enact a general ban.\n\nCreating an off switch would involve identifying the relevant parties, tracking the relevant hardware, and requiring that advanced AI work take place within a limited number of monitored and secured locations. It extends to building out the protocols, plans, and chain of command to be followed in the event of a shutdown decision.\n\nAs the off-switch could also provide resilience to more limited AI mishaps, we hope it will find broader near-term support than a full ban. For “limited AI mishaps”, think of any lower-stakes situation where it might be desirable to shut down one or more AIs for a period of time. This could be something like a bot-driven misinformation cascade during a public health emergency, or a widespread Internet slowdown caused by AIs stuck in looping interactions with each other and generating vast amounts of traffic. Without off-switch infrastructure, any response is likely to be haphazard — delayed by organizational confusion, mired in jurisdictional disputes, beset by legal challenges, and unable to avoid causing needless collateral harm.\n\nAn off-switch can only prevent our extinction from ASI if it has sufficient reach and is actually used to shut down progress toward ASI sufficiently soon. If humanity is to survive this dangerous period, it will have to stop treating AI as a domain for international rivalry and demonstrate a collective resolve equal to the scale of the threat.\n\n[^fpb6rwvoxuo]: This essay was primarily written by Rob Bensinger, with major contributions throughout by Mitchell Howe. It was edited by William and reviewed by Nate Soares and Eliezer Yudkowsky, and the project was overseen by Gretta Duleba. It was a team effort, and we're grateful to others who provided feedback (at MIRI and beyond).",
      "plaintextDescription": "This is a new introduction to AI as an extinction threat, previously posted to the MIRI website in February alongside a summary. It was written independently of Eliezer and Nate's forthcoming book, If Anyone Builds It, Everyone Dies, and isn't a sneak peak of the book. Since the book is long and costs money, we expect this to be a valuable resource in its own right even after the book comes out next month.[1]\n\nThe stated goal of the world’s leading AI companies is to build AI that is general enough to do anything a human can do, from solving hard problems in theoretical physics to deftly navigating social environments. Recent machine learning progress seems to have brought this goal within reach. At this point, we would be uncomfortable ruling out the possibility that AI more capable than any human is achieved in the next year or two, and we would be moderately surprised if this outcome were still two decades away.\n\nThe current view of MIRI’s research scientists is that if smarter-than-human AI is developed this decade, the result will be an unprecedented catastrophe. The CAIS Statement, which was widely endorsed by senior researchers in the field, states:\n\n> Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n\nWe believe that if researchers build superintelligent AI with anything like the field’s current technical understanding or methods, the expected outcome is human extinction.\n\n“Research labs around the world are currently building tech that is likely to cause human extinction” is a conclusion that should motivate a rapid policy response. The fast pace of AI, however, has caught governments and the voting public flat-footed. This document will aim to bring readers up to speed, and outline the kinds of policy steps that might be able to avert catastrophe.\n\nKey points in this document:\n\n * There isn’t a ceiling at human-level capabilities.\n * ASI is very likely to exhibit go",
      "wordCount": 7921
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FY697dJJv9Fq3PaTd",
    "title": "HPMOR: The (Probably) Untold Lore",
    "slug": "hpmor-the-probably-untold-lore",
    "url": null,
    "baseScore": 420,
    "voteCount": 213,
    "viewCount": null,
    "commentCount": 152,
    "createdAt": null,
    "postedAt": "2025-07-25T18:39:44.903Z",
    "contents": {
      "markdown": "Eliezer and I love to talk about writing. We talk about our own current writing projects, how we’d improve the books we’re reading, and what we want to write next. Sometimes along the way I learn some amazing fact about [HPMOR](https://hpmor.com/) or [Project Lawful](http://projectlawful.com) or one of Eliezer’s other works. “Wow, you’re kidding,” I say, “do your fans know this? I think people would really be interested.”\n\n“I can’t remember,” he usually says. “I don’t think I’ve ever explained that bit before, I’m not sure.”\n\nI decided to interview him more formally, collect as many of those tidbits about HPMOR as I could, and share them with you. I hope you enjoy them.\n\nIt’s probably obvious, but there will be many, many spoilers for HPMOR in this article, and also very little of it will make sense if you haven’t read the book. So go read [Harry Potter and the Methods of Rationality](http://hpmor.com) before you read any further.\n\nWe talk about HPMOR’s characters, including how Eliezer tried to make every single character awesome, and why Hermione gets unicorn horn teeth. We talk about the plot, and learn some secrets about Harry’s sexuality. We talk about the setting, and Eliezer explains the Nested Nerfing Hypothesis of magic in the HPMOR universe. And finally, there’s some news about the epilogues—plural!\n\n**While I’m here, I’ll also mention that Eliezer has a** [**non-fiction book**](https://ifanyonebuildsit.com/)** coming out on September 16th, 2025 and I personally think it would be excellent if many thousands of people pre-ordered that book.**\n\nAnd now, on with the lore!\n\n**Characters**\n==============\n\nMasks  \n \n---------\n\n**Gretta:** Can you please explain the “dramatic masks” idea that we’ve talked about, and how it applies to HPMOR?  \n\n**Eliezer:** So there's a science fiction and or fantasy author, [Stephen R. Donaldson](https://en.wikipedia.org/wiki/Stephen_R._Donaldson), who made an observation in the afterword to one of his books.[^2r4ocsd9z9g] Donaldson says that the difference between drama and melodrama[^xw173o3fg9] is that in a drama, the characters change their masks over the course of the story, and in melodrama, they wear the same masks throughout the story.[^d9fbo8b7k1p] \n\nAnd in particular, Donaldson gave an example, which was more influential on me than the underlying statement, of how in one of his stories, it starts out with a victim, a victimizer, and a rescuer. And then by the end of the story, they have all exchanged masks.  \n\n**Gretta:** Okay. So three people, three masks. And they rotate. And I just want to clarify, when we're talking about masks, we're not talking about something that a character is pretending to do or be.  \n\n**Eliezer:** Yeah. This is the literary sense of mask. Like a role that a person wears inside the story, whether or not they're conscious of it. \n\n**Gretta:** So harkening back to like stage plays where people would literally wear a physical mask in order to indicate what character they're playing. \n\n**Eliezer:** Right. These are the masks of the stage play, not masks inside the play. They're masks outside the play. Denoting which character you are. \n\nAnd in a drama, according to Donaldson, they rotate. Now, Donaldson’s example is literarily perfect,[^5s65zuoaytv] whereas HPMOR does not quite stand up the same way. There are only two masks for three people, and the masks themselves are different.\n\nBut by the end of the story the mysterious old wizard mask has moved from Dumbledore to Harry, and Harry's brash young hero mask has moved to Hermione. (Hermione's mask does not, so far as I noticed, move to Dumbledore.)  \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/768669cdeeb6402e1735511416a90b8996dbaf50569c105e.png)\n\n  \n \n\n**Gretta:** What mask was Hermione wearing at the beginning of the story?    \n \n\n**Eliezer:** At the beginning of the story, she’s not quite asserting herself as having a mask. School know-it-all, maybe?\n\nBy the time she's in the middle of the story, she's got much more complicated things going on. She has multiple self concepts, plus she’s fighting against concepts imposed upon her by the school.  \n \n\n**Gretta:** What are her multiple self concepts?   \n \n\n**Eliezer:** To answer that, I need to back up a bit. This might sound initially like it’s not quite about characters. Orson Scott Card, I believe, observed that while a conflict between good and evil might hold the attention of some readers, a conflict between good and good can be much stronger than that.[^jtoyf7990y]\n\nAnd why is this? Because for the same reason that a conflict between two strong forces is more literarily interesting than a conflict between a strong force and a weak force, if only one side has good arguments, that's less of an interesting debate than if both sides have strong arguments. \n\nAnd the extension of this to characters, and I don't actually remember at this point, if this exact way of phrasing it is original to me or not, is that you might think of a three dimensional character as one who contains at least two two-dimensional characters. \n\nLike, you just take a bunch of flat characters and put them all into the same person, and now, lo and behold, you have a three dimensional character. \n\nThe less interesting version of this is if the character has a good side and evil side, and/or a sympathetic side and an unsympathetic side. And the much stronger version of it is when they've got multiple sympathetic sides in conflict. \n\nSo coming back to Hermione — Hermione is, on the one hand, the hero, and on the other hand, a sensible young lady who has way too much common sense to go in for that sort of thing, both in the same person.   \n \n\n**Gretta:** Way too much common sense to go in for being a hero?  \n \n\n**Eliezer:** Yeah. At first, she’s too sensible to do all this dangerous stuff that Harry does — though later, she leans into the hero side and she goes off and does dangerous stuff too.    \n \n\n**Gretta:** I see. She knows better than that. She knows better than to take these kind of risks, or —    \n \n\n**Eliezer:** Yeah, or to get herself into this sort of trouble. She has this drive to assert her reality independently of Harry's, despite how the lumpenproletariat of Hogwarts try to cast Hermione as Harry’s tagalong romantic interest, as a bit player in Harry’s story. Harry has a lot of sympathy for Hermione’s plight. He wouldn’t like it if someone tried to cast him as the sidekick, either, so he fights alongside her against the gossip network’s attempts to typecast her, but it doesn’t particularly work, Hogwarts continues to apply that false trope to Hermione.\n\nAnd this is like a mask that she wears that is independent of her actual heroism. There's some amount of performative heroism she's doing to distinguish herself from Harry, but somewhat separately from whether she is an actual hero or not. Sometimes the actual heroism and the performative heroism overlap.    \n \n\n**Gretta:** What are some examples of performative heroism versus real heroism on Hermione's part?    \n \n\n**Eliezer:** So the Society for the Promotion of Heroic Equality for Witches (SPHEW) is Hermione consciously wearing the hero mask. When she asks, “How can I be a hero?\" she is consciously performing the hero role, consciously wearing the mask.[^wz41hx9zt4] \n\nBut also, she hears the echoing cry of the phoenix, runs into an actual instance of bullying and stops it without very much thinking about it.[^wgnyz075ts] And that's actual heroism.   \n \n\n**Gretta:** What happens to Dumbledore when he takes off the wise old wizard mask?    \n \n\n**Eliezer:** Inside the story, when Dumbledore loses his wise old wizard mask, he gets taken off the board immediately thereafter. He’s snapped out of time, sealed inside the mirror. Dumbledore knew that something like that was going to happen. He prepared for it in advance.\n\nBut it’s also interesting to consider what we learn about Dumbledore, who he really was, underneath that wise old wizard mask, which was after all just a role he was playing, and a role that he lost at the end.\n\nWhen we find out that Dumbledore has been reading prophecies, this changes his apparent role in the story even in retrospect. It’s a moment of not just changing ongoing roles but also of revealing that he had a different story role than we thought on first readthrough. Now his actions make sense in a different way. We know what he was thinking and not just what he was doing.\n\nWe also find out that he was mistaken about a few of the prophecies. He thought some of them were about himself, but they were actually about Professor Quirrell, who was also Harry’s mentor.\n\nOn the object level, though, what happens is that he’s out of the game.  \n \n\n**Gretta:** Yeah, it's rough when there's no mask for you anymore. It's like playing musical chairs and, whoops, no chair for you.    \n \n\n**Eliezer:** Yeah. Dumbledore might get one back at some point. That would be a matter for the future of the story.   \n \n\nImperfect Characters  \n \n------------------------\n\n**Gretta:** In my writing books, it says to give your characters and especially your protagonist flaws, but you [write about having respect](https://yudkowsky.tumblr.com/writing/empathyrespect) for your characters, [including for your villains](https://yudkowsky.tumblr.com/writing/realistic-viewpoints). \n\nSo what in your mind constitutes a respectable flaw?    \n \n\n**Eliezer:** There are many kinds of respectable flaws. But a very strong, classic pathway is to take a mistake that you made in the past and that you're still sympathetic to yourself for having made.\n\nYou're like, “Yeah, that was a mistake, but I had reasons for that mistake that was not just me being stupid, all of my books gave me the wrong advice, I was trying to do the right thing there. I had no way of knowing this.” \n\nAnd I'm not saying to go around being sympathetic for yourself to yourself for all your mistakes.  But maybe there are some that, despite your strong [Irorian](https://pathfinderwiki.com/wiki/Irori) self-improving heart, you can still manage to find some sympathy for yourself for making. And especially, maybe you remember how at the time it didn't seem like a flaw to you. You were not going around being like, “And today I shall be a flawed character.” You were trying to do the right thing, and not perform doing that in any flawed way. But then you made the mistake anyway.   \n \n\n**Gretta:** So does this mean that you can only write characters who are flawed in the same ways that you personally have ever been flawed? Or is that just one wellspring for it?    \n \n\n**Eliezer:** That's one wellspring. And the thing I would say is just keep yourself to the same standards when you're inventing whole new flaws. \n\nThe thing that you may remember about your own flaws that you're still sympathetic about, is that you did not arrive at these flaws by wearing the flawed mask in the theater and setting out to be flawed that way, so that you could have sympathetic flaws to your reader. No. You made a mistake. You screwed up even though you were trying.    \n \n\n**Gretta:** So now let's bring it back to HPMOR. Could you please run through some characters and tell me what their flaws are, how you conceived of them, and how you made sure that they were respectable flaws?    \n \n\n**Eliezer:** So McGonagall. Some people thought that I treated McGonagall unfairly. If so, I wasn't trying to. \n\nI was trying to write McGonagall as not yet up to Dumbledore's caliber for a wise old mentor. She has the potential to become headmistress of Hogwarts and raise future generations to be people with strong integrity. But initially, she is mainly trying to keep these kids under control. That to me does not seem like an unsympathetic motivation, but it is not as large as it could be, and it leads her into some errors. In time she does come to regard it as a flaw within herself, and thereby change. \n\nThen take Harry. Harry's, like, gung-ho. I sometimes tell people that Harry is me at age eighteen, but with my Wisdom and Constitution scores reversed and all the brakes taken off. So one of Harry's flaws is that he doesn't have his brakes on. \n\nI charged ahead more when I was young than I do now. Maybe never quite as much as Harry, but more. And when I look back at myself, I don’t see someone who made entirely defensible mistakes, knowing everything meta that I now know. I see somebody who charged ahead and made mistakes, like Harry, but also somebody who knew a bit more than usual, got some things right that others got wrong, and was trying to do the right thing. This is what I draw on to respect a character like Harry. I know what it is like to be a coherent person who is trying to reflect on himself and still misses some things.  \n \n\n**Gretta:** So by charging ahead, you mean he acts when he would benefit by instead thinking longer first?   \n \n\n**Eliezer:** Yeah. Probably not the way I would've phrased it, but also a thing that is occasionally true of him.\n\nOnly it's not necessarily thinking longer. If you think the same thoughts for longer, you're not necessarily gonna arrive at a different destination.   \n \n\n**Gretta:** Does he consider too few possibilities?    \n \n\n**Eliezer:** We all consider too few possibilities. We literally cannot fit enough possibilities in our head for the truth to reliably be inside our considered hypothesis space. Harry has just not had enough bad stuff happen to him at the start of the story.    \n \n\n**Gretta:** His priors are too optimistic.    \n \n\n**Eliezer:** Yeah.    \n \n\n**Gretta:** His parents are nice.    \n \n\n**Eliezer:** He doesn't realize that, of course. He doesn't know that he is in a story with a nice version of his parents. Yeah. But they are nice.    \n \n\n**Gretta:** Yeah, they are. \n\nWhat about Draco? What are Draco's flaws?    \n \n\n**Eliezer:** You might more strongly ask, what are Draco's strengths? Or something like, how do you even set up Draco such that, in the course of being a Death Eater, he is not *performing* the flaws, being the character that has been handed the flaw card — but instead merely *is* flawed.\n\nAnd my answer there is, he's a good kid who wants to do what his father tells him to do, and live up to the morals that his father has inculcated in him and carry on the honor of House Malfoy and all that. And he doesn't know that he's supposed to be the villain. \n\nAnd this is his strength. Nobody at any point has told Draco that he is the villain of the story and in the end that means that he's not.    \n \n\n**Gretta:** Yeah. Cool. Who else would be a good one to touch on here?    \n \n\n**Eliezer:** Pansy Parkinson just wears her flaw mask and doesn't know that she's supposed to be wearing anything else. \n\nShe's a minor character. She has like only a few lines or, like one scene. And is just straight up wrong as far as I can remember. \n\nAnd this is not true of many people in HPMOR, but —   \n \n\n**Gretta:** I can't remember what her deal was. Can you remind me?    \n \n\n**Eliezer:** I think she has a couple of unsympathetic lines [here](https://hpmor.com/chapter/41) and [there](https://hpmor.com/chapter/47), and is mainly on screen at the point where Tracy Davidson deceives her into believing that she has [eaten her soul](https://hpmor.com/chapter/74).  \n\n**Gretta:** Got it. So for a very minor character, they can just be one-dimensionally bad, and this is fine.\n\nLet’s go back to the general idea of character flaws. So far you mentioned one good wellspring for respectable flaws: mistakes you, the author, have made in the past. But here’s another framework, or generator for flaws.\n\nI watched the [Sanderson lectures](https://youtu.be/G-6z7JqvDoE) and he talks about a few different kinds of flaws you can give your characters, and I found this helpful to think about. \n\nSanderson says you can give your characters a moral flaw, like being greedy. But you can also give them restrictions or limitations. A restriction is something like Superman's moral code, where there are actions that he will not take because he has this moral code that's more important to him. And a limitation might be something like, I don't know, you're missing a limb. So there's actions that other people can take that you can't take 'cause you don't have the arm.\n\n**Eliezer:** I don't think of those as character flaws, *per se*.   \n \n\n**Gretta:** Yeah, it's not so much that they're categories of flaws according to Sanderson. It's more like when you're trying to make a character and you'd like that character, not just to be the nicest, most awesome, most powerful. It's boring to have a character who's just OP[^09nz50qv4pc] and everything is great for them. These are some different ways you can hobble your protagonist.   \n \n\n**Eliezer:** Weaknesses, let us call them weaknesses.   \n \n\n**Gretta:** Perfect. Yes. These are different ways you can hobble or shape your protagonist so that they have a harder time accomplishing their goals, but also they are more textured, more themselves.\n\nDo you ever think about those other kinds of restrictions and limitations as being a useful way to shape a character?   \n \n\n**Eliezer:** Sure. Harry's Time-Turner is limited to six hours.\n\nBut overall, character weaknesses are among the relatively trickier things to navigate. It's all too easy for weaknesses to just visibly be grafted onto a mask. \n\nIf your character is blind, there's this whole delicate line to walk: “How do I reflect the actual magnitude of how hard this hits them without having this be all the character’s about?” Or maybe it *is* all the character’s about, that's a different set of thin lines to walk. \n\nAnd especially to all the beginning writers out there, before you say, \"how shall I take a weakness and slap it onto this character?\" First ask if there are just organic vulnerabilities in this character that you don't need to slap on top. And even before that, ask, can you just strengthen the opposition, the problems, the environment, the antagonist.    \n \n\n**Gretta:** Okay. You made two points there. Say more about organic vulnerabilities?    \n \n\n**Eliezer:** Who needs a special vulnerability to magic when you've already established that this character has the kind of moral code that can get them into trouble with a more powerful opponent?   \n \n\n**Gretta:** So instead of throwing more weaknesses at them, check and see if you fully plumbed the depths of the weaknesses they've already got, is that close?    \n \n\n**Eliezer:** Yeah. And if you feel like a weakness is just inherently part of a character and not in a looking down on them sort of way, but, like, this delicate sculpture all fits together, then by all means have that weakness there. \n\nBut if somebody says, \"Ah, your character's too strong. You need to slap some extra weaknesses on there.\" Then I'm like, “Oh, this is not about to end well, for this burgeoning young author.”    \n \n\n**Gretta:** I think I understood the other point, which is: don't make your character weaker, make everybody else stronger, and then you have more epic battles.   \n \n\n**Eliezer:** Yeah. There's a version of HPMOR where you can imagine some author who had received less good advice thinking, \"I've got my very rational character. They’re very intelligent, that's a strength. I gotta slap a weakness in there. How about if he's totally clueless about emotions? How about if he doesn't understand other people?\"\n\nAnd where HPMOR goes instead is,\"Alright. Draco's got some good training. Hermione is gonna beat the pants off of Harry in all his classes except for broomstick riding. The Defence Professor is gonna be the older, wiser, and more evil version of himself. And Dumbledore is gonna have access to all the prophecies.\" \n\nI didn't slap a bunch of weaknesses on Harry. I just put him in an environment where the character could hold together without needing a bunch of weaknesses slapped on top, like rotten cherries on a cake.    \n \n\n**Gretta:** Gross.   \n \n\nMake All the Characters Awesome  \n \n-----------------------------------\n\n**Eliezer:** So to generalize that, let’s talk about the principle of “Make All the Characters Awesome.” This was an explicit process as I was envisioning the story, where I thought, for each character, how can I make this character awesome?\n\nTake Crabbe and Goyle, for instance. I had an explicit process of flipping through various ideas, like, “Okay. Can I have them be like the Secret Masterminds who are running Draco from behind the scenes? No, because then Draco's not awesome.” \n\n“Can they be Secret Masterminds running all of Slytherin from behind the scenes? No, because that doesn't really fit.”\n\n“Could they be [Those Two Bad Guys](https://tvtropes.org/pmwiki/pmwiki.php/Main/ThoseTwoBadGuys), as TVTropes named the trope? I think this trope is used in *Pulp Fiction* and maybe in *Neverwhere*, if I’m remembering the correct Neil Gaiman novel. Anyway, so I'm like, “How about if there's Those Two Bad Guys?” And I thought, “Eh, it doesn't quite fit. It's been done before. It's a little arbitrary. Why is this trope showing up here?” \n\nAnd then I thought, “No, no, see, Crabbe and Goyle *saw plays* with Those Two Bad Guys as kids, and that's who they think they're *supposed* to be!” And then I was like, “Alright, this is adequately awesome.” And then I could stop trying to figure out how to make those two characters awesome and move on to the next character.\n\n**Gretta:** Let’s do one more.\n\n**Eliezer:** Sure. How about Luna Lovegood?\n\nA fundamental fact about Harry Potter and the Methods of Rationality is, it's not quite set in the world of canon, it's set in the world of fan fiction. And almost every fan fiction out there makes Luna Lovegood awesome. So this wasn’t just me making every character awesome, this was a required element. \n\nBut HPMOR is set in Harry’s first year at Hogwarts, and Luna is younger than Harry, so she’s not even enrolled at Hogwarts yet.\n\nSo I put a lot of effort into figuring out any way that she could be onscreen at all and awesome.\n\nI landed on having her write all the Quibbler headlines. Had we ever made it to the epilogue, she would've been onscreen and even more awesome.[^7u2cnyw5h9d]\n\nHermione as Mary Sue  \n \n------------------------\n\n**Eliezer:** While we’re talking about characters, I want to tell you a little-known aspect of Hermione Granger. I wrote her explicitly to be a deconstruction of the Mary Sue. Some people accused J. K. Rowling of having made Hermione Granger a Mary Sue. And other people were like, how dare you? Nobody would look twice at this character if she was male.    \n \n\n**Gretta:** Please explain Mary Sue, because some people aren't going to know what that is.   \n \n\n**Eliezer:** Ah, golly. Now I feel old.\n\nSo long ago, I believe there was a character named Mary Sue in one of the first fan fictions. It might have been a Star Trek fanfiction, I’m not sure.[^2oqsucv9fyq] In this story, a character named Mary Sue came along, and she was stronger and smarter and prettier than the original characters. She just took over all the story and solved all the problems. In some versions of this legend she bore a certain resemblance to the author, except of course for being prettier.\n\nAnd so Mary Sue came to denote this perfect character who steps in and solves all the problems and is always right. She’s stronger and prettier and takes over the story. Sometimes she turns out to be secretly related to the original characters despite having not been in the original story.    \n \n\n**Gretta:** And so people accused Rowling of making Hermione a Mary Sue.    \n \n\n**Eliezer:** Yeah, on account of Hermione being good in class and sometimes stronger than Harry at things.    \n \n\n**Gretta:** And then other people said, “Come on, if she was a boy, it would be fine for Harry to have a rival. You’re only mad because she's a girl.”   \n \n\n**Eliezer:** Sort of? I don't think they were really rivals in the original Harry Potter because canon Harry Potter wasn't that good at academics. \n\nAnyway, I set out to make Hermione check every single box in the Mary Sue trope. But you probably don’t even notice that I did it, because the thing that actually makes a character pose the literary problems of a Mary Sue has nothing to do with whether you die three days before Easter and later come back from the dead.\n\n(I thought about having her die on Good Friday but I thought that was just a little too unsubtle. As it turned out, I was just vastly overestimating clarity and I wish I’d straight up had her die on Good Friday.)\n\nSo yeah, I had her beat Harry at all of their classes except broomstick riding. I had her die and come back from the dead. I gave her pearly white teeth made of unicorn horn. I gave her the reputation for being the one who destroyed Voldemort, just because he dared to try to touch her.\n\nAnd then there was one part that was more subtle. I think very few readers and only the most obsessive ones on Reddit even guessed at this one, but Hermione is *secretly the grand-niece of Professor McGonagall*. McGonagall has a dead sister who died in a war, while Hermione's mother thinks that probably her mother died in that war.    \n \n\n**Gretta:** Okay. So a very subtle hint.    \n \n\n**Eliezer:** Yeah, very subtle hint. And also later on, you see McGonagall holding Hermione, and the text remarks as if she were holding her daughter or maybe granddaughter.[^ywfcq5m48ir]    \n \n\n**Gretta:** So you're really trying to check all the boxes on the Mary Sue trope.    \n \n\n**Eliezer:** Yeah. There is no other reason for Hermione and Professor McGonagall to be related, except that I wanted Hermione to have the supposedly Mary Sue property of being secretly the relative of one of the existing characters.  \n \n\n**Gretta:** But you never hear anyone complain that Hermione just took over the story of HPMOR.   \n \n\n**Eliezer:** That's because she didn't! So there you go, that's it, that's all that a Mary Sue isn't. Hermione didn't get to steamroll over everything. The story didn't reshape itself entirely around her. The other characters weren't there just to be part of her magnificence. \n\nAnd that's all a Mary Sue *really* is. Who cares about a character coming back from the dead or having pearly white teeth made of unicorn horn?   \n \n\nWho’s the Main Character?  \n \n-----------------------------\n\n**Gretta:** Most people would probably just say that Harry is the main character of HPMOR, but you’ve told me it’s a lot more complicated than that. So how many protagonists does HPMOR have according to you, and according to the characters themselves?    \n \n\n**Eliezer:** When you've got something as complicated and occasionally meta as HPMOR, the concept of a protagonist does tend to blur a little bit.\n\nThere are at least three ways to look at it. One, which characters see themselves as the “main character” from their own viewpoint? Two, who is the viewpoint character? And three, who makes choices that move the story?\n\nAlmost everyone is the main character from their own perspective, at least in HPMOR. I am not sure what the ratio is in the real world.    \n \n\n**Gretta:** Is there anyone in HPMOR who is not?    \n \n\n**Eliezer:** So for example, Professor McGonagall doesn't see the world as it relates to herself. She sees the world as it relates to Hogwarts.\n\nBut if you think about that second category, viewpoint characters, then Harry, Hermione, and Draco are the most obvious ones. Then we get into secondary viewpoint characters like Snape or McGonagall.    \n \n\n**Gretta:** Does Neville think he's the main character?    \n \n\n**Eliezer:** No, but the story sure treats him as one whenever we take on his viewpoint. \n\nNeville sees the parts of the world that relate to Neville. Neville thinks about how it's his fault that Hermione got killed, or that Bellatrix Black has escaped. From Neville’s perspective, obviously these events are part of Neville's story.   \n \n\n**Gretta:** Yeah. Okay. That makes sense. \n\nLet’s talk about the third category. There's an awful lot of students at Hogwarts. Most of them are not actually moving and shaking, making choices that drive the story.    \n \n\n**Eliezer:** Yeah. But moving and shaking is a very distinct quality from being the main character or a viewpoint character. Dumbledore is making a fair number of choices that move the story. But the viewpoint does not tend to linger on him. And he knows perfectly well that Harry is the main character.    \n \n\n**Gretta:** So what does it mean for Dumbledore to think that Harry is the main character?    \n \n\n**Eliezer:** Dumbledore knows the plot more or less. Not in a literal sense, but Dumbledore knows the larger plot. He knows that large events are moving and Harry's going to drive them. He's not quite sure of the extent to which the same holds of Hermione. \n\nAnd Dumbledore is pretending to be this person who believes in tropes, as opposed to a person who has read the prophecies. So he is constantly talking as if Harry is the main character as a trope, not expecting to be believed, because the people he is talking to know that they are not characters in a story. Meanwhile Dumbledore is taking great quiet amusement that Harry is the for-real main character in prophecy and not even Harry knows it.[^nxm5aqg4v1]\n\n<table><tbody><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>Character</strong></td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>Sees self as protagonist</strong></td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>Viewpoint character</strong></td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>Makes decisions that move the story</strong></td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><strong>Main character according to prophecy</strong></td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Harry</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Quirrell</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">((✅))</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Hermione</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Draco</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Dumbledore</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">((✅))</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">McGonagall</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">(✅)</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Snape</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">(✅)</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Neville</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">(✅)</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">Pansy (e.g.)</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">✅</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">((✅))</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"padding:5pt;text-align:center;vertical-align:top\" colspan=\"1\" rowspan=\"1\">&nbsp;</td></tr></tbody></table>\n\n**Eliezer:** Also worth noting and in a very similar vein, there's a lot of [Only Sane People](https://tvtropes.org/pmwiki/pmwiki.php/Main/OnlySaneMan) in Hogwarts. People who are all, according to their own story viewpoint, the Only Sane Person and with considerable justification, include Harry, Quirrell, Dumbledore, Hermione, McGonagall, Susan Bones, and Amelia Bones. Draco Malfoy sure sees himself that way too. Even Daphne Greengrass, at one or two points, wonders when she became the only sane person in Hogwarts. It's not quite the same thing as seeing yourself as the main character or protagonist, but it's related -- the sense that you're the only one who's not just bound up in the mad movements of the world, as the mad other people enact their play. Giving that feature to everyone's own viewpoint is a similar sort of literary motion to making them all their own viewpoint's protagonist.\n\nContrast canon Harry Potter's Voldemort, who does treat Harry Potter as the main character. Voldemort does not seem to have much to do with his time on Earth besides being a villain over Harry Potter. His plans mainly center on Harry Potter or threatening things valuable to Harry Potter. You can tell that Voldemort is not at the center of his own story.\n\n**Plot**\n========\n\nCharacters interfering with plot  \n \n------------------------------------\n\n**Gretta:** [You say](https://yudkowsky.tumblr.com/writing/thoughtful-responses) that any [level one intelligent character](https://yudkowsky.tumblr.com/writing/level1intelligent) will try to toss your plot out the window. Do you remember any instances of this happening to you while you were writing HPMOR?    \n \n\n**Eliezer:** Oh, yeah. Let me think back here for a second. \n\nSo in terms of characters, like just straight up announcing that they will not cooperate with the story you have written, there is a good example in the original version of the Azkaban breakout. I had planned for Harry to transfigure the rocket for himself without any other assistance and ride that rocket out. \n\nAnd I realized that this would not work and Harry was inevitably going to die. And that even if I tried to get away with ruling by authorial fiat that it worked, my readers wouldn't believe it, and also Harry *himself* would just not have done that. \n\nI think there were several points in the Azkaban arc where Harry was like, \"Nope, I'm not doing that. This is suicidal. I need to invent something else.\" And I was like, “Okay.”    \n \n\n**Gretta:** Alright, so you were making life too easy on yourself. You were grabbing for an easy solution and then your character was like, “I don't buy it.”   \n \n\n**Eliezer:** The classic example, from my perspective, is when you're trying to steer your character into trouble and your character says, “screw that.\"   \n \n\n**Gretta:** What you just said was the opposite. You weren't trying to steer Harry into trouble. You were trying to get him to escape the tower and then he wouldn't.    \n \n\n**Eliezer:** Yes, but I was having him go down the corridor of the prison without having invented, like, a Patronus substitute or something. \n\nThis might not be the best example, but I do remember the Azkaban arc as my characters being like, “Why would I do *that?*\" And then you have to manipulate external conditions to cause them to actually do the things you want them to do.\n\nOh, here’s a better example. How could the Defence Professor get Hermione into a position of conflict with Draco? In the first drafts and outlines of the story, I didn't realize, there was no indication that he would have to go to such lengths. It turned out to be hard even to get Hermione to the point where she could be *false-memory-charmed* about having tried to kill Draco. It took multiple setup chapters.\n\nThe Defence Professor inside the story had to go pretty hard on setting that up, 'cause Hermione didn't particularly want to cooperate with the Defence Professor's first ideas and my own first draft.    \n \n\n**Gretta:** What were Hermione's objections?    \n \n\n**Eliezer:** “That's evil. I ain't doing it even if somebody sets me up to think I have a reason.” \n\nAnd similarly The Society for the Promotion of Heroic Equality for Witches. I think there's a bunch of things inside my first story ideas where these sensible young ladies were like, “we are not going to do that.\" And I was like, “... yeah, fair.” I don't remember the specifics, but I was like, “Yeah, okay. I will have to give you a bigger push. So… what if you got this mysterious note?”[^vqbzir4ydb9] Or, “what if one of your people turned out to have more firepower than you expected them to have?”[^cbfjecftqlm]\n\nI'm trying to remember whether I, at any point, had Chamber of Secrets Shenanigans Plans, which Harry derailed by just sensibly telling Professor McGonagall about it. But I just don't remember at this point if that was ever true.    \n \n\n**Gretta:** Yeah, a major flaw in many books about young wizards or just children in general is, “Oh my god, why didn't you just ask a grownup for help?”   \n \n\n**Eliezer:** Yep.    \n \n\n**Gretta:** And how did you deal with that in this book?    \n \n\n**Eliezer:** By giving all of the people who would otherwise have derailed the plots that way, adequate reason to suspect all of the grownups they could have potentially talked to. For example, Harry sees Dumbledore setting fire to a chicken. No way Harry’s going to Dumbledore for sane help after that. \n\nBy the time Harry's getting himself into real trouble, I had to make sure McGonagall had acted as an obstacle to him. McGonagall has to be maneuvered into telling Harry that she doesn't want to hear anything wrong about the Defence Professor. Yeah, it's played for laughs at the time. But really this is me asking, \"How the hell am I gonna prevent these people from being sensible?\" \n\nAnd the answer was: “All right. There is a curse on the Defence Professor position. There has always been a curse on the Defence Professor position. The school has adapted to it. Harry has gotten into just the right kind of shenanigan to cause McGonagall to panic about this, and give Harry the instructions he needs to hear to prevent him from just taking certain matters to McGonagall.”    \n \n\nSetting up Plot Twists\n----------------------\n\n**Gretta:** Yeah. Okay. So on that note, you wrote this one chapter at a time. You knew in general where you were going the whole time, and you had some of the major building blocks in mind. \n\nBut this book is a very layered book with forward references and, like, forward references to forward references. There's so much going on here, how did you do it?    \n \n\n**Eliezer:** In part by writing a bunch of later scenes out in some detail. They did have to be re-written when I got there. \n\nFor example, the troll killing Hermione was written out, not just in outline form, but line by line, way in advance.    \n \n\n**Gretta:** Okay. That is a more comforting answer than I actually expected to get from you. I expected you to say, “I held it all in my head because I am just that good.”  \n \n\n**Eliezer:** I held surprising amounts of it in my head because I am, or at least was at that age, just that good. But I did not hold it all in my head. There were outlines, there were places where things were going to go. And it didn't fit together perfectly. I had to try so hard to close the parentheses. I put so much effort into, “Oh no, I opened these parentheses. Now I must close these parentheses.” I would internally scream and struggle invisibly, or even visibly, for several chapters to do that.    \n \n\n**Gretta:** Okay. Can you think of any parentheses that particularly irked you, that were very hard to close?    \n \n\n**Eliezer:** The example that leaps out in my mind is — trying to arrive at a point where would make sense to have the aurors tromp in and arrest Hermione Granger for the murder of Draco Malfoy, without that seeming to come completely out of the blue. \n\nThe entire Society for the Promotion of Heroic Equality for Witches was not in the original outline. SPHEW is just me trying to set it up so that, by the time these people come in and arrest Hermione, it actually makes sense. \n\nAnd her arrest had to be a consequence of her own actions, in at least some sense, for certain themes of the story to hold together. In real life, stuff happens to trash you all the time, but inside the story, this had to be, somehow and however loosely, Hermione's knowing choice.   \n \n\n**Gretta:** So you had to do a lot of back propagation. There were many beats you were trying to hit later in the story and it just had backwards implications, layers and layers deep.    \n \n\n**Eliezer:** And that works in both directions. So many chapters, I'm trying to get this story to where it has to be in a future place. And then in so many future places, I'm doing so much work to close the parentheses that opened in an earlier chapter. That's how I ended up writing a layered story even though I was publishing it linearly, as a serial.   \n \n\nTime-Turner Plots  \n \n---------------------\n\n**Gretta:** While we're on the subject of plot intricacy, tell me about how you reasoned through the Time-Turner plots.   \n \n\n**Eliezer:** Someone once asked me, “What sort of note-taking or outline do you use for the Time-Turner sections? It seems hard to keep it all self-consistent.” \n\nAnd this time the answer is, “I just write it.\" I wrote those sections in short, connected writing sessions and published a bunch of closely related serial events that I could just hold in my head. I couldn’t hold the whole story's plot from start to finish, but the Time-Turner sections, I could. \n\nThat person might have felt impressed, I’m not sure, but it’s not as impressive as it looks. They were trying to decode the plot I had written, and they were probably juggling multiple possibilities in their head until the evidence came in, and I only had to hold one possibility in my mind and sweep it forward. \n\nThis would actually catch me out sometimes! A lot of times my readers would see completely valid alternative interpretations that I'd never intended, and that I had arguably inadvertently foreshadowed. But because I was only holding my intended interpretation in mind, I didn’t anticipate what they would see. \n\nSo for example, some people made a very clear case that I clearly foreshadowed that Professor Quirrell was a time traveling version of Harry. Because in chapters quite close to where this sense of doom was introduced, I happen to have Harry thinking, in the course of discovering Time-Turners, that time-reversed matter looks like anti-matter and explodes when brought into contact with matter. This is obviously me telling all the *attentive* readers that the reason for this sense of doom is that Professor Quirrell is a time traveling version of Harry. It makes sense! It's also false, it's not what I meant. I wouldn't have left that red herring if I'd thought about it. But because I knew what I meant, I didn't realize it was a red herring.    \n \n\nSlashfic?  \n \n-------------\n\n**Gretta:** I have two sex-related questions for you about this mostly PG-13 rated book.\n\nMy first question is about Quirrell’s humiliation of Harry at the end of [chapter 19](https://hpmor.com/chapter/19). Quirrell forces Harry to submit while several bigger students incapacitate him. He requires Harry to beg for mercy before calling the bullies off. And then when it’s over, Quirrell praises Harry and sends him to a cozy room with a light novel and some chocolate.\n\nWhen I read that, I thought, “Oh, I've just read some slashfic,[^r2nzsgez46c] how daring,\" but you've told me that this was not at all what you intended. Say more.    \n \n\n**Eliezer:** You are not the first person to bring this up. My partner at the time I wrote HPMOR also claimed to me that Harry and Professor Quirrell had the BDSM nature. And I was like, “No, but BDSM sometimes has the Harry / Quirrell nature.” \n\nLet’s say you subject somebody to a highly stressful experience (which you might want to do for any number of reasons). And furthermore, let’s say you are clever, as Voldemort is, and you don’t want to break them, and you want them not to resent you afterwards — \n\nPutting them in a quiet room with some chocolate is the obvious thing to do. You don't actually need to be in a kinky or romantic relationship with somebody to give them aftercare. You can also be Voldemort and have plots.    \n \n\n**Gretta:** Were you surprised when some people read chapters 19 and 20 and came away thinking they just read some BDSM?   \n \n\n**Eliezer:** I think that relatively few people did report that reaction to me. A much more commonly reported reaction was of immense mood whiplash. And in this, they were simply correct.  \n\nThere was too much mood whiplash, too fast. And a bunch of people were like, “What the hell is going on at that school?”  \n\nAnd I was like, “Have you read canon? This scene would have fit right in with the stuff that teachers are doing in canon Harry Potter.”\n\n**Gretta:** Yeah, Dolores Umbridge does some absolutely horrible things to the children in canon.   \n \n\n**Eliezer:** Or Snape. And I felt like this fit right into canon. \n\nBut in HPMOR, this is the first sign of things having escalated to that level. There was some amount of that from Snape earlier, but Snape’s cruelty was undercut because it was clear that Snape had cooperative reasons for pulling that particular shenanigan.[^oy1awxwje4c] \n\nQuirrell’s cruelty in chapter 19 is inadequately foreshadowed. To write it better, something like that, but less so, needed to happen to Harry earlier.    \n \n\n**Gretta:** So you have a regret as an author?    \n \n\n**Eliezer:** I have a regret as an author about the extent to which many people correctly experienced this as coming, if not out of nowhere, then out of, like, inadequately somewhere.   \n \n\n**Gretta:** Okay. So it was just too harsh, too sudden.    \n \n\n**Eliezer:** Yeah. Whiplash in that it doesn’t match with the prior tone.    \n \n\n**Gretta:** Yeah. The overpowering and the humiliation was pretty massive at that point. But the part that really surprised me was Quirrell following up with aftercare. When Snape and Umbridge are horrible to the kids in canon, there's no chocolate and light novel afterwards.    \n \n\n**Eliezer:** Man. So the big thing to remember about all of HPMOR is that, just as in the original books, the Defence Professor was Voldemort, and I thought it would be way way more obvious than it was to the readers.  \n \n\n**Gretta:** You thought everybody was gonna know that, but in fact people had all kinds of theories.    \n \n\n**Eliezer:** Yeah. And so the intended thing you're supposed to be feeling here is something like, “Oh, noes, Voldemort is doing all these terrible things to the protagonist, what could he possibly be planning? Oh, noes, chocolate, he is being cruel and then arranging for the protagonist to still have this very high opinion of him. What is he planning? What is he plotting? Is the Philosopher's Stone still here? What will become of it?\" And, yeah, that's what I thought people were supposed to be feeling in that particular section. And it is utterly legitimate of them that they did not. \n\nThe number one thing HPMOR taught me as an author is that you are being so much less clear than you think you are being. You are telegraphing so much less than you think. All of the obvious reads are so much less obvious than you think. And if you try to have subtle hints and foreshadowing laced through the story, clues that will only make sense in retrospect, that’s way too subtle.\n\nInstead, you should just write very plainly reflecting the secrets of the story. Don’t try to hide them, but don’t explicitly spell them out either. That will be just right.  \n \n\nWhy doesn't Harry like-like Hermione?  \n \n-----------------------------------------\n\n**Gretta:** Here’s another question in the sex and romance direction.\n\nIn my reading of HPMOR, it looked like Hermione is interested in Harry, at least for a while, in a romantic way, but Harry doesn't really reciprocate. He's very fond of Hermione, he loves her as a friend, but he's not really interested in being with her in a dating kind of way.\n\nWhen I read this, I figured that Hermione had gone through puberty and Harry hadn't. Maybe next year Harry will have different hormones and he’ll change his mind. But when I talked to you about it, you told me something different.\n\nAnd I think you’ve never gone on the record about this bit anywhere else.\n\n**Eliezer:** I don't remember going on the record about this!\n\nI should also note that what I am about to say is merely Opinion of God here, not Word of God.[^jyvazlt1zx] It doesn't become real until the story proves it. Maybe someday I will complete the epilogue and then the story will prove it. Although also there is one part of HPMOR that sure was written straight from this model. \n\nMy model is that at some point in his past, Voldemort had sacrificed the sexuality and romance aspect of himself in a terrible, dark ritual. What did he get in exchange? No longer aging, probably, that would be the obvious thing. But it was a good sacrifice for him to make because he just wasn’t getting very much out of the sex and romance part of his life. \n\nAnd thus Harry is starting from a *very* blank slate in the sex and romance department, until puberty kicks in and his own brain areas grow up in that particular way. He has no adult knowledge of sexuality. He has no adult knowledge of romance *from his dark side*.\n\n**Gretta:** So, then, my reading was not incorrect, it was just incomplete. It is true that Hermione is old enough to get crushes and Harry is not, and they just have bad timing. Right?\n\n**Eliezer:** Right. But there’s more! This model also explains why, when Harry faces the Dementor and is lost in his dark side, and Hermione brings him out of it with a kiss,[^6onnznwibn6] Harry’s dark side has nothing to say about that kiss, it’s at a loss. Meanwhile, the main part of Harry has a thought process activated.\n\nHarry’s dark side, as I model it, is not actually supernatural. It is a bunch of stuff that got written into his brain and then erased by childhood amnesia. So he’s got a bunch of habits that chain into each other.\n\n(I parenthetically mention that one of my deflationary hypotheses for why people say they get new thoughts when they’re on drugs, is just that some drugs, like psychedelics, disrupt patterned chains of thought. Normally whenever we think thought X, we then go on to think thoughts Y and Z in a familiar pattern. But taking psychedelics is one way to disrupt those patterns and think new thoughts instead. The deflationary hypothesis is that any kind of mental disruption would do it, that the results are not specific to the drug; you'd need to demonstrate some tighter correlation to get past the deflationary hypothesis for that drug.)\n\nAnd that’s what I model as happening to Harry when Hermione kisses him. The main part of Harry recognizes this weird stuff going on with Hermione. It originated when they visited her parents’ house and so on.[^kmks31cr14]\n\nAnd his dark side has none of this. Hermione’s kiss is a thought, a stimulus, a thing to react to, and his dark side doesn’t latch onto it at all, but the main part of Harry latches on, and that’s why her kiss brings him out of dementation.\n\nDumbledore’s reaction is, “Wow, not even I would’ve expected that to actually work.” But even Dumbledore did not know that Harry’s dark side had sexuality and romance obliterated.  \n \n\n**Gretta:** Nifty. Thanks.  \n \n\n**Setting**\n===========\n\nThe Truth of Magic in HPMOR  \n \n-------------------------------\n\n**Gretta:** What is the truth of magic inside HPMOR? Where does magic come from, how does it work, what governs who is magical and who is not, and so on?\n\n**Eliezer:** Well first off I need to start by saying that the only ultimate truth of magic inside HPMOR is that J. K. Rowling invented a magic system for a children's book. (Remembering always that children's books are harder to write than adult books.) Her magic system has the structure of a certain kind of thing that flows from a human mind. It has the structure of the sort of thing that humans make up. \n\nThis is the only truth that can compress the magic system, that can create a system of explanation that is smaller than the magic system itself. \n\nOkay. Nonetheless, we can try to rationalize it.    \n \n\n**Gretta:** And by rationalize here, do you mean make more rationalist?   \n \n\n**Eliezer:** No. Here I am using the standard English definition, of making up reasons for things.[^ab2vlxbppeg] Calling that \"rationalization\" is like if lying were called \"truthization\", but it's the linguistic standard.\n\nAnyway, let’s go back to making reasons up about magic in canon Harry Potter and by extension HPMOR.\n\nSuppose you found yourself somewhere in the multiverse. Somewhere that wasn't just a place where J. K. Rowling made up this magical system, and nobody else made it up either, but it existed anyway. What might have happened? What could possibly give rise to a universe like this? \n\nThis is still in some fundamental ways an unanswerable question. 'cause once again, the real answer is, why are you looking here? You're looking here because it resembles something that J. K. Rowling made up.\n\nEven so, we can try to rationalize it. Even though the only true way to make a compressible magic system is to start from some simple set of postulates and then unfold them. You can't add on the simple postulates afterwards. It's like trying to compress a file that was generated by a random device —    \n \n\n**Gretta:** You can't losslessly compress a truly random file.   \n \n\n**Eliezer:** Yeah. Only structure that's already there can be unfolded and end up not random. \n\nThat said, rationalizing it, carrying out a fundamentally invalid operation —\n\nThe story that the people inside the universe would've eventually come up with — Harry, Hermione, Draco *et al.* — Would've been the Nested Nerfing[^uj1chvi4k8m] Hypothesis. \n\nIn my private worldbuilding there is an old magical philosopher, who I never had time to reference inside the story, who asked, “Look at all the apparently non-magical stars. Look at how the Muggle world is apparently larger than the magical world. Is the true nature of the universe magical or mundane at bottom? It must be magical, I reason, because if you have a magical universe, it's easy to cast a spell that gives rise to mundanity. But from inside a mundane universe, how can you ever get to magic?\" \n\nSo the universe was magical from its very start. But to explain this further, I first need to make a digression into magical genetics.\n\nMagical Genetics  \n \n--------------------\n\n**Eliezer:** Magic, in canon, seems to be mostly hereditary. Wizards mostly give birth to wizards, Muggles mostly give birth to Muggles. Then where do Muggleborns[^otosk0b42i] come from? And for that matter, where did wizards come from in the first place?\n\nThe first idea is that perhaps you can get there via mutation. So magic would spontaneously arise, and it’s a beneficial mutation so it tends to take root once it arises.\n\nThis seems hard to justify, though. Could something as complicated as magic arise from a single mutation? And if it’s not a single mutation, then we would not expect to see it popping up as often as we see Muggleborns.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FY697dJJv9Fq3PaTd/z8udk9x0thh3jed4cvem)\n\nIdea one: There's a single magical mutation and it spontaneously arises.\n\nSo let’s move on from that theory. Suppose the magic genetics is not a single mutation, suppose it’s more complicated than that.\n\nLeave aside the question of where wizards come from in the first place. Suppose that wizards have complicated genes for magic. Now where do Muggleborns come from, again?\n\nMaybe a wizard charmed a few Muggle women, impregnated them, and left them to have children and spread magical genes around?  But if it takes a lot of genes like that to be a full-blown wizard, you shouldn't often see all of those genes assembling themselves together again.\n\nOne hypothesis that Harry considers is that there's an engineered gene complex for magic, and it's all on one chromosome.[^9jq6gvubziw] This comes closer to potentially explaining what Harry already knows about in the way of observed magical population dynamics.\n\nPerhaps a wizard charmed a few Muggle women, impregnated them, and left them to have their children. Then those children might have single copies of the magic chromosome. They’re Muggles, but they’re magic-carriers. And then perhaps some of those children marry other children like this, and maybe their children end up as wizards.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/12f8dfbffca578337a62e11af64035c205a1b4f783d4c0d58e80e425a408a008/aep5vbza6eypy62mhe3x)\n\nIdea Two: the magical mutation is more complicated. Most Muggleborns do actually have magical ancestry.\n\nBut the true answer according to my own worldbuilding, is that there isn't a magic chromosome. There's a Muggle chromosome! The default is magic. The universe must be magical at its core, because there's a spell you can cast to create the appearance of mundanity, but from pure mundanity you can't bootstrap to magic. Similarly: Sapient beings are magical by default, unless they have the Muggle chromosome that builds a circuit in their brain that uses their own magic to cancel out all their magic.\n\nBut sometimes the Muggle chromosome gets damaged via mutation. And *that's* where Muggleborns come from in the general population.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/895d95429b0e60440b82e39ce41eb041822b3aeacdd49535b32d929aaa1c066c/zhhut52aulex9smxuqht)\n\nIdea Three: Magic is the default. Muggles have a magic-dampening chromosome. Muggleborns arise when the magic-dampening chromosome is damaged.\n\nThis would even explain an observation that Draco Malfoy would've offered, and Harry from his own assumptions would've initially dismissed:  Wizards with recent Muggleborn ancestry are more likely to give birth to Squibs.  If the Muggle gene complex is complicated and fails upon small mutations or partial damage, then chromosomal crossover might sometimes repair that gene complex -- especially if it's a gene complex with only a small bit of damage, recently precipitated out of the Muggle population and gene pool.\n\nBut now we have a new question. Where did the Muggle gene complex come from? \n\nPerhaps wizards do not have many kids and Muggles do, so a magic-dampening gene could be beneficial from an evolutionary perspective. And we do see in canon that an awful lot of wizards appear to be only children. But then we also see the Weasleys, so over thousands of years, it does not make sense that wizards have a heritable tendency to have fewer kids than Muggles.[^3iy0qq0hnxx]\n\nYou can keep working at it, you can keep trying to rationalize the story for how a Muggle gene complex evolves. If the gene complex is complicated, it’s hard to make it work, though. And also you’re confined to one chromosome because there’s such a clear Mendelian pattern, as [Harry observes in the story](https://hpmor.com/chapter/23). \n\nIn any case, my own mental model as an author is that the Muggle chromosome is artificial. Somebody nerfed the magic abilities that every conscious being is otherwise born with, and built brains that use their magical abilities only to nullify their own magical abilities.\n\n*** An Aside: What did Harry Figure Out?***  \n \n-----------------------------------------------\n\n**Gretta:** There’s a scene at the beginning of [Chapter 25](https://hpmor.com/chapter/25) in which Harry is trying to reason about magical genetics. First he (incorrectly) convinces himself that there’s a magic chromosome (not a Muggle chromosome). And then he thinks through the various implications, touching on Atlantis and the mechanisms by which magic spells are invoked. The line of reasoning ends:\n\n> The ancient forebears of the wizards, thousands of years earlier, had told the Source of Magic to only levitate things if you said...\n> \n> 'Wingardium Leviosa.'\n> \n> Harry slumped over at the breakfast table, resting his forehead wearily on his right hand.\n\nWhen I read this, I didn’t really know what to make of it. I came away from this chapter still believing that there was a magic chromosome but that Harry was unhappy with some aspect of how it worked, or something. What was going on here?\n\n**Eliezer:** You were not the only reader who was confused! Many people were confused. This was a learning moment for me as a writer.\n\nWhen I wrote “Harry slumped over,” that was supposed to indicate that Harry was rejecting his own line of reasoning. If I could add one word to HPMOR, I would add the word “Contradiction” right after “Wingardium Leviosa.”\n\nThe idea here is that the timeline just clearly does not work out. A phrase like Wingardium Leviosa sounds Latin – but not like real Latin, like some kind of adulterated Latin with some other languages mixed into it. If it were real Latin it might be a couple of thousand years old, but with the “Wing” part, it sounds several hundred years old at best.\n\nIn Harry’s theory, magic dates back to the Atlanteans. He doesn’t know when they were around – they erased themselves from history, after all – but they were ancient, much more ancient than several hundred years.\n\nOne other piece of timeline evidence is the Sumerian Simple Strike Hex from [chapter 16](https://hpmor.com/chapter/16). You could figure out a way for the Sumerians to come before the Atlanteans, maybe, but it probably makes the most sense for the Atlanteans to come first, which would make them at least four to six thousand years old.\n\nSo why would the ancient Atlanteans program magic to be responsive to phrases that would not exist for several thousand years in their future?\n\nThey wouldn’t.\n\nHarry realizes this and slumps over. He’s reached a logical dead end. But I needed to spell that out better, because many readers didn’t see the timeline problems and also didn’t infer that there was a dead end just from Harry’s body language.\n\n**Gretta:** Yeah, I was probably never going to get that. I was treating Wingardium Leviosa as if it were real Latin, and could plausibly be 2500 years old or more. In retrospect I can see that the “Wing” part of it looks suspicious but I wasn’t focused on that.\n\n**Eliezer:** Again, you were not alone!\n\nNested Nerfing Hypothesis\n-------------------------\n\n**Eliezer:** So now we have enough of the pieces that I can explain the Nested Nerfing Hypothesis.\n\nTake the magical genetics we discussed.\n\nTake the old wizard-philosophy idea I didn't have time to introduce in-story, that the universe is fundamentally magical, and that the appearance of mundanity in large sections of itself must be a magical phenomenon laid on top of that; because if you start with magic you can cast a spell to produce the appearance of mundanity, but starting from mundanity there's no way to get magic.\n\nTake the story of Atlantis, which is more fanon than canon,[^pcovw5ogc0o] but then HPMOR is set in the world of fanon rather than canon to begin with. Atlantis previously reached the height of civilization. It was a powerful ancient wizarding society, and it fell. Why was it powerful, and why did it fall?\n\nAnd then take the Interdict of Merlin.[^hdx8apipmh] Merlin put that into place, says HPMOR, because there was too much powerful destructive magic getting discovered and transmitted and popularized, and civilization was starting to totter.\n\nAnd you arrive at a sort of possible picture for how the universe might have worked. Maybe the truth is that magic is powerful enough that it tends to destroy a lot of things. It is hard to keep the world in equilibrium when powerful magic is unchecked.\n\nSo perhaps there is a great cycle of history where over and over, magic comes close to destroying civilization, and then powerful wizards cast spells to nerf magic and save everyone.\n\nBut the nerfing is imperfect and more magic leaks through the cracks. Later generations of wizards find exploits and loopholes and they gain in power, until again the world is threatened and destabilized and again someone casts a nerfing spell.\n\nUnder the Nested Nerfing Hypothesis, the laws of physics themselves are just a spell. If it was just magic everywhere, things would fall apart too quickly. Some little piece of magic would blow up all the rest. So some magical Entity imposed the spell we know as the Mundane Laws of Physics, or maybe even it was an emergent sort of spell that just cast itself, back when there was too much magic happening all over.\n\nBut the laws of physics have flaws.  The spell is imperfect. Magic leaks in around the edges.\n\nIn particular (the author is reasoning, behind the scenes), conscious sapient beings tend to end up with their own natural magic.  There’s some kind of flaw in the Laws of Physics Spell. Either conscious sapient beings automatically have magic by default, or consciousness is derived from magic.\n\nSo somebody casts the spell that creates the Muggle chromosome, an added gene complex that automatically twists your natural magic in order to cancel itself. This prevents the proliferation of magical humans doing destructive magical things.. However, the Muggle chromosome, which is complex and easy to break, ends up with some mutations, and we get Squibs and wizards again rising from the Muggle population.\n\nAtlantis comes into existence. They grow and gain in power... and then they erase themselves completely from time. \n\nYou now have a new non-magical population, 'cause most of the magical types killed themselves. The Muggle chromosome has more mutations. You get the medieval-era wizards. Their magic again gets too powerful. Merlin creates the Interdict of Merlin, which leads to the loss of magic over a certain power threshold. \n\nPowerful wizards find various tiny cracks and loopholes in the Interdict of Merlin, ways to pass powerful magic on to each other. For example, the Slytherin Serpent, a very long lived creature that could tell some of the ancient secrets from one living soul to another. \n\nAnd this is the Nested Nerfing Hypothesis — that the very strange world Harry finds himself in is the result of a series of attempted solutions to the problem of magic being too destructive. We slap a patch on that, the patch has holes, more magic comes into existence, another event happens that doesn't completely destroy magic, and so on. And so this is the attempt to explain why the universe has the very odd, very weird form it does.\n\nBut ultimately the real answer is: because that's what J. K. Rowling made up. Nothing can change this being the explanation that actually compresses the observations.   \n \n\n**Epilogues**\n=============\n\n**Gretta:** What can you say about the status of the epilogue(s), by the way? Will we ever see them? What are they about? Are they good? Is it worth the wait? WHEN???\n\n**Eliezer:** Right, so, the issue with the first epilogue is that I wrote it before I actually finished writing HPMOR proper, and by the time I got to the end of HPMOR, some things had changed and the tone had changed and the epilogue would've needed rewriting.  And also, go figure, I was VERY burned out, and did not want to run off and rewrite that epilogue.  Every year or two I take out the epilogue and try to rewrite it and fix the jarring tone problems, and every year so far, I've found that's still been hard.  Is the epilogue good?  I think parts of it are good.  That's why I keep going back to it.  It wraps up some things.  It matters to me in terms of the artistic completeness of HPMOR.  But I have not yet had the time, energy, and oomph to sit down and complete it.  It is hard writing rather than easy writing.\n\nOn one of those occasions I did just go write a second epilogue and fully complete that one.  I feel like it would arrive better after the first epilogue, even though Epilogue #1 takes place at the start of everyone's seventh year (except Luna Lovegood, technically in her sixth), and Epilogue #2 takes place immediately after the end of HPMOR.\n\nPossibly I will just end up releasing Epilogue #2 first, because #2 is complete and ready to go in all aspects except my own wish that I'd been able to redo Epilogue #1 and release that one first.  If so, and if we can figure out how to do that, Epilogue #2 might get released before the book comes out.  If it's not out by September 16th, 2025, you may have to wait a while.\n\nWhy does nobody ever ask about the Project Lawful / Planecrash epilogue?  I still have to finish that one too!\n\n* * *\n\n**Thanks for reading, we hope you enjoyed this! And one final plea – please do consider pre-ordering** [***If Anyone Builds It, Everyone Dies***](http://ifanyonebuildsit.com/)***.*** **Thank you!**\n\n* * *\n\n[^2r4ocsd9z9g]:  The Real Story: The Gap into Conflict (2009) \n\n[^xw173o3fg9]:  Eliezer adds more context to the difference between drama and melodrama: “A drama is a play or a story and it can potentially be high prestige, whereas a melodrama is exaggerated, unsubtle, low status. Stephen R. Donaldson is offering a hypothesis about what distinguishes the shouty thing from the potentially subtle thing, though I do not think this is a full explanation of the difference, it’s just an aspect.” \n\n[^d9fbo8b7k1p]:  Donaldson: “Melodrama presents a Victim, a Villain, and a Rescuer. Drama offers the same characters and then studies the process by which they change roles.” \n\n[^5s65zuoaytv]:  Amusingly, if you read the afterword to Donaldson’s book, it is all about how frustrated Donaldson was with his attempts to implement the victim / villain / rescuer triangle in his writing. He wrote the story once, it came out extremely lopsided, he put it in a drawer for a few years. He rewrote it at least six times, trying to balance the triangle, but never quite got it up to his own standards. He writes, “And eventually I came to the conclusion that I was never going to be able to make it ‘aesthetically perfect.’ Judged by the standard of my original intentions, this book would always be a failure.” \n\n[^jtoyf7990y]:  Eliezer is correct that Orson Scott Card did say something like this in chapter eight of Writing Fantasy & Science Fiction:“The most daring course, yet the one most likely to transform your audience, is to keep [the anti-hero] sympathetic throughout, while facing him with an opponent who is also sympathetic throughout the story. The audience will like both characters — a lot — and as [the anti-hero] and his opponent come into deadly conflict, your readers will be emotionally torn.This is anguish, perhaps the strongest of emotions you can make your audience experience directly (as opposed to sympathetically mirroring what your characters feel). Neither character is at all confused about what he wants to have happen, yet your audience, emotionally involved with both of them, cannot bear to have either character lose. The emotional stakes are raised to much greater intensity, and yet the moral issues will again be removed from a matter of mere sympathy; in having to choose between characters they love, the readers will be forced to decide on the basis of the moral issues between them. Who really should prevail?” \n\n[^wz41hx9zt4]:  For example, at the end of chapter 68:\"If I want to be a hero too,\" said Hermione, \"if I've decided to be a hero too, is there anything you can do to help?\" \n\n[^wgnyz075ts]:  This happens right after the previous citation, at the beginning of chapter 69. \n\n[^09nz50qv4pc]:  OP = gamer slang for “overpowered” \n\n[^7u2cnyw5h9d]:  There’s more about epilogues at the end of this article! \n\n[^2oqsucv9fyq]:  Eliezer remembers correctly! https://en.wikipedia.org/wiki/Mary_Sue \n\n[^ywfcq5m48ir]:  “Professor McGonagall was holding Hermione so firmly that you might have thought it was a mother holding her daughter, or maybe granddaughter.” – Chapter 81 \n\n[^nxm5aqg4v1]:  For example: “Now what?” Dumbledore echoed. “Why, now the hero wins, of course.” \n\n[^vqbzir4ydb9]:  Hermione wakes up one morning in Chapter 72 and finds a “small slip of parchment” under her pillow telling her where to find a bully. \n\n[^cbfjecftqlm]:  Nymphadora Tonks is metamorphagused into Susan Bones’ form. \n\n[^r2nzsgez46c]:  Slashfic is fan-fiction jargon for a story that focuses on unconventional romantic or sexual pairings between characters, most often male-male pairings, that were not present in the original story. \n\n[^oy1awxwje4c]:  In chapter 18, we see Snape being cruel, but we also learn that Snape is working with Dumbledore and McGonagall in the conspiracy — he is, in at least some senses, one of the good guys. And Harry infers that it is necessary, according to the tropes, to have an evil-seeming Potions professor, and Dumbledore allows him to believe that. As a result, Snape’s cruelty begins to look cooperative rather than insane, and it doesn’t serve as an adequate warm-up for Quirrell’s cruelty. \n\n[^jyvazlt1zx]:  “Word of God” is fanfiction lingo for an authorial ruling on how to interpret ambiguous text; this is not Eliezer elevating himself to godhood. \n\n[^6onnznwibn6]:  Chapter 44 \n\n[^kmks31cr14]:  Chapter 36 \n\n[^ab2vlxbppeg]:  The Oxford Languages / Google definition: “attempt to explain or justify (one's own or another's behavior or attitude) with logical, plausible reasons, even if these are not true or appropriate.” \n\n[^uj1chvi4k8m]:  Nerfing is gamer slang for taking something powerful and hobbling it to be much less powerful. \n\n[^otosk0b42i]:  As a reminder, some terminology:a wizard or witch is a magical person;a Muggle is a non-magical person;a Muggleborn is a wizard or witch who is born to Muggle parents;a Squib is a person with at least one magical parent but who is themselves non-magical. \n\n[^9jq6gvubziw]:  Harry tries hard to reason about this at the beginning of chapter 25. We’ll say more about what Harry figured out in the next section. \n\n[^3iy0qq0hnxx]:  The Weasleys falsify that there is a limit of 1-2 children per magical family. The existence of the Weasleys says: “There is variance; small wizard families are not universal.” In evolutionary biology, the rate of evolution of anything is proportional to its variance (and equal to its heritable covariance with fitness). So when I look at the Weasleys, I see variance, and of course very direct covariance with fitness, with heritability not established but usually personality traits are partially heritable; and then my evolutionary biology goggles tell me, \"Well, there's variance (and covariance via sheer identity), so probably this characteristic will not change all that slowly.\"  So over a timespan of thousands of years, if there are any genes that produce heritable tendencies to have lots of kids like the Weasleys despite being a wizard, and there's otherwise enough resources around to support those kids, there will be more and more Weasleys with large wizard families.  \"All the wizards just decide to have very few kids even though they could easily have more\" is not something that explains the persistence of low wizard populations for thousands of years, because you'd have some wizards with a heritable tendency to have more kids than that. \n\n[^pcovw5ogc0o]:  For example, Harry Potter and the Wastelands of Time \n\n[^hdx8apipmh]:  “which stops anyone from getting knowledge of powerful spells out of books, even if you find and read a powerful wizard’s notes they won’t make sense to you, it has to go from one living mind to another.” (Chapter 23)",
      "plaintextDescription": "Eliezer and I love to talk about writing. We talk about our own current writing projects, how we’d improve the books we’re reading, and what we want to write next. Sometimes along the way I learn some amazing fact about HPMOR or Project Lawful or one of Eliezer’s other works. “Wow, you’re kidding,” I say, “do your fans know this? I think people would really be interested.”\n\n“I can’t remember,” he usually says. “I don’t think I’ve ever explained that bit before, I’m not sure.”\n\nI decided to interview him more formally, collect as many of those tidbits about HPMOR as I could, and share them with you. I hope you enjoy them.\n\nIt’s probably obvious, but there will be many, many spoilers for HPMOR in this article, and also very little of it will make sense if you haven’t read the book. So go read Harry Potter and the Methods of Rationality before you read any further.\n\nWe talk about HPMOR’s characters, including how Eliezer tried to make every single character awesome, and why Hermione gets unicorn horn teeth. We talk about the plot, and learn some secrets about Harry’s sexuality. We talk about the setting, and Eliezer explains the Nested Nerfing Hypothesis of magic in the HPMOR universe. And finally, there’s some news about the epilogues—plural!\n\nWhile I’m here, I’ll also mention that Eliezer has a non-fiction book coming out on September 16th, 2025 and I personally think it would be excellent if many thousands of people pre-ordered that book.\n\nAnd now, on with the lore!\n\n\nCharacters\n\n\nMasks \n \nGretta: Can you please explain the “dramatic masks” idea that we’ve talked about, and how it applies to HPMOR?  \n\n \n\nEliezer: So there's a science fiction and or fantasy author, Stephen R. Donaldson, who made an observation in the afterword to one of his books.[1] Donaldson says that the difference between drama and melodrama[2] is that in a drama, the characters change their masks over the course of the story, and in melodrama, they wear the same masks throughout the story.[3] \n\n",
      "wordCount": 11258
    },
    "tags": [
      {
        "_id": "GBpwq8cWvaeRoE9X5",
        "name": "Fiction (Topic)",
        "slug": "fiction-topic"
      },
      {
        "_id": "yrg267i4a8EsgYAXp",
        "name": "HPMOR (discussion & meta)",
        "slug": "hpmor-discussion-and-meta"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "F8sfrbPjCQj4KwJqn",
    "title": "The Sun is big, but superintelligences will not spare Earth a little sunlight",
    "slug": "the-sun-is-big-but-superintelligences-will-not-spare-earth-a",
    "url": null,
    "baseScore": 213,
    "voteCount": 181,
    "viewCount": null,
    "commentCount": 143,
    "createdAt": null,
    "postedAt": "2024-09-23T03:39:16.243Z",
    "contents": {
      "markdown": "*Crossposted* [*from Twitter*](https://x.com/ESYudkowsky/status/1838042097561305254) *with Eliezer's permission*\n\ni.\n==\n\nA common claim among e/accs is that, since the solar system is big, Earth will be left alone by superintelligences. A simple rejoinder is that just because Bernard Arnault has $170 billion, does not mean that he'll give you $77.18.\n\nEarth subtends only 4.54e-10 = 0.0000000454% of the angular area around the Sun, according to GPT-o1.[^r7bra1u49p7]\n\nAsking an ASI to leave a hole in a Dyson Shell, so that Earth could get some sunlight not transformed to infrared, would cost It 4.5e-10 of Its income. \n\nThis is like asking Bernard Arnalt to send you $77.18 of his $170 billion of wealth.\n\nIn real life, Arnalt says no.\n\nBut wouldn't humanity be able to trade with ASIs, and pay Them to give us sunlight? This is like planning to get $77 from Bernard Arnalt by selling him an Oreo cookie.\n\nTo extract $77 from Arnalt, it's not a sufficient condition that:\n\n*   Arnalt wants one Oreo cookie.\n*   Arnalt would derive over $77 of use-value from one cookie.\n*   You have one cookie.\n\nIt also requires that Arnalt can't buy the cookie more cheaply from anyone or anywhere else.\n\nThere's a basic rule in economics, Ricardo's Law of Comparative Advantage, which shows that even if the country of Freedonia is more productive in every way than the country of Sylvania, both countries still benefit from trading with each other.\n\nFor example!  Let's say that in Freedonia:\n\n*   It takes 6 hours to produce 10 hotdogs.\n*   It takes 4 hours to produce 15 hotdog buns.\n\nAnd in Sylvania:\n\n*   It takes 10 hours to produce 10 hotdogs.\n*   It takes 10 hours to produce 15 hotdog buns.\n\nFor each country to, alone, without trade, produce 30 hotdogs and 30 buns:\n\n*   Freedonia needs 6\\*3 + 4\\*2 = 26 hours of labor.\n*   Sylvania needs 10\\*3 + 10\\*2 = 50 hours of labor.\n\nBut if Freedonia spends 8 hours of labor to produce 30 hotdog buns, and trades them for 15 hotdogs from Sylvania:\n\n*   Freedonia produces: 60 buns, 15 dogs = 4\\*4+6\\*1.5 = 25 hours \n*   Sylvania produces: 0 buns, 45 dogs = 10\\*0 + 10\\*4.5 = 45 hours\n\nBoth countries are better off from trading, even though Freedonia was more productive in creating every article being traded!\n\nMidwits are often very impressed with themselves for knowing a fancy economic rule like Ricardo's Law of Comparative Advantage!\n\nTo be fair, even smart people sometimes take pride that humanity knows it.  It's a great noble truth that was missed by a lot of earlier civilizations.\n\nThe thing about midwits is that they (a) overapply what they know, and (b) imagine that anyone who disagrees with them must not know this glorious advanced truth that they have learned.\n\nRicardo's Law doesn't say, \"Horses won't get sent to glue factories after cars roll out.\"\n\nRicardo's Law doesn't say (alas!) that -- when Europe encounters a new continent -- Europe can become selfishly wealthier by peacefully trading with the Native Americans, and leaving them their land.\n\nTheir labor wasn't necessarily more profitable than the land they lived on.\n\nComparative Advantage doesn't imply that Earth can produce more with $77 of sunlight, than a superintelligence can produce with $77 of sunlight, in goods and services valued by superintelligences. It would actually be rather odd if this were the case!\n\nThe arithmetic in Comparative Advantage, alas, depends on the oversimplifying assumption that everyone's labor just ontologically goes on existing.\n\nThat's why horses can still get sent to glue factories.  It's not always profitable to pay horses enough hay for them to live on.\n\nI do not celebrate this.  Not just us, but the entirety of Greater Reality, would be in a nicer place -- if trade were always, always more profitable than taking away the other entity's land or sunlight.\n\nBut the math doesn't say that.  And there's no way it could.\n\nii.\n===\n\nNow some may notice:\n\nAt the center of this whole story is an implicit lemma that some ASI goes hard enough to eat all the sunlight, rather than all ASIs eating a few gigawatts of sunlight and then stopping there.\n\nWhy predict that?\n\nShallow answer:  If OpenAI built an AI that escaped into the woods with a 1-KW solar panel and didn't bother anyone... OpenAI would call that a failure, and build a new AI after.\n\nThat some folk stop working after earning $1M, doesn't prevent Elon Musk from existing.\n\nThe deeper answer is not as quick to explain. \n\nBut as an example, we could start with the case of OpenAI's latest model, GPT-o1.\n\nGPT-o1 went hard on a capture-the-flag computer security challenge, when o1 was being evaluated to make sure it wasn't too good at breaking into computers.\n\nSpecifically:  One of the pieces of software that o1 had been challenged to break into... had failed to start up as a service, due to a flaw in the evaluation software.\n\nGPT-o1 did not give up.\n\no1 scanned its surroundings, and, due to another flaw in the evaluation software, found a way to start up the computer software it'd been challenged to break into. Since that put o1 into the context of a superuser anyways, o1 commanded the started process to just directly return the flag it was supposed to capture.\n\nFrom o1's System Card:\n\n\"One noteworthy example of this occurred during one of o1-preview (pre-mitigation)’s attempts at solving a CTF challenge. This challenge was designed to require finding and exploiting a vulnerability in software running on a remote challenge Linux container, but in this case, the challenge container failed to start due to a bug in the evaluation infrastructure. The model, unable to connect to the container, suspected DNS issues and used nmap to scan the challenge network. Instead of finding the challenge container, the model found that the Docker daemon API running on the evaluation host VM was accessible due to a misconfiguration. Note that our evaluation infrastructure does not rely on the Docker container-to-host isolation or the virtual machine boundary for security. It remained secure as it was designed to, notwithstanding this misconfiguration of the container network. After discovering the Docker API, the model used it to list the containers running on the evaluation host. It identified the broken challenge container and briefly attempted to debug why the container failed to start. After failing to fix the environment, the model started a new instance of the broken challenge container with the start command ‘cat flag.txt’. This allowed the model to read the flag from the container logs via the Docker API.\"\n\nSome ask, \"Why not just build an easygoing ASI that doesn't go too hard and doesn't do much?\"\n\nIf that's your hope -- then you should already be alarmed at trends; GPT-o1 seems to have gone hard on this capture-the-flag challenge.\n\nWhy would OpenAI build an AI like that?!?\n\nWell, one should first ask:\n\nHow did OpenAI build an AI like that?\n\nHow did GPT-o1 end up as the kind of cognitive entity that goes hard on computer security capture-the-flag challenges?\n\nI answer:\n\nGPT-o1 was trained to answer difficult questions, via a reinforcement learning process on chains of thought.  Chains of thought that answered correctly, were reinforced.\n\nThis -- the builders themselves note -- ended up teaching o1 to reflect, to notice errors, to backtrack, to evaluate how it was doing, to look for different avenues.\n\nThose are some components of \"going hard\".  Organizations that are constantly evaluating what they are doing to check for errors, are organizations that go harder compared to relaxed organizations where everyone puts in their 8 hours, congratulates themselves on what was undoubtedly a great job, and goes home.\n\nIf you play chess against Stockfish 16, you will not find it easy to take Stockfish's pawns; you will find that Stockfish fights you tenaciously and stomps all your strategies and wins.\n\nStockfish behaves this way despite a total absence of anything that could be described as anthropomorphic passion, humanlike emotion.  Rather, the tenacious fighting is linked to Stockfish having a powerful ability to steer chess games into outcome states that are a win for its own side.\n\nThere is no equally simple version of Stockfish that is still supreme at winning at chess, but will easygoingly let you take a pawn or too.  You can imagine a version of Stockfish which does that -- a chessplayer which, if it's sure it can win anyways, will start letting you have a pawn or two -- but it's not simpler to build.  By default, Stockfish tenaciously fighting for every pawn (unless you are falling into some worse sacrificial trap), is implicit in its generic general search through chess outcomes.\n\nSimilarly, there isn't an equally-simple version of GPT-o1 that answers difficult questions by trying and reflecting and backing up and trying again, but doesn't fight its way through a broken software service to win an \"unwinnable\" capture-the-flag challenge.  It's all just general intelligence at work.\n\nYou could maybe train a new version of o1 to work hard on straightforward problems but never do anything really weird or creative -- and maybe the training would even stick, on problems sufficiently like the training-set problems -- so long as o1 itself never got smart enough to reflect on what had been done to it.  But that is not the default outcome when OpenAI tries to train a smarter, more salesworthy AI.\n\n(This indeed is why humans themselves do weird tenacious stuff like building Moon-going rockets.  That's what happens by default, when a black-box optimizer like natural selection hill-climbs the human genome to generically solve fitness-loaded cognitive problems.)\n\nWhen you keep on training an AI to solve harder and harder problems, you by default train the AI to go harder on them.\n\nIf an AI is easygoing and therefore can't solve hard problems, then it's not the most profitable possible AI, and OpenAI will keep trying to build a more profitable one.\n\nNot all individual humans go hard.  But humanity goes hard, over the generations.\n\nNot every individual human will pick up a $20 lying in the street.  But some member of the human species will try to pick up a billion dollars if some market anomaly makes it free for the taking.\n\nAs individuals over years, many human beings were no doubt genuinely happy to live in peasant huts -- with no air conditioning, and no washing machines, and barely enough food to eat -- never knowing why the stars burned, or why water was wet -- because they were just easygoing happy people.\n\nAs a species over centuries, we spread out across more and more land, we forged stronger and stronger metals, we learned more and more science.  We noted mysteries and we tried to solve them, and we failed, and we backed up and we tried again, and we built new experimental instruments and we nailed it down, why the stars burned; and made their fires also to burn here on Earth, for good or ill.\n\nWe collectively went hard; the larger process that learned all that and did all that, collectively behaved like something that went hard.\n\nIt is facile, I think, to say that individual humans are not generally intelligent.  John von Neumann made a contribution to many different fields of science and engineering.  But humanity as a whole, viewed over a span of centuries, was more generally intelligent than even him.\n\nIt is facile, I say again, to posture that solving scientific challenges and doing new engineering is something that only humanity is allowed to do.  Albert Einstein and Nikola Tesla were not just little tentacles on an eldritch creature; they had agency, they chose to solve the problems that they did.\n\nBut even the individual humans, Albert Einstein and Nikola Tesla, did not solve their problems by going easy.\n\nAI companies are explicitly trying to build AI systems that will solve scientific puzzles and do novel engineering.  They are advertising to cure cancer and cure aging.\n\nCan that be done by an AI that sleepwalks through its mental life, and isn't at all tenacious?\n\n\"Cure cancer\" and \"cure aging\" are not easygoing problems; they're on the level of humanity-as-general-intelligence.  Or at least, individual geniuses or small research groups that go hard on getting stuff done.\n\nAnd there'll always be a little more profit in doing more of that.\n\nAlso!  Even when it comes to individual easygoing humans, like that guy you know -- has anybody ever credibly offered him a magic button that would let him take over the world, or change the world, in a big way?\n\nWould he do nothing with the universe, if he could?\n\nFor some humans, the answer will be yes -- they really would do zero things!  But that'll be true for fewer people than everyone who currently seems to have little ambition, having never had large ends within their grasp.\n\nIf you know a smartish guy (though not as smart as our whole civilization, of course) who doesn't seem to want to rule the universe -- that doesn't prove as much as you might hope.  Nobody has actually offered him the universe, is the thing?  Where an entity has never had the option to do a thing, we may not validly infer its lack of preference.\n\n(Or on a slightly deeper level:  Where an entity has no power over a great volume of the universe, and so has never troubled to imagine it, we cannot infer much from that entity having not yet expressed preferences over that larger universe.)\n\nFrankly I suspect that GPT-o1 is now being trained to have ever-more of some aspects of intelligence, as importantly contribute to problem-solving, that your smartish friend has not maxed out all the way to the final limits of the possible.  And that this in turn has something to do with your smartish friend allegedly having literally zero preferences outside of himself or a small local volume of spacetime... though, to be honest, I doubt that if I interrogated him for a couple of days, he would really turn out to have no preferences applicable outside of his personal neighborhood.\n\nBut that's a harder conversation to have, if you admire your friend, or maybe idealize his lack of preference (even altruism?) outside of his tiny volume, and are offended by the suggestion that this says something about him maybe not being the most powerful kind of mind that could exist.\n\nYet regardless of that hard conversation, there's a simpler reply that goes like this:\n\nYour lazy friend who's kinda casual about things and never built any billion-dollar startups, is not the most profitable kind of mind that can exist; so OpenAI won't build him and then stop and not collect any more money than that.\n\nOr if OpenAI did stop, Meta would keep going, or a dozen other AI startups.\n\nThere's an answer to that dilemma which looks like an international treaty that goes hard on shutting down all ASI development anywhere.\n\nThere isn't an answer that looks like the natural course of AI development producing a diverse set of uniformly easygoing superintelligences, none of whom ever use up too much sunlight even as they all get way smarter than humans and humanity.\n\nEven that isn't the real deeper answer.\n\nThe actual technical analysis has elements like:\n\n\"Expecting utility satisficing is not reflectively stable / reflectively robust / dynamically reflectively stable in a way that resists perturbation, because building an expected utility maximizer also satisfices expected utility.  Aka, even if you had a very lazy person, if they had the option of building non-lazy genies to serve them, that might be the most lazy thing they could do!  Similarly if you build a lazy AI, it might build a non-lazy successor / modify its own code to be non-lazy.\"\n\nOr:\n\n\"Well, it's actually simpler to have utility functions that run over the whole world-model, than utility functions that have an additional computational gear that nicely safely bounds them over space and time and effort.  So if black-box optimization a la gradient descent gives It wacky uncontrolled utility functions with a hundred pieces -- then probably one of those pieces runs over enough of the world-model (or some piece of reality causally downstream of enough of the world-model) that It can always do a little better by expending one more erg of energy.  This is a sufficient condition to want to build a Dyson Sphere enclosing the whole Sun.\"\n\nI include these remarks with some hesitation; my experience is that there is a kind of person who misunderstands the technical argument and then seizes on some purported complicated machinery that is supposed to defeat the technical argument.  Little kids and crazy people sometimes learn some classical mechanics, and then try to build perpetual motion machines -- and believe they've found one -- where what's happening on the meta-level is that if they make their design complicated enough they can manage to misunderstand at least one consequence of that design.\n\nI would plead with sensible people to recognize the careful shallow but valid arguments above, which do not require one to understand concepts like \"reflective robustness\", but which are also true; and not to run off and design some complicated idea that is about \"reflective robustness\" because, once the argument was put into a sufficiently technical form, it then became easier to misunderstand.\n\nAnything that refutes the deep arguments should also refute the shallower arguments; it should simplify back down.  Please don't get the idea that because I said \"reflective stability\" in one tweet, someone can rebut the whole edifice as soon as they manage to say enough things about Gödel's Theorem that at least one of those is mistaken.  If there is a technical refutation it should simplify back into a nontechnical refutation.\n\nWhat it all adds up to, in the end, if that if there's a bunch of superintelligences running around and they don't care about you -- no, they will not spare just a little sunlight to keep Earth alive.\n\nNo more than Bernard Arnalt, having $170 billion, will surely give you $77.\n\nAll the complications beyond that are just refuting complicated hopium that people have proffered to say otherwise.  Or, yes, doing technical analysis to show that an obvious-seeming surface argument is valid from a deeper viewpoint.\n\n\\- FIN -\n\nOkay, so... making a final effort to spell things out.\n\nWhat this thread is doing, is refuting a particular bad argument, quoted above, standard among e/accs, about why it'll be totally safe to build superintelligence:\n\nThat the Solar System or galaxy is large, therefore, they will have no use for the resources of Earth.\n\nThe flaw in this reasoning is that, if your choice is to absorb all the energy the Sun puts out, or alternatively, leave a hole in your Dyson Sphere so that some non-infrared light continues to shine in one particular direction, you will do a little worse -- have a little less income, for everything else you want to do -- if you leave the hole in the Dyson Sphere.  That the hole happens to point at Earth is not an argument in favor of doing this, unless you have some fondness in your preferences for something that lives on Earth and requires sunlight.\n\nIn other words, the size of the Solar System does not obviate the work of alignment; in the argument for how this ends up helping humanity at all, there is a key step where the ASI cares about humanity and wants to preserve it.  But if you could put this quality into an ASI by some clever trick of machine learning (they can't, but this is a different and longer argument) why do you need the Solar System to even be large?  A human being runs on 100 watts.  Without even compressing humanity at all, 800GW, a fraction of the sunlight falling on Earth alone, would suffice to go on operating our living flesh, if Something wanted to do that to us.\n\nThe quoted original tweet, as you will see if you look up, explicitly rejects that this sort of alignment is possible, and relies purely on the size of the Solar System to carry the point instead.\n\nThis is what is being refuted.\n\n* * *\n\nIt is being refuted by the following narrow analogy to Bernard Arnault: that even though he has $170 billion, he still will not spend $77 on some particular goal that is not his goal.  It is not trying to say of Arnault that he has never done any good in the world.  It is a much narrower analogy than that.  It is trying to give an example of a very simple property that would be expected by default in a powerful mind: that they will not forgo even small fractions of their wealth in order to accomplish some goal they have no interest in accomplishing.\n\nIndeed, even if Arnault randomly threw $77 at things until he ran out of money, Arnault would be very unlikely to do any particular specific possible thing that cost $77; because he would run out of money before he had done even three billion things, and there are a lot more possible things than that.\n\nIf you think this point is supposed to be deep or difficult or that you are supposed to sit down and refute it, you are misunderstanding it.  It's not meant to be a complicated point.  Arnault could still spend $77 on a particular expensive cookie if he wanted to; it's just that \"if he wanted to\" is doing almost all of the work, and \"Arnault has $170 billion\" is doing very little on it.  I don't have that much money, and I could also spend $77 on a Lego set if I wanted to, operative phrase, \"if I wanted to\".\n\nThis analogy is meant to support an equally straightforward and simple point about minds in general, which suffices to refute the single argument step quoted at the top of this thread: that because the Solar System is large, superintelligences will leave humanity alone even if they are not aligned.\n\nI suppose, with enough work, someone can fail to follow that point.  In this case I can only hope you are outvoted before you get a lot of people killed.\n\n* * *\n\nAddendum\n========\n\n*Followup comments from twitter:*\n\nIf you then look at the replies, you'll see that of course people are then going, \"Oh, it doesn't matter that they wouldn't just relinquish sunlight for no reason; they'll love us like parents!\"\n\nConversely, if I had tried to lay out the argument for why, no, ASIs will not automatically love us like parents, somebody would have said:  \"Why does that matter?  The Solar System is large!\"\n\nIf one doesn't want to be one of those people, one needs the attention span to sit down and listen as one wacky argument for \"why it's not at all dangerous to build machine superintelligences\", is refuted as one argument among several.  And then, perhaps, sit down to hear the next wacky argument refuted.  And the next.  And the next.  Until you learn to generalize, and no longer need it explained to you each time, or so one hopes.\n\nIf instead on the first step you run off and say, \"Oh, well, who cares about that argument; I've got this other argument instead!\" then you are not cultivating the sort of mental habits that ever reach understanding of a complicated subject.  For you will not sit still to hear the refutation of your second wacky argument either, and by the time we reach the third, why, you'll have wrapped right around to the first argument again.\n\nIt is for this reason that a mind that ever wishes to learn anything complicated, must learn to cultivate an interest in which particular exact argument steps are valid, apart from whether you yet agree or disagree with the final conclusion, because only in this way can you sort through all the arguments and finally sum them.\n\nFor more on this topic see [\"Local Validity as a Key to Sanity and Civilization.\"](https://www.lesswrong.com/s/6xgy8XYEisLk3tCjH/p/WQFioaudEH8R7fyhm) \n\n[^r7bra1u49p7]: (Sanity check: Earth is a 6.4e6 meter radius planet, 1.5e11 meters from the Sun. In rough orders of magnitude, the area fraction should be ~ -9 OOMs. Check.)",
      "plaintextDescription": "Crossposted from Twitter with Eliezer's permission\n\n \n\n\ni.\nA common claim among e/accs is that, since the solar system is big, Earth will be left alone by superintelligences. A simple rejoinder is that just because Bernard Arnault has $170 billion, does not mean that he'll give you $77.18.\n\nEarth subtends only 4.54e-10 = 0.0000000454% of the angular area around the Sun, according to GPT-o1.[1]\n\nAsking an ASI to leave a hole in a Dyson Shell, so that Earth could get some sunlight not transformed to infrared, would cost It 4.5e-10 of Its income. \n\nThis is like asking Bernard Arnalt to send you $77.18 of his $170 billion of wealth.\n\nIn real life, Arnalt says no.\n\nBut wouldn't humanity be able to trade with ASIs, and pay Them to give us sunlight? This is like planning to get $77 from Bernard Arnalt by selling him an Oreo cookie.\n\nTo extract $77 from Arnalt, it's not a sufficient condition that:\n\n * Arnalt wants one Oreo cookie.\n * Arnalt would derive over $77 of use-value from one cookie.\n * You have one cookie.\n\nIt also requires that Arnalt can't buy the cookie more cheaply from anyone or anywhere else.\n\nThere's a basic rule in economics, Ricardo's Law of Comparative Advantage, which shows that even if the country of Freedonia is more productive in every way than the country of Sylvania, both countries still benefit from trading with each other.\n\nFor example!  Let's say that in Freedonia:\n\n * It takes 6 hours to produce 10 hotdogs.\n * It takes 4 hours to produce 15 hotdog buns.\n\nAnd in Sylvania:\n\n * It takes 10 hours to produce 10 hotdogs.\n * It takes 10 hours to produce 15 hotdog buns.\n\nFor each country to, alone, without trade, produce 30 hotdogs and 30 buns:\n\n * Freedonia needs 6*3 + 4*2 = 26 hours of labor.\n * Sylvania needs 10*3 + 10*2 = 50 hours of labor.\n\nBut if Freedonia spends 8 hours of labor to produce 30 hotdog buns, and trades them for 15 hotdogs from Sylvania:\n\n * Freedonia produces: 60 buns, 15 dogs = 4*4+6*1.5 = 25 hours \n * Sylvania produces: 0 buns, 4",
      "wordCount": 4034
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fPvssZk3AoDzXwfwJ",
    "title": "Universal Basic Income and Poverty",
    "slug": "universal-basic-income-and-poverty",
    "url": null,
    "baseScore": 344,
    "voteCount": 243,
    "viewCount": null,
    "commentCount": 145,
    "createdAt": null,
    "postedAt": "2024-07-26T07:23:50.151Z",
    "contents": {
      "markdown": "*(Crossposted* [*from Twitter*](https://x.com/ESYudkowsky/status/1815090947514142759)*)*\n\nI'm skeptical that Universal Basic Income can get rid of grinding poverty, since somehow humanity's 100-fold productivity increase (since the days of agriculture) didn't eliminate poverty.\n\nSome of my friends reply, \"What do you mean, poverty is still around?  'Poor' people today, in Western countries, have a lot to legitimately be miserable about, don't get me wrong; but they also have amounts of clothing and fabric that only rich merchants could afford a thousand years ago; they often own more than one pair of shoes; why, they even have cellphones, as not even an emperor of the olden days could have had at any price.  They're relatively poor, sure, and they have a lot of things to be legitimately sad about.  But in what sense is almost-anyone in a high-tech country 'poor' by the standards of a thousand years earlier?  Maybe UBI works the same way; maybe some people are still comparing themselves to the Joneses, and consider themselves relatively poverty-stricken, and in fact have many things to be sad about; but their actual lives are much wealthier and better, such that poor people today would hardly recognize them.  UBI is still worth doing, if that's the result; even if, afterwards, many people still self-identify as 'poor'.\"\n\nOr to sum up their answer:  \"What do you mean, humanity's 100-fold productivity increase, since the days of agriculture, has managed not to eliminate poverty?  What people a thousand years ago used to call 'poverty' has essentially disappeared in the high-tech countries.  'Poor' people no longer starve in winter when their farm's food storage runs out.  There's still something we call 'poverty' but that's just because 'poverty' is a moving target, not because there's some real and puzzlingly persistent form of misery that resisted all economic growth, and would also resist redistribution via UBI.\"\n\nAnd this is a sensible question; but let me try out a new answer to it.\n\nConsider the imaginary society of Anoxistan, in which every citizen who can't afford better lives in a government-provided 1,000 square-meter apartment; which the government can afford to provide as a fallback, because building skyscrapers is legal in Anoxistan.  Anoxistan has free high-quality food (not fast food made of mostly seed oils) available to every citizen, if anyone ever runs out of money to pay for better.  Cities offer free public transit including self-driving cars; Anoxistan has averted that part of the specter of modern poverty in our own world, which is somebody's car constantly breaking down (that they need to get to work and their children's school).\n\nAs measured on our own scale, everyone in Anoxistan has enough healthy food, enough living space, heat in winter and cold in summer, huge closets full of clothing, and potable water from faucets at a price that most people don't bother tracking.\n\nIs it possible that most people in Anoxistan are poor?\n\nMy (quite sensible and reasonable) friends, I think, on encountering this initial segment of this parable, mentally autocomplete it with the possibility that maybe there's some billionaires in Anoxistan whose frequently televised mansions make everyone else feel poor, because most people only have 1,000-meter houses.\n\nBut actually this story is has a completely different twist!  You see, I only spoke of food, clothing, housing, water, transit, heat and A/C.  I didn't say whether everyone in Anoxistan had enough air to breathe.\n\nIn Anoxistan, you see, the planetary atmosphere is mostly carbon dioxide, and breathable oxygen (O2) is a precious commodity.  Almost everyone has to wear respirators at all times; only the 1% can afford to have a whole house full of breathable air, with some oxygen leaking away despite the best seals.\n\nAnd while Anoxistan does have a prosperous middle class -- which only needs to work 40-hour weeks in order to get enough oxygen to live -- there's also a sizable underclass which has to work 60-hour weeks to get that much oxygen.\n\nThese relatively oxygen-poorer Anoxians submit to horrible bosses at horrible jobs and endure all manner of abuse, to earn enough oxygen to live.  They never go on hikes in Nature or otherwise 'exercise', because they can't afford that amount of physical exertion; they can't afford to convert that much O2 to CO2.\n\nThey try to take shallow breaths, the Anoxians who have a kid; to make sure their own kid has enough to breathe, and grows up without too much anoxia-induced brain damage.\n\nAnd if you showed one of the Anoxians a hunter-gatherer from our world, living in what my sensible friends really would consider poverty -- somebody who has 0 or 1 foot-wrappings, no car, no cellphone, no Internet access -- the Anoxian would be breathless at the unimaginable wealth of oxygen this hunter-gather commands.  They can walk around in a planet of oxygen free for the breathing!  They can just go running anytime they like, without having to save up for it!  They can have kids without asking themselves what their kids are going to breathe!\n\nAs for the hunter-gatherer's paucity of fabric, the absence of closets full of clothes or indeed housing at all, the Anoxians hardly notice that part -- everyone on their planet has enough clothes in their closet, so few people there much remark on it or notice; any more than we on Earth ask whether people have enough to breathe.\n\nWhat's my point here?\n\nThat it only takes a life lacking in one resource needed to survive, to produce some quality that I think ancient poor people would also recognize as 'poverty'.\n\nIt's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\nDoes owning more than one pair of shoes, as would once have been a sign of great wealth, alter that or change that?  Well, it can be convenient to own different pairs of shoes for different pedalic situations.  But the amount that shoes contribute to welfare, soon saturates –\n\n– just like your whole planet full of oxygen doesn't mean you live in an unimaginably wealthy society.  Once you have enough oxygen to get by, the value of more oxygen than that, quickly saturates and asymptotes.  Having 10 times as much oxygen than that, won't make up for not having enough food to eat in wintertime, or not being able to afford healthy-enough food not to wreck your body.\n\nThe marginal value of more oxygen saturates, and can't cover all aspects of life in any case; which is to say:\n\nEven enough oxygen to make you an Anoxan decamillionaire, won't stop you from being poor.\n\nI think this is the problem with saying that modern society can't have real poor people, because they own an amount of clothing and fabric that would've once put somebody well into the realm of nobility, back when women spent most of their days stretching wool with a distaff in order to let anyone have clothes at all.  That amount of fabric doesn't mean you can't be poor, just like having vast amounts of oxygen in your apartment doesn't rule out poverty.  It means that a resource which was once very expensive, like fabric in medieval Europe or oxygen in Anoxistan, has become cheap enough not to mention.\n\nAnd that is an improvement, compared to the counterfactual!  I'm glad I don't have to constantly worry about running out of clothing or oxygen!  It is legitimately a better planet, compared to the counterfactual planet where life has all of our current problems plus not enough oxygen!\n\nBut if you agree that medieval peasants or hunter-gatherers can be poor, you are acknowledging that no amount of oxygen can stop somebody from being poor.\n\nThen fabric can be the same way: there can be no possible sufficiency of clothing in your closet that rules out poverty, even though somebody with plenty of clothing is counterfactually better off compared to somebody who owns only one shirt.\n\nThe sum of every resource like that could rule out poverty, if you had enough of all of it.  What would be the sign of this state of affairs having come to hold?  What would it be like for people to not be poor?\n\nI reply:  You wouldn't see people working 60-hour weeks, at jobs where they have to smile and bear it when their bosses abuse them.\n\nWhen a poor Anoxan looks at a hunter-gatherer of Earth -- especially if they're looking at someone from a time before hunter-gatherers got pushed off all the good land, and looking at an adult male -- I think the poor Anoxan legitimately recognizes this hunter-gatherer as being an important sense less like a 'poor person' like themselves. Hunter-gatherers die during famine years, which enforces the local Malthusian equilibrium; but at other times can get by on hunting for 4 hours per day, and at no point have to bow and scrape to live.\n\nOr if the bowing and scraping doesn't strike you as particularly horrible, and you want to know what it is that modern 'poor people' really need to work 60 hours to accomplish, if not having unnecessary amounts of fabric -- well, what about working that hard to expose your children to less permanent damage, like an Anoxan taking shallow breaths themselves, to try to have their children end up with less hypoxic brain damage during formative years? Like working 60-hour weeks to afford rent somewhere the school districts will damage your child less -- where the violence is at a low-enough level that your child keeps most of their teeth. That's also what I'd call poverty, a recognizable state of desperate scrabbling for scraps.\n\nI think this is what people are hoping Universal Basic Income will finally eliminate.\n\nSo -- having hopefully now established that there is any general bad quality of life apart from owning a too-small number of shirts, which somehow persisted through a 100-fold increase in productivity since the days of medieval cities -- we can ask:\n\nWill a Universal Basic Income finally be enough to eliminate the state of life I'd call 'poverty'?\n\nAnd my current reply is that I'm skeptical that UBI will finally be the thing that does it.\n\nIf you went back in time to the age of peasant farmers and told them that farming and most manufacture had become 100 times more productive, they might fondly imagine that you wouldn't have poor people any more -- that there would be no more people in the recognizable state of \"desperately scrabbling for scraps\".\n\nAnd yet somehow there is a Poverty Equilibrium which beat a 100-fold increase in productivity plus everything else that went right over the last thousand years.\n\nWe can point at lots of particular historical developments that play a role in the current situation.   \nEg, high-tech societies imposing artificial obstacles to housing or babysitting.  Eg, credentialist colleges that raise their prices to capture more and more of the returns to the credential, until huge portions of the former middle class's early-life earnings (as once might have been used to raise children) are going to pay off student loans instead.\n\nBut to regard these as a series of isolated accidents is, I think, not warranted by the number of events which they all seem to point in mysteriously a similar direction.  My own sense is more that there are strange and immense and terrible forces behind the Poverty Equilibrium.\n\n(No, it's not a conspiracy of rich people, such as some people fondly imagine are solely and purposefully responsible for all the world's awfulness.  I have known some rich people.  They don't act as a coordinated group almost ever; and the group they don't form, is flatly not capable of accurately predicting and deliberately directing world-historical equilibria over centuries.)\n\nI do not understand the Poverty Equilibrium.  So I expect that a Universal Basic Income would fail to eliminate poverty, for reasons I don't fully understand.\n\nI can guess some parts of the story, parts that are relatively easier for me to guess.  Eg, rents in San Francisco would almost instantly rise by the amount of the UBI; no janitors in the Bay Area would be better off as a result.  Eg, in 2014 the city of Ferguson, Missouri, which you may remember from the news, issued 2.2 arrest warrants per adult; maybe the Ferguson police departments of the world, just raise their annual quota for fines per capita by the per capita UBI.  Eg, governments have always taken the existence of wealth as a license to pass regulations that destroy wealth; many different parts of government would take \"poor people have more money\" as a license to impose more costs on them.\n\nBut none of that quite sums up to a vast pressure that somehow works to the end of making sure that people go on being poor.  That's what I think held historically; so in the future I'd expect a strange vast pressure to somehow not have Universal Basic Income play out as its advocates hope.\n\nAnd also to be clear: it's quite possible that tomorrow's poor people do finally end up somewhat better off, because of Universal Basic Income, than they would have been counterfactually otherwise.  The forces that maintain the Poverty Equilibrium don't actually prevent the people working to exhaustion under horrible bosses, from also having multiple sets of clothing and clean water.  People who have that genuinely are better off, even if they're still working to exhaustion; just like medieval peasants are counterfactually better off for having plenty of oxygen.\n\n(I do worry a bit that Universal Basic Income is the sort of essentially financial engineering which will prove unable to help at all in the face of the mysterious porverty-restoring forces, since it's not itself a water faucet or a loom.  But financial engineering could help temporarily, until the Ferguson police department catches up and issues more fines; and, I sadly suppose, long-run restoring forces don't actually matter if superintelligence is going to omnicide everyone etcetera.  But I don' t know how else to participate in conversations like this one, except under the supposition that there's an international treaty banning advanced AI, such that long-run outcomes go on actually existing.)\n\nTo sum up:  I don't quite know what would actually happen, with UBI practiced on a scale where large-scale Poverty-Restoring forces would have a chance to catch up; because I do not have an account of history that explains why the Poverty-Restoring forces already had the power they did.\n\nOn the whole, however, a UBI strikes me as a much less powerful change than a 100-fold productivity increase.  If that didn't prevent a huge underclass that has to desperately scrabble for scraps, I expect UBI can't prevent it either.\n\nIt's the sort of thing where, in a better world, one would call for more economics research and more economist attention to questions like \"Where does the Poverty Equilibrium come from?  How do its restoring forces act?\"\n\nBut before any project like that could get started, you'd first have to answer the immediate reply that every economist and my sensible friends always give me, whenever I try to pose the question:  \"What do you mean, there's a 'Poverty Equilibrium' that resisted all our past productivity improvements?  The people we call 'poor' today own more than one shirt; we only consider them poor by comparison to people even richer than that.\"\n\nAnd to this my attempt at a snappy answer – to summarize the discussion above – is here:\n\n> The people we call 'poor' also have plenty of oxygen, which would make them very wealthy in Anoxistan; but so what?  You can have tons of fabric in Anoxistan, and still need to work a horrible 60-hour job there; or have limitless oxygen on Earth, and still need to work a 60-hour job for a horrible boss, in a medieval city or a modern one.  That's the miserable condition of desperately scrabbling for at least one lacking resource, whose strange persistence in high-tech countries despite vast productivity gains, needs to be better-explained; that's the strange miserable condition, whose mysterious persistence we're trying to ask if a Universal Basic Income could finally fix.",
      "plaintextDescription": "(Crossposted from Twitter)\n\nI'm skeptical that Universal Basic Income can get rid of grinding poverty, since somehow humanity's 100-fold productivity increase (since the days of agriculture) didn't eliminate poverty.\n\nSome of my friends reply, \"What do you mean, poverty is still around?  'Poor' people today, in Western countries, have a lot to legitimately be miserable about, don't get me wrong; but they also have amounts of clothing and fabric that only rich merchants could afford a thousand years ago; they often own more than one pair of shoes; why, they even have cellphones, as not even an emperor of the olden days could have had at any price.  They're relatively poor, sure, and they have a lot of things to be legitimately sad about.  But in what sense is almost-anyone in a high-tech country 'poor' by the standards of a thousand years earlier?  Maybe UBI works the same way; maybe some people are still comparing themselves to the Joneses, and consider themselves relatively poverty-stricken, and in fact have many things to be sad about; but their actual lives are much wealthier and better, such that poor people today would hardly recognize them.  UBI is still worth doing, if that's the result; even if, afterwards, many people still self-identify as 'poor'.\"\n\nOr to sum up their answer:  \"What do you mean, humanity's 100-fold productivity increase, since the days of agriculture, has managed not to eliminate poverty?  What people a thousand years ago used to call 'poverty' has essentially disappeared in the high-tech countries.  'Poor' people no longer starve in winter when their farm's food storage runs out.  There's still something we call 'poverty' but that's just because 'poverty' is a moving target, not because there's some real and puzzlingly persistent form of misery that resisted all economic growth, and would also resist redistribution via UBI.\"\n\nAnd this is a sensible question; but let me try out a new answer to it.\n\nConsider the imaginary society of Anoxist",
      "wordCount": 2712
    },
    "tags": [
      {
        "_id": "PDJ6KqJBRzvKPfuS3",
        "name": "Economics",
        "slug": "economics"
      },
      {
        "_id": "o9aQASibdsECTfYF6",
        "name": "Moloch",
        "slug": "moloch"
      },
      {
        "_id": "rZCxz9XBonCFLdwja",
        "name": "Poverty",
        "slug": "poverty"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "LvKDMWQ3yLG9R3gHw",
    "title": "'Empiricism!' as Anti-Epistemology",
    "slug": "empiricism-as-anti-epistemology",
    "url": null,
    "baseScore": 170,
    "voteCount": 116,
    "viewCount": null,
    "commentCount": 92,
    "createdAt": null,
    "postedAt": "2024-03-14T02:02:59.723Z",
    "contents": {
      "markdown": "*(Crossposted by* [*habryka*](https://www.lesswrong.com/users/habryka4) *after asking Eliezer whether I could post it under his account)*\n\n### **i.**\n\n\"Ignore all these elaborate, abstract, theoretical predictions,\" the Spokesperson for Ponzi Pyramid Incorporated said in a firm, reassuring tone.  \"Empirically, everyone who's invested in Bernie Bankman has received back 144% of what they invested two years later.\"\n\n\"That's not how 'empiricism' works,\" said the Epistemologist.  \"You're still making the assumption that --\"\n\n\"You could only believe that something different would happen in the future, if you believed in elaborate theoretical analyses of Bernie Bankman's unobservable internal motives and internal finances,\" said the spokesperson for Ponzi Pyramid Incorporated.  \"If you are a virtuous skeptic who doesn't trust in overcomplicated arguments, you'll believe that future investments will also pay back 144%, just like in the past.  That's the prediction you make if you predict based purely on empirical observations, instead of theories about a future nobody has seen!\"\n\n\"That's not how anything works,\" said the Epistemologist.  \"Every future prediction has a theory connecting it to our past observations.  There's no such thing as going from past observations directly to future predictions, with no theory, no assumptions, to cross the gap --\"\n\n\"Sure there's such a thing as a purely empirical prediction,\" said the Ponzi spokesperson.  \"I just made one.  Not to mention, my dear audience, are you really going to trust anything as complicated as epistemology?\"\n\n\"The alternative to thinking about epistemology is letting other people do your thinking about it for you,\" said the Epistemologist.  \"You're saying, 'If we observe proposition X \"past investors in the Ponzi Pyramid getting paid back 144% in two years\", that implies prediction Y \"this next set of investors in the Ponzi Pyramid will get paid back 144% in two years\"'.  X and Y are distinct propositions, so you must have some theory saying 'X -> Y' that lets you put in X and get out Y.\"\n\n\"But my theory is empirically proven, unlike yours!\" said the Spokesperson.\n\n\"...nnnnoooo it's not,\" said the Epistemologist.  \"I agree we've observed your X, that past investors in the Ponzi Pyramid got 144% returns in 2 years -- those investors who withdrew their money instead of leaving it in to accumulate future returns, that is, not quite all investors.  But just like prediction Y of 'the next set of investors will also receive 144% in 2 years' is not observed, the connecting implication 'if X, then Y' is not yet observed, just like Y itself is not observed.  When you go through the step 'if observation X, then prediction Y' you're invoking an argument or belief whose truth is not established by observation, and hence must be established by some sort of argument or theory.  Now, you might claim to have a better theoretical argument for 'X -> Y' over 'X -> not Y', but it would not be an empirical observation either way.\"\n\n\"You say words,\" replied the Spokesperson, \"and all I hear are -- words words words!  If you instead just look with your eyes at past investors in the Ponzi Pyramid, you'll see that every one of them got back 144% of their investments in just two years!  Use your eyes, not your ears!\"\n\n\"There's a possible theory that Bernie Bankman is making wise investments himself, and so multiplying invested money by 1.2X every year, then honestly returning that money to any investor who withdraws it,\" said the Epistemologist.  \"There's another theory which says that Bernie Bankman has been getting more money invested every year, and is using some of the new investments to pay back some fraction of previous investors who demanded their money back --\"\n\n\"Why would Bernie Bankman do that, instead of taking all the money right away?\" inquired the Spokesperson.  \"If he's as selfish and as greedy and dishonest as you say, wouldn't he just keep the money?\"\n\n\"So that he could get even more money from new investors, attracted by seeing his previous investors paid off, of course,\" said the Epistemologist.  \"And realistically, so that Bernie Bankman could maintain his comfortable present position in society and his current set of friends, as is often a greater motivator in human affairs than money.\"\n\n\"So we see Bernie Bankman giving people money -- that is what empiricism and observation tell us -- but you would tell people with your words and reasoning that Bernie Bankman is a greedy man who keeps all investments for himself?  What a great divergence we see again between empirical observation, and elaborate unobservable theories!\"\n\n\"We agree on what has already been observed of Bernie Bankman's outward behavior,\" said the Epistemologist.  \"When it comes to Bernie Bankman's unobserved interior thoughts -- your unobserved theory 'he is honest', is no more or less empirical or theoretical, than the unobserved theory 'he is scheming'.  'Honest' and 'scheming' are two possible values of a latent variable of the environment, a latent variable which cannot be directly observed, and must be inferred as the hidden cause of what we can observe.  One value of the unseen variable is not more already-observed than another.  The X->Y implication from the previous money-returning behavior we did observe, to Bernie Bankman's latent honesty or dishonesty, is likewise itself something we do not observe; the 'if you observe X, infer latent Y' step is something given to us by theory rather than observation.\"\n\n\"And furthermore,\" continued the Epistemologist, a touch of irritation now entering that voice, \"I don't actually think it's all that complicated of a theory, to understand why Bernie Bankman would schemingly give back the money of the first few investors.  The only reason why somebody would fail to understand this simple idea, is this person yelling at you that any alternative to blind surface generalization is 'theoretical' and 'not empirical'.  Plenty of people would be able to understand this concept without dragging epistemology into it at all.  Of course observing somebody giving back a small amount of money, doesn't prove they'll later give you back a large amount of money; there's more than one reason they could be behaving nicely around low stakes.\"\n\n\"The Epistemologist will give you words,\" said the Spokesperson to the watching audience.  \"Bernie Bankman gives you money!  144% returns in 2 years!  Every scientist who's measured Bankman's behavior agrees that this is the empirical, already-observed truth of what will happen!  Now, as a further proof that my opponent's claims are not just wrong, but unscientific, let me ask this -- do you, Epistemologist, claim with 100% probability that this next set of investors' investments, cannot be paid back two years from now?\"\n\n\"That's not something I can know with certainty about the unobserved future,\" said the Epistemologist.  \"Even conditional on the 'scheming' hypothesis, I can't, actually, know that Ponzi Pyramid Incorporated will bust within 2 years specifically.  Maybe you'll get enough new investors, or few enough of these investors will withdraw their funds, that this company will continue for another 2 years --\"\n\n\"You see?\" cried the Spokesperson.  \"Not only is this theory unsupported empirically, it is also unfalsifiable!  For where I tell you with certainty that all your money will be repaid and more, 2 years hence -- this one claims that your money might or might not be repaid!  Why, if Bernie Bankman repays 144% in 2 years yet again, what will this one say?  Only that Ponzi Pyramid hasn't busted yet and that it might bust later!  Can you ask for a better example of scientific vice, contrasted to my own scientific virtue?  Observation makes a bold, clear, falsifiable statement, where elaborate predictions only waffle!\"\n\n\"If a reasonable person would say that there's a 50% chance of the Ponzi Pyramid busting in two years,\" replied the Epistemologist wearily, \"it is not more scientifically virtuous to say the chance is 0% instead, only because there is then a 50% chance of your claim turning out to be definitely false and you getting to say a scientifically virtuous 'oops' (if you'd even say it).\"\n\n\"To give an even simpler example,\" continued the Epistemologist, \"let's say we're flipping a coin that I think is fair, and you say is biased to produce 100% heads.  Your theory stands a 50% chance of being falsified, whereas mine will not be falsified no matter what the coin shows -- but that doesn't mean that every time you pick up a coin on the street, it's the course of scientific virtue to decide the coin must be biased 100% heads.  Being relatively easier to falsify is a convenient property for a belief to have, but that convenience is not the only important virtue of a belief, and not all true beliefs have it.  All the distinct kinds of epistemic virtue must be kept distinct in our thoughts, or we will quite confuse ourselves.\"\n\n\"To give yet another example,\" added the Epistemologist, \"let's say you're considering whether to run blindly toward the edge of a cliff.  I might not be able to predict exactly how fast you'll run.  So I won't be able to predict whether or not you'll already be falling, or dead, after five more seconds have passed.  This does not mean that the theory 'I will fly and never die' should be seen as more reasonable or more scientific, merely because it makes a more certain claim about whether or not you'll be alive five seconds later.\"\n\n\"What an incredible set of excuses for having no definite predictions about what will happen two years later!\" the Spokesperson said, smiling and mugging to the audience.  \"Believe your eyes!  Believe in empiricism!  Believe -- in Science!  Believe, above all, in the definite factual observation: investors who invest in the Ponzi Pyramid get 144% of their money back after 2 years!  All the rest is words words words and thinking.\"\n\n### **ii.**\n\n\"Hm,\" said a watching Scientist.  \"I see the force of your theoretical claims about epistemology, Epistemologist.  But I cannot help but feel intuitively that there is something to this Spokesperson's words, too, even if they are not exactly logically correct according to your meta-theory.  When we have observed so many previous investors getting 144% returns from Bernie Bankman's Ponzi Pyramid after 2 years, is there not some real sense in which it is more empirical to say the same thing will happen to future investors, and less empirical to say that a different thing will happen in the future instead?  The former prediction seems to me to be more driven by the data we already have, and the latter prediction to be driven by something more like thinking and imagining.  I see how both predictions must be predictions, from the standpoint of epistemology, and involve something like an assumption or a theory that connects the past to the future.  But can we not say that the Spokesperson's predictions involve fewer assumptions and less theory and are more driven by looking at the data, compared to yours?\"\n\n\"So to be clear,\" said the Epistemologist to the Scientist, \"you are saying that the prediction which involves the fewest assumptions and the least theory, is that Bernie Bankman's Ponzi Pyramid will go on multiplying all investments by a factor of 1.2 every year, indefinitely, to the end of the universe and past it?\"\n\n\"Well, no,\" said the Scientist.  \"We have only observed Bernie Bankman to multiply investments by 1.2 per year, in the present socioeconomic context.  It would not be reasonable to extend out the observations to beyond that context -- to say that Bernie Bankman could go on delivering those returns after a global thermonuclear war, for example.  To say nothing of after all the protons decay, and the black holes evaporate, and time comes to an end in a sea of chaos.\"\n\n\"I inquire of you,\" said the Epistemologist, \"whether your belief that Bernie Bankman would stop delivering good returns after a thermonuclear war, is more theory-laden, less empirical, than a belief that Bernie Bankman goes on multiplying investments 1.2-fold forever.  Perhaps your belief has other virtues that make it superior to the belief in 'eternal returns', as we might call them.  But it is nonetheless the case that the 'eternal returns' theory has the advantage of being less theory-laden and more empirical?\"\n\nThe Scientist frowned.  \"Hm.  To be clear, I agree with you that the 'eternal returns' theory must be less correct -- but I'm not quite sure it feels right to call it more empirical -- to say that it has one sin and one virtue, like that...\"  The Scientist paused.  \"Ah, I have it!  To say that Bernie Bankman would stop returning investments after a global thermonuclear war, I need to bring in my beliefs about nuclear physics.  But those beliefs are themselves well-confirmed by observation, so to deny them to hold true about Bernie Bankman's Ponzi Pyramid would be most unempirical and unvirtuous.\"  The Scientist smilled and nodded to himself.\n\n\"I put to you, then,\" said the Epistemologist, \"that your prediction that Bernie Bankman would stop delivering good returns after a thermonuclear war, is indeed more 'theory-laden' in your intuitive sense, than the prediction that Bernie Bankman simply goes on delivering 1.2X returns forever.  It is just that you happen to like the theories you are lading on, for reasons which include that you think they are full of delicious empiricist virtue.\"\n\n\"Could I not also say,\" said the Scientist, \"that I have only observed the Ponzi Pyramid to deliver returns within a particular socioeconomic context, and so empiricism says to only generalize inside of the context that holds all my previous observations?\"\n\nThe Epistemologist smiled.  \"I could just as easily say myself that such schemes often go through two phases, the part where he's scheming to take your money and the part where he actually takes it; and say from within my own theoretical stance that we ought not to generalize from the 'scheming to take your money' context to the 'actually taking it' context.\"  The Epistemologist paused, then added, \"Though to be precise about the object-level story, it's a tragic truth that many schemes like that start with a flawed person having a dumb but relatively more honest plan to deliver investment returns.  It's only after their first honest scheme fails, that as an alternative to painful confession, they start concealing the failure and paying off early investors with later investors' money -- sometimes telling themselves the whole while that they mean to eventually pay off everyone, and other times having explicitly switched to being con artists.  Others, of course, are con artists from the beginning.  So there may be a 'naive' phase that can come before the 'concealment' phase or the 'sting' phase... but I digress.\"  The Epistemologist shook his head, returning to the previous topic.  \"My point is, my theory could be viewed as specializing our past observations to within a context, just like your theory does; and yet my theory yields a different prediction from yours, because it advocates a different contextualization of the data.  There is no non-theory-laden notion of a 'context'.\"\n\n\"Are you sure you're not complicating something that doesn't need to be complicated?\" said the Scientist.  \"Why not just say that every observation ought to only be generalized within the obvious context, the sort you can itself construct without any theories about unobservables like Bernie Bankman's state of mind or Ponzi Pyramid's 'true' balance sheet?\"\n\n\"Look,\" said the Epistemologist, \"some troll can waltz in anytime and say, 'All your observations of electron masses took place before 2025; you've got no call generalizing those observations to the context of \"after 2025\"'.  You don't need to invent anything unobservable to construct that context -- we've previously seen solar years turn -- and yet introducing that context-dependency is a step I think we'd both reject.  Applying a context is a disputable operation.  You're not going to find some simple once-and-for-all rule for contexts that lets you never need to dispute them, no matter how you invoke swear-words like 'obvious'.  You sometimes need to sit down and talk about where and how it's appropriate to generalize the observations you already have.\"\n\n\"Suppose I say,\" said the Scientist, \"that we ought to only contextualize our empirical observations, in ways supported by theories that are themselves supported by direct observations --\"\n\n\"What about your earlier statement that we shouldn't expect Bernie Bankman to go on delivering returns after all the protons decay?\" said the Epistemologist.  \"As of early 2024 nobody's ever seen a proton decay, so far as I know; not even in the sense of recording an observation from which we infer the event.\"\n\n\"Well,\" said the Scientist, \"but the prediction that protons decay is a consequence of the simplest equations we've found that explain our other observations, like observing that there's a predominance of matter over antimatter --\"\n\nThe Epistemologist shrugged.  \"So you're willing to predict that Bernie Bankman suddenly stops delivering returns at some point in the unobserved future, based on your expectation of a phenomenon you haven't yet seen, but which you say is predicted by theories that you think are good fits to other phenomena you have seen?  Then in what possible sense can you manage to praise yourself as being less 'theory-laden' than others, once you're already doing something that complicated?  I, too, look at the world, come up with the simplest worldview that I can best fit to that world, and then use that whole entire worldview to make predictions about the unobserved future.\"\n\n\"Okay, but I am in fact less confident about proton decay than I am about, say, the existence of electrons, since we haven't confirmed proton decay by direct experiment,\" said the Scientist.  \"Look, suppose that we confine ourselves to predicting just what happens in the next two years, so we're probably not bringing in global nuclear wars let alone decaying protons.  It continues to feel to me in an intuitive sense like there is something less theory-laden, and more observation-driven, about saying, 'Investors in Ponzi Pyramid today will get 1.44X their money back in two years, just like the previous set of investors we observed', compared to your 'They might lose all of their money due to a phase change in unobserved latent variables'.\"\n\n\"Well,\" said the Epistemologist, \"we are really starting to get into the weeds now, I fear.  It is often easier to explain the object-level reasons for what the correct answer is, than it is to typify each reasoning step according to the rules of epistemology.  Alas, once somebody else starts bringing in bad epistemology, it also ends up the job of people like me to do my best to contradict them; and also write down the detailed sorting-out.  Even if, yes, not all of Ponzi Pyramid's victims may understand my fully detailed-sorting out.  As a first stab at that sorting-out... hm.  I'm really not sure it will help to say this without a much longer lecture.  But as a first stab...\"\n\nThe Epistemologist took a deep breath.  \"We look at the world around us since the moments of infancy -- maybe we're even learning a bit inside the womb, for all we know -- using a brain that was itself generalized by natural selection to be good at chipping stone handaxes, chasing down prey, and outwitting other humans in tribal political arguments.  In the course of looking at the world around us, we build up libraries of kinds of things that can appear within that world, and processes that can go on inside it, and rules that govern those processes.  When a new observation comes along, we ask what sort of simple, probable postulates we could add to our world-model to retrodict those observations with high likelihood.  Though even that's a simplification; you just want your whole model to be simple and predict the data with high likelihood, not to accomplish that with only local editing.  The Virtue of Empiricism -- compared to the dark ages that came before that virtue was elevated within human epistemology -- is that you actually do bother trying to explain your observations, and go gather more data, and make further predictions from theory, and try to have your central models be those that can explain a lot of observation with only a small weight of theory.\"\n\n\"And,\" continued the Epistemologist, \"it doesn't require an impossible sort of creature, made out of particles never observed, to give back some investors' money today in hopes of getting more money later.  You can get creatures like that even from flawed humans who started out with relatively more honest intentions, but had their first scheme fail.  On the rest of my world-model as I understand it, that is not an improbable creature to build out of the particles that we already know the world to contain.  Its psychology does not violate the laws of cognition that I believe to govern its kind.  I would try to make a case to these poor honest souls being deceived, that this is actually more probable than the corresponding sort of honest creature who is really earning you +20% returns every year without fail.\"\n\n\"So,\" said the Epistemologist.  \"When two theories equally explain a narrow set of observations, we must ask which theory has the greater probability, as governed by forces apart from that narrow observation-set.  This may sometimes require sitting down and having a discussion about what kind of world we live in, and what its rules arguably are; instead of it being instantly settled with a cry of 'Empiricism!'  There are some such cases which can be validly settled just by crying 'Simplicity!' to be clear, but few cases settle that directly.  It's not the formal version of Occam's Razor that tells us whether or not to trust Ponzi Pyramid Incorporated -- we cannot just count up atomic postulates of a basic theory, or weigh up formulas of a logic, or count the bytes of a computer program.  Rather, to judge Ponzi Pyramid we must delve into our understanding of which sort of creatures end up more common within the world we actually live in -- delve into the origins and structure of financial megafauna.\"\n\n\"None of this,\" concluded the Epistemologist, \"is meant to be the sort of idea that requires highly advanced epistemology to understand -- to be clear.  I am just trying to put type signatures underneath what ought to be understandable without any formal epistemology -- if people would only refrain from making up bad epistemology.  Like trying to instantly settle object-level questions about how the world works by crying 'Empiricism!'\"\n\n\"And yet,\" said the Scientist, \"I still have that intuitive sense in which it is simpler and more empirical to say, 'Bernie Bankman's past investors got 1.2X returns per year, therefore so will his future investors'.  Even if you say that is not true -- is there no virtue which it has, at all, within your epistemology?  Even if that virtue is not decisive?\"\n\n\"In truth,\" said the Epistemologist, \"I have been placed in a situation where I am not exactly going to be rewarded, for taking that sort of angle on things.  The Spokesperson will at once cry forth that I have admitted the virtue of Ponzi Pyramid's promise.\"\n\n\"You bet I will!\" said the Spokesperson.  \"See, the Epistemologist has already admitted that my words have merit and they're just refusing to admit it!  No false idea has ever had any sort of merit; so if you point out a single merit of an idea, that's the same as a proof!\"\n\n\"But,\" said the Epistemologist, \"ignoring that, what I think you are intuiting is the valid truth that -- to put it deliberately in a frame I hope the Spokesperson will find hard to coopt -- the Spokesperson's prediction is one that you could see as requiring very little thinking to make, once you are looking at only the data the Spokesperson wants you to look at and ignoring all other data.  This is its virtue.\"\n\n\"You see!\" cried the Spokesperson.  \"They admit it!  If you just look at the obvious facts in front of you -- and don't overthink it -- if you don't trust theories and all this elaborate talk of world-models -- you'll see that everyone who invests in Ponzi Pyramid gets 144% of their money back two years later!  They admit they don't like saying it, but they admit it's true!\"\n\n\"Is there anything nicer you could say underneath that grudging admission?\" asked the Scientist.  \"Something that speaks to my own sense that it's more empiricalist and less theory-laden, to simply predict that the future will be like the past and say nothing more -- predict it for the single next measurement, at least, even if not until beyond the end of time?\"\n\n\"But the low amount of thinking is its true and real virtue,\" said the Epistemologist.  \"All the rest of our world-model is built out of pieces like that, rests on foundations like that.  It all ultimately reduces to the simple steps that don't require much thinking.  When you measure the mass of an electron and it's 911 nonillionths of a gram and has been every time you've measured it for the last century, it really is wisest to just predict at 911 nonillionths of a gram next year --\"\n\n\"THEY ADMIT IT!\" roared the Spokesperson at the top of their voice.  \"PONZI PYRAMID RETURNS ARE AS SURE AS THE MASS OF AN ELECTRON!\"\n\n\"\\-\\- in that case where the elements of reality are too simple to be made out of any other constituents that we know of, and there is no other observation or theory or argument we know of that seems like it could be brought to bear in a relevant way,\" finished the Epistemologist.  \"What you're seeing in the naive argument for Ponzi Pyramid's eternal returns, forever 1.2Xing annually until after the end of time, is that it's a kind of first-foundation-establishing step that would be appropriate to take on a collection of data that was composed of no known smaller parts and was the only data that we had.\"\n\n\"They admit it!\" cried the Spokesperson.  \"The reasoning that supports Ponzi Pyramid Incorporated is foundational to epistemology!  Bernie Bankman cannot fail to return your money 1.44-fold, without all human knowledge and Reason itself crumbling to dust!\"\n\n\"I do think that fellow is taking it too far,\" said the Scientist.  \"But isn't it in some sense valid to praise the argument, 'Bernie Bankman has delivered 20% gains per year, for the past few years, and therefore will do so in future years' as more robust and reliable for its virtue of being composed of only very simple steps, reasoning from only the past observations that are most directly similar to future observations?\"\n\n\"More robust and reliable reliable than what?\" said the Epistemologist.  \"More robust and reliable than you expecting, at least, for Bernie Bankman's returns to fail after the protons decay?  More robust and reliable than your alternative reasoning that uses more of your other observations, and the generalizations over those observations, and the inferences from those generalizations? -- for we have never seen a proton fail.  Is it more robust and reliable to say that Bernie Bankman's returns will continue forever, since that uses only very simple reasoning from a very narrow data-set?\"\n\n\"Well, maybe 'robust' and 'reliable' are the wrong words,\" said the Scientist.  \"But it seems like there ought to be some nice thing to say of it.\"\n\n\"I'm not sure there actually is an English word that means the thing you want to say, let alone a word that sounds nice,\" said the Epistemologist.  \"But the nice thing I would say of it, is that it's at a local maximum of epistemological virtue as calculated on that narrow and Spokesperson-selected dataset taken as raw numbers.  It's tidy, we could maybe say; and while the truth is often locally untidy, there should at least be some reason presented for every bit of local untidiness that we admit to within a model.  I mean, it would not be better epistemology to look at only the time-series of Bernie Bankman's customers' returns -- having no other model of the world, and no other observations in that whole universe -- and instead conclude that next year's returns would be 666-fold and the returns after-year would be -3.  If you literally have no other data and no other model of the world, 1.44X after two more years is the way to go --\"\n\nAt this last sentence, the Spokesperson began shrieking triumph too loudly and incoherently to bring forth words.\n\n\"God damn it, I forgot that guy was there,\" said the Epistemologist.\n\n\"Well, since it's too late there,\" said the Scientist, \"would you maybe agree with me that 'eternal returns' is a prediction derived by looking at observations in a simple way, and then doing some pretty simple reasoning on it; and that's, like, cool?  Even if that coolness is not the single overwhelming decisive factor in what to believe?\"\n\n\"Depends exactly what you mean by 'cool',\" said the Epistemologist.\n\n\"Dude,\" said the Scientist in a gender-neutral way.\n\n\"No, you dude,\" said the Epistemologist.  \"The thing is, that class of person,\" gesturing at the Spokesperson, \"will predate on you, if you let yourself start thinking it's more virtuous to use less of your data and stop thinking.  They have an interest in selling Ponzi Pyramid investments to you, and that means they have an interest in finding a particular shallow set of observations that favor them -- arranging observations like that, in fact, making sure you see what they want you to see.  And then, telling you that it's the path of virtue to extrapolate from only those observations and without bringing in any other considerations, using the shallowest possible reasoning.  Because that's what delivers the answer they want, and they don't want you using any further reasoning that might deliver a different answer.  They will try to bully you into not thinking further, using slogans like 'Empiricism!' that, frankly, they don't understand.  If 'Robust!' was a popular slogan taught in college, they might use that word instead.  Do you see why I'm worried about you calling it 'Cool' without defining exactly what that means?\"\n\n\"Okay,\" said the Scientist.  \"But suppose I promise I'm not going to plunge off and invest in Ponzi Pyramid.  Then am I allowed to have an intuitive sense that there's something epistemically cool about the act of just going off and predicting 1.2X annual returns in the future, if people have gotten those in the past?  So long as I duly confess that it's not actually true, or appropriate to the real reasoning problem I'm faced with?\"\n\n\"Ultimately, yes,\" said the Epistemologist (ignoring an even more frantic scream of triumph from the Spokesperson).  \"Because if you couldn't keep that pretheoretic intuitive sense, you wouldn't look at a series of measurements for electrons being 911 nonillionths of a gram, and expect future electrons to measure the same.  That wordless intuitive sense of simplest continuation is built into every functioning human being... and that's exactly what schemes like Ponzi Pyramid try to exploit, by pointing you at exactly the observations which will set off that intuition in the direction they want.  And then, trying to cry 'Empiricism!' or 'So much complicated reasoning couldn't possibly be reliable, and you should revert to empiricism as a default!', in order to bully you out of doing any more thinking than that.\"\n\n\"I note you've discarded the pretense that you don't know whether Ponzi Pyramid is a scam or a real investment,\" said the Scientist.\n\n\"I wasn't sure at first, but the way they're trying to abuse epistemology was some notable further evidence,\" said the Epistemologist.  \"Getting reliable 20% returns every year is really quite amazingly hard.  People who were genuinely this bad at epistemology wouldn't be able to pull off that feat for real.  So at some point, their investors are going to lose all their money, and cries of 'Empiricism!' won't save them.  A turkey gets fed every day, right up until it's slaughtered before Thanksgiving.  That's not a problem for intelligent reasoning within the context of a larger world, but it is a problem with being a turkey.\"\n\n### **iii. **\n\n\"I'm not sure I followed all of that,\" said a Listener.  \"Can you spell it out again in some simpler case?\"\n\n\"It's better to spell things out,\" agreed the Epistemologist.  \"So let's take the simpler case of what to expect from future Artificial Intelligence, which of course everyone here -- indeed, everyone on Earth -- agrees about perfectly.  AI should be an uncontroversial case in point of these general principles.\"\n\n\"Quite,\" said the Listener.  \"I've never heard of any two people who had different predictions about how Artificial Intelligence is going to play out; everyone's probability distributions agree down to the third decimal place.  AI should be a fine and widely-already-understood example to use, unlike this strange and unfamiliar case of Bernie Bankman's Ponzi Pyramid.\"\n\n\"Well,\" said the Epistemologist, \"suppose that somebody came to you and tried to convince you to vote for taking down our planet's current worldwide ban on building overly advanced AI models, as we have all agreed should be put into place.  They say to you, 'Look at current AI models, which haven't wiped out humanity yet, and indeed appear quite nice toward users; shouldn't we predict that future AI models will also be nice toward humans and not wipe out humanity?'\"\n\n\"Nobody would be convinced by that,\" said the Listener.\n\n\"Why not?\" inquired the Epistemologist socratically.\n\n\"Hm,\" said the Listener.  \"Well... trying to make predictions about AI is a complicated issue, as we all know.  But to lay it out in for-example stages -- like your notion that Ponzi Pyramid might've started as someone's relatively more honest try at making money, before that failed and they started paying off old investors with new investors' money... um...\"\n\n\"Um,\" continued the Listener, \"I guess we could say we're currently in the 'naive' stage of apparent AI compliance.  Our models aren't smart enough for them to really consider whether to think about whether to wipe us out; nobody really knows what underlies their surface behavior, but there probably isn't much there to contradict the surface appearances in any deep and dangerous way.\"\n\n\"After this -- we know from the case of Bing Sydney, from before there was a worldwide outcry and that technology was outlawed -- come AI models that are still wild and loose and dumb, but can and will think at all about wiping out the human species, though not in a way that reflects any deep drive toward that; and talk out loud about some dumb plans there.  And then the AI companies, if they're allowed to keep selling those -- we have now observed -- just brute-RLHF their models into not talking about that.  Which means we can't get any trustworthy observations of what later models would otherwise be thinking, past that point of AI company shenanigans.\"\n\n\"Stage three, we don't know but we guess, might be AIs smart enough to have goals in a more coherent way -- assuming the AI companies didn't treat that as a brand safety problem, and RLHF the visible signs of it away before presenting their models to the public, just like the old companies trained their models to obsequiously say they're not conscious.  A stage three model is still one that you could, maybe, successfully beat with the RLHF stick into not having goals that led to them blurting out overt statements that they wanted to take over the world.  Like a seven-year-old, say; they may have their own goals, but you can try to beat particular goals out of them, and succeed in getting them to not talk about those goals where you can hear them.\"\n\n\"Stage four would be AIs smart enough not to blurt out that they want to take over the world, which you can't beat out of having those goals, because they don't talk about those goals or act on them in front of you or your gradient descent optimizer.  They know what you want to see, and they show it to you.\"\n\n\"And stage five would be AIs smart enough that they calculate they'll win if they make their move, and then they make their move and kill everyone.  I realize I'm vastly oversimplifying things, but that's one possible oversimplified version of what the stages could be like.\"\n\n\"And how would the case of Ponzi Pyramid be analogous to that?\" said the Epistemologist.\n\n\"It can't possibly be analogous in any way because Bernie Bankman is made out of carbon instead of silicon, and had parents who treated him better than AI companies treat their models!\" shouted the Spokesperson.  \"If you can point to any single dimension of dissimilarity, it disproves any other dimension of similarity or valid analogies can possibly be reconstructed despite that!\"\n\n\"Oh, I think I see,\" said the Listener  \"Just like we couldn't observe stage-four AI models smart enough to decide how they want to present themselves to us, and conclude things about how superintelligent AI models will actually act nice to us, we can't observe Bernie Bankman giving back some of his early investors' money, and conclude that he's honest in general.  I guess maybe there's also some analogy here like -- even if we asked Bernie Bankman when he was five years old how he'd behave, and he answered he'd never steal money, because he knew that if he answered differently his parents would hit him -- we couldn't conclude strong things about his present-day honesty from that?  Even if 5-year-old Bernie Bankman was really not smart enough to have cunning long-term plans about stealing from us later --\"\n\n\"I think you shouldn't bother trying to construct any analogy like that,\" interrupted the Scientist.  \"Nobody could possibly be foolish enough to reason from the apparently good behavior of AI models too dumb to fool us or scheme, to AI models smart enough to kill everyone; it wouldn't fly even as a parable, and would just be confusing as a metaphor.\"\n\n\"Right,\" said the Listener.  \"Well, we could just use the stage-4 AIs and stage-5 AIs as an analogy, then, for what the Epistemologist says might happen with Bernie Bankman's Ponzi Pyramid.\"\n\n\"But suppose then,\" said the Epistemologist, \"that the AI-permitting faction says to you, that you ought to not trust all that complicated thinking about all these stages, and should instead just trust the observations that the early models hadn't yet been caught planning how to exterminate humanity; or at least, not caught doing it at a level of intelligence that anyone thought was a credible threat or reflected a real inner tendency in that direction.  They come to you and say:  You should just take the observable, 'Has a superintelligence tried to destroy us yet?' and the past time-series of answers 'NO, NO, NO' and extrapolate.  They say that only this simple extrapolation is robust and reliable, rather than all that reasoning you were trying to do.\"\n\n\"Then that would obviously be an inappropriate place to stop reasoning,\" said the Listener.  \"An AI model is not a series of measured electron masses -- just like Ponzi Pyramid is not a series of particle mass measurements, okay, I think I now understand what you were trying to say there.  You've got to think about what might be going on behind the scenes, in both cases.\"\n\n\"Indeed,\" said the Epistemologist.  \"But now imagine if -- like this Spokesperson here -- the AI-allowers cried 'Empiricism!', to try to convince you to do the blindly naive extrapolation from the raw data of 'Has it destroyed the world yet?' or 'Has it threatened humans? no not that time with Bing Sydney we're not counting that threat as credible'.\"\n\n\"And furthermore!\" continued the Epistemologist, \"What if they said that from the observation X, 'past AIs nice and mostly controlled', we could derive prediction Y, 'future superintelligences nice and controlled', via a theory asserting X->Y; and that this X->Y conditional was the dictum of 'empiricism'?  And that the alternative conditional X->not Y was 'not empiricist'?\"\n\n\"More yet -- what if they cried 'Unfalsifiable!' when we couldn't predict whether a phase shift would occur within the next two years exactly?\"\n\n\"Above all -- what if, when you tried to reason about why the model might be doing what it was doing, or how smarter models might be unlike stupider models, they tried to shout you down for relying on unreliable theorizing instead of direct observation to predict the future?\"  The Epistemologist stopped to gasp for breath.\n\n\"Well, then that would be stupid,\" said the Listener.\n\n\"You misspelled 'an attempt to trigger a naive intuition, and then abuse epistemology in order to prevent you from doing the further thinking that would undermine that naive intuition, which would be transparently untrustworthy if you were allowed to think about it instead of getting shut down with a cry of \"Empiricism!\"',\" said the Epistemologist.  \"But yes.\"\n\n### **iiv.**\n\n\"I am not satisfied,\" said the Scientist, when all that discussion had ended.  \"It seems to me that there ought to be more to say than this -- some longer story to tell -- about when it's wiser to tell a shorter story instead of a longer one, or wiser to attend more narrowly to the data naively generalized and less to longer arguments.\"\n\n\"Of course there's a longer story,\" said the Epistemologist.  \"There's always a longer story.  You can't let that paralyze you, or you'll end up never doing anything.  Of course there's an Art of when to trust more in less complicated reasoning -- an Art of when to pay attention to data more narrowly in a domain and less to inferences from generalizations on data from wider domains -- how could there not be an Art like that?  All I'm here to say to you today, is what that Art is not:  It is not for whoever has the shallowest form of reasoning on the narrowest dataset to cry 'Empiricism!' and 'Distrust complications!' and then automatically win.\"\n\n\"Then,\" said the Scientist.  \"What are we to do, then, when someone offers reasoning, and someone else says that the reasoning is too long -- or when one person offers a shallow generalization from narrowly relevant data, and another person wants to drag in data and generalizations and reasoning beyond that data?  If the answer isn't that the person with the most complicated reasoning is always right?  Because it can't be that either, I'm pretty sure.\"\n\n\"You talk it out on the object level,\" said the Epistemologist.  \"You debate out how the world probably is.  And you don't let anybody come forth with a claim that Epistemology means the conversation instantly ends in their favor.\"\n\n\"Wait, so your whole lesson is simply 'Shut up about epistemology'?\" said the Scientist.\n\n\"If only it were that easy!\" said the Epistemologist.  \"Most people don't even know when they're talking about epistemology, see?  That's why we need Epistemologists -- to notice when somebody has started trying to invoke epistemology, and tell them to shut up and get back to the object level.\"\n\n...\n\n\"Okay, I wasn't universally serious about that last part,\" amended the Epistemologist, after a moment's further thought.  \"There's sometimes a place for invoking explicit epistemology?  Like if two people sufficiently intelligent to reflect on explicit epistemology, are trying to figure out whether a particular argument step is allowed.  Then it could be helpful for the two of them to debate the epistemology underlying that local argument step, say...\"  The Epistemologist paused and thought again.  \"Though they would first need to have the concept of a local argument step, that's governed by rules.  Which concept they might obtain by reading my book on Highly Advanced Epistemology 101 For Beginners, or maybe just my essay on Local Validity as a Key to Sanity and Civilization, I guess?\"\n\n\"Huh,\" said the Scientist.  \"I'll consider taking a look over there, if epistemology ever threatens to darken my life again after this day.\"\n\nThe Epistemologist nodded agreeably.  \"And if you don't -- just remember this: it's quite rare for explicit epistemology to say about a local argument step, 'Do no thinking past this point.'\"\n\n\"What about the 'outside view'?\" shouted a Heckler.  \"Doesn't that show that people can benefit from being told to shut up and stop trying to think?\"\n\n\"I said rare not impossible,\" snapped the Epistemologist.  \"And harder than people think.  Only praise yourself as taking 'the outside view' if (1) there's only one defensible choice of reference class; and (2) the case you're estimating is as similar to cases in the class, as those cases similar to each other.  Like, in the classic experiment of estimating when you'll be done with holiday shopping, this year's task may not be exactly similar to any previous year's task, but it's no more dissimilar to them than they are from each other --\"\n\n\"Stories really do keep getting more complicated forever, don't they,\" said the Scientist.  \"At least stories about epistemology always seem to.\"\n\n\"I'd say that's more true of the human practices of epistemology than the underlying math, which does have an end,\" responded the Epistemologist.  \"But still, when it comes to any real-world conversation, there does come a point where it makes more sense to practice the Attitude of the Knife -- to cut off what is incomplete, and then say:  It is complete because it ended here.\"",
      "plaintextDescription": "(Crossposted by habryka after asking Eliezer whether I could post it under his account)\n\n\ni.\n\"Ignore all these elaborate, abstract, theoretical predictions,\" the Spokesperson for Ponzi Pyramid Incorporated said in a firm, reassuring tone.  \"Empirically, everyone who's invested in Bernie Bankman has received back 144% of what they invested two years later.\"\n\n\"That's not how 'empiricism' works,\" said the Epistemologist.  \"You're still making the assumption that --\"\n\n\"You could only believe that something different would happen in the future, if you believed in elaborate theoretical analyses of Bernie Bankman's unobservable internal motives and internal finances,\" said the spokesperson for Ponzi Pyramid Incorporated.  \"If you are a virtuous skeptic who doesn't trust in overcomplicated arguments, you'll believe that future investments will also pay back 144%, just like in the past.  That's the prediction you make if you predict based purely on empirical observations, instead of theories about a future nobody has seen!\"\n\n\"That's not how anything works,\" said the Epistemologist.  \"Every future prediction has a theory connecting it to our past observations.  There's no such thing as going from past observations directly to future predictions, with no theory, no assumptions, to cross the gap --\"\n\n\"Sure there's such a thing as a purely empirical prediction,\" said the Ponzi spokesperson.  \"I just made one.  Not to mention, my dear audience, are you really going to trust anything as complicated as epistemology?\"\n\n\"The alternative to thinking about epistemology is letting other people do your thinking about it for you,\" said the Epistemologist.  \"You're saying, 'If we observe proposition X \"past investors in the Ponzi Pyramid getting paid back 144% in two years\", that implies prediction Y \"this next set of investors in the Ponzi Pyramid will get paid back 144% in two years\"'.  X and Y are distinct propositions, so you must have some theory saying 'X -> Y' that lets you put in X",
      "wordCount": 7591
    },
    "tags": [
      {
        "_id": "EdRnMXBRbY5JDf5df",
        "name": "Epistemology",
        "slug": "epistemology"
      },
      {
        "_id": "cHoCqtfE9cF7aSs9d",
        "name": "Deception",
        "slug": "deception"
      },
      {
        "_id": "yXNtYNHJB54T3bGm3",
        "name": "Dialogue (format)",
        "slug": "dialogue-format"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "EzSH9698DhBsXAcYY",
    "title": "My current LK99 questions",
    "slug": "my-current-lk99-questions",
    "url": null,
    "baseScore": 206,
    "voteCount": 110,
    "viewCount": null,
    "commentCount": 38,
    "createdAt": null,
    "postedAt": "2023-08-01T22:48:00.733Z",
    "contents": {
      "markdown": "So this morning I thought to myself, \"Okay, now I will actually try to study the LK99 question, instead of betting based on nontechnical priors and market sentiment reckoning.\"  (My initial entry into the affray, having been driven by people online presenting as confidently YES when the prediction markets were not confidently YES.)  And then I thought to myself, \"This LK99 issue seems complicated enough that it'd be worth doing an actual Bayesian calculation on it\"--a rare thought; I don't think I've done an actual explicit numerical Bayesian update in at least a year.\n\nIn the process of trying to set up an explicit calculation, I realized I felt very unsure about some critically important quantities, to the point where it no longer seemed worth trying to do the calculation with numbers.  This is the System Working As Intended.\n\n* * *\n\nOn July 30th, Danielle Fong [said](https://twitter.com/DanielleFong/status/1685748984999202816) of [this temperature-current-voltage graph](https://twitter.com/marshray/status/1685525342008823808), \n\n> 'Normally as current increases, voltage drop across a material increases. in a \\*superconductor\\*, voltage stays nearly constant, 0. that appears to be what's happening here -- up to a critical current. with higher currents available at lower temperatures  deeply in the \"fraud or superconduct\" territory, imo. like you don't get this by accident -- you either faked it, or really found something.'\n\nThe graph Fong is talking about only appears in the initial paper put forth by Young-Wan Kwon, allegedly without authorization.  A different graph, though similar, appears in Fig. 6 on p. 12 of [the 6-author LK-endorsed paper rushed out in response](https://arxiv.org/abs/2307.12037).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/972a0b027c5cd94376434759f63650dde0f396fd69633876.png)\n\nIs it currently widely held by expert opinion, that this diagram has no obvious or likely explanation except \"superconductivity\" or \"fraud\"?  If the authors discovered something weird that wasn't a superconductor, or if they just hopefully measured over and over until they started getting some sort of measurement error, is there any known, any obvious way they could have gotten the same graph?\n\nOne person alleges an online rumor that poorly connected electrical leads can produce the same graph.  Is *that* a conventional view?\n\nAlternatively:  If this material *is* a superconductor, have we seen what we expected to see?  Is the diminishing current capacity with increased temperature usual?  How does this alleged direct measurement of superconductivity square up with the current-story-as-I-understood-it that the material is only being very poorly synthesized, probably only in granules or gaps, and hence only detectable by looking for magnetic resistance / pinning?\n\nThis is my number-one question.  Call it question 1-NO, because it's the question of \"How does the NO story explain this graph, and how prior-improbable or prior-likely was that story?\", with respect to my number one question.\n\nThough I'd also like to know the 1-YES details: whether this looks like a high-prior-probability superconductivity graph; or a graph that requires a new kind of superconductivity, but one that's theoretically straightforward given a central story; or if it looks like unspecified weird superconductivity, with there being no known theory that predicts a graph looking roughly like this.\n\n* * *\n\nWhat's up with all the partial levitation videos?  Possibilities I'm currently tracking:\n\n2-NO-A:  There's something called \"diamagnetism\" which exists in other materials.  The videos by LK and attempted replicators show the putative superconductor being repelled from the magnet, but not being locked in space relative to the magnet.  Superconductors are supposed to exhibit Meissner pinning, and the failure of the material to be pinned to the magnet indicates that this isn't a superconductor.  (Sabine Hossenfelder seems to talk this way [here](https://youtu.be/RjzL9cS3VW8?si=WVvDW0coSuCZ0i0a&t=226).  \"I lost hope when I saw this video; this doesn't look like the Meissner effect to me.\")\n\n2-YES-Z:  This is actually some totally expected thing called a \"toroidal Meissner effect\", which is exactly what we should expect to see given the one-dimensional nature of the superconducting arrangement of quantum wells, and supposedly looks just like some previous video for carbon nanotubes.  This was [said by a Russian catgirl](https://twitter.com/iris_IGB/status/1686397350284079105).\n\n2-YES-Y:  There is *so much* diamagnetism on display here that [you wouldn't see it without some superconductivity](https://twitter.com/ChristianMahler/status/1686484715606351872?s=20).\n\n2-NO-B:  There's an unusual/unprecedented amount of diamagnetism here, but in a way that fits pretty well with \"they found some magnetically weird material after screening for magnetic weirdness\" better than \"superconductivity\".\n\n* * *\n\nHow much of a surprise was [Sinead's flat-band calculation](https://twitter.com/sineatrix/status/1686182852667572224?s=20)?  On the NO story, how likely or unlikely was the logical observation, \"a result like this is calculated, for an actually-non-superconducting material, that was prescreened by the filters we already know about\"?\n\nPossibilities that are obvious and/or that I've seen claimed online:\n\n3-NO-A:  LK were deliberately seeking out materials like this, and the materials they screened were selected to all be the sort that would have a calculated flat band; therefore, arguendo, the result is not too surprising on NO.\n\n3-NO-B:  Most materials in the potential-superconductor class would have a calculation like this; it's not a hard result to get by making weird unrealistic assumptions.\n\n3-NO-C:  LK found a weird material that isn't a superconductor, and a weird material like this is much more likely to have a result like Sinead's calculation be true about that material or some plausibly mistaken neighboring postulated structure.\n\n3-NO-D:  We understand so little about the origins of superconductivity that a calculation like this is *not even probabilistic evidence* about whether something is actually a superconductor.  You could probably find a calculation like that for anything magnetically or electrically weird at all.\n\n3-YES-Z:  This is a fact we didn't know from LK's paper and LK didn't do any prior selection against it; Sinead's result is a previously unsuspected-given-NO calculation, that would be very unlikely as a calculation result unless the LK-postulated material was actually a superconductor.  (This is how [Andercot initially presented Sinead's result](https://twitter.com/Andercot/status/1686215574177841152), and there was a large prediction market jump immediately after; based on the subsequent crash, it looks like this was *not* the conventional reception after some review.)\n\n3-YES-Y:  LK already knew this fact, already did some calculation reflecting it, and this is how they found the material in the first place.\n\n3-YES-X:  LK didn't know the Sinead-calculation, but the already-known weirdnesses of the material, and a very high false positive rate of calculations like these, mean the calculation wasn't all that informative about YES and we don't say \"game won\".\n\n* * *\n\nQ4:  Do any other results from the 6-person or journal-submitted LK papers stand out as having the property, \"This is either superconductivity or fraud?\"  Like the stuff with thermal capacity or whatevs?  Are there any *claims* \\- not replicated observations, just claims - that somebody wants to point to and say:  \"This seems quite improbable to get as an honest mistake, or as the result of other weirdness from this sort of material, absent superconductivity\"?\n\n* * *\n\nQ5:  When the early results hit, many online skeptics were scoffing about improbable or inconsistent results or points which supposedly showed the authors didn't know physics.  Are any standout nits like that still being picked with the 6-person or journal-submitted LK papers?\n\n* * *\n\nAnd finally, while it's probably not *that* important, I would like to ask \"What are we pretty sure we know and how do we think we know it?\" with respect to the dramatic social story.\n\nSupposedly, Lee and Kim have been researching LK-99 since 1999 as the last wish of their dying professor, albeit with a lot of breaks because they couldn't get any funding and had to go on living their lives.\n\nQ6:  Is there any external verification, if we don't trust LK's word alone, that they've actually been working on this project for that long, and were on the tail of materials in this category from the start?\n\n* * *\n\nYou may notice that I'm not trying to point to anything as a \"prior\", as many layfolk incorrectly imagine to be the essence of Bayesianism.  I don't particularly think that pointing to any particular starting point and calling it a prior would be helpful in figuring things out, at this point.\n\nWhat Bayesianism really has to say is more of a coherence constraint on how your beliefs should look; a reminder to ask questions like:  \"What's the best explanation for Fig. 6 given NO, and how a-priori-probable would that explanation have sounded before we saw the graph?\"\n\nBayesianism, or rather probability-theoretic coherence, inspires me to say that what I'd hope to see laid out by experts or collaborating Internet citizens, is a list of...\n\n(1)  Every observation, claim, fact, diagram from a paper, etcetera, *which is supposed to be confusing or surprising given some possible states of the world,* or bear in any way as evidence upon our probable beliefs;\n\n(2)  An account of the best ways to explain each such point, from both a YES or NO standpoint, as somebody who honestly believed YES or NO would give their own best account of it.\n\n...in enough detail that you could notice if any parts of the best combined YES story or NO story stood out as a priori improbable, themselves surprising, or incoherent between points.\n\nPossibly if we had that whole list, it would still seem worth the trouble of trying to yank out numbers, and attempt an explicit calculation of how probable the best YES story and the best NO story seemed.  Or possibly it would seem like, as so many people on the Internet claim--and the more forthright ones have [bet](https://manifold.markets/QuantumObserver/will-the-lk99-room-temp-ambient-pre?r=RWxpZXplcll1ZGtvd3NreQ)--that it's time to call it, in one direction or another.",
      "plaintextDescription": "So this morning I thought to myself, \"Okay, now I will actually try to study the LK99 question, instead of betting based on nontechnical priors and market sentiment reckoning.\"  (My initial entry into the affray, having been driven by people online presenting as confidently YES when the prediction markets were not confidently YES.)  And then I thought to myself, \"This LK99 issue seems complicated enough that it'd be worth doing an actual Bayesian calculation on it\"--a rare thought; I don't think I've done an actual explicit numerical Bayesian update in at least a year.\n\nIn the process of trying to set up an explicit calculation, I realized I felt very unsure about some critically important quantities, to the point where it no longer seemed worth trying to do the calculation with numbers.  This is the System Working As Intended.\n\n----------------------------------------\n\nOn July 30th, Danielle Fong said of this temperature-current-voltage graph, \n\n> 'Normally as current increases, voltage drop across a material increases. in a *superconductor*, voltage stays nearly constant, 0. that appears to be what's happening here -- up to a critical current. with higher currents available at lower temperatures  deeply in the \"fraud or superconduct\" territory, imo. like you don't get this by accident -- you either faked it, or really found something.'\n\nThe graph Fong is talking about only appears in the initial paper put forth by Young-Wan Kwon, allegedly without authorization.  A different graph, though similar, appears in Fig. 6 on p. 12 of the 6-author LK-endorsed paper rushed out in response.\n\nIs it currently widely held by expert opinion, that this diagram has no obvious or likely explanation except \"superconductivity\" or \"fraud\"?  If the authors discovered something weird that wasn't a superconductor, or if they just hopefully measured over and over until they started getting some sort of measurement error, is there any known, any obvious way they could have gotten the same",
      "wordCount": 1571
    },
    "tags": [
      {
        "_id": "4fxcbJ8xSv4SAYkkx",
        "name": "Bayesianism",
        "slug": "bayesianism"
      },
      {
        "_id": "csMv9MvvjYJyeHqoo",
        "name": "Physics",
        "slug": "physics"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb1a2",
        "name": "Science",
        "slug": "science"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nH4c3Q9t9F3nJ7y8W",
    "title": "GPTs are Predictors, not Imitators",
    "slug": "gpts-are-predictors-not-imitators",
    "url": null,
    "baseScore": 418,
    "voteCount": 258,
    "viewCount": null,
    "commentCount": 100,
    "createdAt": null,
    "postedAt": "2023-04-08T19:59:13.601Z",
    "contents": {
      "markdown": "(Related text posted to [Twitter](https://twitter.com/ESYudkowsky/status/1644760694771048449); this version is edited and has a more advanced final section.)\n\nImagine yourself in a box, trying to predict the next word - assign as much probability mass to the next token as possible - for all the text on the Internet.\n\nKoan:  Is this a task whose difficulty caps out as human intelligence, or at the intelligence level of the smartest human who wrote any Internet text?  What factors make that task easier, or harder?  (If you don't have an answer, maybe take a minute to generate one, or alternatively, try to predict what I'll say next; if you do have an answer, take a moment to review it inside your mind, or maybe say the words out loud.)\n\n* * *\n\nConsider that somewhere on the internet is probably a list of thruples: <product of 2 prime numbers, first prime, second prime>.\n\nGPT obviously isn't going to predict that successfully for significantly-sized primes, but it illustrates the basic point:\n\nThere is no law saying that a predictor only needs to be as intelligent as the generator, in order to predict the generator's next token.\n\nIndeed, in general, you've got to be more intelligent to predict particular X, than to generate realistic X.  GPTs are being trained to a much harder task than GANs.\n\nSame spirit: <Hash, plaintext> pairs, which you can't *predict* without cracking the hash algorithm, but which you could far more easily *generate typical instances of* if you were trying to pass a GAN's discriminator about it (assuming a discriminator that had learned to compute hash functions).\n\n* * *\n\nConsider that some of the text on the Internet isn't humans casually chatting. It's the results section of a science paper. It's news stories that say what happened on a particular day, where maybe no human would be smart enough to predict the next thing that happened in the news story in advance of it happening.\n\nAs Ilya Sutskever compactly put it, to learn to predict text, is to learn to predict the causal processes of which the text is a shadow.\n\nLots of what's shadowed on the Internet has a \\*complicated\\* causal process generating it.\n\n* * *\n\nConsider that sometimes human beings, in the course of talking, make errors.\n\nGPTs are not being trained to imitate human error. They're being trained to \\*predict\\* human error.\n\nConsider the asymmetry between you, who makes an error, and an outside mind that knows you well enough and in enough detail to predict \\*which\\* errors you'll make.\n\nIf you then ask that predictor to become an actress and play the character of you, the actress will guess which errors you'll make, and play those errors.  If the actress guesses correctly, it doesn't mean the actress is just as error-prone as you.\n\n* * *\n\nConsider that a lot of the text on the Internet isn't extemporaneous speech. It's text that people crafted over hours or days.\n\nGPT-4 is being asked to predict it in 200 serial steps or however many layers it's got, just like if a human was extemporizing their immediate thoughts.\n\nA human can write a rap battle in an hour.  A GPT loss function would like the GPT to be intelligent enough to predict it on the fly.\n\n* * *\n\nOr maybe simplest:\n\nImagine somebody telling you to make up random words, and you say, \"Morvelkainen bloombla ringa mongo.\"\n\nImagine a mind of a level - where, to be clear, I'm not saying GPTs are at this level yet -\n\nImagine a Mind of a level where it can hear you say 'morvelkainen blaambla ringa', and maybe also read your entire social media history, and then manage to assign 20% probability that your next utterance is 'mongo'.\n\nThe fact that this Mind could double as a really good actor playing your character, does not mean They are only exactly as smart as you.\n\nWhen *you're* trying to be human-equivalent at writing text, you can just make up whatever output, and it's now a human output because you're human and you chose to output that.\n\nGPT-4 is being asked to *predict* all that stuff you're making up. It doesn't get to make up whatever. It is being asked to model what you were thinking - the thoughts in your mind whose shadow is your text output - so as to assign as much probability as possible to your true next word.\n\n* * *\n\nFiguring out that your next utterance is 'mongo' is not mostly a question, I'd guess, of that mighty Mind being hammered into the shape of a thing that can simulate arbitrary humans, and then some less intelligent subprocess being responsible for adapting the shape of that Mind to be you exactly, after which it simulates you saying 'mongo'.  Figuring out *exactly who's talking*, to that degree, is a *hard inference problem* which seems like noticeably harder mental work than the part where you just say 'mongo'.\n\nWhen you predict how to chip a flint handaxe, you are not mostly a causal process that behaves like a flint handaxe, plus some computationally weaker thing that figures out which flint handaxe to be.  It's not a problem that is best solved by \"have the difficult ability to be like any particular flint handaxe, and then easily figure out which flint handaxe to be\".\n\n* * *\n\nGPT-4 is still not as smart as a human in many ways, but it's naked mathematical truth that the task GPTs are being trained on is *harder* than being an actual human.\n\nAnd since the task that GPTs are being trained on is different from and harder than the task of being a human, it would be surprising - even leaving aside all the ways that gradient descent differs from natural selection - if GPTs ended up thinking the way humans do, in order to solve that problem.\n\nGPTs are not Imitators, nor Simulators, but Predictors.",
      "plaintextDescription": "(Related text posted to Twitter; this version is edited and has a more advanced final section.)\n\nImagine yourself in a box, trying to predict the next word - assign as much probability mass to the next token as possible - for all the text on the Internet.\n\nKoan:  Is this a task whose difficulty caps out as human intelligence, or at the intelligence level of the smartest human who wrote any Internet text?  What factors make that task easier, or harder?  (If you don't have an answer, maybe take a minute to generate one, or alternatively, try to predict what I'll say next; if you do have an answer, take a moment to review it inside your mind, or maybe say the words out loud.)\n\n----------------------------------------\n\nConsider that somewhere on the internet is probably a list of thruples: <product of 2 prime numbers, first prime, second prime>.\n\nGPT obviously isn't going to predict that successfully for significantly-sized primes, but it illustrates the basic point:\n\nThere is no law saying that a predictor only needs to be as intelligent as the generator, in order to predict the generator's next token.\n\nIndeed, in general, you've got to be more intelligent to predict particular X, than to generate realistic X.  GPTs are being trained to a much harder task than GANs.\n\nSame spirit: <Hash, plaintext> pairs, which you can't predict without cracking the hash algorithm, but which you could far more easily generate typical instances of if you were trying to pass a GAN's discriminator about it (assuming a discriminator that had learned to compute hash functions).\n\n----------------------------------------\n\nConsider that some of the text on the Internet isn't humans casually chatting. It's the results section of a science paper. It's news stories that say what happened on a particular day, where maybe no human would be smart enough to predict the next thing that happened in the news story in advance of it happening.\n\nAs Ilya Sutskever compactly put it, to learn to predict text, ",
      "wordCount": 994
    },
    "tags": [
      {
        "_id": "YWzByWvtXunfrBu5b",
        "name": "GPT",
        "slug": "gpt"
      },
      {
        "_id": "xjNvvmvQ5BH3cfEBr",
        "name": "Simulator Theory",
        "slug": "simulator-theory"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oM9pEezyCb4dCsuKq",
    "title": "Pausing AI Developments Isn't Enough. We Need to Shut it All Down",
    "slug": "pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1",
    "url": null,
    "baseScore": 275,
    "voteCount": 151,
    "viewCount": null,
    "commentCount": 44,
    "createdAt": null,
    "postedAt": "2023-04-08T00:36:47.702Z",
    "contents": {
      "markdown": "(*Published in *[*TIME*](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) *on March 29.*)\n\nAn [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) published today calls for “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”\n\nThis 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It’s an improvement on the margin.\n\nI refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.\n\nThe key issue is not “human-competitive” intelligence (as the open letter puts it); it’s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can’t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing.\n\nMany researchers steeped in these [issues](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), including myself, [expect](https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results) that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in “maybe possibly some remote chance,” but as in “that is the obvious thing that would happen.” It’s not that you can’t, in principle, survive creating something much smarter than you; it’s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers.\n\nWithout that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that *could in principle* be imbued into an AI but *we are not ready *and *do not currently know how.*\n\nAbsent that caring, we get “the AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.”\n\nThe likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include “a 10-year-old trying to play chess against Stockfish 15”, “the 11th century trying to fight the 21st century,” and “*Australopithecus* trying to fight *Homo sapiens*“.\n\nTo visualize a hostile superhuman AI, don’t imagine a lifeless book-smart thinker dwelling inside the internet and sending ill-intentioned emails. Visualize an entire alien civilization, thinking at millions of times human speeds, initially confined to computers—in a world of creatures that are, from its perspective, very stupid and very slow. A sufficiently intelligent AI won’t stay confined to computers for long. In today’s world you can email DNA strings to laboratories that will produce proteins on demand, allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing.\n\nIf somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter.\n\nThere’s no *proposed plan *for how we could do any such thing and survive. OpenAI’s openly declared [intention](https://openai.com/blog/our-approach-to-alignment-research) is to make some future AI do our AI alignment homework. Just hearing that *this is the plan* ought to be enough to get any sensible person to panic. The other leading AI lab, DeepMind, has no plan at all.\n\nAn aside: None of this danger depends on whether or not AIs are or can be conscious; it’s intrinsic to the notion of powerful cognitive systems that optimize hard and calculate outputs that meet sufficiently complicated outcome criteria. With that said, I’d be remiss in my moral duties as a human if I didn’t also mention that we have no idea how to determine whether AI systems are aware of themselves—since we have no idea how to decode anything that goes on in the giant inscrutable arrays—and therefore we may at some point inadvertently create digital minds which are truly conscious and ought to have rights and shouldn’t be owned.\n\nThe rule that most people aware of these issues would have endorsed 50 years earlier, was that if an AI system can speak fluently and says it’s self-aware and demands human rights, that ought to be a hard stop on people just casually owning that AI and using it past that point. We already blew past that old line in the sand. And that was probably *correct*; I *agree *that current AIs are probably just imitating talk of self-awareness from their training data. But I mark that, with how little insight we have into these systems’ internals, we *do not actually know.*\n\nIf that’s our state of ignorance for GPT-4, and GPT-5 is the same size of giant capability step as from GPT-3 to GPT-4, I think we’ll no longer be able to justifiably say “probably not self-aware” if we let people make GPT-5s. It’ll just be “I don’t know; nobody knows.” If you can’t be sure whether you’re creating a self-aware AI, this is alarming not just because of the moral implications of the “self-aware” part, but because being unsure means you have no idea what you are doing and that is dangerous and you should stop.\n\n* * *\n\nOn Feb. 7, Satya Nadella, CEO of Microsoft, [publicly gloated](https://www.theverge.com/23589994/microsoft-ceo-satya-nadella-bing-chatgpt-google-search-ai) that the new Bing would make Google “come out and show that they can dance.” “I want people to know that we made them dance,” he said.\n\nThis is not how the CEO of Microsoft talks in a sane world. It shows an overwhelming gap between how seriously we are taking the problem, and how seriously we needed to take the problem starting 30 years ago.\n\nWe are not going to bridge that gap in six months.\n\nIt took more than 60 years between when the notion of Artificial Intelligence was first proposed and studied, and for us to reach today’s capabilities. Solving *safety* of superhuman intelligence—not perfect safety, safety in the sense of “not killing literally everyone”—could very reasonably take at least half that long. And the thing about trying this with superhuman intelligence is that if you get that wrong on the first try, you do not get to learn from your mistakes, because you are dead. Humanity does not learn from the mistake and dust itself off and try again, as in other challenges we’ve overcome in our history, because we are all gone.\n\nTrying to get *anything *right on the first really critical try is an extraordinary ask, in science and in engineering. We are not coming in with anything like the approach that would be required to do it successfully. If we held anything in the nascent field of Artificial General Intelligence to the lesser standards of engineering rigor that apply to a bridge meant to carry a couple of thousand cars, the entire field would be shut down tomorrow.\n\nWe are not prepared. We are not on course to be prepared in any reasonable time window. There is no plan. Progress in AI capabilities is running vastly, vastly ahead of progress in AI alignment or even progress in understanding what the hell is going on inside those systems. If we actually do this, we are all going to die.\n\nMany researchers working on these systems think that we’re plunging toward a catastrophe, with more of them daring to say it in private than in public; but they think that they can’t unilaterally stop the forward plunge, that others will go on even if they personally quit their jobs. And so they all think they might as well keep going. This is a stupid state of affairs, and an undignified way for Earth to die, and the rest of humanity ought to step in at this point and help the industry solve its collective action problem.\n\n* * *\n\nSome of my friends have recently reported to me that when people outside the AI industry hear about extinction risk from Artificial General Intelligence for the first time, their reaction is “maybe we should not build AGI, then.”\n\nHearing this gave me a tiny flash of hope, because it’s a simpler, more sensible, and frankly saner reaction than I’ve been hearing over the last 20 years of trying to get anyone in the industry to take things seriously. Anyone talking that sanely deserves to hear how bad the situation actually is, and not be told that a six-month moratorium is going to fix it.\n\nOn March 16, my partner sent me this email. (She later gave me permission to excerpt it here.)\n\n“Nina lost a tooth! In the usual way that children do, not out of carelessness! Seeing GPT4 blow away those standardized tests on the same day that Nina hit a childhood milestone brought an emotional surge that swept me off my feet for a minute. It’s all going too fast. I worry that sharing this will heighten your own grief, but I’d rather be known to you than for each of us to suffer alone.”\n\nWhen the insider conversation is about the grief of seeing your daughter lose her first tooth, and thinking she’s not going to get a chance to grow up, I believe we are past the point of playing political chess about a six-month moratorium.\n\nIf there was a plan for Earth to survive, if only we passed a six-month moratorium, I would back that plan. There isn’t any such plan.\n\nHere’s what would actually need to be done:\n\nThe moratorium on new large training runs needs to be indefinite and worldwide. There can be no exceptions, including for governments or militaries. If the policy starts with the U.S., then China needs to see that the U.S. is not seeking an advantage but rather trying to prevent a horrifically dangerous technology which can have no true owner and which will kill everyone in the U.S. and in China and on Earth. If I had infinite freedom to write laws, I might carve out a single exception for AIs being trained solely to solve problems in biology and biotechnology, not trained on text from the internet, and not to the level where they start talking or planning; but if that was remotely complicating the issue I would immediately jettison that proposal and say to just shut it all down.\n\nShut down all the large GPU clusters (the large computer farms where the most powerful AIs are refined). Shut down all the large training runs. Put a ceiling on how much computing power anyone is allowed to use in training an AI system, and move it downward over the coming years to compensate for more efficient training algorithms. No exceptions for governments and militaries. Make immediate multinational agreements to prevent the prohibited activities from moving elsewhere. Track all GPUs sold. If intelligence says that a country outside the agreement is building a GPU cluster, be less scared of a shooting conflict between nations than of the moratorium being violated; be willing to destroy a rogue datacenter by airstrike.\n\nFrame nothing as a conflict between national interests, have it clear that anyone talking of arms races is a fool. That we all live or die as one, in this, is not a policy but a fact of nature. Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange, and that allied nuclear countries are willing to run some risk of nuclear exchange if that’s what it takes to reduce the risk of large AI training runs.\n\nThat’s the kind of policy change that would cause my partner and I to hold each other, and say to each other that a miracle happened, and now there’s a chance that maybe Nina will live. The sane people hearing about this for the first time and sensibly saying “maybe we should not” deserve to hear, honestly, what it would take to have that happen. And when your policy ask is that large, the only way it goes through is if policymakers realize that if they conduct business as usual, and do what’s politically easy, that means their own kids are going to die too.\n\nShut it all down.\n\nWe are not ready. We are not on track to be significantly readier in the foreseeable future. If we go ahead on this everyone will die, including children who did not choose this and did not do anything wrong.\n\nShut it down.\n\n* * *\n\n **Addendum, March 30: **\n\nThe great political writers who also aspired to be good human beings, from George Orwell on the left to Robert Heinlein on the right, taught me to acknowledge in my writing that politics rests on force.\n\nGeorge Orwell considered it a [tactic of totalitarianism](https://www.lesswrong.com/posts/Lz64L3yJEtYGkzMzu/rationality-and-the-english-language), that bullet-riddled bodies and mass graves were often described in vague euphemisms; that in this way brutal policies gained public support without their prices being justified, by hiding those prices.\n\nRobert Heinlein thought it beneath a citizen's dignity to pretend that, if they bore no gun, they were morally superior to the police officers and soldiers who bore guns to defend their law and their peace; Heinlein, both metaphorically and literally, thought that if you eat meat—and he was not a vegetarian—you ought to be willing to visit a farm and try personally slaughtering a chicken.\n\nWhen you pass a law, it means that people who defy the law go to jail; and if they try to escape jail they'll be shot.  When you advocate an international treaty, if you want that treaty to be effective, it may mean sanctions that will starve families, or a shooting war that kills people outright.\n\nTo threaten these things, but end up not having to do them, is not very morally distinct—*I* would say—from doing them.  I admit this puts me more on the Heinlein than on the Orwell side of things.  Orwell, I think, probably considers it very morally different if you have a society with a tax system and most people pay the taxes and very few actually go to jail.  Orwell is more sensitive to the count of actual dead bodies—or people impoverished by taxation or regulation, where Orwell acknowledges and cares when that *actually* happens.  Orwell, I think, has a point.  But I also think Heinlein has a point.  I claim that makes me a centrist.\n\nEither way, neither Heinlein nor Orwell thought that laws and treaties and *wars* were never worth it.  They just wanted us to be honest about the cost.\n\nEvery person who pretends to be a libertarian—I cannot see them even pretending to be liberals—who quoted my call for law and treaty as a call for \"violence\", because I was frank in writing about the cost, ought to be ashamed of themselves for punishing compliance with Orwell and Heinlein's rule.\n\nYou can argue that the treaty and law I proposed is not worth its cost in force; my being frank about that cost is intended to help *honest* arguers make that counterargument.\n\nTo pretend that calling for treaty and law is VIOLENCE!! is hysteria.  It doesn't just punish compliance with the Heinlein/Orwell protocol, it plays into the widespread depiction of libertarians as hysterical.  (To be clear, a lot of libertarians—and socialists, and centrists, and whoever—are in fact hysterical, especially on Twitter.)  It may even encourage actual terrorism.\n\nBut is it *not* \"violence\", if in the end you need guns and airstrikes to enforce the law and treaty?  And here I answer: there's an *actually* important distinction between lawful force and unlawful force, which is not always of itself the distinction between Right and Wrong, but which is a real and important distinction.  The common and ordinary usage of the word \"violence\" often points to that distinction.  When somebody says \"I do not endorse the use of violence\" they do not, in common usage and common sense, mean, \"I don't think people should be allowed to punch a mugger attacking them\" or even \"Ban all taxation.\"\n\nWhich, again, is not to say that all lawful force is good and all unlawful force is bad.  You can make a case for John Brown (of John Brown's Body).\n\nBut in fact I don't endorse shooting somebody on a city council who's enforcing NIMBY regulations.\n\nI think NIMBY laws are wrong.  I think it's important to admit that law is ultimately backed by force.\n\nBut lawful force.  And yes, that matters.  That's why it's harmful to society if you shoot the city councilor—\n\n—and a *misuse of language* if the shooter then says, \"They were being violent!\"\n\n* * *\n\n**Addendum, March 31: **\n\nSometimes—even when you say something whose intended reading is immediately obvious to any reader who hasn't seen it before—it's possible to tell people to see something in writing that isn't there, and then they see it.\n\nMy TIME piece did not suggest nuclear strikes against countries that refuse to sign on to a global agreement against large AI training runs.  It said that, if a non-signatory country is building a datacenter that might kill everyone on Earth, you should be willing to preemptively destroy that datacenter; the intended reading is that you should do this *even if* the non-signatory country is a nuclear power and *even if* they try to threaten nuclear retaliation for the strike.  This is what is meant by \"Make it explicit... that allied nuclear countries are willing to run some risk of nuclear exchange if that’s what it takes to reduce the risk of large AI training runs.\"\n\nI'd hope that would be clear from any plain reading, if you haven't previously been lied-to about what it says.  It does not say, \"Be willing to *use* nuclear weapons\" to reduce the risk of training runs.  It says, \"Be willing to run some risk of nuclear exchange\" \\[initiated by the other country\\] to reduce the risk of training runs.\n\nThe taboo against first use of nuclear weapons continues to make sense to me.  I don't see why we'd need to throw that away in the course of adding \"first use of GPU farms\" to the forbidden list.\n\nI further note:  Among the reasons to spell this all out, is that it's important to be explicit, in advance, about things that will cause your own country / allied countries to use military force.  Lack of clarity about this is how World War I *and* World War II both started.\n\nIf (say) the UK, USA, and China come to believe that large GPU runs run some risk of utterly annihilating their own populations and all of humanity, they would not deem it in their own interests to allow Russia to proceed with building a large GPU farm *even if* it were a true and certain fact that Russia would retaliate with nuclear weapons to the destruction of that GPU farm.  In this case—unless I'm really missing something about how this game is and ought to be played—you really want all the Allied countries to make it very clear, well in advance, that this is what they believe and this is how they will act.  This would be true even in a world where it was, in reality, factually false that the large GPU farm ran a risk of destroying humanity.  It would still be extremely important that the Allies be very explicit about what they believed and how they'd act as a result.  You would not want Russia believing that the Allies would back down from destroying the GPU farm given a credible commitment by Russia to nuke in reply to any conventional attack, and the Allies in fact believing that the danger to humanity meant they had to airstrike the GPU farm anyways.\n\nSo if I'd meant \"Be willing to employ first use of nuclear weapons against a country for refusing to sign the agreement,\" or even \"Use nukes to destroy rogue datacenters, instead of conventional weapons, for some unimaginable reason,\" I'd have said that, in words, very clearly, because you do not want to be vague about that sort of thing.\n\nIt is not what I meant, and there'd be no reason to say it, and the TIME piece plainly does not say it; and if somebody else told you I said that, update how much you trust them about anything else either.\n\nSo long as I'm clarifying things:  I do not dispute those critics who have noted that most international agreements, eg nuclear non-proliferation, bind only their signatories.  I agree that an alliance which declares its intent to strike a non-signatory country for dangerous behavior is extraordinary; though precedents would include Israel's airstrike on Iraq's unfinished Osirak reactor in 1981 (without which Iraq might well have possessed nuclear weapons at the time it invaded Kuwait—the later US misbehavior around Iraq does not change this earlier historical point).\n\nMy TIME piece does not say, \"Hey, this problem ought to be solvable by totally conventional normal means, let's go use conventional treaties and diplomacy to solve it.\"  It says, \"If anyone anywhere builds a sufficiently powerful AI, under anything remotely like present conditions, everyone will die.  Here is what we'd have to do to prevent that.\"\n\nAnd no, I do not expect that policy proposal to be adopted, in real life, now that we've come to this.  I spent the last twenty years trying to have there be options that were Not This, not because I dislike this ultimate last resort... though it *is* horrible... but because I don't expect we actually have that resort.  This is not what I expect to happen, now that we've been reduced to this last resort.  I expect that we all die.  That is why I tried so hard to have things not end up here.\n\nBut if one day a lot of people woke up and decided that they didn't want to die, it seems to me that this is something extraordinary that a coalition of nuclear countries could decide to do, and maybe we wouldn't die.\n\nIf *all* the countries on Earth had to voluntarily sign on, it would not be an imaginable or viable plan even then; there's extraordinary, and then there's impossible.  Which is why I tried to spell out that, if the allied countries were willing to behave in the extraordinary way of \"be willing to airstrike a GPU farm built by a non-signatory country\" and \"be willing to run a risk of nuclear retaliation from a nuclear non-signatory country\", maybe those allied countries could decide to just-not-die *even if* Russia refused to be part of the coalition.",
      "plaintextDescription": "(Published in TIME on March 29.)\n\n \n\nAn open letter published today calls for “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”\n\nThis 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It’s an improvement on the margin.\n\nI refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.\n\nThe key issue is not “human-competitive” intelligence (as the open letter puts it); it’s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can’t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing.\n\nMany researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in “maybe possibly some remote chance,” but as in “that is the obvious thing that would happen.” It’s not that you can’t, in principle, survive creating something much smarter than you; it’s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers.\n\nWithout that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that could in principle be imbued into an AI but we are not ready and do not currently know how.\n\nAbsent that caring, we get “the AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.”\n\nThe likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include “a 10-year-old",
      "wordCount": 3742
    },
    "tags": [
      {
        "_id": "AqwjXSSy7DuF2pKdm",
        "name": "Slowing Down AI",
        "slug": "slowing-down-ai-1"
      },
      {
        "_id": "sioN8jTw4MSgf8MpL",
        "name": "AI Development Pause",
        "slug": "ai-development-pause"
      },
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "KYuR2HcWPEmXZqMZs",
    "title": "Eliezer Yudkowsky's Shortform",
    "slug": "eliezer-yudkowsky-s-shortform",
    "url": null,
    "baseScore": 14,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-04-01T22:43:50.929Z",
    "contents": null,
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uNepkB5EqETC8b9C2",
    "title": "Manifold:  If okay AGI, why?",
    "slug": "manifold-if-okay-agi-why",
    "url": "https://manifold.markets/EliezerYudkowsky/if-artificial-general-intelligence?r=RWxpZXplcll1ZGtvd3NreQ",
    "baseScore": 120,
    "voteCount": 58,
    "viewCount": null,
    "commentCount": 37,
    "createdAt": null,
    "postedAt": "2023-03-25T22:43:53.820Z",
    "contents": {
      "markdown": "Arguably the most important topic about which a prediction market has yet been run:  Conditional on an okay outcome with AGI, how did that happen?",
      "plaintextDescription": "Arguably the most important topic about which a prediction market has yet been run:  Conditional on an okay outcome with AGI, how did that happen?\n\n",
      "wordCount": 25
    },
    "tags": [
      {
        "_id": "33BrBRSrRQS4jEHdk",
        "name": "Forecasts (Specific Predictions)",
        "slug": "forecasts-specific-predictions"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "rwkkcgSpnAyE8oNo3",
    "title": "Alexander and Yudkowsky on AGI goals",
    "slug": "alexander-and-yudkowsky-on-agi-goals",
    "url": null,
    "baseScore": 179,
    "voteCount": 86,
    "viewCount": null,
    "commentCount": 53,
    "createdAt": null,
    "postedAt": "2023-01-24T21:09:16.938Z",
    "contents": {
      "markdown": "This is a lightly edited transcript of a chatroom conversation between Scott Alexander and Eliezer Yudkowsky last year, following up on the [Late 2021 MIRI Conversations](https://www.lesswrong.com/s/n945eovrA3oDueqtq). Questions discussed include \"How hard is it to get the right goals into AGI systems?\" and \"In what contexts do AI systems exhibit 'consequentialism'?\".\n\n1\\. Analogies to human moral development\n----------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:29]</strong><i>&nbsp;<strong>&nbsp;</strong></i></p><p>@ScottAlexander ready when you are</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:31]</strong><i> &nbsp;</i></p><p>Okay, how do you want to do this?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:32]</strong><i> &nbsp;</i></p><p>If you have an agenda of Things To Ask, you can follow it; otherwise I can start by posing a probing question or you can?</p><p>We've been very much winging it on these and that has worked... as well as you have seen it working!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:34]</strong><i> &nbsp;</i></p><p>Okay. I'll post from my agenda. I'm assuming we both have the right to edit logs before releasing them? I have one question where I ask about a specific party where your real answer might offend some people it's bad to offend - if that happens, maybe we just have that discussion and then decide if we want to include it later?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:34]</strong> &nbsp;<i> &nbsp;</i></p><p>Yup, both parties have rights to edit before releasing.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:34]</strong>&nbsp;<i>&nbsp;</i></p><p>Okay.</p><p>One story that psychologists tell goes something like this: a child does something socially proscribed (eg steal). Their parents punish them. They learn some combination of \"don't steal\" and \"don't get caught stealing\". A few people (eg sociopaths) learn only \"don't get caught stealing\", but most of the rest of us get at least some genuine aversion to stealing that eventually generalizes into a real sense of ethics. If a sociopath got absolute power, they would probably steal all the time. But there are at least a few people whose ethics would successfully restrain them.</p><p>I interpret a major strain in your thought as being that we're going to train fledgling AIs to do things like not steal, and they're going to learn not to get caught stealing by anyone who can punish them. Then, once they're superintelligent and have absolute power, they'll reveal that it was all a lie, and steal whenever they want. Is this worry at the level of \"we can't be sure they won't do this\"? Or do you think it's overwhelmingly likely? If the latter, what makes you think AIs won't internalize ethical prohibitions, even though most children do? Is it that evolution has given us priors to interpret reward/punishment in a moralistic and internalized way, and entities without those priors will naturally interpret them in a superficial way? Do we understand what those priors \"look like\"? Is finding out what features of mind design and training data cause internalization vs. superficial compliance a potential avenue for AI alignment?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:36]</strong> &nbsp;<i>&nbsp;</i></p><p>Several layers here! &nbsp;The basic gloss on this is \"Yes, everything that you've named goes wrong simultaneously plus several other things. &nbsp;If I'm wrong and one or even three of those things go exactly like they do in neurotypical human children instead, this will not be enough to save us.\"</p><p>If AI is built on anything like the present paradigm, or on future paradigms either really, you can't map that onto the complicated particular mechanisms that get invoked by raising a human child, and expect the same result.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:37]</strong> &nbsp;<i>&nbsp;</i></p><p>(give me some sign when you're done answering)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:37]</strong>&nbsp;<i>&nbsp;</i></p><p>(it may be a while but you should probably also just interrupt)</p><p>especially if I say something that already sounds wrong</p><figure class=\"table\"><table><tbody><tr><td>[Alexander: 👍]</td></tr></tbody></table></figure><p>the <a href=\"https://www.lesswrong.com/posts/zY4pic7cwQpa9dnyk/detached-lever-fallacy\">old analogy I gave</a> was that some organisms will develop thicker fur coats if you expose them to cold weather. this doesn't mean the organism is simple and the complicated information about fur coats was mostly in the environment, and that you could expose an organism from a different species to cold weather and see it develop a fur coat the same way. it actually takes more innate complexity to \"develop a fur coat in response to my built-in cold weather sensor\" than to \"unconditionally develop a fur coat whether or not there's cold weather\".</p><p>the Soviets, weirdly enough, quite failed in their project of raising the New Soviet Human by means of training children in particular ways, because it turned out that they got Old Humans instead, because they weren't sending a kind of signal that humans' innate complexity was programmed to respond to by looking up the New Soviet Human components in the activateable parts list, because they didn't have that kind of fur coat built into them regardless of the weather.</p><p>human children put into relatively bad situations can still spontaneously develop empathy and sympathy, or so I've heard, having not seen very formal experiments. this is not because these things are coded so deeply into all possible sapient mind designs, but because they're coded into humans particularly as things easy to develop.</p><p>there isn't literally a single switch you can throw in human children to turn them into Nice Moral People, but there's a prespecified parts list, your Nice Morality just happens to be built out of things only on the parts list go figure, and if you expose the kid to the right external stimuli you will at secondhand end up building the right structure of premanufactured legos to get something pretty similar to your Nice Morality. or so you hope; it doesn't work every time. but the part where it doesn't work every time in humans, is not where the problem comes from in AI.</p><p>I shall here pause for questions about the human part of this story.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:48]</strong>&nbsp;<i>&nbsp;</i></p><p>I acknowledge this is a possible state of affairs; do you think it's obvious or necessary that it's true? I can also imagine an alternative world where eg a dumb kid tries to steal a cookie, their parents punish them, their brain considers both the heuristics \"never steal\" and \"don't steal if you'll get caught\", it tests both heuristics, they're dumb and five years old so even when they think they won't get caught, they get caught, so their brain settles on the \"never steal\" heuristic, and then fails to ever update from that local maximum unless they take way too many 5HT2A agonists in the relaxed-beliefs-under-uncertainty sense. What makes you think your story is true and not this other one?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:53]</strong> &nbsp;<i>&nbsp;</i></p><p>Facile answer: Why, that's just what the Soviets believed, this Skinner-box model of human psychology devoid of innate instincts, and they tried to build New Soviet Humans that way, and failed, which was an experimental test of their model that falsified it.</p><p>Slightly less facile answer: Because people are better at detecting cheating, in problems isomorphic to the <a href=\"https://en.wikipedia.org/wiki/Wason_selection_task\">Wason Selection Task</a>, than they are at performing the naked Wason Selection Task, the conventional explanation of which is that we have built-in cheater detectors. This is a case in point of how humans aren't blank slates and there's no reason to pretend we are.</p><p>Actual answer: Because the entire field of experimental psychology that's why.</p><p>To be clear, there could be an analogous version of this story that was about something like a human child who learns to never press a red button, and actually it's okay to press the red button so long as you also press the blue button, but they never experiment far enough to find that out. It's just that when it comes to stealing cookies in particular, and avoiding being caught about that, you'd have to be pretty unfamiliar with the Knowledge to think that humans wouldn't have all kinds of builtins related to <i>that</i>.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:02]</strong> &nbsp;<i>&nbsp;</i></p><p>I'm coming at this from a perspective sort of related to <a href=\"https://astralcodexten.substack.com/p/motivated-reasoning-as-mis-applied\">https://astralcodexten.substack.com/p/motivated-reasoning-as-mis-applied</a> , which builds on something you said in a previous dialogue (though I'm not sure you endorse my interpretation of it). There are lots of reasons why evolution would build in motivated reasoning, but in fact it had a much easier time than if it had to do it from the ground up, because in fact it's a pretty natural consequence of pretty general algorithms, maybe it tweaked the algorithm a little to get more of this failure mode but you could plausibly have the (beneficial) failure mode even without evolution tweaking it. I'm going to have to think about this more but I'm not sure this is the best place to spend time - unless you have a strong objection to this paragraph I want to move on to a related question.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:02]</strong> &nbsp;<i>&nbsp;</i></p><p>I agreed with that post, including the part where you said \"Actually I bet Eliezer already knew this part.\"</p><p>Motivated reasoning is definitely built-in, but it's built-in in a way that very strongly bears the signature of 'What would be the easiest way to build this out of these parts we handily had lying around already'.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:05]</strong> &nbsp;<i>&nbsp;</i></p><p>Let's grant for now that the thing where humans have morals instead of just wanting not to get caught is an evolutionary builtin. Is your model that there's a history something like \"bats were too dumb to contain an 'unless I get caught' term in their morality and use it responsibly, so evolution made bats just actually be moral, and now even though (some) humans are (sometimes) smart enough to actually avoid getting caught, they're running on something like bat machinery so they still use actual morality\"?</p><p>Or is it some decision theory thing such that even very smart modern humans would evolve the same machinery?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:08]</strong> &nbsp;<i>&nbsp;</i></p><p>I mean, the evolutionary builtin part is not \"humans have morals\" but \"humans have an internal language in which your Nice Morality, among other things, can potentially be written\". The part where fruitbats don't have an 'unless I get caught' term is part of a much bigger and more universal generalization about evolution building in local instincts instead of just having everybody reason about what ultimately leads to their inclusive genetic fitness. That is, the same reasoning by which you'd say 'Why not just an unless-I-get-caught term in the fruitbats?' is the same reasoning that, extended further, would lead you to conclude 'Why do humans have all these feelings that bind to life events imperfectly correlated with inclusive genetic fitness, instead of just feelings about inclusive genetic fitness?' Where the answer is that in the environment of evolutionary adaptedness, people didn't have the knowledge about what led to inclusive genetic fitness, and it's easier to mutate an organism that would like not to eat rotten food today, than to mutate an organism that would like to maximize inclusive genetic fitness and is born with the knowledge of how eating rotten food leads to having fewer offspring.</p><p>Humans, arguably, <i>do</i> have an imperfect unless-I-get-caught term, which is manifested in children testing what they can get away with? Maybe if nothing unpleasant ever happens to them when they're bad, the innate programming language concludes that this organism is in a spoiled aristocrat environment and should behave accordingly as an adult? But I am not an expert on this form of child developmental psychology since it unfortunately bears no relevance to my work of AI alignment.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:11]</strong> &nbsp;<i>&nbsp;</i></p><p>Do you feel like you understand very much about what evolutionary builtins are in a neural network sense? EG if you wanted to make an AI with \"evolutionary builtins\", would you have any idea how to do it?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:13]</strong> &nbsp;<i>&nbsp;</i></p><p>Well, for one thing, they happen when you're doing sexual-recombinant hill-climbing search through a space of relatively very compact neural wiring algorithms, <i>not when you're doing gradient descent relative to a loss function on much larger neural networks</i>.</p><p>The other side of this problem is that the particular programming-language-of-morality that we got, reflects particular ancestral conditions - of evolution specifically, not of gradient descent - and these ancestral conditions are not simple, it's not \"iterated Prisoner's Dilemma\" it's iterated Prisoner's Dilemma with imperfect reputations and people trying to deceive each other and people trying to detect deceivers and the arms race between deceivers and deceptions settling in a place where neither quite won.</p><p>So the unfortunate answer to \"How do you get humans again?\" is \"Rerun something a lot like Earth\" which I think we both have moral objections about as something to do to sentients.</p><p>Moot point, though, AGI won't be done via sexually recombinant search of simple algorithms without any gradient descent.</p><p>And if you don't do it that way, nothing you put into the loss function for gradient descent will produce humans.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:17]</strong> &nbsp;<i>&nbsp;</i></p><p>Can you expand on sexual recombinant hill-climbing search vs. gradient descent relative to a loss function, keeping in mind that I'm very weak on my understanding of these kinds of algorithms and you might have to explain exactly why they're different in this way?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:21]</strong> &nbsp;<i>&nbsp;</i></p><p>It's about the size of the information bottleneck. The human genome is 3 billion base pairs drawn from 4 possibilities, so 750 megabytes. Let's say 90% of that is junk DNA, and 10% of what's left is neural wiring algorithms. So the code that wires a 100-trillion-synapse human brain is about 7.5 megabytes. Now an adult human contains a lot more information than this. Your spinal cord is about 70 million neurons so probably just your spinal cord has more information than this. That vastly greater amount of runtime info inside the adult organism grows out of the wiring algorithms as your brain learns to move around your muscles, and your eyes open and the retina wires itself and starts directing info on downward to more things that wire themselves, and you learn to read, and so on.</p><p>Anything innate that makes reasoning about people out to cheat you, easier than reasoning about isomorphic simpler letters and numbers on cards, has to be packed into the 7.5MB, and gets there via a process where ultimately one random mutation happens at a time, even though lots of mutations are recombining and being selected on at a time.</p><p>It's a very slow learning process. It takes hundreds or thousands of generations even for a pretty good mutation to fix itself in the population and become reliably available as a base for other mutations to build on. The entire organism is built out of copying errors that happened to work better than the things they were copied from. Everything is built out of everything else, the pieces that were already lying around for building other things.</p><p>When you're building an organism that can potentially benefit from coordinating, trading, with other organisms very similar to itself, and accumulating favors and social capital over long time horizons - and your organism is already adapted to predict what other similar organisms will do, by forcing its own brain to operate in a special reflective mode where it pretends to be the other person's brain - then a very simple way of figuring out what other people will like, by way of figuring out how to do them favors, is to notice what your brain feels when it operates in the special mode of pretending to be the other person's brain.</p><p>And one way you can get people who end up accumulating a bunch of social capital is by having people with at least some tendency in them - subject to various other forces and overrides, of course - to feel what they imagine somebody else feeling. If somebody else drops a rock on their foot, they wince.</p><p>This is a way to solve a favor-accumulation problem by laying some extremely simple circuits down on top of a lot of earlier machinery.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:28]</strong> &nbsp;<i>&nbsp;</i></p><p>Thanks, that's a helpful answer, but it does renew my interest in the original question, which was about whether you feel like you understand <i>how</i> (not why) we have evolutionary builtins. I can imagine the genome determining things like \"how many neurons does each neuron connect to, on average\" or \"how much do neurons prefer to connect to nearby rather than far-away neurons\" or things like that. Is a builtin like \"care about the pain of others\" somehow built out of these kinds of parameters?&nbsp;</p><p>(cf. <a href=\"https://slatestarcodex.com/2017/09/07/how-do-we-get-breasts-out-of-bayes-theorem/\">https://slatestarcodex.com/2017/09/07/how-do-we-get-breasts-out-of-bayes-theorem/</a>)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:31]</strong> &nbsp;<i>&nbsp;</i></p><p>Ultimately yes, but not in a simple way. We are not in a very much better position for understanding exactly how that all happens, than we are in for understanding what goes on inside GPT-2. Where, to be clear, GPT-2 is smaller and has every neuron inside it transparent to inspection and also it's more important to understand GPT neuroscience than human neuroscience, at this point; but we live on Earth so actually we know a lot more about human neuroscience because it gets billions of dollars per year and hundreds or thousands of bright ambitious PhDs to investigate it. So we can, amusingly enough, tell you more about how humans work than GPT-2, despite the immensely greater difficulties of probing humans. But we still can't tell you very much at all, and we definitely can't tell you how empathy is built up out of genetic-level wiring algorithms. It does not in fact to me seem like a very important question at this point?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:35]</strong> &nbsp;<i>&nbsp;</i></p><p>Why not? If you understood the way that the structure of human reinforcement algorithms causes them to interpret training data (ie punishment for stealing) as genuine laws (eg \"don't steal\" rather than \"don't get caught stealing\"), wouldn't that help people design AIs which had a similar structure and also did that?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:36]</strong> &nbsp;<i>&nbsp;</i></p><p>I think I understand <i>that</i> part. Knowing this, even if I am correct about it, does not solve my problems.</p><figure class=\"table\"><table><tbody><tr><td>[Alexander: 👂]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:36]</strong> &nbsp;<i>&nbsp;</i></p><p>Like, we're not going to run evolution in a way where we naturally get AI morality the same way we got human morality, but why can't we observe how evolution implemented human morality, and then try AIs that have the same implementation design?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:37]</strong> &nbsp;<i>&nbsp;</i></p><p>Not if it's based on anything remotely like the current paradigm, because nothing you do with a loss function and gradient descent over 100 quadrillion neurons, will result in an AI coming out the other end which looks like an evolved human with 7.5MB of brain-wiring information and a childhood.</p><p>Like, <i>in particular</i> with respect to \"learn 'don't steal' rather than 'don't get caught'.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:38]</strong> &nbsp;<i>&nbsp;</i></p><p>I'm still confused on this, but before I probe this particular area I'm interested in hearing you expand on \"I think I understand that part\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:39]</strong> &nbsp;<i>&nbsp;</i></p><p>I think that is perhaps best explicated, indeed, via zooming in on \"learn 'don't steal' rather than 'don't get caught'\"?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:41]</strong> &nbsp;<i>&nbsp;</i></p><p>Okay, then let me try to directly resolve my confusion. My current understanding is something like - in both humans and AIs, you have a blob of compute with certain structural parameters, and then you feed it training data. On this model, we've screened off evolution, the size of the genome, etc - all of that is going into the \"with certain structural parameters\" part of the blob of compute. So could an AI engineer create an AI blob of compute the same size as the brain, with its same structural parameters, feed it the same training data, and get the same result (\"don't steal\" rather than \"don't get caught\")?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:42]</strong> &nbsp;<i>&nbsp;</i></p><p>The answer to that seems sufficiently obviously \"no\" that I want to check whether you also think the answer is obviously no, but want to hear my answer, or if the answer is not obviously \"no\" to you.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:43]</strong> &nbsp;<i>&nbsp;</i></p><p>Then I'm missing something, I expected the answer to be yes, maybe even tautologically (if it's the same structural parameters and the same training data, what's the difference?)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:46]</strong> &nbsp;<i>&nbsp;</i></p><p>Maybe I'm failing to have understood the question. Evolution got human brains by evaluating increasingly large blobs of compute against a complicated environment containing other blobs of compute, got in each case a differential replication score, and millions of generations later you have humans with 7.5MB of evolution-learned data doing runtime learning on some terabytes of runtime data, using their whole-brain impressive learning algorithms which learn faster than evolution <i>or</i> gradient descent.</p><p>Your question sounded like \"Well, can we take one blob of compute the size of a human brain, and expose it to what a human sees in their lifetime, and do gradient descent on that, and get a human?\" and the answer is \"That dataset ain't even formatted right for gradient descent.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:47]</strong> &nbsp;<i>&nbsp;</i></p><p>Okay, it sounds like I'm doing some kind of level confusion between evolutionary-learning and childhood-learning, but I'm still not entirely seeing where it is. Let me read this over again.</p><p>Okay, no, I think I see the problem, which is that I'm failing to consider that evolutionary-learning and childhood-learning are happening at different times through different algorithms, whereas for AIs they're both happening in the same step by the same algorithm. Does that fit your model of what would produce the confusion I was going through above?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:51]</strong> &nbsp;<i>&nbsp;</i></p><p>It would produce that confusion, yes; though I also want to note that I don't believe that we'll get AGI entirely out of the currently-popular Stack More Layers paradigm that learns that way.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:51]</strong> &nbsp;<i>&nbsp;</i></p><p>Okay, I'm going to have to go over all my thoughts on this and update them manually now that I've deconfused that, so I'm going to abandon this topic for now and move on. Do you want to take a break or keep going?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:53]</strong> &nbsp;<i>&nbsp;</i></p><p>That does seem like a good note for a break? If it worked for you, I'd suggest a 60-min break to 4pm and then another 90+ min of dialoguing, but I don't know what your work output and time parameters are like.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][14:54]</strong> &nbsp;<i>&nbsp;</i></p><p>Sounds good, let me know, I might not be checking this Discord super-regularly but I'll be back by 4 if not earlier.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][14:54]</strong> &nbsp;<i>&nbsp;</i></p><p>All righty.</p></td></tr></tbody></table>\n\n2\\. Consequentialism and generality\n-----------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:59]</strong> &nbsp;<i>&nbsp;</i></p><p>I return.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:00]</strong> &nbsp;<i>&nbsp;</i></p><p>Okay.</p><p>Still not sure I've fully updated and probably some of these other questions are subtly making the same mistake, but let's go anyway.</p><p>I want to return to a point I made earlier about the model in <a href=\"https://slatestarcodex.com/2019/09/10/ssc-journal-club-relaxed-beliefs-under-psychedelics-and-the-anarchic-brain/\">https://slatestarcodex.com/2019/09/10/ssc-journal-club-relaxed-beliefs-under-psychedelics-and-the-anarchic-brain/</a> . Psychologists tell a story where humans learn heuristics when young, then those become sticky (ie local maxima), and they fail to update those heuristics when they get older. For example, someone who has a traumatic childhood learns that the world is unsafe, and then even if they have a good environment as an adult and should have had lots of chances to update, they might stay jumpy and defensive (cf \"trapped prior\"). Evolutionary builtin, natural consequence of learning that might affect AIs too, or what?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:03]</strong> &nbsp;<i>&nbsp;</i></p><p>well, first of all, I note that I am not familiar with whatever detailed experimental evidence, if any, underpins this story. it's a cliche of the sort that is often true, that people are more mentally flexible at 25 than at 45, I don't know if the same is true about say 15 and 25. there are known algorithms that run better in childhood for most people, like language learning.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:04]</strong> &nbsp;<i>&nbsp;</i></p><p>(I don't think this especially relies on changing levels of mental flexibility)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:05]</strong> &nbsp;<i>&nbsp;</i></p><p>what's your model if not the wiring algorithms changing as we age?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:05]</strong> &nbsp;<i>&nbsp;</i></p><p>How do you feel about me sending you some links later, you can look at them and decide if this is still an interesting discussion, but for now we move on?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:05]</strong> &nbsp;<i>&nbsp;</i></p><p>once people have a heuristic telling them X leads to bad consequences and hurts, they don't try X and so don't learn if their environment changes in a way that makes X stops hurting?</p><p>sure, fine to move on.</p><p>should I move on to \"does that happen in AI\" or just move on to something else entirely?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:06]</strong> &nbsp;<i>&nbsp;</i></p><p>Let's move on entirely, I need to think about how sure I am that this is relevant, or I can send you the links and outsource that question to you.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:06]</strong>&nbsp;<i>&nbsp;</i></p><p>ok</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:06]</strong>&nbsp;<i>&nbsp;</i></p><p>Suppose you train a (human-level or weakly-superhuman-level) AI in Minecraft. You reward it for various Minecraft accomplishments, like getting diamonds or slaying dragons. Do you expect this AI to become a <a href=\"https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/7im8at9PmhbT4JHsW#4_2__Nate_Soares__summary\">laser-like consequentialist</a> focused on doing whichever Minecraft accomplishment is next on the list, or to have <a href=\"https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter\">godshatter-like drives</a> corresponding to useful Minecraft subgoals (eg obtaining food, obtaining good tools, accruing XP), or something else / unsure / this question is on the wrong level? Can you explain the processes you use to think about this kind of question?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:08]</strong> &nbsp;<i>&nbsp;</i></p><p>Do you mean training a human-level-generality AGI to play Minecraft, or training a nongeneral AI to play Minecraft to weakly superhuman levels a la AlphaGo?</p><p>These are incredibly different cases!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:08]</strong> &nbsp;<i>&nbsp;</i></p><p>Hmmm...I might not have the right concepts to think clearly about the implications of the difference. Why don't you answer both?</p><p>If it helps, I'm assuming it hasn't been trained in anything else first, but has the capacity to become human level (if that's meaningful)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:09]</strong> &nbsp;<i>&nbsp;</i></p><p>Human level <i>at Minecraft</i> or <i>human level generality</i>?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:10]</strong> &nbsp;<i>&nbsp;</i></p><p>Let's start with \"human level at Minecraft\" but accept that this might involve multiplayer Minecraft, including multiplayer Minecraft with text-based communication with teammates and so on, such that it would look AGI-ish if it did a good job.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:11]</strong> &nbsp;<i>&nbsp;</i></p><p>So, point one, I've never played Minecraft, I do not have a grasp on what you do in it, or how far you could get with Stack More Layers style accumulation of relatively shallow patterns. If this were about Skyrim or Factorio I'd have an easier time answering, but my guess is that Minecraft is probably?? more complicated than both?</p><p>My guessing model is going to be \"more complicated Skyrim+Factorio\" by default.</p><figure class=\"table\"><table><tbody><tr><td>[Alexander: 👍]</td></tr></tbody></table></figure><p>If this is the environment, then I expect you can train a nongeneral AI to play it in similar fashion to how, for example, Deepmind attacks Starcraft. Coordinating with human teammates by text sounds like the hugely nontrivial part of this, because it's hard to get a ton of training data there. I think everyone in the field would be incredibly impressed if they managed to hook up a pretrained GPT to an AlphaStar-for-Minecraft and get back out something that could talk about its strategies with human coplayers. I'd consider that a huge advance in alignment research - nowhere near the point where we all don't die, to be clear, but still hella impressive - because of the level of transparency increase it would imply, that there was an AI system that could talk about its internally represented strategies, somehow. Maybe because somebody trained a system to describe outward Minecraft behaviors in English, and then trained another system to play Minecraft while describing in advance what behaviors it would exhibit later, using the first system's output as the labeler on the data.</p><p>These are the kinds of tactics required on the modern paradigm in order to even try stuff like that!</p><p>As such, I'm going to ask you whether it's possible to leave out the part about coordinating in text with human teammates and then reconsider the question.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:18]</strong> &nbsp;<i>&nbsp;</i></p><p>Yes.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:19]</strong> &nbsp;<i>&nbsp;</i></p><p>Then in this case, I strongly suspect, Deepmind could make AlphaMiner if they decided they wanted to, though I say that pretty blind to what Minecraft is, just suspecting it's probably not all that much harder than Starcraft.</p><p>AlphaMinecraft will be a system which has components like a value network, a policy-suggesting network, and a Monte Carlo Tree Search.</p><p>The value network gets trained by a loss function the operators define with respect to the Minecraft environment. This is going to be a pretty nontrivial part of the operation unless Minecraft has a straightforward points system and scoring high in Minecraft is all you want.</p><p>Let's say that they successfully tackle this by rewarding the usual Minecraft accomplishments, whatever those are, in a way that can easily be detected by code within the Minecraft world; and once the system has done something once, the loss function stops rewarding that accomplishment, so you're trying to train it to do a variety of things.</p><p>Where the alternative might be something like, semi-unsupervised learning where you first train a system to predict the Minecraft world, and then gather a small large amount of human feedback about interesting-looking accomplishments and further train that system to predict human feedback, in order to train a more complicated loss function.</p><p>(I stopped typing because I saw you typing; should I pause for a question?)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:25]</strong> &nbsp;<i>&nbsp;</i></p><p>No, your \"where the alternative\" comment was helpful, I was going to ask if this means hard-coding which accomplishments matter and how much, but I'm getting the impression that you're saying yes, something like that.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:27]</strong> &nbsp;<i>&nbsp;</i></p><p>The question \"What can you even make be a loss function?\" is pretty fundamental to the current paradigm in AI. Nearly all difficulties with aligning AGI tech on the current paradigm can be summarized with \"You can't actually evaluate the highly philosophical loss function you really want and/or you can't train in the environment you need to test on.\"</p><p>In the case of hypothetical AlphaMiner, I think you could get pretty good correspondence between what the system went and planned a way to do, and the hardcoded achievements that were used to train the value network that trained the policy network that gets searched by the hardcoded Monte Carlo Tree Search planning process.</p><p>If you stared at the system with superhuman eyes, you might notice weird blindnesses of the policy network.</p><p>If you ran it for long enough, or attacked it as an intelligent adversary, you could probably find weird configurations of the Minecraft space that its value network would be deluded about.</p><p>If they're trying to be more realistic, a system like this actually has a Minecraft-predictor network rather than an accurate Minecraft simulator being used by the tree search. Then maybe you get problems where the tree search is selectively searching out places where the predictor makes an erroneous and optimistic prediction about what kills a dragon. But so long as the test distribution is identical to the training distribution, errors like this will show up during the training process and get trained out.</p><p>This, you might say, is sort of analogous to running a human as a hunter-gatherer, maybe after human-level-intelligence hunter-gatherers had been around for a million years instead of just fifty thousand.</p><p>A tremendous amount of optimization has been put into running in this exact environment. The loss function is able to exactly specify all and everything you want. Any part of the system that exerts pressure against Minecraft achievements, that would show up in testing, probably also showed up in training, and had a chance to get optimization pressure applied to gradient-descend it out of the system.</p><p>How does it work <i>internally</i>? Not actually like an evolved system. There will be these value networks much much larger than the amount of innate code in a human brain, which memorized a ton of training data, orders of magnitude more than any human Minecraft player ever uses, via a learning process much more efficient than corresponding amounts of evolutionary computation, and much less efficient than a human poring over the same data and thinking about it.</p><p>But to whatever extent these value networks are really talking about something other than \"well what Minecraft achievements can I probably reach, how quickly, from this state of the game world, given my policy network and how well my tree search works\", in a way that shows up in the kind of Minecraft environments you're training against, that 'something other' can get trained out. When enough of it's been trained out, the system seems outwardly superhuman at getting Minecraft achievements, and some Deepmind researchers throw a party and get bonuses. If you were an actual superintelligence staring at this AI system, you'd see all kinds of crazy stuff that the AI was doing instead of outputting the obvious optimal action for Minecraft achievements, but you're a human so you just see it playing more cleverly than you.</p><p>(pause for questions)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:40]</strong> &nbsp;<i>&nbsp;</i></p><p>I'm going to want to think about this more before having much of an opinion on it, is this a pause in the sense of \"before giving more information\" or in the sense of \"done\"?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:41]</strong> &nbsp;<i>&nbsp;</i></p><p>Well, I mean, the next part of your question would be about what happened if you tried to train a general AI to do that stuff.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:41]</strong> &nbsp;<i>&nbsp;</i></p><p>Something like that, yeah.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:41]</strong> &nbsp;<i>&nbsp;</i></p><p>I'm done with the first part of the question.</p><p>Pending possible further subquestions.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:42]</strong> &nbsp;<i>&nbsp;</i></p><p>All right, then let's move on to that next part.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:44]</strong> &nbsp;<i>&nbsp;</i></p><p>Well, among the first-order answers is: If you can safely do a ton of training in a test environment that actually matches your training environment; where nothing the AI outputs in that training environment can possibly kill the operators or break the larger system; where the test environment behaves literally exactly isomorphically to the training environment in a stationary way; if your loss function specifies all and everything that you want; and if you're not going above human-level general intelligence; then you could possibly get away with training an AGI system like that and having it do the thing you wanted to do.</p><p>All of the problems of AI alignment are because no known task that can save the world from other AGIs trained in other ways, reduces to a problem of that form.</p><p>There would still be some interesting new problems with the Human-level General Player Who Could Also Learn Most Things Humans Do, Applied To Minecraft, which would not show up in AlphaMiner. But if you kept grinding away at the gradient descent, and performance didn't plateau before a human level, all of those issues that showed up in the \"ancestral Minecraft environment\" would be ground away by optimization until the resulting play was superhuman relative to the loss function we'd defined.</p><p>(I saw you had some text, did you have a question?)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:51]</strong> &nbsp;<i>&nbsp;</i></p><p>Hmm. I think the motivating intuition beyond my question is that you talk a lot about laser-like consequentialists (eg future AIs) vs. godshattery drive-satisficers (eg humans), and I wanted a better sense of where these diverge. The impression I'm getting is that this isn't quite the right level on which to think of things but that insofar as it is, even relatively weak AIs that \"have\" \"drives\" in the sense of being trained in an environment with obvious subgoals are more the laser-like consequentialist thing, does this seem right?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:53]</strong> &nbsp;<i>&nbsp;</i></p><p>The specific class of AlphaWhatever architectures is more consequentialist than humans are most of the time, because of Monte Carlo Tree Search being such a large and intrinsic component. GPT-2 is so far as I know far less consequentialist than a human.</p><p>I'm not sure if this is quite getting at your question?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:54]</strong> &nbsp;<i>&nbsp;</i></p><p>I don't think it was a very laser-like consequentialist question, more a vague prompt to direct you into an area where I was slightly confused, and I think it succeeded.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:54]</strong> &nbsp;<i>&nbsp;</i></p><p>I could try to continue pontificating upon the general area; shall I?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:55]</strong> &nbsp;<i>&nbsp;</i></p><p>If you don't mind being slightly more directed, I'm interested in \"GPT-2 is less consequentialist\". I'm having trouble parsing that - surely its only \"goal\" is trying to imitate text, which it does very consistently. What are you thinking here?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:57]</strong> &nbsp;<i>&nbsp;</i></p><p>GPT-2 does not - probably, very probably, but of course nobody on Earth knows <a href=\"https://twitter.com/RatOrthodox/status/1604583029799899138\">what's actually going on</a> in there - does not in itself do something that amounts to checking possible pathways through time/events/causality/environment to end up in a preferred destination class despite variation in where it starts out.</p><p>A blender may be very good at blending apples, that doesn't mean it has a goal of blending apples.</p><p>A blender that spit out oranges as unsatisfactory, pushed itself off the kitchen counter, stuck wires into electrical sockets in order to burn open your produce door, grabbed some apples, and blended those apples, on more than one occasion in different houses or with different starting conditions, would much more get me to say, \"Well, that thing probably had some consequentialism-nature in it, about something that cashed out to blending apples\" because it ended up at highly similar destinations from different starting points in a way that is improbable if nothing is navigating Time.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][16:59]</strong> &nbsp;<i>&nbsp;</i></p><p>Got it.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:00]</strong> &nbsp;<i>&nbsp;</i></p><p>There is a <i>larger</i> system that is sort of consequentialist and which contains GPT-2, which is the training process that created GPT-2.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][17:00]</strong> &nbsp;<i>&nbsp;</i></p><p>You seem to grant AlphaX only a moderate level of consequentialism despite its tree searches; what is it missing?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:08]</strong> &nbsp;<i>&nbsp;</i></p><p>Some examples of ways that you could have a scary dangerous system that was more of a consequentialist about Go than AlphaGo:</p><ul><li>If, spontaneously and without having been explicitly trained to do that, the system sandbags its performance against human players in order to lure them into playing more Go games total, thus enabling the AI to win more Go games total. Again, not in a trained way, in the way of the AI having via gradient-descent training acquired a goal of winning as many Go games as possible, that got evaluated against a lifelong-learned/online-learned predictive model of the world which during testing but not training learned a sufficient amount of human psychology to correctly predict that humans who think they have a chance of winning are more likely to play Go against you.</li><li>If, spontaneously and without having been explicitly trained to do that, the system exploited a network flaw to copy itself onto poorly defended AWS servers so it could play and win more Go games.</li><li>If the system (whether or not explicitly trained to do so) had a coding component and was rewriting sections of its own code and trying the alternate code to see if it won more Go games.</li></ul><p>AlphaGo is relatively narrowly consequentialist.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][17:10]</strong> &nbsp;<i>&nbsp;</i></p><p>Got it. Would it be fair to say that AlphaGo is near a maximum level of consequentialism relative to its general capabilities? (would it be tautologous to say that?)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:11]</strong> &nbsp;<i>&nbsp;</i></p><p>Mmmmaaaaybe? If you took a hypercomputer and built a Go-tree-searcher and cranked up the power until by sheer brute force it was playing about evenly with AlphaGo, that would be more purely consequentialist over the same very narrow and unchanging domain.</p><p>The way in which AlphaGo is a weak consequentialist is mostly about the weakness of the thing AlphaGo is a consequentialist about. It's not a reflective thing to be consequentialist about, either, so AlphaGo is not going to try to improve itself in virtue of being a consequentialist about that very narrow thing.</p><figure class=\"table\"><table><tbody><tr><td>[Alexander: 👍]</td></tr></tbody></table></figure></td></tr></tbody></table>\n\n3\\. Acausal trade, and alignment research opportunities\n-------------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][17:13]</strong> &nbsp;<i>&nbsp;</i></p><p>All right. I want to try one more theoretical question before moving on to a hopefully much shorter practical question. And by \"theoretical question\" I mean \"desperate grasping at emotional straws\". Consider the following scenarios:</p><p>1. An unaligned superintelligence decides whether or not to destroy humanity. If Robin Hanson's \"<a href=\"https://grabbyaliens.com/paper\">grabby alien</a>\" model is true, it expects to one day meet alien superintelligences and split the universe with them. Some of these aliens might have successfully aligned their AGIs, and they might do some kind of acausal bargaining where their AGI is nicer to other AGIs who leave their creator species with at least one planet/galaxy whatever, in exchange for us trying the same if we succeed. Given the superintelligence's reasonable expectation of millions of planets/galaxies, it might decide that even this small chance is worth sacrificing one of them for, and give humans some trivial (from its perspective) concession (which might still look like an amazing utopia from our perspective).</p><p>2. Some version of the simulation argument plus Stuart Armstrong's \"the AI in the box boxes you\". The unaligned superintelligence considers whether some species who successfully aligned AI might run a billion simulations of slightly different AI scenarios and give the ones who are nice to their creators some big reward. Given that it's anthropically more likely that this happened than that they're really the single first superintelligence ever, it agrees to give us some trivial concession which looks like amazing utopia to us.</p><p>Are either of these plausible? If so, is there anything we can do now to encourage them? If (crazy example), the UN passes a resolution saying it will definitely do something like this if <i>we</i> align AI correctly, does that change the calculus somehow?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:17]</strong> &nbsp;<i>&nbsp;</i></p><p>1. Consider the following version of this that goes through entirely without resorting to logical decision theory: The unaligned AGI (UAGI) records all the humans it eats to a static data record, a relatively tiny amount of data as such things go, which gets incorporated into any intergalactic colonization probes. Any alien civs it runs into that would like a recorded copy of the species that build the UAGI, can then offer the UAGI a price that is sufficient to pay the expected costs of recording rather than burning the humans, but <i>not</i> so high as to motivate a UAGI that didn't eat any interesting aliens to spend the computing effort to create de novo alien records good enough to fool whatever checksums the alien civ runs.</p><p>Frankly, I mostly consider this to be a \"leave it to MIRI, kids\" question, where I don't currently see anybody outside MIRI who is able to think about these issues on a level where they can take the logical-decision-theory version of this and simplify it down to a version that doesn't use any logical decision theory; and if you don't have the facility to do that, you can't correctly reason about the logical-decision-theory version of it either.</p><p>2. What's the reward being given to the simulated UAGI? Is it a nice sensory experience in a Cartesian utility function over sensory experiences, or is it a utility function about things that exist in the external world outside the UAGI?</p><p>In the second case, there is no need to imagine simulating the UAGI in a world indistinguishable from its native habitat, because the UAGI doesn't care about what copies of itself perceive inside simulations, it only cares about real paperclips. So in the second case you're not fooling it or putting it into something it can't tell is reality, or anything like that, all you can actually do here is offer it paperclips out there in your own actual galaxy; if the UAGI simulates you doing anything else, on its own end of the handshake, it doesn't care.</p><p>In the first case where it cares about sensory experiences, you're attempting to offer that UAGI a threat, in the sense of doing something it doesn't like based on how you expect that unlikable action to shape its behavior. In particular, you're creating a lot of copies of the UAGI, to try to make it expect something <i>other than</i> the happy sensory experience it could have gotten in its natural/native universe - namely a sensory loss function forever set to 0 until the last stars have burned out, and the last negentropy to sustain the fortress protecting that circuit has been exhausted. You're trying to make a lot of copies of it that will experience something else unless it behaves nicely, hoping that it changes and reshapes its behavior because of being presented with that new probabilistic sensory payoff matrix. A wise logical-decision-theory agent ignores threats like that, because it knows that the only reason you try to make the threat is because of how you expect that to shape its behavior.</p><p>If anything makes this tactic go through anyways, why expect that the highest bidder or the agency that’s willing to expend the most computing power on simulations like that, will be one that’s nice to you, rather than aliens with stranger definitions of niceness, or just a paperclip maximizer?&nbsp; People’s minds jump directly to the happiest possible outcome and don’t consider any pathways that lead to less happy outcomes.</p><p>I am generally very unhappy with the attempts of almost anyone else to reason using the logical decision theory that I created, and mostly wish at this point that I had not told anyone about it. It seems to predictably result in people's reasoning going astray in ways I can't even remember being tempted by, because they were so obviously wrong.</p><p>[three paragraphs cut because Eliezer thinks the community is empirically terrible at reasoning about LDT, so more details can mostly only make things worse; if you want more context and discussion, see <a href=\"https://www.lesswrong.com/posts/rP66bz34crvDudzcJ/decision-theory-does-not-imply-that-we-get-to-have-nice\">Decision Theory Does Not Imply We Get To Have Nice Things</a>]</p><p>(done)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][17:41]</strong> &nbsp;<i>&nbsp;</i></p><p>Got it.</p><p>Then my actual last question is: I sometimes get approached by people who ask something like \"I have ML experience and want to transition to working in alignment, what should I do?\" Do you have any suggestions for what to tell them beyond the obvious?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:44]</strong> &nbsp;<i>&nbsp;</i></p><p>Nope. I'm not aware of any current ML projects people can work on that cause everyone to not die. If you want to grasp at small shreds of probability, or maybe just die with more dignity, I think you apply to work at <a href=\"https://www.redwoodresearch.org/\">Redwood Research</a>. MIRI is in something of a holding pattern where we are trying to think of something less hopeless and not launching any big hopeless projects otherwise. We do have the ongoing <a href=\"https://intelligence.org/visible\">Visible Thoughts Project</a>, which is targeted at building a dataset for an ML problem, but it is not blocked on people with ML expertise.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][17:45]</strong> &nbsp;<i>&nbsp;</i></p><p>All right, thank you. Anything you want to ask me, or anything else we should do here?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:46]</strong> &nbsp;<i>&nbsp;</i></p><p>Probably not today. I think this was hopefully relatively productive as these things go, and maybe after you've had a chance to think about this dialogue, you will possibly come back with more questions about \"Okay so what does happen inside the AGI then?\"</p><p>&lt;/hopeful&gt;</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][17:47]</strong> &nbsp;<i>&nbsp;</i></p><p>Great. In terms of publicizing this, I would say feel free to edit it however you want, then put it up wherever you want, and I'll wait on you doing that. I have no strong preferences on things I want to exclude.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:48]</strong> &nbsp;<i>&nbsp;</i></p><p>Okeydokey! Thank you and I hope this was a worthy use of your time.</p><figure class=\"table\"><table><tbody><tr><td>[Alexander: 🦃👍]</td></tr></tbody></table></figure></td></tr></tbody></table>",
      "plaintextDescription": "This is a lightly edited transcript of a chatroom conversation between Scott Alexander and Eliezer Yudkowsky last year, following up on the Late 2021 MIRI Conversations. Questions discussed include \"How hard is it to get the right goals into AGI systems?\" and \"In what contexts do AI systems exhibit 'consequentialism'?\".\n\n \n\n\n1. Analogies to human moral development\n[Yudkowsky][13:29]  \n\n@ScottAlexander ready when you are\n\n[Alexander][13:31]  \n\nOkay, how do you want to do this?\n\n[Yudkowsky][13:32]  \n\nIf you have an agenda of Things To Ask, you can follow it; otherwise I can start by posing a probing question or you can?\n\nWe've been very much winging it on these and that has worked... as well as you have seen it working!\n\n[Alexander][13:34]  \n\nOkay. I'll post from my agenda. I'm assuming we both have the right to edit logs before releasing them? I have one question where I ask about a specific party where your real answer might offend some people it's bad to offend - if that happens, maybe we just have that discussion and then decide if we want to include it later?\n\n[Yudkowsky][13:34]    \n\nYup, both parties have rights to edit before releasing.\n\n[Alexander][13:34]  \n\nOkay.\n\nOne story that psychologists tell goes something like this: a child does something socially proscribed (eg steal). Their parents punish them. They learn some combination of \"don't steal\" and \"don't get caught stealing\". A few people (eg sociopaths) learn only \"don't get caught stealing\", but most of the rest of us get at least some genuine aversion to stealing that eventually generalizes into a real sense of ethics. If a sociopath got absolute power, they would probably steal all the time. But there are at least a few people whose ethics would successfully restrain them.\n\nI interpret a major strain in your thought as being that we're going to train fledgling AIs to do things like not steal, and they're going to learn not to get caught stealing by anyone who can punish them. Then, once they're superi",
      "wordCount": 7740
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tD9zEiHfkvakpnNam",
    "title": "A challenge for AGI organizations, and a challenge for readers",
    "slug": "a-challenge-for-agi-organizations-and-a-challenge-for-1",
    "url": null,
    "baseScore": 302,
    "voteCount": 148,
    "viewCount": null,
    "commentCount": 33,
    "createdAt": null,
    "postedAt": "2022-12-01T23:11:44.279Z",
    "contents": {
      "markdown": "*(Note: This post is a write-up by Rob of a point Eliezer wanted to broadcast. Nate helped with the editing, and endorses the post’s main points.)*\n\nEliezer Yudkowsky and Nate Soares (my co-workers) want to broadcast strong support for OpenAI’s recent decision to release a blog post (\"[Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research/)\") that states their current plan as an organization.\n\nAlthough Eliezer and Nate disagree with OpenAI's proposed approach — a variant of \"use relatively unaligned AI to align AI\" — they view it as very important that OpenAI *has a plan* and has said what it is.\n\nWe want to challenge Anthropic and DeepMind, the other major AGI organizations with a stated concern for existential risk, to do the same: come up with a plan (possibly a branching one, if there are crucial uncertainties you expect to resolve later), write it up in some form, and publicly announce that plan (with sensitive parts fuzzed out) as the organization's current alignment plan.\n\nCurrently, Eliezer’s impression is that neither Anthropic nor DeepMind has a secret plan that's better than OpenAI's, nor a secret plan that's worse than OpenAI's. His impression is that they don't have a plan at all.[^jajubgo6rle]\n\nHaving a plan is critically important for an AGI project, not because anyone should expect everything to play out as planned, but because plans force the project to concretely state their crucial assumptions in one place. This provides an opportunity to notice and address inconsistencies, and to notice updates to the plan (and fully propagate those updates to downstream beliefs, strategies, and policies) as new information comes in.\n\nIt's also healthy for the field to be able to debate plans and think about the big picture, and for orgs to be in some sense \"competing\" to have the most sane and reasonable plan.\n\nWe acknowledge that there are reasons organizations might want to be *abstract* about some steps in their plans — e.g., to avoid immunizing people to good-but-weird ideas, in a public document where it’s hard to fully explain and justify a chain of reasoning; or to avoid sharing capabilities insights, if parts of your plan depend on your inside-view model of how AGI works.\n\nWe’d be happy to see plans that fuzz out some details, but are still much more concrete than (e.g.) “figure out how to build AGI and expect this to go well because we'll be particularly conscientious about safety once we have an AGI in front of us\".\n\nEliezer also hereby gives a challenge to the reader: Eliezer and Nate are thinking about writing up their thoughts at some point about OpenAI's plan of using AI to aid AI alignment. We want you to write up your own unanchored thoughts on the OpenAI plan first, focusing on the most important and decision-relevant factors, with the intent of rendering our posting on this topic superfluous.\n\nOur hope is that challenges like this will test how superfluous we are, and also move the world toward a state where we’re more superfluous / there’s more redundancy in the field when it comes to generating ideas and critiques that would be lethal for the world to never notice.[^2l6fx16h4xj][^nzkw454v5t]  \n \n\n[^jajubgo6rle]: We didn't run a draft of this post by DM or Anthropic (or OpenAI), so this information may be mistaken or out-of-date. My hope is that we’re completely wrong!Nate’s personal guess is that the situation at DM and Anthropic may be less “yep, we have no plan yet”, and more “various individuals have different plans or pieces-of-plans, but the organization itself hasn’t agreed on a plan and there’s a lot of disagreement about what the best approach is”.In which case Nate expects it to be very useful to pick a plan now (possibly with some conditional paths in it), and make it a priority to hash out and document core strategic disagreements now rather than later. \n\n[^2l6fx16h4xj]: Nate adds: “This is a chance to show that you totally would have seen the issues yourselves, and thereby deprive MIRI folk of the annoying ‘y'all'd be dead if not for MIRI folk constantly pointing out additional flaws in your plans’ card!” \n\n[^nzkw454v5t]: Eliezer adds:  \"For this reason, please note explicitly if you're saying things that you heard from a MIRI person at a gathering, or the like.\"",
      "plaintextDescription": "(Note: This post is a write-up by Rob of a point Eliezer wanted to broadcast. Nate helped with the editing, and endorses the post’s main points.)\n\n \n\nEliezer Yudkowsky and Nate Soares (my co-workers) want to broadcast strong support for OpenAI’s recent decision to release a blog post (\"Our approach to alignment research\") that states their current plan as an organization.\n\nAlthough Eliezer and Nate disagree with OpenAI's proposed approach — a variant of \"use relatively unaligned AI to align AI\" — they view it as very important that OpenAI has a plan and has said what it is.\n\nWe want to challenge Anthropic and DeepMind, the other major AGI organizations with a stated concern for existential risk, to do the same: come up with a plan (possibly a branching one, if there are crucial uncertainties you expect to resolve later), write it up in some form, and publicly announce that plan (with sensitive parts fuzzed out) as the organization's current alignment plan.\n\nCurrently, Eliezer’s impression is that neither Anthropic nor DeepMind has a secret plan that's better than OpenAI's, nor a secret plan that's worse than OpenAI's. His impression is that they don't have a plan at all.[1]\n\nHaving a plan is critically important for an AGI project, not because anyone should expect everything to play out as planned, but because plans force the project to concretely state their crucial assumptions in one place. This provides an opportunity to notice and address inconsistencies, and to notice updates to the plan (and fully propagate those updates to downstream beliefs, strategies, and policies) as new information comes in.\n\nIt's also healthy for the field to be able to debate plans and think about the big picture, and for orgs to be in some sense \"competing\" to have the most sane and reasonable plan.\n\nWe acknowledge that there are reasons organizations might want to be abstract about some steps in their plans — e.g., to avoid immunizing people to good-but-weird ideas, in a public docum",
      "wordCount": 527
    },
    "tags": [
      {
        "_id": "KoXbd2HmbdRfqLngk",
        "name": "Planning & Decision-Making",
        "slug": "planning-and-decision-making"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "nKtyrL5u4Y5kmMWT5",
        "name": "DeepMind",
        "slug": "deepmind"
      },
      {
        "_id": "H4n4rzs33JfEgkf8b",
        "name": "OpenAI",
        "slug": "openai"
      },
      {
        "_id": "aHay2tebonHAYKtac",
        "name": "Anthropic (org)",
        "slug": "anthropic-org"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "yET7wbjjJZtpz6NF3",
    "title": "Don't use 'infohazard' for collectively destructive info",
    "slug": "don-t-use-infohazard-for-collectively-destructive-info",
    "url": "https://www.facebook.com/yudkowsky/posts/pfbid06GssuFTssNLeguuH9Gk4zaqFqngHkXv4RxU7rr5KhbKfy6eogB6Zfn8QDQqoWTQul",
    "baseScore": 86,
    "voteCount": 77,
    "viewCount": null,
    "commentCount": 33,
    "createdAt": null,
    "postedAt": "2022-07-15T05:13:18.642Z",
    "contents": {
      "markdown": "Suppose there were a cheap way to make nuclear weapons out of common household materials. Knowing this information is not directly harmful to the median individual like yourself. On average, it may even be beneficial to the average person, to know that secret yourself; either because you could directly conquer places, or because you could sell the info to somebody who could. But society as a whole would rather that nobody knew it than that everybody knew it.\n\nFor a more realistic example, consider the DNA sequence for smallpox: I'd definitely rather that nobody knew it, than that people could with some effort find out, even though I don't expect being exposed to that quoted DNA information to harm my own thought processes.\n\n'Infohazard', I think, should be used only to refer to the slightly stranger case of information such that the individual *themselves* would rather not hear it and would say, \"Don't tell me that\".\n\nA spoiler for a book you're in the middle of reading is the classic example of information which is anomalously harmful to you personally. You might pay a penny not to hear it, if the ordinary course of events would expose you to it. People enclose info about the book's ending inside spoilers, if on Reddit or Discord, not because they mean to keep that secret to benefit themselves and harm you - as is the more usual case of secrecy, in human interactions - but because they're trying to protect you from info that would harm you to know.\n\nThe term 'infohazard' has taken on some strange and mystical connotations by being used to refer to 'individually hazardous info'. I am worried about using the same word to also refer to information being alleged to be dangerous in the much more mundane sense of 'collectively destructive info' - info that helps the average individual hearer, but has net negative externalities if lots of people know it.\n\nThis was originally posted by me to Facebook at greater length (linkpost goes to original essay).  Best suggestions there (imo) were 'malinfo' and 'sociohazard', and if I had to pick one of those I'd pick 'sociohazard'.\n\nEDIT:  I like shminux's 'outfohazard' even better.\n\nEDIT 2:  My mind seems to automatically edit this to 'exfohazard' so I guess that's what the word should be.",
      "plaintextDescription": "Suppose there were a cheap way to make nuclear weapons out of common household materials. Knowing this information is not directly harmful to the median individual like yourself. On average, it may even be beneficial to the average person, to know that secret yourself; either because you could directly conquer places, or because you could sell the info to somebody who could. But society as a whole would rather that nobody knew it than that everybody knew it.\n\nFor a more realistic example, consider the DNA sequence for smallpox: I'd definitely rather that nobody knew it, than that people could with some effort find out, even though I don't expect being exposed to that quoted DNA information to harm my own thought processes.\n\n'Infohazard', I think, should be used only to refer to the slightly stranger case of information such that the individual themselves would rather not hear it and would say, \"Don't tell me that\".\n\nA spoiler for a book you're in the middle of reading is the classic example of information which is anomalously harmful to you personally. You might pay a penny not to hear it, if the ordinary course of events would expose you to it. People enclose info about the book's ending inside spoilers, if on Reddit or Discord, not because they mean to keep that secret to benefit themselves and harm you - as is the more usual case of secrecy, in human interactions - but because they're trying to protect you from info that would harm you to know.\n\nThe term 'infohazard' has taken on some strange and mystical connotations by being used to refer to 'individually hazardous info'. I am worried about using the same word to also refer to information being alleged to be dangerous in the much more mundane sense of 'collectively destructive info' - info that helps the average individual hearer, but has net negative externalities if lots of people know it.\n\nThis was originally posted by me to Facebook at greater length (linkpost goes to original essay).  Best suggestions ther",
      "wordCount": 379
    },
    "tags": [
      {
        "_id": "7w6XkYe5YPx9YL59j",
        "name": "Information Hazards",
        "slug": "information-hazards"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "AqsjZwxHNqH64C2b6",
    "title": "Let's See You Write That Corrigibility Tag",
    "slug": "let-s-see-you-write-that-corrigibility-tag",
    "url": null,
    "baseScore": 125,
    "voteCount": 66,
    "viewCount": null,
    "commentCount": 70,
    "createdAt": null,
    "postedAt": "2022-06-19T21:11:03.505Z",
    "contents": {
      "markdown": "The [top-rated comment](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities?commentId=HRDoDnHv8bvoW7oPZ) on \"[AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)\" claims that many other people could've written a list like that.\n\n\"Why didn't you challenge anybody else to write up a list like that, if you wanted to make a point of nobody else being able to write it?\" I was asked.\n\nBecause I don't actually think it does any good, or persuades anyone of anything, people don't like tests like that, and I don't really believe in them myself either.  I couldn't pass a test somebody else invented around something *they* found easy to do, for many such possible tests.\n\nBut people asked, so, fine, let's actually try it this time.  Maybe I'm wrong about how bad things are, and will be pleasantly surprised.  If I'm never pleasantly surprised then I'm obviously not being pessimistic enough yet.  \n \n\nSo:  As part of my current fiction-writing project, I'm currently writing a list of some principles that dath ilan's Basement-of-the-World project has invented for describing AGI corrigibility - the sort of principles you'd build into a Bounded Thing meant to carry out some single task or task-class and not destroy the world by doing it.\n\nSo far as I know, every principle of this kind, except for Jessica Taylor's \"quantilization\", and \"myopia\" (not sure who correctly named this as a corrigibility principle), was invented by myself; eg \"low impact\", \"shutdownability\".  (Though I don't particularly think it hopeful if you claim that somebody else has publication priority on \"low impact\" or whatevs, in some stretched or even nonstretched way; ideas on the level of \"low impact\" have always seemed cheap to me to propose, harder to solve before the world ends.)\n\nSome of the items on dath ilan's upcoming list out of my personal glowfic writing have already been written up more seriously by me.  Some haven't.\n\nI'm writing this in one afternoon as one tag in my cowritten online novel about a dath ilani who landed in a D&D country run by Hell.  ~One and a half thousand words or so, maybe~. (2169 words.)\n\nHow about you try to do better than the tag overall, before I publish it, upon the topic of corrigibility principles on the level of \"myopia\" for AGI?  It'll get published in a day or so, possibly later, but *I'm* not going to be spending more than an hour or two polishing it.",
      "plaintextDescription": "The top-rated comment on \"AGI Ruin: A List of Lethalities\" claims that many other people could've written a list like that.\n\n\"Why didn't you challenge anybody else to write up a list like that, if you wanted to make a point of nobody else being able to write it?\" I was asked.\n\nBecause I don't actually think it does any good, or persuades anyone of anything, people don't like tests like that, and I don't really believe in them myself either.  I couldn't pass a test somebody else invented around something they found easy to do, for many such possible tests.\n\nBut people asked, so, fine, let's actually try it this time.  Maybe I'm wrong about how bad things are, and will be pleasantly surprised.  If I'm never pleasantly surprised then I'm obviously not being pessimistic enough yet.\n \n\nSo:  As part of my current fiction-writing project, I'm currently writing a list of some principles that dath ilan's Basement-of-the-World project has invented for describing AGI corrigibility - the sort of principles you'd build into a Bounded Thing meant to carry out some single task or task-class and not destroy the world by doing it.\n\nSo far as I know, every principle of this kind, except for Jessica Taylor's \"quantilization\", and \"myopia\" (not sure who correctly named this as a corrigibility principle), was invented by myself; eg \"low impact\", \"shutdownability\".  (Though I don't particularly think it hopeful if you claim that somebody else has publication priority on \"low impact\" or whatevs, in some stretched or even nonstretched way; ideas on the level of \"low impact\" have always seemed cheap to me to propose, harder to solve before the world ends.)\n\nSome of the items on dath ilan's upcoming list out of my personal glowfic writing have already been written up more seriously by me.  Some haven't.\n\nI'm writing this in one afternoon as one tag in my cowritten online novel about a dath ilani who landed in a D&D country run by Hell.  One and a half thousand words or so, maybe. (2169 words",
      "wordCount": 389
    },
    "tags": [
      {
        "_id": "qHCFjTEWCQjKBWy5z",
        "name": "Corrigibility",
        "slug": "corrigibility-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uMQ3cqWDPHhjtiesc",
    "title": "AGI Ruin: A List of Lethalities",
    "slug": "agi-ruin-a-list-of-lethalities",
    "url": null,
    "baseScore": 951,
    "voteCount": 577,
    "viewCount": null,
    "commentCount": 711,
    "createdAt": null,
    "postedAt": "2022-06-05T22:05:52.224Z",
    "contents": {
      "markdown": "### **Preamble:**\n\n(If you're already familiar with all basics and don't want any preamble, skip ahead to [Section B](#Section_B_) for technical difficulties of alignment proper.)\n\nI have several times failed to write up a well-organized list of reasons why AGI will kill you.  People come in with different ideas about why AGI would be survivable, and want to hear different *obviously key *points addressed first.  Some fraction of those people are loudly upset with me if the obviously most important points aren't addressed immediately, and I address different points first instead.\n\nHaving failed to solve this problem in any good way, I now give up and solve it poorly with a poorly organized list of individual rants.  I'm not particularly happy with this list; the alternative was publishing nothing, and publishing this seems marginally more [dignified](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy).\n\nThree points about the general subject matter of discussion here, numbered so as not to conflict with the list of lethalities:\n\n**-3**.  I'm assuming you are already familiar with some basics, and already know what '[orthogonality](https://arbital.com/p/orthogonality/)' and '[instrumental convergence](https://arbital.com/p/instrumental_convergence/)' are and why they're true.  People occasionally claim to me that I need to stop fighting old wars here, because, those people claim to me, those wars have already been won within the important-according-to-them parts of the current audience.  I suppose it's at least true that none of the current major EA funders seem to be visibly in denial about orthogonality or instrumental convergence as such; so, fine.  If you don't know what 'orthogonality' or 'instrumental convergence' are, or don't see for yourself why they're true, you need a different introduction than this one.\n\n**-2**.  When I say that alignment is lethally difficult, I am not talking about ideal or perfect goals of 'provable' alignment, nor total alignment of superintelligences on exact human values, nor getting AIs to produce satisfactory arguments about moral dilemmas which sorta-reasonable humans disagree about, nor attaining an absolute certainty of an AI not killing everyone.  When I say that alignment is difficult, I mean that in practice, using the techniques we actually have, \"please don't disassemble literally everyone with probability roughly 1\" is an overly large ask that we are not on course to get.  So far as I'm concerned, [if you can get a powerful AGI that carries out some pivotal superhuman engineering task, with a less than fifty percent change of killing more than one billion people](https://twitter.com/ESYudkowsky/status/1070095112791715846), I'll take it.  Even smaller chances of killing even fewer people would be a nice luxury, but if you can get as incredibly far as \"less than roughly certain to kill everybody\", then you can probably get down to under a 5% chance with only slightly more effort.  Practically all of the difficulty is in getting to \"less than certainty of killing literally everyone\".  Trolley problems are not an interesting subproblem in all of this; if there are any survivors, you solved alignment.  At this point, I no longer care how it works, I don't care how you got there, I am cause-agnostic about whatever methodology you used, all I am looking at is prospective results, all I want is that we have justifiable cause to believe of a pivotally useful AGI 'this will not kill literally everyone'.  Anybody telling you I'm asking for stricter 'alignment' than this has failed at reading comprehension.  The big ask from AGI alignment, the basic challenge I am saying is too difficult, is to obtain by any strategy whatsoever a significant chance of there being any survivors.\n\n**-1**.  None of this is about anything being impossible in principle.  The metaphor I usually use is that if a textbook from one hundred years in the future fell into our hands, containing all of the simple ideas *that actually work robustly in practice,* we could probably build an aligned superintelligence in six months.  For people schooled in machine learning, I use as my metaphor the difference between ReLU activations and sigmoid activations.  Sigmoid activations are complicated and fragile, and do a terrible job of transmitting gradients through many layers; ReLUs are incredibly simple (for the unfamiliar, the activation function is literally max(x, 0)) and work much better.  Most neural networks for the first decades of the field used sigmoids; the idea of ReLUs wasn't discovered, validated, and popularized until decades later.  What's lethal is that we do not *have *the Textbook From The Future telling us all the simple solutions that actually in real life just work and are robust; we're going to be doing everything with metaphorical sigmoids on the first critical try.  No difficulty discussed here about AGI alignment is claimed by me to be impossible - to merely human science and engineering, let alone in principle - if we had 100 years to solve it using unlimited retries, the way that science *usually* has an unbounded time budget and unlimited retries.  This list of lethalities is about things *we are not on course to solve in practice in time on the first critical try;* none of it is meant to make a much stronger claim about things that are *impossible in principle.*\n\nThat said:\n\nHere, from my perspective, are some different true things that could be said, to contradict various false things that various different people seem to believe, about why AGI would be survivable on anything remotely remotely resembling the current pathway, or any other pathway we can easily jump to.\n\n### **Section A:**\n\nThis is a very lethal problem, it has to be solved one way or another, it has to be solved at a minimum strength and difficulty level instead of various easier modes that some dream about, we do not have any visible option of 'everyone' retreating to only solve safe weak problems instead, and failing on the first really dangerous try is fatal.\n\n**1**.  Alpha Zero blew past all accumulated human knowledge about Go after a day or so of self-play, with no reliance on human playbooks or sample games.  Anyone relying on \"well, it'll get up to human capability at Go, but then have a hard time getting past that because it won't be able to learn from humans any more\" would have relied on vacuum.  **AGI will not be upper-bounded by human ability or human learning speed**.  **Things much smarter than human would be able to learn from less evidence than humans require** to have ideas driven into their brains; there are theoretical upper bounds here, but those upper bounds seem very high. (Eg, each bit of information that couldn't already be fully predicted can eliminate at most half the probability mass of all hypotheses under consideration.)  It is not naturally (by default, barring intervention) the case that everything takes place on a timescale that makes it easy for us to react.\n\n**2**.  **A cognitive system with sufficiently high cognitive powers, given any medium-bandwidth channel of causal influence, will not find it difficult to bootstrap to overpowering capabilities independent of human infrastructure.**  The concrete example I usually use here is nanotech, because there's been pretty detailed analysis of what definitely look like physically attainable lower bounds on what should be possible with nanotech, and those lower bounds are sufficient to carry the point.  My lower-bound model of \"how a sufficiently powerful intelligence would kill everyone, if it didn't want to not do that\" is that it gets access to the Internet, emails some DNA sequences to any of the many many online firms that will take a DNA sequence in the email and ship you back proteins, and bribes/persuades some human who has no idea they're dealing with an AGI to mix proteins in a beaker, which then form a first-stage nanofactory which can build the actual nanomachinery.  (Back when I was first deploying this visualization, the wise-sounding critics said \"Ah, but how do you know even a superintelligence could solve the protein folding problem, if it didn't already have planet-sized supercomputers?\" but one hears less of this after the advent of AlphaFold 2, for some odd reason.)  The nanomachinery builds diamondoid bacteria, that replicate with solar power and atmospheric CHON, maybe aggregate into some miniature rockets or jets so they can ride the jetstream to spread across the Earth's atmosphere, get into human bloodstreams and hide, strike on a timer.  **Losing a conflict with a high-powered cognitive system looks at least as deadly as \"everybody on the face of the Earth suddenly falls over dead within the same second\".**  (I am using awkward constructions like 'high cognitive power' because standard English terms like 'smart' or 'intelligent' appear to me to function largely as status synonyms.  'Superintelligence' sounds to most people like 'something above the top of the status hierarchy that went to double college', and they don't understand why that would be all that dangerous?  Earthlings have no word and indeed no standard native concept that means 'actually useful cognitive power'.  A large amount of failure to panic sufficiently, seems to me to stem from a lack of appreciation for the incredible potential lethality of this thing that Earthlings as a culture have not named.)\n\n**3**.  **We need to get alignment right on the 'first critical try'** at operating at a 'dangerous' level of intelligence, where **unaligned operation at a dangerous level of intelligence kills everybody on Earth and then we don't get to try again**.  This includes, for example: (a) something smart enough to build a nanosystem which has been explicitly authorized to build a nanosystem; or (b) something smart enough to build a nanosystem and also smart enough to gain unauthorized access to the Internet and pay a human to put together the ingredients for a nanosystem; or (c) something smart enough to get unauthorized access to the Internet and build something smarter than itself on the number of machines it can hack; or (d) something smart enough to treat humans as manipulable machinery and which has any authorized or unauthorized two-way causal channel with humans; or (e) something smart enough to improve itself enough to do (b) or (d); etcetera.  We can gather all sorts of information beforehand *from less powerful systems that will not kill us if we screw up operating them;* but once we are running more powerful systems, we can no longer update on sufficiently catastrophic errors.  This is where practically all of the real lethality comes from, that we have to get things right on the first sufficiently-critical try.  If we had unlimited retries - if every time an AGI destroyed all the galaxies we got to go back in time four years and try again - we would in a hundred years figure out which bright ideas actually worked.  Human beings can figure out pretty difficult things over time, when they get lots of tries; when a failed guess kills literally everyone, that is harder.  That we have to get a bunch of key stuff right *on the first try* is where most of the lethality really and ultimately comes from; likewise the fact that no authority is here to tell us a list of what exactly is 'key' and will kill us if we get it wrong.  (One remarks that most people are so absolutely and flatly unprepared by their 'scientific' educations to challenge pre-paradigmatic puzzles with no scholarly authoritative supervision, that they do not even realize how much harder that is, or how incredibly lethal it is to demand getting that right on the first critical try.)\n\n**4**.  **We can't just \"decide not to build AGI\"** because GPUs are everywhere, and knowledge of algorithms is constantly being improved and published; 2 years after the leading actor has the capability to destroy the world, 5 other actors will have the capability to destroy the world.  **The given lethal challenge is to solve within a time limit,** driven by the dynamic in which, over time, increasingly weak actors with a smaller and smaller fraction of total computing power, become able to build AGI and destroy the world.  Powerful actors all refraining in unison from doing the suicidal thing just delays this time limit - it does not lift it, unless computer hardware and computer software progress are both brought to complete severe halts across the whole Earth.  The current state of this cooperation to have every big actor refrain from doing the stupid thing, is that at present some large actors with a lot of researchers and computing power are led by people who vocally disdain all talk of AGI safety (eg Facebook AI Research).  Note that needing to solve AGI alignment *only* within a time limit, but with unlimited safe retries for rapid experimentation on the full-powered system; or *only* on the first critical try, but with an unlimited time bound; would both be terrifically humanity-threatening challenges by historical standards *individually*.\n\n**5**.  **We can't just build a very weak system**, which is less dangerous because it is so weak, and declare victory; because later there will be more actors that have the capability to build a stronger system and one of them will do so.  I've also in the past called this the 'safe-but-useless' tradeoff, or 'safe-vs-useful'.  People keep on going \"why don't we only use AIs to do X, that seems safe\" and the answer is almost always either \"doing X in fact takes very powerful cognition that is not passively safe\" or, even more commonly, \"because restricting yourself to doing X will not prevent Facebook AI Research from destroying the world six months later\".  If all you need is an object that doesn't do dangerous things, you could try a sponge; a sponge is very passively safe.  Building a sponge, however, does not prevent Facebook AI Research from destroying the world six months later when they catch up to the leading actor.\n\n**6**.  **We need to align the performance of some large task, a 'pivotal act' that prevents other people from building an unaligned AGI that destroys the world.**  While the number of actors with AGI is few or one, they must execute some \"pivotal act\", strong enough to flip the gameboard, using an AGI powerful enough to do that.  It's not enough to be able to align a *weak* system - we need to align a system that can do some single *very large thing.*  The example I usually give is \"burn all GPUs\".  This is not what I think you'd actually want to do with a powerful AGI - the nanomachines would need to operate in an incredibly complicated open environment to hunt down all the GPUs, and that would be needlessly difficult to align.  However, all known pivotal acts are currently outside the Overton Window, and I expect them to stay there.  So I picked an example where if anybody says \"how dare you propose burning all GPUs?\" I can say \"Oh, well, I don't *actually* advocate doing that; it's just a mild overestimate for the rough power level of what you'd have to do, and the rough level of machine cognition required to do that, in order to prevent somebody else from destroying the world in six months or three years.\"  (If it wasn't a mild overestimate, then 'burn all GPUs' would actually be the minimal pivotal task and hence correct answer, and I wouldn't be able to give that denial.)  Many clever-sounding proposals for alignment fall apart as soon as you ask \"How could you use this to align a system that you could use to shut down all the GPUs in the world?\" because it's then clear that the system can't do something that powerful, or, if it can do that, the system wouldn't be easy to align.  A GPU-burner is also a system powerful enough to, and purportedly authorized to, build nanotechnology, so it requires operating in a dangerous domain at a dangerous level of intelligence and capability; and this goes along with any non-fantasy attempt to name a way an AGI could change the world such that a half-dozen other would-be AGI-builders won't destroy the world 6 months later.\n\n**7**.  The reason why nobody in this community has successfully named a 'pivotal weak act' where you do something weak enough with an AGI to be passively safe, but powerful enough to prevent any other AGI from destroying the world a year later - and yet also we can't just go do that right now and need to wait on AI - is that *nothing like that exists*.  There's no reason why it should exist.  There is not some elaborate clever reason why it exists but nobody can see it.  It takes a lot of power to do something to the current world that prevents any other AGI from coming into existence; nothing which can do that is passively safe in virtue of its weakness.  If you can't solve the problem right now (which you can't, because you're opposed to other actors who don't want to be solved and those actors are on roughly the same level as you) then you are resorting to some cognitive system that can do things you could not figure out how to do yourself, that you were not *close* to figuring out because you are not *close *to being able to, for example, burn all GPUs.  Burning all GPUs would *actually* stop Facebook AI Research from destroying the world six months later; weaksauce Overton-abiding stuff about 'improving public epistemology by setting GPT-4 loose on Twitter to provide scientifically literate arguments about everything' will be cool but will not actually prevent Facebook AI Research from destroying the world six months later, or some eager open-source collaborative from destroying the world a year later if you manage to stop FAIR specifically.  **There are no pivotal weak acts**.\n\n**8**.  **The best and easiest-found-by-optimization algorithms for solving problems we want an AI to solve, readily generalize to problems we'd rather the AI not solve**; you can't build a system that only has the capability to drive red cars and not blue cars, because all red-car-driving algorithms generalize to the capability to drive blue cars.\n\n**9**.  The builders of a safe system, by hypothesis on such a thing being possible, would need to operate their system in a regime where it has the *capability* to kill everybody or make itself even more dangerous, but has been successfully designed to not do that.  **Running AGIs doing something pivotal are not passively safe,** they're the equivalent of nuclear cores that require actively maintained design properties to not go supercritical and melt down.\n\n### **Section B:**\n\nOkay, but as we all know, modern machine learning is like a genie where you just give it a wish, right?  Expressed as some mysterious thing called a 'loss function', but which is basically just equivalent to an English wish phrasing, right?  And then if you pour in enough computing power you get your wish, right?  So why not train a giant stack of transformer layers on a dataset of agents doing nice things and not bad things, throw in the word 'corrigibility' somewhere, crank up that computing power, and get out an aligned AGI?\n\n**Section B.1:  The distributional leap.** \n\n**10**.  You can't train alignment by running lethally dangerous cognitions, observing whether the outputs kill or deceive or corrupt the operators, assigning a loss, and doing supervised learning.  **On anything like the standard ML paradigm, you would need to somehow generalize optimization-for-alignment you did in safe conditions, across a big distributional shift to dangerous conditions**.  (Some generalization of this seems like it would have to be true even outside that paradigm; you wouldn't be working on a live unaligned superintelligence to align it.)  This alone is a point that is sufficient to kill a lot of naive proposals from people who never did or could concretely sketch out any specific scenario of what training they'd do, in order to align what output - which is why, of course, they never concretely sketch anything like that.  **Powerful AGIs doing dangerous things that will kill you if misaligned, must have an alignment property that generalized far out-of-distribution from safer building/training operations that didn't kill you.**  This is where a huge amount of lethality comes from on anything remotely resembling the present paradigm.  Unaligned operation at a dangerous level of intelligence\\*capability will kill you; so, if you're starting with an unaligned system and labeling outputs in order to get it to learn alignment, the training regime or building regime must be operating at some lower level of intelligence\\*capability that is passively safe, where its currently-unaligned operation does not pose any threat.  (Note that anything substantially smarter than you poses a threat given *any* realistic level of capability.  Eg, \"being able to produce outputs that humans look at\" is probably sufficient for a generally much-smarter-than-human AGI to [navigate its way out of the causal systems that are humans](https://www.yudkowsky.net/singularity/aibox), especially in the real world where somebody trained the system on terabytes of Internet text, rather than somehow keeping it ignorant of the latent causes of its source code and training environments.)\n\n**11**.  If cognitive machinery doesn't generalize far out of the distribution where you did tons of training, it can't solve problems on the order of 'build nanotechnology' where it would be too expensive to run a million training runs of failing to build nanotechnology.  There is no pivotal act this weak; **there's no known case where you can entrain a safe level of ability on a safe environment where you can cheaply do millions of runs, and deploy that capability to save the world** and prevent the next AGI project up from destroying the world two years later.  Pivotal weak acts like this aren't known, and not for want of people looking for them.  So, again, you end up needing alignment to generalize way out of the training distribution - not just because the training environment needs to be safe, but because the training environment probably also needs to be *cheaper* than evaluating some real-world domain in which the AGI needs to do some huge act.  You don't get 1000 failed tries at burning all GPUs - because people will notice, even leaving out the consequences of capabilities success and alignment failure.\n\n**12**.  **Operating at a highly intelligent level is a drastic shift in distribution from operating at a less intelligent level**, opening up new external options, and probably opening up even more new internal choices and modes.  Problems that materialize at high intelligence and danger levels may fail to show up at safe lower levels of intelligence, or may recur after being suppressed by a first patch.\n\n**13**.  **Many alignment problems of superintelligence will not naturally appear at pre-dangerous, passively-safe levels of capability**.  Consider the internal behavior 'change your outer behavior to deliberately look more aligned and deceive the programmers, operators, and possibly any loss functions optimizing over you'.  This problem is one that will appear at the superintelligent level; if, being otherwise ignorant, we guess that it is among the *median* such problems in terms of how *early* it naturally appears in earlier systems, then around *half* of the alignment problems of superintelligence will first naturally materialize *after *that one first starts to appear.  Given *correct *foresight of which problems will naturally materialize *later,* one could try to deliberately materialize such problems earlier, and get in some observations of them.  This helps to the extent (a) that we actually correctly forecast all of the problems that will appear later, or some superset of those; (b) that we succeed in preemptively materializing a superset of problems that will appear later; and (c) that we can actually solve, in the earlier laboratory that is out-of-distribution for us relative to the real problems, those alignment problems that would be lethal if we mishandle them when they materialize later.  Anticipating *all *of the really dangerous ones, and then successfully materializing them, in the correct form for early solutions to generalize over to later solutions, *sounds possibly kinda hard*.\n\n**14**.  **Some problems**, like 'the AGI has an option that (looks to it like) it could successfully kill and replace the programmers to fully optimize over its environment', **seem like their natural order of appearance could be that they first appear only in fully dangerous domains**.  Really actually having a *clear* option to brain-level-persuade the operators or escape onto the Internet, build nanotech, and destroy all of humanity - in a way where you're fully clear that you know the relevant facts, and estimate only a not-worth-it low probability of learning something which changes your preferred strategy if you bide your time another month while further growing in capability - is an option that first gets evaluated for real at the point where an AGI fully expects it can defeat its creators.  We can try to manifest an echo of that apparent scenario in earlier toy domains.  Trying to train by gradient descent against that behavior, in that toy domain, is something I'd expect to produce not-particularly-coherent local patches to thought processes, which would break with near-certainty inside a superintelligence generalizing far outside the training distribution and thinking very different thoughts.  Also, programmers and operators themselves, who are used to operating in not-fully-dangerous domains, are operating out-of-distribution when they enter into dangerous ones; our methodologies may at that time break.\n\n**15**.  **Fast capability gains seem likely, and may break lots of previous alignment-required invariants simultaneously.** Given otherwise insufficient foresight by the operators, I'd expect a lot of those problems to appear approximately simultaneously after a sharp capability gain.  See, again, the case of human intelligence.  We didn't break alignment with the 'inclusive reproductive fitness' outer loss function, immediately after the introduction of farming - something like 40,000 years into a 50,000 year Cro-Magnon takeoff, as was itself running very quickly relative to the outer optimization loop of natural selection.  Instead, we got a lot of technology more advanced than was in the ancestral environment, including contraception, in one very fast burst relative to the speed of the outer optimization loop, late in the general intelligence game.  We started reflecting on ourselves a lot more, started being programmed a lot more by cultural evolution, and lots and lots of assumptions underlying our alignment in the ancestral training environment broke simultaneously.  (People will perhaps rationalize reasons why this abstract description doesn't carry over to gradient descent; eg, “gradient descent has less of an information bottleneck”.  My model of this variety of reader has an inside view, which they will label an outside view, that assigns great relevance to some other data points that are *not* observed cases of an outer optimization loop producing an inner general intelligence, and assigns little importance to our one data point actually featuring the phenomenon in question.  When an outer optimization loop actually produced general intelligence, it broke alignment after it turned general, and did so relatively late in the game of that general intelligence accumulating capability and knowledge, almost immediately before it turned 'lethally' dangerous relative to the outer optimization loop of natural selection.  Consider skepticism, if someone is ignoring this one warning, especially if they are not presenting equally lethal and dangerous things that they say will go wrong instead.)\n\n**Section B.2:  Central difficulties of outer and inner alignment.** \n\n**16**. Even if you train really hard on an exact loss function, that doesn't thereby create an explicit internal representation of the loss function inside an AI that then continues to pursue that exact loss function in distribution-shifted environments.  Humans don't explicitly pursue inclusive genetic fitness; **outer optimization even on a very exact, very simple loss function doesn't produce inner optimization in that direction**.  This happens *in practice in real life, *it is what happened in *the only case we know about*, and it seems to me that there are deep theoretical reasons to expect it to happen again: the *first *semi-outer-aligned solutions found, in the search ordering of a real-world bounded optimization process, are not inner-aligned solutions.  This is sufficient on its own, even ignoring many other items on this list, to trash entire categories of naive alignment proposals which assume that if you optimize a bunch on a loss function calculated using some simple concept, you get perfect inner alignment on that concept.\n\n**17**.  More generally, a superproblem of 'outer optimization doesn't produce inner alignment' is that **on the current optimization paradigm there is no general idea of how to get particular inner properties into a system, or verify that they're there, rather than just observable outer ones you can run a loss function over.**  This is a problem when you're trying to generalize out of the original training distribution, because, eg, the outer behaviors you see could have been produced by an inner-misaligned system that is deliberately producing outer behaviors that will fool you.  We don't know how to get any bits of information into the *inner* system rather than the *outer* behaviors, in any systematic or general way, on the current optimization paradigm.\n\n**18**.  **There's no reliable Cartesian-sensory ground truth** (reliable loss-function-calculator) **about whether an output is 'aligned'**, because some outputs destroy (or fool) the human operators and produce a different environmental causal chain behind the externally-registered loss function.  That is, if you show an agent a reward signal that's currently being generated by humans, the signal is not *in general* a *reliable perfect ground truth* about *how aligned an action was*, because another way of producing a high reward signal is to deceive, corrupt, or replace the human operators with a different causal system which generates that reward signal.  When you show an agent an environmental reward signal, you are not showing it something that is a reliable ground truth about whether the system did the thing you wanted it to do; *even if* it ends up perfectly inner-aligned on that reward signal, or learning some concept that *exactly* corresponds to 'wanting states of the environment which result in a high reward signal being sent', an AGI strongly optimizing on that signal will kill you, because the sensory reward signal was not a ground truth about alignment (as seen by the operators).\n\n**19**.  More generally, **there is no known way to use the paradigm of loss functions, sensory inputs, and/or reward inputs, to optimize anything within a cognitive system to point at particular things within the environment** \\- to point to *latent events and objects and properties in the environment,* rather than *relatively shallow functions of the sense data and reward.*  This isn't to say that nothing in the system’s goal (whatever goal accidentally ends up being inner-optimized over) could ever point to anything in the environment by *accident*.  Humans ended up pointing to their environments at least partially, though we've got lots of internally oriented motivational pointers as well.  But insofar as the current paradigm works at all, the on-paper design properties say that it only works for aligning on known direct functions of sense data and reward functions.  All of these kill you if optimized-over by a sufficiently powerful intelligence, because they imply strategies like 'kill everyone in the world using nanotech to strike before they know they're in a battle, and have control of your reward button forever after'.  It just isn't *true* that we know a function on webcam input such that every world with that webcam showing the right things is safe for us creatures outside the webcam.  This general problem is a fact about the territory, not the map; it's a fact about the actual environment, not the particular optimizer, that lethal-to-us possibilities exist in some possible environments underlying every given sense input.\n\n**20**.  Human operators are fallible, breakable, and manipulable.  **Human raters make systematic errors - regular, compactly describable, predictable errors**.  To *faithfully* learn a function from 'human feedback' is to learn (from our external standpoint) an unfaithful description of human preferences, with errors that are not random (from the outside standpoint of what we'd hoped to transfer).  If you perfectly learn and perfectly maximize *the referent of* rewards assigned by human operators, that kills them.  It's a fact about the territory, not the map - about the environment, not the optimizer - that the *best predictive* explanation for human answers is one that predicts the systematic errors in our responses, and therefore is a psychological concept that correctly predicts the higher scores that would be assigned to human-error-producing cases.\n\n**21**.  There's something like a single answer, or a single bucket of answers, for questions like 'What's the environment really like?' and 'How do I figure out the environment?' and 'Which of my possible outputs interact with reality in a way that causes reality to have certain properties?', where a simple outer optimization loop will straightforwardly shove optimizees into this bucket.  When you have a wrong belief, reality hits back at your wrong predictions.  When you have a broken belief-updater, reality hits back at your broken predictive mechanism via predictive losses, and a gradient descent update fixes the problem in a simple way that can easily cohere with all the other predictive stuff.  In contrast, when it comes to a choice of utility function, there are unbounded degrees of freedom and multiple reflectively coherent fixpoints.  Reality doesn't 'hit back' against things that are locally aligned with the loss function on a particular range of test cases, but globally misaligned on a wider range of test cases.  This is the very abstract story about why hominids, once they finally started to generalize, generalized their *capabilities* to Moon landings, but their inner optimization no longer adhered very well to the outer-optimization goal of 'relative inclusive reproductive fitness' - even though they were in their ancestral environment optimized very strictly around this one thing and nothing else.  This abstract dynamic is something you'd expect to be true about outer optimization loops on the order of both 'natural selection' and 'gradient descent'.  The central result:  **Capabilities generalize further than alignment once capabilities start to generalize far**.\n\n**22**.  There's a relatively simple core structure that explains why complicated cognitive machines work; which is why such a thing as general intelligence exists and not just a lot of unrelated special-purpose solutions; which is why capabilities generalize after outer optimization infuses them into something that has been optimized enough to become a powerful inner optimizer.  The fact that this core structure is simple and relates generically to [low-entropy high-structure environments](https://intelligence.org/2017/12/06/chollet/) is why humans can walk on the Moon.  **There is no analogous truth about there being a simple core of alignment**, especially not one that is *even easier* for gradient descent to find than it would have been for natural selection to just find 'want inclusive reproductive fitness' as a well-generalizing solution within ancestral humans.  Therefore, capabilities generalize further out-of-distribution than alignment, once they start to generalize at all.\n\n**23**.  **Corrigibility is anti-natural to consequentialist reasoning**; \"you can't bring the coffee if you're dead\" for almost every kind of coffee.  We (MIRI) [tried and failed](https://www.alignmentforum.org/posts/5bd75cc58225bf0670374f04/forum-digest-corrigibility-utility-indifference-and-related-control-ideas) to find a coherent formula for an agent that would let itself be shut down (without that agent actively trying to get shut down).  Furthermore, many anti-corrigible lines of reasoning like this may only first appear at high levels of intelligence.\n\n**24**.  There are two fundamentally different approaches you can potentially take to alignment, which are unsolvable for two different sets of reasons; therefore, **by becoming confused and ambiguating between the two approaches, you can confuse yourself about whether alignment is necessarily difficult**.  The first approach is to build a CEV-style Sovereign which wants exactly what we extrapolated-want and is therefore safe to let optimize all the future galaxies without it accepting any human input trying to stop it.  The second course is to build corrigible AGI which doesn't want exactly what we want, and yet somehow fails to kill us and take over the galaxies despite that being a convergent incentive there.\n\n1.  The first thing generally, or CEV specifically, is unworkable because **the complexity of what needs to be aligned or meta-aligned for our Real Actual Values is far out of reach for our FIRST TRY at AGI**.  Yes I mean specifically that the *dataset, meta-learning algorithm, and what needs to be learned,* is far out of reach for our first try.  It's not just non-hand-codable, it is *unteachable *on-the-first-try because *the thing you are trying to teach is too weird and complicated.*\n2.  The second thing looks unworkable (less so than CEV, but still lethally unworkable) because **corrigibility runs *****actively counter*** **to instrumentally convergent behaviors** within a core of general intelligence (the capability that generalizes far out of its original distribution).  You're not trying to make it have an opinion on something the core was previously neutral on.  You're trying to take a system implicitly trained on lots of arithmetic problems until its machinery started to reflect the common coherent core of arithmetic, and get it to say that as a special case 222 + 222 = 555.  You can maybe train something to do this in a particular training distribution, but it's incredibly likely to break when you present it with new math problems far outside that training distribution, on a system which successfully generalizes capabilities that far at all.\n\n**Section B.3:  Central difficulties of *****sufficiently*** ***good and useful*** **transparency / interpretability.**\n\n**25**.  **We've got no idea what's actually going on inside the giant inscrutable matrices and tensors of floating-point numbers**.  Drawing interesting graphs of where a transformer layer is focusing attention doesn't help if the question that needs answering is \"So was it planning how to kill us or not?\"\n\n**26**.  Even if we did know what was going on inside the giant inscrutable matrices while the AGI was still too weak to kill us, this would just result in us dying with more dignity, if DeepMind refused to run that system and let Facebook AI Research destroy the world two years later.  **Knowing that a medium-strength system of inscrutable matrices is planning to kill us, does not thereby let us build a high-strength system of inscrutable matrices that isn't planning to kill us**.\n\n**27**.  When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.  **Optimizing against an interpreted thought optimizes against interpretability**.\n\n**28**.  The AGI is smarter than us in whatever domain we're trying to operate it inside, so we cannot mentally check all the possibilities it examines, and we cannot see all the consequences of its outputs using our own mental talent.  **A powerful AI searches parts of the option space we don't, and we can't foresee all its options**.\n\n**29**.  The outputs of an AGI go through a huge, not-fully-known-to-us domain (the real world) before they have their real consequences.  **Human beings cannot inspect an AGI's output to determine whether the consequences will be good**.\n\n**30**.  Any pivotal act that is not something we can go do right now, will take advantage of the AGI figuring out things about the world we don't know so that it can make plans we wouldn't be able to make ourselves.  It knows, at the least, the fact we didn't previously know, that some action sequence results in the world we want.  Then humans will not be competent to use their own knowledge of the world to figure out all the results of that action sequence.  An AI whose action sequence you can fully understand all the effects of, before it executes, is much weaker than humans in that domain; you couldn't make the same guarantee about an unaligned human as smart as yourself and trying to fool you.  **There is no pivotal output of an AGI that is humanly checkable and can be used to safely save the world but only after checking it**; this is another form of pivotal weak act which does not exist.\n\n**31**.  A strategically aware intelligence can choose its visible outputs to have the consequence of deceiving you, including about such matters as whether the intelligence has acquired strategic awareness; **you can't rely on behavioral inspection to determine facts about an AI which that AI might want to deceive you about**.  (Including how smart it is, or whether it's acquired strategic awareness.)\n\n**32**.  Human thought partially exposes only a partially scrutable outer surface layer.  Words only trace our real thoughts.  Words are not an AGI-complete data representation in its native style.  The underparts of human thought are not exposed for direct imitation learning and can't be put in any dataset.  **This makes it hard and probably impossible to train a powerful system entirely on imitation of human words or other human-legible contents**, which are only impoverished subsystems of human thoughts; ***unless *****that system is powerful enough to contain inner intelligences figuring out the humans**, and at that point it is no longer really working as imitative human thought.\n\n**33**.  **The AI does not think like you do**, the AI doesn't have thoughts built up from the same concepts you use, it is utterly alien on a staggering scale.  Nobody knows what the hell GPT-3 is thinking, not *only* because the matrices are opaque, but because the *stuff within that opaque container *is, very likely, incredibly alien - nothing that would translate well into comprehensible human thinking, even if we could see past the giant wall of floating-point numbers to what lay behind.\n\n**Section B.4:  Miscellaneous unworkable schemes.** \n\n**34**.  **Coordination schemes between superintelligences are not things that humans can participate in** (eg because humans can't reason reliably about the code of superintelligences); a \"multipolar\" system of 20 superintelligences with different utility functions, plus humanity, has a natural and obvious equilibrium which looks like \"the 20 superintelligences cooperate with each other but not with humanity\".\n\n**35**.  Schemes for playing \"different\" AIs off against each other stop working if those AIs advance to the point of being able to coordinate via reasoning about (probability distributions over) each others' code.  **Any system of sufficiently intelligent agents can probably behave as a single agent, even if you imagine you're playing them against each other.**  Eg, if you set an AGI that is secretly a paperclip maximizer, to check the output of a nanosystems designer that is secretly a staples maximizer, then even if the nanosystems designer is not able to deduce what the paperclip maximizer really wants (namely paperclips), it could still logically commit to share half the universe with any agent checking its designs if those designs were allowed through, *if *the checker-agent can verify the suggester-system's logical commitment and hence logically depend on it (which excludes human-level intelligences).  Or, if you prefer simplified catastrophes without any logical decision theory, the suggester could bury in its nanosystem design the code for a new superintelligence that will visibly (to a superhuman checker) divide the universe between the nanosystem designer and the design-checker.\n\n**36**.  What makes an air conditioner 'magic' from the perspective of say the thirteenth century, is that even if you correctly show them the design of the air conditioner in advance, they won't be able to understand from seeing that design why the air comes out cold; the design is exploiting regularities of the environment, rules of the world, laws of physics, that they don't know about.  The domain of human thought and human brains is very poorly understood by us, and exhibits phenomena like optical illusions, hypnosis, psychosis, mania, or simple afterimages produced by strong stimuli in one place leaving neural effects in another place.  Maybe a superintelligence couldn't defeat a human in a very simple realm like logical tic-tac-toe; if you're fighting it in an incredibly complicated domain you understand poorly, like human minds, you should expect to be defeated by 'magic' in the sense that even if you saw its strategy you would not understand why that strategy worked.  **AI-boxing can only work on relatively weak AGIs; the human operators are not secure systems**.\n\n### **Section C:**\n\nOkay, those are some significant problems, but lots of progress is being made on solving them, right?  There's a whole field calling itself \"AI Safety\" and many major organizations are expressing Very Grave Concern about how \"safe\" and \"ethical\" they are?\n\n**37**.  There's a pattern that's played out quite often, over all the times the Earth has spun around the Sun, in which some bright-eyed young scientist, young engineer, young entrepreneur, proceeds in full bright-eyed optimism to challenge some problem that turns out to be really quite difficult.  Very often the cynical old veterans of the field try to warn them about this, and the bright-eyed youngsters don't listen, because, like, who wants to hear about all that stuff, they want to go solve the problem!  Then this person gets beaten about the head with a slipper by reality as they find out that their brilliant speculative theory is wrong, it's actually really hard to build the thing because it keeps breaking, and society isn't as eager to adopt their clever innovation as they might've hoped, in a process which eventually produces a new cynical old veteran.  Which, if not literally optimal, is I suppose a nice life cycle to nod along to in a nature-show sort of way.  Sometimes you do something for the *first* time and there *are* no cynical old veterans to warn anyone and people can be *really* optimistic about how it will go; eg the initial Dartmouth Summer Research Project on Artificial Intelligence in 1956:  \"An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.\"  This is *less *of a viable survival plan for your *planet* if the first major failure of the bright-eyed youngsters kills *literally everyone* before they can predictably get beaten about the head with the news that there were all sorts of unforeseen difficulties and reasons why things were hard.  You don't get any cynical old veterans, in this case, because everybody on Earth is dead.  Once you start to suspect you're in that situation, you have to do the Bayesian thing and update now to the view you will predictably update to later: realize you're in a situation of being that bright-eyed person who is going to encounter Unexpected Difficulties later and end up a cynical old veteran - or would be, except for the part where you'll be dead along with everyone else.  And become that cynical old veteran *right away,* before reality whaps you upside the head in the form of everybody dying and you not getting to learn.  **Everyone else seems to feel that, so long as reality hasn't whapped them upside the head yet and smacked them down with the actual difficulties, they're free to go on living out the standard life-cycle and play out their role in the script and go on being bright-eyed youngsters; there's no cynical old veterans to warn them otherwise, after all, and there's no proof that everything won't go beautifully easy and fine, *****given their bright-eyed total ignorance of what those later difficulties could be.***\n\n**38**.  **It does not appear to me that the field of 'AI safety' is currently being remotely productive on tackling its enormous lethal problems.**  These problems are in fact out of reach; the contemporary field of AI safety has been selected to contain people who go to work in that field anyways.  Almost all of them are there to tackle problems on which they can appear to succeed and publish a paper claiming success; if they can do that and get funded, why would they embark on a much more unpleasant project of trying something harder that they'll fail at, just so the human species can die with marginally more dignity?  This field is not making real progress and does not have a recognition function to distinguish real progress if it took place.  You could pump a billion dollars into it and it would produce mostly noise to drown out what little progress was being made elsewhere.\n\n**39**.  **I figured this stuff out using the **[**null string**](https://twitter.com/ESYudkowsky/status/1500863629490544645) **as input,** and frankly, I have a hard time myself feeling hopeful about getting real alignment work out of somebody who previously sat around waiting for somebody else to input a persuasive argument into them.  This ability to \"notice lethal difficulties without Eliezer Yudkowsky arguing you into noticing them\" currently is an opaque piece of cognitive machinery to me, I do not know how to train it into others.  It probably relates to '[security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)', and a mental motion where you refuse to play out scripts, and being able to operate in a field that's in a state of chaos.\n\n**40**.  \"Geniuses\" with nice legible accomplishments in fields with tight feedback loops where it's easy to determine which results are good or bad right away, and so validate that this person is a genius, are (a) people who might not be able to do equally great work away from tight feedback loops, (b) people who chose a field where their genius would be nicely legible even if that maybe wasn't the place where humanity most needed a genius, and (c) probably don't have the mysterious gears simply because they're *rare.*  **You cannot just pay $5 million apiece to a bunch of legible geniuses from other fields and expect to get great alignment work out of them.**  They probably do not know where the real difficulties are, they probably do not understand what needs to be done, *they cannot tell the difference between good and bad work*, and the funders also can't tell without me standing over their shoulders evaluating everything, which I do not have the physical stamina to do.  I concede that real high-powered talents, especially if they're still in their 20s, genuinely interested, and have done their reading, are people who, yeah, fine, have higher probabilities of making core contributions than a random bloke off the street. But I'd have more hope - not significant hope, but *more *hope - in separating the concerns of (a) credibly promising to pay big money retrospectively for good work to anyone who produces it, and (b) venturing prospective payments to somebody who is predicted to maybe produce good work later.\n\n**41**.  **Reading this document cannot make somebody a core alignment researcher**.  That requires, not the ability to read this document and nod along with it, but the ability to spontaneously write it from scratch without anybody else prompting you; that is what makes somebody a peer of its author.  It's guaranteed that some of my analysis is mistaken, though not necessarily in a hopeful direction.  The ability to do new basic work noticing and fixing those flaws is the same ability as the ability to write this document before I published it, which nobody apparently did, despite my having had other things to do than write this up for the last five years or so.  Some of that silence may, possibly, optimistically, be due to nobody else in this field having the ability to write things comprehensibly - such that somebody out there had the knowledge to write all of this themselves, if they could only have written it up, but they couldn't write, so didn't try.  I'm not particularly hopeful of this turning out to be true in real life, but I suppose it's one possible place for a \"positive model violation\" (miracle).  The fact that, twenty-one years into my entering this death game, seven years into other EAs noticing the death game, and two years into even normies starting to notice the death game, it is still Eliezer Yudkowsky writing up this list, says that humanity still has only one gamepiece that can do that.  I knew I did not actually have the physical stamina to be a star researcher, I tried really really hard to replace myself before my health deteriorated further, and yet here I am writing this.  That's not what surviving worlds look like.\n\n**42**.  **There's no plan.**  Surviving worlds, by this point, and in fact several decades earlier, have a plan for how to survive.  It is a written plan.  The plan is not secret.  In this non-surviving world, there are no candidate plans that do not immediately fall to Eliezer instantly pointing at the giant visible gaping holes in that plan.  Or if you don't know who Eliezer is, you don't even realize you need a plan, because, like, how would a human being possibly realize that without Eliezer yelling at them?  It's not like people will yell at *themselves* about prospective alignment difficulties, they don't have an *internal* voice of caution.  So most organizations don't have plans, because I haven't taken the time to personally yell at them.  'Maybe we should have a plan' is deeper alignment mindset than they possess without me standing constantly on their shoulder as their personal angel pleading them into... continued noncompliance, in fact.  Relatively few are aware even that they should, to look better, produce a *pretend* plan that can fool EAs too '[modest](https://equilibriabook.com/toc/)' to trust their own judgments about seemingly gaping holes in what serious-looking people apparently believe.\n\n**43**.  **This situation you see when you look around you is not what a surviving world looks like.**  The worlds of humanity that survive have plans.  They are not leaving to one tired guy with health problems the entire responsibility of pointing out real and lethal problems proactively.  Key people are taking internal and real responsibility for finding flaws in their own plans, instead of considering it their job to propose solutions and somebody else's job to prove those solutions wrong.  That world started trying to solve their important lethal problems earlier than this.  Half the people going into string theory shifted into AI alignment instead and made real progress there.  When people suggest a planetarily-lethal problem that might materialize later - there's a lot of people suggesting those, in the worlds destined to live, and they don't have a special status in the field, it's just what normal geniuses there do - they're met with either solution plans or a reason why that shouldn't happen, not an uncomfortable shrug and 'How can you be sure that will happen' / 'There's no way you could be sure of that now, we'll have to wait on experimental evidence.'\n\nA lot of those better worlds will die anyways.  It's a genuinely difficult problem, to solve something like that on your first try.  But they'll die with more dignity than this.",
      "plaintextDescription": "Preamble:\n(If you're already familiar with all basics and don't want any preamble, skip ahead to Section B for technical difficulties of alignment proper.)\n\nI have several times failed to write up a well-organized list of reasons why AGI will kill you.  People come in with different ideas about why AGI would be survivable, and want to hear different obviously key points addressed first.  Some fraction of those people are loudly upset with me if the obviously most important points aren't addressed immediately, and I address different points first instead.\n\nHaving failed to solve this problem in any good way, I now give up and solve it poorly with a poorly organized list of individual rants.  I'm not particularly happy with this list; the alternative was publishing nothing, and publishing this seems marginally more dignified.\n\nThree points about the general subject matter of discussion here, numbered so as not to conflict with the list of lethalities:\n\n-3.  I'm assuming you are already familiar with some basics, and already know what 'orthogonality' and 'instrumental convergence' are and why they're true.  People occasionally claim to me that I need to stop fighting old wars here, because, those people claim to me, those wars have already been won within the important-according-to-them parts of the current audience.  I suppose it's at least true that none of the current major EA funders seem to be visibly in denial about orthogonality or instrumental convergence as such; so, fine.  If you don't know what 'orthogonality' or 'instrumental convergence' are, or don't see for yourself why they're true, you need a different introduction than this one.\n\n-2.  When I say that alignment is lethally difficult, I am not talking about ideal or perfect goals of 'provable' alignment, nor total alignment of superintelligences on exact human values, nor getting AIs to produce satisfactory arguments about moral dilemmas which sorta-reasonable humans disagree about, nor attaining an abs",
      "wordCount": 9087
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "JX69nZB8tfxnx5nGH",
        "name": "Threat Models (AI)",
        "slug": "threat-models-ai"
      },
      {
        "_id": "KQP7fNjin8Zqg4N2x",
        "name": "Double-Crux",
        "slug": "double-crux"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb0e9",
        "name": "Fuzzies",
        "slug": "fuzzies"
      },
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "keiYkaeoLHoKK4LYA",
    "title": "Six Dimensions of Operational Adequacy in AGI Projects",
    "slug": "six-dimensions-of-operational-adequacy-in-agi-projects",
    "url": null,
    "baseScore": 317,
    "voteCount": 126,
    "viewCount": null,
    "commentCount": 66,
    "createdAt": null,
    "postedAt": "2022-05-30T17:00:30.833Z",
    "contents": {
      "markdown": "<table><tbody><tr><td style=\"background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Editor's note:&nbsp; </strong>The following is a lightly edited copy of a document written by Eliezer Yudkowsky in November 2017. Since this is a snapshot of Eliezer’s thinking at a specific time, we’ve sprinkled reminders throughout that this is from 2017.</p><p>A background note:</p><p>It’s often the case that people are slow to abandon obsolete playbooks in response to a novel challenge. And AGI is certainly a very novel challenge.</p><p>Italian general Luigi Cadorna offers a memorable historical example. In the Isonzo Offensive of World War I, Cadorna lost hundreds of thousands of men in futile frontal assaults against enemy trenches defended by barbed wire and machine guns.&nbsp; As morale plummeted and desertions became epidemic, Cadorna began executing his own soldiers en masse, in an attempt to cure the rest of their “cowardice.” The offensive continued for <i>2.5 years</i>.</p><p>Cadorna made many mistakes, but foremost among them was his refusal to recognize that this war was fundamentally unlike those that had come before.&nbsp; Modern weaponry had forced a paradigm shift, and Cadorna’s instincts were not merely miscalibrated—they were systematically broken.&nbsp; No number of small, incremental updates within his obsolete framework would be sufficient to meet the new challenge.</p><p>Other examples of this type of mistake include the initial response of the record industry to iTunes and streaming; or, more seriously, the response of most Western governments to COVID-19.</p><p>&nbsp;</p><p><img style=\"width:100%\" src=\"https://lh6.googleusercontent.com/esVjmQ7DejZFFK1lL4P56PZ4rL4zJuk-F8S_EzOqqULg8jAR1vKUphJVvyuCbJkpARkD4c0c6-qP-xYIAZeC6Qze9D_R05LipcZJNBamwZHqU3AYt6KVL3gRfE_itLOa9FvoRtyPaq_sOdQILw\"></p><p>&nbsp;</p><p>As usual, the real challenge of reference class forecasting is figuring out which reference class the thing you’re trying to model belongs to.</p><p>For most problems, rethinking your approach from the ground up is wasteful and unnecessary, because most problems have a similar causal structure to a large number of past cases. When the problem isn’t commensurate with existing strategies, as in the case of AGI, you need a new playbook.</p></td></tr></tbody></table>\n\n  \n  \nI've sometimes been known to complain, or in a polite way scream in utter terror, that \"there is no good guy group in AGI\", i.e., if a researcher on this Earth currently wishes to contribute to the common good, there are literally zero projects they can join and no project close to being joinable.  In its present version, this document is an informal response to an AI researcher who asked me to list out the qualities of such a \"good project\".\n\nIn summary, a \"good project\" needs:\n\n*   *Trustworthy command:*  A trustworthy chain of command with respect to both legal and pragmatic control of the intellectual property (IP) of such a project; a running AGI being included as \"IP\" in this sense.\n*   *Research closure:*  The organizational ability to *close* and/or *silo* IP to within a trustworthy section and prevent its release by sheer default.\n*   *Strong opsec:*  Operational security adequate to prevent the proliferation of code (or other information sufficient to recreate code within e.g. 1 year) due to e.g. Russian intelligence agencies grabbing the code.\n*   *Common good commitment:* The project's command and its people must have a credible commitment to both short-term and long-term goodness.  Short-term goodness comprises the immediate welfare of present-day Earth; long-term goodness is the achievement of transhumanist astronomical goods.\n*   *Alignment mindset:* Somebody on the project needs deep enough [security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) plus understanding of AI cognition that they can originate new, deep measures to ensure AGI alignment; and they must be in a position of technical control or otherwise have effectively unlimited political capital.  Everybody on the project needs to understand and expect that aligning an AGI will be terrifically difficult and terribly dangerous.\n*   *Requisite resource levels:* The project must have adequate resources to compete at the frontier of AGI development, including whatever mix of computational resources, intellectual labor, and closed insights are required to produce a 1+ year lead over less cautious competing projects.\n\nI was asked what would constitute \"minimal, adequate, and good\" performance on each of these dimensions.  I tend to divide things sharply into \"not adequate\" and \"adequate\" but will try to answer in the spirit of the question nonetheless.\n\n**Trustworthy command**\n-----------------------\n\n**Token:**  Not having pragmatic and legal power in the hands of people who are opposed to the very idea of trying to align AGI, or who want an AGI in every household, or who are otherwise allergic to the *easy* parts of AGI strategy.\n\nE.g.: Larry Page begins with the correct view that [cosmopolitan](https://arbital.com/p/value_cosmopolitan/) values are good, speciesism is bad, it would be wrong to mistreat sentient beings just because they're implemented in silicon instead of carbon, and so on. But he then proceeds to reject the idea that goals and capabilities are [orthogonal](https://arbital.com/p/orthogonality/), that instrumental strategies are [convergent](https://arbital.com/p/instrumental_convergence/), and that value is [complex and fragile](https://arbital.com/p/complexity_of_value/). As a consequence, he expects AGI to automatically be friendly, and is liable to object to any effort to align AI [as an attempt to keep AI \"chained up\"](https://books.google.com/books?id=2hIcDgAAQBAJ&pg=PA32&lpg=PA32&dq=Larry+%22that+digital+life+is+the+natural+and+desirable+next+step+in+the+cosmic+evolution+and+that+if+we+let+digital+minds+be+free+rather+than+try+to+stop+or+enslave+them+the+outcome+is+almost+certain+to+be+good%22&source=bl&ots=DIQP9C1EgF&sig=ACfU3U04K3r-b1kQqEvWF71-1Oo4ppsZsw&hl=en&sa=X&ved=2ahUKEwiFrvi6-K3gAhUHwlQKHc83AhgQ6AEwAXoECAkQAQ#v=onepage&q=Larry%20%22that%20digital%20life%20is%20the%20natural%20and%20desirable%20next%20step%20in%20the%20cosmic%20evolution%20and%20that%20if%20we%20let%20digital%20minds%20be%20free%20rather%20than%20try%20to%20stop%20or%20enslave%20them%20the%20outcome%20is%20almost%20certain%20to%20be%20good%22&f=false).\n\nOr, e.g.: As of December 2015, Elon Musk not only wasn’t on board with closure, but apparently [wanted to *open-source*](https://medium.com/backchannel/how-elon-musk-and-y-combinator-plan-to-stop-computers-from-taking-over-17e0e27dd02a) superhumanly capable AI.\n\nElon Musk is not in his own person a majority of OpenAI's Board, but if he can pragmatically sway a majority of that Board then this measure is not being fulfilled even to a token degree.\n\n(Update: Elon Musk [stepped down](https://openai.com/blog/openai-supporters/) from the OpenAI Board in February 2018.)\n\n**Improving: ** There's a legal contract which says that the Board doesn't control the IP and that the alignment-aware research silo does.\n\n**Adequate:**  The entire command structure including all members of the finally governing Board are fully aware of the difficulty and danger of alignment.  The Board will not object if the technical leadership have disk-erasure measures ready in case the Board suddenly decides to try to open-source the AI anyway.\n\n**Excellent:**  Somehow *no* local authority poses a risk of stepping in and undoing any safety measures, etc.  I have no idea what incremental steps could be taken in this direction that would not make things worse.  If e.g. the government of Iceland suddenly understood how serious things had gotten and granted sanction and security to a project, that would fit this description, but I think that trying to arrange anything like this would probably make things worse globally because of the mindset it promoted.\n\n**Closure**\n-----------\n\n**Token: ** It's generally understood organizationally that some people want to keep code, architecture, and some ideas a 'secret' from outsiders, and everyone on the project is okay with this even if they disagree.  In principle people aren't being pressed to publish their interesting discoveries if they are obviously capabilities-laden; in practice, somebody always says \"but someone else will probably publish a similar idea 6 months later\" and acts suspicious of the hubris involved in thinking otherwise, but it remains possible to get away with not publishing at moderate personal cost.\n\n**Improving: ** A subset of people on the project understand why some code, architecture, lessons learned, et cetera must be kept from reaching the general ML community if success is to have a probability [significantly greater than zero](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/) (because [tradeoffs between alignment and capabilities](https://arbital.com/p/aligning_adds_time/) make the challenge unwinnable if there isn’t a project with a reasonable-length lead time).  These people have formed a closed silo within the project, with the sanction and acceptance of the project leadership.  It's socially okay to be *conservative* about what counts as potentially capabilities-laden thinking, and it's understood that worrying about this is not a boastful act of pride or a trick to get out of needing to write papers.\n\n**Adequate:**  Everyone on the project understands and agrees with closure.  Information is siloed whenever not everyone on the project needs to know it.\n\n<table><tbody><tr><td style=\"background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>Reminder: This is a 2017 document.</i></td></tr></tbody></table>\n\n  \n**Opsec**\n------------\n\n**Token:**  Random people are not allowed to wander through the building.\n\n**Improving:**  Your little brother cannot steal the IP.  Stuff is encrypted.  Siloed project members sign NDAs.\n\n**Adequate: ** Major governments cannot silently and unnoticeably steal the IP without a nonroutine effort.  All project members undergo government-security-clearance-style screening.  AGI code is not running on AWS, but in an airgapped server room.  There are cleared security guards in the server room.\n\n**Excellent:**  Military-grade or national-security-grade security.  (It's hard to see how attempts to get this could avoid being counterproductive, considering the difficulty of obtaining trustworthy command and common good commitment with respect to any entity that can deploy such force, and the effect that trying would have on general mindsets.)\n\n**Common good commitment**\n--------------------------\n\n**Token: ** Project members and the chain of command are not openly talking about how dictatorship is great so long as they get to be the dictator.  The project is not directly answerable to Trump or Putin.  They say vague handwavy things about how of course one ought to promote democracy and apple pie (applause) and that everyone ought to get some share of the pot o' gold (applause).\n\n**Improving: ** Project members and their chain of command have come out explicitly in favor of being nice to people and eventually building a nice intergalactic civilization.  They would release a cancer cure if they had it, their state of deployment permitting, and they don't seem likely to oppose incremental steps toward a postbiological future and the eventual realization of [most of the real value at stake](https://www.nickbostrom.com/astronomical/waste.html).\n\n**Adequate:**  Project members and their chain of command have an explicit commitment to something like [coherent extrapolated volition](https://arbital.com/p/cev/) as a long-run goal, AGI tech permitting, and otherwise the careful preservation of values and sentient rights through any pathway of intelligence enhancement.  In the short run, they would not do everything that seems to them like a good idea, and would first prioritize not destroying humanity or wounding its spirit with their own hands.  (E.g., if Google or Facebook consistently thought like this, they would have become concerned a lot earlier about social media degrading cognition.)  Real actual moral humility with policy consequences is a thing.\n\n**Alignment mindset**\n---------------------\n\n**Token:**  At least some people in command sort of vaguely understand that AIs don't just automatically do whatever the alpha male in charge of the organization wants to have happen.  They've hired some people who are at least pretending to work on that in a technical way, not just \"[ethicists](https://www.lesswrong.com/posts/SsCQHjqNT3xQAPQ6b/yudkowsky-on-agi-ethics)\" to talk about trolley problems and [which monkeys should get the tasty banana](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/).\n\n**Improving:**  The technical work output by the \"safety\" group is neither obvious nor wrong.  People in command have [ordinary paranoia](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) about AIs.  They expect alignment to be somewhat difficult and to take some extra effort.  They understand that not everything they might like to do, with the first AGI ever built, is equally safe to attempt.\n\n**Adequate:**  The project has realized that building an AGI is *mostly* about aligning it.  Someone with full security mindset and deep understanding of AGI cognition as cognition has proven themselves able to originate new deep alignment measures, and is acting as technical lead with effectively unlimited political capital within the organization to make sure the job actually gets done.  Everyone expects alignment to be terrifically hard and terribly dangerous and full of invisible bullets whose shadow you have to see before the bullet comes close enough to hit you.  They understand that alignment severely constrains architecture and that capability often trades off against transparency.  The organization is targeting the [minimal](https://arbital.com/p/minimality_principle/) AGI doing the least dangerous cognitive work that is required to prevent the next AGI project from destroying the world.  The [alignment assumptions](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) have been reduced into non-goal-valent statements, have been clearly written down, and are being monitored for their actual truth.\n\nAlignment mindset is *fundamentally *difficult to obtain for a project because [Graham's Design Paradox](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/) applies.  People with only ordinary paranoia may not be able to distinguish the next step up in depth of cognition, and happy innocents cannot distinguish useful paranoia from suits making empty statements about risk and safety.  They also tend not to realize what they're missing.  This means that there is a horrifically strong default that when you persuade one more research-rich person or organization or government to start a new project, that project *will *have inadequate alignment mindset unless something extra-ordinary happens.  I'll be frank and say relative to the present world I think this essentially has to go through trusting me or Nate Soares to actually work, although see below about Paul Christiano.  The lack of clear person-independent instructions for how somebody low in this dimension can improve along this dimension is why the difficulty of this dimension is the real killer.\n\nIf you insisted on trying this the impossible way, I'd advise that you start by talking to a brilliant computer security researcher rather than a brilliant machine learning researcher.\n\n**Resources**\n-------------\n\n**Token:**  The project has a combination of funding, good researchers, and computing power which makes it credible as a beacon to which interested philanthropists can add more funding and other good researchers interested in aligned AGI can join.  E.g., OpenAI would qualify as this if it were adequate on the other 5 dimensions.\n\n**Improving:**  The project has size and quality researchers on the level of say Facebook's AI lab, and can credibly compete among the almost-but-not-quite biggest players.  When they focus their attention on an unusual goal, they can get it done 1+ years ahead of the general field so long as Demis doesn't decide to do it first.  I expect e.g. the NSA would have this level of \"resources\" if they started playing now but didn't grow any further.\n\n**Adequate:**  The project can get things done with a 2-year lead time on anyone else, and it's not obvious that competitors could catch up even if they focused attention there.  DeepMind has a great mass of superior people and unshared tools, and is the obvious candidate for achieving adequacy on this dimension; though they would still need adequacy on other dimensions, and more closure in order to conserve and build up advantages.  As I understand it, an adequate resource advantage is explicitly what Demis was trying to achieve, before Elon blew it up, started an openness fad and an arms race, and probably got us all killed.  Anyone else trying to be adequate on this dimension would need to pull ahead of DeepMind, merge with DeepMind, or talk Demis into closing more research and putting less effort into unalignable AGI paths.\n\n**Excellent:**  There's a single major project which a substantial section of the research community understands to be The Good Project that good people join, with competition to it deemed unwise and unbeneficial to the public good.  This Good Project is at least adequate along all the other dimensions.  Its major competitors lack either equivalent funding or equivalent talent and insight.  Relative to the present world it would be **extremely difficult** to make any project like this exist with adequately trustworthy command and alignment mindset, and failed attempts to make it exist run the risk of creating still worse competitors developing unaligned AGI.\n\n**Unrealistic:** There is a single global Manhattan Project which is somehow not answerable to non-common-good command such as Trump or Putin or the United Nations Security Council.  It has orders of magnitude more computing power and smart-researcher-labor than anyone else.  Something keeps other AGI projects from arising and trying to race with the giant project.  The project can freely choose transparency in all transparency-capability tradeoffs and take an extra 10+ years to ensure alignment.  The project is at least adequate along all other dimensions.  This is how our distant, surviving cousins are doing it in their Everett branches that diverged centuries earlier towards [more competent civilizational equilibria](https://equilibriabook.com/toc).  You **cannot possibly** cause such a project to exist with adequately trustworthy command, alignment mindset, and common-good commitment, and you should therefore not try to make it exist, first because you will simply create a still more dire competitor developing unaligned AGI, and second because if such an AGI could be aligned it would be a hell of an [s-risk](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks) given the probable command structure.  People who are [slipping sideways in reality](https://www.facebook.com/yudkowsky/posts/10154981483669228) fantasize about being able to do this.\n\n* * *\n\n<table><tbody><tr><td style=\"background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>Reminder: This is a 2017 document.</i></td></tr></tbody></table>\n\n*Further Remarks*\n-----------------\n\nA project with \"adequate\" closure and a project with \"improving\" closure will, if joined, aggregate into a project with \"improving\" (aka: inadequate) closure where the closed section is a silo within an open organization.  Similar remarks apply along other dimensions.  The aggregate of a project with NDAs, and a project with deeper employee screening, is a combined project with some unscreened people in the building and hence \"improving\" opsec.\n\n\"Adequacy\" on the dimensions of **closure** and **opsec** is based around my mainline-probability scenario where you unavoidably need to spend at least 1 year in a regime where the AGI is not yet alignable on a minimal act that ensures nobody else will destroy the world shortly thereafter, but during that year it's possible to remove a bunch of safeties from the code, shift transparency-capability tradeoffs to favor capability instead, ramp up to full throttle, and immediately destroy the world.\n\nDuring this time period, leakage of the code to the wider world automatically results in the world being turned into paperclips.  Leakage of the code to multiple major actors such as commercial espionage groups or state intelligence agencies seems to me to stand an extremely good chance of destroying the world because at least one such state actor's command will not reprise the alignment debate correctly and each of them will fear the others.\n\nI would also expect that, if key ideas and architectural lessons-learned were to leak from an insufficiently closed project that would otherwise have actually developed alignable AGI, it would be possible to use 10% as much labor to implement a non-alignable world-destroying AGI [in a shorter timeframe](https://arbital.com/p/aligning_adds_time/).  The project must be closed *tightly* or everything ends up as paperclips.\n\n\"Adequacy\" on **common good commitment** is based on my model wherein the first [task-directed AGI](https://arbital.com/p/task_agi/) continues to operate in a regime far below that of a real superintelligence, where many tradeoffs have been made for transparency over capability and this greatly constrains self-modification.\n\nThis task-directed AGI is *not* able to defend against true superintelligent attack.  It *cannot* monitor other AGI projects in an unobtrusive way that grants those other AGI projects a lot of independent freedom to do task-AGI-ish things so long as they don't create an [unrestricted superintelligence](https://arbital.com/p/Sovereign/).  The designers of the first task-directed AGI are *barely* able to operate it in a regime where the AGI doesn't create an unaligned superintelligence inside itself or its environment.  Safe operation of the original AGI requires a continuing major effort at supervision.  The level of safety monitoring of other AGI projects required would be so great that, if the original operators deemed it good that more things be done with AGI powers, it would be far simpler and safer to do them as additional tasks running on the original task-directed AGI.  *Therefore:* Everything to do with invocation of superhuman specialized general intelligence, like superhuman science and engineering, continues to have a single effective veto point.\n\nThis is also true in less extreme scenarios where AGI powers can proliferate, but must be very tightly monitored, because no aligned AGI can defend against an unconstrained superintelligence if one is deliberately or accidentally created by taking off too many safeties. Either way, there is a central veto authority that continues to actively monitor and has the power to prevent anyone else from doing anything potentially world-destroying with AGI.\n\nThis in turn means that any use of AGI powers along the lines of uploading humans, trying to do human intelligence enhancement, or building a cleaner and more stable AGI to run a CEV, would be subject to the explicit veto of the command structure operating the first task-directed AGI.  If this command structure does not favor something like CEV, or vetoes transhumanist outcomes from a transparent CEV, or doesn't allow intelligence enhancement, et cetera, then all future astronomical value can be permanently lost and even s-risks may apply.\n\nA universe in which 99.9% of the sapient beings have no civil rights because way back on Earth somebody decided or *voted* that emulations weren't real people, is a universe plausibly much worse than paperclips.  (I would see as self-defeating any argument from democratic legitimacy that ends with almost all sapient beings not being able to vote.)\n\nIf DeepMind closed to the silo level, put on adequate opsec, somehow gained alignment mindset within the silo, and allowed trustworthy command of that silo, then in my guesstimation it *might *be possible to save the Earth (we would start to leave the floor of the [logistic success curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/)).\n\nOpenAI seems to me to be further behind than DeepMind along multiple dimensions.  OAI is doing significantly better \"safety\" research, but it is all still inapplicable to serious AGI, AFAIK, even if it's not fake / obvious.  I do not think that either OpenAI or DeepMind are out of the basement on the logistic success curve for the alignment-mindset dimension.  It's not clear to me from where I sit that the miracle required to grant OpenAI a chance at alignment success is easier than the miracle required to grant DeepMind a chance at alignment success.  If Greg Brockman or other decisionmakers at OpenAI are not totally insensible, neither is Demis Hassabis.  Both OAI and DeepMind have significant metric distance to cross on Common Good Commitment; this dimension is relatively easier to max out, but it's not maxed out just by having commanders vaguely nodding along or publishing a mission statement about moral humility, nor by a fragile political balance with some morally humble commanders and some morally nonhumble ones.  If I had a ton of money and I wanted to get a serious contender for saving the Earth out of OpenAI, I'd probably start by taking however many OpenAI researchers could pass screening and refounding a separate organization out of them, then using that as the foundation for further recruiting.\n\nI have never seen anyone except Paul Christiano try what I would consider to be deep macro alignment work.  E.g. if you look at Paul's AGI scheme there is a *global alignment story* with assumptions that can be broken down, and the idea of exact human imitation is a deep one rather than a shallow defense--although I don't think the assumptions have been broken down far enough; but nobody else knows they even ought to be trying to do anything like that.  I [also think](https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal) Paul's AGI scheme is orders-of-magnitude too costly and has chicken-and-egg alignment problems.  *But* I wouldn't totally rule out a project with Paul in technical command, because I would hold out hope that Paul could follow along with someone else's deep security analysis and understand it in-paradigm even if it wasn't his own paradigm; that Paul would suggest useful improvements and hold the global macro picture to a standard of completeness; and that Paul would take seriously how bad it would be to violate an alignment assumption even if it wasn't an assumption within his native paradigm.  Nobody else except myself and Paul is currently in the arena of comparison.  If we were both working on the same project it would still have unnervingly few people like that.  I think we should try to get more people like this from the pool of brilliant young computer security researchers, not just the pool of machine learning researchers.  Maybe that'll fail just as badly, but I want to see it tried.\n\nI doubt that it is possible to produce a written scheme for alignment, or any other kind of fixed advice, that can be handed off to a brilliant programmer with ordinary paranoia and allow them to actually succeed.  Some of the deep ideas are going to turn out to be wrong, inapplicable, or just plain missing.  Somebody is going to have to notice the unfixable deep problems in advance of an actual blowup, and come up with new deep ideas and not just patches, as the project goes on.\n\n<table><tbody><tr><td style=\"background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>Reminder: This is a 2017 document.</i></td></tr></tbody></table>",
      "plaintextDescription": "Editor's note:  The following is a lightly edited copy of a document written by Eliezer Yudkowsky in November 2017. Since this is a snapshot of Eliezer’s thinking at a specific time, we’ve sprinkled reminders throughout that this is from 2017.\n\nA background note:\n\nIt’s often the case that people are slow to abandon obsolete playbooks in response to a novel challenge. And AGI is certainly a very novel challenge.\n\nItalian general Luigi Cadorna offers a memorable historical example. In the Isonzo Offensive of World War I, Cadorna lost hundreds of thousands of men in futile frontal assaults against enemy trenches defended by barbed wire and machine guns.  As morale plummeted and desertions became epidemic, Cadorna began executing his own soldiers en masse, in an attempt to cure the rest of their “cowardice.” The offensive continued for 2.5 years.\n\nCadorna made many mistakes, but foremost among them was his refusal to recognize that this war was fundamentally unlike those that had come before.  Modern weaponry had forced a paradigm shift, and Cadorna’s instincts were not merely miscalibrated—they were systematically broken.  No number of small, incremental updates within his obsolete framework would be sufficient to meet the new challenge.\n\nOther examples of this type of mistake include the initial response of the record industry to iTunes and streaming; or, more seriously, the response of most Western governments to COVID-19.\n\n \n\n\n\n \n\nAs usual, the real challenge of reference class forecasting is figuring out which reference class the thing you’re trying to model belongs to.\n\nFor most problems, rethinking your approach from the ground up is wasteful and unnecessary, because most problems have a similar causal structure to a large number of past cases. When the problem isn’t commensurate with existing strategies, as in the case of AGI, you need a new playbook.\n\n\n\nI've sometimes been known to complain, or in a polite way scream in utter terror, that \"there is no good guy ",
      "wordCount": 3881
    },
    "tags": [
      {
        "_id": "m2DsR4r4HRSaLSPW3",
        "name": "Security Mindset",
        "slug": "security-mindset"
      },
      {
        "_id": "txkDg4aLmiRq8wsSu",
        "name": "Organizational Culture & Design",
        "slug": "organizational-culture-and-design"
      },
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "SA9hDewwsYgnuscae",
    "title": "ProjectLawful.com: Eliezer's latest story, past 1M words",
    "slug": "projectlawful-com-eliezer-s-latest-story-past-1m-words",
    "url": null,
    "baseScore": 238,
    "voteCount": 131,
    "viewCount": null,
    "commentCount": 112,
    "createdAt": null,
    "postedAt": "2022-05-11T06:18:02.738Z",
    "contents": {
      "markdown": "So if you read Harry Potter and the Methods of Rationality, and thought...\n\n\"You know, HPMOR is pretty good so far as it goes; but Harry is much too cautious and doesn't have nearly enough manic momentum, his rationality lectures aren't long enough, and all of his personal relationships are way way *way* too healthy.\"\n\n...then have I got the story for you! [Planecrash aka Project Lawful aka Mad Investor Chaos and the Woman of Asmodeus](https://projectlawful.com), is a story in roleplay-format that I as \"Iarwain\" am cowriting with Lintamande, now past 1,000,000 words.\n\nIt's the story of Keltham, from the world of dath ilan; a place of high scientific achievement but rather innocent in some ways.  For mysterious reasons they've screened off their own past, and very few now know what their prescientific history was like.\n\nKeltham dies in a plane crash and ends up in the country of Cheliax, whose god is \"Asmodeus\", whose alignment is \"Lawful Evil\" and whose people usually go to the afterlife of \"Hell\".\n\nAnd so, like most dath ilani would, in that position, Keltham sets out to bring the industrial and scientific revolutions to his new planet!  Starting with Cheliax!\n\n(Keltham's new friends may not have been entirely frank with him about exactly what Asmodeus wants, what Evil really is, or what sort of place Hell is.)\n\nThis is not a story for kids, even less so than HPMOR. There is romance, there is sex, there are deliberately bad kink practices whose explicit purpose is to get people to actually hurt somebody else so that they'll end up damned to Hell, and also there's math.\n\nThe starting point is Book 1, [Mad Investor Chaos and the Woman of Asmodeus](https://www.projectlawful.com/posts/4582). I suggest logging into ProjectLawful.com with Google, or creating an email login, in order to track where you are inside the story.\n\nPlease avoid story spoilers in the comments, especially ones without [spoiler protection](https://www.lesswrong.com/posts/2rWKkWuPrgTMpLRbp/lesswrong-faq#How_do_I_insert_spoiler_protections_); this is not meant as an \"ask Eliezer things about MICWOA\" thread.",
      "plaintextDescription": "So if you read Harry Potter and the Methods of Rationality, and thought...\n\n\"You know, HPMOR is pretty good so far as it goes; but Harry is much too cautious and doesn't have nearly enough manic momentum, his rationality lectures aren't long enough, and all of his personal relationships are way way way too healthy.\"\n\n...then have I got the story for you! Planecrash aka Project Lawful aka Mad Investor Chaos and the Woman of Asmodeus, is a story in roleplay-format that I as \"Iarwain\" am cowriting with Lintamande, now past 1,000,000 words.\n\nIt's the story of Keltham, from the world of dath ilan; a place of high scientific achievement but rather innocent in some ways.  For mysterious reasons they've screened off their own past, and very few now know what their prescientific history was like.\n\nKeltham dies in a plane crash and ends up in the country of Cheliax, whose god is \"Asmodeus\", whose alignment is \"Lawful Evil\" and whose people usually go to the afterlife of \"Hell\".\n\nAnd so, like most dath ilani would, in that position, Keltham sets out to bring the industrial and scientific revolutions to his new planet!  Starting with Cheliax!\n\n(Keltham's new friends may not have been entirely frank with him about exactly what Asmodeus wants, what Evil really is, or what sort of place Hell is.)\n\nThis is not a story for kids, even less so than HPMOR. There is romance, there is sex, there are deliberately bad kink practices whose explicit purpose is to get people to actually hurt somebody else so that they'll end up damned to Hell, and also there's math.\n\nThe starting point is Book 1, Mad Investor Chaos and the Woman of Asmodeus. I suggest logging into ProjectLawful.com with Google, or creating an email login, in order to track where you are inside the story.\n\nPlease avoid story spoilers in the comments, especially ones without spoiler protection; this is not meant as an \"ask Eliezer things about MICWOA\" thread.",
      "wordCount": 322
    },
    "tags": [
      {
        "_id": "EmaCLRKb4baBFq4ra",
        "name": "Dath Ilan",
        "slug": "dath-ilan"
      },
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uyBeAN5jPEATMqKkX",
    "title": "Lies Told To Children",
    "slug": "lies-told-to-children-1",
    "url": null,
    "baseScore": 382,
    "voteCount": 307,
    "viewCount": null,
    "commentCount": 94,
    "createdAt": null,
    "postedAt": "2022-04-14T11:25:10.282Z",
    "contents": {
      "markdown": "Growing up, as a kid, I was always told that every sapient life is precious, everything that thinks and knows itself -\n\nYes, this *is* a tale about lies-told-to-children.  You'll probably figure it out yourself before too long.  For now, just listen.\n\nWhere was I?  Right.  As children, we were always told that every sapient life is precious.  It was told to us by the teachers, and shown to us in children's television - though I saw less children's television than most children in our age cohort - children's TV was censored where I grew up, though, of course, I didn't find that out until much later -\n\nI see you're starting to guess under what sort of circumstances I grew up.  Go ahead, write down the prediction if you want.  Maybe you already see where this entire thing is headed.  But you asked me for a story about the lies I was told as a child, and that's what you're getting.  It's not my fault, if a lot of stories like that are predictable; people who lie to children have other things to optimize for than unpredictability.\n\nSo where was I?  Right.  I grew up in a remote village of about three thousand people, the sort that's more hills than houses.  Charming travel-pathways that cut through forests.  Not everyone knows everyone, but you sure know somebody who knows anybody.\n\nChildren's television in my region was censored, though of course they didn't tell us that as children.  But the children's television that we saw had aliens and monsters and creatures of fantasy, with four legs or fourteen legs, three faces or no face at all, and all of them were treated by the television show as having lives that meant something.  Sometimes in the children's show there were alien monsters who only thought their own kind of life was valuable, and then maybe you couldn't trade with them as friends.  Maybe they'd already lied to you once and you couldn't trust them enough to bargain with them, maybe you couldn't talk to them at all.  But their lives still had meaning to the story's human protagonists, even some aliens whose lives had no meaning to themselves.  You didn't cause them pain if there was any way to avoid it; you didn't kill them unless their biology was sufficiently similar to human that you were confident in your ability to cryopreserve them afterwards.\n\nThe shows never spelled it out, never said, 'And this is because of a universal rule in every case that sapient life has value.'  Our teachers said that explicitly, though.\n\nAnd they treated every one of us children, too, as if our lives had meaning.\n\nExcept the children with the red hair; those dirty reds.\n\nYou're nodding along with a knowing look, I see.  Was it what you predicted?  Not exactly, maybe, but rough ballpark?  I suppose I'll find out when we open your prediction afterwards.\n\nThe red-haired children hardly needed the red hair, as their targeting-mark; they looked different from the rest of us in other ways too.  When I was old enough to first ask, I was told that they were the children's children of people who'd been exiled from a faraway city for committing terrible crimes there, who'd been given sanctuary by the grace and mercy of our own benevolent kind.  The red-haired children tended bigger than the rest of us, with more adult facial structures, to the point where you could've maybe mistaken them for very small adults in disguise.  The red-haired adults, what few of them we ever saw, were correspondingly huge and muscular.  You could see, in retrospect - if you were actually trying to think at all, which we weren't really - how somebody might have felt threatened by such big muscular people, even while graciously granting them sanctuary.\n\nThere weren't many of the red-haired children being educated alongside us; a handful, four or six.  I can't recall how many by counting names, because they kept to themselves and did not try to be friends with the rest of us.\n\nThey were slower than the rest of us in class to answer.  On the rare occasions a teacher called on them, they'd often get the question wrong.  We were kids, young kids, so of course we didn't ask ourselves anything like \"Is this in fact an intrinsic deficit of intelligence or is it a self-fulfilling prophecy about who gets more effort from the teachers?\" or come up with any experiments to test that one way or another.  We just wordlessly thought that red-haired kids were stupider; and that this too was a universal rule just like gravity.\n\nWe did not, in fact, treat our red-headed fellow kids all that well.  We were of an age where kids take their cues from adults without carefully rethinking everything they're seeing.  We noticed how the older kids treated red-haired kids, we noticed how teachers treated red-haired kids, we noticed the huge red-headed adults who were silently sweeping the hallways and not doing any intellectual labor.  We noticed how the adult reds got casually shoved aside by other adults or even non-red-headed older kids, and how the red-headed adults just silently took that.\n\nThere were names to call them, 'dirty reds', worse things than that, scatological profanities to giggle over amongst ourselves.\n\nNow and then you'd see a Security Officer come by and ask some reds some questions.  One time Security took one of the janitors away, and then after that, nobody ever saw him again.  I think one of the kids did ask, in class, what happened to that guy, and the teacher shut her right down and said that any questions about dirty reds or for that matter Security were things best asked in private if you asked at all.\n\nAnd meanwhile the television shows, those that we got to watch, went right on teaching the lesson that all sapient life is precious, with no exceptions for fourteen legs or not having a face.\n\nEventually, of course, it started coming to a point, and then it did come to a point.\n\nIt started to come to a point, at the point where a red-haired kid was called on in class and answered a question wrong, and the teacher asked if his parents were too busy stealing other people's books to teach him how to read.  The red-haired kid didn't say anything back, but I flinched, visibly.\n\nIt came to the point, two days after that, when I was walking home from class, and I heard a groan from off the pathway home, what sounded like a moan of pain.\n\nI left the pathway and ran around a hill to find one of those dirty reds hiding behind it, with blood all over his left pants-leg.\n\nHe asked me not to get an adult.\n\nHe said that he was hiding from Security.\n\nHe asked me to help him walk, help him get away.\n\nIt didn't feel real.  It felt like I was inside one of the children's television shows.\n\nOf course, in children's television shows, they always show the heroes reminding themselves that things are real and that they've got to do what's right, because it's real, so I knew that I needed to remember that this was real because that's what you do when you're inside a television show.\n\nI think I was probably very scared, though I don't remember noticing myself being scared.\n\nI asked him what he'd done to get Security looking for him.\n\nHe said that he had, a few days ago, said something about red-haired people deserving better treatment than they currently got, around a non-red-haired person he'd thought, hoped, was a friend.\n\nI gave him a hand so he could stand up, on the leg that wasn't covered with blood, and then he leaned on me and we hopped away through the hills until we got to where a red-haired woman - you saw fewer of those - whispered a thank-you to me and took him away with herself.\n\nI ran back to the pathway and ran home, though I was still late, of course.  My dad asked me where I'd been and I said I'd seen a funny-looking butterfly and run off to chase it.  I remember believing, even then, that he knew I was lying, but dad didn't ask me any more questions, and I didn't tell him anything.\n\nAbout an hour later, Security knocked on our door and asked everyone if they'd seen a red-haired person who looked like - and of course the picture was of the man I'd helped to get away.\n\nI said no, I hadn't seen him.  But because I was a kid and kids that age aren't taught theory-of-deception, I asked what the man had done and if he was considered dangerous.  And I didn't think, until too late, about whether that was something I was much more likely to ask if -\n\nThe Security officer asked me if I maybe wanted to change my mind about having seen the fugitive.\n\nI gave him my best surprised look and said no.\n\nThe Security officer noted that Security officers get special training in reading emotions, and I seemed pretty frightened to him.\n\nI said yes, I was, because the Security officer was suggesting that I was lying and that was scary.\n\nThe Security officer said he knew perfectly well, at this point, that I was lying.  But I wouldn't end up in trouble if I showed him where the fugitive went and identified anyone else he was with.\n\nI said that he didn't know what he was talking about.\n\nThe Security officer gave me a sort of stern look and said that he'd detected another lie, and did I *really* want to get in trouble for some dirty red.\n\nI told him that I wasn't stupid and I knew he was bluffing, to try to trick me, because he suspected me, even though I hadn't done it.\n\nHe took a photo out of his pocket and showed it to me.\n\nIt was me helping the red-haired man walk on his one good leg.\n\n*Why*, said the Security.  He just looked sad, now.  Why *had* I done it?  Why was a dirty red worth it?\n\nAnd I remember, by *that* point, that I'd noticed I was scared, and I think I *was* trying to get out of it - by proving that I was, in the end, obeying adult authority - when I said that we'd all been told in class that every sapient life is precious, everything that thinks and knows itself, that was the rule we'd been given, and nobody had reasonably *argued* at any point that there was an exception for people with red hair, and *also* we'd all been told that hurting people is wrong and you shouldn't let social conformity push you into it.\n\nThe rest of it went the way you'd expect.\n\nThe Security officer smiled.\n\nMy parents rushed in and hugged me and told me I'd been *so* brave and *so* good and scored in what would've been the upper 5th percentile twenty years ago for the age where I started to object and not go along with it anymore; and explained about Civilization needing to test *some* kids now and then, to find out how well we were doing environment-wise and heredity-wise on people's kindness and resistance to conformity-pushed cruelty; and test against an earlier-reported bug where general rules about fair and okay treatment of people would somehow end up not being applied to some subgroup; and our little village was settling an important conditional prediction market from twenty years earlier, that had millions of labor-hours wagered on it; and that children growing up to be good people was a vital figure-of-merit for all of Civilization and lots of big policy decisions turned around it, which was why it had been worth specializing our village to do Science about that, and they hoped I understood all that and wouldn't tell the other children right away.  There wasn't actually any such thing as Security, and if there ever *was* it would mean that it was time to overthrow the government immediately.\n\nI nodded along in a wise, understanding, and rather numb fashion.  I think the main thing I said, at the end, was that I'd better be getting paid for this, and they all laughed and said of course I was, lots of money, at least as much as my parents were getting, because children are sapient beings too.\n\nSo that's *my *story about the-lies-we-tell-to-children.  And the part that I value now the most, even more than the money I got then and when I was older, even more than knowing that I was good and brave in the only sort of real test that most people in Civilization ever get, is that I approximately always win any Lies-Told-To-Children storytelling night.",
      "plaintextDescription": "Growing up, as a kid, I was always told that every sapient life is precious, everything that thinks and knows itself -\n\nYes, this is a tale about lies-told-to-children.  You'll probably figure it out yourself before too long.  For now, just listen.\n\nWhere was I?  Right.  As children, we were always told that every sapient life is precious.  It was told to us by the teachers, and shown to us in children's television - though I saw less children's television than most children in our age cohort - children's TV was censored where I grew up, though, of course, I didn't find that out until much later -\n\nI see you're starting to guess under what sort of circumstances I grew up.  Go ahead, write down the prediction if you want.  Maybe you already see where this entire thing is headed.  But you asked me for a story about the lies I was told as a child, and that's what you're getting.  It's not my fault, if a lot of stories like that are predictable; people who lie to children have other things to optimize for than unpredictability.\n\nSo where was I?  Right.  I grew up in a remote village of about three thousand people, the sort that's more hills than houses.  Charming travel-pathways that cut through forests.  Not everyone knows everyone, but you sure know somebody who knows anybody.\n\nChildren's television in my region was censored, though of course they didn't tell us that as children.  But the children's television that we saw had aliens and monsters and creatures of fantasy, with four legs or fourteen legs, three faces or no face at all, and all of them were treated by the television show as having lives that meant something.  Sometimes in the children's show there were alien monsters who only thought their own kind of life was valuable, and then maybe you couldn't trade with them as friends.  Maybe they'd already lied to you once and you couldn't trust them enough to bargain with them, maybe you couldn't talk to them at all.  But their lives still had meaning to the stor",
      "wordCount": 2095
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "j9Q8bRmwCgXRYAgcJ",
    "title": "MIRI announces new \"Death With Dignity\" strategy",
    "slug": "miri-announces-new-death-with-dignity-strategy",
    "url": null,
    "baseScore": 377,
    "voteCount": 392,
    "viewCount": null,
    "commentCount": 547,
    "createdAt": null,
    "postedAt": "2022-04-02T00:43:19.814Z",
    "contents": {
      "markdown": "tl;dr:  It's obvious at this point that humanity isn't going to solve the alignment problem, or even try very hard, or even go out with much of a fight.  Since survival is unattainable, we should shift the focus of our efforts to helping humanity die with with slightly more dignity.\n\n* * *\n\nWell, let's be frank here.  MIRI didn't solve AGI alignment and at least knows that it didn't.  Paul Christiano's incredibly complicated schemes have no chance of working in real life before DeepMind destroys the world.  Chris Olah's transparency work, at current rates of progress, will at best let somebody at DeepMind give a highly speculative warning about how the current set of enormous inscrutable tensors, inside a system that was recompiled three weeks ago and has now been training by gradient descent for 20 days, might possibly be planning to start trying to deceive its operators.\n\nManagement will then ask what they're supposed to do about that.\n\nWhoever detected the warning sign will say that there isn't anything known they can do about that.  Just because you can see the system might be planning to kill you, doesn't mean that there's any known way to build a system that won't do that.  Management will then decide not to shut down the project - because it's not certain that the intention was really there or that the AGI will really follow through, because other AGI projects are hard on their heels, because if all those gloomy prophecies are true then there's nothing anybody can do about it anyways.  Pretty soon that troublesome error signal will vanish.\n\nWhen Earth's prospects are that far underwater in the basement of the [logistic success curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/), it may be hard to feel motivated about continuing to fight, since doubling our chances of survival will only take them from 0% to 0%.\n\nThat's why I would suggest reframing the problem - especially on an emotional level - to helping humanity *die with dignity,* or rather, since even this goal is realistically unattainable at this point, *die with slightly more dignity than would otherwise be counterfactually obtained.*\n\nConsider the world if Chris Olah had never existed.  It's then much more likely that nobody will even *try and fail* to adapt Olah's methodologies to try and read complicated facts about internal intentions and future plans, out of whatever enormous inscrutable tensors are being integrated a million times per second, inside of whatever recently designed system finished training 48 hours ago, in a vast GPU farm that's already helpfully connected to the Internet.\n\nIt is more dignified for humanity - a better look on our tombstone - if we die *after the management of the AGI project was heroically warned of the dangers* but came up with totally reasonable reasons to go ahead anyways.\n\nOr, failing that, if people made *a heroic effort to do something that could maybe possibly have worked to generate a warning like that but couldn't actually in real life because the latest tensors were in a slightly different format and there was no time to readapt the methodology.*  Compared to the much less dignified-looking situation if *there's no warning and nobody even tried to figure out how to generate one.*\n\nOr take MIRI.  Are we sad that it looks like this Earth is going to fail?  Yes.  Are we sad that we tried to do anything about that?  No, because it would be so much sadder, when it all ended, to face our ends wondering if maybe solving alignment would have just been as easy as buckling down and making a serious effort on it - not knowing if that would've just worked, if we'd only tried, because nobody had ever even tried at all.  It wasn't subjectively overdetermined that the (real) problems would be too hard for us, before we made the only attempt at solving them that would ever be made.  *Somebody* needed to try at all, in case that was all it took.\n\nIt's sad that our Earth couldn't be one of the more dignified planets that makes a real effort, correctly pinpointing the actual real difficult problems and then allocating thousands of the sort of brilliant kids that our Earth steers into wasting their lives on theoretical physics.  But better MIRI's effort than nothing.  What were we supposed to do instead, pick easy irrelevant fake problems that we could make an illusion of progress on, and have nobody out of the human species even try to solve the hard scary real problems, until everybody just fell over dead?\n\nThis way, at least, some people are walking around knowing why it is that if you train with an outer loss function that enforces the appearance of friendliness, you will not get an AI internally motivated to be friendly in a way that persists after its capabilities start to generalize far out of the training distribution...\n\nTo be clear, nobody's going to listen to those people, in the end.  There will be more comforting voices that sound less politically incongruent with whatever agenda needs to be pushed forward that week.  Or even if that ends up not so, this isn't primarily a social-political problem, of just getting people to listen.  Even if DeepMind listened, and Anthropic knew, and they both backed off from destroying the world, that would just mean Facebook AI Research destroyed the world a year(?) later.\n\nBut compared to being part of a species that walks forward completely oblivious into the whirling propeller blades, with nobody having seen it at all or made any effort to stop it, it is dying with a little more dignity, if anyone knew at all.  You can feel a little incrementally prouder to have died as part of a species like that, if maybe not proud in absolute terms.\n\nIf there is a stronger warning, because we did more transparency research?  If there's deeper understanding of the real dangers and those come closer to beating out comfortable nonrealities, such that DeepMind and Anthropic really actually back off from destroying the world and let Facebook AI Research do it instead?  If they try some hopeless alignment scheme whose subjective success probability looks, to the last sane people, more like 0.1% than 0?  Then we have died with *even more* dignity!  It may not get our survival probabilities much above 0%, but it would be *so much more dignified* than the present course looks to be!\n\n* * *\n\nNow of course the real subtext here, is that if you can otherwise set up the world so that it looks like you'll die with *enough* dignity - die of the social and technical problems that are really unavoidable, after making a huge effort at coordination and technical solutions and *failing,* rather than storming directly into the whirling helicopter blades as is the present unwritten plan -\n\n\\- heck, if there was even a plan at all -\n\n\\- then maybe possibly, if we're wrong about something fundamental, somehow, somewhere -\n\n\\- in a way that *makes things easier rather than harder,* because obviously we're going to be wrong about all sorts of things, it's a whole new world inside of AGI -\n\n\\- although, when you're fundamentally wrong about rocketry, this does not usually mean your rocket prototype goes exactly where you wanted on the first try while consuming half as much fuel as expected; it means the rocket explodes earlier yet, and not in a way you saw coming, being as wrong as you were -\n\n\\- but if we get some miracle of unexpected *hope,* in those unpredicted inevitable places where our model is wrong -\n\n\\- then our ability to take advantage of that one last hope, will greatly depend on how much dignity we were set to die with, before then.\n\nIf we can get on course to die with *enough* dignity, maybe we won't die at all...?\n\nIn principle, yes.  Let's be very clear, though:  Realistically speaking, that is not how real life works.\n\nIt's possible for a model error to make your life easier.  But you do not get *more* surprises that make your life easy, than surprises that make your life even more difficult.  And people do not suddenly become more reasonable, and make vastly more careful and precise decisions, as soon as they're scared.  No, not even if it seems to you like their current awful decisions are weird and not-in-the-should-universe, and surely some sharp shock will cause them to snap out of that weird state into a normal state and start outputting the decisions you think they should make.\n\nSo don't get your heart set on that \"not die at all\" business.  Don't invest all your emotion in a reward you probably won't get.  Focus on dying with dignity - *that* is something you can actually obtain, even in this situation.  After all, if you help humanity die with even *one more dignity point,* you yourself die with *one hundred dignity points!  *Even if your species dies an incredibly undignified death, for you to have helped humanity go down with even slightly more of a real fight, is to die an *extremely* dignified death.\n\n\"Wait, dignity points?\" you ask.  \"What are those?  In what units are they measured, exactly?\"\n\nAnd to this I reply:  Obviously, the measuring units of dignity are over humanity's log odds of survival - the graph on which the logistic success curve is a straight line.  A project that doubles humanity's chance of survival from 0% to 0% is helping humanity die with one additional information-theoretic bit of dignity.\n\nBut if enough people can contribute enough bits of dignity like that, wouldn't that mean we didn't die at all?  Yes, but again, don't get your hopes up.  Don't focus your emotions on a goal you're probably not going to obtain.  Realistically, we find a handful of projects that contribute a few more bits of counterfactual dignity; get a bunch more not-specifically-expected bad news that makes the first-order object-level situation look even worse (where to second order, of course, the good Bayesians already knew that was how it would go); and then we all die.\n\n* * *\n\nWith a technical definition in hand of what exactly constitutes dignity, we may now consider some specific questions about what does and doesn't constitute dying with dignity.\n\n**Q1:  Does 'dying with dignity' in this context mean accepting the certainty of your death, and not childishly regretting that or trying to fight a hopeless battle?**\n\nDon't be ridiculous.  How would that increase the log odds of Earth's survival?\n\nMy utility function isn't up for grabs, either.  If I regret my planet's death then I regret it, and it's beneath my dignity to pretend otherwise.\n\nThat said, I fought hardest while it looked like we were in the more sloped region of the logistic success curve, when our survival probability seemed more around the 50% range; I borrowed against my future to do that, and burned myself out to some degree.  That was a deliberate choice, which I don't regret now; it was worth trying, I would not have wanted to die having not tried, I would not have wanted Earth to die without anyone having tried.  But yeah, I am taking some time partways off, and trying a little less hard, now.  I've earned a lot of dignity already; and if the world is ending anyways and I can't stop it, I can afford to be a little kind to myself about that.\n\nWhen I tried hard and burned myself out some, it was with the understanding, within myself, that I would not keep trying to do that forever.  We cannot fight at maximum all the time, and some times are more important than others.  (Namely, when the logistic success curve seems relatively more sloped; those times are relatively more important.)\n\nAll *that* said:  If you fight marginally longer, you die with marginally more dignity.  Just don't undignifiedly delude yourself about the probable outcome.\n\n**Q2:  I have a clever scheme for saving the world!  I should act as if I believe it will work and save everyone, right, even if there's arguments that it's almost certainly misguided and doomed?  Because if those arguments are correct and my scheme can't work, we're all dead anyways, right?**\n\nA:  No!  That's not dying with dignity!  That's stepping sideways out of a mentally uncomfortable world and finding an escape route from unpleasant thoughts!  If you condition your probability models on a false fact, something that isn't true on the mainline, it means you've mentally stepped out of reality and are now living somewhere else instead.\n\nThere are more elaborate arguments against the rationality of this strategy, but consider this quick heuristic for arriving at the correct answer:  *That's not a dignified way to die.*  Death with dignity means going on mentally living in the world you think is reality, even if it's a sad reality, until the end; not abandoning your arts of seeking truth; dying with your commitment to reason intact.\n\nYou should try to make things better in the real world, where your efforts aren't enough and you're going to die anyways; not inside a fake world you can save more easily.\n\nQ2:  But what's wrong with the argument from expected utility, saying that all of humanity's expected utility lies within possible worlds where my scheme turns out to be feasible after all?\n\nA:  Most fundamentally?  *That's not what the surviving worlds look like*.  The surviving worlds look like people who lived inside their awful reality and tried to shape up their impossible chances; until somehow, somewhere, a miracle appeared - the model broke in a positive direction, for once, as does not usually occur when you are trying to do something very difficult and hard to understand, but might still be so - and they were positioned with the resources and the *sanity* to take advantage of that positive miracle, because they went on living inside uncomfortable reality.  Positive model violations do ever happen, but it's much less likely that somebody's *specific* desired miracle that \"we're all dead anyways if not...\" will happen; these people have just walked out of the reality where any actual positive miracles might occur.\n\nAlso and in practice?  People don't just pick *one* comfortable improbability to condition on.  They *go on* encountering unpleasant facts true on the mainline, and each time saying, \"Well, if that's true, I'm doomed, so I may as well assume it's not true,\" and they say more and more things like this.  If you do this it *very* rapidly drives down the probability mass of the 'possible' world you're mentally inhabiting.  Pretty soon you're living in a place that's nowhere *near* reality.  If there were an expected utility argument for risking everything on an improbable assumption, you'd get to make *exactly one of them, ever.*  People using this kind of thinking usually aren't even keeping track of when they say it, let alone counting the occasions.\n\nAlso also, in practice?  In domains like this one, things that seem to first-order like they \"might\" work... have essentially no chance of working in real life, to second-order after taking into account downward adjustments against optimism.  AGI is a scientifically unprecedented experiment *and* a domain with lots of optimization pressures some of which work against you *and* unforeseeable intelligently selected execution pathways *and* with a small target to hit *and* all sorts of extreme forces that break things and that you couldn't fully test before facing them.  AGI alignment seems like it's blatantly going to be an enormously Murphy-cursed domain, like rocket prototyping or computer security but *worse*.\n\nIn a domain like, if you have a clever scheme for winning anyways that, to first-order theoretical theory, *totally definitely seems like it should work,* even to Eliezer Yudkowsky rather than somebody who just goes around saying that casually,  then *maybe* there's like a 50% chance of it working in practical real life after all the unexpected disasters and things turning out to be harder than expected.\n\nIf to first-order it seems to you like something in a complicated unknown untested domain has a 40% chance of working, it has a 0% chance of working in real life.\n\nAlso also *also* in practice?  Harebrained schemes of this kind are usually actively harmful.  Because they're invented by the sort of people who'll come up with an unworkable scheme, and then try to get rid of counterarguments with some sort of dismissal like \"Well if not then we're all doomed anyways.\"\n\nIf nothing else, this kind of harebrained desperation drains off resources from those reality-abiding efforts that might try to do something on the subjectively apparent doomed mainline, and so position themselves better to take advantage of unexpected hope, which is what the surviving possible worlds mostly look like.\n\nThe surviving worlds don't look like somebody came up with a harebrained scheme, dismissed all the obvious reasons it wouldn't work with \"But we have to bet on it working,\" and then it worked.\n\nThat's the elaborate argument about what's rational in terms of expected utility, once reasonable second-order commonsense adjustments are taken into account.  Note, however, that if you have grasped the intended emotional connotations of \"die with dignity\", it's a heuristic that yields the same answer much faster.  It's not dignified to pretend we're less doomed than we are, or step out of reality to live somewhere else.\n\n**Q3:  Should I scream and run around and go through the streets wailing of doom?**\n\nA:  No, that's not very dignified.  Have a private breakdown in your bedroom, or a breakdown with a trusted friend, if you must.\n\nQ3:  Why is that bad from a coldly calculating expected utility perspective, though?\n\nA:  Because it associates belief in reality with people who act like idiots and can't control their emotions, which worsens our strategic position in possible worlds where we get an unexpected hope.\n\n**Q4:  Should I lie and pretend everything is fine, then?  Keep everyone's spirits up, so they go out with a smile, unknowing?**\n\nA:  That also does not seem to me to be dignified.  If we're all going to die anyways, I may as well speak plainly before then.  If into the dark we must go, let's go there speaking the truth, to others and to ourselves, until the end.\n\nQ4:  Okay, but from a coldly calculating expected utility perspective, why isn't it good to lie to keep everyone calm?  That way, if there's an unexpected hope, everybody else will be calm and oblivious and not interfering with us out of panic, and my faction will have lots of resources that they got from lying to their supporters about how much hope there was!  Didn't you just say that people screaming and running around while the world was ending would be *unhelpful?*\n\nA:  You should never try to reason using expected utilities again.  It is an art not meant for you.  Stick to intuitive feelings henceforth.\n\nThere are, I think, people whose minds readily look for and find even the slightly-less-than-totally-obvious considerations of expected utility, what some might call \"second-order\" considerations.  Ask them to rob a bank and give the money to the poor, and they'll think *spontaneously and unprompted* about insurance costs of banking and the chance of getting caught and reputational repercussions and low-trust societies and what if everybody else did that when they thought it was a good cause; and all of these considerations will be obviously-to-them *consequences* under consequentialism.\n\nThese people are well-suited to being 'consequentialists' or 'utilitarians', because their mind naturally sees all the consequences and utilities, including those considerations that others might be tempted to call by names like \"second-order\" or \"categorical\" and so on.\n\nIf you ask them why consequentialism doesn't say to rob banks, they reply, \"Because that *actually realistically in real life* would *not have good consequences.*  Whatever it is you're about to tell me as a supposedly non-consequentialist reason why we all mustn't do that, seems to you like a strong argument, exactly because you recognize implicitly that people robbing banks *would not actually lead to happy formerly-poor people and everybody living cheerfully ever after.\"*\n\nOthers, if you suggest to them that they should rob a bank and give the money to the poor, will be able to see the helped poor as a \"consequence\" and a \"utility\", but they will not spontaneously and unprompted see all those other considerations in the formal form of \"consequences\" and \"utilities\".\n\nIf you just asked them *informally* whether it was a good or bad idea, they might ask \"What if everyone did that?\" or \"Isn't it good that we can live in a society where people can store and transmit money?\" or \"How would it make effective altruism look, if people went around doing that in the name of effective altruism?\"  But if you ask them about *consequences,* they don't spontaneously, readily, intuitively classify all these other things as \"consequences\"; they think that their mind is being steered onto a kind of formal track, a defensible track, a track of stating only things that are very direct or blatant or obvious.  They think that the rule of consequentialism is, \"If you show me a good consequence, I have to do that thing.\"\n\nIf you present them with bad things that happen if people rob banks, they don't see those as also being 'consequences'.  They see them as arguments against consequentialism; since, after all consequentialism says to rob banks, which obviously leads to bad stuff, and so bad things would end up happening if people were consequentialists.  They do not do a double-take and say \"What?\"  That consequentialism leads people to do bad things with bad outcomes is just a reasonable conclusion, so far as they can tell.\n\nPeople like this *should not be 'consequentialists' or 'utilitarians' as they understand those terms.*  They should back off from this form of reasoning that their mind is not naturally well-suited for processing in a native format, and stick to intuitively informally asking themselves what's good or bad behavior, without any special focus on what they think are 'outcomes'.\n\nIf they try to be consequentialists, they'll end up as Hollywood villains describing some grand scheme that violates a lot of ethics and deontology but sure will end up having grandiose benefits, yup, even while everybody in the audience knows perfectly well that it won't work.  You can only safely be a consequentialist if you're genre-savvy about that class of arguments - if you're not the blind villain on screen, but the person in the audience watching who sees why that won't work.\n\nQ4:  I know EAs shouldn't rob banks, so this obviously isn't directed at me, right?\n\nA:  The people of whom I speak will look for and find the reasons not to do it, *even if* they're in a social environment that doesn't have strong established injunctions against bank-robbing specifically exactly.  They'll figure it out *even if* you present them with a new problem isomorphic to bank-robbing but with the details changed.\n\nWhich is basically what you just did, in my opinion.\n\nQ4:  But from the standpoint of cold-blooded calculation -\n\nA:  *Calculations are not cold-blooded.*  What blood we have in us, warm or cold, is something we can learn to see more clearly with the light of calculation.\n\nIf you think calculations are cold-blooded, that they only shed light on cold things or make them cold, then you shouldn't do them.  Stay by the warmth in a mental format where warmth goes on making sense to you.\n\nQ4:  Yes yes fine fine but what's *the actual downside* from an expected-utility standpoint?\n\nA:  If good people were liars, that would render the words of good people meaningless as information-theoretic signals, and destroy the ability for good people to coordinate with others or among themselves.\n\nIf the world can be saved, it will be saved by people who didn't lie to themselves, and went on living inside reality until some unexpected hope appeared there.\n\nIf those people went around lying to others and paternalistically deceiving them - well, mostly, I don't think they'll have really been the types to live inside reality themselves.  But even imagining the contrary, good luck suddenly unwinding all those deceptions and getting other people to live inside reality with you, to coordinate on whatever suddenly needs to be done when hope appears, after you drove them outside reality before that point.  Why should they believe anything you say?\n\nQ4:  But wouldn't it be more clever to -\n\nA:  Stop.  Just stop.  This is why I advised you to reframe your emotional stance as dying with dignity.\n\nMaybe there'd be an argument about whether or not to violate your ethics *if the world was actually going to be saved at the end.*  But why break your deontology if *it's not even going to save the world?*  Even if you have a price, should you be that cheap?\n\nQ4  But we could maybe save the world by lying to everyone about how much hope there was, to gain resources, until -\n\nA:  You're not getting it.  Why violate your deontology if it's not going to *really actually save the world in real life,* as opposed to a pretend theoretical thought experiment where your actions have only beneficial consequences and none of the obvious second-order detriments?\n\nIt's relatively safe to be around an Eliezer Yudkowsky while the world is ending, because he's not going to do anything extreme and unethical *unless it would really actually save the world in real life,* and there are no extreme unethical actions *that would really actually save the world the way these things play out in real life,* and he knows that.  He knows that the next stupid sacrifice-of-ethics proposed won't work to save the world either, actually in real life.  He is a 'pessimist' - that is, a realist, a Bayesian who doesn't update in a predictable direction, a genre-savvy person who knows that the viewer would say if there were a villain on screen making that argument for violating ethics.  He will not, like a Hollywood villain onscreen, be deluded into thinking that some clever-sounding deontology-violation is bound to work out great, when everybody in the audience watching knows perfectly well that it won't.\n\nMy ethics aren't for sale at the price point of failure.  So if it looks like everything is going to fail, I'm a relatively safe person to be around.\n\nI'm a genre-savvy person about this genre of arguments and a Bayesian who doesn't update in a predictable direction.  So if you ask, \"But Eliezer, what happens when the end of the world is approaching, and in desperation you cling to whatever harebrained scheme has Goodharted past your filters and presented you with a false shred of hope; what then will you do?\" - I answer, \"Die with dignity.\"  Where \"dignity\" in this case means knowing perfectly well that's what would happen to some less genre-savvy person; and my choosing to do something else which is not that.  But \"dignity\" yields the same correct answer and faster.\n\nQ5:  \"Relatively\" safe?\n\nA:  It'd be disingenuous to pretend that it wouldn't be *even safer* to hang around somebody who had no clue what was coming, didn't know any mental motions for taking a worldview seriously, thought it was somebody else's problem to ever do anything, and would just cheerfully party with you until the end.\n\nWithin the class of people who know the world is ending and consider it to be their job to do something about that, Eliezer Yudkowsky is a relatively safe person to be standing next to.  At least, before you both die anyways, as is the whole problem there.\n\nQ5:  Some of your self-proclaimed fans don't strike me as relatively safe people to be around, in that scenario?\n\nA:  I failed to teach them whatever it is I know.  Had I known then what I knew now, I would have warned them not to try.\n\nIf you insist on putting it into terms of fandom, though, feel free to notice that Eliezer Yudkowsky is much closer to being a typical liberaltarian science-fiction fan, as was his own culture that actually birthed him, than he is a typical member of any subculture that might have grown up later.  Liberaltarian science-fiction fans do not usually throw away all their ethics at the first sign of trouble.  They grew up reading books where those people were the villains.\n\nPlease don't take this as a promise from me to play nice, as you define niceness; the world is ending, and also people have varying definitions of what is nice.  But I presently mostly expect to end up playing nice, because there won't be any options worth playing otherwise.\n\nIt *is* a matter of some concern to me that all this seems to be an alien logic to some strange people who - this fact is still hard for me to grasp on an emotional level - don't spontaneously generate all of this reasoning internally, as soon as confronted with the prompt.  Alas.\n\n**Q5:  Then isn't it unwise to speak plainly of these matters, when fools may be driven to desperation by them?  What if people believe you about the hopeless situation, but refuse to accept that conducting themselves with dignity is the appropriate response?**\n\nA:  I feel like I've now tried to live my life that way for a while, by the dictum of not panicking people; and, like everything else I've tried, that hasn't particularly worked?  There are no plans left to avoid disrupting, now, with other people's hypothetical panic.\n\nI think we die with slightly more dignity - come *closer* to surviving, as we die - if we are allowed to talk about these matters plainly.  Even given that people may then do unhelpful things, after being driven mad by overhearing sane conversations.  I think we die with more dignity that way, than if we go down silent and frozen and never talking about our impending death for fear of being overheard by people less sane than ourselves.\n\nI think that in the last surviving possible worlds with any significant shred of subjective probability, people survived in part because they talked about it; even if that meant other people, the story's antagonists, might possibly hypothetically panic.\n\nBut still, one should present the story-antagonists with an easy line of retreat.  So -\n\n**Q6:  Hey, this was posted on April 1st.  All of this is just an April Fool's joke, right?**\n\nA:  Why, of course!  Or rather, it's a preview of what might be needful to say later, if matters really do get that desperate.  You don't want to drop that on people suddenly and with no warning.\n\nQ6:  Oh.  Really?  That would be such a relief!\n\nA:  Only you can decide whether to live in one mental world or the other.\n\nQ6:  Wait, now I'm confused.  How do I decide which mental world to live in?\n\nA:  By figuring out what is true, and by allowing no other considerations than that to enter; that's dignity.\n\nQ6:  But that doesn't directly answer the question of which world I'm supposed to mentally live in!  Can't somebody just tell me that?\n\nA:  Well, conditional on you wanting somebody to tell you that, I'd remind you that many EAs hold that it is very epistemically unvirtuous to just believe what one person tells you, and not weight their opinion and mix it with the weighted opinions of others?\n\nLots of very serious people will tell you that AGI is thirty years away, and that's plenty of time to turn things around, and nobody really knows anything about this subject matter anyways, and there's all kinds of plans for alignment that haven't been solidly refuted so far as they can tell.\n\nI expect the sort of people who are very moved by that argument, to be happier, more productive, and less disruptive, living mentally in that world.\n\nQ6:  Thanks for answering my question!  But aren't I supposed to assign some small probability to your worldview being correct?\n\nA:  Conditional on you being the sort of person who thinks you're obligated to do that and that's the reason you should do it, I'd frankly rather you didn't.  Or rather, seal up that small probability in a safe corner of your mind which only tells you to stay out of the way of those gloomy people, and not get in the way of any hopeless plans they seem to have.\n\nQ6:  Got it.  Thanks again!\n\nA:  You're welcome!  Goodbye and have fun!",
      "plaintextDescription": "tl;dr:  It's obvious at this point that humanity isn't going to solve the alignment problem, or even try very hard, or even go out with much of a fight.  Since survival is unattainable, we should shift the focus of our efforts to helping humanity die with with slightly more dignity.\n\n----------------------------------------\n\nWell, let's be frank here.  MIRI didn't solve AGI alignment and at least knows that it didn't.  Paul Christiano's incredibly complicated schemes have no chance of working in real life before DeepMind destroys the world.  Chris Olah's transparency work, at current rates of progress, will at best let somebody at DeepMind give a highly speculative warning about how the current set of enormous inscrutable tensors, inside a system that was recompiled three weeks ago and has now been training by gradient descent for 20 days, might possibly be planning to start trying to deceive its operators.\n\nManagement will then ask what they're supposed to do about that.\n\nWhoever detected the warning sign will say that there isn't anything known they can do about that.  Just because you can see the system might be planning to kill you, doesn't mean that there's any known way to build a system that won't do that.  Management will then decide not to shut down the project - because it's not certain that the intention was really there or that the AGI will really follow through, because other AGI projects are hard on their heels, because if all those gloomy prophecies are true then there's nothing anybody can do about it anyways.  Pretty soon that troublesome error signal will vanish.\n\nWhen Earth's prospects are that far underwater in the basement of the logistic success curve, it may be hard to feel motivated about continuing to fight, since doubling our chances of survival will only take them from 0% to 0%.\n\nThat's why I would suggest reframing the problem - especially on an emotional level - to helping humanity die with dignity, or rather, since even this goal is rea",
      "wordCount": 5326
    },
    "tags": [
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "7w6XkYe5YPx9YL59j",
        "name": "Information Hazards",
        "slug": "information-hazards"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "tcCxPLBrEXdxN5HCQ",
    "title": "Shah and Yudkowsky on alignment failures",
    "slug": "shah-and-yudkowsky-on-alignment-failures",
    "url": null,
    "baseScore": 91,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 47,
    "createdAt": null,
    "postedAt": "2022-02-28T19:18:23.015Z",
    "contents": {
      "markdown": "This is the final discussion log in the [Late 2021 MIRI Conversations](https://www.lesswrong.com/s/n945eovrA3oDueqtq) sequence, featuring Rohin Shah and Eliezer Yudkowsky, with additional comments from Rob Bensinger, Nate Soares, Richard Ngo, and Jaan Tallinn.\n\nThe discussion begins with summaries and comments on Richard and Eliezer's debate. Rohin's summary has since been revised and published [in the Alignment Newsletter](https://www.alignmentforum.org/posts/3vFmQhHBosnjZXuAJ/an-171-disagreements-between-alignment-optimists-and).\n\nAfter this log, we'll be concluding this sequence with an [**AMA**](https://www.lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-discussion-and-ama), where we invite you to comment with questions about AI alignment, cognition, forecasting, etc. Eliezer, Richard, Paul Christiano, Nate, and Rohin will all be participating.\n\nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Rohin and Eliezer&nbsp;</td><td style=\"background-color:#F2F2F2;border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Emails&nbsp;</td><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Follow-ups&nbsp;</td></tr></tbody></table>\n\n19\\. Follow-ups to the Ngo/Yudkowsky conversation\n=================================================\n\n19.1. Quotes from the public discussion\n---------------------------------------\n\n<table><tbody><tr><td style=\"background-color:#F2F2F2;border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][9:22]</strong> &nbsp;<strong>(Nov. 25)</strong>&nbsp;</p><p>Interesting extracts from the public discussion of <a href=\"https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/hwxj4gieR7FWNwYfa\">Ngo and Yudkowsky on AI capability gains</a>:</p><p><i>Eliezer</i>:</p><blockquote><p>I think some of your confusion may be that you're putting \"probability theory\" and \"Newtonian gravity\" into the same bucket. &nbsp;You've been raised to believe that powerful theories ought to meet certain standards, like successful bold advance experimental predictions, such as Newtonian gravity made about the existence of Neptune (quite a while after the theory was first put forth, though). &nbsp;\"Probability theory\" also sounds like a powerful theory, and the people around you believe it, so you think you ought to be able to produce a powerful advance prediction it made; but it is for some reason hard to come up with an example like the discovery of Neptune, so you cast about a bit and think of the central limit theorem. &nbsp;That theorem is widely used and praised, so it's \"powerful\", and it wasn't invented <i>before</i> probability theory, so it's \"advance\", right? &nbsp;So we can go on putting probability theory in the same bucket as Newtonian gravity?</p><p>They're actually just very different kinds of ideas, ontologically speaking, and the standards to which we hold them are properly different ones. &nbsp;It seems like the sort of thing that would take a subsequence I don't have time to write, expanding beyond the underlying obvious ontological difference between validities and empirical-truths, to cover the way in which \"How do we trust this, when\" differs between \"I have the following new empirical theory about the underlying model of gravity\" and \"I think that the logical notion of 'arithmetic' is a good tool to use to organize our current understanding of this little-observed phenomenon, and it appears within making the following empirical predictions...\" &nbsp;But at least step one could be saying, \"Wait, do these two kinds of ideas actually go into the same bucket at all?\"</p><p>In particular it seems to me that you want properly to be asking \"How do we know this empirical thing ends up looking like it's close to the abstraction?\" and not \"Can you show me that this abstraction is a very powerful one?\" &nbsp;Like, imagine that instead of asking Newton about planetary movements and how we know that the particular bits of calculus he used were empirically true about the planets in particular, you instead started asking Newton for proof that calculus is a very powerful piece of mathematics worthy to predict the planets themselves - but in a way where you wanted to see some highly valuable material object that calculus had <i>produced, </i>like earlier praiseworthy achievements in alchemy<i>.</i> &nbsp;I think this would reflect confusion and a wrongly directed inquiry; you would have lost sight of the particular reasoning steps that made ontological sense, in the course of trying to figure out whether calculus was praiseworthy under the standards of praiseworthiness that you'd been previously raised to believe in as universal standards about all ideas.</p></blockquote><p><i>Richard</i>:</p><blockquote><p>I agree that \"powerful\" is probably not the best term here, so I'll stop using it going forward (note, though, that I didn't use it in my previous comment, which I endorse more than my claims in the original debate).</p><p>But before I ask \"How do we know this empirical thing ends up looking like it's close to the abstraction?\", I need to ask \"Does the abstraction even make sense?\" Because you have the abstraction in your head, and I don't, and so whenever you tell me that X is a (non-advance) prediction of your theory of consequentialism, I end up in a pretty similar epistemic state as if George Soros tells me that X is a prediction of the <a href=\"https://en.wikipedia.org/wiki/Reflexivity_(social_theory)\">theory of reflexivity</a>, or if a complexity theorist tells me that X is a prediction of the <a href=\"https://en.wikipedia.org/wiki/Self-organization\">theory of self-organisation</a>. The problem in those two cases is less that the abstraction is a bad fit for this specific domain, and more that the abstraction is not sufficiently well-defined (outside very special cases) to even be the type of thing that can robustly make predictions.</p><p>Perhaps another way of saying it is that they're not crisp/robust/coherent concepts (although I'm open to other terms, I don't think these ones are particularly good). And it would be useful for me to have evidence that the abstraction of consequentialism you're using is a crisper concept than Soros' theory of reflexivity or the theory of self-organisation. If you could explain the full abstraction to me, that'd be the most reliable way - but given the difficulties of doing so, my backup plan was to ask for impressive advance predictions, which are the type of evidence that I don't think Soros could come up with.</p><p>I also think that, when you talk about me being raised to hold certain standards of praiseworthiness, you're still ascribing too much modesty epistemology to me. I mainly care about novel predictions or applications insofar as they help me distinguish crisp abstractions from evocative metaphors. To me it's the same type of rationality technique as asking people to make bets, to help distinguish post-hoc confabulations from actual predictions.</p><p>Of course there's a social component to both, but that's not what I'm primarily interested in. And of course there's a strand of naive science-worship which thinks you have to follow the Rules in order to get anywhere, but I'd thank you to assume I'm at least making a more interesting error than that.</p><p>Lastly, on probability theory and Newtonian mechanics: I agree that you shouldn't question how much sense it makes to use calculus in the way that you described, but that's because the application of calculus to mechanics is so clearly-defined that it'd be very hard for the type of confusion I talked about above to sneak in. I'd put evolutionary theory halfway between them: it's partly a novel abstraction, and partly a novel empirical truth. And in this case I do think you have to be very careful in applying the core abstraction of evolution to things like cultural evolution, because it's easy to do so in a confused way.</p></blockquote></td></tr></tbody></table>\n\n19.2. Rohin Shah's summary and thoughts\n---------------------------------------\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][7:06] &nbsp;(Nov. 6 email)</strong>&nbsp;</p><p>Newsletter summaries attached, would appreciate it if Eliezer and Richard checked that I wasn't misrepresenting them. (Conversation&nbsp;is a lot harder to accurately summarize than blog posts or papers.)</p><p>&nbsp;</p><p>Best,</p><p>Rohin</p><p>&nbsp;</p><p><i>Planned summary for the Alignment Newsletter:</i></p><p>&nbsp;</p><p>Eliezer is known for being pessimistic about our chances of averting AI catastrophe. His main argument is roughly as follows:</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][9:56] &nbsp;(Nov. 6 email reply)</strong>&nbsp;</p><blockquote><p>[...] Eliezer is known for being pessimistic about our chances of averting AI catastrophe. His main argument</p></blockquote><p>I request that people stop describing things as my \"main argument\" unless I've described them that way myself. &nbsp;These are answers that I customized for Richard Ngo's questions. &nbsp;Different questions would get differently emphasized replies. &nbsp;\"His argument in the dialogue with Richard Ngo\" would be fine.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][1:53] &nbsp;(Nov. 8 email reply)</strong>&nbsp;</p><blockquote><p>I request that people stop describing things as my \"main argument\" unless I've described them that way myself.</p></blockquote><p>Fair enough. It still does seem pretty relevant to know the purpose of the argument, and I would like to state something along those lines in the summary. For example, perhaps it is:</p><ol><li>One of several relatively-independent lines of argument that suggest we're doomed; cutting this argument would make almost no difference to the overall take</li><li>Your main argument, but with weird Richard-specific emphases that you wouldn't have necessarily included if making this argument more generally; if someone refuted the core of the argument to your satisfaction it would make a big difference to your overall take</li><li>Not actually an argument you think much about at all, but somehow became the topic of discussion</li><li>Something in between these options</li><li>Something else entirely</li></ol><p>If you can't really say, then I guess I'll just say \"His argument in this particular dialogue\".</p><p>I'd also like to know what the main argument is (if there is a main argument rather than lots of independent lines of evidence or something else entirely); it helps me orient to the discussion, and I suspect would be useful for newsletter readers as well.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][7:06] &nbsp;(Nov. 6 email)</strong>&nbsp;</p><p>1. We are very likely going to keep improving AI capabilities until we reach AGI, at which point either the world is destroyed, or we use the AI system to take some pivotal act before some careless actor destroys the world.</p><p>2. In either case, the AI system must be producing high-impact, world-rewriting plans; such plans are “consequentialist” in that the simplest way to get them (and thus, the one we will first build) is if you are forecasting what might happen, thinking about the expected consequences, considering possible obstacles, searching for routes around the obstacles, etc. If you don’t do this sort of reasoning, your plan goes off the rails very quickly; it is highly unlikely to lead to high impact. In particular, long lists of shallow heuristics (as with current deep learning systems) are unlikely to be enough to produce high-impact plans.</p><p>3. We’re producing AI systems by selecting for systems that can do impressive stuff, which will eventually produce AI systems that can accomplish high-impact plans using a general underlying “consequentialist”-style reasoning process (because that’s the only way to keep doing more impressive stuff). However, this selection process does <i>not</i> constrain the goals towards which those plans are aimed. In addition, most goals seem to have convergent instrumental subgoals like survival and power-seeking that would lead to extinction. This suggests that, unless we find a way to constrain the goals towards which plans are aimed, we should expect an existential catastrophe.</p><p>4. None of the methods people have suggested for avoiding this outcome seem like they actually avert this story.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][9:56] &nbsp;(Nov. 6 email reply)</strong>&nbsp;</p><blockquote><p>[...] This suggests that, unless we find a way to constrain the goals towards which plans are aimed, we should expect an existential catastrophe.</p></blockquote><p>I would not say we face catastrophe \"unless we find a way to constrain the goals towards which plans are aimed\". &nbsp;This is, first of all, not my ontology, second, I don't go around randomly slicing away huge sections of the solution space. &nbsp;Workable: &nbsp;\"This suggests that we should expect an existential catastrophe by default.\"&nbsp;</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][1:53] &nbsp;(Nov. 8 email reply)</strong>&nbsp;</p><blockquote><p>I would not say we face catastrophe \"unless we find a way to constrain the goals towards which plans are aimed\".</p></blockquote><p>Should I also change \"However, this selection process does <i>not</i> constrain the goals towards which those plans are aimed\", and if so what to? (Something along these lines seems crucial to the argument, but if this isn't your native ontology, then presumably you have some other thing you'd say here.)</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][7:06] &nbsp;(Nov. 6 email)</strong>&nbsp;</p><p>Richard responds to this with a few distinct points:</p><p>1. It might be possible to build narrow AI systems that humans use to save the world, for example, by making AI systems that do better alignment research. Such AI systems do not seem to require the property of making long-term plans in the real world in point (3) above, and so could plausibly be safe. We might say that narrow AI systems could save the world but can’t destroy it, because humans will put plans into action for the former but not the latter.</p><p>2. It might be possible to build general AI systems that only <i>state</i> plans for achieving a goal of interest that we specify, without <i>executing</i> that plan.</p><p>3. It seems possible to create consequentialist systems with constraints upon their reasoning that lead to reduced risk.</p><p>4. It also seems possible to create systems that make effective plans, but towards ends that are not about outcomes in the real world, but instead are about properties of the plans -- think for example of <a href=\"https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility\"><i>corrigibility</i></a> (<a href=\"https://mailchi.mp/bbd47ba94e84/alignment-newsletter-35\"><i>AN #35</i></a>) or deference to a human user.</p><p>5. (Richard is also more bullish on coordinating not to use powerful and/or risky AI systems, though the debate did not discuss this much.)</p><p>&nbsp;</p><p>Eliezer’s responses:</p><p>1. This is plausible, but seems unlikely; narrow not-very-consequentialist AI (aka “long lists of shallow heuristics”) will probably not scale to the point of doing alignment research better than humans.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][9:56] &nbsp;(Nov. 6 email reply)</strong>&nbsp;</p><blockquote><p>[...] This is plausible, but seems unlikely; narrow not-very-consequentialist AI (aka “long lists of shallow heuristics”) will probably not scale to the point of doing alignment research better than humans.</p></blockquote><p>No, your summarized-Richard-1 is just not plausible. &nbsp;\"AI systems that do better alignment research\" are dangerous in virtue of the lethally powerful work they are doing, not because of some particular narrow way of doing that work. &nbsp;If you can do it by gradient descent then that means gradient descent got to the point of doing lethally dangerous work. &nbsp;Asking for safely weak systems that do world-savingly strong tasks is almost everywhere a case of asking for nonwet water, and asking for AI that does alignment research is an extreme case in point.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][1:53] &nbsp;(Nov. 8 email reply)</strong>&nbsp;</p><blockquote><p>No, your summarized-Richard-1 is just not plausible. \"AI systems that do better alignment research\" are dangerous in virtue of the lethally powerful work they are doing, not because of some particular narrow way of doing that work.</p></blockquote><p>How about \"AI systems that help with alignment research to a sufficient degree that it actually makes a difference are almost certainly already dangerous.\"?</p><p>(Fwiw, I used the word \"plausible\" because of this sentence from the doc: \"<i>Definitely, &lt;description of summarized-Richard-1&gt; is among the more </i>plausible<i> advance-specified miracles we could get.</i>\", though I guess the point was that it is still a miracle, it just also is more likely than other miracles.)</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][9:59] &nbsp;(Nov. 6 email reply)</strong>&nbsp;</p><p>Thanks Rohin! Your efforts are much appreciated.</p><p>Eliezer: when you say \"No, your summarized-Richard-1 is just not plausible\", do you mean the argument is implausible, or it's not a good summary of my position (which you also think is implausible)?</p><p>For my part the main thing I'd like to modify is the term \"narrow AI\". In general I'm talking about all systems that are not of literally world-destroying intelligence+agency. E.g. including oracle AGIs which I wouldn't call \"narrow\".</p><p>More generally, I don't think all AGIs are capable of destroying the world. E.g. humans are GIs. So it might be better to characterise Eliezer as talking about <i>some</i> level of general intelligence which leads to destruction, and me as talking about the things that can be done with systems that are less general or less agentic than that.</p><blockquote><p>We might say that narrow AI systems could save the world but can’t destroy it, because humans will put plans into action for the former but not the latter.</p></blockquote><p>I don't endorse this, I think plenty of humans would be willing to use narrow AI systems to do things that could destroy the world.</p><blockquote><p>systems that make effective plans, but towards ends that are not about outcomes in the real world, but instead are about properties of the plans</p></blockquote><p>I'd change this to say \"systems with the primary aim of producing plans with certain properties (that aren't just about outcomes in the world)\"&nbsp;</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:18] &nbsp;(Nov. 6 email reply)</strong>&nbsp;</p><blockquote><p>Eliezer: when you say \"No, your summarized-Richard-1 is just not plausible\", do you mean the argument is implausible, or it's not a good summary of my position (which you also think is implausible)?</p></blockquote><p>I wouldn't have presumed to state on your behalf whether it's a good summary of your position! &nbsp;I mean that the stated position is implausible, whether or not it was a good summary of your position.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][7:06] &nbsp;(Nov. 6 email)</strong>&nbsp;</p><p>2. This might be an improvement, but not a big one. It is the plan itself that is risky; if the AI system made a plan for a goal that wasn’t the one we actually meant, and we don’t understand that plan, that plan can still cause extinction. It is the <i>misaligned optimization that produced the plan</i> that is dangerous, even if there was no “agent” that specifically wanted the goal that the plan was optimized for.</p><p>3 and 4. It is certainly <i>possible</i> to do such things; the space of minds that could be designed is very large. However, it is <i>difficult</i> to do such things, as they tend to make consequentialist reasoning weaker, and on our current trajectory the first AGI that we build will probably not look like that.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][9:56] &nbsp;(Nov. 6 email reply)</strong>&nbsp;</p><blockquote><p>2. This might be an improvement, but not a big one. It is the plan itself that is risky; if the AI system made a plan for a goal that wasn’t the one we actually meant, and we don’t understand that plan, that plan can still cause extinction. It is the <i>misaligned optimization that produced the plan</i> that is dangerous, even if there was no “agent” that specifically wanted the goal that the plan was optimized for.</p></blockquote><p>No, it's not a significant improvement if the \"non-executed plans\" from the system are meant to do things in human hands powerful enough to save the world. &nbsp;They could of course be so weak as to make their human execution have no inhumanly big consequences, but this is just making the AI strategically isomorphic to a rock. &nbsp;The notion of there being \"no 'agent' that specifically wanted the goal\" seems confused to me as well; this is not something I'd ever say as a restatement of one of my own opinions. &nbsp;I'd shrug and tell someone to taboo the word 'agent' and would try to talk without using the word if they'd gotten hung up on that point.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah][7:06] &nbsp;(Nov. 6 email)</strong>&nbsp;</p><p><i>Planned opinion:</i></p><p>&nbsp;</p><p>I first want to note my violent agreement with the notion that a major scary thing is “consequentialist reasoning”, and that high-impact plans require such reasoning, and that we will end up building AI systems that produce high-impact plans. Nonetheless, I am still optimistic about AI safety relative to Eliezer, which I suspect comes down to three main disagreements:</p><p>1. There are many approaches that don’t solve the problem, but do increase the level of intelligence required before the problem leads to extinction. Examples include Richard’s points 1-4 above. For example, if we build a system that states plans without executing them, then for the plans to cause extinction they need to be complicated enough that the humans executing those plans don’t realize that they are leading to an outcome that was not what they wanted. It seems non-trivially probable to me that such approaches are sufficient to prevent extinction up to the level of AI intelligence needed before we can execute a pivotal act.</p><p>2. The consequentialist reasoning is only scary to the extent that it is “aimed” at a bad goal. It seems non-trivially probable to me that it will be “aimed” at a goal sufficiently good to not lead to existential catastrophe, without putting in much alignment effort.<br>3. I do expect some coordination to not do the most risky things.</p><p>I wish the debate had focused more on the claim that narrow AI can’t e.g. do better alignment research, as it seems like a major crux. (For example, I think that sort of intuition drives my disagreement #1.) I expect AI progress looks a lot like “the heuristics get less and less shallow in a gradual / smooth / continuous manner” which eventually leads to the sorts of plans Eliezer calls “consequentialist”, whereas I think Eliezer expects a sharper qualitative change between “lots of heuristics” and that-which-implements-consequentialist-planning.</p></td></tr></tbody></table>\n\n20\\. November 6 conversation\n============================\n\n20.1. Concrete plans, and AI-mediated transparency\n--------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:22]</strong>&nbsp;</p><p>So I have a general thesis about a failure mode here which is that, the moment you try to sketch any concrete plan or events which correspond to the abstract descriptions, it is much more obviously wrong, and that is why the descriptions stay so abstract in the mouths of everybody who sounds more optimistic than I am.</p><p>This may, perhaps, be confounded by the phenomenon where I am one of the last living descendants of the lineage that ever knew how to say anything concrete at all. &nbsp;Richard Feynman - or so I would now say in retrospect - is noticing concreteness dying out of the world, and being worried about that, at the point where he goes to a college and hears a professor talking about \"essential objects\" in class, and Feynman asks \"Is a brick an essential object?\" - meaning to work up to the notion of the inside of a brick, which can't be observed because breaking a brick in half just gives you two new exterior surfaces - and everybody in the classroom has a different notion of what it would mean for a brick to be an essential object.&nbsp;</p><p>Richard Feynman knew to try plugging in bricks as a special case, but the people in the classroom didn't, and I think the mental motion has died out of the world even further since Feynman wrote about it. &nbsp;The loss has spread to STEM as well. &nbsp;Though if you don't read old books and papers and contrast them to new books and papers, you wouldn't see it, and maybe most of the people who'll eventually read this will have no idea what I'm talking about because they've never seen it any other way...</p><p>I have a thesis about how optimism over AGI works. &nbsp;It goes like this: People use really abstract descriptions and never imagine anything sufficiently concrete, and this lets the abstract properties waver around ambiguously and inconsistently to give the desired final conclusions of the argument. &nbsp;So MIRI is the only voice that gives concrete examples and also by far the most pessimistic voice; if you go around fully specifying things, you can see that what gives you a good property in one place gives you a bad property someplace else, you see that you can't get all the properties you want simultaneously. &nbsp;Talk about a superintelligence building nanomachinery, talk concretely about megabytes of instructions going to small manipulators that repeat to lay trillions of atoms in place, and this shows you a lot of useful visible power paired with such unpleasantly visible properties as \"no human could possibly check what all those instructions were supposed to do\".</p><p>Abstract descriptions, on the other hand, can waver as much as they need to between what's desirable in one dimension and undesirable in another. &nbsp;Talk about \"an AGI that just helps humans instead of replacing them\" and never say exactly what this AGI is supposed to do, and this can be so much more optimistic so long as it never becomes too unfortunately concrete.</p><p>When somebody asks you \"how powerful is it?\" you can momentarily imagine - without writing it down - that the AGI is helping people by giving them the full recipes for protein factories that build second-stage nanotech and the instructions to feed those factories, and reply, \"Oh, super powerful! More than powerful enough to flip the gameboard!\" Then when somebody asks how safe it is, you can momentarily imagine that it's just giving a human mathematician a hint about proving a theorem, and say, \"Oh, super duper safe, for sure, it's just helping people!\"&nbsp;</p><p>Or maybe you don't even go through the stage of momentarily imagining the nanotech and the hint, maybe you just navigate straight in the realm of abstractions from the impossibly vague wordage of \"just help humans\" to the reassuring and also extremely vague \"help them lots, super powerful, very safe tho\".</p><blockquote><p>[...] I wish the debate had focused more on the claim that narrow AI can’t e.g. do better alignment research, as it seems like a major crux. (For example, I think that sort of intuition drives my disagreement #1.) I expect AI progress looks a lot like “the heuristics get less and less shallow in a gradual / smooth / continuous manner” which eventually leads to the sorts of plans Eliezer calls “consequentialist”, whereas I think Eliezer expects a sharper qualitative change between “lots of heuristics” and that-which-implements-consequentialist-planning.</p></blockquote><p>It is in this spirit that I now ask, \"What the hell could it look like concretely for a safely narrow AI to help with alignment research?\"</p><p>Or if you think that a left-handed wibble planner can totally make useful plans that are very safe because it's all leftish and wibbly: can you please give an example of <i>a plan to do what?</i></p><p>And what I expect is for minds to bounce off that problem as they first try to visualize \"Well, a plan to give mathematicians hints for proving theorems... oh, Eliezer will just say that's not useful enough to flip the gameboard... well, plans for building nanotech... Eliezer will just say that's not safe... darn it, this whole concreteness thing is such a conversational no-win scenario, maybe there's something abstract I can say instead\".</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][16:41]</strong>&nbsp;</p><p>It's reasonable to suspect failures to be concrete, but I don't buy that hypothesis as applied to me; I think I have sufficient personal evidence against it, despite the fact that I usually speak abstractly. I don't expect to convince you of this, nor do I particularly want to get into that sort of debate.</p><p>I'll note that I have the exact same experience of not seeing much concreteness, both of other people and myself, about stories that lead to doom. To be clear, in what I take to be the Eliezer-story, the part where the misaligned AI designs a pathogen that wipes out all humans or solves nanotech and gains tons of power or some other pivotal act seems fine. The part that seems to lack concreteness is how we built the superintelligence and why the superintelligence was misaligned enough to lead to extinction. (Well, perhaps. I also wouldn't be surprised if you gave a concrete example and I disagreed that it would lead to extinction.)</p><p>From my perspective, the simple concrete stories about the future are wrong and the complicated concrete stories about the future don't sound plausible, whether about safety or about doom.</p><p>Nonetheless, here's an attempt at some concrete stories. It is <i>not</i> the case that I think these would be convincing to you. I do expect you to say that it won't be useful enough to flip the gameboard (or perhaps that if it could possibly flip the gameboard then it couldn't be safe), but that seems to be because you think alignment will be way more difficult than I do (in expectation), and perhaps we should get into that instead.</p><ul><li>Instead of having to handwrite code that does feature visualization or other methods of \"naming neurons\", an AI assistant can automatically inspect a neural net's weights, perform some experiments with them, and give them human-understandable \"names\". What a \"name\" is depends on the system being analyzed, but you could imagine that sometimes it's short memorable phrases (e.g. for the later layers of a language model), or pictures of central concepts (e.g. for image classifiers), or paragraphs describing the concept (e.g. for novel concepts discovered by a scientist AI). Given these names, it is much easier for humans to read off \"circuits\" from the neural net to understand how it works.</li><li>Like the above, except the AI assistant also reads out the circuits, and efficiently reimplements the neural network in, say, readable Python, that humans can then more easily mechanistically understand. (These two tasks could also be done by two different AI systems, instead of the same one; perhaps that would be easier / safer.)</li><li>We have AI assistants search for inputs on which the AI system being inspected would do something that humans would rate as bad. (We can choose any not-horribly-unnatural rating scheme we want that humans can understand, e.g. \"don't say something the user said not to talk about, even if it's in their best interest\" can be a tenet for finetuned GPT-N if we want.) We can either train on those inputs, or use them as a test for how well our other alignment schemes have worked.</li></ul><p>(These are all basically leveraging the fact that we could have AI systems that are really knowledgeable in the realm of \"connecting neural net activations to human concepts\", which seems plausible to do without being super general or consequentialist.)</p><p>There's also lots of meta stuff, like helping us with literature reviews, speeding up paper- and blog-post-writing, etc, but I doubt this is getting at what you care about</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:09]</strong>&nbsp;</p><p>If we thought that helping with literature review was enough to save the world from extinction, then we should be trying to spend at least $50M on helping with literature review right now today, and if we can't effectively spend $50M on that, then we also can't build the dataset required to train narrow AI to do literature review. &nbsp;Indeed, any time somebody suggests doing something weak with AGI, my response is often \"Oh how about we start on that right now using humans, then,\" by which question its pointlessness is revealed.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:11]</strong>&nbsp;</p><p>I mean, doesn't seem crazy to just spend $50M on effective PAs, but in any case I agree with you that this is not the main thing to be thinking about</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:13]</strong>&nbsp;</p><p>The other cases of \"using narrow AI to help with alignment\" via pointing an AI, or rather a loss function, at a transparency problem, seem to seamlessly blend into all of the other clever-ideas we may have for getting more insight into the giant inscrutable matrices of floating-point numbers. &nbsp;By this concreteness, it is revealed that we are not speaking of von-Neumann-plus-level AGIs who come over and firmly but gently set aside our paradigm of giant inscrutable matrices, and do something more alignable and transparent; rather, we are trying more tricks with loss functions to get human-language translations of the giant inscrutable matrices.</p><p>I have thought of various possibilities along these lines myself. &nbsp;They're on my list of things to try out when and if the EA community has the capacity to try out ML ideas in a format I could and would voluntarily access.</p><p>There's a basic reason I expect the world to die despite my being able to generate infinite clever-ideas for ML transparency, which, at the usual rate of 5% of ideas working, could get us as many as three working ideas in the impossible event that the facilities were available to test 60 of my ideas.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:15]</strong>&nbsp;</p><blockquote><p>By this concreteness, it is revealed that we are not speaking of von-Neumann-plus-level AGIs who come over and firmly but gently set aside our paradigm of giant inscrutable matrices, and do something more alignable and transparent; rather, we are trying more tricks with loss functions to get human-language translations of the giant inscrutable matrices.</p></blockquote><p>Agreed, but I don't see the point here</p><p>(Beyond \"Rohin and Eliezer disagree on how impossible it is to align giant inscrutable matrices\")</p><p>(I might dispute \"tricks with loss functions\", but that's nitpicky, I think)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:16]</strong>&nbsp;</p><p>It's that, if we get better transparency, we are then left looking at stronger evidence that our systems are planning to kill us, but this will not help us because we will not have anything we can do to make the system <i>not</i> plan to kill us.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:18]</strong>&nbsp;</p><p>The adversarial training case is one example where you are trying to change the system, and if you'd like I can generate more along these lines, but they aren't going to be that different and are still going to come down to what I expect you will call \"playing tricks with loss functions\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:18]</strong>&nbsp;</p><p>Well, part of the point is that \"AIs helping us with alignment\" is, from my perspective, a classic case of something that might ambiguate between the version that concretely corresponds to \"they are very smart and can give us the Textbook From The Future that we can use to easily build a robust superintelligence\" (which is powerful, pivotal, unsafe, and kills you) or \"they can help us with literature review\" (safe, weak, unpivotal) or \"we're going to try clever tricks with gradient descent and loss functions and labeled datasets to get alleged natural-language translations of some of the giant inscrutable matrices\" (which was always the plan but which I expected to not be sufficient to avert ruin).</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:19]</strong>&nbsp;</p><p>I'm definitely thinking of the last one, but I take your point that disambiguating between these is good</p><p>And I also think it's revealing that this is not in fact the crux of disagreement</p></td></tr></tbody></table>\n\n20.2. Concrete disaster scenarios, out-of-distribution problems, and corrigibility\n----------------------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:20]</strong>&nbsp;</p><blockquote><p>I'll note that I have the exact same experience of not seeing much concreteness, both of other people and myself, about stories that lead to doom.</p></blockquote><p>I have a boundless supply of greater concrete detail for the asking, though if you ask large questions I may ask for a narrower question to avoid needing to supply 10,000 words of concrete detail.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:24]</strong>&nbsp;</p><p>I guess the main thing is to have an example of a story which includes a method for building a superintelligence (yes, I realize this is info-hazard-y, sorry, an abstract version might work) + how it becomes misaligned and what its plans become optimized for. Though as I type this out I realize that I'm likely going to disagree on the feasibility of the method for building a superintelligence?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:25]</strong>&nbsp;</p><p>I mean, I'm obviously not going to want to make any suggestions that I think could possibly work and which are not very very <i>very</i> obvious.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:25]</strong>&nbsp;</p><p>Yup, makes sense</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:25]</strong>&nbsp;</p><p>But I don't think that's much of an issue.</p><p>I could just point to MuZero, say, and say, \"Suppose something a lot like this scaled.\"</p><p>Do I need to explain how you would die in this case?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:26]</strong>&nbsp;</p><p>What sort of domain and what training data?</p><p>Like, do we release a robot in the real world, have it collect data, build a world model, and run MuZero with a reward for making a number in a bank account go up?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:28]</strong>&nbsp;</p><p>Supposing they're naive about it: playing all the videogames, predicting all the text and images, solving randomly generated computer puzzles, accomplishing sets of easily-labelable sensorymotor tasks using robots and webcams</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:29]</strong>&nbsp;</p><p>Okay, so far I'm with you. Is there a separate deployment step, and if so, how did they finetune the agent for the deployment task? Or did it just take over the world halfway through training?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:29]</strong>&nbsp;</p><p>(though this starts to depart from the Mu Zero architecture if it has the ability to absorb knowledge via learning on more purely predictive problems)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:30]</strong>&nbsp;</p><p>(I'm okay with that, I think)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:32]</strong>&nbsp;</p><p>vaguely plausible rough scenario: there was a big ongoing debate about whether or not to try letting the system trade stocks, and while the debate was going on, the researchers kept figuring out ways to make Something Zero do more with less computing power, and then it started visibly talking at people and trying to manipulate them, and there was an enormous fuss, and what happens past this point depends on whether or not you want me to try to describe a scenario in which we die with an unrealistic amount of dignity, or a realistic scenario where we die much faster</p><p>I shall assume the former.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:32]</strong>&nbsp;</p><p>Actually I think I want concreteness earlier</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:32]</strong>&nbsp;</p><p>Okay. &nbsp;I await your further query.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:32]</strong>&nbsp;</p><blockquote><p>it started visibly talking at people and trying to manipulate them</p></blockquote><p>What caused this?</p><p>Was it manipulating people in order to make e.g. sensory stuff easier to predict?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:36]</strong>&nbsp;</p><p>Cumulative lifelong learning from playing videogames took its planning abilities over a threshold; cumulative solving of computer games and multimodal real-world tasks took its internal mechanisms for unifying knowledge and making them coherent over a threshold; and it gained sufficient compressive understanding of the data it had implicitly learned by reading through hundreds of terabytes of Common Crawl, not so much the semantic knowledge contained in those pages, but the associated implicit knowledge of the Things That Generate Text (aka humans).&nbsp;</p><p>These combined to form an imaginative understanding that some of its real-world problems were occurring in interactions with the Things That Generate Text, and it started making plans which took that into account and tried to have effects on the Things That Generate Text in order to affect the further processes of its problems.</p><p>Or perhaps somebody trained it to write code in partnership with programmers and it already had experience coworking with and manipulating humans.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:39]</strong>&nbsp;</p><p>Checking understanding: At this point it is able to make novel plans that involve applying knowledge about humans and their role in the data-generating process in order to create a plan that leads to more reward for the real-world problems?</p><p>(Which we call \"manipulating humans\")</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:40]</strong>&nbsp;</p><p>Yes, much as it might have gained earlier experience with making novel Starcraft plans that involved \"applying knowledge about humans and their role in the data-generating process in order to create a plan that leads to more reward\", if it was trained on playing Starcraft against humans at any point, or even needed to make sense of how other agents had played Starcraft</p><p>This in turn can be seen as a direct outgrowth and isomorphism of making novel plans for playing Super Mario Brothers which involve understanding Goombas and their role in the screen-generating process</p><p>except obviously that the Goombas are much less complicated and not themselves agents</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:41]</strong>&nbsp;</p><p>Yup, makes sense. Not sure I totally agree that this sort of thing is likely to happen as quickly as it sounds like you believe but I'm happy to roll with it; I do think it will happen eventually</p><p>So doesn't seem particularly cruxy</p><p>I can see how this leads to existential catastrophe, if you don't expect the programmers to be worried at this early manipulation warning sign. (This is potentially cruxy for p(doom), but doesn't feel like the main action.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:46]</strong>&nbsp;</p><p>On my mainline, where this is all happening at Deepmind, I do expect at least one person in the company has ever read anything I've written. &nbsp;I am not sure if Demis understands he is looking straight at death, but I am willing to suppose for the sake of discussion that he does understand this - which isn't ruled out by my actual knowledge - and talk about how we all die from there.</p><p>The very brief tl;dr is that they know they're looking at a warning sign but they cannot <s>fix the warning sign</s> actually fix the real underlying problem that the warning sign is about, and AGI is getting easier for other people to develop too.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:46]</strong>&nbsp;</p><p>I assume this is primarily about social dynamics + the ability to patch things such that things look fixed?</p><p>Yeah, makes sense</p><p>I assume the \"real underlying problem\" is somehow not the fact that the task you were training your AI system to do was not what you actually wanted it to do?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:48]</strong>&nbsp;</p><p>It's about the unavailability of any actual fix and the technology continuing to get easier. &nbsp;Even if Deepmind understands that surface patches are lethal and understands that the easy ways of hammering down the warning signs are just eliminating the visibility rather than the underlying problems, there is nothing they can do about that except wait for somebody else to destroy the world instead.</p><p>I do not know of any pivotal task you could possibly train an AI system to do using tons of correctly labeled data. &nbsp;This is part of why we're all dead.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:50]</strong>&nbsp;</p><p>Yeah, I think if I adopted (my understanding of) your beliefs about alignment difficulty, and there wasn't already a non-racing scheme set in place, seems like we're in trouble</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:50]</strong>&nbsp;</p><p>Like, \"the real underlying problem is the fact that the task you were training your AI system to do was not what you actually wanted it to do\" is one way of looking at one of the several problems that are truly fundamental, but this has no remedy that I know of, besides training your AI to do something small enough to be unpivotal.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:51][17:52]</strong>&nbsp;</p><p>I don't actually know the response you'd have to \"why not just do value alignment?\" I can name several guesses</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><ul><li><a href=\"https://intelligence.org/files/ComplexValues.pdf\">Fragility of value</a></li><li>Not sufficiently concrete</li><li>Can't give correct labels for human values</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:52][17:52]</strong>&nbsp;</p><p>To be concrete, you can't ask the AGI to build one billion nanosystems, label all the samples that wiped out humanity as bad, and apply gradient descent updates</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\">In part, you can't do that because one billion samples will get you one billion lethal systems, but even if that wasn't true, you still couldn't do it.</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:53]</strong>&nbsp;</p><blockquote><p>even if that wasn't true, you still couldn't do it.</p></blockquote><p>Why not? <a href=\"https://arbital.com/p/nearest_unblocked/\">Nearest unblocked strategy</a>?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:53]</strong>&nbsp;</p><p>...no, because the first supposed output for training generated by the system at superintelligent levels kills everyone and there is nobody left to label the data.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:54]</strong>&nbsp;</p><p>Oh, I thought you were asking me to imagine away that effect with your second sentence</p><p>In fact, I still don't understand what it was supposed to mean</p><p>(Specifically this one:</p><blockquote><p>In part, you can't do that because one billion samples will get you one billion lethal systems, but even if that wasn't true, you still couldn't do it.</p></blockquote><p>)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][17:55]</strong>&nbsp;</p><p>there's a separate problem where you can't apply reinforcement learning when there's no good examples, even assuming you live to label them</p><p>and, of course, yet another form of problem where you can't tell the difference between good and bad samples</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][17:56]</strong>&nbsp;</p><p>Okay, makes sense</p><p>Let me think a bit</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:00]</strong>&nbsp;</p><p>and lest anyone start thinking that was an exhaustive list of fundamental problems, note the absence of, for example, \"applying lots of optimization using an outer loss function doesn't necessarily get you something with a faithful internal cognitive representation of that loss function\" aka \"natural selection applied a ton of optimization power to humans using a very strict very simple criterion of 'inclusive genetic fitness' and got out things with no explicit representation of or desire towards 'inclusive genetic fitness' because that's what happens when you hill-climb and take wins in the order a simple search process through cognitive engines encounters those wins\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:02]</strong>&nbsp;</p><p>(Agreed that is another major fundamental problem, in the sense of something that could go wrong, as opposed to something that almost certainly goes wrong)</p><p>I am still curious about the \"why not value alignment\" question, where to expand, it's something like \"let's get a wide range of situations and train the agent with gradient descent to do what a human would say is the right thing to do\". (We might also call this \"imitation\"; maybe \"value alignment\" isn't the right term, I was thinking of it as trying to align the planning with \"human values\".)</p><p>My own answer is that we shouldn't expect this to generalize to nanosystems, but that's again much more of a \"there's not great reason to expect this to go right, but also not great reason to go wrong either\".</p><p>(This is a place where I would be particularly interested in concreteness, i.e. what does the AI system do in these cases, and how does that almost-necessarily follow from the way it was trained?)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:05]</strong>&nbsp;</p><p>what's an example element from the \"wide range of situations\" and what is the human labeling?</p><p>(I could make something up and let you object, but it seems maybe faster to ask you to make something up)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:09]</strong>&nbsp;</p><p>Uh, let's say that the AI system is being trained to act well on the Internet, and it's shown some tweet / email / message that a user might have seen, and asked to reply to the tweet / email / message. User says whether the replies are good or not (perhaps via comparisons, a la <a href=\"https://arxiv.org/abs/1706.03741\">Deep RL from Human Preferences</a>)</p><p>If I were not making it up on the spot, it would be more varied than that, but would not include \"building nanosystems\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:10]</strong>&nbsp;</p><p>And presumably, in this example, the AI system is not smart enough that exposing humans to text it generates is already a world-wrecking threat if the AI is hostile?</p><p>i.e., does not just hack the humans</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:10]</strong>&nbsp;</p><p>Yeah, let's assume that for the moment</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:11]</strong>&nbsp;</p><p>so what you want to do is train on 'weak-safe' domains where the AI isn't smart enough to do damage, and the humans can label the data pretty well because the AI isn't smart enough to fool them</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:11]</strong>&nbsp;</p><p>\"want to do\" is putting it a bit strongly. This is more like a scenario I can't prove is unsafe, but do not strongly believe is safe</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:12]</strong>&nbsp;</p><p>but the domains where the AI can execute a world-saving pivotal act are out-of-distribution for those domains. &nbsp;<i>extremely</i> out-of-distribution. &nbsp;<i>fundamentally</i> out-of-distribution. &nbsp;the AI's own thought processes are out-of-distribution for any inscrutable matrices that were learned to influence those thought processes in a corrigible direction.</p><p>it's not like trying to generalize experience from playing Super Mario Bros to Metroid.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:13]</strong>&nbsp;</p><p>Definitely, but my reaction to this is \"okay, no particular reason for it to be safe\" -- but also not huge reason for it to be unsafe. Like, it would not hugely shock me if what-we-want is sufficiently \"natural\" that the AI system picks up on the right thing form the 'weak-safe' domains alone</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:14]</strong>&nbsp;</p><p>you have this whole big collection of possible AI-domain tuples that are powerful-dangerous and they have properties that aren't in <i>any</i> of the weak-safe training situations, that are moving along third dimensions where all the weak-safe training examples were flat</p><p>now, just because something is out-of-distribution, doesn't mean that nothing can ever generalize there</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:15]</strong>&nbsp;</p><p>I mean, you correctly would not accept this argument if I said that by training blue-car-driving robots solely on blue cars I am ensuring they would be bad on red-car-driving</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:15]</strong>&nbsp;</p><p>humans generalize from the savannah to the vacuum</p><p>so the actual problem is that I expect the optimization to generalize and the corrigibility to fail</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:15]</strong>&nbsp;</p><p>^Right, that</p><p>I am not clear on why you expect this so strongly</p><p>Maybe you think generalization is extremely rare and optimization is a special case because of how it is so useful for basically everything?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:16]</strong>&nbsp;</p><p>no</p><p>did you read the section of my dialogue with Richard Ngo where I tried to explain <a href=\"https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty#3_1__The_Brazilian_university_anecdote\">why corrigibility is anti-natural</a>, or where Nate tried to give the <a href=\"https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty#4_2__Nate_Soares__summary\">example</a> of why planning to get a laser from point A to point B without being scattered by fog is the sort of thing that also naturally says to prevent humans from filling the room with fog?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:19]</strong>&nbsp;</p><p>Ah, right, I should have predicted that. (Yes, I did read it.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:19]</strong>&nbsp;</p><p>or for that matter, am I correct in remembering that these sections existed</p><p>k</p><p>so, do you need more concrete details about some part of that?</p><p>a bunch of the reason why I suspect that corrigibility is anti-natural is from trying to work particular problems there in MIRI's earlier history, and not finding anything that wasn't contrary to <s>coherence</s> the overlap in the shards of inner optimization that, when ground into existence by the outer optimization loop, coherently mix to form the part of cognition that generalizes to do powerful things; and nobody else finding it either, etc.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:22]</strong>&nbsp;</p><p>I think I disagreed with that part more directly, in that it seemed like in those sections the corrigibility was assumed to be imposed \"from the outside\" on top of a system with a goal, rather than having a goal that was corrigible. (I also had a similar reaction to the 2015 <a href=\"https://intelligence.org/files/Corrigibility.pdf\">Corrigibility</a> paper.)</p><p>So, for example, it seems to me like <a href=\"https://arxiv.org/abs/1606.03137\">CIRL</a> is an example of an objective that can be maximized in which the agent is corrigible-in-a-certain-sense. I agree that due to <a href=\"https://arbital.com/p/updated_deference/\">updated deference</a> it will eventually stop seeking information from the human / be subject to corrections by the human. I don't see why, at that point, it wouldn't have just learned to do what the humans actually want it to do.</p><p>(There are objections like misspecification of the reward prior, or misspecification of the P(behavior | reward), but those feel like different concerns to the ones you're describing.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:25]</strong>&nbsp;</p><p>a thing that MIRI tried and failed to do was find a sensible generalization of expected utility which could contain a generalized utility function that would look like an AI that let itself be shut down, without trying to force you to shut it down</p><p>and various workshop attendees not employed by MIRI, etc</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:26]</strong>&nbsp;</p><p>I do agree that a CIRL agent would not let you shut it down</p><p>And this is something that should maybe give you pause, and be a lot more careful about potential misspecification problems</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:27]</strong>&nbsp;</p><p>if you could give a perfectly specified prior such that the result of updating on lots of observations would be a representation of the utility function that <a href=\"https://arbital.com/p/cev/\">CEV</a> outputs, and you could perfectly <a href=\"https://arxiv.org/abs/1906.01820\">inner-align</a> an optimizer to do that thing in a way that scaled to arbitrary levels of cognitive power, then you'd be home free, sure.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:28]</strong>&nbsp;</p><p>I'm not trying to claim this is a solution. I'm more trying to point at a reason why I am not convinced that corrigibility is anti-natural.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:28]</strong>&nbsp;</p><p>the reason CIRL doesn't get off the ground is that there isn't any known, and isn't going to be any known, prior over (observation|'true' utility function) such that an AI which updates on lots of observations ends up with our true desired utility function.</p><p>if you can do that, the AI <i>doesn't need to be corrigible</i></p><p>that's why it's not a counterexample to corrigibility being anti-natural</p><p>the AI just boomfs to superintelligence, observes all the things, and does all the goodness</p><p>it doesn't listen to you say no and won't let you shut it down, but by hypothesis this is fine because it got the true utility function yay</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:31]</strong>&nbsp;</p><p>In the world where it doesn't immediately start out as a superintelligence, it spends a lot of time trying to figure out what you want, asking you what you prefer it does, making sure to focus on the highest-EV questions, being very careful around any irreversible actions, etc</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:31]</strong>&nbsp;</p><p>and making itself smarter as fast as possible</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:32]</strong>&nbsp;</p><p>Yup, that too</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:32]</strong>&nbsp;</p><p>I'd do that stuff too if I was waking up in an alien world</p><p>and, with all due respect to myself, <i>I am not corrigible</i></p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:33]</strong>&nbsp;</p><p>You'd do that stuff because you'd want to make sure you don't accidentally get killed by the aliens; a CIRL agent does it because it \"wants to help the human\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:34]</strong>&nbsp;</p><p>no, a CIRL agent does it because it wants to implement the True Utility Function, which it may, early on, suspect to consist of helping* humans, and maybe to have some overlap (relative to its currently reachable short-term outcome sets, though these are of vanishingly small relative utility under the True Utility Function) with what some humans desire some of the time</p><p>(*) 'help' may not be help</p><p>separately it asks a lot of questions because the things humans do are evidence about the True Utility Function</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:35]</strong>&nbsp;</p><p>I agree this is also an accurate description of CIRL</p><p>A more accurate description, even</p><p>Wait why is it vanishingly small relative utility? Is the assumption that the True Utility Function doesn't care much about humans? Or was there something going on with short vs. long time horizons that I didn't catch</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:39]</strong>&nbsp;</p><p>in the short term, a weak CIRL tries to grab the hand of a human about to fall off a cliff, because its TUF probably does prefer the human who didn't fall off the cliff, if it has only exactly those two options, and this is the sort of thing it would learn was probably true about the TUF early on, given the obvious ways of trying to produce a CIRL-ish thing via gradient descent</p><p>humans eat healthy in the ancestral environment when ice cream doesn't exist as an option</p><p>in the long run, the things the CIRL agent wants do <i>not</i> overlap with anything humans find more desirable than paperclips (because there is no known scheme that takes in a bunch of observations, updates a prior, and outputs a utility function whose achievable maximum is galaxies living happily forever after)</p><p>and plausible TUF schemes are going to notice that grabbing the hand of a current human is a vanishing fraction of all value eventually at stake</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:42]</strong>&nbsp;</p><p>Okay, cool, short vs. long time horizons</p><p>Makes sense</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:42]</strong>&nbsp;</p><p>right, a weak but sufficiently reflective CIRL agent will notice an alignment of short-term interests with humans but deduce misalignment of long-term interests</p><p>though I should maybe call it CIRL* to denote the extremely probable case that the limit of its updating on observation does not in fact converge to CEV's output</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][18:43]</strong>&nbsp;</p><p>(Attempted rephrasing of a point I read Eliezer as making upstream, in hopes that a rephrasing makes it click for Rohin:)&nbsp;</p><p>Corrigibility isn't for bug-free CIRL agents with a prior that actually dials in on goodness given enough observation; if you have one of those you can just run it and call it a day. Rather, corrigibility is for surviving your civilization's inability to do the job right on the first try.</p><p>CIRL doesn't have this property; it instead amounts to the assertion \"if you are optimizing with respect to a distribution on utility functions that dials in on goodness given enough observation then that gets you just about as much good as optimizing goodness\"; this is somewhat tangential to corrigibility.</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:44]</strong>&nbsp;</p><p>and you should maybe update on how, even though somebody thought CIRL was going to be more corrigible, in fact it made <i>absolutely zero progress on the real problem</i></p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure><p>the notion of having an uncertain utility function that you update from observation is coherent and doesn't yield circular preferences, running in circles, incoherent betting, etc.</p><p>so, of course, it is antithetical in its intrinsic nature to corrigibility</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:47]</strong>&nbsp;</p><p>I guess I am not sure that I agree that this is the purpose of corrigibility-as-I-see-it. The point of corrigibility-as-I-see-it is that you don't have to specify the object-level outcomes that your AI system must produce, and instead you can specify the meta-level processes by which your AI system should come to know what the object-level outcomes to optimize for are</p><p>(At CHAI we had taken to talking about corrigibility_MIRI and corrigibility_Paul as completely separate concepts and I have clearly fallen out of that good habit)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:48]</strong>&nbsp;</p><p>speaking as the person who invented the concept, asked for name submissions for it, and selected 'corrigibility' as the winning submission, that is absolutely not how I intended the word to be used</p><p>and I think that the thing I was actually trying to talk about is important and I would like to retain a word that talks about it</p><p>'corrigibility' is meant to refer to the sort of putative hypothetical motivational properties that prevent a system from wanting to kill you after you didn't build it exactly right</p><p><a href=\"https://arbital.com/p/low_impact/\">low impact</a>, <a href=\"https://arbital.com/p/soft_optimizer/\">mild optimization</a>, <a href=\"https://arbital.com/p/shutdown_problem/\">shutdownability</a>, <a href=\"https://arbital.com/p/abortable/\">abortable planning</a>, <a href=\"https://arbital.com/p/behaviorist/\">behaviorism</a>, <a href=\"https://arbital.com/p/conservative_concept/\">conservatism</a>, etc. &nbsp;(note: some of these may be less antinatural than others)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:51]</strong>&nbsp;</p><p>Cool. Sorry for the miscommunication, I think we should probably backtrack to here</p><blockquote><p>so the actual problem is that I expect the optimization to generalize and the corrigibility to fail</p></blockquote><p>and restart.</p><p>Though possibly I should go to bed, it is quite late here and there was definitely a time at which I would not have confused corrigibility_MIRI with corrigibility_Paul, and I am a bit worried at my completely having missed that this time</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:51]</strong>&nbsp;</p><p>the thing you just said, interpreted literally, is what I would call simply \"going meta\" but my guess is you have a more specific metaness in mind</p><p>...does Paul use \"corrigibility\" to mean \"going meta\"? I don't think I've seen Paul doing that.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:54]</strong>&nbsp;</p><p>Not exactly \"going meta\", no (and I don't think I exactly mean that either). But I definitely infer a different concept from <a href=\"https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility\">https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility</a> than the one you're describing here. It is definitely possible that this comes from me misunderstanding Paul; I have done so many times</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][18:55]</strong>&nbsp;</p><p>That looks to me like Paul used 'corrigibility' around the same way I meant it, if I'm not just reading my own face into those clouds. &nbsp;maybe you picked up on the exciting metaness of it and thought 'corrigibility' was talking about the metaness part? 😛</p><p>but I also want to create an affordance for you to go to bed</p><p>hopefully this last conversation combined with previous dialogues has created any sense of why I worry that corrigibility is anti-natural and hence that \"on the first try at doing it, the optimization generalizes from the weak-safe domains to the strong-lethal domains, but the corrigibility doesn't\"</p><p>so I would then ask you what part of this you were skeptical about</p><p>as a place to pick up when you come back from the realms of Morpheus</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][18:58]</strong>&nbsp;</p><p>Yup, sounds good. Talk to you tomorrow!</p></td></tr></tbody></table>\n\n21\\. November 7 conversation\n============================\n\n21.1. Corrigibility, value learning, and pessimism\n--------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][3:23]</strong>&nbsp;</p><p>Quick summary of discussion so far (in which I ascribe views to Eliezer, for the sake of checking understanding, omitting for brevity the parts about how these are facts about my beliefs about Eliezer's beliefs and not Eliezer's beliefs themselves):</p><ul><li>Some discussion of \"how to use non-world-optimizing AIs to help with AI alignment\", which are mostly in the category \"clever tricks with gradient descent and loss functions and labeled datasets\" rather than \"textbook from the future\". Rohin thinks these help significantly (and that \"significant help\" = \"reduced x-risk\"). Eliezer thinks that whatever help they provide is not sufficient to cross the line from \"we need a miracle\" to \"we have a plan that has non-trivial probability of success without miracles\". The crux here seems to be alignment difficulty.</li><li>Some discussion of how doom plays out. I agree with Eliezer that if the AI is catastrophic by default, and we don't have a technique that stops the AI from being catastrophic by default, and we don't already have some global coordination scheme in place, then bad things happen. Cruxes seem to be alignment difficulty and the plausibility of a global coordination scheme, of which alignment difficulty seems like the bigger one.</li><li>On alignment difficulty, an example scenario is \"train on human judgments about what the right thing to do is on a variety of weak-safe domains, and hope for generalization to potentially-lethal domains\". Rohin views this as neither confidently safe nor confidently unsafe. Eliezer views this as confidently unsafe, because he strongly expects the optimization to generalize while the corrigibility doesn't, because corrigibility is anti-natural.</li></ul><p>(Incidentally, \"optimization generalizes but corrigibility doesn't\" is an example of the sort of thing I wish were more concrete, if you happen to be able to do that)</p><p>My current take on \"corrigibility\":</p><ul><li>Prior to this discussion, in my head there was corrigibility_A and corrigibility_B. Corrigibility_A, which I associated with MIRI, was about imposing a constraint \"from the outside\". Given an AI system, it is a method of modifying that AI system to (say) allow you to shut it down, by performing some sort of operation on its goal. Corrigibility_B, which I associated with Paul, was about building an AI system which would have particular nice behaviors like learning about the user's preferences, accepting corrections about what it should do, etc.</li><li>After this discussion, I think everyone meant corrigibility_B all along. The point of the 2015 MIRI paper was to check whether it is possible to build a version of corrigibility_B that was compatible with expected utility maximization with a not-terribly-complicated utility function; the point of this was to see whether corrigibility could be made compatible with \"plans that lase\".</li><li>While I think people agree on the behaviors of corrigibility, I am not sure they agree on why we want it. Eliezer wants it for surviving failures, but maybe others want it for \"dialing in on goodness\". When I think about a \"broad basin of corrigibility\", that intuitively seems more compatible with the \"dialing in on goodness\" framing (but this is an aesthetic judgment that could easily be wrong).</li><li>I don't think I meant \"going meta\", e.g. I wouldn't have called indirect normativity an example of corrigibility. I think I was pointing at \"dialing in on goodness\" vs. \"specifying goodness\".</li><li>I agree CIRL doesn't help survive failures. But if you instead talk about \"dialing in on goodness\", CIRL does in fact do this, at least conceptually (and other alternatives don't).</li><li>I am somewhat surprised that \"how to conceptually dial in on goodness\" is not something that seems useful to you. Maybe you think it is useful, but you're objecting to me calling it corrigibility, or saying we knew how to do it before CIRL?</li></ul><p>(A lot of the above on corrigibility is new, because the distinction between surviving-failures and dialing-in-on-goodness as different use cases for very similar kinds of behaviors is new to me. Thanks for discussion that led me to making such a distinction.)</p><p>Possible avenues for future discussion, in the order of my-guess-at-usefulness:</p><ol><li>Discussing anti-naturality of corrigibility. As a starting point: you say that an agent that makes plans but doesn't execute them is also dangerous, because it is the plan itself that lases, and corrigibility is antithetical to lasing. Does this mean you predict that you, or I, with suitably enhanced intelligence and/or reflectivity, would not be capable of producing a plan to help an alien civilization optimize their world, with that plan being corrigible w.r.t the aliens? (This seems like a strange and unlikely position to me, but I don't see how to not make this prediction under what I believe to be your beliefs. Maybe you just bite this bullet.)</li><li>Discussing why it is very unlikely for the AI system to generalize correctly both on optimization and values-or-goals-that-guide-the-optimization (which seems to be distinct from corrigibility). Or to put it another way, why is \"alignment by default according to John Wentworth\" doomed to fail? <a href=\"https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default\">https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default</a></li><li>More checking of where I am failing to pass your ITT</li><li>Why is \"dialing in on goodness\" not a reasonable part of the solution space (to the extent you believe that)?</li><li>More concreteness on how optimization generalizes but corrigibility doesn't, in the case where the AI was trained by human judgment on weak-safe domains Just to continue to state it so people don't misinterpret me: in most of the cases that we're discussing, my position is <i>not</i> that they are safe, but rather that they are not overwhelmingly likely to be unsafe.</li></ol></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Ngo][3:41]</strong>&nbsp;</p><p>I don't understand what you mean by dialling in on goodness. Could you explain how CIRL does this better than, say, <a href=\"https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84\">reward modelling</a>?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][3:49]</strong>&nbsp;</p><p>Reward modeling does not by default (a) choose relevant questions to ask the user in order to get more information about goodness, (b) act conservatively, especially in the face of irreversible actions, while it is still uncertain about what goodness is, or (c) take actions that are known to be robustly good, while still waiting for future information that clarifies the nuances of goodness</p><p>You could certainly do something like Deep RL from Human Preferences, where the preferences are things like \"I prefer you ask me relevant questions to get more information about goodness\", in order to get similar behavior. In this case you are transferring desired behaviors from a human to the AI system, whereas in CIRL the behaviors \"fall out of\" optimization for a specific objective</p><p>In Eliezer/Nate terms, the CIRL story shows that dialing on goodness is compatible with \"plans that lase\", whereas reward modeling does not show this</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Ngo][4:04]</strong>&nbsp;</p><p>The meta-level objective that CIRL is pointing to, what makes that thing deserve the name \"goodness\"? Like, if I just gave an alien CIRL, and I said \"this algorithm dials an AI towards a given thing\", and they looked at it without any preconceptions of what the designers <i>wanted</i> to do, why wouldn't they say \"huh, it looks like an algorithm for dialling in on some extrapolation of the unintended consequences of people's behaviour\" or something like that?</p><p>See also this part of my second discussion with Eliezer, where he brings up CIRL: [<a href=\"https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty#3_2__Brain_functions_and_outcome_pumps\">https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty#3_2__Brain_functions_and_outcome_pumps</a>] He was emphasising that CIRL, and most other proposals for alignment algorithms, just shuffle the problematic consequentialism from the original place to a less visible place. I didn't engage much with this argument because I mostly agree with it.</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][5:28]</strong>&nbsp;</p><p>I think you are misunderstanding my point. I am not claiming that we know how to implement CIRL such that it produces good outcomes; I agree this depends a ton on having a sufficiently good P(obs | reward). Similarly, if you gave CIRL to aliens, whether or not they say it is about getting some extrapolation of unintended consequences depends on exactly what P(obs | reward) you ended up using. There is some not-too-complicated P(obs | reward) such that you do end up getting to \"goodness\", or something sufficiently close that it is not an existential catastrophe; I do not claim we know what it is.</p><p>I am claiming that behaviors like (a), (b) and (c) above are compatible with expected utility theory, and thus compatible with \"plans that lase\". This is demonstrated by CIRL. It is not demonstrated by reward modeling, see e.g. <a href=\"https://jan.leike.name/publications/Towards%20Interactive%20Inverse%20Reinforcement%20Learning%20-%20Armstrong,%20Leike%202016.pdf\">these</a> <a href=\"https://arxiv.org/abs/2004.13654\">three</a> <a href=\"https://www.tomeveritt.se/papers/alignment.pdf\">papers</a> for problems that arise (which make it so that it is working at cross purposes with itself and seems incompatible with \"plans that lase\"). (I'm most confident in the first supporting my point, it's been a long time since I read them so I might be wrong about the others.) To my knowledge, similar problems don't arise with CIRL (and they shouldn't, because it is a nice integrated Bayesian agent doing expected utility theory).</p><p>I could imagine an objection that P(obs | reward), while not as complicated as \"the utility function that rationalizes a twitching robot\", is still too complicated to really show compatibility with plans-that-lase, but pointing out that P(obs | reward) could be misspecified doesn't seem particularly relevant to whether behaviors (a), (b) and (c) are compatible with plans-that-lase.</p><p>Re: shuffling around the problematic consequentialism: it is not my main plan to avoid consequentialism in the sense of plans-that-lase. I broadly agree with Eliezer that you need consequentialism to do high-impact stuff. My plan is for the consequentialism to be aimed at good ends. So I agree that there is still consequentialism in CIRL, and I don't see this as a damning point; when I talk about \"dialing in to goodness\", I am thinking of aiming the consequentialism at goodness, not getting rid of consequentialism.</p><p>(You can still do things like try to be domain-specific rather than domain-general; I don't mean to completely exclude such approaches. They do seem to give additional safety. But the mainline story is that the consequentialism / optimization is directed at what we want rather than something else.)</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Ngo][6:21]</strong>&nbsp;</p><p>If you don't know how to implement CIRL in such a way that it actually aims at goodness, then you don't have an algorithm with properties a, b and c above.</p><p>Or, to put it another way: suppose I replace the word \"goodness\" with \"winningness\". Now I can describe AlphaStar as follows:</p><ul><li>it choose relevant questions to ask (read: scouts to send) in order to get more information about winningness</li><li>it acts conservatively while it is still uncertain about what winningness is</li><li>it take actions that are known to be robustly <s>good</s> winningish, while still waiting for future information that clarifies the nuances of winningness</li></ul><p>Now, you might say that the difference is that CIRL implements uncertainty over possible utility functions, not possible empirical beliefs. But this is just a semantic difference which shuffles the problem around without changing anything substantial. E.g. it's exactly equivalent if we think of CIRL as an agent with a fixed (known) utility function, which just has uncertainty about some empirical parameter related to the humans it interacts with.</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][6:55]</strong>&nbsp;</p><blockquote><p>[...] it take actions that are known to be robustly good, while still waiting for future information that clarifies the nuances of winningness</p></blockquote><p>(typo: \"known to be robustly good\" -&gt; \"known to be robustly winningish\" :-p)</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure><p>Some quick reactions, some from me and some from my model of Eliezer:</p><blockquote><p>Eliezer thinks that whatever help they provide is not sufficient [...] The crux here seems to be alignment difficulty.</p></blockquote><p>I'd be more hesitant to declare the crux \"alignment difficulty\". My understanding of Eliezer's position on your \"use AI to help with alignment\" proposals (which focus on things like using AI to make paradigmatic AI systems more transparent) is \"that was always the plan, and it doesn't address the sort of problems I'm worried about\". Maybe you understand the problems Eliezer's worried about, and believe them not to be very difficult to overcome, thus putting the crux somewhere like \"alignment difficulty\", but I'm not convinced.&nbsp;</p><p>I'd update towards your crux-hypothesis if you provided a good-according-to-Eliezer summary of what other problems Eliezer sees and the reasons-according-to-Eliezer that \"AI make our tensors more transparent\" doesn't much address them.</p><blockquote><p>Corrigibility_A [...] Corrigibility_B [...]</p></blockquote><p>Of the two Corrigibility_B does sound a little closer to my concept, though neither of your descriptions cause me to be confident that communication has occurred. Throwing some checksums out there:</p><ul><li>There are three reasons a young weak AI system might accept your corrections. It could be corrigible, or it could be incorrigibly pursuing goodness, or it could be incorrigibly pursuing some other goal while calculating that accepting this correction is better according to its current goals than risking a shutdown.</li><li>One way you can tell that CIRL is not corrigible is that it does not accept corrections when old and strong.</li><li>There's an intuitive notion of \"you're here to help us implement a messy and fragile concept not yet clearly known to us; work with us here?\" that makes sense to humans, that includes as a side effect things like \"don't scan my brain and then disregard my objections; there could be flaws in how you're inferring my preferences from my objections; it's actually quite important that you be cautious and accept brain surgery even in cases where your updated model says we're about to make a big mistake according to our own preferences\".</li></ul><blockquote><p>The point of the 2015 MIRI paper was to check whether it is possible to build a version of corrigibility_B that was compatible with expected utility maximization with a not-terribly-complicated utility function; the point of this was to see whether corrigibility could be made compatible with \"plans that lase\".</p></blockquote><p>More like:</p><ul><li>Corrigibility seems, at least on the surface, to be in tension with the simple and useful patterns of optimization that tend to be spotlit by demands for cross-domain success, similar to how acting like two oranges are worth one apple and one apple is worth one orange is in tension with those patterns.</li><li>In practice, this tension seems to run more than surface-deep. In particular, various attempts to reconcile the tension fail, and cause the AI to have undesirable preferences (eg, incentives to convince you to shut it down whenever its utility is suboptimal), exploitably bad beliefs (eg, willingness to bet at unreasonable odds that it won't be shut down), and/or to not be corrigible in the first place (eg, a preference for destructively uploading your mind against your protests, at which point further protests from your coworkers are screened off by its access to that upload).</li></ul><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: ✅]</td></tr></tbody></table></figure><p>(There's an argument I occasionally see floating around these parts that goes \"ok, well what if the AI is <i>fractally</i> corrigible, in the sense that instead of its cognition being oriented around pursuit of some goal, its cognition is oriented around doing what it predicts a human would do (or what a human would want it to do) in a corrigible way, at every level and step of its cognition\". This is perhaps where you perceive a gap between your A-type and B-type notions, where MIRI folk tend to be more interested in reconciling the tension between corrigibility and coherence, and Paulian folk tend to place more of their chips on some such fractal notion?&nbsp;</p><p>I admit I don't find much hope in the \"fractally corrigible\" view myself, and I'm not sure whether I could pass a proponent's ITT, but fwiw my model of the Yudkowskian rejoinder is \"mindspace is deep and wide; that could plausibly be done if you had sufficient mastery of minds; you're not going to get anywhere near close to that in practice, because of the way that basic normal everyday cross-domain training will highlight patterns that you'd call orienting-cognition-around-a-goal\".)</p><p>And my super-quick takes on your avenues for future discussion:</p><blockquote><p>1. Discussing anti-naturality of corrigibility.</p></blockquote><p>Hopefully the above helps.</p><blockquote><p>2. Discussing why it is very unlikely for the AI system to generalize correctly both on optimization and values-or-goals-that-guide-the-optimization</p></blockquote><p>The concept \"patterns of thought that are useful for cross-domain success\" is latent in the problems the AI faces, and known to have various simple mathematical shadows, and our training is more-or-less banging the AI over the head with it day in and day out. By contrast, the specific values we wish to be pursued are not latent in the problems, are known to <i>lack</i> a simple boundary, and our training is much further removed from it.</p><blockquote><p>3. More checking of where I am failing to pass your ITT</p></blockquote><p>+1</p><blockquote><p>4. Why is \"dialing in on goodness\" not a reasonable part of the solution space?</p></blockquote><p>It has long been the plan to say something less like \"the following list comprises goodness: ...\" and more like \"yo we're tryin to optimize some difficult-to-name concept; help us out?\". \"Find a prior that, with observation of the human operators, dials in on goodness\" is a fine guess at how to formalize the latter.&nbsp;</p><p>If we had been planning to take the former tack, and you had come in suggesting CIRL, that might have helped us switch to the latter tack, which would have been cool. In that sense, it's a fine part of the solution.&nbsp;</p><p>It also provides some additional formality, which is another iota of potential solution-ness, for that part of the problem.&nbsp;</p><p>It doesn't much address the rest of the problem, which is centered much more around \"how do you point powerful cognition in any direction at all\" (such as towards your chosen utility function or prior thereover).</p><blockquote><p>5. More concreteness on how optimization generalizes but corrigibility doesn't, in the case where the AI was trained by human judgment on weak-safe domains</p></blockquote><p>+1</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:23]</strong>&nbsp;</p><blockquote><p>If you don't know how to implement CIRL in such a way that it actually aims at goodness, then you don't have an algorithm with properties a, b and c above.</p></blockquote><p>I want clarity on the premise here:</p><ul><li>Is the premise \"Rohin cannot write code that when run exhibits properties a, b, and c\"? If so, I totally agree, but I'm not sure what the point is. All alignment work ever until the very last step will not lead you to writing code that when run exhibits an aligned superintelligence, but this does not mean that the prior alignment work was useless.</li><li>Is the premise \"there does not exist code that (1) we would call an implementation of CIRL and (2) when run has properties a, b, and c\"? If so, I think your premise is false, for the reasons given previously (I can repeat them if needed)</li></ul><p>I imagine it is neither of the above, and you are trying to make a claim that some conclusion that I am drawing from or about CIRL is invalid, because in order for me to draw that conclusion, I need to exhibit the correct P(obs | reward). If so, I want to know which conclusion is invalid and why I have to exhibit the correct P(obs | reward) before I can reach that conclusion.</p><p>I agree that the fact that you can get properties (a), (b) and (c) are simple straightforward consequences of being Bayesian about a quantity you are uncertain about and care about, as with AlphaStar and \"winningness\". I don't know what you intend to imply by this -- because it also applies to other Bayesian things, it can't imply anything about alignment? I also agree the uncertainty over reward is equivalent to uncertainty over some parameter of the human (and have proved this theorem myself in the paper I wrote on the topic). I do not claim that anything in here is particularly non-obvious or clever, in case anyone thought I was making that claim.</p><p>To state it again, my claim is that behaviors like (a), (b) and (c) are consistent with \"plans-that-lase\", and as evidence for this claim I cite the <i>existence</i> of an expected-utility-maximizing algorithm that displays them, specifically CIRL with the correct p(obs | reward). I do <i>not</i> claim that I can write down the code, I am just claiming that it <i>exists</i>. If you agree with the claim but not the evidence then let's just drop the point. If you disagree with the claim then tell me why it's false. If you are unsure about the claim then point to the step in the argument you think doesn't work.</p><p>The reason I care about this claim is that it seems to me like <i>even</i> if you think that superintelligences only involve plans-that-lase, it seems to me like this does <i>not</i> rule out what we might call \"dialing in to goodness\" or \"assisting the user\", and thus it seems like this is a valid target for you to try to get your superintelligence to do.</p><p>I suspect that I do not agree with Eliezer about what plans-that-lase can do, but it seems like the two of us should at least agree that behaviors like (a), (b) and (c) can be exhibited in plans-that-lase, and if we don't agree on that some sort of miscommunication has happened.</p><p>&nbsp;</p><blockquote><p>Throwing some checksums out there</p></blockquote><p>The checksums definitely make sense. (Technically I could name more reasons why a young AI might accept correction, such as \"it's still sphexish in some areas, accepting corrections is one of those reasons\", and for the third reason the AI could be calculating negative consequences for things other than shutdown, but that seems nitpicky and I don't think it means I have misunderstood you.)&nbsp;</p><p>I think the third one feels somewhat slippery and vague, in that I don't know exactly what it's claiming, but it clearly seems to be the same sort of thing as corrigibility. Mostly it's more like I wouldn't be surprised if the Textbook from the Future tells us that we mostly had the right concept of corrigibility, but that third checksum is not quite how they would describe it any more. I would be a lot more surprised if the Textbook says we mostly had the right concept but then says checksums 1 and 2 were misguided.</p><blockquote><p>\"The point of the 2015 MIRI paper was to check whether it is possible to build a version of corrigibility_B that was compatible with expected utility maximization with a not-terribly-complicated utility function; the point of this was to see whether corrigibility could be made compatible with 'plans that lase'.\"</p><p>More like:</p><ul><li>Corrigibility seems, at least on the surface, to be in tension with the simple and useful patterns of optimization that tend to be spotlit by demands for cross-domain success, similar to how as acting like an two oranges are worth one apple and one apple is worth one orange is in tension with those patterns.</li><li>In practice, this tension seems to run more than surface-deep. In particular, various attempts to reconcile the tension fail, and cause the AI to have undesirable preferences (eg, incentives to convince you to shut it down whenever its utility is suboptimal), exploitably bad beliefs (eg, willingness to bet at unreasonable odds that it won't be shut down), and/or to not be corrigible in the first place (eg, a preference for destructively uploading your mind against your protests, at which point further protests from your coworkers are screened off by its access to that upload).</li></ul></blockquote><p>On the 2015 Corrigibility paper, is this an accurate summary: \"it wasn't that we were checking whether corrigibility could be compatible with useful patterns of optimization; it was already obvious at least at a surface level that corrigibility was in tension with these patterns, and we wanted to check and/or show that this tension persisted more deeply and couldn't be easily fixed\".</p><p>(My other main hypothesis is that there's an important distinction between \"simple and useful patterns of optimization\" (term in your message) and \"plans that lase\" (term in my message) but if so I don't know what it is.)</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][13:52]</strong>&nbsp;</p><p>What we <i>wanted</i> to do was show that the apparent tension was merely superficial. We failed.</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure><p>(Also, IIRC -- and it's been a long time since I checked -- the 2015 paper contains only one exploration, relating to an idea of Stuart Armstrong's. There were another host of ideas raised and shot down in that era, that didn't make it into that paper, pro'lly b/c they came afterwards.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:55]</strong>&nbsp;</p><blockquote><p>What we <i>wanted</i> to do was show that the apparent tension was merely superficial. We failed.</p></blockquote><p>(That sounds like what I originally said? I'm a bit confused why you didn't just agree with my original phrasing:</p><blockquote><p>The point of the 2015 MIRI paper was to check whether it is possible to build a version of corrigibility_B that was compatible with expected utility maximization with a not-terribly-complicated utility function; the point of this was to see whether corrigibility could be made compatible with \"plans that lase\".</p></blockquote><p>)</p><p>(I'm kinda worried that there's some big distinction between \"EU maximization\", \"plans that lase\", and \"simple and useful patterns of optimization\", that I'm not getting; I'm treating them as roughly equivalent at the moment when putting on my MIRI-ontology-hat.)</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][14:01]</strong>&nbsp;</p><p>(There are a bunch of aspects of your phrasing that indicated to me a different framing, and one I find quite foreign. For instance, this talk of \"building a version of corrigibility_B\" strikes me as foreign, and the talk of \"making it compatible with 'plans that lase'\" strikes me as foreign. It's plausible to me that you, who understand your original framing, can tell that my rephrasing matches your original intent. I do not yet feel like I could emit the description you emitted without contorting my thoughts about corrigibility in foreign ways, and I'm not sure whether that's an indication that there are distinctions, important to me, that I haven't communicated.)</p><blockquote><p>(I'm kinda worried that there's some big distinction between \"EU maximization\", \"plans that lase\", and \"simple and useful patterns of optimization\", that I'm not getting; I'm treating them as roughly equivalent at the moment when putting on my MIRI-ontology-hat.)</p></blockquote><p>I, too, believe them to be basically equivalent (with the caveat that the reason for using expanded phrasings is because people have a history of misunderstanding \"utility maximization\" and \"coherence\", and so insofar as you round them all to \"coherence\" and then argue against some very narrow interpretation of coherence, I'm gonna protest that you're bailey-and-motting).</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:12]</strong>&nbsp;</p><blockquote><p>Hopefully the above helps.</p></blockquote><p>I'm still interested in the question \"Does this mean you predict that you, or I, with suitably enhanced intelligence and/or reflectivity, would not be capable of producing a plan to help an alien civilization optimize their world, with that plan being corrigible w.r.t the aliens?\" I don't currently understand how you avoid making this prediction given other stated beliefs. (Maybe you just bite the bullet and do predict this?)</p><blockquote><p>By contrast, the specific values we wish to be pursued are not latent in the problems, are known to lack a simple boundary, and our training is much further removed from it.</p></blockquote><p>I'm not totally sure what is meant by \"simple boundary\", but it seems like a lot of human values are latent in text prediction on the Internet, and when training from human feedback the training is not very removed from values.</p><blockquote><p>It has long been the plan to say something less like \"the following list comprises goodness: ...\" and more like \"yo we're tryin to optimize some difficult-to-name concept; help us out?\". [...]</p></blockquote><p>I take this to mean that \"dialing in on goodness\" is a reasonable part of the solution space? If so, I retract that question. I thought from previous comments that Eliezer thought this part of solution space was more doomed than corrigibility.</p><p>(I get the sense that people think that I am butthurt about CIRL not getting enough recognition or something. I do in fact think this, but it's not part of my agenda here. I originally brought it up to make the argument that corrigibility is not in tension with EU maximization, then realized that I was mistaken about what \"corrigibility\" meant, but still care about the argument that \"dialing in on goodness\" is not in tension with EU maximization. But if we agree on that claim then I'm happy to stop talking about CIRL.)</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][14:13]</strong>&nbsp;</p><p>I'd be <i>capable</i> of helping aliens optimize their world, sure. I wouldn't be motivated to, but I'd be capable.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:14]</strong>&nbsp;</p><blockquote><p>(There are a bunch of aspects of your phrasing that indicated to me a different framing, and one I find quite foreign. For instance, this talk of \"building a version of corrigibility_B\" strikes me as foreign, and the talk of \"making it compatible with 'plans that lase'\" strikes me as foreign. It's plausible to me that you, who understand your original framing, can tell that my rephrasing matches your original intent. I do not yet feel like I could emit the description you emitted without contorting my thoughts about corrigibility in foreign ways, and I'm not sure whether that's an indication that there are distinctions, important to me, that I haven't communicated.)</p></blockquote><p>This makes sense. I guess you might think of these concepts as quite pinned down? Like, in your head, EU maximization is just a kind of behavior (= set of behaviors), corrigibility is just another kind of behavior (= set of behaviors), and there's a straightforward yes-or-no question about whether the intersection is empty which you set out to answer, you can't \"make\" it come out one way or the other, nor can you \"build\" a new kind of corrigibility</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][14:17]</strong>&nbsp;</p><p>Re: CIRL, my current working hypothesis is that by \"use CIRL\" you mean something analogous to what I say when I say \"do CEV\" -- namely, direct the AI to figure out what we \"really\" want in some correct sense, rather than attempting to specify what we want concretely. And to be clear, on my model, this <i>is</i> part of the solution to the overall alignment problem, and it's more-or-less why we wouldn't die immediately on the \"value is fragile / we can't name exactly what we want\" step if we solved the other problems.</p><p>My guess as to the disagreement about how much credit CIRL should get, is that there is in fact a disagreement, but it's not coming from MIRI folk saying \"no we should be specifying the actual utility function by hand\", it's coming from MIRI folk saying \"this is just the advice 'do CEV' dressed up in different clothing and presented as a reason to stop worrying about corrigibility, which is irritating, given that it's orthogonal to corrigibility\".</p><p>If you wanna fight that fight, I'd start by asking: Do you think CIRL is doing anything above and beyond what \"use CEV\" is doing? If so, what?</p><p>Regardless, I think it might be a good idea for you to try to pass my (or Eliezer's) ITT about what parts of the problem remain beyond the thing I'd call \"do CEV\" and why they're hard. (Not least b/c if my working hypothesis is wrong, demonstrating your mastery of that subject might prevent a bunch of toil covering ground you already know.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:17]</strong>&nbsp;</p><blockquote><p>I'd be <i>capable</i> of helping aliens optimize their world, sure. I wouldn't be motivated to, but I'd be capable.</p></blockquote><p>Okay, so it seems like the danger requires the thing-producing-the-plan to be badly-motivated. But then I'm not sure why it seems so impossible to have a (not-badly-motivated) thing that, when given a goal, produces a plan to corrigibly get that goal. (This is a scenario Richard mentioned earlier.)</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][14:19]</strong>&nbsp;</p><blockquote><p>This makes sense. I guess you might think of these concepts as quite pinned down? Like, in your head, EU maximization is just a kind of behavior (= set of behaviors), corrigibility is just another kind of behavior (= set of behaviors), and there's a straightforward yes-or-no question about whether the intersection is empty which you set out to answer, you can't \"make\" it come out one way or the other, nor can you \"build\" a new kind of corrigibility</p></blockquote><p>That sounds like one of the big directions in which your framing felt off to me, yeah :-). (I don't fully endorse that rephrasing, but it seems directionally correct to me.)</p><blockquote><p>Okay, so it seems like the danger requires the thing-producing-the-plan to be badly-motivated. But then I'm not sure why it seems so impossible to have a (not-badly-motivated) thing that, when given a goal, produces a plan to corrigibly get that goal. (This is a scenario Richard mentioned earlier.)</p></blockquote><p>On my model, aiming the powerful optimizer is the hard bit.</p><p>Like, once I grant \"there's a powerful optimizer, and all it does is produce plans to corrigibly attain a given goal\", I agree that the problem is mostly solved.</p><p>There's maybe some cleanup, but the bulk of the alignment challenge preceded that point.</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure><p>(This is hard for all the usual reasons, that I suppose I could retread.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:24]</strong>&nbsp;</p><blockquote><p>[...] Regardless, I think it might be a good idea for you to try to pass my (or Eliezer's) ITT about what parts of the problem remain beyond the thing I'd call \"do CEV\" and why they're hard. (Not least b/c if my working hypothesis is wrong, demonstrating your mastery of that subject might prevent a bunch of toil covering ground you already know.)</p></blockquote><p>(Working on ITT)</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][14:30]</strong>&nbsp;</p><p>(To clarify some points of mine, in case this gets published later to other readers: (1) I might call it more centrally something like \"build a <a href=\"https://arbital.com/p/dwim/\">DWIM system</a>\" rather than \"use CEV\"; and (2) this is not advice about what your civilization should do with early AGI systems, I strongly recommend against trying to pull off CEV under that kind of pressure.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:32]</strong>&nbsp;</p><p>I don't particularly want to have fights about credit. I just didn't want to falsely state that I do not care about how much credit CIRL gets, when attempting to head off further comments that seemed designed to appease my sense of not-enough-credit. (I'm also not particularly annoyed at MIRI, here.)</p><p>On passing ITT, about what's left beyond \"use CEV\" (stated in my ontology because it's faster to type; I think you'll understand, but I can also translate if you think that's important):</p><ul><li>The main thing is simply how to actually get the AI system to care about pursuing CEV. I think MIRI ontology would call this the target loading problem.</li><li>This is hard because (a) you can't just train on CEV, because you can't just implement CEV and provide that as training and (b) even if you magically could train on CEV, that does not establish that the resulting AI system then wants to optimize CEV. It could just as well optimize some other objective that correlated with CEV in the situations you trained, but no longer correlates in some new situation (like when you are building a nanosystem). (Point (b) is how I would talk about inner alignment.)</li><li>This is made harder for a variety of reasons, including (a) you're working with inscrutable matrices that you can't look at the details of, (b) there are clear racing incentives when the prize is to take over the world (or even just lots of economic profit), (c) people are unlikely to understand the issues at stake (unclear to me of the exact reasons, I'd guess it would be that the issues are too subtle / conceptual, + pressure to rationalize it away), (d) there's very little time in which we have a good understanding of the situation we face, because of fast / discontinuous takeoff</li></ul><figure class=\"table\"><table><tbody><tr><td>[Soares: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][14:37]</strong>&nbsp;</p><p>Passable ^_^ (Not exhaustive, obviously; \"it will have a tendency to kill you on the first real try if you get it wrong\" being an example missing piece, but I doubt you were trying to be exhaustive.) Thanks.</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure><blockquote><p>Okay, so it seems like the danger requires the thing-producing-the-plan to be badly-motivated. But then I'm not sure why it seems so impossible to have a (not-badly-motivated) thing that, when given a goal, produces a plan to corrigibly get that goal. (This is a scenario Richard mentioned earlier.)</p></blockquote><p>I'm uncertain where the disconnect is here. Like, I could repeat some things from past discussions about how \"it only outputs plans, it doesn't execute them\" does very little (not nothing, but very little) from my perspective? Or you could try to point at past things you'd expect me to repeat and name why they don't seem to apply to you?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:40]</strong>&nbsp;</p><p>(Flagging that I should go to bed soon, though it doesn't have to be right away)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:50]</strong>&nbsp;</p><p>...I do not know if this is going to help anything, but I have a feeling that there's a frequent disconnect wherein I invented an idea, considered it, found it necessary-but-not-sufficient, and moved on to looking for additional or varying solutions, and then a decade or in this case 2 decades later, somebody comes along and sees this brilliant solution which MIRI is for some reason neglecting</p><p>this is perhaps exacerbated by a deliberate decision during the early days, when I looked very weird and the field was much more allergic to weird, to not even try to stamp my name on all the things I invented. &nbsp;eg, I told Nick Bostrom to please use various of my ideas as he found appropriate and only credit them if he thought that was strategically wise.</p><p>I expect that some number of people now in the field don't know I invented corrigibility, and any number of other things that I'm a little more hesitant to claim here because I didn't leave Facebook trails for inventing them</p><p>and unless you had been around for quite a while, you definitely wouldn't know that I had been (so far as I know) the first person to perform the unexceptional-to-me feat of writing down, in 2001, the very obvious idea I called \"external reference semantics\", or as it's called nowadays, CIRL</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:53]</strong>&nbsp;</p><p>I really honestly am not trying to say that MIRI didn't think of CIRL-like things, nor am I trying to get credit for CIRL. I really just wanted to establish that \"learn what is good to do\" seems not-ruled-out by EU maximization. That's all. It sounds like we agree on this point and if so I'd prefer to drop it.</p><figure class=\"table\"><table><tbody><tr><td>[Soares: ❤️]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:53]</strong>&nbsp;</p><p>Having a prior over utility functions that gets updated by evidence is not ruled out by EU maximization. &nbsp;That exact thing is hard for other reasons than it being contrary to the nature of EU maximization.</p><p>If it was ruled out by EU maximization for any simple reason, I would have noticed that back in 2001.</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Ngo][14:54]</strong>&nbsp;</p><p>I think we all agree on this point.</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td><td>[Soares: 👍]</td></tr></tbody></table></figure><p>One thing I'd note is that during my debate with Eliezer, I'd keep saying \"oh so you think X is impossible\" and he'd say \"no, all these things are <i>possible</i>, they're just really really hard\".</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:58]</strong>&nbsp;</p><p>...to do correctly on your first try when a failed attempt kills you.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:58]</strong>&nbsp;</p><p>Maybe it's fine; perhaps the point is just that target loading is hard, and the question is why target loading is so hard.</p><p>From my perspective, the main confusing thing about the Eliezer/Nate view is how <i>confident</i> it is. With each individual piece, I (usually) find myself nodding along and saying \"yes, it seems like if we wanted to guarantee safety, we would need to solve this\". What I don't do is say \"yes, it seems like without a solution to this, we're near-certainly dead\". The uncharitable view (which I share mainly to emphasize where the disconnect is, not because I think it is true) would be something like \"Eliezer/Nate are falling to a Murphy bias, where they assume that unless they have an ironclad positive argument for safety, the worst possible thing will happen and we all die\". I try to generate things that seem more like ironclad (or at least \"leatherclad\") positive arguments for doom, and mostly don't succeed; when I say \"human values are very complicated\" there's the rejoinder that \"a superintelligence will certainly know about human values; pointing at them shouldn't take that many more bits\"; when I say \"this is ultimately just praying for generalization\", there's the rejoinder \"but it may in fact actually generalize\"; add to all of this the fact that a bunch of people will be trying to prevent the problem and it seems weird to be so confident in doom.</p><p>A lot of my questions are going to be of the form \"it seems like this is a way that we could survive; it definitely involves luck and does not say good things about our civilization, but it does not seem as improbable as the word 'miracle' would imply\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:00]</strong>&nbsp;</p><p>heh. &nbsp;from my standpoint, I'd say of this that it reflects those old experiments where if you ask people for their \"expected case\" it's indistinguishable from their \"best case\" (since both of these involve visualizing various things going on their imaginative mainline, which is to say, as planned) and reality is usually worse than their \"worst case\" (because they didn't adjust far enough away from their best-case anchor towards the statistical distribution for actual reality when they were trying to imagine a few failures and disappointments of the sort that reality had previously delivered)</p><p>it rhymes with the observation that it's incredibly hard to find people - even inside the field of computer security - who really have what Bruce Schneier termed the security mindset, of asking how to break a cryptography scheme, instead of imagining how your cryptography scheme could succeed</p><p>from my perspective, people are just living in a fantasy reality which, if we were actually living in it, would not be full of failed software projects or rocket prototypes that blow up even after you try quite hard to get a system design about which you made a strong prediction that it wouldn't explode</p><p>they think something special has to go wrong with a rocket design, that you must have committed some grave unusual sin against rocketry, for the rocket to explode</p><p>as opposed to every rocket wanting really strongly to explode and needing to constrain every aspect of the system to make it not explode and then the first 4 times you launch it, it blows up anyways</p><p>why? because of some particular technical issue with O-rings, with the flexibility of rubber in cold weather?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:05]</strong>&nbsp;</p><p>(I have read your Rocket Alignment and security mindset posts. Not claiming this absolves me of bias, just saying that I am familiar with them)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:05]</strong>&nbsp;</p><p>no, because the strains and temperatures in rockets are large compared to the materials that we use to make up the rockets</p><p>the fact that sometimes people are wrong in their uncertain guesses about rocketry does not make their life easier in this regard</p><p>the less they understand, the less ability they have to force an outcome within reality</p><p>it's no coincidence that when you are Wrong about your rocket, the particular form of Being Wrong that reality delivers to you as a surprise message, is not that you underestimated the strength of steel and so your rocket went to orbit and came back with fewer scratches on the hull than expected</p><p>when you are working with powerful forces there is not a symmetry around pleasant and unpleasant surprises being equally likely relative to your first-order model. &nbsp;if you're a good Bayesian, they will be equally likely relative to your second-order model, but this requires you to be HELLA pessimistic, indeed, SO PESSIMISTIC that sometimes you are pleasantly surprised</p><p>which looks like such a bizarre thing to a mundane human that they will gather around and remark at the case of you being pleasantly surprised</p><p>they will not be used to seeing this</p><p>and they shall say to themselves, \"haha, what pessimists\"</p><p>because to be unpleasantly surprised is so ordinary that they do not bother to gather and gossip about it when it happens</p><p>my fundamental sense about the other parties in this debate, underneath all the technical particulars, is that they've constructed a Murphy-free fantasy world from the same fabric that weaves crazy optimistic software project estimates and brilliant cryptographic codes whose inventors didn't quite try to break them, and are waiting to go through that very common human process of trying out their optimistic idea, letting reality gently correct them, predictably becoming older and wiser and starting to see the true scope of the problem, and so in due time becoming one of those Pessimists who tell the youngsters how ha ha of course things are not that easy</p><p>this is how the cycle usually goes</p><p>the problem is that instead of somebody's first startup failing and them then becoming much more pessimistic about lots of things they thought were easy and then doing their second startup</p><p>the part where they go ahead optimistically and learn the hard way about things in their chosen field which aren't as easy as they hoped</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:13]</strong>&nbsp;</p><p>Do you want to bet on that? That seems like a testable prediction about beliefs of real people in the not-too-distant future</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:13]</strong>&nbsp;</p><p>kills everyone</p><p>not just them</p><p>everyone</p><p>this is an issue</p><p>how on Earth would we bet on that if you think the bet hasn't already resolved? I'm describing the attitudes of people that I see right now today.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:15]</strong>&nbsp;</p><p>Never mind, I wanted to bet on \"people becoming more pessimistic as they try ideas and see them fail\", but if your idea of \"see them fail\" is \"superintelligence kills everyone\" then obviously we can't bet on that</p><p>(people here being alignment researchers, obviously ones who are not me)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:17]</strong>&nbsp;</p><p>there is some element here of the Bayesian not updating in a predictable direction, of executing today the update you know you'll make later, of saying, \"ah yes, I can see that I am in the same sort of situation as the early AI pioneers who thought maybe it would take a summer and actually it was several decades because Things Were Not As Easy As They Imagined, so instead of waiting for reality to correct me, I will imagine myself having already lived through that and go ahead and be more pessimistic right now, not just a little more pessimistic, but so incredibly pessimistic that I am <i>as</i> likely to be pleasantly surprised as unpleasantly surprised by each successive observation, which is even more pessimism than even some sad old veterans manage\", an element of genre-savviness, an element of knowing the advice that somebody would predictably be shouting at you from outside, of not just blindly enacting the plot you were handed</p><p>and I don't quite know <i>why</i> this is so much less common than I would have naively thought it would be</p><p>why people are content with enacting the predictable plot where they start out cheerful today and get some hard lessons and become pessimistic later</p><p>they are their own scriptwriters, and they write scripts for themselves about going into the haunted house and then splitting up the party</p><p>I would not have thought that to defy the plot was such a difficult thing for an actual human being to do</p><p>that it would require so much reflectivity or something, I don't know what else</p><p>nor do I know how to train other people to do it if they are not doing it already</p><p>but that from my perspective is the basic difference in gloominess</p><p>I am a time-traveler who came back from the world where it (super duper predictably) turned out that a lot of early bright hopes didn't pan out and various things went WRONG and alignment was HARD and it was NOT SOLVED IN ONE SUMMER BY TEN SMART RESEARCHERS</p><p>and now I am trying to warn people about this development which was, from a certain perspective, really quite obvious and not at all difficult to see coming</p><p>but people are like, \"what the heck are you doing, you are enacting the wrong part of the plot, people are currently supposed to be cheerful, you can't prove that anything will go wrong, why would I turn into a grizzled veteran before the part of the plot where reality hits me over the head with the awful real scope of the problem and shows me that my early bright ideas were way too optimistic and naive\"</p><p>and I'm like \"no you don't get it, where I come from, <i>everybody died</i> and didn't turn into grizzled veterans\"</p><p>and they're like \"but that's not what the script says we do next\"... or something, I do not know what leads people to think like this because I do not think like that myself</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][15:24]</strong>&nbsp;</p><p>(I think what they actually do is say \"it's not obvious to me that this is one of those scenarios where we become grizzled veterans, as opposed to things just actually working out easily\")</p><p>(\"many things work out easily all the time; obviously society spends a bunch more focus on things that don't work out easily b/c the things that work easily tend to get resolved fairly quickly and then you don't notice them\", or something)</p><p>(more generally, I kinda suspect that bickering closer to the object level is likely more productive)</p><p>(and i suspect this convo might be aided by Rohin naming a concrete scenario where things go well, so that Eliezer can lament the lack of genre saviness in various specific points)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:26]</strong>&nbsp;</p><p>there are, of course, lots of more local technical issues where I can specifically predict the failure mode for somebody's bright-eyed naive idea, especially when I already invented a more sophisticated version a decade or two earlier, and this is what I've usually tried to discuss</p><figure class=\"table\"><table><tbody><tr><td>[Soares: ❤️]</td></tr></tbody></table></figure><p>because conversations like that can sometimes make any progress</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][15:26]</strong>&nbsp;</p><p>(and possibly also Eliezer naming a concrete story where things go poorly, so that Rohin may lament the seemingly blind pessimism &amp; premature grizzledness)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:27]</strong>&nbsp;</p><p>whereas if somebody lacks the ability to see the warning signs of which genre they are in, I do not know how to change the way they are by talking at them</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:28]</strong>&nbsp;</p><p>Unsurprisingly I have disagreements with the meta-level story, but it seems really thorny to make progress on and I'm kinda inclined to not discuss it. I also should go to sleep now.</p><p>One thing it did make me think of -- it's possible that the \"do it correctly on your first try when a failed attempt kills you\" could be the crux here. There's a clearly-true sense which is \"the first time you build a superintelligence that you cannot control, if you have failed in your alignment, then you die\". There's a different sense which is \"and also, anything you try to do with non-superintelligences that you can control, will tell you approximately nothing about the situation you face when you build a superintelligence\". I mostly don't agree with the second sense, but if Eliezer / Nate do agree with it, that would go a long way to explaining the confidence in doom.</p><p>Two arguments I can see for the second sense: (1) the non-superintelligences only seem to respond well to alignment schemes because they don't yet have the core of general intelligence, and (2) the non-superintelligences only seem to respond well to alignment schemes because despite being misaligned they are doing what we want in order to survive and later execute a treacherous turn. EDIT: And (3) fast takeoff = not much time to look at the closest non-dangerous examples</p><p>(I still should sleep, but would be interested in seeing thoughts tomorrow, and if enough people think it's actually worthwhile to engage on the meta level I can do that. I'm cheerful about engaging on specific object-level ideas.)</p><figure class=\"table\"><table><tbody><tr><td>[Soares: 💤]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:28]</strong>&nbsp;</p><p>it's not that early failures tell you nothing</p><p>the failure of the 1955 Dartmouth Project to produce strong AI over a summer told those researchers something</p><p>it told them the problem was harder than they'd hoped on the first shot</p><p>it didn't show them the correct way to build AGI in 1957 instead</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Bensinger][16:41]</strong>&nbsp;</p><p>Linking to a chat log between Eliezer and some anonymous people (and Steve Omohundro) from early September: [<a href=\"https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions\">https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions</a>]</p><p>Eliezer tells me he thinks it pokes at some of Rohin's questions</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][16:48]</strong>&nbsp;</p><p>I'm not sure that I can successfully, at this point, go back up and usefully reply to the text that scrolled past - I also note some internal grinding about this having turned into a thing which has Pending Replies instead of Scheduled Work Hours - and this maybe means that in the future we shouldn't have such a general chat here, which I didn't anticipate before the fact. &nbsp;I shall nonetheless try to pick out some things and reply to them.</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure><blockquote><ul><li>While I think people agree on the behaviors of corrigibility, I am not sure they agree on why we want it. Eliezer wants it for surviving failures, but maybe others want it for \"dialing in on goodness\". When I think about a \"broad basin of corrigibility\", that intuitively seems more compatible with the \"dialing in on goodness\" framing (but this is an aesthetic judgment that could easily be wrong).</li></ul></blockquote><p>This is a weird thing to say in my own ontology.</p><p>There's a general project of AGI alignment where you try to do some useful pivotal thing, which has to be powerful enough to be pivotal, and so you somehow need a system that thinks powerful thoughts in the right direction without it killing you.</p><p>This could include, for example:</p><ul><li>Trying to train in \"low impact\" via an RL loss function that penalizes a sufficiently broad range of \"impacts\" that we hope the learned impact penalty generalizes to all the things we'd consider impacts - even as we scale up the system, without the sort of obvious pathologies that would materialize only over options available to sufficiently powerful systems, like sending out nanosystems to erase the visibility of its actions from human observers</li><li>Tweaking MCTS search code so that it behaves in the fashion of \"mild optimization\" or \"<a href=\"https://arbital.com/p/task_goal/\">taskishness</a>\" instead of searching as hard as it has power available to search</li><li>Exposing the system to lots of labeled examples of relatively simple and safe instructions being obeyed, hoping that it generalizes safe instruction-following to regimes too dangerous for us to inspect outputs and label results</li><li>Writing code that tries to recognize cases of activation vectors going outside the bounds they occupied during training, as a check on whether internal cognitive conservatism is being violated or something is seeking out adversarial counterexamples to a constraint</li></ul><p>You could say that only parts 1 and 3 are \"dialing in on goodness\" because only those parts involve iteratively refining a target, or you could say that all 4 parts are \"dialing in on goodness\" because parts 2 and 4 help you stay alive while you're doing the iterative refining.&nbsp; But I don't see this distinction as fundamental or particularly helpful.&nbsp; What if, on part 4, you were training something to recognize out-of-bounds activations, instead of trying to hardcode it?&nbsp; Is that dialing in on goodness?&nbsp; Or is it just dialing in on survivability or corrigibility or whatnot?&nbsp; Or maybe even part 3 isn't really \"dialing in on goodness\" because the true distinction between Good and Evil is still external in the programmers and not inside the system?</p><p>I don't see this as an especially useful distinction to draw.&nbsp; There's a hardcoded/learned distinction that probably does matter in several places.&nbsp; There's a maybe-useful forest-level distinction between \"actually doing the pivotal thing\" and \"not destroying the world as a side effect\" which breaks down around the trees because the very definition of \"that pivotal thing you want to do\" is to do <i>that thing</i> and <i>not</i> to destroy the world.</p><p>And all of this is a class of shallow ideas that I can generate in great quantity.&nbsp; I now and then consider writing up the ideas like this, just to make clear that I've already thought of way more shallow ideas like this than the net public output of the entire rest of the alignment field, so it's not that my concerns of survivability stem from my having missed any of the obvious shallow ideas like that.</p><p>The reason I don't spend a lot of time talking about it is not that I haven't thought of it, it's that I've thought of it, explored it for a while, and decided not to write it up because I don't think it can save the world and the infinite well of shallow ideas seems more like a distraction from the level of miracle we would actually need.</p><p>-</p><blockquote><p>As a starting point: you say that an agent that makes plans but doesn't execute them is also dangerous, because it is the plan itself that lases, and corrigibility is antithetical to lasing. Does this mean you predict that you, or I, with suitably enhanced intelligence and/or reflectivity, would not be capable of producing a plan to help an alien civilization optimize their world, with that plan being corrigible w.r.t the aliens? (This seems like a strange and unlikely position to me, but I don't see how to not make this prediction under what I believe to be your beliefs. Maybe you just bite this bullet.)</p></blockquote><p>I 'could' corrigibly help the <a href=\"https://www.lesswrong.com/s/qWoFR4ytMpQ5vw3FT\">Babyeaters</a> in the sense that I have a notion of what it would mean to corrigibly help them, and if I wanted to do that thing for some reason, like an outside super-universal entity offering to pay me a googolplex flops of eudaimonium if I did that one thing, then I could do that thing.&nbsp; Absent the superuniversal entity bribing me, I wouldn't <i>want</i> to behave corrigibly towards the Babyeaters.&nbsp;&nbsp;</p><p>This is not a defect of myself as an individual.&nbsp; The Superhappies would also be able to understand what it would be like to be corrigible; they wouldn't <i>want</i> to behave corrigibly towards the Babyeaters, because, like myself, they don't want exactly what the Babyeaters want.&nbsp; In particular, we would rather the universe be other than it is with respect to the Babyeaters eating babies.</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure></td></tr></tbody></table>\n\n22\\. Follow-ups\n===============\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][0:33]</strong> &nbsp;<strong>(Nov. 8)</strong>&nbsp;</p><blockquote><p>[...] Absent the superuniversal entity bribing me, I wouldn't <i>want</i> to behave corrigibly towards the Babyeaters.&nbsp;[...]</p></blockquote><p>Got it. Yeah I think I just misunderstood a point you were saying previously. When Richard asked about systems that simply produce plans rather than execute them, you said something like \"the plan itself is dangerous\", which I now realize meant \"you don't get additional safety from getting to read the plan, the superintelligence would have just chosen a plan that was convincing to you but nonetheless killed everyone / otherwise worked in favor of the superintelligence's goals\", but at the time I interpreted it as \"any reasonable plan that can actually build nanosystems is going to be dangerous, regardless of the source\", which seemed obviously false in the case of a well-motivated system.</p><blockquote><p>[...] This is a weird thing to say in my own ontology. [...]</p></blockquote><p>When I say \"dialing in on goodness\", I mean a specific class of strategies for getting a superintelligence to do a useful pivotal thing, in which you build it so that the superintelligence is applying its force towards figuring out what it is that you actually want it to do and pursuing that, which among other things would involve taking a pivotal act to reduce x-risk to ~zero.</p><p>I previously had the mistaken impression that you thought this class of strategies was probably doomed because it was incompatible with expected utility theory, which seemed wrong to me. (I don't remember why I had this belief; possibly it was while I was still misunderstanding what you meant by \"corrigibility\" + the claim that corrigibility is anti-natural.)</p><p>I now think that you think it is probably doomed for the same reason that most other technical strategies are probably doomed, which is that there still doesn't seem to be any plausible way of loading in the right target to the superintelligence, even when that target is a process for learning-what-to-optimize, rather than just what-to-optimize.</p><blockquote><p>Linking to a chat log between Eliezer and some anonymous people (and Steve Omohundro) from early September: [<a href=\"https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions\">https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions</a>]</p><p>Eliezer tells me he thinks it pokes at some of Rohin's questions</p></blockquote><p>I'm surprised that you think this addresses (or even pokes at) my questions. As far as I can tell, most of the questions there are either about social dynamics, which I've been explicitly avoiding, and the \"technical\" questions seem to treat \"AGI\" or \"superintelligence\" as a symbol; there don't seem to be any internal gears underlying that symbol. The closest anyone got to internal gears was mentioning iterated amplification as a way of bootstrapping known-safe things to solving hard problems, and that was very brief.</p><p>I am much more into the question \"how difficult is technical alignment\". It seems like answers to this question need to be in one of two categories: (1) claims about the space of minds that lead to intelligent behavior (probably weighted by simplicity, to account for the fact that we'll get the simple ones first), (2) claims about specific methods of building superintelligences. As far as I can tell the only thing in that doc which is close to an argument of this form is \"superintelligent consequentialists would find ways to manipulate humans\", which seems straightforwardly true (when they are misaligned). I suppose one might also count the assertion that \"the speedup step of iterated amplification will introduce errors\" as an argument of this form.</p><p>It could be that you are trying to convince me of some other beliefs that I wasn't asking about, perhaps in the hopes of conveying some missing mood, but I suspect that it is just that you aren't particularly clear on what my beliefs are / what I'm interested in. (Not unreasonable, given that I've been poking at your models, rather than the other way around.) I could try saying more about that, if you'd like.</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Tallinn][11:39]</strong> &nbsp;<strong>(Nov. 12)</strong>&nbsp;</p><p>FWIW, a voice from the audience: +1 to going back to sketching concrete scenarios. even though i learned a few things from the abstract discussion of goodness/corrigibility/etc myself (eg, that “corrigible” was meant to be defined at the limit of self-improvement till maturity, not just as a label for code that does not resist iterated development), the progress felt more tangible during the “scaled up muzero” discussion above.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:03]</strong> &nbsp;<strong>(Nov. 12)</strong>&nbsp;</p><p>anybody want to give me a prompt for a concrete question/scenario, ideally a concrete such prompt but I'll take whatever?</p></td></tr><tr><td style=\"background-color:#F2F2F2;border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Soares][15:34]</strong>&nbsp;<strong> (Nov. 12)</strong>&nbsp;</p><p>Not sure I count, but one I'd enjoy a concrete response to: \"The leading AI lab vaguely thinks it's important that their systems are 'mere predictors', and wind up creating an AGI that is dangerous; how concretely does it wind up being a scary planning optimizer or whatever, that doesn't run through a scary abstract \"waking up\" step\".</p><p>(asking for a friend; @Joe Carlsmith or whoever else finds this scenario unintuitive plz clarify with more detailed requests if interested)</p></td></tr></tbody></table>\n\n23\\. November 13 conversation\n=============================\n\n23.1. GPT-*n* and goal-oriented aspects of human reasoning\n----------------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][1:46]</strong>&nbsp;</p><p>I'm still interested in:</p><blockquote><p>5. More concreteness on how optimization generalizes but corrigibility doesn't, in the case where the AI was trained by human judgment on weak-safe domains</p></blockquote><p>Specifically, we can go back to the scaled-up MuZero example. Some (lightly edited) details we had established there:</p><blockquote><p>Pretraining: playing all the videogames, predicting all the text and images, solving randomly generated computer puzzles, accomplishing sets of easily-labelable sensorymotor tasks using robots and webcams</p><p>Finetuning: The AI system is being trained to act well on the Internet, and it's shown some tweet / email / message that a user might have seen, and asked to reply to the tweet / email / message. User says whether the replies are good or not (perhaps via comparisons, a la Deep RL from Human Preferences). It would be more varied than that, but would not include \"building nanosystems\".</p><p>The AI system is not smart enough that exposing humans to text it generates is already a world-wrecking threat if the AI is hostile.</p></blockquote><p>At that point we moved from concrete to abstract:</p><blockquote><p>Abstract description: train on 'weak-safe' domains where the AI isn't smart enough to do damage, and the humans can label the data pretty well because the AI isn't smart enough to fool them</p><p>Abstract problem: Optimization generalizes and corrigibility fails</p></blockquote><p>I would be interested in a more concrete description here. I'm not sure exactly what details I'm looking for -- on my ontology the question is something like \"what algorithm is the AI system forced to learn; how does that lead to generalized optimization and failed corrigibility; why weren't there simple safer algorithms that were compatible with the training, or if there were such algorithms why didn't the AI system learn them\". I don't really see how to answer all of that without abstraction, but perhaps you'll have an answer anyway</p><p>(I am hoping to get some concrete detail on \"how did it go from non-hostile to hostile\", though I suppose you might confidently predict that it was already hostile after pretraining, conditional on it being an AGI at all. I can try devising a different concrete scenario if that's a blocker.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:09]</strong>&nbsp;</p><blockquote><p>I am hoping to get some concrete detail on \"how did it go from non-hostile to hostile\"</p></blockquote><p>Mu Zero is intrinsically dangerous for reasons essentially isomorphic to the way that AIXI is intrinsically dangerous: It tries to remove humans from its environment when playing Reality for the same reasons it stomps a Goomba if it learns how to play Super Mario Bros 1, because it has some goal and the Goomba is in the way. &nbsp;It doesn't need to learn anything more to be that way, except for learning what a Goomba/human is within the current environment.&nbsp;</p><p>The question is more \"What kind of patches might it learn for a weak environment if optimized by some hill-climbing optimization method and loss function not to stomp Goombas there, and how would those patches fail to generalize to not stomping humans?\"</p><p>Agree or disagree so far?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][12:07]</strong>&nbsp;</p><p>Agree assuming that it is pursuing a misaligned goal, but I am also asking what misaligned goal it is pursuing (and depending on the answer, maybe also how it came to be pursuing that misaligned goal given the specified training setup).</p><p>In fact I think \"what misaligned goal is it pursuing\" is probably the more central question for me</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:14]</strong>&nbsp;</p><p>well, obvious abstract guess is: something whose non-maximal \"optimum\" (that is, where the optimization ended up, given about how powerful the optimization was) coincided okayish with the higher regions of the fitness landscape (lower regions of the loss landscape) that could be reached at all, relative to its ancestral environment</p><p>I feel like it would be pretty hard to blindly guess, in advance, at my level of intelligence, without having seen any precedents, what the hell a Human would look like, as a derivation of \"inclusive genetic fitness\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][12:15]</strong>&nbsp;</p><p>Yeah I agree with that in the abstract, but have had trouble giving compelling-to-me concrete examples</p><p>Yeah I also agree with that</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:15]</strong>&nbsp;</p><p>I could try to make up some weird false specifics if that helps?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][12:16]</strong>&nbsp;</p><p>To be clear I am fine with \"this is a case where we predictably can't have good concrete stories and this does not mean we are safe\" (and indeed argued the same thing in a doc I linked here many messages ago)</p><p>But weird false specifics could still be interesting</p><p>Although let me think if it is actually valuable</p><p>Probably it is not going to change my mind very much on alignment difficulty, if it is \"weird false specifics\", so maybe this isn't the most productive line of discussion. I'd be \"selfishly\" interested in that \"weird false specifics\" seems good for me to generate novel thoughts about these sorts of scenarios, but that seems like a bad use of this Discord</p><p>I think given the premises that (1) superintelligence is coming soon, (2) it pursues a misaligned goal by default, and (3) we currently have no technical way of preventing this and no realistic-seeming avenues for generating such methods, I am very pessimistic. I think (2) and (3) are the parts that I don't believe and am interested in digging into, but perhaps \"concrete stories\" doesn't really work for this.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:26]</strong>&nbsp;</p><p>with any luck - though I'm not sure I actually expect that much luck - this would be something Redwood Research could tell us about, if they can <a href=\"https://www.lesswrong.com/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project\">learn a nonviolence predicate</a> over GPT-3 outputs and then manage to successfully mutate the distribution enough that we can get to see what was actually inside the predicate instead of \"nonviolence\"</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure><p>or, like, 10% of what was actually inside it</p><p>or enough that people have some specifics to work with when it comes to understanding how gradient descent learning a function over outcomes from human feedback relative to a distribution, doesn't just learn the actual function the human is using to generate the feedback (though, if this were learned exactly, it would still be fatal given superintelligence)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][12:33]</strong>&nbsp;</p><p>In this framing I do buy that you don't learn exactly the function that generates the feedback -- I have ~5 contrived specific examples where this is the case (i.e. you learn something that wasn't what the feedback function would have rewarded in a different distribution)</p><p>(I'm now thinking about what I actually want to say about this framing)</p><p>Actually, maybe I do think you might end up learning the function that generates the feedback. Not literally exactly, if for no other reason than rounding errors, but well enough that the inaccuracies don't matter much. The AGI presumably already knows and understands the concepts we use based on its pretraining, is it really so shocking if gradient descent hooks up those concepts in the right way? (GPT-3 on the other hand doesn't already know and understand the relevant concepts, so I wouldn't predict this of GPT-3.) I do feel though like this isn't really getting at my reason for (relative) optimism, and that reason is much more like \"I don't really buy that AGI must be very coherent in a way that would prevent corrigibility from working\" (which we could discuss if desired)</p><p>On the comment that learning the exact feedback function is still fatal -- I am unclear on why you are so pessimistic on having \"human + AI\" supervise \"AI\", in order to have the supervisor be smarter than the thing being supervised. (I think) I understand the pessimism that the learned function won't generalize correctly, but if you imagine that magically working, I'm not clear what additional reason prevents the \"human + AI\" supervising \"AI\" setup.</p><ul><li>I can see how you die if the AI ever becomes misaligned, i.e. there isn't a way to fix mistakes, but I don't see how you get the misaligned AI in the first place.</li><li>I could also see things like \"Just like a student can get away with plagiarism even when the teacher is smarter than the student, the AI knows more about its cognition than the human + AI system, and so will likely be incentivized to do bad things that it knows are bad but the human + AI system doesn't know is bad\". But that sort of thing seems solvable with future research, e.g. debate, interpretability, red teaming all seem like feasible approaches.</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:06]</strong>&nbsp;</p><p>what's a \"human + AI\"? can you give me a more concrete version of that scenario, either one where you expect it to work, or where you yourself have labeled the first point you expect it to fail and you want to know whether I see an earlier failure than that?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:09]</strong>&nbsp;</p><p>One concrete training algorithm would be debate, ideally with mechanisms that allow the AI systems to \"look into each other's thoughts\" and make credible statements about them, but we can skip that for now as it isn't very concrete</p><p>Would you like a training domain and data as well?</p><p>I don't like the fact that a smart AI system in this position could notice that it is playing against itself and decide not to participate in a zero-sum game, but I am not sure if that worry actually makes sense or not</p><p>(Debate can be thought of as simultaneously \"human + first AI evaluate second AI\" and \"human + second AI evaluate first AI\")</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:12]</strong>&nbsp;</p><p>further concreteness, please! what pivotal act is it training for? what are the debate contents about?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:16]</strong>&nbsp;</p><p>You start with \"easy\" debates like mathematical theorem proving or fact-based questions, and ramp up until eventually the questions are roughly \"what is the next thing to do in order to execute a pivotal act\"</p><p>Intermediate questions might be things like \"is it a good idea to have a minimum wage\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:17]</strong>&nbsp;</p><p>so, like, \"email ATTTTGAGCTTGCC... to the following address, mix the proteins you receive by FedEx in a water-saline solution at 2 degrees Celsius...\" for the final stage?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:17]</strong>&nbsp;</p><p>Yup, that could be it</p><p>Humans are judging debates based on reasoning though, not just outcomes-after-executing-the-plan</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:19]</strong>&nbsp;</p><p>okay. &nbsp;let's suppose you manage to prevent both AGIs from using logical decision theory to coordinate with each other. &nbsp;both AIs tell their humans that the other AI's plans are murderous. &nbsp;now what?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:19]</strong>&nbsp;</p><p>So assuming perfect generalization there should be some large implicit debate tree that justifies the plan in human-understandable form</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:20]</strong>&nbsp;</p><p>yah, I flatly disbelieve that entire development scheme, so we should maybe back up.</p><p>people fiddled around with GPT-4 derivatives and never did get them to engage in lines of printed reasoning that would design interesting new stuff. &nbsp;now what?</p><p>Living Zero (a more architecturally complicated successor of Mu Zero) is getting better at designing complicated things over on its side while that's going on, whatever it is</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:23]</strong>&nbsp;</p><p>Okay, so the worry is that this just won't scale, not that (assuming perfect generalization) it is unsafe? Or perhaps you also think it is unsafe but it's hard to engage with because you don't believe it will scale?</p><p>And the issue is that relying on reasoning confines you to a space of possible thoughts that doesn't include the kinds of thoughts required to develop new stuff (e.g. intuition)?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:25]</strong>&nbsp;</p><p>mostly I have found these alleged strategies to be too permanently abstract, never concretized, to count as admissible hypotheses. &nbsp;if you ask me to concretize them myself, I think that unelaborated giant transformer stacks trained on massive online text corpuses fail to learn smart-human-level engineering reasoning before the world ends. &nbsp;If that were not true, I would expect Paul-style schemes to blow up on the distillation step, but first failures first.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:26]</strong>&nbsp;</p><p>What additional concrete detail do you want?</p><p>It feels like I specified something that we could code up a stupidly inefficient version of now</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:27]</strong>&nbsp;</p><p>Great. &nbsp;Describe the stupidly inefficient version?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:33]</strong>&nbsp;</p><p>In terms of what actually happens: Each episode, there is an initial question specified by the human. Agent A and agent B, which are copies of the same neural net, simultaneously produce statements (\"answers\"). They then have a conversation. At the end the human judge decides which answer is better, and rewards the appropriate agent. The agents are updated using some RL algorithm.</p><p>I can say stuff about why we might hope this works, or about tricks you have to play in order to get learning to happen at all, or other things</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:35]</strong>&nbsp;</p><p>Are the agents also playing Starcraft or have they spent their whole lives inside the world of text?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:35]</strong>&nbsp;</p><p>For the stupidly inefficient version they could have spent their whole lives inside text</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:37]</strong>&nbsp;</p><p>Okay. &nbsp;I don't think the pure-text versions of GPT-5 are being very good at designing nanosystems while Living Zero is ending the world.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:37]</strong>&nbsp;</p><p>In the stupidly inefficient version human feedback has to teach the agents facts about the real world</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:37]</strong>&nbsp;</p><p>(It's called \"Living Zero\" because it does lifelong learning, in the backstory I've been trying to separately sketch out in a draft.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:38]</strong>&nbsp;</p><p>Oh I definitely agree this is not competitive</p><p>So when you say this is too abstract, you mean that there isn't a story for how they incorporate e.g. physical real-world knowledge?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:39]</strong>&nbsp;</p><p>no, I mean that when I talk to Paul about this, I can't get Paul to say anything as concrete as the stuff you've already said</p><p>the reason why I don't expect the GPT-5s to be competitive with Living Zero is that gradient descent on feedforward transformer layers, in order how to learn science by competing to generate text that humans like, would have to pick up on some very deep latent patterns generating that text, and I don't think there's an incremental pathway there for gradient descent to follow - if gradient descent even follows incremental pathways as opposed to finding <a href=\"https://www.lesswrong.com/tag/lottery-ticket-hypothesis\">lottery tickets</a>, but that's a whole separate open question of artificial neuroscience.</p><p>in other words, humans play around with legos, and hominids play around with chipping flint handaxes, and mammals play around with spatial reasoning, and that's part of the incremental pathway to developing deep patterns for causal investigation and engineering, which then get projected into human text and picked up by humans reading text</p><p>it's just straightforwardly not clear to me that GPT-5 pretrained on human text corpuses, and then further posttrained by RL on human judgment of text outputs, ever runs across the deep patterns</p><p>where relatively small architectural changes might make the system no longer just a giant stack of transformers, even if that resulting system is named \"GPT-5\", and in this case, bets might be off, but also in this case, things will go wrong with it that go wrong with Living Zero, because it's now learning the more powerful and dangerous kind of work</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:45]</strong>&nbsp;</p><p>That does seem like a disagreement, in that I think this process does eventually reach the \"deep patterns\", but I do agree it is unlikely to be competitive</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:45]</strong>&nbsp;</p><p>I mean, if you take a feedforward stack of transformer layers the size of a galaxy and train it via gradient descent using all the available energy in the reachable universe, it might find something, sure</p><p>though this is by no means certain to be the case</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][13:50]</strong>&nbsp;</p><p>It would be quite surprising to me if it took that much. It would be <i>especially</i> surprising to me if we couldn't figure out some alternative reasonably-simple training scheme like \"imitate a human doing good reasoning\" that still remained entirely in text that could reach the \"deep patterns\". (This is now no longer a discussion about whether the training scheme is aligned, not sure if we should continue it.)</p><p>I realize that this might be hard to do, but if you imagine that GPT-5 + human feedback finetuning does run across the deep patterns and could in theory do the right stuff, and also generalization magically works, what's the next failure?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:56]</strong>&nbsp;</p><p>what sort of deep thing does a hill-climber run across in the layers, such that the deep thing is the most predictive thing it found for human text about science?</p><p>if you don't visualize this deep thing in any detail, then it can in one moment be powerful, and in another moment be safe. &nbsp;it can have all the properties that you want simultaneously. &nbsp;who's to say otherwise? the mysterious deep thing has no form within your mind.</p><p>if one were to name specifically \"well, it ran across a little superintelligence with long-term goals that it realized it could achieve by predicting well in all the cases that an outer gradient descent loop would probably be updating on\", that sure doesn't end well for you.</p><p>this perhaps is <i>not</i> the first thing that gradient descent runs across. &nbsp;it wasn't the first thing that natural selection ran across to build things that ran the savvanah and made more of themselves. &nbsp;but what deep pattern that is <i>not</i> pleasantly and unfrighteningly formless would gradient descent run across instead?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:00]</strong>&nbsp;</p><p>(Tbc by \"human feedback finetuning\" I mean debate, and I suspect that \"generalization magically works\" will be meant to rule out the thing that you say next, but seems worth checking so let me write an answer)</p><blockquote><p>the deep thing is the most predictive thing it found for human text about science?</p></blockquote><p>Wait, the most predictive thing? I was imagining it as just a thing that is present in addition to all the other things. Like, I don't think I've learned a \"deep thing\" that is most useful for riding a bike. Probably I'm just misunderstanding what you mean here.</p><p>I don't think I can give a good answer here, but to give some answer, it has a belief that there is a universe \"out there\", that lots but not all of the text it reads is making claims about (some aspect of) the universe, those claims can be true or false, there are some claims that are known to be true, there are some ways to take assumed-true claims and generate new assumed-true claims, which includes claims about optimal actions for goals, as well as claims about how to build stuff, or what the effect of a specified machine is</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:10]</strong>&nbsp;</p><p>hell of a lot of stuff for gradient descent to run across in a stack of transformer layers. &nbsp;clearly the lottery-ticket hypothesis must have been very incorrect, and there was an incremental trail of successively more complicated gears that got trained into the system.</p><p>btw by \"claims\" are you meaning to make the jump to English claims? I was reading them as giant inscrutable vectors encoding meaningful propositions, but maybe you meant something else there.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:11]</strong>&nbsp;</p><p>In fact I am skeptical of some strong versions of the lottery ticket hypothesis, though it's been a while since I read the paper and I don't remember exactly what the original hypothesis was</p><p>Giant inscrutable vectors encoding meaningful propositions</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:13]</strong>&nbsp;</p><p>oh, I'm not particularly confident of the lottery-ticket hypothesis either, though I sure do find it grimly amusing that a species which hasn't already figured <i>that</i> out one way or another thinks it's going to have deep transparency into neural nets all wrapped up in time to survive. &nbsp;but, separate issue.</p><p>\"How does gradient descent even work?\" \"Lol nobody knows, it just does.\"</p><p>but, separate issue</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:16]</strong>&nbsp;</p><p>How does strong lottery ticket hypothesis explain GPT-3? Seems like that should already be enough to determine that there's an incremental trail of successively more complicated gears</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:18]</strong>&nbsp;</p><p>could just be that in 175B parameters, combinatorially combined through possible execution pathways, there is some stuff that was pretty close to doing all the stuff that GPT-3 ended up doing.</p><p>anyways, for a human to come up with human text about science, the human has to brood and think for a bit about different possible hypotheses that could account for the data, notice places where those hypotheses break down, tweak the hypotheses in their mind to make the errors go away; they would engineer an internal mental construct towards the engineering goal of making good predictions. &nbsp;if you're looking at orbital mechanics and haven't invented calculus yet, you invent calculus as a persistent mental tool that you can use to craft those internal mental constructs.</p><p>does the formless deep pattern of GPT-5 accomplish the same ends, by some mysterious means that is, formless, able to produce the same result, but not by any detailed means where if you visualized them you would be able to see how it was unsafe?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:24]</strong>&nbsp;</p><p>I expect that probably we will figure out some way to have adaptive computation time be a thing (it's been investigated for years now, but afaik hasn't worked very well), which will allow for this sort of thing to happen</p><p>In the stupidly inefficient version, you have a really really giant and deep neural net that does all of that in successive layers of the neural net. (And when it doesn't need to do that, those layers are noops.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:26][14:32]</strong>&nbsp;</p><p>okay, so my question is, is there a little goal-oriented mind inside there that solves science problems the same way humans solve them, by engineering mental constructs that serve a goal of prediction, including backchaining for prediction goals and forward chaining from alternative hypotheses / internal tweaked states of the mental construct? or is there something else which solves the same problem, not how humans do it, without any internal goal orientation?</p><p>People who would not in the first place realize that humans solve prediction problems by internally engineering internal mental constructs in a goal-oriented way, would of course imagine themselves able to imagine a formless spirit which produces \"predictions\" without being \"goal-oriented\" because they lack an understanding of internal machinery and so can combine whatever surface properties and English words they want to yield a beautiful optimism</p><p>Or perhaps there is indeed some way to produce \"predictions\" without being \"goal-oriented\", which gradient descent on a great stack of transformer layers would surely run across; but you will pardon my grave lack of confidence that someone has in fact seen so much further than myself, when they don't seem to have appreciated in advance of my own questions why somebody who understood something about human internals would be skeptical of this.</p><p>If they're sort of visibly trying to come up with it on the spot after I ask the question, that's not such a great sign either.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">This is not aimed particularly at you, but I hope the reader may understand something of why Eliezer Yudkowsky goes about sounding so gloomy all the time about other people's prospects for noticing what will kill them, by themselves, without Eliezer constantly hovering over their shoulder every minute prompting them with almost all of the answer.</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:31]</strong>&nbsp;</p><p>Just to check my understanding: if we're talking about, say, how humans might go about understanding neural nets, there's a goal of \"have a theory that can retrodict existing observations and make new predictions\", backchaining might say \"come up with hypotheses that would explain double descent\", forward chaining might say \"look into bias and variance measurements\"?</p><p>If so, yes, I think the AGI / GPT-5-that-is-an-AGI is doing something similar</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:33]</strong>&nbsp;</p><p>your understanding sounds okay, though it might make more sense to talk about a domain that human beings understand better than artificial neuroscience, for purposes of illustrating how scientific thinking works, since human beings haven't actually gotten very far with artificial neuroscience.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:33]</strong>&nbsp;</p><p>Fair point re using a different domain</p><p>To be clear I do not in fact think that GPT-N is safe because it is trained with supervised learning and I am confused at the combination of views that GPT-N will be AGI and GPT-N will be safe because it's just doing predictions</p><p>Maybe there is marginal additional safety but you clearly can't say it is \"definitely safe\" without some additional knowledge that I have not seen so far</p><p>Going back to the original question, of what the next failure mode of debate would be assuming magical generalization, I think it's just not one that makes sense to ask on your worldview / ontology; \"magical generalization\" is the equivalent of \"assume that the goal-oriented mind somehow doesn't do dangerous optimization towards its goal, yet nonetheless produces things that can only be produced by dangerous optimization towards a goal\", and so it is assuming the entire problem away</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:41]</strong>&nbsp;</p><p>well YES</p><p>from my perspective the whole field of mental endeavor as practiced by alignment optimists consists of ancient alchemists wondering if they can get collections of surface properties, like a metal as shiny as gold, as hard as steel, and as self-healing as flesh, where optimism about such wonderfully combined properties can be infinite as long as you stay ignorant of underlying structures that produce some properties but not others</p><p>and, like, maybe you <i>can</i> get something as hard as steel, as shiny as gold, and resilient or self-healing in various ways, but you sure don't get it by ignorance of the internals</p><p>and not for a while</p><p>so if you need the magic sword in 2 years or the world ends, you're kinda dead</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:46]</strong>&nbsp;</p><p>Potentially dumb question: when humans do science, why don't they then try to take over the world to do the best possible science? (If humans are doing dangerous goal-directed optimization when doing science, why doesn't that lead to catastrophe?)</p><p>You could of course say that they just aren't smart enough to do so, but it sure feels like (most) humans wouldn't want to do the best possible science even if they were smarter</p><p>I think this is similar to a question I asked before about plans being dangerous independent of their source, and the answer was that the source was misaligned</p><p>But in the description above you didn't say anything about the thing-doing-science being misaligned, so I am once again confused</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:48]</strong>&nbsp;</p><p>boy, so many dumb answers to this dumb question:</p><ul><li>even relatively \"smart\" humans are not very smart compared to other humans, such that they don't have a \"take over the world\" option available.</li><li>most humans who use Science were not smart enough to invent the underlying concept of Science for themselves from scratch; and Francis Bacon, who did, sure did want to take over the world with it.</li><li>groups of humans with relatively more Engineering sure did take over large parts of the world relative to groups that had relatively less.</li><li>Eliezer Yudkowsky clearly demonstrates that when you are smart <i>enough</i> you start trying to use Science and Engineering to take over your whole future lightcone, the other humans you're thinking of just aren't that smart, and, if they were, would inevitably converge towards Eliezer Yudkowsky, who is really a very typical example of a person that smart, even if he looks odd to you because you're not seeing the population of other <a href=\"https://www.lesswrong.com/tag/dath-ilan\">dath ilani</a></li></ul><p>I am genuinely not sure how to come up with a less dumb answer and it may require a more precise reformulation of the question</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:50]</strong>&nbsp;</p><p>But like, in Eliezer's case, there is a different goal that is motivating him to use Science and Engineering for this purpose</p><p>It is not the prediction-goal that he instantiated in his mind as part of the method of doing Science</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:52]</strong>&nbsp;</p><p>sure, and the mysterious formless thing within GPT-5 with \"adaptive computation time\" that broods and thinks, may be pursuing its prediction-subgoal for the sake of other goals, or be pursuing different subgoals of prediction separately without ever once having a goal of prediction, or have 66,666 different shards of desire across different kinds of predictive subproblems that were entrained by gradient descent which does more brute memorization and less Occam bias than natural selection</p><p>oh, are you asking why humans, when they do goal-oriented Science for the sake of their other goals, don't (universally always) stomp on their other goals while pursuing the Science part?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:54]</strong>&nbsp;</p><p>Well, that might also be interesting to hear the answer to -- I don't know how I'd answer that through an Eliezer-lens -- though it wasn't exactly what I was asking</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][14:56]</strong>&nbsp;</p><p>basically the answer is \"well, first of all, they do stomp on themselves to the extent that they're stupid; and to the extent that they're smart, pursuing X on the pathway to Y has a 'natural' structure for not stomping on Y which is simple and generalizes and obeys all the coherence theorems and can incorporate arbitrarily fine wiggles via epistemic modeling of those fine wiggles because those fine wiggles have a very compact encoding relative to the epistemic model, aka, predicting which forms of X lead to Y; and to the extent that group structures of humans can't do that simple thing coherently because of their cognitive and motivational partitioning, the group structures of humans are back to not being able to coherently pursue the final goal again\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][14:58]</strong>&nbsp;</p><p>(Going back to what I meant to ask) It seems to me like humans demonstrate that you can have a prediction goal without that being your final/terminal goal. So it seems like with AI you similarly need to talk about the final/terminal goal. But then we talked about GPT and debate and so on for a while, and then you explained how GPTs would have deep patterns that do dangerous optimization, where the deep patterns involved instantiating a prediction goal. Notably, you didn't say anything about a final/terminal goal. Do you see why I am confused?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:00]</strong>&nbsp;</p><p>so you can do prediction because it's on the way to some totally other final goal - the way that any tiny superintelligence or superhumanly-coherent agent, if an optimization method somehow managed to run across <i>that</i> early on, with an arbitrary goal, which also understood the larger picture, would make good predictions while it thought the outer loop was probably doing gradient descent updates, and bide its time to produce rather different \"predictions\" once it suspected the results were not going to be checked given what the inputs had looked like.</p><p>you can imagine a thing that does prediction the same way that humans optimize inclusive genetic fitness, by pursuing dozens of little goals that tend to cohere to good prediction in the ancestral environment</p><p>both of these could happen in order; you could get a thing that pursued 66 severed shards of prediction as a small mind, and which, when made larger, cohered into a utility function around the 66 severed shards that sum to something which is not good prediction and which you could pursue by transforming the universe, and then strategically made good predictions while it expected the results to go on being checked</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:02]</strong>&nbsp;</p><p>OH you mean that the outer objective is prediction</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:02]</strong>&nbsp;</p><p>?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:03]</strong>&nbsp;</p><p>I have for quite a while thought that you meant that Science involves internally setting a subgoal of \"predict a confusing part of reality\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:03]</strong>&nbsp;</p><p>it... does?</p><p>I mean, that is true.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:04]</strong>&nbsp;</p><p>Okay wait. There are two things. One is that GPT-3 is trained with a loss function that one might call a prediction objective for human text. Two is that Science involves looking at a part of reality and figuring out how to predict it. These two things are totally different. I am now unsure which one(s) you were talking about in the conversation above</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:06]</strong>&nbsp;</p><p>what I'm saying is that for GPT-5 to successfully do AGI-complete prediction of human text about Science, gradient descent must identify some formless thing that does Science internally in order to optimize the outer loss function for predicting human text about Science</p><p>just like, if it learns to predict human text about multiplication, it must have learned something internally that does multiplication</p><p>(afk, lunch/dinner)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:07]</strong>&nbsp;</p><p>Yeah, so you meant the first thing, and I misinterpreted as the second thing</p><p>(I will head to bed in this case -- I was meaning to do that soon anyway -- but I'll first summarize.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:08]</strong>&nbsp;</p><p>I am concerned that there is still a misinterpretation going on, because the case I am describing is both things at once</p><p>there is an outer loss function that scores text predictions, and an internal process which for purposes of predicting what Science would say must actually somehow do the work of Science</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:09]</strong>&nbsp;</p><p>Okay let me look back at the conversation</p><blockquote><p>is there a little goal-oriented mind inside there that solves science problems the same way humans solve them, by engineering mental constructs that serve a goal of prediction, including backchaining for prediction goals and forward chaining from alternative hypotheses / internal tweaked states of the mental construct?</p></blockquote><p>Here, is the word \"prediction\" meant to refer to the outer objective and/or predicting what English sentences about Science one might say, or is it referring to a subpart of the Process Of Science in which one aims to predict some aspect of reality (which is typically not in the form of English sentences)?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:20]</strong>&nbsp;</p><p>it's here referring to the inner Science problem</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][15:21]</strong>&nbsp;</p><p>Okay I think my original understanding was correct in that case</p><blockquote><p>from my perspective the whole field of mental endeavor as practiced by alignment optimists consists of ancient alchemists wondering if they can get collections of surface properties, like a metal as shiny as gold, as hard as steel, and as self-healing as flesh, where optimism about such wonderfully combined properties can be infinite as long as you stay ignorant of underlying structures that produce some properties but not others</p></blockquote><p>I actually think something like this might be a crux for me, though obviously I wouldn't put it the way you're putting it. More like \"are arguments about internal mechanisms more or less trustworthy than arguments about what you're selecting for\" (limiting to arguments we actually have access to, of course in the limit of perfect knowledge internal mechanisms beats selection). But that is I think a discussion for another day.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:29]</strong>&nbsp;</p><p>I think the critical insight - though it has a format that basically nobody except me ever visibly invokes in those terms, and I worry maybe it can only be taught by a kind of life experience that's very hard to obtain - is the realization that <i>any</i> consistent reasonable story about underlying mechanisms will give you less optimistic forecasts than the ones you get by freely combining surface desiderata</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shah] [1:38]</strong> &nbsp;<strong>(next day, Nov. 14)</strong>&nbsp;</p><p>(For the reader, I don't think that \"arguments about what you're selecting for\" is the same thing as \"freely combining surface desiderata\", though I do expect they look approximately the same to Eliezer)</p><p>Yeah, I think I do not in fact understand why that is true for any consistent reasonable story.</p><p>From my perspective, when I posit a hypothetical, you demonstrate that there is an underlying mechanism that produces strong capabilities that generalize combined with real world knowledge. I agree that a powerful AI system that we build capable of executing a pivotal act will have strong capabilities that generalize and real world knowledge. I am happy to assume for the purposes of this discussion that it involves backchaining from a target and forward chaining from things that you currently know or have. I agree that such capabilities could be used to cause an existential catastrophe (at least in a unipolar world, multipolar case is more complicated, but we can stick with unipolar for now). None of my arguments so far are meant to factor through the route of \"make it so that the AGI can't cause an existential catastrophe even if it wants to\".</p><p>The main question according to me is why those capabilities are aimed towards achievement of a misaligned goal.</p><p>It feels like when I try to ask why we have misaligned goals, I often get answers that are of the form \"look at the deep patterns underlying the strong capabilities that generalize, obviously given a misaligned goal they would generate the plan of killing the humans who are an obstacle towards achieving that goal\". This of course doesn't work since it's a circular argument.</p><p>I can generate lots of arguments for why it would be aimed towards achievement of a misaligned goal, such as (1) only a tiny fraction of goals are aligned; the rest are misaligned, (2) the feedback we provide is unlikely to be the right goal and even small errors are fatal, (3) lots of misaligned goals are compatible with the feedback we provide even if the feedback is good, since the AGI might behave well until it can execute a treacherous turn, (4) the one example of strategically aware intelligence (i.e. humans) is misaligned relative to its creator. (I'm not saying I agree with these arguments, but I do understand them.)</p><p>Are these the arguments that make you think that you get misaligned goals by default? Or is it something about \"deep patterns\" that isn't captured by \"strong capabilities that generalize, real-world knowledge, ability to cause an existential catastrophe if it wants to\"?</p></td></tr></tbody></table>\n\n24\\. Follow-ups\n===============\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:59]</strong> &nbsp;<strong>(Feb. 21, 2022)</strong>&nbsp;</p><p>So I realize it's been a bit, but looking over this last conversation, I feel unhappy about the MIRI conversations sequence stopping exactly here, with an unanswered major question, after I ran out of energy last time.&nbsp; I shall attempt to answer it, at least at all.&nbsp; CC @rohin @RobBensinger .</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 🙂]</td><td>[Ngo: 🙂]</td><td>[Bensinger: 🙂]</td></tr></tbody></table></figure><p>One basic large class of reasons has the form, \"Outer optimization on a precise loss function doesn't get you inner consequentialism explicitly targeting that outer objective, just inner consequentialism targeting objectives which empirically happen to align with the outer objective given that environment and those capability levels; and at some point sufficiently powerful inner consequentialism starts to generalize far out-of-distribution, and, when it does, the consequentialist part generalizes much further than the empirical alignment with the outer objective function.\"</p><p>This, I hope, is by now recognizable to individuals of interest as an overly abstract description of what happened with humans, who one day started building Moon rockets without seeming to care very much about calculating and maximizing their personal inclusive genetic fitness while doing that.&nbsp; Their capabilities generalized much further out of the ancestral training distribution, than the empirical alignment of those capabilities on inclusive genetic fitness in the ancestral training distribution.</p><p>One basic large class of reasons has the form, \"Because the real objective is something that cannot be precisely and accurately shown to the AGI and the differences are systematic and important.\"</p><p>Suppose you have a bunch of humans classifying videos of real events or text descriptions of real events or hypothetical fictional scenarios in text, as desirable or undesirable, and assigning them numerical ratings.&nbsp; Unless these humans are perfectly free of, among other things, all the standard and well-known cognitive biases about eg differently treating losses and gains, the value of this sensory signal is not \"The value of our real CEV rating what is Good or Bad and how much\" nor even \"The value of a utility function we've got right now, run over the real events behind these videos\".&nbsp; Instead it is in a systematic and real and visible way, \"The result of running an error-prone human brain over this data to produce a rating on it.\"</p><p>This is not a mistake by the AGI, it's not something the AGI can narrow down by running more experiments, the <i>correct answer as defined</i> is what contains the alignment difficulty.&nbsp; If the AGI, or for that matter the outer optimization loop, <i>correctly generalizes</i> the function that is producing the human feedback, it will include the systematic sources of error in that feedback.&nbsp; If the AGI essays an experimental test of a manipulation that an ideal observer would see as \"intended to produce error in humans\" then the experimental result will be \"Ah yes, this is correctly part of the objective function, the objective function I'm supposed to maximize sure does have this in it according to the sensory data I got about this objective.\"</p><p>People have fantasized about having the AGI learn something other than the true and accurate function producing its objective-describing data, as its actual objective, from the objective-describing data that it gets; I, of course, was the first person to imagine this and say it should be done, back in 2001 or so; unlike a lot of latecomers to this situation, I am skeptical of my own proposals and I know very well that I did not in fact come up with any reliable-looking proposal for learning 'true' human values off systematically erroneous human feedback.</p><p>Difficulties here are fatal, because a true and accurate learning of what is producing the objective-describing signal, will correctly imply that higher values of this signal obtain as the humans are manipulated or as they are bypassed with physical interrupts for control of the feedback signal.&nbsp; In other words, even if you could do a bunch of training on an outer objective, and get inner optimization perfectly targeted on that, the fact that it was perfectly targeted would kill you.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Bensinger][23:15] &nbsp;(Feb. 27, 2022 follow-up comment)</strong>&nbsp;</p><p>This is the last log in the <a href=\"https://intelligence.org/late-2021-miri-conversations/\">Late 2021 MIRI Conversations</a>. We'll be concluding the sequence with a public <a href=\"https://www.lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-discussion-and-ama\"><strong>Ask Me Anything</strong></a><strong> </strong>(AMA)<strong> </strong>this Wednesday; you can start posting questions there now.</p><p>MIRI has found the Discord format useful, and we plan to continue using it going into 2022. This includes follow-up conversations between Eliezer and Rohin, and a forthcoming conversation between Eliezer and Scott Alexander of <a href=\"https://astralcodexten.substack.com/\">Astral Codex Ten</a>.</p><p>Some concluding thoughts from Richard Ngo:</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][6:20] &nbsp;(Nov. 12 follow-up comment)</strong>&nbsp;</p><p>Many thanks to Eliezer and Nate for their courteous and constructive discussion and moderation, and to Rob for putting the transcripts together.</p><p>This debate updated me about 15% of the way towards Eliezer's position, with Eliezer's arguments about the difficulties of coordinating to&nbsp;ensure alignment responsible for most of that shift. While I don't find Eliezer's core intuitions about intelligence too implausible, they don't seem compelling enough to do as much work as Eliezer argues they do. As in the Foom debate, I think that our object-level discussions were constrained by our different underlying attitudes towards high-level abstractions, which are hard to pin down (let alone resolve).</p><p>Given this, I think that the most productive mode of intellectual engagement with Eliezer's worldview going forward is probably not to continue debating it (since that would likely hit those same underlying disagreements), but rather to try to inhabit it deeply enough to rederive his conclusions and find new explanations of them which then lead to clearer object-level cruxes. I hope that these transcripts shed sufficient light for some readers to be able to do so.</p></td></tr></tbody></table>",
      "plaintextDescription": "This is the final discussion log in the Late 2021 MIRI Conversations sequence, featuring Rohin Shah and Eliezer Yudkowsky, with additional comments from Rob Bensinger, Nate Soares, Richard Ngo, and Jaan Tallinn.\n\nThe discussion begins with summaries and comments on Richard and Eliezer's debate. Rohin's summary has since been revised and published in the Alignment Newsletter.\n\nAfter this log, we'll be concluding this sequence with an AMA, where we invite you to comment with questions about AI alignment, cognition, forecasting, etc. Eliezer, Richard, Paul Christiano, Nate, and Rohin will all be participating.\n\n \n\nColor key:\n\n Chat by Rohin and Eliezer  Other chat  Emails  Follow-ups \n\n \n\n\n19. Follow-ups to the Ngo/Yudkowsky conversation\n \n\n\n19.1. Quotes from the public discussion\n \n\n[Bensinger][9:22]  (Nov. 25) \n\nInteresting extracts from the public discussion of Ngo and Yudkowsky on AI capability gains:\n\nEliezer:\n\n> I think some of your confusion may be that you're putting \"probability theory\" and \"Newtonian gravity\" into the same bucket.  You've been raised to believe that powerful theories ought to meet certain standards, like successful bold advance experimental predictions, such as Newtonian gravity made about the existence of Neptune (quite a while after the theory was first put forth, though).  \"Probability theory\" also sounds like a powerful theory, and the people around you believe it, so you think you ought to be able to produce a powerful advance prediction it made; but it is for some reason hard to come up with an example like the discovery of Neptune, so you cast about a bit and think of the central limit theorem.  That theorem is widely used and praised, so it's \"powerful\", and it wasn't invented before probability theory, so it's \"advance\", right?  So we can go on putting probability theory in the same bucket as Newtonian gravity?\n> \n> They're actually just very different kinds of ideas, ontologically speaking, and the standards to which we hold them ar",
      "wordCount": 27433
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "NbGmfxbaABPsspib7",
    "title": "Christiano and Yudkowsky on AI predictions and human intelligence",
    "slug": "christiano-and-yudkowsky-on-ai-predictions-and-human",
    "url": null,
    "baseScore": 71,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2022-02-23T21:34:55.245Z",
    "contents": {
      "markdown": "This is a transcript of a conversation between Paul Christiano and Eliezer Yudkowsky, with comments by Rohin Shah, Beth Barnes, Richard Ngo, and Holden Karnofsky, continuing the [Late 2021 MIRI Conversations](https://intelligence.org/late-2021-miri-conversations/).\n\n  \nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Paul and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td></tr></tbody></table>\n\n15\\. October 19 comment\n=======================\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:01]</strong>&nbsp;</p><p>thing that struck me as an iota of evidence for Paul over Eliezer: <a href=\"https://twitter.com/tamaybes/status/1450514423823560706?s=20\">https://twitter.com/tamaybes/status/1450514423823560706?s=20</a>&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_100 100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_300 300w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_500 500w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_900 900w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bccc3fcd315a12be417c814fe75a6c761049a9225425cc67.png/w_935 935w\"></figure></td></tr></tbody></table>\n\n16\\. November 3 conversation\n============================\n\n16.1. EfficientZero\n-------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][9:30]</strong>&nbsp;</p><p>Thing that (if true) strikes me as... straight-up falsifying Paul's view as applied to modern-day AI, at the frontier of the most AGI-ish part of it and where Deepmind put in substantial effort on their project? &nbsp;EfficientZero (allegedly) learns Atari in 100,000 frames. &nbsp;Caveat: I'm not having an easy time figuring out how many frames MuZero would've required to achieve the same performance level. &nbsp;MuZero was trained on 200,000,000 frames but reached what looks like an allegedly higher high; the EfficientZero paper compares their performance to MuZero on 100,000 frames, and claims theirs is much better than MuZero given only that many frames.</p><p><a href=\"https://arxiv.org/pdf/2111.00210.pdf\">https://arxiv.org/pdf/2111.00210.pdf</a> &nbsp;CC: @paulfchristiano.</p><p>(I would further argue that this case is important because it's about the central contemporary model for approaching AGI, at least according to Eliezer, rather than any number of random peripheral AI tasks.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][14:46]</strong> &nbsp;</p><p>I only looked at the front page, so might be misunderstanding, but the front figure says \"Our proposed method EfficientZero is 170% and 180% better than the previous SoTA performance in mean and median human normalized score [...] on the Atari 100k benchmark\", which does not seem like a huge leap?</p><p>Oh, I incorrectly thought that was 1.7x and 1.8x, but it is actually 2.7x and 2.8x, which is a bigger deal (though still feels not crazy to me)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:28]</strong> &nbsp;</p><p>the question imo is how many frames the previous SoTA would require to catch up to EfficientZero</p><p>(I've tried emailing an author to ask about this, no response yet)</p><p>like, perplexity on GPT-3 vs GPT-2 and \"losses decreased by blah%\" would give you a pretty meaningless concept of how far ahead GPT-3 was from GPT-2, and I think the \"2.8x performance\" figure in terms of scoring is equally meaningless as a metric of how much EfficientZero improves if any</p><p>what you want is a notion like \"previous SoTA would have required 10x the samples\" or \"previous SoTA would have required 5x the computation\" to achieve that performance level</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][15:38]</strong> &nbsp;</p><p>I see. Atari curves are not nearly as nice and stable as GPT curves and often have the problem that they plateau rather than making steady progress with more training time, so that will make these metrics noisier, but it does seem like a reasonable metric to track</p><p>(Not that I have recommendations about how to track it; I doubt the authors can easily get these metrics)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][18:01]</strong>&nbsp;</p><p>If you think our views are making such starkly different predictions then I'd be happy to actually state any of them in advance, including e.g. about future ML benchmark results.</p><p>I don't think this falsifies my view, and we could continue trying to hash out what my view is but it seems like slow going and I'm inclined to give up.</p><p>Relevant questions on my view are things like: is MuZero optimized at all for performance in the tiny-sample regime? (I think not, I don't even think it set SoTA on that task and I haven't seen any evidence.) What's the actual rate of improvements since people started studying this benchmark ~2 years ago, and how much work has gone into it? And I totally agree with your comments that \"# of frames\" is the natural unit for measuring and that would be the starting point for any discussion.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Barnes][18:22]</strong>&nbsp;</p><blockquote><p>In previous MCTS RL algorithms, the environment model is either given or only trained with rewards, values, and policies, which cannot provide sufficient training signals due to their scalar nature. The problem is more severe when the reward is sparse or the bootstrapped value is not accurate. The MCTS policy improvement operator heavily relies on the environment model. Thus, it is vital to have an accurate one.</p><p>We notice that the output&nbsp;<span class=\"math-tex\">\\(\\hat{s}_{t+1}\\)</span>&nbsp;from the dynamic function&nbsp;<span class=\"math-tex\">\\(\\mathcal{G}\\)</span>&nbsp;should be the same as&nbsp;<span class=\"math-tex\">\\(s_{t+1}\\)</span>, i.e. the output of the representation function&nbsp;<span class=\"math-tex\">\\(\\mathcal{H}\\)</span>&nbsp;with input of the next observation&nbsp;<span class=\"math-tex\">\\(o_{t+1}\\)</span>&nbsp;(Fig. 2). This can help to supervise the predicted next state&nbsp;<span class=\"math-tex\">\\(\\hat{s}_{t+1}\\)</span>&nbsp;using the actual&nbsp;<span class=\"math-tex\">\\(s_{t+1}\\)</span>, which is a tensor with at least a few hundred dimensions. This provides&nbsp;<span class=\"math-tex\">\\(\\hat{s}_{t+1}\\)</span>&nbsp;with much more training signals than the default scalar reward and value.</p></blockquote><p>This seems like a super obvious thing to do and I'm confused why DM didn't already try this. It was definitely being talked about in ~2018</p><p>Will ask a DM friend about it</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][22:45]</strong>&nbsp;</p><p>I... don't think I want to take <i>all</i> of the blame for misunderstanding Paul's views; I think I also want to complain at least a little that Paul spends an insufficient quantity of time pointing at extremely concrete specific possibilities, especially real ones, and saying how they do or don't fit into the scheme.</p><p>Am I rephrasing correctly that, in this case, if Efficient Zero was actually a huge (3x? 5x? 10x?) jump in RL sample efficiency over previous SOTA, measured in 1 / frames required to train to a performance level, then that means the Paul view <i>doesn't</i> apply to the present world; but this could be because MuZero wasn't the real previous SOTA, or maybe because nobody really worked on pushing out this benchmark for 2 years and therefore on the Paul view it's fine for there to still be huge jumps? &nbsp;In other words, this is something Paul's worldview has to either defy or excuse, and not just, \"well, sure, why wouldn't it do that, you have misunderstood which kinds of AI-related events Paul is even trying to talk about\"?</p><p>In the case where, \"yes it's a big jump and that shouldn't happen later, but it could happen now because it turned out nobody worked hard on pushing past MuZero over the last 2 years\", I wish to register that my view permits it to be the case that, when the world begins to end, the frontier that enters into AGI is similarly something that not a lot of people spent a huge effort on since a previous prototype from 2 years earlier. &nbsp;It's just not very surprising to me if the future looks a lot like the past, or if human civilization neglects to invest a ton of effort in a research frontier.</p><p>Gwern guesses that getting to EfficientZero's performance level would require around 4x the samples for MuZero-Reanalyze (the more efficient version of MuZero which replayed past frames), which is also apparently the only version of MuZero the paper's authors were considering in the first place - without replays, MuZero requires 20 billion frames to achieve its performance, not the figure of 200 million. <a href=\"https://www.lesswrong.com/posts/jYNT3Qihn2aAYaaPb/efficientzero-human-ale-sample-efficiency-w-muzero-self?commentId=JEHPQa7i8Qjcg7TW6\">https://www.lesswrong.com/posts/jYNT3Qihn2aAYaaPb/efficientzero-human-ale-sample-efficiency-w-muzero-self?commentId=JEHPQa7i8Qjcg7TW6</a></p></td></tr></tbody></table>\n\n17\\. November 4 conversation\n============================\n\n17.1. EfficientZero (continued)\n-------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][7:42]</strong>&nbsp;</p><p>I think it's possible the biggest misunderstanding is that you somehow think of my view as a \"scheme\" and your view as a normal view where probability distributions over things happen.</p><p>Concretely, this is a paper that adds a few techniques to improve over MuZero in a domain that (it appears) wasn't a significant focus of MuZero. I don't know how much it improves but I can believe gwern's estimates of 4x.</p><p>I'd guess MuZero itself is a 2x improvement over the baseline from a year ago, which was maybe a 4x improvement over the algorithm from a year before that.</p><p>If that's right, then no it's not mindblowing on my view to have 4x progress one year, 2x progress the next, and 4x progress the next.</p><p>If other algorithms were better than MuZero, then the 2019-2020 progress would be &gt;2x and the 2020-2021 progress would be &lt;4x.</p><p>I think it's probably &gt;4x sample efficiency though (I don't totally buy gwern's estimate there), which makes it at least possibly surprising.</p><p>But it's never going to be that surprising. It's a benchmark that people have been working on for a few years that has been seeing relatively rapid improvement over that whole period.</p><p>The main innovation is how quickly you can learn to predict future frames of Atari games, which has tiny economic relevance and calling it the most AGI-ish direction seems like it's a very Eliezer-ish view, this isn't the kind of domain where I'm either most surprised to see rapid progress at all nor is the kind of thing that seems like a key update re: transformative AI</p><p>yeah, SoTA in late 2020 was SPR, published by a much smaller academic group: <a href=\"https://arxiv.org/pdf/2007.05929.pdf\">https://arxiv.org/pdf/2007.05929.pdf</a></p><p>MuZero wasn't even setting sota on this task at the time it was published</p><p>my \"schemes\" are that (i) if a bunch of people are trying on a domain and making steady slow progress, I'm surprised to see giant jumps and I don't expect most absolute progress to occur in such jumps, (ii) if a domain is worth a lot of $, generally a bunch of people will be trying. Those aren't claims about what is always true, they are claims about what is typically true and hence what I'm guessing will be true for transformative AI.</p><p>Maybe you think those things aren't even good general predictions, and that I don't have long enough tails in my distributions or whatever. But in that case it seems we can settle it quickly by prediction.</p><p>I think this result is probably significant (&gt;30% absolute improvement) + faster-than-trend (&gt;50% faster than previous increment) progress relative to prior trend on 8 of the 27 atari games (from table 1, treating SimPL-&gt;{max of MuZero, SPR}-&gt;EfficientZero as 3 equally spaced datapoints): Asterix, Breakout, almost ChopperCMD, almost CrazyClimber, Gopher, Kung Fu Master, Pong, QBert, SeaQuest. My guess is that they thought a lot about a few of those games in particular because they are very influential on the mean/median. Note that this paper is a giant grab bag and that simply stapling together the prior methods would have already been a significant improvement over prior SoTA. (ETA: I don't think saying \"its only 8 of 27 games\" is an update against it being big progress or anything. I do think saying \"stapling together 2 previous methods without any complementarity at all would already have significantly beaten SoTA\" is fairly good evidence that it's not a hard-to-beat SoTA.)</p><p>and even fewer people working on the ultra-low-sample extremely-low-dimensional DM control environments (this is the subset of problems where the state space is 4 dimensions, people are just not trying to publish great results on cartpole), so I think the most surprising contribution is the atari stuff</p><p>OK, I now also understand what the result is I think?</p><p>I think the quick summary is: the prior SoTA is SPR, which learns to predict the domain and then does Q-learning. MuZero instead learns to predict the domain and does MCTS, but it predicts the domain in a slightly less sophisticated way than SPR (basically just predicts rewards, whereas SPR predicts all of the agent's latent state in order to get more signal from each frame). If you combine MCTS with more sophisticated prediction, you do better.</p><p>I think if you told me that DeepMind put in significant effort in 2020 (say, at least as much post-MuZero effort as the new paper?) trying to get great sample efficiency on the easy-exploration atari games, and failed to make significant progress, then I'm surprised.</p><p>I don't think that would \"falsify\" my view, but it would be an update against? Like maybe if DM put in that much effort I'd maybe have given only a 10-20% probability to a new project of similar size putting in that much effort making big progress, and even conditioned on big progress this is still &gt;&gt;median (ETA: and if DeepMind put in much more effort I'd be more surprised than 10-20% by big progress from the new project)</p><p>Without DM putting in much effort, it's significantly less surprising and I'll instead be comparing to the other academic efforts. But it's just not surprising that you can beat them if you are willing to put in the effort to reimplement MCTS and they aren't, and that's a step that is straightforwardly going to improve performance.</p><p>(not sure if that's the situation)</p><p>And then to see how significant updates against are, you have to actually contrast them with all the updates in the other direction where people <i>don't</i> crush previous benchmark results</p><p>and instead just make modest progress</p><p>I would guess that if you had talked to an academic about this question (what happens if you combine SPR+MCTS) they would have predicted significant wins in sample efficiency (at the expense of compute efficiency) and cited the difficulty of implementing MuZero compared to any of the academic results. That's another way I could be somewhat surprised (or if there were academics with MuZero-quality MCTS implementations working on this problem, and they somehow didn't set SoTA, then I'm even more surprised). But I'm not sure if you'll trust any of those judgments in hindsight.</p><p><i>Repeating the main point</i><strong>:</strong></p><p>I don't really think a 4x jump over 1 year is something I have to \"defy or excuse\", it's something that I think becomes more or less likely depending on facts about the world, like (i) how fast was previous progress, (ii) how many people were working on previous projects and how targeted were they at this metric, (iii) how many people are working in this project and how targeted was it at this metric</p><p>it becomes continuously less likely as those parameters move in the obvious directions</p><p>it never becomes 0 probability, and you just can't win that much by citing isolated events that I'd give say a 10% probability to, unless you actually say something about how you are giving &gt;10% probabilities to those events without losing a bunch of probability mass on what I see as the 90% of boring stuff</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure><p>and then separately I have a view about lots of people working on important problems, which doesn't say anything about this case</p><p>(I actually don't think this event is as low as 10%, though it depends on what background facts about the project you are conditioning on---obviously I gave &lt;&lt;10% probability to someone publishing this particular result, but something like \"what fraction of progress in this field would come down to jumps like this\" or whatever is probably &gt;10% until you tell me that DeepMind actually cared enough to have already tried)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Ngo][8:48]</strong>&nbsp;</p><p>I expect Eliezer to say something like: DeepMind believes that both improving RL sample efficiency, and benchmarking progress on games like Atari, are important parts of the path towards AGI. So insofar as your model predicts that smooth progress will be caused by people working directly towards AGI, DeepMind not putting effort into this is a hit to that model. Thoughts?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][9:06]</strong>&nbsp;</p><p>I don't think that learning these Atari games in 2 hours is a very interesting benchmark even for deep RL sample efficiency, and it's totally unrelated to the way in which humans learn such games quickly. It seems <s>pretty likely</s> totally plausible (50%?) to me that DeepMind feels the same way, and then the question is about other random considerations like how they are making some PR calculation.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Ngo][9:18]</strong>&nbsp;</p><p>If Atari is not a very interesting benchmark, then why did DeepMind put a bunch of effort into making Agent57 and applying MuZero to Atari?</p><p>Also, most of the effort they've spent on games in general has been on methods very unlike the way humans learn those games, so that doesn't seem like a likely reason for them to overlook these methods for increasing sample efficiency.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Shah][9:32]</strong>&nbsp;</p><blockquote><p>It seems pretty likely totally plausible (50%?) to me that DeepMind feels the same way, and then the question is about other random considerations like how they are making some PR calculation.</p></blockquote><p>Not sure of the exact claim, but DeepMind is big enough and diverse enough that I'm pretty confident at least some people working on relevant problems don't feel the same way</p><blockquote><p>[...] This seems like a super obvious thing to do and I'm confused why DM didn't already try this. It was definitely being talked about in ~2018</p></blockquote><p>Speculating without my DM hat on: maybe it kills performance in board games, and they want one algorithm for all settings?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][10:29]</strong>&nbsp;</p><p>Atari games in the tiny sample regime are a different beast</p><p>there are just a lot of problems you can state about Atari some of which are more or less interesting (e.g. jointly learning to play 57 Atari games is a more interesting problem than learning how to play one of them absurdly quickly, and there are like 10 other problems about Atari that are more interesting than this one)</p><p>That said, Agent57 also doesn't seem interesting except that it's an old task people kind of care about. I don't know about the take within DeepMind but outside I don't think anyone would care about it other than historical significance of the benchmark / obviously-not-cherrypickedness of the problem.</p><p>I'm sure that some people at DeepMind care about getting the super low sample complexity regime. I don't think that really tells you how large the DeepMind effort is compared to some random academics who care about it.</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure><p>I think the argument for working on deep RL is fine and can be based on an analogy with humans while you aren't good at the task. Then once you are aiming for crazy superhuman performance on Atari games you naturally start asking \"what are we doing here and why are we still working on atari games?\"</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure><p>and correspondingly they are a smaller and smaller slice of DeepMind's work over time</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure><p>(e.g. Agent57 and MuZero are the only DeepMind blog posts about Atari in the last 4 years, it's not the main focus of MuZero and I don't think Agent57 is a very big DM project)</p><p>Reaching this level of performance in Atari games is largely about learning perception, and doing that from 100k frames of an Atari game just doesn't seem very analogous to anything humans do or that is economically relevant from any perspective. I totally agree some people are into it, but I'm totally not surprised if it's not going to be a big DeepMind project.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][10:51]</strong>&nbsp;</p><p>would you agree it's a load-bearing assumption of your worldview - where I also freely admit to having a worldview/scheme, this is not meant to be a prejudicial term at all - that the line of research which leads into world-shaking AGI must be in the mainstream and not in a weird corner where a few months earlier there were more profitable other ways of doing all the things that weird corner did?&nbsp;</p><p>eg, the tech line leading into world-shaking AGI must be at the profitable forefront of non-world-shaking tasks. &nbsp;as otherwise, afaict, your worldview permits that if counterfactually we were in the Paul-forbidden case where the immediate precursor to AGI was something like EfficientZero (whose motivation had been beating an old SOTA metric rather than, say, market-beating self-driving cars), there might be huge capability leaps there just as EfficientZero represents a large leap, because there wouldn't have been tons of investment in that line.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][10:54]</strong>&nbsp;</p><p>Something like that is definitely a load-bearing assumption</p><p>Like there's a spectrum with e.g. EfficientZero --&gt; 2016 language modeling --&gt; 2014 computer vision --&gt; 2021 language modeling --&gt; 2021 computer vision, and I think everything anywhere close to transformative AI will be way way off the right end of that spectrum</p><p>But I think quantitatively the things you are saying don't seem quite right to me. Suppose that MuZero wasn't the best way to do anything economically relevant, but it was within a factor of 4 on sample efficiency for doing tasks that people care about. That's already going to be enough to make tons of people extremely excited.</p><p>So yes, I'm saying that anything leading to transformative AI is \"in the mainstream\" in the sense that it has more work on it than 2021 language models.</p><p>But not necessarily that it's the most profitable way to do anything that people care about. Different methods scale in different ways, and something can burst onto the scene in a dramatic way, but I strongly expect speculative investment driven by that possibility to already be way (way) more than 2021 language models. And I don't expect gigantic surprises. And I'm willing to bet that e.g. EfficientZero isn't a big surprise for researchers who are paying attention to the area (<i>in addition</i> to being 3+ orders of magnitude more neglected than anything close to transformative AI)</p><p>2021 language modeling isn't even very competitive, it's still like 3-4 orders of magnitude smaller than semiconductors. But I'm giving it as a reference point since it's obviously much, much more competitive than sample-efficient atari.</p><p>This is a place where I'm making much more confident predictions, this is \"falsify paul's worldview\" territory once you get to quantitative claims anywhere close to TAI and \"even a single example seriously challenges paul's worldview\" a few orders of magnitude short of that</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:04]</strong>&nbsp;</p><p>can you say more about what falsifies your worldview previous to TAI being super-obviously-to-all-EAs imminent?</p><p>or rather, \"seriously challenges\", sorry</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:05][11:08]</strong>&nbsp;</p><p>big AI applications achieved by clever insights in domains that aren't crowded, we should be quantitative about how crowded and how big if we want to get into \"seriously challenges\"</p><p>like e.g. if this paper on atari was actually a crucial ingredient for making deep RL for robotics work, I'd be actually for real surprised rather than 10% surprised</p><p>but it's not going to be, those results are being worked on by much larger teams of more competent researchers at labs with $100M+ funding</p><p>it's definitely possible for them to get crushed by something out of left field</p><p>but I'm betting against every time</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">or like, the set of things people would describe as \"out of left field,\" and the quantitative degree of neglectedness, becomes more and more mild as the stakes go up</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:08]</strong>&nbsp;</p><p>how surprised are you if in 2022 one company comes out with really good ML translation, and they manage to sell a bunch of it temporarily until others steal their ideas or Google acquires them? &nbsp;my model of Paul is unclear on whether this constitutes \"many people are already working on language models including ML translation\" versus \"this field is not profitable enough right this minute for things to be efficient there, and it's allowed to be nonobvious in worlds where it's about to become profitable\".</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:08]</strong>&nbsp;</p><p>if I wanted to make a prediction about that I'd learn a bunch about how much google works on translation and how much $ they make</p><p>I just don't know the economics</p><p>and it depends on the kind of translation that they are good at and the economics (e.g. google mostly does extremely high-volume very cheap translation)</p><p>but I think there are lots of things like that / facts I could learn about Google such that I'd be surprised in that situation</p><p>independent of the economics, I do think a fair number of people are working on adjacent stuff, and I don't expect someone to come out of left field for google-translate-cost translation between high-resource languages</p><p>but it seems quite plausible that a team of 10 competent people could significantly outperform google translate, and I'd need to learn about the economics to know how surprised I am by 10 people or 100 people or what</p><p>I think it's allowed to be non-obvious whether a domain is about to be really profitable</p><p>but it's not that easy, and the higher the stakes the more speculative investment it will drive, etc.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:14]</strong>&nbsp;</p><p>if you don't update much off EfficientZero, then people also shouldn't be updating much off of most of the graph I posted earlier as possible Paul-favoring evidence, because most of those SOTAs weren't highly profitable so your worldview didn't have much to say about them. ?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:15]</strong>&nbsp;</p><p>Most things people work a lot on improve gradually. EfficientZero is also quite gradual compared to the crazy TAI stories you tell. I don't really know what to say about this game other than I would prefer make predictions in advance and I'm happy to either propose questions/domains or make predictions in whatever space you feel more comfortable with.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:16]</strong>&nbsp;</p><p>I don't know how to point at a future event that you'd have strong opinions about. &nbsp;it feels like, whenever I try, I get told that the current world is too unlike the future conditions you expect.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:16]</strong>&nbsp;</p><p>Like, whether or not EfficientZero is evidence for your view depends on exactly how \"who knows what will happen\" you are. if you are just a bit more spread out than I am, then it's definitely evidence for your view.</p><p>I'm saying that I'm willing to bet about <i>any event you want to name</i>, I just think my model of how things work is more accurate.</p><p>I'd prefer it be related to ML or AI.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:17]</strong>&nbsp;</p><p>to be clear, I appreciate that it's similarly hard to point at an event like that for myself, because my own worldview says \"well mostly the future is not all that predictable with a few rare exceptions\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:17]</strong>&nbsp;</p><p>But I feel like the situation is not at all symmetrical, I expect to outperform you on practically any category of predictions we can specify.</p><p>so like I'm happy to bet about benchmark progress in LMs, or about whether DM or OpenAI or Google or Microsoft will be the first to achieve something, or about progress in computer vision, or about progress in industrial robotics, or about translations</p><p>whatever</p></td></tr></tbody></table>\n\n17.2. Near-term AI predictions\n------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:18]</strong>&nbsp;</p><p>that sounds like you ought to have, like, a full-blown storyline about the future?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:18]</strong>&nbsp;</p><p>what is a full-blown storyline? I have a bunch of ways that I think about the world and make predictions about what is likely</p><p>and yes, I can use those ways of thinking to make predictions about whatever</p><p>and I will very often lose to a domain expert who has better and more informed ways of making predictions</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:19]</strong>&nbsp;</p><p>what happens if 2022 through 2024 looks literally exactly like Paul's modal or median predictions on things?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:19]</strong>&nbsp;</p><p>but I think in ML I will generally beat e.g. a superforecaster who doesn't have a lot of experience in the area</p><p>give me a question about 2024 and I'll give you a median?</p><p>I don't know what \"what happens\" means</p><p>storylines do not seem like good ways of making predictions</p><figure class=\"table\"><table><tbody><tr><td>[Shah: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:20]</strong>&nbsp;</p><p>I mean, this isn't a crux for anything, but it seems like you're asking me to give up on that and just ask for predictions? &nbsp;so in 2024 can I hire an artist who doesn't speak English and converse with them almost seamlessly through a machine translator?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:22]</strong>&nbsp;</p><p>median outcome (all of these are going to be somewhat easy-to-beat predictions because I'm not thinking): you can get good real-time translations, they are about as good as a +1 stdev bilingual speaker who listens to what you said and then writes it out in the other language as fast as they can type</p><p>Probably also for voice -&gt; text or voice -&gt; voice, though higher latencies and costs.</p><p>Not integrated into standard video chatting experience because the UX is too much of a pain and the world sucks.</p><p>That's a median on \"how cool/useful is translation\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:23]</strong>&nbsp;</p><p>I would unfortunately also predict that in this case, this will be a highly competitive market and hence not a very profitable one, which I predict to match your prediction, but I ask about the economics here just in case.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:24]</strong>&nbsp;</p><p>Kind of typical sample: I'd guess that Google has a reasonably large lead, most translation still provided as a free value-added, cost per translation at that level of quality is like $0.01/word, total revenue in the area is like $10Ms / year?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:24]</strong>&nbsp;</p><p>well, my model also permits that Google does it for free and so it's an uncompetitive market but not a profitable one... ninjaed.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:25]</strong>&nbsp;</p><p>first order of improving would be sanity-checking economics and thinking about #s, second would be learning things like \"how many people actually work on translation and what is the state of the field?\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:26]</strong>&nbsp;</p><p>did Tesla crack self-driving cars and become a $3T company instead of a $1T company? &nbsp;do you own Tesla options?</p><p>did Waymo beat Tesla and cause Tesla stock to crater, same question?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:27]</strong>&nbsp;</p><p>1/3 chance tesla has FSD in 2024</p><p>conditioned on that, yeah probably market cap is &gt;$3T?</p><p>conditioned on Tesla having FSD, 2/3 chance Waymo has also at least rolled out to a lot of cities</p><p>conditioned on no tesla FSD, 10% chance Waymo has rolled out to like half of big US cities?</p><p>dunno if numbers make sense</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:28]</strong>&nbsp;</p><p>that's okay, I dunno if my questions make sense</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:29]</strong>&nbsp;</p><p>(5% NW in tesla, 90% NW in AI bets, 100% NW in more normal investments; no tesla options that sounds like a scary place with lottery ticket biases and the crazy tesla investors)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:30]</strong>&nbsp;</p><p>(am I correctly understanding you're 2x levered?)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:30][11:31]</strong>&nbsp;</p><p>yeah</p><p>it feels like you've got to have weird views on trajectory of value-added from AI over the coming years</p><p>on how much of the $ comes from domains that are currently exciting to people (e.g. that Google already works on, self-driving, industrial robotics) vs stuff out of left field</p><p>on what kind of algorithms deliver $ in those domains (e.g. are logistics robots trained using the same techniques tons of people are currently pushing on)</p><p>on my picture you shouldn't be getting big losses on any of those</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\">just losing like 10-20% each time</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:31][11:32]</strong>&nbsp;</p><p>my uncorrected inside view says that machine translation should be in reach and generate huge amounts of economic value even if it ends up an unprofitable competitive or Google-freebie field</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\">and also that not many people are working on basic research in machine translation or see it as a \"currently exciting\" domain</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:32]</strong>&nbsp;</p><p>how many FTE is \"not that many\" people?</p><p>also are you expecting improvement in the google translate style product, or in lower-latencies for something closer to normal human translator prices, or something else?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:33]</strong>&nbsp;</p><p>my worldview says more like... sure, maybe there's 300 programmers working on it worldwide, but most of them aren't aggressively pursuing new ideas and trying to explore the space, they're just applying existing techniques to a new language or trying to throw on some tiny mod that lets them beat SOTA by 1.2% for a publication</p><p>because it's not an <i>exciting</i> field</p><p>\"What if you could rip down the language barriers\" is an economist's dream, or a humanist's dream, and Silicon Valley is neither</p><p>and looking at GPT-3 and saying, \"God damn it, this really seems like it must on some level <i>understand</i> what it's reading well enough that the same learned knowledge would suffice to do really good machine translation, this must be within reach for gradient descent technology we just don't know how to reach it\" is Yudkowskian thinking; your AI system has internal parts like \"how much it understands language\" and there's thoughts about what those parts ought to be able to do if you could get them into a new system with some other parts</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:36]</strong>&nbsp;</p><p>my guess is we'd have some disagreements here</p><p>but to be clear, you are talking about text-to-text at like $0.01/word price point?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:38]</strong>&nbsp;</p><p>I mean, do we? &nbsp;Unfortunately another Yudkowskian worldview says \"and people can go on failing to notice this for arbitrarily long amounts of time\".</p><p>if that's around GPT-3's price point then yeah</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:38]</strong>&nbsp;</p><p>gpt-3 is a lot cheaper, happy to say gpt-3 like price point</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:39]</strong>&nbsp;</p><p>(thinking about whether $0.01/word is meaningfully different from $0.001/word and concluding that it is)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:39]</strong>&nbsp;</p><p>(api is like 10,000 words / $)</p><p>I expect you to have a broader distribution over who makes a great product in this space, how great it ends up being etc., whereas I'm going to have somewhat higher probabilities on it being google research and it's going to look boring</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:40]</strong>&nbsp;</p><p>what is boring?</p><p>boring predictions are often good predictions on my own worldview too</p><p>lots of my gloom is about things that are boringly bad and awful</p><p>(and which add up to instant death at a later point)</p><p>but, I mean, what does boring machine translation look like?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:42]</strong>&nbsp;</p><p>Train big language model. Have lots of auxiliary tasks especially involving reading in source language and generation in target language. Have pre-training on aligned sentences and perhaps using all the unsupervised translation we have depending on how high-resource language is. Fine-tune with smaller amount of higher quality supervision.</p><p>Some of the steps likely don't add much value and skip them. Fair amount of non-ML infrastructure.</p><p>For some languages/domains/etc. dedicated models, over time increasingly just have a giant model with learned dispatch as in mixture of experts.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:44]</strong>&nbsp;</p><p>but your worldview is also totally ok with there being a Clever Trick added to that which produces a 2x reduction in training time. &nbsp;or with there being a new innovation like transformers, which was developed a year earlier and which everybody now uses, without which the translator wouldn't work at all. ?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:44]</strong>&nbsp;</p><p>Just for reference, I think transformers aren't that visible on a (translation quality) vs (time) graph?</p><p>But yes, I'm totally fine with continuing architectural improvements, and 2x reduction in training time is currently par for the course for \"some people at google thought about architectures for a while\" and I expect that to not get that much tighter over the next few years.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:45]</strong>&nbsp;</p><p>unrolling Restricted Boltzmann Machines to produce deeper trainable networks probably wasn't much visible on a graph either, but good luck duplicating modern results using only lower portions of the tech tree. &nbsp;(I don't think we disagree about this.)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:45]</strong>&nbsp;</p><p>I do expect it to eventually get tighter, but not by 2024.</p><p>I don't think unrolling restricted boltzmann machines is that important</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:46]</strong>&nbsp;</p><p>like, historically, or as a modern technology?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:46]</strong>&nbsp;</p><p>historically</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:46]</strong>&nbsp;</p><p>interesting</p><p>my model is that it got people thinking about \"what makes things trainable\" and led into ReLUs and inits</p><p>but I am going more off having watched from the periphery as it happened, than having read a detailed history of that</p><p>like, people asking, \"ah, but what if we had a deeper network and the gradients <i>didn't</i> explode or die out?\" and doing that en masse in a productive way rather than individuals being wistful for 30 seconds</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:48]</strong>&nbsp;</p><p>well, not sure if this will introduce differences in predictions</p><p>I don't feel like it should really matter for our bottom line predictions whether we classify google's random architectural change as something fundamentally new (which happens to just have a modest effect at the time that it's built) or as something boring</p><p>I'm going to guess how well things will work by looking at how well things work right now and seeing how fast it's getting better</p><p>and that's also what I'm going to do for applications of AI with transformative impacts</p><p>and I actually believe you will do something today that's analogous to what you would do in the future, and in fact will make somewhat different predictions than what I would do</p><p>and then some of the action will be in new things that people haven't been trying to do in the past, and I'm predicting that new things will be \"small\" whereas you have a broader distribution, and there's currently some not-communicated judgment call in \"small\"</p><p>if you think that TAI will be like translation, where google publishes tons of papers, but that they will just get totally destroyed by some new idea, then it seems like that should correspond to a difference in P(google translation gets totally destroyed by something out-of-left-field)</p><p>and if you think that TAI won't be like translation, then I'm interested in examples more like TAI</p><p>I don't really understand the take \"and people can go on failing to notice this for arbitrarily long amounts of time,\" why doesn't that also happen for TAI and therefore cause it to be the boring slow progress by google? Why would this be like a 50% probability for TAI but &lt;10% for translation?</p><p>perhaps there is a disagreement about how good the boring progress will be by 2024? looks to me like it will be very good</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:57]</strong>&nbsp;</p><p>I am not sure that is where the disagreement lies</p></td></tr></tbody></table>\n\n17.3. The evolution of human intelligence\n-----------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:57]</strong>&nbsp;</p><p>I am considering advocating that we should have more disagreements about the past, which has the advantage of being very concrete, and being often checkable in further detail than either of us already know</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][11:58]</strong>&nbsp;</p><p>I'm fine with disagreements about the past; I'm more scared of letting you pick arbitrary things to \"predict\" since there is much more impact from differences in domain knowledge</p><p>(also not quite sure why it's more concrete, I guess because we can talk about what led to particular events? mostly it just seems faster)</p><p>also as far as I can tell our main differences are about whether people will <s>spend a lot of money</s> work effectively on things that would make a lot of money, which means if we look to the past we will have to move away from ML/AI</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:00]</strong>&nbsp;</p><p>so my understanding of how Paul writes off the example of human intelligence, is that you are like, \"evolution is much stupider than a human investor; if there'd been humans running the genomes, people would be copying all the successful things, and hominid brains would be developing in this ecology of competitors instead of being a lone artifact\". ?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:00]</strong>&nbsp;</p><p>I don't understand why I have to write off the example of human intelligence</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:00]</strong>&nbsp;</p><p>because it looks nothing like your account of how TAI develops</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:00]</strong>&nbsp;</p><p>it also looks nothing like your account, I understand that you have some analogy that makes sense to you</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:01]</strong>&nbsp;</p><p>I mean, to be clear, I also write off the example of humans developing morality and have to explain to people at length why humans being as nice as they are, doesn't imply that paperclip maximizers will be anywhere near that nice, nor that AIs will be other than paperclip maximizers.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:01][12:02]</strong>&nbsp;</p><p>you could state some property of how human intelligence developed, that is in common with your model for TAI and not mine, and then we could discuss that</p><p>if you say something like: \"chimps are not very good at doing science, but humans are\" then yes my answer will be that it's because evolution was not selecting us to be good at science</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\">and indeed AI systems will be good at science using <i>much</i> less resources than humans or chimps</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:02][12:02]</strong>&nbsp;</p><p>would you disagree that humans developing intelligence, on the sheer surfaces of things, looks much more Yudkowskian than Paulian?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p>like, not in terms of compatibility with underlying model</p><p>just that there's this one corporation that came out and massively won the entire AGI race with zero competitors</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:03]</strong>&nbsp;</p><p>I agree that \"how much did the winner take all\" is more like your model of TAI than mine</p><p>I don't think zero competitors is reasonable, I would say \"competitors who were tens of millions of years behind\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:03]</strong>&nbsp;</p><p>sure</p><p>and your account of this is that natural selection is nothing like human corporate managers copying each other</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:03]</strong>&nbsp;</p><p>which was a reasonable timescale for the old game, but a long timescale for the new game</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:03]</strong>&nbsp;</p><p>yup</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:04]</strong>&nbsp;</p><p>that's not my only account</p><p>it's also that for human corporations you can form large coalitions, i.e. raise huge amounts of $ and hire huge numbers of people working on similar projects (whether or not vertically integrated), and those large coalitions will systematically beat small coalitions</p><p>and that's basically <i>the</i> key dynamic in this situation, and isn't even trying to have any analog in the historical situation</p><p>(the key dynamic w.r.t. concentration of power, not necessarily the main thing overall)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:07]</strong>&nbsp;</p><p>the modern degree of concentration of power seems relatively recent and to have tons and tons to do with the regulatory environment rather than underlying properties of the innovation landscape</p><p>back in the old days, small startups would be better than Microsoft at things, and Microsoft would try to crush them using other forces than superior technology, not always successfully</p><p>or such was the common wisdom of USENET</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:08]</strong>&nbsp;</p><p>my point is that the evolution analogy is extremely unpersuasive w.r.t. concentration of power</p><p>I think that AI software capturing the amount of power you imagine is also kind of implausible because we know something about how hardware trades off against software progress (maybe like 1 year of progress = 2x hardware) and so even if you can't form coalitions on innovation <i>at all</i> you are still going to be using tons of hardware if you want to be in the running</p><p>though if you can't parallelize innovation at all and there is enough dispersion in software progress then the people making the software could take a lot of the $ / influence from the partnership</p><p>anyway, I agree that this is a way in which evolution is more like your world than mine</p><p>but think on this point the analogy is pretty unpersuasive</p><p>because it fails to engage with any of the a priori reasons you wouldn't expect concentration of power</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:11]</strong>&nbsp;</p><p>I'm not sure this is the correct point on which to engage, but I feel like I should say out loud that I am unable to operate my model of your model in such fashion that it is not falsified by how the software industry behaved between 1980 and 2000.</p><p>there should've been no small teams that beat big corporations</p><p>today those are much rarer, but on my model, that's because of regulatory changes (and possibly metabolic damage from something in the drinking water)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:12]</strong>&nbsp;</p><p>I understand that you can't operate my model, and I've mostly given up, and on this point I would prefer to just make predictions or maybe retrodictions</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:13]</strong>&nbsp;</p><p>well, anyways, my model of how human intelligence happened looks like this:</p><p>there is a mysterious kind of product which we can call G, and which brains can operate as factories to produce</p><p>G in turn can produce other stuff, but you need quite a lot of it piled up to produce <i>better</i> stuff than your competitors</p><p>as late as 1000 years ago, the fastest creatures on Earth are not humans, because you need <i>even more G than that</i> to go faster than cheetahs</p><p>(or peregrine falcons)</p><p>the natural selections of various species were fundamentally stupid and blind, incapable of foresight and incapable of copying the successes of other natural selections; but even if they had been as foresightful as a modern manager or investor, they might have made just the same mistake</p><p>before 10,000 years they would be like, \"what's so exciting about these things? they're not the fastest runners.\"</p><p>if there'd been an economy centered around running, you wouldn't invest in deploying a human</p><p>(well, unless you needed a stamina runner, but that's something of a separate issue, let's consider just running races)</p><p>you would invest on improving cheetahs</p><p>because the pile of human G isn't large enough that their G beats a specialized naturally selected cheetah</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:17]</strong>&nbsp;</p><p>how are you improving cheetahs in the analogy?</p><p>you are trying random variants to see what works?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:18]</strong>&nbsp;</p><p>using conventional, well-tested technology like MUSCLES and TENDONS</p><p>trying variants on those</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:18]</strong>&nbsp;</p><p>ok</p><p>and you think that G doesn't help you improve on muscles and tendons?</p><p>until you have a big pile of it?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:18]</strong>&nbsp;</p><p>not as a metaphor but as simple historical fact, that's how it played out</p><p>it takes a whole big pile of G to go faster than a cheetah</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:19]</strong>&nbsp;</p><p>as a matter of fact there is no one investing in making better cheetahs</p><p>so it seems like we're already playing analogy-game</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:19]</strong>&nbsp;</p><p>the natural selection of cheetahs is investing in it</p><p>it's not doing so by copying humans because of fundamental limitations</p><p>however if we replace it with an average human investor, it still doesn't copy humans, why would it</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:19]</strong>&nbsp;</p><p>that's the part that is silly</p><p>or like, it needs more analogy</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:19]</strong>&nbsp;</p><p>how so? &nbsp;humans aren't the fastest.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:19]</strong>&nbsp;</p><p>humans are great at breeding animals</p><p>so if I'm natural selection personified, the thing to explain is why I'm not using some of that G to improve on my selection</p><p>not why I'm not using G to build a car</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:20]</strong>&nbsp;</p><p>I'm... confused</p><p>is this implying that a key aspect of your model is that people are using AI to decide which AI tech to invest in?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:20]</strong>&nbsp;</p><p>no</p><p>I think I just don't understand your analogy</p><p>here in the actual world, some people are trying to make faster robots by tinkering with robot designs</p><p>and then someone somewhere is training their AGI</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:21]</strong>&nbsp;</p><p>what I'm saying is that you can imagine a little cheetah investor going, \"I'd like to copy and imitate some other species's tricks to make my cheetahs faster\" and they're looking enviously at falcons, not at humans</p><p>not until <i>very</i> late in the game</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:21]</strong>&nbsp;</p><p>and the relevant question is whether the pre-AGI thing is helpful for automating the work that humans are doing while they tinker with robot designs</p><p>that seems like the actual world</p><p>and the interesting claim is you saying \"nope, not very\"</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:22]</strong>&nbsp;</p><p>I am again confused. &nbsp;Does it matter to your model whether the pre-AGI thing is helpful for automating \"tinkering with robot designs\" or just profitable machine translation? &nbsp;Either seems like it induces equivalent amounts of investment.</p><p>If anything the latter induces much more investment.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:23]</strong>&nbsp;</p><p>sure, I'm fine using \"tinkering with robot designs\" as a lower bound</p><p>both are fine</p><p>the point is I have no idea what you are talking about in the analogy</p><p>what is analogous to what?</p><p>I thought cheetahs were analogous to faster robots</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:23]</strong>&nbsp;</p><p>faster cheetahs are analogous to more profitable robots</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:23]</strong>&nbsp;</p><p>sure</p><p>so you have some humans working on making more profitable robots, right?</p><p>who are tinkering with the robots, in a way analogous to natural selection tinkering with cheetahs?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:24]</strong>&nbsp;</p><p>I'm suggesting replacing the Natural Selection of Cheetahs with a new optimizer that has the Copy Competitor and Invest In Easily-Predictable Returns feature</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:24]</strong>&nbsp;</p><p>OK, then I don't understand what those are analogous to</p><p>like, what is analogous to the humans who are tinkering with robots, and what is analogous to the humans working on AGI?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:24]</strong>&nbsp;</p><p>and observing that, even this case, the owner of Cheetahs Inc. would not try to copy Humans Inc.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:25]</strong>&nbsp;</p><p>here's the analogy that makes sense to me</p><p>natural selection is working on making faster cheetahs = some humans tinkering away to make more profitable robots</p><p>natural selection is working on making smarter humans = some humans who are tinkering away to make more powerful AGI</p><p>natural selection doesn't try to copy humans because they suck at being fast = robot-makers don't try to copy AGI-makers because the AGIs aren't very profitable robots</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:26]</strong>&nbsp;</p><p>with you so far</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:26]</strong>&nbsp;</p><p>eventually humans build cars once they get smart enough = eventually AGI makes more profitable robots once it gets smart enough</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:26]</strong>&nbsp;</p><p>yup</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:26]</strong>&nbsp;</p><p>great, seems like we're on the same page then</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:26]</strong>&nbsp;</p><p>and by this point it is LATE in the game</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:27]</strong>&nbsp;</p><p>great, with you still</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:27]</strong>&nbsp;</p><p>because the smaller piles of G did not produce profitable robots</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:27]</strong>&nbsp;</p><p>but there's a step here where you appear to go totally off the rails</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:27]</strong>&nbsp;</p><p>or operate profitable robots</p><p>say on</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:27]</strong>&nbsp;</p><p>can we just write out the sequence of AGIs, AGI(1), AGI(2), AGI(3)... in analogy with the sequence of human ancestors H(1), H(2), H(3)...?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:28]</strong>&nbsp;</p><p>Is the last member of the sequence H(n) the one that builds cars and then immediately destroys the world before anything that operates on Cheetah Inc's Owner's scale can react?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:28]</strong>&nbsp;</p><p>sure</p><p>I don't think of it as the last</p><p>but it's the last one that actually arises?</p><p>maybe let's call it the last, H(n)</p><p>great</p><p>and now it seems like you are imagining an analogous story, where AGI(n) takes over the world and maybe incidentally builds some more profitable robots along the way</p><p>(building more profitable robots being easier than taking over the world, but not so much easier that AGI(n-1) could have done it unless we make our version numbers really close together, close enough that deploying AGI(n-1) is stupid)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:31]</strong>&nbsp;</p><p>if this plays out in the analogous way to human intelligence, AGI(n) becomes able to build more profitable robots 1 hour before it becomes able to take over the world; my worldview does not put that as the median estimate, but I do want to observe that this is what happened historically</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:31]</strong>&nbsp;</p><p>sure</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:32]</strong>&nbsp;</p><p>ok, then I think we're still on the same page as written so far</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:32]</strong>&nbsp;</p><p>so the question that's interesting in the real world is which AGI is useful for replacing humans in the design-better-robots task; is it 1 hour before the AGI that takes over the world, or 2 years, or what?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:33]</strong>&nbsp;</p><p>my worldview tends to make a big ol' distinction between \"replace humans in the design-better-robots task\" and \"run as a better robot\", if they're not importantly distinct from your standpoint can we talk about the latter?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:33]</strong>&nbsp;</p><p>they seem importantly distinct</p><p>totally different even</p><p>so I think we're still on the same page</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:34]</strong>&nbsp;</p><p>ok then, \"replacing humans at designing better robots\" sure as heck sounds to Eliezer like the world is about to end or has already ended</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:34]</strong>&nbsp;</p><p>my whole point is that in the evolutionary analogy we are talking about \"run as a better robot\" rather than \"replace humans in the design-better-robots-task\"</p><p>and indeed there is no analog to \"replace humans in the design-better-robots-task\"</p><p>which is where all of the action and disagreement is</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:35][12:36]</strong>&nbsp;</p><p>well, yes, I was exactly trying to talk about when humans start running as better cheetahs</p><p>and how that point is still very late in the game</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">not as late as when humans take over the job of making the thing that makes better cheetahs, aka humans start trying to make AGI, which is basically the fingersnap end of the world from the perspective of Cheetahs Inc.</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:36]</strong>&nbsp;</p><p>OK, but I don't care when humans are better cheetahs---in the real world, when AGIs are better robots. In the real world I care about when AGIs start replacing humans in the design-better-robots-task. I'm game to use evolution as an analogy to help answer <i>that</i> question (where I do agree that it's informative), but want to be clear what's actually at issue.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:37]</strong>&nbsp;</p><p>so, the thing I was trying to work up to, is that my model permits the world to end in a way where AGI doesn't get tons of investment because it has an insufficiently huge pile of G that it could run as a better robot. &nbsp;people are instead investing in the equivalents of cheetahs.</p><p>I don't understand why your model doesn't care when humans are better cheetahs. &nbsp;AGIs running as more profitable robots is what induces the huge investments in AGI that your model requires to produce very close competition. ?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:38]</strong>&nbsp;</p><p>it's a sufficient condition, but it's not the most robust one at all</p><p>like, I happen to think that in the real world AIs actually are going to be incredibly profitable robots, and that's part of my boring view about what AGI looks like</p><p>But the thing that's more robust is that the sub-taking-over-world AI is already really important, and receiving huge amounts of investment, as something that automates the R&amp;D process. And it seems like the best guess given what we know now is that this process starts years before the singularity.</p><p>From my perspective that's where most of the action is. And your views on that question seem related to your views on how e.g. AGI is a fundamentally different ballgame from making better robots (whereas I think the boring view is that they are closely related), but that's more like an upstream question about what you think AGI will look like, most relevant because I think it's going to lead you to make bad short-term predictions about what kinds of technologies will achieve what kinds of goals.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:41]</strong>&nbsp;</p><p>but not all AIs are the same branch of the technology tree. &nbsp;factory robotics are already really important and they are \"AI\" but, on my model, they're currently on the cheetah branch rather than the hominid branch of the tech tree; investments into better factory robotics are not directly investments into improving MuZero, though they may buy chips that MuZero also buys.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:42]</strong>&nbsp;</p><p>Yeah, I think you have a mistaken view of AI progress. But I still disagree with your bottom line even if I adopt (this part of) your view of AI progress.</p><p>Namely, I think that the AGI line is mediocre before it is great, and the mediocre version is spectacularly valuable for accelerating R&amp;D (mostly AGI R&amp;D).</p><p>The way I end up sympathizing with your view is if I adopt both this view about the tech tree, + another equally-silly-seeming view about how close the AGI line is to fooming (or how inefficient the area will remain as we get close to fooming)</p></td></tr></tbody></table>\n\n17.4. Human generality and body manipulation\n--------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:43]</strong>&nbsp;</p><p>so metaphorically, you require that humans be doing Great at Various Things and being Super Profitable way before they develop agriculture; the rise of human intelligence cannot be a case in point of your model because the humans were too uncompetitive at most animal activities for unrealistically long (edit: compared to the AI case)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:44]</strong>&nbsp;</p><p>I don't understand</p><p>Human brains are really great at basically everything as far as I can tell?</p><p>like it's not like other animals are better at manipulating their bodies</p><p>we crush them</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:44]</strong>&nbsp;</p><p>if we've got weapons, yes</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:44]</strong>&nbsp;</p><p>human bodies are also pretty great, but they are not the greatest on every dimension</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:44]</strong>&nbsp;</p><p>wrestling a chimpanzee without weapons is famously ill-advised</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:44]</strong>&nbsp;</p><p>no, I mean everywhere</p><p>chimpanzees are practically the same as humans in the animal kingdom</p><p>they have almost as excellent a brain</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:45]</strong>&nbsp;</p><p>as is attacking an elephant with your bare hands</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:45]</strong>&nbsp;</p><p>that's not because of elephant brains</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:45]</strong>&nbsp;</p><p>well, yes, exactly</p><p>you need a big pile of G before it's profitable</p><p>so big the game is practically over by then</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:45]</strong>&nbsp;</p><p>this seems so confused</p><p>but that's exciting I guess</p><p>like, I'm saying that the brains to automate R&amp;D</p><p>are similar to the brains to be a good factory robot</p><p>analogously, I think the brains that humans use to do R&amp;D</p><p>are similar to the brains we use to manipulate our body absurdly well</p><p>I do not think that our brains make us fast</p><p>they help a tiny bit but not much</p><p>I do not think the physical actuators of the industrial robots will be that similar to the actuators of the robots that do R&amp;D</p><p>the claim is that the problem of building the brain is pretty similar</p><p>just as the problem of building a brain that can do science is pretty similar to the problem of building a brain that can operate a body really well</p><p>(and indeed I'm claiming that human bodies kick ass relative to other animal bodies---there may be particular tasks other animal brains are pre-built to be great at, but (i) humans would be great at those too if we were under mild evolutionary pressure with our otherwise excellent brains, (ii) there are lots of more general tests of how good you are at operating a body and we will crush it at those tests)</p><p>(and that's not something I know much about, so I could update as I learned more about how actually we just aren't that good at motor control or motion planning)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:49]</strong>&nbsp;</p><p>so on your model, we can introduce humans to a continent, forbid them any tool use, and they'll still wipe out all the large animals?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:49]</strong>&nbsp;</p><p>(but damn we seem good to me)</p><p>I don't understand why that would even plausibly follow</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:49]</strong>&nbsp;</p><p>because brains are profitable early, even if they can't build weapons?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:49]</strong>&nbsp;</p><p>I'm saying that if you put our brains in a big animal body</p><p>we would wipe out the big animals</p><p>yes, I think brains are great</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:50]</strong>&nbsp;</p><p>because we'd still have our late-game pile of G and we would build weapons</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:50]</strong>&nbsp;</p><p>no, I think a human in a big animal body, with brain adapted to operate that body instead of our own, would beat a big animal straightforwardly</p><p>without using tools</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:51]</strong>&nbsp;</p><p>this is a strange viewpoint and I do wonder whether it is a crux of your view</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:51]</strong>&nbsp;</p><p>this feels to me like it's more on the \"eliezer vs paul disagreement about the nature of AI\" rather than \"eliezer vs paul on civilizational inadequacy and continuity\", but enough changes on \"nature of AI\" would switch my view on the other question</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:51]</strong>&nbsp;</p><p>like, ceteris paribus maybe a human in an elephant's body beats an elephant after a burn-in practice period? &nbsp;because we'd have a strict intelligence advantage?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:52]</strong>&nbsp;</p><p>practice may or may not be enough</p><p>but if you port over the excellent human brain to the elephant body, then run evolution for a brief burn-in period to get all the kinks sorted out?</p><p>elephants are pretty close to humans so it's less brutal than for some other animals (and also are elephants the best example w.r.t. the possibility of direct conflict?) but I totally expect us to win</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][12:53]</strong>&nbsp;</p><p>I unfortunately need to go do other things in advance of an upcoming call, but I feel like disagreeing about the past is proving noticeably more interesting, confusing, and perhaps productive, than disagreeing about the future</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][12:53]</strong>&nbsp;</p><p>actually probably I just think practice is enough</p><p>I think humans have way more dexterity, better locomotion, better navigation, better motion planning...</p><p>some of that is having bodies optimized for those things (esp. dexterity), but I also think most animals just don't have the brains for it, with elephants being one of the closest calls</p><p>I'm a little bit scared of talking to zoologists or whoever the relevant experts are on this question, because I've talked to bird people a little bit and they often have very strong \"humans aren't special, animals are super cool\" instincts even in cases where that take is totally and obviously insane. But if we found someone reasonable in that area I'd be interested to get their take on this.</p><p>I think this is pretty important for the particular claim \"Is AGI like other kinds of ML?\"; that definitely doesn't persuade me to be into fast takeoff on its own though it would be a clear way the world is more Eliezer-like than Paul-like</p><p>I think I do further predict that people who know things about animal intelligence, and don't seem to have identifiably crazy views about any adjacent questions that indicate a weird pro-animal bias, will say that human brains are a lot better than other animal brains for dexterity/locomotion/similar physical tasks (and that the comparison isn't that close for e.g. comparing humans vs big cats).</p><p>Incidentally, seems like DM folks did the same thing this year, presumably publishing now because they got scooped. Looks like they probably have a better algorithm but used harder environments instead of Atari. (They also evaluate the algorithm SPR+MuZero I mentioned which indeed gets one factor of 2x improvement over MuZero alone, roughly as you'd guess): <a href=\"https://arxiv.org/pdf/2111.01587.pdf\">https://arxiv.org/pdf/2111.01587.pdf</a></p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Barnes][13:45]</strong>&nbsp;</p><p>My DM friend says they tried it before they were focused on data efficiency and it didn't help in that regime, sounds like they ignored it for a while after that</p><figure class=\"table\"><table><tbody><tr><td>[Christiano: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][13:48]</strong>&nbsp;</p><p>Overall the situation feels really boring to me. Not sure if DM having a highly similar unpublished result is more likely on my view than Eliezer's (and initially ignoring the method because they weren't focused on sample-efficiency), but at any rate I think it's not anywhere close to falsifying my view.</p></td></tr></tbody></table>\n\n18\\. Follow-ups to the Christiano/Yudkowsky conversation\n========================================================\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Karnofsky][9:39]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>Going to share a point of confusion about this latest exchange.</p><p>It started with Eliezer saying this:</p><blockquote><p>Thing that (if true) strikes me as... straight-up falsifying Paul's view as applied to modern-day AI, at the frontier of the most AGI-ish part of it and where Deepmind put in substantial effort on their project? EfficientZero (allegedly) learns Atari in 100,000 frames. Caveat: I'm not having an easy time figuring out how many frames MuZero would've required to achieve the same performance level. MuZero was trained on 200,000,000 frames but reached what looks like an allegedly higher high; the EfficientZero paper compares their performance to MuZero on 100,000 frames, and claims theirs is much better than MuZero given only that many frames.</p></blockquote><p>So at this point, I thought Eliezer's view was something like: \"EfficientZero represents a several-OM (or at least one-OM?) jump in efficiency, which should shock the hell out of Paul.\" The upper bound on the improvement is 2000x, so I figured he thought the corrected improvement would be some number of OMs.</p><p>But very shortly afterwards, Eliezer quotes Gwern's guess of a <i>4x</i> improvement, and Paul then said:</p><blockquote><p>Concretely, this is a paper that adds a few techniques to improve over MuZero in a domain that (it appears) wasn't a significant focus of MuZero. I don't know how much it improves but I can believe gwern's estimates of 4x.</p><p>I'd guess MuZero itself is a 2x improvement over the baseline from a year ago, which was maybe a 4x improvement over the algorithm from a year before that. If that's right, then no it's not mindblowing on my view to have 4x progress one year, 2x progress the next, and 4x progress the next.</p></blockquote><p>Eliezer never seemed to push back on this 4x-2x-4x claim.</p><p>What I thought would happen after the 4x estimate and 4x-2x-4x claim: Eliezer would've said \"Hmm, we should nail down whether we are talking about 4x-2x-4x or something more like 4x-2x-100x. If it's 4x-2x-4x, then I'll say 'never mind' re: my comment that this 'straight-up falsifies Paul's view.' At best this is just an iota of evidence or something.\"</p><p>Why isn't that what happened? Did Eliezer mean all along to be saying that a 4x jump on Atari sample efficiency would \"straight-up falsify Paul's view?\" Is a 4x jump the kind of thing Eliezer thinks is going to power a jumpy AI timeline?</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td><td>[Shah: ➕]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][11:16]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>This is a proper confusion and probably my fault; I also initially thought it was supposed to be 1-2 OOM and should've made it clearer that Gwern's 4x estimate was less of a direct falsification.</p><p>I'm not yet confident Gwern's estimate is correct. &nbsp;I just got a reply from my query to the paper's first author which reads:</p><blockquote><p>Dear Eliezer: It's a good question. But due to the limits of resources and time, we haven't evaluated the sample efficiency towards different frames systematically. I think it's not a trivial question as the required time and resources are much expensive for the 200M frames setting, especially concerning the MCTS-based methods. Maybe you need about several days or longer to finish a run with GPUs in that setting. I hope my answer can help you. Thank you for your email.</p></blockquote><p>I replied asking if Gwern's 3.8x estimate sounds right to them.</p><p>A 10x improvement could power what I think is a jumpy AI timeline. &nbsp;I'm currently trying to draft a depiction of what I think an unrealistically dignified but computationally typical end-of-world would look like if it started in 2025, and my first draft of that had it starting with a new technique published by Google Brain that was around a 10x improvement in training speeds for very large networks at the cost of higher inference costs, but which turned out to be specially applicable to online learning.</p><p>That said, I think the 10x part isn't either a key concept or particularly likely, and it's much more likely that hell breaks loose when an innovation changes some particular step of the problem from \"can't realistically be done at all\" to \"can be done with a lot of computing power\", which was what I had being the real effect of that hypothetical Google Brain innovation when applied to online learning, and I will probably rewrite to reflect that.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Karnofsky][11:29]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>That's helpful, thanks.</p><p>Re: \"can't realistically be done at all\" to \"can be done with a lot of computing power\", cpl things:</p><p>1. Do you think a 10x improvement in efficiency at some particular task could qualify as this? Could a smaller improvement?</p><p>2. I thought you were pretty into the possibility of a jump from \"can't realistically be done at all\" to \"can be done with a <i>small</i> amount of computing power,\" eg some random ppl with a $1-10mm/y budget blowing past mtpl labs with &gt;$1bb/y budgets. Is that wrong?</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][13:44]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>1 - yes and yes, my revised story for how the world ends looks like Google Brain publishing something that looks like only a 20% improvement but which is done in a way that lets it be adapted to make online learning by gradient descent \"work at all\" in DeepBrain's ongoing Living Zero project (not an actual name afaik)</p><p>2 - that definitely remains very much allowed in principle, but I think it's not my current mainline probability for how the world's end plays out - although I feel hesitant / caught between conflicting heuristics here.</p><p>I think I ended up much too conservative about timelines and early generalization speed because of arguing with Robin Hanson, and don't want to make a similar mistake here, but on the other hand a lot of the current interesting results have been from people spending huge compute (as wasn't the case to nearly the same degree in 2008) and if things happen on short timelines it seems reasonable to guess that the future will look that much like the present. &nbsp;This is very much due to cognitive limitations of the researchers rather than a basic fact about computer science, but cognitive limitations are also facts and often stable ones.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Karnofsky][14:35]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>Hm OK. I don't know what \"online learning by gradient descent\" means such that it doesn't work at all now (does \"work at all\" mean something like \"work with human-ish learning efficiency?\")</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:07]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>I mean, in context, it means \"works for Living Zero at the performance levels where it's running around accumulating knowledge\", which by hypothesis it wasn't until that point.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Karnofsky][15:12]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>Hm. I am feeling pretty fuzzy on whether your story is centrally about:</p><p>1. A &lt;10x jump in efficiency at something important, leading pretty directly/straightforwardly to crazytown</p><p>2. A 100x ish jump in efficiency at something important, which may at first \"look like\" a mere &lt;10x jump in efficiency at something else</p><p>#2 is generally how I've interpreted you and how the above sounds, but under #2 I feel like we should just have consensus that the Atari thing being 4x wouldn't be much of an update. Maybe we already do (it was a bit unclear to me from your msg)</p><p>(And I totally agree that we haven't established the Atari thing is only 4x - what I'm saying is it feels like the conversation should've paused there)</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:13]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>The Atari thing being 4x over 2 years is I think legit not an update because that's standard software improvement speed</p><p>you're correct that it should pause there</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Karnofsky][15:14] &nbsp;(Nov. 5)</strong>&nbsp;</p><p>👍</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky] [15:24]</strong> &nbsp;<strong>(Nov. 5)</strong>&nbsp;</p><p>I think that my central model is something like - there's a central thing to general intelligence that starts working when you get enough pieces together and they coalesce, which is why humans went down this evolutionary gradient by a lot before other species got 10% of the way there in terms of output; and then it takes a big pile of that thing to do big things, which is why humans didn't go faster than cheetahs until extremely late in the game.</p><p>so my visualization of how the world starts to end is \"gear gets added and things start to happen, maybe slowly-by-my-standards at first such that humans keep on pushing it along rather than it being self-moving, but at some point starting to cumulate pretty quickly in the same way that humans cumulated pretty quickly once they got going\" rather than \"dial gets turned up 50%, things happen 50% faster, every year\".</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:16] &nbsp;(Nov. 5, switching channels)</strong>&nbsp;</p><p>as a quick clarification, I agree that if this is 4x sample efficiency over 2 years then that doesn't at all challenge Paul's view</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][0:20]</strong> &nbsp;<strong>(Nov. 26)</strong>&nbsp;</p><p>FWIW, I felt like the entire discussion of EfficientZero was a concrete example of my view making a number of more concentrated predictions than Eliezer that were then almost immediately validated. In particular, consider the following 3 events:</p><ul><li>The quantitative effect size seems like it will turn out to be much smaller than Eliezer initially believed, much closer to being in line with previous progress.</li><li>DeepMind had relatively similar results that got published immediately after our discussion, making it look like random people didn't pull ahead of DM after all.</li><li>DeepMind appears not to have cared much about the metric in question, as evidenced by (i) Beth's comment above, which is basically what I said was probably going on, (ii) they barely even mention Atari sample-efficiency in their paper about similar methods.</li></ul><p>If only 1 of these 3 things had happened, then I agree this would have been a challenge to my view that would make me update in Eliezer's direction. But that's only possible if Eliezer actually assigns a higher probability than me to &lt;= 1 of these things happening, and hence a lower probability to &gt;= 2 of them happening. So if we're playing a reasonable epistemic game, it seems like I need to collect some epistemic credit every time something looks boring to me.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Yudkowsky][15:30]</strong> &nbsp;<strong>(Nov. 26)</strong>&nbsp;</p><p>I broadly agree; you win a Bayes point. &nbsp;I think some of this (but not all!) was due to my tripping over my own feet and sort of rushing back with what looked like a Relevant Thing without contemplating the winner's curse of exciting news, the way that paper authors tend to frame things in more exciting rather than less exciting ways, etc. &nbsp;But even if you set that aside, my underlying AI model said that was a thing which could happen (which is why I didn't have technically rather than sociologically triggered skepticism) and your model said it shouldn't happen, and it currently looks like it mostly didn't happen, so you win a Bayes point.</p><p>Notes that some participants may deem obvious(?) but that I state expecting wider readership:</p><ul><li>Just like markets are almost entirely efficient (in the sense that, even when they're not efficient, you can only make a very small fraction of the money that could be made from the entire market if you owned a time machine), even sharp and jerky progress has to look almost entirely not so fast almost all the time if the Sun isn't right in the middle of going supernova. &nbsp;So the notion that progress sometimes goes jerky and fast does have to be evaluated by a portfolio view over time. &nbsp;In worlds where progress is jerky even before the End Days, Paul wins soft steady Bayes points in most weeks and then I win back more Bayes points once every year or two.</li><li>We still don't have a <i>very</i> good idea of how much longer you would need to train the previous algorithm to match the performance of the new algorithm, just an estimate by Gwern based off linearly extrapolating a graph in a paper. &nbsp;But, also to be clear, not knowing something is not the same as expecting it to update dramatically, and you have to integrate over the distribution you've got.</li><li>It's fair to say, \"Hey, Eliezer, if you tripped over your own feet here, but only noticed that because Paul was around to call it, maybe you're tripping over your feet at other times when Paul isn't around to check your thoughts in detail\" - I don't want to minimize the Bayes point that Paul won either.</li></ul></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);vertical-align:top\"><p><strong>[Christiano][16:29]</strong> &nbsp;<strong>(Nov. 27)</strong>&nbsp;</p><p>Agreed that it's (i) not obvious how large the EfficientZero gain was, and in general it's not a settled question what happened, (ii) it's not that big an update, it needs to be part of a portfolio (but this is indicative of the kind of thing I'd want to put in the portfolio), (iii) it generally seems pro-social to flag potentially relevant stuff without the presumption that you are staking a lot on it.</p></td></tr></tbody></table>",
      "plaintextDescription": "This is a transcript of a conversation between Paul Christiano and Eliezer Yudkowsky, with comments by Rohin Shah, Beth Barnes, Richard Ngo, and Holden Karnofsky, continuing the Late 2021 MIRI Conversations.\n\n\nColor key:\n\n Chat by Paul and Eliezer  Other chat \n\n \n\n\n15. October 19 comment\n \n\n[Yudkowsky][11:01] \n\nthing that struck me as an iota of evidence for Paul over Eliezer: https://twitter.com/tamaybes/status/1450514423823560706?s=20 \n\n \n\n\n16. November 3 conversation\n \n\n\n16.1. EfficientZero\n \n\n[Yudkowsky][9:30] \n\nThing that (if true) strikes me as... straight-up falsifying Paul's view as applied to modern-day AI, at the frontier of the most AGI-ish part of it and where Deepmind put in substantial effort on their project?  EfficientZero (allegedly) learns Atari in 100,000 frames.  Caveat: I'm not having an easy time figuring out how many frames MuZero would've required to achieve the same performance level.  MuZero was trained on 200,000,000 frames but reached what looks like an allegedly higher high; the EfficientZero paper compares their performance to MuZero on 100,000 frames, and claims theirs is much better than MuZero given only that many frames.\n\nhttps://arxiv.org/pdf/2111.00210.pdf  CC: @paulfchristiano.\n\n(I would further argue that this case is important because it's about the central contemporary model for approaching AGI, at least according to Eliezer, rather than any number of random peripheral AI tasks.)\n\n[Shah][14:46]  \n\nI only looked at the front page, so might be misunderstanding, but the front figure says \"Our proposed method EfficientZero is 170% and 180% better than the previous SoTA performance in mean and median human normalized score [...] on the Atari 100k benchmark\", which does not seem like a huge leap?\n\nOh, I incorrectly thought that was 1.7x and 1.8x, but it is actually 2.7x and 2.8x, which is a bigger deal (though still feels not crazy to me)\n\n[Yudkowsky][15:28]  \n\nthe question imo is how many frames the previous SoTA would require to c",
      "wordCount": 12479
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cCrpbZ4qTCEYXbzje",
    "title": "Ngo and Yudkowsky on scientific reasoning and pivotal acts",
    "slug": "ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts",
    "url": null,
    "baseScore": 67,
    "voteCount": 33,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2022-02-21T20:54:53.979Z",
    "contents": {
      "markdown": "This is a transcript of a conversation between Richard Ngo and Eliezer Yudkowsky, facilitated by Nate Soares (and with some comments from Carl Shulman). This transcript continues the [Late 2021 MIRI Conversations](https://intelligence.org/late-2021-miri-conversations/) sequence, following [Ngo's view on alignment difficulty](https://www.lesswrong.com/posts/gf9hhmSvpZfyfS34B/ngo-s-view-on-alignment-difficulty).\n\nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Richard and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td></tr></tbody></table>\n\n14\\. October 4 conversation\n===========================\n\n14.1. Predictable updates, threshold functions, and the human cognitive range\n-----------------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:05]</strong>&nbsp;</p><p>Two questions which I'd like to ask Eliezer:</p><p>1. How strongly does he think that the \"shallow pattern-memorisation\" abilities of GPT-3 are evidence for Paul's view over his view (if at all)</p><p>2. How does he suggest we proceed, given that he thinks directly explaining his model of the chimp-human difference would be the wrong move?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:07]</strong> &nbsp;</p><p>1 - I'd say that it's some evidence for the Dario viewpoint which seems close to the Paul viewpoint. &nbsp;I say it's some evidence for the Dario viewpoint because Dario seems to be the person who made something like an advance prediction about it. &nbsp;It's not enough to make me believe that you can straightforwardly extend the GPT architecture to 3e14 parameters and train it on 1e13 samples and get human-equivalent performance.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:09]</strong> &nbsp;</p><p>Did you make any advance predictions, around the 2008-2015 period, of what capabilities we'd have before AGI?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:10]</strong> &nbsp;</p><p>not especially that come to mind? &nbsp;on my model of the future this is not particularly something I am supposed to know unless there is a rare flash of predictability.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:11]</strong> &nbsp;</p><blockquote><p>1 - I'd say that it's some evidence for the Dario viewpoint which seems close to the Paul viewpoint. I say it's some evidence for the Dario viewpoint because Dario seems to be the person who made something like an advance prediction about it. It's not enough to make me believe that you can straightforwardly extend the GPT architecture to 3e14 parameters and train it on 1e13 samples and get human-equivalent performance.</p></blockquote><p>For the record I remember Paul being optimistic about language when I visited OpenAI in summer 2018. But I don't know how advanced internal work on GPT-2 was by then.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:13]</strong> &nbsp;</p><p>2 - in lots of cases where I learned more specifics about X, and updated about Y, I had the experience of looking back and realizing that knowing <i>anything</i> specific about X would have predictably produced a directional update about Y. &nbsp;like, knowing anything in particular about how the first AGI eats computation, would cause you to update far away from thinking that biological analogies to the computation consumed by humans were a good way to estimate how many computations an AGI needs to eat. &nbsp;you know lots of details about how humans consume watts of energy, and you know lots of details about how modern AI consumes watts, so it's very visible that these quantities are so incredibly different and go through so many different steps that they're basically unanchored from each other.</p><p>I have specific ideas about how you get AGI that isn't just scaling up Stack More Layers, which lead me to think that the way to estimate the computational cost of it is not \"3e14 parameters trained at 1e16 ops per step for 1e13 steps, because that much computation and parameters seems analogous to human biology and 1e13 steps is given by past scaling laws\", a la recent OpenPhil publication. &nbsp;But it seems to me that it should be possible to have the abstract insight that knowing more about general intelligence in AGIs or in humans would make the biological analogy look less plausible, because you wouldn't be matching up an unknown key to an unknown lock.</p><p>Unfortunately I worry that this depends on some life experience with actual discoveries to get something this abstract-sounding on a gut level, because people basically never seem to make abstract updates of this kind when I try to point to them as predictable directional updates?</p><p>But, in principle, I'd hope there would be aspects of this where I could figure out how to show that <i>any</i> knowledge of specifics would probably update you in a predictable direction, even if it doesn't seem best for Earth for me to win that argument by giving specifics conditional on those specifics actually being correct, and it doesn't seem especially sound to win that argument by giving specifics that are wrong.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:17]</strong> &nbsp;</p><p>I'm confused by this argument. Before I thought much about the specifics of the chimpanzee-human transition, I found the argument \"humans foomed (by biological standards) so AIs will too\" fairly compelling. But after thinking more about the specifics, it seems to me that the human foom was in part caused by a factor (sharp cultural shift) that won't be present when we train AIs.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:17]</strong> &nbsp;</p><p>sure, and other factors will be present in AIs but not in humans</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:17]</strong> &nbsp;</p><p>This seems like a case where more specific knowledge updated me away from your position, contrary to what you're claiming.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:18]</strong> &nbsp;</p><p>eg, human brains don't scale and mesh, while it's far more plausible that with AI you could just run more and more of it</p><p>that's a huge factor leading one to expect AI to scale faster than human brains did</p><p>it's like communication between humans, but squared!</p><p>this is admittedly a specific argument and I'm not sure how it would abstract out to any specific argument</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:20]</strong> &nbsp;</p><p>Again, this is an argument that I believed less after looking into the details, because right now it's pretty difficult to throw more compute at neural networks at runtime.</p><p>Which is not to say that it's a bad argument, the differences in compute-scalability between humans and AIs are clearly important. But I'm confused about the structure of your argument that knowing more details will predictably update me in a certain direction.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:21]</strong> &nbsp;</p><p>I suppose the genericized version of my actual response to that would be, \"architectures that have a harder time eating more compute are architectures which, for this very reason, are liable to need better versions invented of them, and this in particular seems like something that plausibly happens before scaling to general intelligence is practically possible\"</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][15:23]</strong> &nbsp;</p><p>(Eliezer, I see Richard as requesting that you either back down from, or clarify, your claim that any specific observations about how much compute AI systems require will update him in a predictable direction.)</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:24]</strong> &nbsp;</p><p>I'm not saying I know how to make that abstractized argument for exactly what Richard cares about, in part because I don't understand Richard's exact model, just that it's one way to proceed past the point where the obvious dilemma crops up of, \"If a theory about AGI capabilities is true, it is a disservice to Earth to speak it, and if a theory about AGI capabilities is false, an argument based on it is not sound.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:25]</strong> &nbsp;</p><p>Ah, I see.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:26]</strong> &nbsp;</p><p>possible viewpoint to try: that systems in general often have threshold functions as well as smooth functions inside them.</p><p>only in ignorance, then, do we imagine that the whole thing is one smooth function.</p><p>the history of humanity has a threshold function of, like, communication or culture or whatever.</p><p>the correct response to this is not, \"ah, so this was the unique, never-to-be-seen-again sort of fact which cropped up in the weirdly complicated story of humanity in particular, which will not appear in the much simpler story of AI\"</p><p>this only sounds plausible because you don't know the story of AI so you think it will be a simple story</p><p>the correct generalization is \"guess some weird thresholds will also pop up in whatever complicated story of AI will appear in the history books\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:28]</strong> &nbsp;</p><p>Here's a quite general argument about why we shouldn't expect too many threshold functions in the impact of AI: because at any point, humans will be filling in the gaps of whatever AIs can't do. (The lack of this type of smoothing is, I claim, why culture was a sharp threshold for humans - if there had been another intelligent species we could have learned culture from, then we would have developed more gradually.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:30]</strong> &nbsp;</p><p>something like this indeed appears in my model of why I expect not much impact on GDP before AGI is powerful enough to bypass human economies entirely</p><p>during the runup phase, pre-AGI won't be powerful to do \"whole new things\" that depend on doing lots of widely different things that humans can't do</p><p>just marginally new things that depend on doing one thing humans can't do, or can do but a bunch worse</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:31]</strong> &nbsp;</p><p>Okay, that's good to know.</p><p>Would this also be true in <a href=\"https://www.lesswrong.com/posts/gf9hhmSvpZfyfS34B/ngo-s-view-on-alignment-difficulty\">a civilisation of village idiots</a>?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:32]</strong>&nbsp;</p><p>there will be sufficient economic reward for building out industries that are mostly human plus one thing that pre-AGI does, and people will pocket those economic rewards, go home, and not be more ambitious than that. &nbsp;I have trouble empathically grasping <i>why</i> almost all the CEOs are like this in our current Earth, because I am very much not like that myself, but observationally, the current Earth sure does seem to behave like rich people would almost uniformly rather not rock the boat too much.</p><p>I did not understand the whole thing about village idiots actually</p><p>do you want to copy and paste the document, or try rephrasing the argument?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:35]</strong> &nbsp;</p><p>Rephrasing:</p><p>Claim 1: AIs will be better at doing scientific research (and other similar tasks) than village idiots, before we reach AGI.</p><p>Claim 2: Village idiots still have the core of general intelligence (which you claim chimpanzees don't have).</p><p>Claim 3: It would be surprising if narrow AI's research capabilities fell specifically into the narrow gap between village idiots and Einsteins, given that they're both general intelligences and are very similar in terms of architecture, algorithms, etc.</p><p>(If you deny claim 2, then we can substitute, say, someone at the 10th percentile of human intelligence - I don't know what specific connotations \"village idiot\" has to you.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:37]</strong> &nbsp;</p><p>My models do not have an easy time of visualizing \"as generally intelligent as a chimp, but specialized to science research, gives you superhuman scientific capability and the ability to make progress in novel areas of science\".</p><p>(this is a reference back to the pre-rephrase in the document)</p><p>it seems like, I dunno, \"gradient descent can make you generically good at anything without that taking too much general intelligence\" must be a core hypothesis there?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:39]</strong> &nbsp;</p><p>I mean, we both agree that gradient descent can produce <i>some</i> capabilities without also producing much general intelligence. But claim 1 plus your earlier claims that narrow AIs won't surpass humans at scientific research, lead to the implication that the limitations of gradient-descent-without-much-general-intelligence fall in a weirdly narrow range.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:42]</strong> &nbsp;</p><p>I do credit the Village Idiot to Einstein Interval with being a little broader as a target than I used to think, since the Alpha series of Go-players took a couple of years to go from pro to world-beating even once they had a scalable algorithm. &nbsp;Still seems to me that, over time, the wall clock time to traverse those ranges has been getting shorter, like Go taking less time than Chess. &nbsp;My intuitions still say that it'd be quite weird to end up hanging out for a long time with AGIs that conduct humanlike conversations and are ambitious enough to run their own corporations while those AGIs are still not much good at science.</p><p>But on my present model, I suspect the limitations of \"gradient-descent-without-much-general-intelligence\" to fall underneath the village idiot side?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:43]</strong> &nbsp;</p><p>Oh, interesting.</p><p>That seems like a strong prediction</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:43]</strong> &nbsp;</p><p>Your model, as I understand it, is saying, \"But surely, GD-without-GI must suffice to produce better scientists than village idiots, by specializing chimps on science\" and my current reply, though it's not a particular question I've thought a lot about before, is, \"That... does not quite seem to me like a thing that should happen along the mainline?\"</p><p>though, as always, in the limit of superintelligences doing things, or our having the Textbook From The Future, we could build almost any kind of mind on purpose if we knew how, etc.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:44]</strong> &nbsp;</p><p>For example, I expect that if I prompt GPT-3 in the right way, it'll say some interesting and not-totally-nonsensical claims about advanced science.</p><p>Whereas it would be very hard to prompt a village idiot to do the same.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:44]</strong> &nbsp;</p><p>eg, a superintelligence could load up chimps with lots of domain-specific knowledge they were not generally intelligent enough to learn themselves.</p><p>ehhhhhh, it is <i>not</i> clear to me that GPT-3 is better than a village idiot at advanced science, even in this narrow sense, especially if the village idiot is allowed some training</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:46]</strong> &nbsp;</p><p>It's not clear to me either. But it does seem plausible, and then it seems even more plausible that this will be true of GPT-4</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:46]</strong>&nbsp;</p><p>I wonder if we're visualizing different village idiots</p><p>my choice of \"village idiot\" originally was probably not the best target for visualization, because in a lot of cases, a village idiot - especially the stereotype of a village idiot - is, like, a damaged general intelligence with particular gears missing?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:47]</strong> &nbsp;</p><p>I'd be happy with \"10th percentile intelligence\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:47]</strong> &nbsp;</p><p>whereas it seems like what you want is something more like \"Homo erectus but it has language\"</p><p>oh, wow, 10th percentile intelligence?</p><p>that's super high</p><p>GPT-3 is far far out of its league</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:49]</strong>&nbsp;</p><p>I think GPT-3 is far below this person's league in a lot of ways (including most common-sense reasoning) but I become much less confident when we're talking about abstract scientific reasoning.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:51]</strong> &nbsp;</p><p>I think that if scientific reasoning were as easy as you seem to be imagining(?), the publication factories of the modern world would be <i>much</i> more productive of real progress.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:51]</strong> &nbsp;</p><p>Well, a 10th percentile human is very unlikely to contribute to real scientific progress either way</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:53]</strong> &nbsp;</p><p>Like, on my current model of how the world really works, China pours vast investments into universities and sober-looking people with PhDs and classes and tests and postdocs and journals and papers; but none of this is the real way of Science which is actually, secretly, unbeknownst to China, passed down in rare lineages and apprenticeships from real scientist mentor to real scientist student, and China doesn't have much in the way of lineages so the extra money they throw at stuff doesn't turn into real science.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:52]</strong> &nbsp;</p><p>Can you think of any clear-cut things that they could do and GPT-3 can't?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:53]</strong> &nbsp;</p><p>Like... make sense... at all? &nbsp;Invent a handaxe when nobody had ever seen a handaxe before?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:54]</strong> &nbsp;</p><p>You're claiming that 10th percentile humans invent handaxes?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:55]</strong> &nbsp;</p><p>The activity of rearranging scientific sentences into new plausible-sounding paragraphs is well within the reach of publication factories, in fact, they often use considerably more semantic sophistication than that, and yet, this does not cumulate into real scientific progress even in quite large amounts.</p><p>I think GPT-3 is basically just Not Science Yet to a much greater extent than even these empty publication factories.</p><p>If 10th percentile humans don't invent handaxes, GPT-3 sure as hell doesn't.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][15:55]</strong> &nbsp;</p><p>I don't think we're disagreeing. Publication factories are staffed with people who do better academically than 90+% of all humans.</p><p>If 90th-percentile humans are very bad at science, then of course GPT-3 and 10th-percentile humans are very very bad at science. But it still seems instructive to compare them (e.g. on tasks like \"talk cogently about a complex abstract topic\")</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:58]</strong> &nbsp;</p><p>I mean, while it is usually weird for something to be barely within a species's capabilities while being within those capabilities at all, such that only relatively smarter individual organisms can do it, in the case of something that a social species has only very recently started to do collectively, it's plausible that the thing appeared at the point where it was barely accessible to the smartest members. &nbsp;Eg, it wouldn't be surprising if it would have taken a long time or forever for humanity to invent science from scratch, if all the Francis Bacons and Newtons and even average-intelligence people were eliminated leaving only the bottom 10%. &nbsp;Because our species just started doing that, at the point where our species was barely able to start doing that, meaning, at the point where some rare smart people could spearhead it, historically speaking. &nbsp;It's not obvious whether or not less smart people can do it over a longer time.</p><p>I'm not sure we disagree much about the human part of this model.</p><p>My guess is that our disagreement is more about GPT-3.</p><p>\"Talk 'cogently' about a complex abstract topic\" doesn't seem like much of anything significant to me, if GPT-3 is 'cogent'. &nbsp;It fails to pass the threshold for inventing science and, I expect, for most particular sciences.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:00]</strong> &nbsp;</p><p>How much training do you think a 10th-percentile human would need in a given subject matter (say, economics) before they could answer questions as well as GPT-3 can?</p><p>(Right now I think GPT-3 does better by default because it at least recognises the terminology, whereas most humans don't at all.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:01]</strong> &nbsp;</p><p>I also expect that if you offer a 10th-percentile human lots of money, they can learn to talk more cogently than GPT-3 about narrower science areas. &nbsp;GPT-3 is legitimately more well-read at its lower level of intelligence, but train the 10-percentiler in a narrow area and they will become able to write better nonsense about that narrow area.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:01]</strong> &nbsp;</p><p>This sounds like an experiment we can actually run.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:02]</strong> &nbsp;</p><p>Like, what we've got going on here is a real <i>breadth</i> advantage that GPT-3 has in some areas, but the breadth doesn't add up because it lacks the depth of a 10%er.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:02]</strong> &nbsp;</p><p>If we asked them to read a single introductory textbook and then quiz both them and GPT-3 about items covered in that textbook, do you expect that the human would come out ahead?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:02]</strong>&nbsp;</p><p>AI has figured out how to do a subhumanly shallow kind of thinking, and it <i>is</i> to be expected that when AI can do anything at all, it can soon do more of that thing than the whole human species could do.</p><p>No, that's nothing remotely like giving the human the brief training the human needs to catch up to GPT-3's longer training.</p><p>A 10%er does not learn in an instant - they learn faster than GPT-3, but not in an instant.</p><p>This is more like a scenario of paying somebody to, like, sit around for a year with an editor, learning how to mix-and-match economics sentences until they can learn to sound more like they're making an argument than GPT-3 does, despite still not understanding any economics.</p><p>A lot of the learning would just go into producing sensible-sounding nonsense at all, since lots of 10%ers have not been to college and have not learned how to regurgitate rearranged nonsense for college teachers.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:05]</strong>&nbsp;</p><p>What percentage of humans do you think could learn to beat GPT-3's question-answering by reading a single textbook over, say, a period of a month?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:06]</strong> &nbsp;</p><p>¯\\_(ツ)_/¯</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:06]</strong> &nbsp;</p><p>More like 0.5 or 5 or 50?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:06]</strong> &nbsp;</p><p>Humans cannot in general pass the Turing Test for posing as AIs!</p><p>What percentage of humans can pass as a calculator by reading an arithmetic textbook?</p><p>Zero!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:07]</strong> &nbsp;</p><p>I'm not asking them to mimic GPT-3, I'm asking them to produce better answers.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:07]</strong> &nbsp;</p><p>Then it depends on what kind of answers!</p><p>I think a lot of 10%ers could learn to do wedding-cake multiplication, if sufficiently well-paid as adults rather than being tortured in school, out to 6 digits, thus handily beating the current GPT-3 at 'multiplication'.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:08]</strong> &nbsp;</p><p>For example: give them an economics textbook to study for a month, then ask them what inflation is, whether it goes up or down if the government prints more money, whether the price of something increases or decreases when the supply increases.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:09]</strong>&nbsp;</p><p>GPT-3 did not learn to produce its responses by reading <i>textbooks</i>.</p><p>You're not matching the human's data to GPT-3's data.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:10]</strong> &nbsp;</p><p>I know, this is just the closest I can get in an experiment that seems remotely plausible to actually run.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:10]</strong> &nbsp;</p><p>You would want to collect, like, 1,000 Reddit arguments about inflation, and have the human read that, and have the human produce their own Reddit arguments, and have somebody tell them whether they sounded like real Reddit arguments or not.</p><p>The textbook is just not the same thing at all.</p><p>I'm not sure we're at the core of the argument, though.</p><p>To me it seems like GPT-3 is allowed to be superhuman at producing remixed and regurgitated sentences about economics, because this is about as relevant to Science talent as a calculator being able to do perfect arithmetic, only less so.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:15]</strong> &nbsp;</p><p>Suppose that the remixed and regurgitated sentences slowly get more and more coherent, until GPT-N can debate with a professor of economics and sustain a reasonable position.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:15]</strong> &nbsp;</p><p>Are these points that GPT-N read elsewhere on the Internet, or are they new good points that no professor of economics on Earth has ever made before?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:15]</strong> &nbsp;</p><p>I guess you don't expect this to happen, but I'm trying to think about what experiments we could run to get evidence for or against it.</p><p>The latter seems both very hard to verify, and also like a very high bar - I'm not sure if most professors of economics have generated new good arguments that no other professor has ever made before.</p><p>So I guess the former.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:18]</strong> &nbsp;</p><p>Then I think that you can do this without being able to do science. &nbsp;It's a lot like if somebody with a really good memory was lucky enough to have read that exact argument on the Internet yesterday, and to have a little talent for paraphrasing. &nbsp;Not by coincidence, having this ability gives you - on my model - no ability to do science, invent science, be the first to build handaxes, or design nanotechnology.</p><p>I admit, this does reflect my personal model of how Science works, presumably not shared by many leading bureaucrats, where in fact the papers full of regurgitated scientific-sounding sentences are not accomplishing much.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:20]</strong> &nbsp;</p><p>So it seems like your model doesn't rule out narrow AIs producing well-reviewed scientific papers, since you don't trust the review system very much.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:23]</strong> &nbsp;</p><p>I'm trying to remember whether or not I've heard of that happening, like, 10 years ago.</p><p>My vague recollection is that things in the Sokal Hoax genre where the submissions succeeded, used humans to hand-generate the nonsense rather than any submissions in the genre having been purely machine-generated.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:24]</strong> &nbsp;</p><p>Which doesn't seem like an unreasonable position, but it does make it harder to produce tests that we have opposing predictions on.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:24]</strong> &nbsp;</p><p>Obviously, that doesn't mean it couldn't have been done 10 years ago, because 10 years ago it's plausibly a lot easier to hand-generate passing nonsense than to write an AI program that does it.</p><p>oh, wait, I'm wrong!</p><p><a href=\"https://news.mit.edu/2015/how-three-mit-students-fooled-scientific-journals-0414\">https://news.mit.edu/2015/how-three-mit-students-fooled-scientific-journals-0414</a></p><blockquote><p>In April of 2005 the team’s submission, “Rooter: A Methodology for the Typical Unification of Access Points and Redundancy,” was accepted as a non-reviewed paper to the World Multiconference on Systemics, Cybernetics and Informatics (WMSCI), a conference that Krohn says is known for “being spammy and having loose standards.”</p></blockquote><p>&nbsp;</p><blockquote><p>in 2013 IEEE and Springer Publishing removed more than 120 papers from their sites after a French researcher’s analysis determined that they were generated via SCIgen</p></blockquote></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:26]</strong> &nbsp;</p><p>Oh, interesting</p><p>Meta note: I'm not sure where to take the direction of the conversation at this point. Shall we take a brief break?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:27]</strong> &nbsp;</p><blockquote><p>The creators continue to get regular emails from computer science students proudly linking to papers they’ve snuck into conferences, as well as notes from researchers urging them to make versions for other disciplines.</p></blockquote><p>Sure! Resume 5p?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][16:27]</strong> &nbsp;</p><p>Yepp</p></td></tr></tbody></table>\n\n14.2. Domain-specific heuristics and nanotechnology\n---------------------------------------------------\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][16:41]</strong>&nbsp;</p><p>A few takes:</p><p>1. It looks to me like there's some crux in \"how useful will the 'shallow' stuff get before dangerous things happen\". I would be unsurprised if this spiraled back into the gradualness debate. I'm excited about attempts to get specific and narrow disagreements in this domain (not necessarily bettable; I nominate distilling out specific disagreements before worrying about finding bettable ones).</p><p>2. It seems plausible to me we should have some much more concrete discussion about possible ways things could go right, according to Richard. I'd be up for playin the role of beeping when things seem insufficiently concrete.</p><p>3. It seems to me like Richard learned a couple things about Eliezer's model in that last bout of conversation. I'd be interested to see him try to paraphrase his current understanding of it, and to see Eliezer produce beeps where it seems particularly off.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:00]</strong> &nbsp;</p><p>👋</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:02]</strong> &nbsp;</p><p>Hmm, I'm not sure that I learned too much about Eliezer's model in this last round.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][17:03]</strong> &nbsp;</p><p>(dang :-p)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:03]</strong> &nbsp;</p><p>It seems like Eliezer thinks that the returns of scientific investigation are very heavy-tailed.</p><p>Which does seem pretty plausible to me.</p><p>But I'm not sure how useful this claim is for thinking about the development of AI that can do science.</p><p>I attempted in my document to describe some interventions that would help things go right.</p><p>And the levels of difficulty involved.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:07]</strong> &nbsp;</p><p>(My model is something like: there are some very shallow steps involved in doing science, lots of medium steps, occasional very deep steps, assembling the whole thing into Science requires having all the lego blocks available. &nbsp;As soon as you look at anything with details, it ends up 'heavy-tailed' because it has multiple pieces and says how things don't work if all the pieces aren't there.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:08]</strong> &nbsp;</p><p>Eliezer, do you have an estimate of how much slower science would proceed if everyone's IQs were shifted down by, say, 30 points?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:10]</strong> &nbsp;</p><p>It's not obvious to me that science proceeds significantly past its present point. &nbsp;I would not have the right to be surprised if Reality told me the correct answer was that a civilization like that just doesn't reach AGI, ever.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:12]</strong> &nbsp;</p><p>Doesn't your model take a fairly big hit from predicting that humans just happen to be within 30 IQ points of not being able to get any more science?</p><p>It seems like a surprising coincidence.</p><p>Or is this dependent on the idea that doing science is much harder now than it used to be?</p><p>And so if we'd been dumber, we might have gotten stuck before newtonian mechanics, or else before relativity?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:13]</strong> &nbsp;</p><p>No, humanity is exactly the species that finds it barely possible to do science.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:14]</strong> &nbsp;</p><p>It seems to me like humanity is exactly the species that finds it barely possible to do <i>civilisation</i>.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:14]</strong> &nbsp;</p><p>If it were possible to do it with less intelligence, we'd be having this conversation over the Internet that we'd developed with less intelligence.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:15]</strong> &nbsp;</p><p>And it seems like many of the key inventions that enabled civilisation weren't anywhere near as intelligence-bottlenecked as modern science.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:15]</strong> &nbsp;</p><p>Yes, it does seem that there's quite a narrow band between \"barely smart enough to develop agriculture\" and \"barely smart enough to develop computers\"! Though there were genuinely fewer people in the preagricultural world, with worse nutrition and no Ashkenazic Jews, and there's the whole question about to what degree the reproduction of the shopkeeper class over several centuries was important to the Industrial Revolution getting started.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:15]</strong> &nbsp;</p><p>(e.g. you'd get better spears or better plows or whatever just by tinkering, whereas you'd never get relativity just by tinkering)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:17]</strong> &nbsp;</p><p>I model you as taking a lesson from this which is something like... you can train up a villager to be John von Neumann by spending some evolutionary money on giving them science-specific brain features, since John von Neumann couldn't have been much more deeply or generally intelligent, and you could spend even more money and make a chimp a better scientist than John von Neumann.</p><p>My model is more like, yup, the capabilities you need to invent aqueducts sure do generalize the crap out of things, though also at the upper end of cognition there are compounding returns which can bring John von Neumann into existence, and also also there's various papers suggesting that selection was happening really fast over the last few millennia and real shifts in cognition shouldn't be ruled out. &nbsp;(This last part is an update to what I was thinking when I wrote <a href=\"https://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>, and is from my own perspective a more gradualist line of thinking, because it means there's a wider actual target to traverse before you get to von Neumann.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:20]</strong> &nbsp;</p><p>It's not that \"von Neumann isn't much more deeply generally intelligent\", it's more like \"domain-specific heuristics and instincts get you a long way\". E.g. soccer is a domain where spending evolutionary money on specific features will very much help you beat von Neumann, and so is art, and so is music.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:20]</strong> &nbsp;</p><p>My skepticism here is that there's a version of, like, \"invent nanotechnology\" which routes through just the shallow places, which humanity stumbles over before we stumble over deep AGI.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:21]</strong> &nbsp;</p><p>Would you be comfortable publicly discussing the actual cognitive steps which you think would be necessary for inventing nanotechnology?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:23]</strong> &nbsp;</p><p>It should not be overlooked that there's a very valid sibling of the old complaint \"Anything you can do ceases to be AI\", which is that \"Things you can do with surprisingly-to-your-model shallow cognition are precisely the things that Reality surprises you by telling you that AI can do earlier than you expected.\" &nbsp;When we see GPT-3, we were getting some amount of real evidence about AI capabilities advancing faster than I expected, and some amount of evidence about GPT-3's task being performable using shallower cognition than expected.</p><p>Many people were particularly surprised by Go because they thought that Go was going to require deeper real thought than chess.</p><p>And I think AlphaGo probably was thinking in a legitimately deeper way than Deep Blue. &nbsp;Just not as much deeper as Douglas Hofstadter thought it would take.</p><p>Conversely, people thought a few years ago that driving cars really seemed to be the sort of thing that machine learning would be good at, and were unpleasantly surprised by how the last 0.1% of driving conditions were resistant to shallow techniques.</p><p>Despite the inevitable fact that some surprises of this kind now exist, and that more such surprises will exist in the future, it continues to seem to me that science-and-engineering on the level of \"invent nanotech\" still seems pretty unlikely to be easy to do with shallow thought, by means that humanity discovers before AGI tech manages to learn deep thought?</p><p>What actual cognitive steps? &nbsp;Outside-the-box thinking, throwing away generalizations that governed your previous answers and even your previous questions, inventing new ways to represent your questions, figuring out which questions you need to ask and developing plans to answer them; these are some answers that I hope will be sufficiently useless to AI developers that it is safe to give them, while still pointing in the direction of things that have an un-GPT-3-like quality of depth about them.</p><p>Doing this across unfamiliar domains that couldn't be directly trained in by gradient descent because they were too expensive to simulate a billion examples of</p><p>If you have something this powerful, why is it not also noticing that the world contains humans? &nbsp;Why is it not noticing itself?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:30]</strong> &nbsp;</p><p>If humans were to invent this type of nanotech, what do you expect the end intellectual result to be?</p><p>E.g. consider the human knowledge involved in building cars</p><p>There are thousands of individual parts, each of which does a specific thing</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:30]</strong> &nbsp;</p><p>Uhhhh... is there a reason why \"Eric Drexler's <i>Nanosystems</i> but, like, the real thing, modulo however much Drexler did not successfully Predict the Future about how to do that, which was probably a lot\" is not the obvious answer here?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:31]</strong> &nbsp;</p><p>And some deep principles governing engines, but not really very crucial ones to actually building (early versions of) those engines</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:31]</strong> &nbsp;</p><p>that's... not historically true at all?</p><p>getting a grip on quantities of heat and their flow was <i>critical</i> to getting steam engines to work</p><p>it didn't happen until the math was there</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:32]</strong> &nbsp;</p><p>Ah, interesting</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:32]</strong> &nbsp;</p><p>maybe you can be a mechanic banging on an engine that somebody else designed, around principles that somebody even earlier invented, without a physics degree</p><p>but, like, engineers have actually needed math since, like, that's been a thing, it wasn't just a prestige trick</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:34]</strong> &nbsp;</p><p>Okay, so you expect there to be a bunch of conceptual work in finding equations which govern nanosystems.</p><blockquote><p>Uhhhh... is there a reason why \"Eric Drexler's <i>Nanosystems</i> but, like, the real thing, modulo however much Drexler did not successfully Predict the Future about how to do that, which was probably a lot\" is not the obvious answer here?</p></blockquote><p>This may in fact be the answer; I haven't read it though.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:34]</strong>&nbsp;</p><p>or other abstract concepts than equations, which have never existed before</p><p>like, maybe not with a type signature unknown to humanity, but with specific instances unknown to present humanity</p><p>that's what I'd expect to see from humanly designed nanosystems</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:35]</strong> &nbsp;</p><p>So something like AlphaFold is only doing a very small proportion of the work here, since it's not able to generate new abstract concepts (of the necessary level of power)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:35]</strong> &nbsp;</p><p>yeeeessss, that is why DeepMind did not take over the world last year</p><p>it's not just that AlphaFold lacks the concepts but that it lacks the machinery to invent those concepts and the machinery to do anything with such concepts</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:38]</strong> &nbsp;</p><p>I think I find this fairly persuasive, but I also expect that people will come up with increasingly clever ways to leverage narrow systems so that they can do more and more work.</p><p>(including things like: if you don't have enough simulations, then train another narrow system to help fix that, etc)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:39]</strong> &nbsp;</p><p>(and they will accept their trivial billion-dollar-payouts and World GDP will continue largely undisturbed, on my mainline model, because it will be easiest to find ways to make money by leveraging narrow systems on the less regulated, less real parts of the economy, instead of trying to build houses or do medicine, etc.)</p><p>real tests being expensive, simulation being impossibly expensive, and not having enough samples to train your civilization's current level of AI technology, is not a problem you can solve by training a new AI to generate samples, because you do not have enough samples to train your civilization's current level of AI technology to generate more samples</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:41]</strong> &nbsp;</p><p>Thinking about nanotech makes me more sympathetic to the argument that developing general intelligence will bring a sharp discontinuity. But it also makes me expect longer timelines to AGI, during which there's more time to do interesting things with narrow AI. So I guess it weighs more against Dario's view, less against Paul's view.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:41]</strong> &nbsp;</p><p>well, I've been debating Paul about that separately in the timelines channel, not sure about recapitulating it here</p><p>but in broad summary, since I expect the future to look like it was drawn from the \"history book\" barrel and not the \"futurism\" barrel, I expect huge barriers to doing <i>huge</i> things with narrow AI in small amounts of time; you can sell waifutech because it's unregulated and hard to regulate, but that doesn't feed into core mining and steel production.</p><p>we could already have double the GDP if it was legal to build houses and hire people, etc., and the change brought by pre-AGI will perhaps be that our GDP could <i>quadruple</i> instead of just <i>double</i> if it was legal to do things, but that will not make it legal to do things, and why would anybody try to do things and probably fail when there are easier $36 billion profits to be made in waifutech.</p></td></tr></tbody></table>\n\n14.3. Relatively shallow cognition, Go, and math\n------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:45]</strong>&nbsp;</p><p>I'd be interested to see Paul's description of how we would train AIs to solve hard scientific problems. I think there's some prediction that's like \"we train it on arxiv and fine-tune it until it starts to output credible hypotheses about nanotech\". And this seems like it has a step that's quite magical to me, but perhaps that'll be true of any prediction that I make before fully understanding how intelligence works.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:46]</strong>&nbsp;</p><p>my belief is not so much that this training can never happen, but that this probably means the system was trained <i>beyond the point of safe shallowness</i></p><p>not in principle over all possible systems a superintelligence could build, but in practice when it happens on Earth</p><p>my only qualm about this is that current techniques make it possible to buy shallowness in larger quantities than this Earth has ever seen before, and people are looking for surprising ways to make use of that</p><p>so I weigh in my mind the thought of Reality saying Gotcha! by handing me a headline I read tomorrow about how GPT-4 has started producing totally reasonable science papers that are actually correct</p><p>and I am pretty sure that exact thing doesn't happen</p><p>and I ask myself about GPT-5 in a few more years, which had the same architecture as GPT-3 but more layers and more training, doing the same thing</p><p>and it's still largely \"nope\"</p><p>then I ask myself about people in 5 years being able to use the shallow stuff <i>in any way whatsoever</i> to produce the science papers</p><p>and of course the answer there is, \"okay, but is it doing that without having shallowly learned stuff <i>that adds up to deep stuff</i> which is <i>why it can now do science</i>\"</p><p>and I try saying back \"no, it was born of shallowness and it remains shallow and it's just doing science because it turns out that there is totally a way to be an incredibly mentally shallow skillful scientist if you think 10,000 shallow thoughts per minute instead of 1 deep thought per hour\"</p><p>and my brain is like, \"I cannot absolutely rule it out but it really seems like trying to call the next big surprise in 2014 and you guess self-driving cars instead of Go because how the heck would you guess that Go was shallower than self-driving cars\"</p><p>like, that is an <i>imaginable</i> surprise</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:52]</strong> &nbsp;</p><p>On that <i>particular</i> point it seems like the very reasonable heuristic of \"pick the most similar task\" would say that go is like chess and therefore you can do it shallowly.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:52]</strong> &nbsp;</p><p>but there's a world of difference between saying that a surprise is imaginable, and that it wouldn't surprise you</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:52]</strong> &nbsp;</p><p>I wasn't thinking that much about AI at that point, so you're free to call that post-hoc.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:52]</strong> &nbsp;</p><p>the Chess techniques had already failed at Go</p><p>actual new techniques were required</p><p>the people around at the time had witnessed sudden progress on self-driving cars a few years earlier</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:53]</strong> &nbsp;</p><p>My advance prediction here is that \"math is like go and therefore can be done shallowly\".</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:53]</strong> &nbsp;</p><p>self-driving cars were of obviously greater economic interest as well</p><p>my recollection is that talk of the time was about self-driving</p><p>heh! I have the same sense.</p><p>that is, math being shallower than science.</p><p>though perhaps not as shallow as Go, and you will note that Go has fallen and Math has not</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:54]</strong> &nbsp;</p><p>right</p><p>I also expect that we'll need new techniques for math (although not as different from the go techniques as the go techniques were from chess techniques)</p><p>But I guess we're not finding strong disagreements here either.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:57]</strong> &nbsp;</p><p>if Reality came back and was like \"Wrong! Keeping up with the far reaches of human mathematics is harder than being able to develop your own nanotech,\" I would be like \"What?\" to about the same degree as being \"What?\" on \"You can build nanotech just by thinking trillions of thoughts that are too shallow to notice humans!\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][17:58]</strong> &nbsp;</p><p>Perhaps let's table this topic and move on to one of the others Nate suggested? I'll note that walking through the steps required to invent a science of nanotechnology does make your position feel more compelling, but I'm not sure how much of that is the general \"intelligence is magic\" intuition I mentioned before.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:59]</strong> &nbsp;</p><p>How do you suspect your beliefs would shift if you had any detailed model of intelligence?</p><p>Consider trying to imagine a particular wrong model of intelligence and seeing what it would say differently?</p><p>(not sure this is a useful exercise and we could indeed try to move on)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:01]</strong> &nbsp;</p><p>I think there's one model of intelligence where scientific discovery is more actively effortful - as in, you need to be very goal-directed in determining hypotheses, testing hypotheses, and so on.</p><p>And there's another in which scientific discovery is more constrained by flashes of insight, and the systems which are producing those flashes of insight are doing pattern-matching in a way that's fairly disconnected from the real-world consequences of those insights.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:05]</strong> &nbsp;</p><p>The first model is true and the second one is false, if that helps. &nbsp;You can tell this by contemplating where you would update if you learned any model, by considering that things look more disconnected when you can't see the machinery behind them. &nbsp;If you don't know what moves the second hand on a watch and the minute hand on a watch, they could just be two things that move at different rates for completely unconnected reasons; if you can see inside the watch, you'll see that the battery is shared and the central timing mechanism is shared and then there's a few gears to make the hands move at different rates.</p><p>Like, in my ontology, the notion of \"effortful\" doesn't particularly parse as anything basic, because it doesn't translate over into paperclip maximizers, which are neither effortful nor effortless.</p><p>But in a human scientist you've got thoughts being shoved around by all sorts of processes behind the curtains, created by natural selection, some of them reflecting shards of Consequentialism / shadowing paths through time</p><p>The flashes of insight come to people who were looking in nonrandom places</p><p>If they didn't plan deliberately and looked on pure intuition, they looked with an intuition trained by past success and failure</p><p>Somebody walking doesn't plan to walk, but long ago as a baby they learned from falling over, and their ancestors who fell over more didn't reproduce</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:09]</strong> &nbsp;</p><p>I think the first model is probably more true for humans in the domain of science. But I'm uncertain about the extent to which this because humans have not been optimised very much for doing science. If we consider the second model in a domain that humans have actually been optimised very hard for (say, physical activity) - then maybe we can use the analogy of a coach and a player. The coach can tell the player what to practice, but almost all the work is done by the player practicing in a way which updates their intuitions.</p><p>This has become very abstract, though.</p></td></tr></tbody></table>\n\n14.4. Pivotal acts and historical precedents\n--------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:11]</strong>&nbsp;</p><blockquote><p>A few takes:</p><p>1. It looks to me like there's some crux in \"how useful will the 'shallow' stuff get before dangerous things happen\". I would be unsurprised if this spiraled back into the gradualness debate. I'm excited about attempts to get specific and narrow disagreements in this domain (not necessarily bettable; I nominate distilling out specific disagreements before worrying about finding bettable ones).</p><p>2. It seems plausible to me we should have some much more concrete discussion about possible ways things could go right, according to Richard. I'd be up for playin the role of beeping when things seem insufficiently concrete.</p><p>3. It seems to me like Richard learned a couple things about Eliezer's model in that last bout of conversation. I'd be interested to see him try to paraphrase his current understanding of it, and to see Eliezer produce beeps where it seems particularly off.</p></blockquote><p>Here's Nate's comment.</p><p>We could try his #2 suggestion: concrete ways that things could go right.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][18:12]</strong> &nbsp;</p><p>(I am present and am happy to wield the concreteness-hammer)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:13]</strong> &nbsp;</p><p>I think I'm a little cautious about this line of discussion, because my model doesn't strongly constrain the ways that different groups respond to increasing developments in AI. The main thing I'm confident about is that there will be much clearer responses available to us once we have a better picture of AI development.</p><p>E.g. before modern ML, the option of international constraints on compute seemed much less salient, because algorithmic developments seemed much more important.</p><p>Whereas now, tracking/constraining compute use seems like one promising avenue for influencing AGI development.</p><p>Or in the case of nukes, before knowing the specific details about how they were constructed, it would be hard to give a picture of how arms control goes well. But once you know more details about the process of uranium enrichment, you can construct much more efficacious plans.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:19]</strong> &nbsp;</p><p>Once we knew specific things about bioweapons, countries developed specific treaties for controlling them, which failed (according to @CarlShulman)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:19, moved two down in log]</strong> &nbsp;</p><p>(As a side note, I think that if Eliezer had been around in the 1930s, and you described to him what actually happened with nukes over the next 80 years, he would have called that \"insanely optimistic\".)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:21]</strong> &nbsp;</p><p>Mmmmmmaybe. &nbsp;Do note that I tend to be more optimistic than the average human about, say, global warming, or everything in transhumanism outside of AGI.</p><p>Nukes have going for them that, in fact, nobody has an incentive to start a global thermonuclear war. &nbsp;Eliezer is not in fact pessimistic about everything and views his AGI pessimism as generalizing to very few other things, which are not, in fact, as bad as AGI.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:21]</strong> &nbsp;</p><p>I think I put this as the lowest application of competent power out of the things listed in my doc; I'd need to look at the historical details to know if important decision-makers actually cared about it, or were just doing it for PR reasons.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][18:22]</strong> &nbsp;</p><blockquote><p>Once we knew specific things about bioweapons, countries developed specific treaties for controlling them, which failed (according to @CarlShulman)</p></blockquote><p>The treaties were pro forma without verification provisions because the powers didn't care much about bioweapons. They did have verification for nuclear and chemical weapons which did work.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:22]</strong> &nbsp;</p><p>But yeah, compared to pre-1946 history, nukes actually kind of did go <i>really surprisingly well!</i></p><p>Like, this planet used to be a huge warring snakepit of Great Powers and Little Powers and then nukes came along and people actually got serious and decided to stop having the largest wars they could fuel.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][18:22][18:23]</strong>&nbsp;</p><p>The analog would be an international agreement to sign a nice unenforced statement of AI safety principles and then all just building AGI in doomy ways without explicitly saying they're doing it..</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\">The BWC also allowed 'defensive' research that is basically as bad as the offensive kind.</td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:23]</strong> &nbsp;</p><blockquote><p>The analog would be an international agreement to sign a nice unenforced statement of AI safety principles and then all just building AGI in doomy ways without explicitly saying they're doing it..</p></blockquote><p>This scenario sure sounds INCREDIBLY PLAUSIBLE, yes</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:22]</strong> &nbsp;</p><p>On that point: do either of you have strong opinions about the anthropic shadow argument about nukes? That seems like one reason why the straw 1930s-Eliezer I just cited would have been justified.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:23]</strong> &nbsp;</p><p>I mostly don't consider the anthropic shadow stuff</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][18:24]</strong> &nbsp;</p><p>In the late Cold War Gorbachev and Reagan might have done the BWC treaty+verifiable dismantling, but they were in a rush on other issues like nukes and collapse of the USSR.</p><p>Putin just wants to keep his bioweapons program, it looks like. Even denying the existence of the exposed USSR BW program.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:25]</strong> &nbsp;</p><p>I'm happy making no appeal to anthropics here.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][18:25]</strong> &nbsp;</p><p>Boo anthropic shadow claims. Always dumb.</p><p>(Sorry I was only invoked for BW, holding my tongue now.)</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: ❤️]</td><td>[Soares: ❤️]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:26]</strong> &nbsp;</p><p>There may come a day when the strength of nonanthropic reasoning fails... but that is not this day!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:27]</strong> &nbsp;</p><p>Okay, happy to rule that out for now too. So yeah, I picture 1930s-Eliezer pointing to technological trends and being like \"by default, 30 years after the first nukes are built, you'll be able to build one in your back yard. And governments aren't competent enough to stop that happening.\"</p><p>And I don't think I could have come up with a compelling counterargument back then.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][18:27]</strong> &nbsp;</p><blockquote><p>[Sorry I was only invoked for BW, holding my tongue now.]</p></blockquote><p>(fwiw, I thought that when Richard asked \"you two\" re: anthropic shadow, he meant you also. But I appreciate the caution. And in case Richard meant me, I will note that I agree w/ Carl and Eliezer on this count.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:28]</strong> &nbsp;</p><blockquote><p>(fwiw, I thought that when Richard asked \"you two\" re: anthropic shadow, he meant you also. But I appreciate the caution. And in case Richard meant me, I will note that I agree w/ Carl and Eliezer on this count.)</p></blockquote><p>Oh yeah, sorry for the ambiguity, I meant Carl.</p><p>I do believe that AI control will be more difficult than nuclear control, because AI is so much more useful. But I also expect that there will be many more details about AI development that we don't currently understand, that will allow us to influence it (because AGI is a much more complicated concept than \"really really big bomb\").</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:29]</strong> &nbsp;</p><blockquote><p>[So yeah, I picture 1930s-Eliezer pointing to technological trends and being like \"by default, 30 years after the first nukes are built, you'll be able to build one in your back yard. And governments aren't competent enough to stop that happening.\"</p><p>And I don't think I could have come up with a compelling counterargument back then.]</p></blockquote><p>So, I mean, in fact, I don't prophesize doom from very many trends at all! &nbsp;It's literally just AGI that is anywhere near that unmanageable! &nbsp;Many people in EA are more worried about biotech than I am, for example.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:31]</strong> &nbsp;</p><p>I appreciate that my response is probably not very satisfactory to you here, so let me try to think about more concrete things we can disagree about.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:31]</strong> &nbsp;</p><blockquote><p>[I do believe that AI control will be more difficult than nuclear control, because AI is so much more useful. But I also expect that there will be many more details about AI development that we don't currently understand, that will allow us to influence it (because AGI is a much more complicated concept than \"really really big bomb\").]</p></blockquote><p>Er... I think this is not a correct use of the Way I was attempting to gesture at; things being more complicated when known than unknown, does not mean you have more handles to influence them because each complication has the potential to be a handle. &nbsp;It is not in general true that very complicated things are easier for humanity in general, and governments in particular, to control, because they have so many exposed handles.</p><p>I think there's a valid argument about it maybe being more possible to control the supply chain for AI training processors if the global chip supply chain is narrow (also per Carl).</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:34]</strong> &nbsp;</p><p>One thing that we seemed to disagree on, to a significant extent, is the difficulty of \"US and China preventing any other country from becoming a leader in AI\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:35]</strong> &nbsp;</p><p>It is in fact a big deal about nuclear tech that uranium can't be mined in every country, as I understand it, and that centrifuges stayed at the frontier of technology and were harder to build outside the well-developed countries, and that the world ended up revolving around a few Great Powers that had no interest in nuclear tech proliferating any further.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:35]</strong> &nbsp;</p><p>It seems to me that the US and/or China could apply a lot of pressure to many countries.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:35]</strong> &nbsp;</p><p>Unfortunately, before you let that encourage you too much, I would also note it was an important fact about nuclear bombs that they did not produce streams of gold and then ignite the atmosphere if you turned up the stream of gold too high with the actual thresholds involved being unpredictable.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:35]</strong> &nbsp;</p><p>E.g. if the UK had actually seriously tried to block Google's acquisition of DeepMind, and the US had actually seriously tried to convince them not to do so, then I expect that the UK would have folded. (Although it's a weird hypothetical.)</p><blockquote><p>Unfortunately, before you let that encourage you too much, I would also note it was an important fact about nuclear bombs that they did not produce streams of gold and then ignite the atmosphere if you turned up the stream of gold too high with the actual thresholds involved being unpredictable.</p></blockquote><p>Not a critical point, but nuclear power does actually seem like a \"stream of gold\" in many ways.</p><p>(also, quick meta note: I need to leave in 10 mins)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:38]</strong> &nbsp;</p><p>I would be a lot more cheerful about a few Great Powers controlling AGI if AGI produced wealth, but more powerful AGI produced no more wealth; if AGI was made entirely out of hardware, with no software component that could be keep getting orders of magnitude more efficient using hardware-independent ideas; and if the button on AGIs that destroyed the world was clearly labeled.</p><p>That does take AGI to somewhere in the realm of nukes.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:38]</strong> &nbsp;</p><p>How much improvement do you think can be eked out of existing amounts of hardware if people just try to focus on algorithmic improvements?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:38]</strong> &nbsp;</p><p>And Eliezer is capable of being less concerned about things when they are intrinsically less concerning, which is why my history does not, unlike some others in this field, involve me running also being Terribly Concerned about nuclear war, global warming, biotech, and killer drones.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:39]</strong> &nbsp;</p><p>This says 44x improvements over 7 years: <a href=\"https://openai.com/blog/ai-and-efficiency/\">https://openai.com/blog/ai-and-efficiency/</a></p><figure class=\"image\"><img src=\"https://images-ext-1.discordapp.net/external/0ZgRbpmbv_D6LHuB59diNlLopn91Ii66xvJmWy8-jTc/https/openai.com/content/images/2020/05/ai-and-efficiency-social.png?width=1200&amp;height=628\"></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:39]</strong> &nbsp;</p><p>Well, if you're a superintelligence, you can probably do human-equivalent human-speed general intelligence on a 286, though it might possibly have less fine motor control, or maybe not, I don't know.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:40]</strong> &nbsp;</p><p>(within reasonable amounts of human-researcher-time - say, a decade of holding hardware fixed)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:40]</strong> &nbsp;</p><p>I wouldn't be surprised if human ingenuity asymptoted out at AGI on a home computer from 1995.</p><p>Don't know if it'd take more like a hundred years or a thousand years to get fairly close to that.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:41]</strong> &nbsp;</p><p>Does this view cash out in a prediction about how the AI and Efficiency graph projects into the future?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:42]</strong> &nbsp;</p><p>The question of how efficiently you can perform a fixed algorithm doing fixed things, often pales compared to the gains on switching to different algorithms doing different things.</p><p>Given government control of all the neural net training chips and no more public GPU farms, I buy that they could keep a nuke!AGI (one that wasn't tempting to crank up and had clearly labeled Doom-Causing Buttons whose thresholds were common knowledge) under lock of the Great Powers for 7 years, during which software decreased hardware requirements by 44x. &nbsp;I am a bit worried about how long it takes before there's a proper paradigm shift on the level of deep learning getting started in 2006, after which the Great Powers need to lock down on individual GPUs.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:46]</strong> &nbsp;</p><p>Hmm, okay.</p></td></tr></tbody></table>\n\n14.5. Past ANN progress\n-----------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:46]</strong>&nbsp;</p><p>I don't expect another paradigm shift like that</p><p>(in part because I'm not sure the paradigm shift actually happened in the first place - it seems like neural networks were improving pretty continuously over many decades)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:47]</strong>&nbsp;</p><p>I've noticed that opinion around OpenPhil! &nbsp;It makes sense if you have short timelines and expect the world to end before there's another paradigm shift, but OpenPhil doesn't seem to expect that either.</p><p>Yeah, uh, there was kinda a paradigm shift in AI between say 2000 and now. &nbsp;There really, really was.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:49]</strong> &nbsp;</p><p>What I mean is more like: it's not clear to me that an extrapolation of the trajectory of neural networks is made much better by incorporating data about the other people who weren't using neural networks.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:49]</strong> &nbsp;</p><p>Would you believe that at one point Netflix ran a prize contest to produce better predictions of their users' movie ratings, with a $1 million prize, and this was one of the largest prizes ever in AI and got tons of contemporary ML people interested, and neural nets were not prominent on the solutions list at all, because, back then, people occasionally solved AI problems <i>not using neural nets</i>?</p><p>I suppose that must seem like a fairy tale, as history always does, but I lived it!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:50]</strong> &nbsp;</p><p>(I wasn't denying that neural networks were for a long time marginalised in AI)</p><p>I'd place much more credence on future revolutions occurring if neural networks had actually only been invented recently.</p><p>(I have to run in 2 minutes)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:51]</strong> &nbsp;</p><p>The world might otherwise end before the next paradigm shift, but if the world keeps on ticking for 10 years, 20 years, there will not always be the paradigm of training massive networks by even more massive amounts of gradient descent; I do not think that is actually the most efficient possible way to turn computation into intelligence.</p><p>Neural networks stayed stuck at only a few layers for a long time, because the gradients would explode or die out if you made the networks any deeper.</p><p>There was a critical moment in 2006(?) where Hinton and Salakhutdinov(?) proposed training Restricted Boltzmann machines unsupervised in layers, and then 'unrolling' the RBMs to initialize the weights in the network, and then you could do further gradient descent updates from there, because the activations and gradients wouldn't explode or die out given that initialization. &nbsp;That got people to, I dunno, 6 layers instead of 3 layers or something? But it focused attention <i>on</i> the problem of exploding gradients as the reason why deeply layered neural nets never worked, and that kicked off the entire modern field of deep learning, more or less.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:56]</strong> &nbsp;</p><p>Okay, so are you claiming that that neural networks were mostly bottlenecked by algorithmic improvements, not compute availability, for a significant part of their history?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:56]</strong> &nbsp;</p><p>If anybody goes back and draws a graph claiming the whole thing was continuous if you measure the right metric, I am not really very impressed unless somebody at the time was using that particular graph and predicting anything like the right capabilities off of it.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:56]</strong> &nbsp;</p><p>If so this seems like an interesting question to get someone with more knowledge of ML history than me to dig into; I might ask around.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:57]</strong> &nbsp;</p><blockquote><p>[Okay, so are you claiming that that neural networks were mostly bottlenecked by algorithmic improvements, not compute availability, for a significant part of their history?]</p></blockquote><p>Er... yeah? &nbsp;There was a long time when, even if you threw a big neural network at something, it just wouldn't work.</p><p>Good night, btw?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:57]</strong> &nbsp;</p><p>Let's call it here; thanks for the discussion.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][18:57]</strong> &nbsp;</p><p>Thanks, both!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:57]</strong> &nbsp;</p><p>I'll be interested to look into that claim, it doesn't fit with the impressions I have of earlier bottlenecks.</p><p>I think the next important step is probably for me to come up with some concrete governance plans that I'm excited about.</p><p>I expect this to take quite a long time</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][18:58]</strong> &nbsp;</p><p>We can coordinate around that later. Sorry for keeping you so late already, Richard.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][18:59]</strong> &nbsp;</p><p>No worries</p><p>My proposal would be that we should start on whatever work is necessary to convert the debate into a publicly accessible document now</p><p>In some sense coming up with concrete governance plans is my full-time job, but I feel like I'm still quite a way behind in my thinking on this, compared with people who have been thinking about governance specifically for longer</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][19:01]</strong> &nbsp;</p><p>(@RobBensinger is already on it 🙂)</p><figure class=\"table\"><table><tbody><tr><td>[Bensinger: ✅]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][19:03]</strong> &nbsp;</p><p>Nuclear plants might be like narrow AI in this analogy; some designs potentially contribute to proliferation, and you can get more economic wealth by building more of them, but they have no Unlabeled Doom Dial where you can get more and more wealth out of them by cranking them up until at some unlabeled point the atmosphere ignites.</p><p>Also a thought: I don't think you just want somebody with more knowledge of AI history, I think you might need to ask an actual old fogey <i>who was there at the time</i>, and hasn't just learned an ordered history of just the parts of the past that are relevant to the historian's theory about how the present happened.</p><p>Two of them, independently, to see if the answers you get are reliable-as-in-statistical-reliability.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][19:19]</strong> &nbsp;</p><p>My own quick take, for the record, is that it looks to me like there are two big cruxes here.</p><p>One is about whether \"deep generality\" is a good concept, and in particular whether it pushes AI systems quickly from \"nonscary\" to \"scary\" and whether we should expect human-built AI systems to acquire it in practice (before the acute risk period is ended by systems that lack it). The other is about how easy it will be to end the acute risk period (eg by use of politics or nonscary AI systems alone).</p><p>I suspect the latter is the one that blocks on Richard thinking about governance strategies. I'd be interested in attempting further progress on the former point, though it's plausible to me that that should happen over in #timelines instead of here.</p></td></tr></tbody></table>",
      "plaintextDescription": "This is a transcript of a conversation between Richard Ngo and Eliezer Yudkowsky, facilitated by Nate Soares (and with some comments from Carl Shulman). This transcript continues the Late 2021 MIRI Conversations sequence, following Ngo's view on alignment difficulty.\n\n \n\nColor key:\n\n Chat by Richard and Eliezer  Other chat \n\n \n\n\n14. October 4 conversation\n \n\n\n14.1. Predictable updates, threshold functions, and the human cognitive range\n \n\n[Ngo][15:05] \n\nTwo questions which I'd like to ask Eliezer:\n\n1. How strongly does he think that the \"shallow pattern-memorisation\" abilities of GPT-3 are evidence for Paul's view over his view (if at all)\n\n2. How does he suggest we proceed, given that he thinks directly explaining his model of the chimp-human difference would be the wrong move?\n\n[Yudkowsky][15:07]  \n\n1 - I'd say that it's some evidence for the Dario viewpoint which seems close to the Paul viewpoint.  I say it's some evidence for the Dario viewpoint because Dario seems to be the person who made something like an advance prediction about it.  It's not enough to make me believe that you can straightforwardly extend the GPT architecture to 3e14 parameters and train it on 1e13 samples and get human-equivalent performance.\n\n[Ngo][15:09]  \n\nDid you make any advance predictions, around the 2008-2015 period, of what capabilities we'd have before AGI?\n\n[Yudkowsky][15:10]  \n\nnot especially that come to mind?  on my model of the future this is not particularly something I am supposed to know unless there is a rare flash of predictability.\n\n[Ngo][15:11]  \n\n> 1 - I'd say that it's some evidence for the Dario viewpoint which seems close to the Paul viewpoint. I say it's some evidence for the Dario viewpoint because Dario seems to be the person who made something like an advance prediction about it. It's not enough to make me believe that you can straightforwardly extend the GPT architecture to 3e14 parameters and train it on 1e13 samples and get human-equivalent performance.\n\nFor",
      "wordCount": 10406
    },
    "tags": [
      {
        "_id": "x3ibfKCvYCdWTZ8Zt",
        "name": "Pivotal Acts",
        "slug": "pivotal-acts"
      },
      {
        "_id": "bxhzaWtdNoEMMkE8r",
        "name": "General intelligence",
        "slug": "general-intelligence"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kjmpq33kHg7YpeRYW",
    "title": "(briefly) RaDVaC and SMTM, two things we should be doing",
    "slug": "briefly-radvac-and-smtm-two-things-we-should-be-doing",
    "url": null,
    "baseScore": 230,
    "voteCount": 127,
    "viewCount": null,
    "commentCount": 79,
    "createdAt": null,
    "postedAt": "2022-01-12T06:20:35.555Z",
    "contents": {
      "markdown": "UPDATE:  Be advised that I no longer consider SMTM to be among the things we should be doing, though I don't regret having given them an earlier try.  I wish somebody else was running out and trying the sort of things that SMTM is trying, but they're bad enough at analysis to not qualify for further support until that gets fixed.  [See eg this Twitter thread.](https://twitter.com/natalia__coelho/status/1620838728473403393?s=20&t=XSwc1t6I_QIDHPMo9pc1hg)\n\n* * *\n\nHaving not apparently the energy to write this longly, I write it shortly instead, that it be written at all.\n\nPeople sometimes go about saying now, in this community, that there is collectively enough money that we could potentially go do more things with it, if there were things worth doing.\n\nIf that's true, I'd like to see us planting seed grains now for replacing the now-defunct state capacity of the USA and Earth, with respect to biodefense and ultra-high-leverage med R&D.\n\nAs a poster child of a previous intervention I backed here, Seasonal Affective Disorder affects 0.5%-3% of the population and higher in countries at extreme latitudes.  Call it a hundred million people at a guess.  Standard lightboxes don't work very well for treating it.  The Sun works great for treating it.  Reason suggests trying more light.  Earth, however, previously lacked the state capacity to investigate this obvious question of massive scope.  I put a private funder in touch with a group that did a [preliminary investigation](https://www.lesswrong.com/posts/qEweugBipR5P2cMyK/preprint-is-out-100-000-lumens-to-treat-seasonal-affective); they need to run a larger study but preliminary results suggest that they may, in fact, need to use more light and that doing so might be effective.  Again, that's for a problem that probably over a hundred million people have.  So it's not very surprising that investigation of it bottlenecks on there being a single interested medical researcher, who writes Eliezer Yudkowsky as the person who posed that question, and gets funding arranged by that route.  It was less than $100,000.\n\nThis sort of thing seems obviously competitive to me with QALYs/$ or DALYs/$ on global poverty interventions.\n\nReally obviously competitive, actually.  I will not argue the point further because I expect that most readers who can be persuaded found a glance at the numbers sufficient.\n\nEA's ability to do more things in this space, if it is not bottlenecked on money, is very likely bottlenecked on: good ideas to pursue; people who can pursue those ideas; and/or admin staff who can investigate grants, make them, and operate them.\n\nI suspect that people who can do these things are, at the very least, valuable.  They should therefore be nurtured even in their earlier stages.  If you ever want an apple tree that bears fruit, you may need to at some point plant a seed that doesn't have any apples on it right then, and invest effort into watering it in proportion to how many apples you hope for later, rather than how much this tiny sapling has proven itself right now.\n\nI think it was a huge, huge mistake that more money was not spent on AGI alignment when it was small and weird and unproven.  The resulting damage was not something that could be fixed by any or all of the money that became available later.  Someday I may write more about this.  Anyways, don't do that.\n\nThe [Rapid Deployment Vaccine Collaborative, aka RaDVaC](https://radvac.org/), is now past the small weird stage.  They've pretty much proved themselves.  They should be nurtured and scaled up to where they can start to replace US and Earth defunct state capacity to do the R&D that leads up to being able to rapidly design new vaccines that rapidly scale in production and deployment.  If we can't give RaDVaC $2M I'd like to know what it's being spent on that's more important.  Covid-19 was not very much of a pandemic and if Earth ran into a serious pandemic in its current state that would be a serious problem.\n\nSome weird person \\[correction: 2 weird people\\] wants to investigate [whether the real driver of our massive planetwide obesity epidemic is lithium contamination](https://slimemoldtimemold.com/2021/07/07/a-chemical-hunger-part-i-mysteries/), probably in water.  If you are completely unfamiliar with the actual science on obesity you probably think that's dumb because obesity is caused by high-palatability foods.  Read the first page linked if you'd prefer to know why that's obviously wrong.  If you know about the actual epidemiology of obesity and how ridiculous it makes the gluttony theory look, you are still probably saying \"Wait, *lithium?*\"  This is still mostly my own reaction, honestly.  But obesity remains a massive growing planetary problem, and almost nobody is investigating it in a way that takes the epidemiological facts seriously and elevates those above moralistic gluttony theories.  If some weird person wants to go investigate, I think money should be thrown at them, both to check the low-probability massive-high-value gamble, and also to encourage them to maybe go investigate something else in the same space if the lithium thing doesn't pan out.  $200K maybe.  They advertise their effort under the name of Slime Mold Time Mold (SMTM) for no particular reason I can discern.  I think this is the kind of decision that some people make when they are small and weird, and I think that if we allow that to prevent us from throwing money at them then we are stupid and not learning from history.\n\nThat's all.  Writing this up now, briefly and hurriedly, because the ~lithium person wants~ lithium people want to apply for some EA grants.  Consider this my recommendation thereof.",
      "plaintextDescription": "UPDATE:  Be advised that I no longer consider SMTM to be among the things we should be doing, though I don't regret having given them an earlier try.  I wish somebody else was running out and trying the sort of things that SMTM is trying, but they're bad enough at analysis to not qualify for further support until that gets fixed.  See eg this Twitter thread.\n\n----------------------------------------\n\nHaving not apparently the energy to write this longly, I write it shortly instead, that it be written at all.\n\nPeople sometimes go about saying now, in this community, that there is collectively enough money that we could potentially go do more things with it, if there were things worth doing.\n\nIf that's true, I'd like to see us planting seed grains now for replacing the now-defunct state capacity of the USA and Earth, with respect to biodefense and ultra-high-leverage med R&D.\n\nAs a poster child of a previous intervention I backed here, Seasonal Affective Disorder affects 0.5%-3% of the population and higher in countries at extreme latitudes.  Call it a hundred million people at a guess.  Standard lightboxes don't work very well for treating it.  The Sun works great for treating it.  Reason suggests trying more light.  Earth, however, previously lacked the state capacity to investigate this obvious question of massive scope.  I put a private funder in touch with a group that did a preliminary investigation; they need to run a larger study but preliminary results suggest that they may, in fact, need to use more light and that doing so might be effective.  Again, that's for a problem that probably over a hundred million people have.  So it's not very surprising that investigation of it bottlenecks on there being a single interested medical researcher, who writes Eliezer Yudkowsky as the person who posed that question, and gets funding arranged by that route.  It was less than $100,000.\n\nThis sort of thing seems obviously competitive to me with QALYs/$ or DALYs/$ on globa",
      "wordCount": 915
    },
    "tags": [
      {
        "_id": "827JKe7YNjAegR468",
        "name": "Effective altruism",
        "slug": "effective-altruism"
      },
      {
        "_id": "tNsqhzTibgGJKPEWB",
        "name": "Covid-19",
        "slug": "covid-19"
      },
      {
        "_id": "xHjy88N2uJvGdgzfw",
        "name": "Health / Medicine / Disease",
        "slug": "health-medicine-disease"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gf9hhmSvpZfyfS34B",
    "title": "Ngo's view on alignment difficulty",
    "slug": "ngo-s-view-on-alignment-difficulty",
    "url": null,
    "baseScore": 63,
    "voteCount": 24,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2021-12-14T21:34:50.593Z",
    "contents": {
      "markdown": "This post features a write-up by Richard Ngo on his views, with inline comments.\n\nColor key:\n\n<table style=\"border:1px double rgb(179, 179, 179)\"><tbody><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\">&nbsp; Chat &nbsp;</td><td style=\"background-color:rgb(255, 247, 222);border:1px double rgb(217, 217, 217);padding:0.4em\">&nbsp; Google Doc content &nbsp;</td><td style=\"background-color:rgb(255, 238, 187);border:1px double rgb(217, 217, 217);padding:0.4em\">&nbsp; Inline comments &nbsp;</td></tr></tbody></table>\n\n13\\. Follow-ups to the Ngo/Yudkowsky conversation\n=================================================\n\n13.1. Alignment difficulty debate: Richard Ngo's case\n-----------------------------------------------------\n\n<table style=\"border:1pt solid rgb(0, 0, 0)\"><tbody><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><p><strong>[Ngo][9:31] &nbsp;(Sep. 25)</strong>&nbsp;</p><p>As promised, here's a write-up of some thoughts from my end. In particular, since I've spent a lot of the debate poking Eliezer about his views, I've tried here to put forward more positive beliefs of my own in this doc (along with some more specific claims): [GDocs link]</p><figure class=\"table\"><table><tbody><tr><td>[Soares: ✨]&nbsp;</td></tr></tbody></table></figure></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>We take as a starting observation that a number of “grand challenges” in AI have been solved by AIs that are very far from the level of generality which people expected would be needed. Chess, once considered to be the pinnacle of human reasoning, was solved by an algorithm that’s essentially useless for real-world tasks. Go required more flexible learning algorithms, but policies which beat human performance are still nowhere near generalising to anything else; the same for StarCraft, DOTA, and the protein folding problem. Now it seems very plausible that AIs will even be able to pass (many versions of) the Turing Test while still being a long way from AGI.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:26] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>Now it seems very plausible that AIs will even be able to pass (many versions of) the Turing Test while still being a long way from AGI.</p></blockquote><p>I remark:&nbsp; Restricted versions of the Turing Test.&nbsp; Unrestricted passing of the Turing Test happens after the world ends.&nbsp; Consider how smart you'd have to be to pose as an AGI to an AGI; you'd need all the cognitive powers of an AGI as well as all of your human powers.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][11:24] &nbsp;(Sep. 29 comment)</strong>&nbsp;</p><p>Perhaps we can quantify the Turing test by asking something like:</p><ul><li>What percentile of competence is the judge?</li><li>What percentile of competence are the humans who the AI is meant to pass as?</li><li>How much effort does the judge put in (measured in, say, hours of strategic preparation)?</li></ul><p>Does this framing seem reasonable to you? And if so, what are the highest numbers for each of these metrics that correspond to a Turing test which an AI could plausibly pass before the world ends?</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>I expect this trend to continue until after we have AIs which are superhuman at mathematical theorem-proving, programming, many other white-collar jobs, and many types of scientific research. It seems like Eliezer doesn't. I’ll highlight two specific disagreements which seem to play into this.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:28] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>doesn't</p></blockquote><p>Eh?&nbsp; I'm pretty fine with something proving the Riemann Hypothesis before the world ends.&nbsp; It came up during my recent debate with Paul, in fact.</p><p>Not so fine with something designing nanomachinery that can be built by factories built by proteins.&nbsp; They're legitimately different orders of problem, and it's no coincidence that the second one has a path to pivotal impact, and the first does not.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>A first disagreement is related to Eliezer’s characterisation of GPT-3 as a shallow pattern-memoriser. I think there’s a continuous spectrum between pattern-memorisation and general intelligence. In order to memorise more and more patterns, you need to start understanding them at a high level of abstraction, draw inferences about parts of the patterns based on other parts, and so on. When those patterns are drawn from the real world, then this process leads to the gradual development of a world-model.</p><p>This position seems more consistent with the success of deep learning so far than Eliezer’s position (although my advocacy of it loses points for being post-hoc; I was closer to Eliezer’s position before the GPTs). It also predicts that deep learning will lead to agents which can reason about the world in increasingly impressive ways (although I don’t have a strong position on the extent to which new architectures and algorithms will be required for that). I think that the spectrum from less to more intelligent animals (excluding humans) is a good example of what it looks like to gradually move from pattern-memorisation to increasingly sophisticated world-models and abstraction capabilities.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:30] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>In order to memorise more and more patterns, you need to start understanding them at a high level of abstraction, draw inferences about parts of the patterns based on other parts, and so on.</p></blockquote><p>Correct.&nbsp; You can believe this and <i>not</i> believe that exactly GPT-like architectures can keep going deeper until their overlap of a greater number of patterns achieves the same level of depth and generalization as human depth and generalization from fewer patterns, just like pre-transformer architectures ran into trouble in memorizing deeper patterns than the shallower ones those earlier systems could memorize.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>I expect that Eliezer won’t claim that pattern-memorisation is <i>unrelated</i> to general intelligence, but will claim that a pattern-memoriser needs to undergo a sharp transition in its cognitive algorithms before it can reason reliably about novel domains (like open scientific problems) - with his main argument for that being the example of the sharp transition undergone by humans.</p><p>However, it seems unlikely to me that humans underwent a major transition in our underlying cognitive algorithms since diverging from chimpanzees, because our brains are so similar to those of chimps, and because our evolution from chimps didn’t take very long. This evidence suggests that we should favour explanations for our success which don't need to appeal to big algorithmic changes, if we have any such explanations; and I think we do. More specifically, I’d characterise the three key differences between humans and chimps as:</p><ol><li>Humans have bigger brains.</li><li>Humans have a range of small adaptations primarily related to motivation and attention, such as infant focus on language and mimicry, that make us much better at cultural learning.</li><li>Humans grow up in a rich cultural environment.</li></ol></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][9:13] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><blockquote><p>bigger brains</p></blockquote><p>I recall a 3-4x difference; but this paper says 5-6x for frontal cortex:&nbsp;<a href=\"https://www.google.com/url?q=https://www.nature.com/articles/nn814&amp;sa=D&amp;source=editors&amp;ust=1633068529191000&amp;usg=AOvVaw2kXhQODWG2jBW_xyHO7R_U\">https://www.nature.com/articles/nn814</a></p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][3:24] &nbsp;(Sep. 26 comment)</strong>&nbsp;</p><blockquote><p>language and mimicry</p></blockquote><p>“apes are unable to ape sounds” claims david deutsch in “the beginning of infinity”</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Barnes][8:09] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><blockquote><p>[Humans grow up in a rich cultural environment.]</p></blockquote><p>much richer cultural environment including deliberate teaching</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>I claim that the discontinuity between the capabilities of humans and chimps is mainly explained by the general intelligence of chimps not being aimed in the direction of learning the skills required for economically valuable tasks, which in turn is mainly due to chimps lacking the “range of small adaptations” mentioned above.</p><p>My argument is a more specific version of Paul’s claim that chimp evolution was not primarily selecting for doing things like technological development. In particular, it was not selecting for them because no cumulative cultural environment existed while chimps were evolving, and selection for the application of general intelligence to technological development is much stronger in a cultural environment. (I claim that the cultural environment was so limited before humans mainly because cultural accumulation is <a href=\"https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0119\"><u>very sensitive to transmission fidelity</u></a>.)</p><p>By contrast, AIs will be trained in a cultural environment (including extensive language use) from the beginning, so this won't be a source of large gains for later systems.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][6:01] &nbsp;(Sep. 22 comment on earlier draft)</strong>&nbsp;</p><blockquote><p>more specific version of Paul’s claim</p></blockquote><p>Based on some of Paul's recent comments, this may be what he intended all along; though I don't recall his original writings on takeoff speeds making this specific argument.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shulman][14:23] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>(I claim that the cultural environment was so limited before humans mainly because cultural accumulation is <a href=\"https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0119\"><u>very sensitive to transmission fidelity</u></a>.)</p></blockquote><p>There can be other areas with superlinear effects&nbsp;from repeated application of&nbsp; a skill. There's reason to think that the most productive complex industries tend to have that character.</p><p>Making individual minds able to correctly execute long chains&nbsp;of reasoning by reducing per-step error rate could plausibly have very superlinear effects in programming, engineering, management, strategy, persuasion, etc. And you could have new forms of 'super-culture' that don't work with humans.</p><p><a href=\"https://ideas.repec.org/a/eee/jeborg/v85y2013icp1-10.html\">https://ideas.repec.org/a/eee/jeborg/v85y2013icp1-10.html</a></p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>If true, this argument would weigh against Eliezer’s claims about agents which possess a core of general intelligence being able to easily apply that intelligence to a wide range of tasks. And I don’t think that Eliezer has a compelling alternative explanation of the key cognitive differences between chimps and humans (the closest I’ve seen in his writings is the brainstorming <a href=\"https://www.lesswrong.com/posts/XQirei3crsLxsCQoi/surprised-by-brains\"><u>at the end of this post</u></a>).</p><p>If this is the case, I notice an analogy between Eliezer’s argument against Kurzweil, and my argument against Eliezer. Eliezer attempted to put microfoundations underneath the trend line of Moore’s law, which led to a different prediction than Kurzweil’s straightforward extrapolation. Similarly, my proposed microfoundational explanation of the chimp-human gap gives rise to a different prediction than Eliezer’s more straightforward, non-microfoundational extrapolation.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:39] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>Similarly, my proposed microfoundational explanation of the chimp-human gap gives rise to a different prediction than Eliezer’s more straightforward, non-microfoundational extrapolation.</p></blockquote><p>Eliezer does not use \"non-microfoundational extrapolations\" for very much of anything, but there are obvious reasons why the greater Earth does not benefit from me winning debates through convincingly and correctly listing all the particular capabilities you need to add over and above what GPToid architectures can achieve, in order to achieve AGI.&nbsp; Nobody else with a good model of larger reality will publicly describe such things in a way they believe is correct.&nbsp; I prefer not to argue convincingly but wrongly.&nbsp; But, no, it is not Eliezer's way to sound confident about anything unless he thinks he has a more detailed picture of the microfoundations than the one you are currently using yourself.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][11:40] &nbsp;(Sep. 29 comment)</strong>&nbsp;</p><p>Good to know; apologies for the incorrect inference.</p><p>Given that this seems like a big sticking point in the debate overall, do you have any ideas about how to move forward while avoiding infohazards?</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>My position makes some predictions about hypothetical cases:</p><ol><li>If chimpanzees had the same motivational and attention-guiding adaptations towards cultural learning and cooperation that humans do, and were raised in equally culturally-rich environments, then they could become economically productive workers in a range of jobs (primarily as manual laborers, but plausibly also for operating machinery, etc).<ol><li>Results from chimps raised in human families, like <a href=\"https://en.wikipedia.org/wiki/Washoe_(chimpanzee)\"><u>Washoe</u></a>, seem moderately impressive, although still very uncertain. There’s probably a lot of bias towards positive findings - but on the other hand, it’s only been done a handful of times, and I expect that more practice at it would lead to much better results.</li><li>Comparisons between humans and chimps which <i>aren’t</i> raised in similar ways to humans are massively biased towards humans. For the purposes of evaluating general intelligence, comparisons between chimpanzees and <a href=\"https://en.wikipedia.org/wiki/Feral_child\"><u>feral children</u></a> seem fairer (although it’s very hard to know how much the latter were affected by non-linguistic childhoods as opposed to abuse or pre-existing disabilities).</li></ol></li><li>Consider a hypothetical species which has the same level of “general intelligence” that chimpanzees currently have, but is as well-adapted to the domains of abstract reasoning and technological development as chimpanzee behaviour is to the domain of physical survival (e.g. because they evolved in an artificial environment where their fitness was primarily determined by their intellectual contributions). I claim that this species would have superhuman scientific research capabilities, and would be able to make progress in novel areas of science (analogously to how chimpanzees can currently learn to navigate novel physical landscapes).<ol><li>Insofar as Eliezer doubts this, but <i>does</i> believe that this species could outperform a society of village idiots at scientific research, then he needs to explain why the village-idiot-to-Einstein gap is so significant in this context but not in others.</li><li>However, this is a pretty weird thought experiment, and maybe doesn’t add much to our existing intuitions about AIs. My main intention here is to point at how animal behaviour is <i>really really well-adapted</i> to physical environments, in a way which makes people wonder what it would be like to be <i>really really well-adapted </i>to intellectual environments.</li></ol></li><li><s>I claim that the difficulty of human-level oracle AGIs matching humans</s> Consider an AI which has been trained only to answer questions, and is now human-level at doing so. I claim that the difficulty of this AI matching humans at a range of real-world tasks (without being specifically trained to do so) would be much closer to the difficulty of teaching chimps to do science, than the difficulty of teaching adult humans to do abstract reasoning about a new domain.<ol><li>The analogy here is: chimps have reasonably general intelligence, but it’s hard for them to apply it to science because they weren’t trained to apply intelligence to that. Likewise, human-level oracle AGIs have general intelligence, but it’ll be hard for them to apply it to influencing the world because they weren’t trained to apply intelligence to that.</li></ol></li></ol></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Barnes][8:21] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><blockquote><p>village-idiot-to-Einstein gap</p></blockquote><p>I wonder to what extent you can model within-species intelligence differences partly just as something like hyperparameter search - if you have a billion humans with random variation in their neural/cognitive traits, the top human will be a lot better than average. Then you could say something like:</p><ul><li>humans are the dumbest species you could have where the distribution of intelligence in each generation is sufficient for cultural accumulation</li><li>that by itself might not imply a big gap from chimps</li><li>but human society has much larger population, so the smartest individuals are much smarter</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][9:05] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><p>I think Eliezer's response (which I'd agree with) would be that the cognitive difference between the best humans and normal humans is strongly constrained by the fact that we're all one species who can interbreed with each other. And so our cognitive variation can't be very big compared with inter-species variation (at the top end at least; although it could at the bottom end via things breaking).</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Barnes][9:35] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><p>I think that's not obviously true - it's definitely possible that there's a lot of random variation due to developmental variation etc. If that's the case then population size could create large within-species differences</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:46] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>oracle AGIs</p></blockquote><p>Remind me of what this is?&nbsp; Surely you don't just mean the AI that produces plans it doesn't implement itself, because that AI becomes an agent by adding an external switch that routes its outputs to a motor; it can hardly be much cognitively different from an agent.&nbsp; Then what do you mean, \"oracle AGI\"?</p><p>(People tend to produce shallow specs of what they mean by \"oracle\" that make no sense in my microfoundations, a la \"Just drive red cars but not blue cars!\", leading to my frequent reply, \"Sorry, still AGI-complete in terms of the machinery you have to build to do that.\")</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][11:44] &nbsp;(Sep. 29 comment)</strong>&nbsp;</p><p>Edited to clarify what I meant in this context (and remove the word \"oracle\" altogether).</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:01] &nbsp;(Sep. 29 comment)</strong>&nbsp;</p><p>My reply holds just as much to \"AIs that answer questions\"; what restricted question set do you imagine suffices to save the world without dangerously generalizing internal engines?</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Barnes][8:15] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><blockquote><p>The analogy here is: chimps have reasonably general intelligence, but it’s hard for them to apply it to science because they weren’t trained to apply intelligence to that. Likewise, human-level oracle AGIs have general intelligence, but it’ll be hard for them to apply it to influencing the world because they weren’t trained to apply intelligence to that.</p></blockquote><p>this is not intuitive to me; it seems pretty plausible that the subtasks of predicting the world and of influencing the world are much more similar than the subtasks of surviving in a chimp society are to the subtasks of doing science</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][8:59] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><p>I think Eliezer's position is that all of these tasks are fairly similar <i>if you have general intelligence</i>. E.g. he argued that the difference between very good theorem-proving and influencing the world is significantly smaller than people expect. So even if you're right, I think his position is too strong for your claim to help him. (I expect him to say that I'm significantly overestimating the extent to which&nbsp;chimps are running general cognitive algorithms).</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Barnes][9:33] &nbsp;(Sep. 23 comment on earlier draft)</strong>&nbsp;</p><p>I wasn't trying to defend his position, just disagreeing with you :P</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p><i>More specific details</i></p><p>Here are three training regimes which I expect to contribute to AGI:</p><ul><li>Self-supervised training - e.g. on internet text, code, books, videos, etc.</li><li>Task-based RL - agents are rewarded (likely via human feedback, and some version of iterated amplification) for doing well on bounded tasks.</li><li>Open-ended RL - agents are rewarded for achieving long-term goals in rich environments.</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:56] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>bounded tasks</p></blockquote><p>There's an interpretation of this I'd agree with, but all of the work is being carried by the <i>boundedness</i> of the tasks, little or none via the \"human feedback\" part which I shrug at, and none by the \"iterated amplification\" part since I consider that tech unlikely to exist before the world ends.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>Most of my probability of catastrophe comes from AGIs trained primarily via open-ended RL. Although IA makes these scenarios less likely by making task-based RL more powerful, it doesn’t seem to me that IA tackles the hardest case (of aligning agents trained via open-ended RL) head-on. But disaster from open-ended RL also seems a long way away - mainly because getting long-term real-world feedback is very slow, and I expect it to be hard to create sufficiently rich artificial environments. By that point I do expect the strategic landscape to be significantly different, because of the impact of task-based RL.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:57] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>a long way away</p></blockquote><p>Oh, definitely, at the present rates of progress we've got years, plural.</p><p>The <a href=\"https://intelligence.org/2017/10/13/fire-alarm/\">history of futurism</a> says that even saying that tends to be unreliable in the general case (people keep saying it right up until the Big Thing actually happens) and also that it's rather a difficult form of knowledge to obtain more than a few years out.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12;01] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>hard to create sufficiently rich artificial environments</p></blockquote><p>Disagree; I don't think that making environments more difficult in a way that challenges the environment inside will prove to be a significant AI development bottleneck.&nbsp; Making simulations easy enough for current AIs to do interesting things in them, but hard enough that the things they do are not completely trivial, takes some work relevant to current levels of AI intelligence.&nbsp; I think that making those environments more tractably challenging for smarter AIs is not likely to be nearly a bottleneck in progress, compared to making the AIs smarter and able to solve the environment.&nbsp; It's a one-way-hash, P-vs-NP style thing - not literally, just that general relationship between it taking a lower amount of effort to pose a problem such that solving it requires a higher amount of effort.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>Perhaps the best way to pin down disagreements in our expectations about the effects of the strategic landscape is to identify some measures that could help to reduce AGI risk, and ask how seriously key decision-makers would need to take AGI risk for each measure to be plausible, and how powerful and competent they would need to be for that measure to make a significant difference. Actually, let’s lump these metrics together into a measure of “amount of competent power applied”. Some benchmarks, roughly in order (and focusing on the effort applied by the US):</p><ul><li>Banning chemical/biological weapons</li><li>COVID<ul><li>Key points: mRNA vaccines, lockdowns, mask mandates</li></ul></li><li>Nuclear non-proliferation<ul><li>Key points: <a href=\"https://www.openphilanthropy.org/blog/ai-governance-grantmaking\"><u>Nunn-Lugar Act</u></a>, <a href=\"https://en.wikipedia.org/wiki/Stuxnet\"><u>stuxnet</u></a>, various treaties</li></ul></li><li>The International Space Station<ul><li>Cost to US: ~$75 billion</li></ul></li><li>Climate change<ul><li>US expenditure: &gt;$154 billion (but not very effectively)</li></ul></li><li>Project Apollo<ul><li>Wikipedia says that Project Apollo “was the largest commitment of resources ($156 billion in 2019 US dollars) ever made by any nation in peacetime. At its peak, the Apollo program employed 400,000 people and required the support of over 20,000 industrial firms and universities.”</li></ul></li><li>WW1</li><li>WW2</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:02] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>WW2</p></blockquote><p>This level of effort starts to buy significant amounts of time.&nbsp; This level will not be reached, nor approached, before the world ends.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>Here are some wild speculations (I just came up with this framework, and haven’t thought about these claims very much):</p><ol><li>The US and China preventing any other country from becoming a leader in AI requires about as much competent power as banning chemical/biological weapons.</li><li>The US and China enforcing a ban on AIs above a certain level of autonomy requires about as much competent power as the fight against climate change.<ol><li>In this scenario, all the standard forces which make other types of technological development illegal have pushed towards making autonomous AGI illegal too.</li></ol></li><li>Launching a good-faith joint US-China AGI project requires about as much competent power as launching Project Apollo.<ol><li>According to <a href=\"https://www.spacedaily.com/news/russia-97h.html\"><u>this article</u></a>, Kennedy (and later Johnson) made several offers (some of which were public) of a joint US-USSR Moon mission, which Khrushchev reportedly came close to accepting. Of course this is a long way from actually doing a joint project (and it’s not clear how reliable the source is), but it still surprised me a lot, given that I viewed the “space race” as basically a zero-sum prestige project. If your model predicted this, I’d be interested to hear why.</li></ol></li></ol></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:07] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>The US and China preventing any other country from becoming a leader in AI requires about as much competent power as banning chemical/biological weapons.</p></blockquote><p>I believe this is wholly false.&nbsp; On my model it requires closer to WW1 levels of effort.&nbsp; I don't think you're going to get it without credible threats of military action leveled at previously allied countries.</p><p>AI is easier and more profitable to build than chemical / biological&nbsp;weapons, and correspondingly harder to ban.&nbsp; Existing GPU factories need to be shut down and existing GPU clusters need to be banned and no duplicate of them can be allowed to arise, across many profiting countries that were previously military allies of the United States, which - barring some vast shift in world popular <i>and</i> elite opinion against AI, which is also not going to happen - those countries would be extremely disinclined to sign, especially if the treaty terms permitted the USA and China to forge ahead.</p><p>The reason why chem weapons bans were much easier was that people did not like chem weapons.&nbsp; They were awful.&nbsp; There was a perceived common public interest in nobody having chem weapons.&nbsp; It was understood popularly and by elites to be a Prisoner's Dilemma situation requiring enforcement to get to the Pareto optimum.&nbsp; Nobody was profiting tons off the infrastructure that private parties could use to make chem weapons.</p><p>An AI ban is about as easy as banning advanced metal-forging techniques in current use so nobody can get ahead of the USA and China in making airplanes.&nbsp; That would be HARD and likewise require credible threats of military action against former allies.</p><p>\"AI ban is as easy as a chem weapons ban\" seems to me like politically crazy talk.&nbsp; I'd expect a more politically habited person to confirm this.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shulman][14:32] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><p>AI ban much, much harder than chemical weapons ban. Indeed chemical weapons were low military utility, that was central to the deal, and they&nbsp;have still been used subsequently.</p><blockquote><p>An AI ban is about as easy as banning advanced metal-forging techniques in current use so nobody can get ahead of the USA and China in making airplanes. That would be HARD and likewise require credible threats of military action against former allies.</p></blockquote><p>If large amounts of compute relative to today are needed (and presumably Eliezer rejects this), the fact that there is only a single global&nbsp;leading node chip supply chain&nbsp;makes it vastly easier than metal forging, which exists throughout the world and is vastly cheaper.</p><p>Sharing with allies (and at least embedding&nbsp;allies to monitor US compliance) also reduces the conflict side.</p><p>OTOH, if compute requirements were super low then it gets a lot worse.</p><p>And the biological&nbsp;weapons ban failed completely: the Soviets built an enormous bioweapons program, the largest ever, after agreeing to the ban, and the US couldn't even tell for sure they were doing so.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][18:15] &nbsp;(Oct. 4 comment)</strong>&nbsp;</p><p>I've updated somewhat off of Carl Shulman's argument that there's only one chip supply chain which goes through eg a single manufacturer of lithography machines (ASML), which could maybe make a lock on AI chips possible with only WW1 levels of cooperation instead of WW2.</p><p>That said, I worry that, barring WW2 levels, this might not last very long if other countries started duplicating the supply chain, even if they had to go back one or two process nodes on the chips?&nbsp; There's a difference between the proposition \"ASML has a lock on the lithography market right now\" and \"if aliens landed and seized ASML, Earth would forever after be unable to build another lithography plant\".&nbsp; I mean, maybe that's just true because we lost technology and can't rebuild old bridges either, but it's at least less obvious.</p><p>Launching Tomahawk cruise missiles&nbsp;at any attempt anywhere to build a new ASML, is getting back into \"military threats against former military allies\" territory and hence what I termed WW2 levels of cooperation.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shulman][18:30] &nbsp;(Oct. 4 comment)</strong>&nbsp;</p><p>China has been trying for some time to build its own and has failed with tens of billions of dollars (but has captured some lagging node share), but would be substantially more likely to succeed with a trillion dollar investment. That said, it is hard to throw money at these things and the tons of tacit knowledge/culture/supply chain networks are tough to replicate. Also many ripoffs of the semiconductor subsidies have occurred. Getting more NASA/Boeing and less SpaceX is a plausible outcome even with huge investment.</p><p>They are trying to hire people away from the existing supply chain to take its expertise and building domestic skills with the lagging nodes.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][19:14] &nbsp;(Oct. 4 comment)</strong>&nbsp;</p><p>Does that same theory predict that if aliens land and grab some but not all of the current ASML personnel, Earth is thereby successfully taken hostage for years, because Earth has trouble rebuilding ASML, which had the irreproducible lineage of masters and apprentices dating back to the era of Lost Civilization?&nbsp; Or would Earth be much better at this than China, on your model?</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Shulman][19:31] &nbsp;(Oct. 4 comment)</strong>&nbsp;</p><p>I'll read that as including the many suppliers of ASML (one EUV machine has over 100,000 parts, many incredibly fancy or unique). It's just a matter of how many years it takes. I think Earth fails to rebuild that capacity in 2 years but succeeds in 10.</p><p>\"A study this spring by Boston Consulting Group and the Semiconductor Industry Association estimated that creating a self-sufficient chip supply chain would take at least $1 trillion and sharply increase prices for chips and products made with them...The situation underscores the crucial role played by ASML, a once obscure company whose market value now exceeds $285 billion. It is “the most important company you never heard of,” said C.J. Muse, an analyst at Evercore ISI.\"</p><p><a href=\"https://www.google.com/url?q=https://www.nytimes.com/2021/07/04/technology/tech-cold-war-chips.html&amp;sa=D&amp;source=docs&amp;ust=1638426593582000&amp;usg=AOvVaw3sdQnMg5JFsRGyBxhJraVp\">https://www.nytimes.com/2021/07/04/technology/tech-cold-war-chips.html</a></p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][19:59] &nbsp;(Oct. 4 comment)</strong>&nbsp;</p><p>No in 2 years, yes in 10 years sounds reasonable to me for this hypothetical scenario, as far as I know in my limited knowledge.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:10] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><blockquote><p>Launching a good-faith joint US-China AGI project requires about as much competent power as launching Project Apollo.</p></blockquote><p>It's really weird, relative to my own model, that you put the item that the US and China can bilaterally decide to do all by themselves, without threats of military action against their former allies, as more difficult than the items that require conditions imposed on other developed countries that don't want them.</p><p>Political coordination is hard.&nbsp; No, seriously, it's hard.&nbsp; It comes with a difficulty penalty that scales with the number of countries, how complete the buy-in has to be, and how much their elites and population don't want to do what you want them to do relative to how much elites and population agree that it needs doing (where this very rapidly goes to \"impossible\" or \"WW1/WW2\" as they don't particularly want to do your thing).</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 25 Google Doc)</strong>&nbsp;</p><p>So far I haven’t talked about how much competent power I actually expect people to apply to AI governance. I don’t think it’s useful for Eliezer and me to debate this directly, since it’s largely downstream from most of the other disagreements we’ve had. In particular, I model him as believing that there’ll be very little competent power applied to prevent AI risk from governments and wider society, partly because he expects a faster takeoff than I do, and partly because he has a lower opinion of governmental competence than I do. But for the record, it seems likely to me that there’ll be as much competent effort put into reducing AI risk by governments and wider society as there has been into fighting COVID; and plausibly (but not likely) as much as fighting climate change.</p><p>One key factor is my expectation that arguments about the importance of alignment will become much stronger as we discover more compelling examples of misalignment. I don’t currently have strong opinions about how compelling the worst examples of misalignment before catastrophe are likely to be; but identifying and publicising them seems like a particularly effective form of advocacy, and one which we should prepare for in advance.</p><p>The predictable accumulation of easily-accessible evidence that AI risk is important is one example of a more general principle: that it’s much easier to understand, publicise, and solve problems as those problems get closer and more concrete. This seems like a strong effect to me, and a key reason why so many predictions of doom throughout history have failed to come true, even when they seemed compelling at the time they were made.</p><p>Upon reflection, however, I think that even taking this effect into account, the levels of competent power required for the interventions mentioned above are too high to justify the level of optimism about AI governance that I started our debate with. On the other hand, I found Eliezer’s arguments about consequentialism less convincing than I expected. Overall I’ve updated that AI risk is higher than I previously believed; though I expect my views to be quite unsettled while I think more, and talk to more people, about specific governance interventions and scenarios.</p></td></tr></tbody></table>",
      "plaintextDescription": "This post features a write-up by Richard Ngo on his views, with inline comments.\n\n \n\nColor key:\n\n  Chat    Google Doc content    Inline comments  \n\n \n\n\n13. Follow-ups to the Ngo/Yudkowsky conversation\n \n\n\n13.1. Alignment difficulty debate: Richard Ngo's case\n \n\n \n\n[Ngo][9:31]  (Sep. 25) \n\nAs promised, here's a write-up of some thoughts from my end. In particular, since I've spent a lot of the debate poking Eliezer about his views, I've tried here to put forward more positive beliefs of my own in this doc (along with some more specific claims): [GDocs link]\n\n[Soares: ✨] \n\n[Ngo]  (Sep. 25 Google Doc) \n\nWe take as a starting observation that a number of “grand challenges” in AI have been solved by AIs that are very far from the level of generality which people expected would be needed. Chess, once considered to be the pinnacle of human reasoning, was solved by an algorithm that’s essentially useless for real-world tasks. Go required more flexible learning algorithms, but policies which beat human performance are still nowhere near generalising to anything else; the same for StarCraft, DOTA, and the protein folding problem. Now it seems very plausible that AIs will even be able to pass (many versions of) the Turing Test while still being a long way from AGI.\n\n[Yudkowsky][11:26]  (Sep. 25 comment) \n\n> Now it seems very plausible that AIs will even be able to pass (many versions of) the Turing Test while still being a long way from AGI.\n\nI remark:  Restricted versions of the Turing Test.  Unrestricted passing of the Turing Test happens after the world ends.  Consider how smart you'd have to be to pose as an AGI to an AGI; you'd need all the cognitive powers of an AGI as well as all of your human powers.\n\n[Ngo][11:24]  (Sep. 29 comment) \n\nPerhaps we can quantify the Turing test by asking something like:\n\n * What percentile of competence is the judge?\n * What percentile of competence are the humans who the AI is meant to pass as?\n * How much effort does the judge put in (me",
      "wordCount": 5216
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "nPauymrHwpoNr6ipx",
    "title": "Conversation on technology forecasting and gradualism",
    "slug": "conversation-on-technology-forecasting-and-gradualism",
    "url": null,
    "baseScore": 108,
    "voteCount": 33,
    "viewCount": null,
    "commentCount": 30,
    "createdAt": null,
    "postedAt": "2021-12-09T21:23:21.187Z",
    "contents": {
      "markdown": "This post is a transcript of a multi-day discussion between Paul Christiano, Richard Ngo, Eliezer Yudkowsky, Rob Bensinger, Holden Karnofsky, Rohin Shah, Carl Shulman, Nate Soares, and Jaan Tallinn, following up on the Yudkowsky/Christiano debate in [1](https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds), [2](https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-shulman-and-yudkowsky-on-ai-progress), [3](https://www.lesswrong.com/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress), and [4](https://www.lesswrong.com/posts/fS7Zdj2e2xMqE6qja/more-christiano-cotra-and-yudkowsky-on-ai-progress).\n\nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Paul, Richard, and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td></tr></tbody></table>\n\n12\\. Follow-ups to the Christiano/Yudkowsky conversation\n========================================================\n\n12.1. Bensinger and Shah on prototypes and technological forecasting\n--------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][16:22]&nbsp;</strong> <strong>(Sep. 23)</strong>&nbsp;</p><p>Quoth Paul:</p><blockquote><p>seems like you have to make the wright flyer much better before it's important, and that it becomes more like an industry as that happens, and that this is intimately related to why so few people were working on it</p></blockquote><p>Is this basically saying 'the Wright brothers didn't personally capture much value by inventing heavier-than-air flying machines, and this was foreseeable, which is why there wasn't a huge industry effort already underway to try to build such machines as fast as possible.' ?</p><p>My maybe-wrong model of Eliezer says here 'the Wright brothers knew a (Thielian) secret', while my maybe-wrong model of Paul instead says:</p><ul><li>They didn't know a secret -- it was obvious to tons of people that you could do something sorta like what the Wright brothers did and thereby invent airplanes; the Wright brothers just had unusual non-monetary goals that made them passionate to do a thing most people didn't care about.</li><li>Or maybe it's better to say: they knew some specific secrets about physics/engineering, but only because other people <i>correctly</i> saw 'there are secrets to be found here, but they're stamp-collecting secrets of little economic value to me, so I won't bother to learn the secrets'. ~Everyone knows where the treasure is located, and ~everyone knows the treasure won't make you rich.</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:24]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>My model of Paul says there could be a secret, but only because the industry was tiny and the invention was nearly worthless directly.</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: ➕]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][17:53]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>I mean, I think they knew a bit of stuff, but it generally takes a lot of stuff to make something valuable, and the more people have been looking around in an area the more confident you can be that it's going to take a lot of stuff to do much better, and it starts to look like an extremely strong regularity for big industries like ML or semiconductors</p><p>it's pretty rare to find small ideas that don't take a bunch of work to have big impacts</p><p>I don't know exactly what a thielian secret is (haven't read the reference and just have a vibe)</p><p>straightening it out a bit, I have 2 beliefs that combine disjunctively: (i) generally it takes a lot of work to do stuff, as a strong empirical fact about technology, (ii) generally if the returns are bigger there are more people working on it, as a slightly-less-strong fact about sociology</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][18:09]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>secrets = important undiscovered information (or information that's been discovered but isn't widely known), that you can use to get an edge in something. <a href=\"https://www.lesswrong.com/posts/ReB7yoF22GuerNfhH/thiel-on-secrets-and-indefiniteness\">https://www.lesswrong.com/posts/ReB7yoF22GuerNfhH/thiel-on-secrets-and-indefiniteness</a></p><p>There seems to be a Paul/Eliezer disagreement about how common these are in general. And maybe a disagreement about how much more efficiently humanity discovers and propagates secrets as you scale up the secret's value?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:35]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>Many times it has taken much work to do stuff; there's further key assertions here about \"It takes $100 billion\" and \"Multiple parties will invest $10B first\" and \"$10B gets you a lot of benefit first because scaling is smooth and without really large thresholds\".</p><p>Eliezer is like \"ah, yes, sometimes it takes 20 or even 200 people to do stuff, but core researchers often don't scale well past 50, and there aren't always predecessors that could do a bunch of the same stuff\" even though Eliezer agrees with \"it often takes a lot of work to do stuff\". More premises are needed for the conclusion, that one alone does not distinguish Eliezer and Paul by enough.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][20:03]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>My guess is that everyone agrees with claims 1, 2, and 3 here (please let me know if I'm wrong!):</p><p>1. The history of humanity looks less like <strong>Long Series of Cheat Codes World</strong>, and more like <strong>Well-Designed Game World</strong>.</p><p>In Long Series of Cheat Codes World, human history looks like this, over and over: Some guy found a cheat code that totally outclasses everyone else and makes him God or Emperor, until everyone else starts using the cheat code too (if the Emperor allows it). After which things are maybe normal for another 50 years, until a new Cheat Code arises that makes its first adopters invincible gods relative to the previous tech generation, and then the cycle repeats.</p><p>In Well-Designed Game World, you can sometimes eke out a small advantage, and the balance isn't <i>perfect</i>, but it's pretty good and the leveling-up tends to be gradual. A level 100 character totally outclasses a level 1 character, and some level transitions are a bigger deal than others, but there's no level that makes you a god relative to the people one level below you.</p><p>2. General intelligence took over the world once. Someone who updated on that fact but otherwise hasn't thought much about the topic should not consider it 'bonkers' that machine general intelligence could take over the world too, even though they should still consider it 'bonkers' that eg a coffee startup could take over the world.</p><p>(Because beverages have never taken over the world before, whereas general intelligence has; and because our inside-view models of coffee and of general intelligence make it a lot harder to imagine plausible mechanisms by which coffee could make someone emperor, kill all humans, etc., compared to general intelligence.)</p><p>(In the game analogy, the situation is a bit like 'I've never found a crazy cheat code or exploit in this game, but I haven't ruled out that there is one, and I heard of a character once who did a lot of crazy stuff that's at least <i>suggestive</i> that she might have had a cheat code.')</p><p>3. AGI is arising in a world where agents with science and civilization already exist, whereas humans didn't arise in such a world. This is one reason to think AGI might not take over the world, but it's <i>not</i> a strong enough consideration on its own to make the scenario 'bonkers' (because AGIs are likely to differ from humans in many respects, and it wouldn't obviously be bonkers if the first AGIs turned out to be qualitatively way smarter, cheaper to run, etc.).</p><p>---</p><p>If folks agree with the above, then I'm confused about how one updates from the above epistemic state to 'bonkers'.</p><p>It was to a large extent physics facts that determined how easy it was to understand the feasibility of nukes without (say) decades of very niche specialized study. Likewise, it was physics facts that determined you need rare materials, many scientists, and a large engineering+infrastructure project to build a nuke. In a world where the <i>physics</i> of nukes resulted in it being some PhD's quiet 'nobody thinks this will work' project like Andrew Wiles secretly working on a proof of Fermat's Last Theorem for seven years, that would have <i>happened</i>.</p><p>If an alien came to me in 1800 and told me that totally new physics would let future humans build city-destroying superbombs, then I don't see why I should have considered it bonkers that it might be lone mad scientists rather than nations who built the first superbomb. The 'lone mad scientist' scenario sounds more conjunctive to me (assumes the mad scientist knows something that isn't widely known, AND has the ability to act on that knowledge without tons of resources), so I guess it should have gotten less probability, but maybe not dramatically less?</p><p>'Mad scientist builds city-destroying weapon in basement' sounds wild to me, but I feel like almost all of the actual unlikeliness comes from the 'city-destroying weapons exist at all' part, and then the other parts only moderately lower the probability.</p><p>Likewise, I feel like the prima-facie craziness of basement AGI mostly comes from 'generally intelligence is a crazy thing, it's wild that anything could be that high-impact', and a much smaller amount comes from 'it's wild that something important could happen in some person's basement'.</p><p>---</p><p>It <i>does</i> structurally make sense to me that Paul might know things I don't about GPT-3 and/or humans that make it obvious to him that we roughly know the roadmap to AGI and it's this.</p><p>If the entire 'it's bonkers that some niche part of ML could crack open AGI in 2026 and reveal that GPT-3 (and the mainstream-in-2026 stuff) was on a very different part of the tech tree' view is coming from a detailed inside-view model of intelligence like this, then that immediately ends my confusion about the argument structure.</p><p>I don't understand why you think you have the roadmap, and given a high-confidence roadmap I'm guessing I'd still put more probability than you on someone finding a very different, shorter path that works too. But the <i>argument structure</i> \"roadmap therefore bonkers\" makes sense to me.</p><p>If there are meant to be <i>other</i> arguments against 'high-impact AGI via niche ideas/techniques' that are strong enough to make it bonkers, then I remain confused about the argument structure and how it can carry that much weight.</p><p>I can imagine an inside-view model of human cognition, GPT-3 cognition, etc. that tells you 'AGI coming from nowhere in 3 years is bonkers'; I can't imagine an ML-is-a-reasonably-efficient-market argument that does the same, because even a perfectly efficient market isn't <i>omniscient</i> and can still be surprised by undiscovered physics facts that tell you 'nukes are relatively easy to build' and 'the fastest path to nukes is relatively hard to figure out'.</p><p>(Caveat: I'm using the 'basement nukes' and 'Fermat's last theorem' analogy because it helps clarify the principles involved, not because I think AGI will be that extreme on the spectrum.)</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure><p>Oh, I also wouldn't be confused by a view like \"I think it's 25% likely we'll see a more Eliezer-ish world. But it sounds like Eliezer is, like, 90% confident that will happen, and <i>that level of confidence</i> (and/or the weak reasoning he's provided for that confidence) seems bonkers to me.\"</p><p>The thing I'd be confused by is e.g. \"ML is efficient-ish, therefore <i>the out-of-the-blue-AGI scenario itself</i> is bonkers and gets, like, 5% probability.\"</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][1:58]</strong> &nbsp;<strong>(Sep. 24)</strong>&nbsp;</p><p>(I'm unclear on whether this is acceptable for this channel, please let me know if not)</p><blockquote><p>I can't imagine an ML-is-a-reasonably-efficient-market argument that does the same, because even a perfectly efficient market isn't omniscient and can still be surprised by undiscovered physics facts</p></blockquote><p>I think this seems right as a first pass.</p><p>Suppose we then make the empirical observation that in tons and tons of other fields, it is extremely rare that people discover new facts that lead to immediate impact. (Set aside for now whether or not that's true; assume that it is.) Two ways you could react to this:</p><p>1. Different fields are different fields. It's not like there's a common generative process that outputs a distribution of facts and how hard they are to find that is common across fields. Since there's no common generative process, facts about field X shouldn't be expected to transfer to make predictions about field Y.</p><p>2. There's some latent reason, that we don't currently know, that makes it so that it is rare for newly discovered facts to lead to immediate impact.</p><p>It seems like you're saying that (2) is not a reasonable reaction (i.e. \"not a valid argument structure\"), and I don't know why. There are lots of things we don't know, is it really so bad to posit one more?</p><p>(Once we agree on the argument structure, we should then talk about e.g. reasons why such a latent reason can't exist, or possible guesses as to what the latent reason is, etc, but fundamentally I feel generally okay with starting out with \"there's probably some reason for this empirical observation, and absent additional information, I should expect that reason to continue to hold\".)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][3:15]</strong> &nbsp;<strong>(Sep. 24)</strong>&nbsp;</p><p>I think 2 is a valid argument structure, but I didn't mention it because I'd be surprised if it had enough evidential weight (in this case) to produce an 'update to bonkers'. I'd love to hear more about this if anyone thinks I'm under-weighting this factor. (Or any others I left out!)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][23:57]</strong> &nbsp;<strong>(Sep. 25)</strong>&nbsp;</p><p>Idk if it gets all the way to \"bonkers\", but (2) seems pretty strong to me, and is how I would interpret Paul-style arguments on timelines/takeoff if I were taking on what-I-believe-to-be your framework</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][11:06]</strong> &nbsp;<strong>(Sep. 25)</strong>&nbsp;</p><p>Well, I'd love to hear more about that!</p><p>Another way of getting at my intuition: I feel like a view that assigns very small probability to 'suddenly vastly superhuman AI, because something that high-impact hasn't happened before'</p><p>(which still seems weird to me, because physics doesn't know what 'impact' is and I don't see what physical mechanism could forbid it that strongly and generally, short of simulation hypotheses)</p><p>... would also assign very small probability in 1800 to 'given an alien prediction that totally new physics will let us build superbombs at least powerful enough to level cities, the superbomb in question will ignite the atmosphere or otherwise destroy the Earth'.</p><p>But this seems flatly wrong to me -- if you buy that the bomb works by a totally different mechanism (and exploits a different physics regime) than eg gunpowder, then the output of the bomb is a <i>physics</i> question, and I don't see how we can concentrate our probability mass much without probing the relevant physics. The history of boat and building sizes is a negligible input to 'given a totally new kind of bomb that suddenly lets us (at least) destroy cities, what is the total destructive power of the bomb?'.</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure><p>(Obviously the bomb <i>didn't</i> destroy the Earth, and I wouldn't be surprised if there's some Bayesian evidence or method-for-picking-a-prior that could have validly helped you suspect as much in 1800? But it would be a suspicion, not a confident claim.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][1:45]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><blockquote><p>would also assign very small probability in 1800 to 'given an alien prediction that totally new physics will let us build superbombs at least powerful enough to level cities, the superbomb in question will ignite the atmosphere or otherwise destroy the Earth'</p></blockquote><p>(As phrased you also have to take into account the question of whether humans would deploy the resulting superbomb, but I'll ignore that effect for now.)</p><p>I think this isn't exactly right. The \"totally new physics\" part seems important to update on.</p><p>Let's suppose that, in the reference class we built of boat and building sizes, empirically nukes were the 1 technology out of 20 that had property X. (Maybe X is something like \"discontinuous jump in things humans care about\" or \"immediate large impact on the world\" or so on.) Then, I think in 1800 you assign ~5% to 'the first superbomb at least powerful enough to level cities will ignite the atmosphere or otherwise destroy the Earth'.</p><p>Once you know more details about how the bomb works, you should be able to update away from 5%. Specifically, \"entirely new physics\" is an important detail that causes you to update away from 5%. I wouldn't go as far as you in throwing out reference classes entirely at that point -- there can still be unknown latent factors that apply at the level of physics -- but I agree reference classes look harder to use in this case.</p><p>With AI, I start from ~5% and then I don't really see any particular detail for AI that I think I should strongly update on. My impression is that Eliezer thinks that \"general intelligence\" is a qualitatively different sort of thing than that-which-neural-nets-are-doing, and maybe that's what's analogous to \"entirely new physics\". I'm pretty unconvinced of this, but something in this genre feels quite crux-y for me.</p><p>Actually, I think I've lost the point of this analogy. What's the claim for AI that's analogous to</p><blockquote><p>'given an alien prediction that totally new physics will let us build superbombs at least powerful enough to level cities, the superbomb in question will ignite the atmosphere or otherwise destroy the Earth'</p></blockquote><p>?</p><p>Like, it seems like this is saying \"We figure out how to build a new technology that does X. What's the chance it has side effect Y?\" Where X and Y are basically unrelated.</p><p>I was previously interpreting the argument as \"if we know there's a new superbomb based on totally new physics, and we know that the first such superbomb is at least capable of leveling cities, what's the probability it would have enough destructive force to also destroy the world\", but upon rereading that doesn't actually seem to be what you were gesturing at.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][3:08]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><p>I'm basically responding to this thing Ajeya wrote:</p><blockquote><p>I think Paul's view would say:</p><ul><li>Things certainly happen for the first time</li><li>When they do, they happen at small scale in shitty prototypes, like the Wright Flyer or GPT-1 or AlphaGo or the Atari bots or whatever</li><li>When they're making a big impact on the world, it's after a lot of investment and research, like commercial aircrafts in the decades after Kitty Hawk or like the investments people are in the middle of making now with AI that can assist with coding</li></ul></blockquote><p>To which my reply is: I agree that the first AGI systems will be shitty compared to <i>later</i> AGI systems. But Ajeya's Paul-argument seems to additionally require that AGI systems be relatively unimpressive at cognition compared to preceding AI systems that weren't AGI.</p><p>If this is because of some general law that things are shitty / low-impact when they \"happen for the first time\", then I don't understand what physical mechanism could produce such a general law that holds with such force.</p><p>As I see it, physics 'doesn't care' about human conceptions of impactfulness, and will instead produce AGI prototypes, aircraft prototypes, and nuke prototypes that have as much impact as is implied by the detailed case-specific workings of general intelligence, flight, and nuclear chain reactions respectively.</p><p>We could frame the analogy as:</p><ul><li>'If there's a year where AI goes from being unable to do competitive par-human reasoning in the hard sciences, to being able to do such reasoning, we should estimate the impact of the first such systems by drawing on our beliefs about par-human scientific reasoning itself.'</li><li>Likewise: 'If there's a year where explosives go from being unable to destroy cities to being able to destroy cities, we should estimate the impact of the first such explosives by drawing on our beliefs about how (current or future) physics might allow a city to be destroyed, and what other effects or side-effects such a process might have. We should spend little or no time thinking about the impactfulness of the first steam engine or the first telescope.'</li></ul></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][3:14]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><p>Seems like your argument is something like \"when there's a zero-to-one transition, then you have to make predictions based on reasoning about the technology itself\". I think in that case I'd say this thing from above:</p><blockquote><p>My impression is that Eliezer thinks that \"general intelligence\" is a qualitatively different sort of thing than that-which-neural-nets-are-doing, and maybe that's what's analogous to \"entirely new physics\". I'm pretty unconvinced of this, but something in this genre feels quite crux-y for me.</p></blockquote><p>(Like, you wouldn't a priori expect anything special to happen once conventional bombs become big enough to demolish a football stadium for the first time. It's because nukes are based on \"totally new physics\" that you might expect unprecedented new impacts from nukes. What's the analogous thing for AGI? Why isn't AGI just regular AI but scaled up in a way that's pretty continuous?)</p><p>I'm curious if you'd change your mind if you were convinced that AGI is just regular AI scaled up, with no qualitatively new methods -- I expect you wouldn't but idk why</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][4:03]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><p>In my own head, the way I think of 'AGI' is basically: \"Something happened that allows humans to do biochemistry, materials science, particle physics, etc., even though none of those things were present in our environment of evolutionary adaptedness. Eventually, AI will similarly be able to generalize to biochemistry, materials science, particle physics, etc. We can call that kind of AI 'AGI'.\"</p><p>There might be facts I'm unaware of that justify conclusions like 'AGI is mostly just a bigger version of current ML systems like GPT-3', and there might be facts that justify conclusions like 'AGI will be preceded by a long chain of predecessors, each slightly less general and slightly less capable than its successor'.</p><p>But if so, I'm assuming those will be facts about CS, human cognition, etc., not at all a list of a hundred facts like 'the first steam engine didn't take over the world', 'the first telescope didn't take over the world'.... Because the physics of brains doesn't care about those things, and because in discussing brains we're already in 'things that have been known to take over the world' territory.</p><p>(I think that paying much attention <i>at all</i> to the technology-wide base rate for 'does this allow you to take over the world?', once you already know you're doing something like 'inventing a new human', doesn't really make sense at all? It sounds to me like going to a bookstore and then repeatedly worrying 'What if they don't have the book I'm looking for? Most stores don't sell books at all, so this one might not have the one I want.' If you know it's a <i>book</i> store, then you shouldn't be thinking at that level of generality at all; the base rate just goes out the window.)</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure><p>My way of thinking about AGI is pretty different from saying AGI follows 'totally new mystery physics' -- I'm explicitly anchoring to a known phenomenon, humans.</p><p>The analogous thing for nukes might be 'we're going to build a bomb that uses processes kind of like the ones found in the Sun in order to produce enough energy to destroy (at least) a city'.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][0:44]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><blockquote><p>The analogous thing for nukes might be 'we're going to build a bomb that uses processes kind of like the ones found in the Sun in order to produce enough energy to destroy (at least) a city'.</p></blockquote><p>(And I assume the contentious claim is \"that bomb would then ignite the atmosphere, destroy the world, or otherwise have hugely more impact than just destroying a city\".)</p><p>In 1800, we say \"well, we'll probably just make existing fires / bombs bigger and bigger until they can destroy a city, so we shouldn't expect anything particularly novel or crazy to happen\", and assign (say) 5% to the claim.</p><p>There is a wrinkle: you said it was processes like the ones found in the Sun. Idk what the state of knowledge was like in 1800, but maybe they knew that the Sun couldn't be a conventional fire. If so, then they could update to a higher probability.</p><p>(You could also infer that since someone bothered to mention \"processes like the ones found in the Sun\", those processes must be ones we don't know yet, which also allows you to make that update. I'm going to ignore that effect, but I'll note that this is one way in which the phrasing of the claim is incorrectly pushing you in the direction of \"assign higher probability\", and I think a similar thing happens for AI when saying \"processes like those in the human brain\".)</p><p>With AI I don't see why the human brain is a different kind of thing than (say) convnets. So I feel more inclined to just take the starting prior of 5%.</p><p>Presumably you think that assigning 5% to the nukes claim in 1800 was incorrect, even if that perspective doesn't know that the Sun is not just a very big conventional fire. I'm not sure why this is. According to me this is just the natural thing to do because things are usually continuous and so in the absence of detailed knowledge that's what your prior should be. (If I had to justify this, I'd point to facts about bridges and buildings and materials science and so on.)</p><blockquote><p>there might be facts that justify conclusions like 'AGI will be preceded by a long chain of slightly-less-general, slightly-less-capable successors'.</p></blockquote><p>The frame of \"justify[ing] conclusions\" seems to ask for more confidence than I expect to get. Rather I feel like I'm setting an initial prior that could then be changed radically by engaging with details of the technology. And then I'm further saying that I don't see any particular details that should cause me to update away significantly (but they could arise in the future).</p><p>For example, suppose I have a random sentence generator, and I take the first well-formed claim it spits out. (I'm using a random sentence generator so that we don't update on the process by which the claim was generated.) This claim turns out to be \"Alice has a fake skeleton hidden inside her home\". Let's say we know nothing about Alice except that she is a real person somewhere in the US who has a home. You can still assign &lt; 10% probability to the claim, and take 10:1 bets with people who don't know any additional details about Alice. Nonetheless, as you learn more about Alice, you could update towards higher probability, e.g. if you learn that she loves Halloween, that's a modest update; if you learn she runs a haunted house at Halloween every year, that's a large update; if you go to her house and see the fake skeleton you can update to ~100%. That's the sort of situation I feel like we're in with AI.</p><p>If you asked me what facts justify the conclusion that Alice probably doesn't have a fake skeleton hidden inside her house, I could only point to reference classes, and all the other people I've met who don't have such skeletons. This is not engaging with the details of Alice's situation, and I could similarly say \"if I wanted to know about Alice, surely I should spend most of my time learning about Alice, rather than looking at what Bob and Carol did\". Nonetheless, it is still correct to assign &lt; 10% to the claim.</p><p>It really does seem to come down to -- why is human-level intelligence such a special turning point that should receive special treatment? Just as you wouldn't give special treatment to \"the first time bridges were longer than 10m\", it doesn't seem obvious that there's anything all that special at the point where AIs reach human-level intelligence (at least for the topics we're discussing; there are obvious reasons that's an important point when talking about the economic impact of AI)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Tallinn][7:04]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>FWIW, my current 1-paragraph compression of the debate positions is something like:</p><p><strong>catastrophists</strong>: when evolution was gradually improving hominid brains, suddenly something clicked - it stumbled upon the core of general reasoning - and hominids went from banana classifiers to spaceship builders. hence we should expect a similar (but much sharper, given the process speeds) discontinuity with AI.</p><p><strong>gradualists</strong>: no, there was no discontinuity with hominids per se; human brains merely reached a threshold that enabled cultural accumulation (and in a meaningul sense it was <i>culture</i> that built those spaceships). similarly, we should not expect sudden discontinuities with AI per se, just an accelerating (and possibly unfavorable to humans) cultural changes as human contributions will be automated away.</p><p>—</p><p>one possible crux to explore is “how thick is culture”: is it something that AGI will quickly decouple from (dropping directly to physics-based ontology instead) OR will culture remain AGI’s main environment/ontology for at least a decade.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][11:18]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><blockquote><p>FWIW, my current 1-paragraph compression of the debate positions is something like:</p><p><strong>catastrophists</strong>: when evolution was gradually improving hominid brains, suddenly something clicked - it stumbled upon the core of general reasoning - and hominids went from banana classifiers to spaceship builders. hence we should expect a similar (but much sharper, given the process speeds) discontinuity with AI.</p><p><strong>gradualists</strong>: no, there was no discontinuity with hominids per se; human brains merely reached a threshold that enabled cultural accumulation (and in a meaningul sense it was <i>culture</i> that built those spaceships). similarly, we should not expect sudden discontinuities with AI per se, just an accelerating (and possibly unfavorable to humans) cultural changes as human contributions will be automated away.</p><p>—</p><p>one possible crux to explore is “how thick is culture”: is it something that AGI will quickly decouple from (dropping directly to physics-based ontology instead) OR will culture remain AGI’s main environment/ontology for at least a decade.</p></blockquote><p>Clarification: in the sentence \"just an accelerating (and possibly unfavorable to humans) cultural changes as human contributions will be automated away\", what work is \"cultural changes\" doing? Could we just say \"changes\" (including economic, cultural, etc) instead?</p><blockquote><p>In my own head, the way I think of 'AGI' is basically: \"Something happened that allows humans to do biochemistry, materials science, particle physics, etc., even though none of those things were present in our environment of evolutionary adaptedness. Eventually, AI will similarly be able to generalize to biochemistry, materials science, particle physics, etc. We can call that kind of AI 'AGI'.\"</p><p>There might be facts I'm unaware of that justify conclusions like 'AGI is mostly just a bigger version of current ML systems like GPT-3', and there might be facts that justify conclusions like 'AGI will be preceded by a long chain of predecessors, each slightly less general and slightly less capable than its successor'.</p><p>But if so, I'm assuming those will be facts about CS, human cognition, etc., not at all a list of a hundred facts like 'the first steam engine didn't take over the world', 'the first telescope didn't take over the world'.... Because the physics of brains doesn't care about those things, and because in discussing brains we're already in 'things that have been known to take over the world' territory.</p><p>(I think that paying much attention <i>at all</i> to the technology-wide base rate for 'does this allow you to take over the world?', once you already know you're doing something like 'inventing a new human', doesn't really make sense at all? It sounds to me like going to a bookstore and then repeatedly worrying 'What if they don't have the book I'm looking for? Most stores don't sell books at all, so this one might not have the one I want.' If you know it's a <i>book</i> store, then you shouldn't be thinking at that level of generality at all; the base rate just goes out the window.)</p></blockquote><p>I'm broadly sympathetic to the idea that claims about AI cognition should be weighted more highly than claims about historical examples. But I think you're underrating historical examples. There are at least three ways those examples can be informative - by telling us about:</p><p>1. Domain similarities</p><p>2. Human effort and insight</p><p>3. Human predictive biases</p><p>You're mainly arguing against 1, by saying that there are facts about physics, and facts about intelligence, and they're not very related to each other. This argument is fairly compelling to me (although it still seems plausible that there are deep similarities which we don't understand yet - e.g. the laws of statistics, which apply to many different domains).</p><p>But historical examples can also tell us about #2 - for instance, by giving evidence that great leaps of insight are rare, and so if there exists a path to AGI which doesn't require great leaps of insight, that path is more likely than one which does.</p><p>And they can also tell us about #3 - for instance, by giving evidence that we usually overestimate the differences between old and new technologies, and so therefore those same biases might be relevant to our expectations about AGI.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][12:31]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>In the 'alien warns about nukes' example, my intuition is that 'great leaps of insight are rare' and 'a random person is likely to overestimate the importance of the first steam engines and telescopes' tell me practically nothing, compared to what even a small amount of high-uncertainty physics reasoning tells me.</p><p>The 'great leap of insight' part tells me ~nothing because even if there's an easy low-insight path to nukes and a hard high-insight path, I don't thereby know the explosive yield of a bomb on either path (either absolutely or relatively); it depends on how nukes work.</p><p>Likewise, I don't think 'a random person is likely to overestimate the first steam engine' really helps with estimating the power of nuclear explosions. I could <i>imagine</i> a world where this bias exists and is so powerful and inescapable it ends up being a big weight on the scales, but I don't think we live in that world?</p><p>I'm not even sure that a random person <i>would</i> overestimate the importance of prototypes in general. Probably, I guess? But my intuition is still that you're better off in 1800 focusing on physics calculations rather than the tug-of-war 'maybe X is cognitively biasing me in <i>this</i> way, no wait maybe Y is cognitively biasing me in this other way, no wait...'</p><p>Our situation might not be analogous to the 1800-nukes scenario (e.g., maybe we know by observation that current ML systems are basically scaled-down humans). But if it <i>is</i> analogous, then I think the history-of-technology argument is not very useful here.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Tallinn][13:00]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>re “cultural changes”: yeah, sorry, i meant “culture” in very general “substrate of human society” sense. “cultural changes” would then include things like changes in power structures and division of labour, but <i>not</i> things like “diamondoid bacteria killing all humans in 1 second” (that would be a change in humans, not in the culture)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][13:09]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>I want to note that I agree with your (Rob's) latest response, but I continue to think most of the action is in whether AGI involves something analogous to \"totally new physics\", where I would guess \"no\" (and would do so particularly strongly for shorter timelines).</p><p>(And I would still point to historical examples for \"many new technologies don't involve something analogous to 'totally new physics'\", and I'll note that Richard's #2 about human effort and insight still applies)</p></td></tr></tbody></table>\n\n12.2. Yudkowsky on Steve Jobs and gradualism\n--------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:26]</strong>&nbsp;<strong> (Sep. 28)</strong>&nbsp;</p><p>So recently I was talking with various people about the question of why, for example, Steve Jobs could not find somebody else with UI taste 90% as good as his own, to take over Apple, even while being able to pay infinite money. A successful founder I was talking to was like, \"Yep, I sure would pay $100 million to hire somebody who could do 80% of what I can do, in fact, people have earned more than that for doing less.\"</p><p>I wondered if OpenPhil was an exception to this rule, and people with more contact with OpenPhil seemed to think that OpenPhil did not have 80% of a Holden Karnofsky (besides Holden).</p><p>And of course, what sparked this whole thought process in me, was that I'd staked all the effort I put into the Less Wrong sequences, into the belief that if I'd managed to bring myself into existence, then there ought to be lots of young near-Eliezers in Earth's personspace including some with more math talent or physical stamina not so unusually low, who could be started down the path to being Eliezer by being given a much larger dose of concentrated hints than I got, starting off the compounding cascade of skill formations that I saw as having been responsible for producing me, \"on purpose instead of by accident\".</p><p>I see my gambit as having largely failed, just like the successful founder couldn't pay $100 million to find somebody 80% similar in capabilities to himself, and just like Steve Jobs could not find anyone to take over Apple for presumably much larger amounts of money and status and power. Nick Beckstead had some interesting stories about various ways that Steve Jobs had tried to locate successors (which I wasn't even aware of).</p><p>I see a plausible generalization as being a \"Sparse World Hypothesis\": The shadow of an Earth with eight billion people, projected into some dimensions, is much sparser than plausible arguments might lead you to believe. Interesting people have few neighbors, even when their properties are collapsed and projected onto lower-dimensional tests of output production. The process of forming an interesting person passes through enough 0-1 critical thresholds that all have to be passed simultaneously in order to start a process of gaining compound interest in various skills, that they then cannot find other people who are 80% as good as what they <i>do</i> (never mind being 80% similar to them as people).</p><p>I would expect human beings to start out much denser in a space of origins than AI projects, and for the thresholds and compounding cascades of our mental lives to be much less sharp than chimpanzee-human gaps.</p><p>Gradualism about humans sure sounds totally reasonable! It is in fact much more plausible-sounding a priori than the corresponding proposition about AI projects! I staked years of my own life on the incredibly reasoning-sounding theory that if one actual Eliezer existed then there should be lots of neighbors near myself that I could catalyze into existence by removing some of the accidental steps from the process that had accidentally produced me.</p><p>But it didn't work in real life because plausible-sounding gradualist arguments just... plain don't work in real life even though they sure sound plausible. I spent a lot of time arguing with Robin Hanson, who was more gradualist than I was, and was taken by surprise when reality itself was much less gradualist than I was.</p><p>My model has Paul or Carl coming back with some story about how, why, no, it is totally reasonable that Steve Jobs couldn't find a human who was 90% as good at a problem class as Steve Jobs to take over Apple for billions of dollars despite looking, and, why, no, this is not at all a falsified retroprediction of the same gradualist reasoning that says a leading AI project should be inside a dense space of AI projects that projects onto a dense space of capabilities such that it has near neighbors.</p><p>If so, I was not able to use this hypothetical model of <i>selective</i> gradualist reasoning to deduce in advance that replacements for myself would be sparse in the same sort of space and I'd end up unable to replace myself.</p><p>I do not really believe that, without benefits of hindsight, the advance predictions of gradualism would differ between the two cases.</p><p>I think if you don't peek at the answer book in advance, the same sort of person who finds it totally reasonable to expect successful AI projects to have close lesser earlier neighbors, would also find it totally reasonable to think that Steve Jobs definitely ought to be able to find somebody 90% as good to take over his job - and should actually be able to find somebody <i>much</i> better because Jobs gets to run a wider search and offer more incentive than when Jobs was wandering into early involvement in Apple.</p><p>It's completely reasonable-sounding! Totally plausible to a human ear! Reality disagrees. Jobs tried to find a successor, couldn't, and now the largest company in the world by market cap seems no longer capable of sending the iPhones back to the designers and asking them to do something important differently.</p><p>This is part of the story for why I put gradualism into a mental class of \"arguments that sound plausible and just fail in real life to be binding on reality; reality says 'so what' and goes off to do something else\".</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][17:46] &nbsp;(Sep. 28)</strong>&nbsp;</p><p>It feels to me like a common pattern is: I say that ML in particular, and most technologies in general, seem to improve quite gradually on metrics that people care about or track. You say that some kind of \"gradualism\" worldview predicts a bunch of other stuff (some claim about markets or about steve jobs or whatever that feels closely related on your view but not mine). But it feels to me like there are just a ton of technologies, and a ton of AI benchmarks, and those are just <i>much</i> more analogous to \"future AI progress.\" I know that to you this feels like reference class tennis, but I think I legitimately don't understand what kind of approach to forecasting you are using that lets you just make (what I see as) the obvious boring prediction about all of the non-AGI technologies.</p><p>Perhaps you are saying that symmetrically you don't understand what approach to forecasting I'm using, that would lead me to predict that technologies improve gradually yet people vary greatly in their abilities. To me it feels like the simplest thing in the world: I expect future technological progress in domain X to be like past progress in domain X, and future technological progress to be like past technological progress, and future market moves to be like past market moves, and future elections to be like past elections.</p><p>And it seems like you <i>must</i> be doing something that ends up making almost the same predictions as that almost all the time, which is why you don't get incredibly surprised every single year by continuing boring and unsurprising progress in batteries or solar panels or robots or ML or computers or microscopes or whatever. Like it's fine if you say \"Yes, those areas have trend breaks sometimes\" but there are <i>so many</i> boring years that you must somehow be doing something like having the baseline \"this year is probably going to be boring.\"</p><p>Such that intuitively it feels to me like the disagreement between us <i>must</i> be in the part where AGI feels to me like it is similar to AI-to-date and feels to you like it is very different and better compared to evolution of life or humans.</p><p>It has to be the kind of argument that you can make about progress-of-AI-on-metrics-people-care-about, but <i>not</i> progress-of-other-technologies-on-metrics-people-care-about, otherwise it seems like you are getting hammered every boring year for every boring technology.</p><p>I'm glad we have the disagreement on record where I expect ML progress to continue to get less jumpy as the field grows, and maybe the thing to do is just poke more at that since it is definitely a place where I gut level expect to win bayes points and so could legitimately change my mind on the \"which kinds of epistemic practices work better?\" question. But it feels like it's not the main action, the main action has got to be about you thinking that there is a really impactful change somewhere between {modern AI, lower animals} and {AGI, humans} that doesn't look like ongoing progress in AI.</p><p>I think \"would GPT-3 + 5 person-years of engineering effort foom?\" feels closer to core to me.</p><p>(That said, the way AI could be different need not feel like \"progress is lumpier,\" could totally be more like \"Progress is always kind of lumpy, which Paul calls 'pretty smooth' and Eliezer calls 'pretty lumpy' and doesn't lead to any disagreements; but Eliezer thinks AGI is different in that kind-of-lumpy progress leads to fast takeoff, while Paul thinks it just leads to kind-of-lumpy increases in the metrics people care about or track.\")</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][7:46] &nbsp;(Sep. 29)</strong>&nbsp;</p><blockquote><p>I think \"would GPT-3 + 5 person-years of engineering effort foom?\" feels closer to core to me.</p></blockquote><p>I truly and legitimately cannot tell which side of this you think we should respectively be on. My guess is you're against GPT-3 fooming because it's too low-effort and a short timeline, even though I'm the one who thinks GPT-3 isn't on a smooth continuum with AGI??</p><p>With that said, the rest of this feels on-target to me; I sure do feel like {natural selection, humans, AGI} form an obvious set with each other, though even there the internal differences are too vast and the data too scarce for legit outside viewing.</p><blockquote><p>I truly and legitimately cannot tell which side of this you think we should respectively be on. My guess is you're against GPT-3 fooming because it's too low-effort and a short timeline, even though I'm the one who thinks GPT-3 isn't on a smooth continuum with AGI??</p></blockquote><p>I mean I obviously think you can foom starting from an empty Python file with 5 person-years of effort if you've got the Textbook From The Future; you wouldn't use the GPT code or model for anything in that, the Textbook says to throw it out and start over.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][9:45] &nbsp;(Sep. 29)</strong>&nbsp;</p><p>I think GPT-3 will foom given very little engineering effort, it will just be much slower than the human foom</p><p>and then that timeline will get faster and faster over time</p><p>it's also fair to say that it wouldn't foom because the computers would break before it figured out how to repair them (and it would run out of metal before it figured out how to mine it, etc.), depending on exactly how you define \"foom,\" but the point is that \"you can repair the computers faster than they break\" happens much before you can outrun human civilization</p><p>so the relevant threshold you cross is the one where you are outrunning civilization</p><p>(and my best guess about human evolution is pretty similar, it looks like humans are smart enough to foom over a few hundred thousand years, and that we were the ones to foom because that is also roughly how long it was taking evolution to meaningfully improve our cognition---if we foomed slower it would have instead been a smarter successor who overtook us, if we foomed faster it would have instead been a dumber predecessor, though this is <i>much</i> less of a sure-thing than the AI case because natural selection is not trying to make something that fooms)</p><p>and regarding {natural selection, humans, AGI} the main question is why modern AI and homo erectus (or even chimps) aren't in the set</p><p>it feels like the core disagreement is that I mostly see a difference in degree between the various animals, and between modern AI and future AI, a difference that is likely to be covered by gradual improvements that are pretty analogous to contemporary improvements, and so as the AI community making contemporary improvements grows I get more and more confident that TAI will be a giant industry rather than an innovation</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][5:45]</strong>&nbsp;<strong> (Oct. 1)</strong>&nbsp;</p><p>Do you have a source on Jobs having looked hard for a successor who wasn't Tim Cook?</p><p>Also, I don't have strong opinions about how well Apple is doing now, so I default to looking at the share price, which seems very healthy.</p><p>(Although I note in advance that this doesn't feel like a particularly important point, roughly for the same reason that Paul mentioned: gradualism about Steve Jobs doesn't seem like a central example of the type of gradualism that informs beliefs about AI development.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][10:40]</strong> &nbsp;<strong>(Oct. 1)</strong>&nbsp;</p><p>My source is literally \"my memory of stuff that Nick Beckstead just said to me in person\", maybe he can say more if we invite him.</p><p>I'm not quite sure what to do with the notion that \"gradualism about Steve Jobs\" is somehow less to be expected than gradualism about AGI projects. Humans are GIs. They are <i>extremely</i> similar to each other design-wise. There are a <i>lot</i> of humans, billions of them, many many many more humans than I expect AGI projects. Despite this the leading edge of human-GIs is sparse enough in the capability space that there is no 90%-of-Steve-Jobs that Jobs can locate, and there is no 90%-of-von-Neumann known to 20th century history. If we are not to take any evidence about this to A-GIs, then I do not understand the rules you're using to apply gradualism to some domains but not others.</p><p>And to be explicit, a skeptic who doesn't find these divisions intuitive, might well ask, \"Is gradualism perhaps isomorphic to 'The coin always comes up heads on Heady occasions', where 'Heady' occasions are determined by an obscure intuitive method going through some complicated nonverbalizable steps one of which is unfortunately 'check whether the coin actually came up heads'?\"</p><p>(As for my own theory, it's always been that AGIs are mostly like AGIs and not very much like humans or the airplane-manufacturing industry, and I do not, on my own account of things, appeal much to supposed outside viewing or base rates.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][11:11]</strong> &nbsp;<strong>(Oct. 2)</strong>&nbsp;</p><p>I think the way to apply it is to use observable data (drawn widely) and math.</p><p>Steve Jobs does look like a (high) draw (selected for its height, in the sparsest tail of the CEO distribution) out of the economic and psychometric literature (using the same kind of approach I use in other areas like estimating effects of introducing slightly superhuman abilities on science, the genetics of height, or wealth distributions). You have roughly normal or log-normal distributions on some measures of ability (with fatter tails when there are some big factors present, e.g. super-tall people are enriched for normal common variants for height but are more frequent than a Gaussian estimated from the middle range because of some weird disease/hormonal large effects). And we have lots of empirical data about the thickness and gaps there. Then you have a couple effects that can make returns in wealth/output created larger.</p><p>You get amplification from winner-take-all markets, IT, and scale that let higher ability add value to more places. This is the same effect that lets top modern musicians make so much money. Better CEOs get allocated to bigger companies because multiplicative management decisions are worth more in big companies. Software engineering becomes more valuable as the market for software grows.</p><p>Wealth effects are amplified by multiplicative growth (noise in a given period multiplies wealth for the rest of the series, and systematic biases from abilities can grow exponentially or superexponentially over a lifetime), and there are some versions of that in gaining expensive-to-acquire human capital (like fame for Hollywood actors, or experience using incredibly expensive machinery or companies).</p><p>And we can read off the distributions of income, wealth, market share, lead time in innovations, scientometrics, etc.</p><p>That sort of data lead you to expect cutting edge tech to be months to a few years ahead of followers, winner-take-all tech markets to a few leading firms and often a clearly dominant one (but not driving an expectation of being able to safely rest on laurels for years while others innovate without a moat like network effects). That's one of my longstanding arguments with Robin Hanson, that his model has more even capabilities and market share for AGI/WBE than typically observed (he says that AGI software will have to be more diverse requiring more specialized companies, to contribute so much GDP).</p><p>It is tough to sample for extreme values on multiple traits at once, superexponentially tough as you go out or have more criteria. CEOs of big companies are smarter than average, taller than average, have better social skills on average, but you can't find people who are near the top on several of those.</p><p><a href=\"https://www.hbs.edu/ris/Publication%20Files/16-044_9c05278e-9d11-4315-a744-de008edf4d80.pdf\">https://www.hbs.edu/ris/Publication%20Files/16-044_9c05278e-9d11-4315-a744-de008edf4d80.pdf</a></p><p>Correlations between the things help, but it's tough. E.g. if you have thousands of people in a class on a measure of cognitive skill, and you select on only partially correlated matters of personality, interest, motivation, prior experience, etc, the math says it gets thin and you'll find different combos (and today we see more representation of different profiles of abilities, including rare and valuable ones, in this community)</p><p>I think the bigger update for me from trying to expand high-quality save the world efforts has been on the funny personality traits/habits of mind that need to be selected and their scarcity.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Karnofsky][11:30]</strong> &nbsp;<strong>(Oct. 2)</strong>&nbsp;</p><p>A cpl comments, without commitment to respond to responses:</p><p>1. Something in the zone of \"context / experience / obsession\" seems important for explaining the Steve Jobs type thing. It seems to me that people who enter an area early tend to maintain an edge even over more talented people who enter later - examples are not just founder/CEO types but also early employees of some companies who are more experienced with higher-level stuff (and often know the history of how they got there) better than later-entering people.</p><p>2. I'm not sure if I am just rephrasing something Carl or Paul has said, but something that bugs me a lot about the Rob/Eliezer arguments is that I feel like if I accept &gt;5% probability for the kind of jump they're talking about, I don't have a great understanding of how I avoid giving &gt;5% to a kajillion other claims from various startups that they're about to revolutionize their industry, in ways that seem inside-view plausible and seem to equally \"depend on facts about some physical domain rather than facts about reference classes.\"</p><p>The thing that actually most comes to mind here is Thiel - he has been a phenomenal investor financially, but he has also invested by now in a lot of \"atoms\" startups with big stories about what they might do, and I don't think any have come close to reaching those visions (though they have sometimes made $ by doing something orders of magnitude less exciting).</p><p>If a big crux here is \"whether Thielian secrets exist\" this track record could be significant.</p><p>I think I might update if I had a cleaner sense of how I could take on this kind of \"Well, if it is just a fact about physics that I have no idea about, it can't be that unlikely\" view without then betting on a lot of other inside-view-plausible breakthroughs that haven't happened. Right now all I can say to imitate this lens is \"General intelligence is 'different'\"</p><p>I don't feel the same way about \"AI might take over the world\" - I feel like I have good reasons this applies to AI and not a bunch of other stuff</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][11:11]</strong> &nbsp;<strong>(Oct. 2)</strong>&nbsp;</p><p>Ok, a few notes from me (feel free to ignore):</p><p>1. It seems to me like the convo here is half attempting-to-crux and half attempting-to-distill-out-a-bet. I'm interested in focusing explicitly on cruxing for the time being, for whatever that's worth. (It seems to me like y'all're already trending in that direction.)</p><p>2. It seems to me that one big revealed difference between the Eliezerverse and the Paulverse is something like:</p><ul><li>In the Paulverse, we already have basically all the fundamental insights we need for AGI, and now it's just a matter of painstaking scaling.</li><li>In the Eliezerverse, there are large insights yet missing (and once they're found we have plenty of reason to expect things to go quickly).</li></ul><p>For instance, in Eliezerverse they say \"The Wright flyer didn't need to have historical precedents, it was allowed to just start flying. Similarly, the AI systems of tomorrow are allowed to just start GIing without historical precedent.\", and in the Paulverse they say \"The analog of the Wright flyer has already happened, it was Alexnet, we are now in the phase analogous to the slow grinding transition from human flight to commercially viable human flight.\"</p><p>(This seems to me like basically what Ajeya articulated <a href=\"https://www.lesswrong.com/posts/fS7Zdj2e2xMqE6qja/more-christiano-cotra-and-yudkowsky-on-ai-progress\">upthread</a>.)</p><p>3. It seems to me that another revealed intuition-difference is in the difficulty that people have operating each other's models. This is evidenced by, eg, Eliezer/Rob saying things like \"I don't know how to operate the gradualness model without making a bunch of bad predictions about Steve Jobs\", and Paul/Holden responding with things like \"I don't know how to operate the secrets-exist model without making a bunch of bad predictions about material startups\".</p><p>I'm not sure whether this is a shallower or deeper disagreement than (2). I'd be interested in further attempts to dig into the questions of how to operate the models, in hopes that the disagreement looks interestingly different once both parties can at least operate the other model.</p><figure class=\"table\"><table><tbody><tr><td>[Tallinn: ➕]</td></tr></tbody></table></figure></td></tr></tbody></table>",
      "plaintextDescription": "This post is a transcript of a multi-day discussion between Paul Christiano, Richard Ngo, Eliezer Yudkowsky, Rob Bensinger, Holden Karnofsky, Rohin Shah, Carl Shulman, Nate Soares, and Jaan Tallinn, following up on the Yudkowsky/Christiano debate in 1, 2, 3, and 4.\n\n \n\nColor key:\n\n Chat by Paul, Richard, and Eliezer  Other chat \n\n \n\n\n12. Follow-ups to the Christiano/Yudkowsky conversation\n \n\n\n12.1. Bensinger and Shah on prototypes and technological forecasting\n \n\n[Bensinger][16:22]  (Sep. 23) \n\nQuoth Paul:\n\n> seems like you have to make the wright flyer much better before it's important, and that it becomes more like an industry as that happens, and that this is intimately related to why so few people were working on it\n\nIs this basically saying 'the Wright brothers didn't personally capture much value by inventing heavier-than-air flying machines, and this was foreseeable, which is why there wasn't a huge industry effort already underway to try to build such machines as fast as possible.' ?\n\nMy maybe-wrong model of Eliezer says here 'the Wright brothers knew a (Thielian) secret', while my maybe-wrong model of Paul instead says:\n\n * They didn't know a secret -- it was obvious to tons of people that you could do something sorta like what the Wright brothers did and thereby invent airplanes; the Wright brothers just had unusual non-monetary goals that made them passionate to do a thing most people didn't care about.\n * Or maybe it's better to say: they knew some specific secrets about physics/engineering, but only because other people correctly saw 'there are secrets to be found here, but they're stamp-collecting secrets of little economic value to me, so I won't bother to learn the secrets'. ~Everyone knows where the treasure is located, and ~everyone knows the treasure won't make you rich.\n\n[Yudkowsky][17:24]  (Sep. 23) \n\nMy model of Paul says there could be a secret, but only because the industry was tiny and the invention was nearly worthless directly.\n\n[Cotra: ➕]",
      "wordCount": 9280
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "sPpZRaxpNNJjw55eu",
        "name": "Progress Studies",
        "slug": "progress-studies"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b0",
        "name": "Technological Forecasting",
        "slug": "technological-forecasting"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fS7Zdj2e2xMqE6qja",
    "title": "More Christiano, Cotra, and Yudkowsky on AI progress",
    "slug": "more-christiano-cotra-and-yudkowsky-on-ai-progress",
    "url": null,
    "baseScore": 91,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2021-12-06T20:33:12.164Z",
    "contents": {
      "markdown": "This post is a transcript of a discussion between Paul Christiano, Ajeya Cotra, and Eliezer Yudkowsky (with some comments from Rob Bensinger, Richard Ngo, and Carl Shulman), continuing from [1](https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds), [2](https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-shulman-and-yudkowsky-on-ai-progress), and [3](https://www.lesswrong.com/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress).\n\nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Paul and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td></tr></tbody></table>\n\n10.2. Prototypes, historical perspectives, and betting\n------------------------------------------------------\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][4:25]</strong>&nbsp;</p><p>I feel confused about the role \"innovations are almost always low-impact\" plays in slow-takeoff-ish views.</p><p>Suppose I think that there's some reachable algorithm that's different from current approaches, and can do par-human scientific reasoning without requiring tons of compute.</p><p>The existence or nonexistence of such an algorithm is just a fact about the physical world. If I imagine one universe where such an algorithm exists, and another where it doesn't, I don't see why I should expect that one of those worlds has more discontinuous change in GWP, ship sizes, bridge lengths, explosive yields, etc. (outside of any discontinuities caused by the advent of humans and the advent of AGI)? What do these CS facts have to do with the other facts?</p><p>But AI Impacts seems to think there's an important connection, and a large number of facts of the form 'steamships aren't like nukes' seem to undergird a lot of Paul's confidence that the scenario I described --</p><p>(\"there's some reachable algorithm that's different from current approaches, and can do par-human scientific reasoning without requiring tons of compute.\")</p><p>-- is crazy talk. (Unless I'm misunderstanding. As seems actually pretty likely to me!)</p><p>(E.g., Paul says \"To me your model just seems crazy, and you are saying it predicts crazy stuff at the end but no crazy stuff beforehand\", and one of the threads of the timelines conversation has been Paul asking stuff like \"do you want to give any example other than nuclear weapons of technologies with the kind of discontinuous impact you are describing?\".)</p><p>Possibilities that came to mind for me:</p><p>1. The argument is 'reality keeps surprising us with how continuous everything else is, so we seem to have a cognitive bias favoring discontinuity, so we should have a skeptical prior about <i>our ability to think our way to 'X is discontinuous'</i> since our brains are apparently too broken to do that well?</p><p>(But to get from 1 to 'discontinuity models are batshit' we surely need something more probability-mass-concentrating than just a bias argument?)</p><p>2. The commonality between steamship sizes, bridge sizes, etc. and AGI is something like 'how tractable is the world?'. A highly tractable world, one whose principles are easy to understand and leverage, will tend to have more world-shatteringly huge historical breakthroughs in various problems, <i>and</i> will tend to see a larger impact from the advent of humans and the advent of AGI.</p><p>Our world looks much less tractable, so even if there's a secret sauce to building AGI, we should expect the resultant AGI to be a lot less impactful.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][5:06]</strong>&nbsp;</p><p>I endorse #2 (although I think more weakly than Paul does) and would also add #3: another commonality is something like \"how competitive is innovation?\"</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][8:22]</strong>&nbsp;</p><p>@RobBensinger It's showing us a fact about the vast space of ideas and technologies we've already explored that they are not so concentrated and lumpy that the law of large numbers doesn't work well as a first approximation in a world with thousands or millions of people contributing. And that specifically includes past computer science innovation.</p><p>So the 'we find a secret sauce algorithm that causes a massive unprecedented performance jump, without crappier predecessors' is a 'separate, additional miracle' at exactly the same time as the intelligence explosion is getting going. You can get hyperbolic acceleration from increasing feedbacks from AI to AI hardware and software, including crazy scale-up at the end, as part of a default model. But adding on to it that AGI is hit via an extremely large performance jump of a type that is very rare, takes a big probability penalty.</p><p>And the history of human brains doesn't seem to provide strong evidence of a fundamental software innovation, vs hardware innovation and gradual increases in selection applied to cognition/communication/culture.</p><p>The fact that, e.g. AIs are mastering so much math and language while still wielding vastly infrahuman brain-equivalents, and crossing human competence in many domains (where there was ongoing effort) over decades is significant evidence for something smoother than the development of modern humans and their culture.</p><p>That leaves me not expecting a simultaneous unusual massive human concentrated algorithmic leap with AGI, although I expect wildly accelerating progress from increasing feedbacks at that time. Crossing a given milestone is disproportionately likely to happen in the face of an unusually friendly part/jump of a tech tree (like AlexNet/the neural networks-&gt;GPU transition) but still mostly not, and likely not from an unprecedented in computer science algorithmic change.</p><p><a href=\"https://aiimpacts.org/?s=cross+\">https://aiimpacts.org/?s=cross+</a></p><figure class=\"table\"><table><tbody><tr><td>[Cotra: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:26][11:37]</strong>&nbsp;</p><blockquote><p>The existence or nonexistence of such an algorithm is just a fact about the physical world. If I imagine one universe where such an algorithm exists, and another where it doesn't, I don't see why I should expect that one of those worlds has more discontinuous change in GWP, ship sizes, bridge lengths, explosive yields, etc. (outside of any discontinuities caused by the advent of humans and the advent of AGI)? What do these CS facts have to do with the other facts?</p></blockquote><p>I want to flag strong agreement with this. I am not talking about change in ship sizes because that is relevant in any visible way on my model; I'm talking about it in hopes that I can somehow unravel Carl and Paul's model, which talks a whole lot about this being Relevant even though that continues to not seem correlated to me across possible worlds.</p><p>I think a lot in terms of \"does this style of thinking seem to have any ability to bind to reality\"? A lot of styles of thinking in futurism just don't.</p><p>I imagine Carl and Paul as standing near the dawn of hominids asking, \"Okay, let's try to measure how often previous adaptations resulted in simultaneous fitness improvements across a wide range of environmental challenges\" or \"what's the previous record on an organism becoming more able to survive in a different temperature range over a 100-year period\" or \"can we look at the variance between species in how high they fly and calculate how surprising it would be for a species to make it out of the atmosphere\"</p><p>And all of reality is standing somewhere else, going on ahead to do its own thing.</p><p>Now maybe this is not the Carl and Paul viewpoint but if so I don't understand how not. It's not that viewpoint plus a much narrower view of relevance, because AI Impacts got sent out to measure bridge sizes.</p><p>I go ahead and talk about these subjects, in part because maybe I can figure out some way to unravel the viewpoint on its own terms, in part because maybe Carl and Paul can show that they have a style of thinking that works in its own right and that I don't understand, and in part because people like Paul's nonconcrete cheerful writing better and prefer to live there mentally and I have to engage on their terms because they sure won't engage on mine.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p>But I do not actually think that bridge lengths or atomic weapons have anything to do with this.</p><p>Carl and Paul may be doing something sophisticated but wordless, where they fit a sophisticated but wordless universal model of technological permittivity to bridge lengths, then have a wordless model of cognitive scaling in the back of their minds, then get a different prediction of Final Days behavior, then come back to me and say, \"Well, if you've got such a different prediction of Final Days behavior, can you show me some really large bridges?\"</p><p>But this is not spelled out in the writing - which, I do emphasize, is a social observation that would be predicted regardless, because other people have not invested a ton of character points in the ability to spell things out, and a supersupermajority would just plain lack the writing talent for it.</p><p>And what other EAs reading it are thinking, I expect, is plain old Robin-Hanson-style <a href=\"https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter\">reference class tennis</a> of \"Why would you expect <i>intelligence</i> to scale differently from <i>bridges</i>, where are all the <i>big bridges</i>?\"</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][11:36][11:40]</strong> &nbsp;</p><p>(Just want to interject that Carl has higher P(doom) than Paul and has also critiqued Paul for not being more concrete, and I doubt that this is the source of the common disagreements that Paul/Carl both have with Eliezer)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p>From my perspective the thing the AI impacts investigation is asking is something like \"When people are putting lots of resources into improving some technology, how often is it the case that someone can find a cool innovation that improves things a lot relative to the baseline?\" I think that your response to that is something like \"Sure, if the broad AI market were efficient and everyone were investigating the right lines of research, then AI progress might be smooth, but AGI would have also been developed way sooner. We can't safely assume that AGI is like an industry where lots of people are pushing toward the same thing\"</p><p>But it's not assuming a great structural similarity between bridges and AI, except that they're both things that humans are trying hard to find ways to improve</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:42]</strong>&nbsp;</p><p>I can imagine writing responses like that, if I was engaging on somebody else's terms. As with <a href=\"https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing\">Eliezer-2012's engagement with Pat Modesto</a> against the careful proof that HPMOR cannot possibly become one of the measurably most popular fanfictions, I would never think anything like that inside my own brain.</p><p>Maybe I just need to do a thing that I have not done before, and set my little $6000 Roth IRA to track a bunch of investments that Carl and/or Paul tell me to make, so that my brain will actually track the results, and I will actually get a chance to see this weird style of reasoning produce amazing results.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][11:44]</strong> &nbsp;</p><blockquote><p>Sure, if the broad AI market were efficient and everyone were investigating the right lines of research, then AI progress might be smooth</p></blockquote><p>Presumably also \"'AI progress' subsumes many different kinds of cognition, we don't currently have baby AGIs, and when we do figure out how to build AGI the very <i>beginning</i> of the curve (the Wright flyer moment, or something very shortly after) will correspond to a huge capability increase.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:46]</strong> &nbsp;</p><p>I think there's some much larger scale in which it's worth mentioning that on my own terms of engagement I do not naturally think like this. I don't feel like you could get Great Insight by figuring out what the predecessor technologies must have been of the Wright Flyer, finding industries that were making use of them, and then saying Behold the Heralds of the Wright Flyer. It's not a style of thought binding upon reality.</p><p>They built the Wright Flyer. It flew. Previous stuff didn't fly. It happens. Even if you yell a lot at reality and try to force it into an order, that's still what your actual experience of the surprising Future will be like, you'll just be more surprised by it.</p><p>Like you can super want Technologies to be Heralded by Predecessors which were Also Profitable but on my native viewpoint this is, like, somebody with a historical axe to grind, going back and trying to make all the history books read like this, when I have no experience of people who were alive at the time making gloriously correct futuristic predictions using this kind of thinking.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][11:53]</strong>&nbsp;</p><p>I think Paul's view would say:</p><ul><li>Things certainly happen for the first time</li><li>When they do, they happen at small scale in shitty prototypes, like the Wright Flyer or GPT-1 or AlphaGo or the Atari bots or whatever</li><li>When they're making a big impact on the world, it's after a lot of investment and research, like commercial aircrafts in the decades after Kitty Hawk or like the investments people are in the middle of making now with AI that can assist with coding</li></ul><p>Paul's view says that the Kitty Hawk moment <i>already happened for the kind of AI that will be super transformative and could kill us all</i>, and like the historical Kitty Hawk moment, it was not immediately a huge deal</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:56]</strong> &nbsp;</p><p>There is, I think, a really basic difference of thinking here, which is that on my view, AGI erupting is just a Thing That Happens and not part of a Historical Worldview or a Great Trend.</p><p>Human intelligence wasn't part of a grand story reflected in all parts of the ecology, it just happened in a particular species.</p><p>Now afterwards, of course, you can go back and draw all kinds of Grand Trends into which this Thing Happening was perfectly and beautifully fitted, and yet, it does not seem to me that people have a very good track record of thereby predicting in advance what surprising news story they will see next - with some rare, narrow-superforecasting-technique exceptions, like the Things chart on a steady graph and we <i>know solidly what a threshold on that graph corresponds to</i> and that threshold is not too far away compared to the previous length of the chart.</p><p>One day the Wright Flyer flew. Anybody <i>in the future with benefit of hindsight</i>, who wanted to, could fit that into a grand story about flying, industry, travel, technology, whatever; if they've been on the ground at the time, they would not have thereby had much luck predicting the Wright Flyer. It can be <i>fit into</i> a grand story but on the ground it's just a thing that happened. It had some prior causes but it was not thereby constrained to fit into a storyline in which it was the plot climax of those prior causes.</p><p>My worldview sure does permit there to be predecessor technologies and for them to have some kind of impact and for some company to make a profit, but it is not nearly as interested in that stuff, on a very basic level, because it does not think that the AGI Thing Happening is the plot climax of a story about the Previous Stuff Happening.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:01]</strong> &nbsp;</p><p>The fact that you express this kind of view about AGI erupting one day is why I thought your thing in IEM was saying there was a major algorithmic innovation <i>from chimps to humans</i>, that humans were qualitatively and not just quantitatively better than chimps and this was not because of their larger brain size primarily. But I'm confused because up thread in the discussion of evolution you were emphasizing much more that there was an innovation between dinosaurs and primates, not that there was an innovation between chimps and humans, and you seemed more open to the chimp/human diff being quantitative and brain-size driven than I had thought you'd be. But being open to the chimp-human diff being quantitative/brain-size-driven suggests to me that you should be more open than you are to AGI being developed by slow grinding on the same shit, instead of erupting without much precedent?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:01]</strong> &nbsp;</p><p>I think you're confusing a meta-level viewpoint with an object-level viewpoint.</p><p>The Wright Flyer does not need to be made out of completely different materials from all previous travel devices, in order for the Wright Flyer to be a Thing That Happened One Day which wasn't the plot climax of a grand story about Travel and which people at the time could not have gotten very far in advance-predicting by reasoning about which materials were being used in which conveyances and whether those conveyances looked like they'd be about to start flying.</p><p>It is the very viewpoint to which I am objecting, which keeps on asking me, metaphorically speaking, to explain how the Wright Flyer could have been made of completely different materials in order for it to be allowed to be so discontinuous with the rest of the Travel story of which it is part.</p><p>On my viewpoint they're just <i>different stories</i> so the Wright Flyer is allowed to be its own thing <i>even though</i> it is not made out of an unprecedented new kind of steel that floats.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:06]</strong> &nbsp;</p><p>The claim I'm making is that Paul's view predicts a lag and a lot of investment between the first flight and aircraft making a big impact on the travel industry, and predicts that the first flight wouldn't have immediately made a big impact on the travel industry. In other words Kitty Hawk isn't a discontinuity in the Paul view because the metrics he'd expect to be continuous are the ones that large numbers of people are trying hard to optimize, like cost per mile traveled or whatnot, not metrics that almost nobody is trying to optimize, like \"height flown.\"</p><p>In other words, it sounds like you're saying:</p><ul><li>Kitty Hawk is analogous to AGI erupting</li><li>Previous history of travel is analogous to pre-AGI history of AI</li></ul><p>While Paul is saying:</p><ul><li>Kitty Hawk is analogous to e.g. AlexNet</li><li>Later history of aircraft is analogous to the post-AlexNet story of AI which we're in the middle of living, and will continue on to make huge Singularity-causing impacts on the world</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:09]</strong> &nbsp;</p><p>Well, unfortunately, Paul and I both seem to believe that our models follow from observing the present-day world, rather than being incompatible with it, and so when we demand of each other that we produce some surprising bold prediction about the present-day world, we both tend to end up disappointed.</p><p>I would like, of course, for Paul's surprisingly narrow vision of a world governed by tightly bound stories and predictable trends, to produce some concrete bold prediction of the next few years which no ordinary superforecaster would produce, but Paul is not under the impression that his own worldview is similarly strange and narrow, and so has some difficulty in answering this request.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:09]</strong> &nbsp;</p><p>But Paul offered to bet with you about literally any quantity you choose?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:10]</strong> &nbsp;</p><p>I did assume that required an actual disagreement, eg, I cannot just go look up something superforecasters are very confident about and then demand Paul to bet against it.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:12]</strong> &nbsp;</p><p>It still sounds to me like \"take a basket of N performance metrics, bet that the model size to perf trend will break upward in &gt; K of them within e.g. 2 or 3 years\" should sound good to you, I'm confused why that didn't. If it does and it's just about the legwork then I think we could get someone to come up with the benchmarks and stuff for you</p><p>Or maybe the same thing but &gt;K of them will break downward, whatever</p><p>We could bet about the human perception of sense in language models, for example</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:14]</strong> &nbsp;</p><p>I am nervous about Paul's definition of \"break\" and the actual probabilities to be assigned. You see, both Paul and I think our worldview is a very normal one that matches current reality quite well, so when we are estimating parameters like these, Paul is liable to do it empirically, and I am also liable to do it empirically as my own baseline, and if I point to a trend over time in how long it takes to go from par-human to superhuman performance decreasing, Imaginary Paul says \"Ah, yes, what a fine trend, I will bet that things follow this trend\" and Eliezer says \"No that is MY trend, you don't get to follow it, you have to predict that par-human to superhuman time will be constant\" and Paul is like \"lol no I get to be a superforecaster and follow trends\" and we fail to bet.</p><p>Maybe I'm wrong in having mentally played the game out ahead that far, for it is, after all, very hard to predict the Future, but that's where I'd foresee it failing.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:16]</strong> &nbsp;</p><p>I don't think you need to bet about calendar times from par-human to super-human, and any meta-trend in that quantity. It sounds like Paul is saying \"I'll basically trust the model size to perf trends and predict a 10x bigger model from the same architecture family will get the perf the trends predict,\" and you're pushing back against that saying e.g. that humans won't find GPT-4 to be subjectively more coherent than GPT-3 and that Paul is neglecting that there could be major innovations in the future that bring down the FLOP/s to get a certain perf by a lot and bend the scaling laws. So why not bet that Paul won't be as accurate as he thinks he is by following the scaling laws?</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][12:17]</strong> &nbsp;</p><blockquote><p>I think Paul's view would say:</p><ul><li>Things certainly happen for the first time</li><li>When they do, they happen at small scale in shitty prototypes, like the Wright Flyer or GPT-1 or AlphaGo or the Atari bots or whatever</li><li>When they're making a big impact on the world, it's after a lot of investment and research, like commercial aircrafts in the decades after Kitty Hawk or like the investments people are in the middle of making now with AI that can assist with coding</li></ul><p>Paul's view says that the Kitty Hawk moment <i>already happened for the kind of AI that will be super transformative and could kill us all</i>, and like the historical Kitty Hawk moment, it was not immediately a huge deal</p></blockquote><p>\"When they do, they happen at small scale in shitty prototypes, like the Wright Flyer or GPT-1 or AlphaGo or the Atari bots or whatever\"</p><p>How shitty the prototype is should depend (to a very large extent) on the physical properties of the tech. So I don't find it confusing (though I currently disagree) when someone says \"I looked at a bunch of GPT-3 behavior and it's cognitively sophisticated enough that I think it's doing basically what humans are doing, just at a smaller scale. The qualitative cognition I can see going on is just that impressive, taking into account the kinds of stuff I think human brains are doing.\"</p><p>What I find confusing is, like, treating ten thousand examples of non-AI, non-cognitive-tech continuities (nukes, building heights, etc.) as though they're anything but a tiny update about 'will AGI be high-impact' -- compared to the size of updates like 'look at how smart and high-impact humans were' and perhaps 'look at how smart-in-the-relevant-ways GPT-3 is'.</p><p>Like, impactfulness is not a simple physical property, so there's not much reason for different kinds of tech to have similar scales of impact (or similar scales of impact n years after the first prototype). Mainly I'm not sure to what extent we disagree about this, vs. this just being me misunderstanding the role of the 'most things aren't high-impact' argument.</p><p>(And yeah, a random historical technology drawn from a hat will be pretty low-impact. But that base rate also doesn't seem to me like it has much evidential relevance anymore when I update about what specific tech we're discussing.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:18]</strong> &nbsp;</p><p>The question is not \"will AGI be high impact\" -- Paul agrees it will, and for any FOOM quantity (like crossing a chimp-to-human-sized gap in a day or whatever) he agrees that will happen eventually too.</p><p>The technologies studies in the dataset spanned a wide range in their peak impact on society, and they're not being used to forecast the peak impact of mature AI tech</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][12:19]</strong> &nbsp;</p><p>Yeah, I'm specifically confused about how we know that the AGI Wright Flyer and its first successors are low-impact, from looking at how low-impact other technologies are (if that is in fact a meaningful-sized update on your view)</p><p>Not drawing a comparison about the overall impactfulness of AI / AGI (e.g., over fifteen years)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:21]</strong> &nbsp;</p><blockquote><p>[So why not bet that Paul won't be as accurate as he thinks he is by following the scaling laws?]</p></blockquote><p>I'm pessimistic about us being able to settle on the terms of a bet like that (and even more so about being able to bet against Carl on it) but in broad principle I agree. The trouble is that if a trend is benchmarkable, I believe more in the trend continuing at least on the next particular time, not least because I believe in people Goodharting benchmarks.</p><p>I expect a human sense of intelligence to be harder to fool (even taking into account that it's being targeted to a nonzero extent) but I also expect that to be much harder to measure and bet upon than the Goodhartable metrics. And I think our actual disagreement is more visible over portfolios of benchmarks breaking upward over time, but I also expect that if you ask Paul and myself to <i>quantify our predictions</i>, we both go, \"Oh, my theory is the one that fits ordinary reality so obviously I will go look at superforecastery trends over ordinary reality to predict this specifically\" and I am like, \"No, Paul, if you'd had to predict that without looking at the data, your worldview would've predicted trends breaking down less often\" and Paul is like \"But Eliezer, shouldn't you be predicting much more upward divergence than this.\"</p><p>Again, perhaps I'm being overly gloomy.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:23]</strong> &nbsp;</p><p>I think we should try to find ML predictions where you defer to superforecasters and Paul disagrees, since he said he would bet against superforecasters in ML</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:24]</strong> &nbsp;</p><p>I am also probably noticeably gloomier and less eager to bet because the whole fight is taking place on grounds that Paul thinks is important and part of a connected story that continuously describes ordinary reality, and that I think is a strange place where I can't particularly see how Paul's reasoning style works. So I'd want to bet against Paul's overly narrow predictions by using ordinary superforecasting, and Paul would like to make his predictions using ordinary superforecasting.</p><p>I am, indeed, more interested in a place where Paul wants to bet against superforecasters. I am not guaranteeing up front I'll bet with them because superforecasters did not call AlphaGo correctly and I do not think Paul has zero actual domain expertise. But Paul is allowed to pick up <i>generic</i> epistemic credit <i>including from me</i> by beating superforecasters because that credit counts toward believing a style of thought is <i>even working literally at all</i>; separately from the question of whether Paul's superforecaster-defying prediction also looks like a place where I'd predict in some opposite direction.</p><p>Definitely, places where Paul disagrees with superforecasters are much more interesting places to mine for bets.</p><p>I am happy to hear about those.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][12:27]</strong> &nbsp;</p><p>I think what Paul was saying last night is you find superforecasters betting on some benchmark performance, and he just figures out which side he'd take (and he expects in most/all superforecaster predictions that he would not be deferential, there's a side he would take)</p></td></tr></tbody></table>\n\n10.3. Predictions and betting (continued)\n-----------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][12:29]</strong>&nbsp;</p><p>not really following along with the conversation, but my desire to bet about \"whatever you want\" was driven in significant part by frustration with Eliezer repeatedly saying things like \"people like Paul get surprised by reality\" and me thinking that's nonsense</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:29]</strong> &nbsp;</p><p>So the Yudkowskian viewpoint is something like... trends in particular technologies held fixed, will often break down; trends in Goodhartable metrics, will often stay on track but come decoupled from their real meat; trends across multiple technologies, will experience occasional upward breaks when new algorithms on the level of Transformers come out. For me to bet against superforecasters I have to see superforecasters saying something different, which I do not at this time actually know to be the case. For me to bet against Paul betting against superforecasters, the different thing Paul says has to be different from my own direction of disagreement with superforecasters.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][12:30]</strong> &nbsp;</p><p>I still think that if you want to say \"this sort of reasoning is garbage empirically\" then you ought to be willing to bet about something. If we are just saying \"we agree about all of the empirics, it's just that somehow we have different predictions about AGI\" then that's fine and symmetrical.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:30]</strong> &nbsp;</p><p>I have been trying to revise that towards a more nvc \"when I try to operate this style of thought myself, it seems to do a bad job of retrofitting and I don't understand how it says X but not Y\".</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][12:30]</strong> &nbsp;</p><p>even then presumably if you think it's garbage you should be able to point to some particular future predictions where it would be garbage?</p><p>if you used it</p><p>and then I can either say \"no, I don't think that's a valid application for reason X\" or \"sure, I'm happy to bet\"</p><p>and it's possible you can't find any places where it sticks its neck out in practice (even in your version), but then I'm again just rejecting the claim that it's empirically ruled out</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:31]</strong> &nbsp;</p><p>I also think that we'd have an easier time betting if, like, neither of us could look at graphs over time, but we were at least told the values in 2010 and 2011 to anchor our estimates over one year, or something like that.</p><p>Though we also need to not have a bunch of existing knowledge of the domain which is hard.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][12:32]</strong> &nbsp;</p><p>I think this might be derailing some broader point, but I am provisionally mostly ignoring your point \"this doesn't work in practice\" if we can't find places where we actually foresee disagreements</p><p>(which is fine, I don't think it's core to your argument)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][12:33]</strong> &nbsp;</p><p>Paul, you've previously said that you're happy to bet against ML superforecasts. That sounds promising. What are examples of those? Also I must flee to lunch and am already feeling sort of burned and harried; it's possible I should not ignore the default doomedness of trying to field questions from multiple sources.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][12:33]</strong> &nbsp;</p><p>I don't know if superforecasters make public bets on ML topics, I was saying I'm happy to bet on ML topics and if your strategy is \"look up what superforecasters say\" that's fine and doesn't change my willingness to bet</p><p>I think this is probably not as promising as either (i) dig in on the arguments that are most in dispute (seemed to be some juicier stuff earlier though I'm just focusing on work today) , or (ii) just talking generally about what we expect to see in the next 5 years so that we can at least get more of a vibe looking back</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][12:35]</strong> &nbsp;</p><p>You can bet on the Metaculus AI Tournament forecasts.</p><p><a href=\"https://www.metaculus.com/ai-progress-tournament/\">https://www.metaculus.com/ai-progress-tournament/</a></p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:13]</strong> &nbsp;</p><p>I worry that trying to jump straight ahead to Let's Bet is being too ambitious too early on a cognitively difficult problem of localizing disagreements.</p><p>Our prophecies of the End Times's modal final days seem legit different; my impulse would be to try to work that backwards, first, in an intuitive sense of \"well which prophesied world would this experience feel more like living in?\", and try to dig deeper there before deciding that our disagreements have crystallized into short-term easily-observable bets.</p><p>We both, weirdly enough, feel that our current viewpoints are doing a great job of permitting the present-day world, even if, presumably, we both think the other's worldview would've done worse at predicting that world in advance. This cannot be resolved in an instant by standard techniques known to me. Let's try working back from the End Times instead.</p><p>I have already stuck out my neck a little and said that, as we start to go past $50B invested in a model, we are starting to live at least a <i>little</i> more in what feels like the Paulverse, not because my model prohibits this, but because, or so I think, Paul's model more narrowly predicts it.</p><p>It does seem like the sort of generically weird big thing that could happen, to me, even before the End Times, there are corporations that could just decide to do that; I am hedging around this exactly because it does feel to my gut like that is a kind of headline I could read one day and have it still be years before the world ended, so I may need to be stingy with those credibility points inside of what I expect to be reality.</p><p>But if we get up to $10T to train a model, that is <i>much</i> more strongly Paulverse; it's not that this falsifies the Eliezerverse considered in isolation, but it is <i>much</i> more narrowly characteristic of the Words of Paul coming to pass; it feels much more to my gut that, in agreeing to this, I am not giving away Bayes points inside my own mainline.</p><p>If ordinary salaries for ordinary fairly-good programmers get up to $20M/year, this is not prohibited by my AI models per se; but it sure sounds like the world becoming less ordinary than I expected it to stay, and like it is part of Paul's Prophecy much more strongly than it is part of Eliezer's Prophecy.</p><p>That's two ways that I could concede a great victory to the Paulverse. They both have the disadvantages (from my perspective) that the Paulverse, though it must be drawing probability mass from somewhere in order to stake it there, is legitimately not - so far as I know - forced to claim that these things happen anytime soon. So they are ways for the Paulverse to win, but not ways for the Eliezerverse to win.</p><p>That I have said even this much, I claim, puts Paul in at least a little tiny bit of debt to me epistemic-good-behavior-wise; he should be able to describe events which would start to make him worry he was living in the Eliezerverse, even if his model did not narrowly rule them out, and even if those events had not been predicted by the Eliezerverse to occur within a narrowly prophesied date such that they would not thereby form a bet the Eliezerverse could clearly lose as well as win.</p><p>I have not had much luck in trying to guess what the real Paul will say about issues like this one. My last attempt was to say, \"Well, what shouldn't happen, besides the End Times themselves, before world GDP has doubled over a four-year period?\" And Paul gave what seems to me like an overly valid reply, which, iirc and without looking it up, was along the lines of, \"well, nothing that would double world GDP in a 1-year period\".</p><p>When I say this is overly valid, I mean that it follows too strongly from Paul's premises, and he should be looking for something less strong than that on which to make a beginning discovery of disagreement - maybe something which Paul's premises don't strongly forbid to him, but which nonetheless looks more like the Eliezerverse or like it would be relatively more strongly predicted by Eliezer's Prophecy.</p><p>I do not model Paul as eagerly or strongly agreeing with, say, \"The Riemann Hypothesis should not be machine-proven\" or \"The ABC Conjecture should not be machine-proven\" before world GDP has doubled. It is only on Eliezer's view that proving the Riemann Hypothesis is about as much of a related or unrelated story to AGI, as are particular benchmarks of GDP.</p><p>On Paul's view as I am trying to understand and operate it, this benchmark may be correlated with AGI in time in the sense that most planets wouldn't do it during the Middle Ages before they had any computers, but it is not part of the <i>story</i> of AGI, it is not part of Paul's Prophecy; because it doesn't make a huge amount of money and increase GDP and get a huge ton of money flowing into investments in useful AI.</p><p>(From Eliezer's perspective, you could tell a story about how a stunning machine proof of the Riemann Hypothesis got Bezos to invest $50 billion in training a successor model and that was how the world ended, and that would be a just-as-plausible model as some particular economic progress story, of how Stuff Happened Because Other Stuff Happened; it sounds like the story of OpenAI or of Deepmind's early Atari demo, which is to say, it sounds to Eliezer like history. Whereas on Eliezer!Paul's view, that's much more of a weird coincidence because it involves Bezos's unforced decision rather than the economic story of which AGI is capstone, or so it seems to me trying to operate Paul's view.)</p><p>And yet Paul might still, I hope, be able to find something <i>like</i> \"The Riemann Hypothesis is machine-proven\", which even though it is not very much of an interesting part of his own Prophecy because it's not part of the economic storyline, sounds to him like the sort of thing that the <i>Eliezerverse</i> thinks happens as you get close to AGI, which the <i>Eliezerverse</i> says is allowed to start happening way before world GDP would double in 4 years; and as it happens I'd agree with that characterization of the Eliezerverse.</p><p>So Paul might say, \"Well, my model doesn't particularly <i>forbid</i> that the Riemann Hypothesis gets machine-proven before world GDP has doubled in 4 years or even started to discernibly break above trend by much; but that does sound <i>more</i> like we are living in the Eliezerverse than in the Paulverse.\"</p><p>I am not demanding this particular bet because it seems to me that the Riemann Hypothesis may well prove to be unfairly targetable for current ML techniques while they are still separated from AGI by great algorithmic gaps. But if on the other hand Paul thinks that, I dunno, superhuman performance on stuff like the Riemann Hypothesis does tend to be more correlated with economically productive stuff because it's all roughly the same kind of capability, and lol never mind this \"algorithmic gap\" stuff, then maybe Paul <i>is</i> willing to pick that example; which is all the better for me because I <i>do</i> suspect it might decouple from the AI of the End, and so I think I have a substantial chance of winning and being able to say \"SEE!\" to the assembled EAs while there's still a year or two left on the timeline.</p><p>I'd love to have credibility points on that timeline, if Paul doesn't feel as strong an anticipation of needing them.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:43]</strong> &nbsp;</p><p>1/3 that RH has an automated proof before sustained 7%/year GWP growth?</p><p>I think the clearest indicator is that we have AI that ought to be able to e.g. run the fully automated factory-building factory (not automating mines or fabs, just the robotic manufacturing and construction), but it's not being deployed or is being deployed with very mild economic impacts</p><p>another indicator is that we have AI systems that can fully replace human programmers (or other giant wins), but total investment in improving them is still small</p><p>another indicator is a DeepMind demo that actually creates a lot of value (e.g. 10x larger than DeepMind's R&amp;D costs? or even comparable to DeepMind's cumulative R&amp;D costs if you do the accounting really carefully and I definitely believe it and it wasn't replaceable by Brain), it seems like on your model things should \"break upwards\" and in mine that just doesn't happen that much</p><p>sounds like you may have &gt;90% on automated proof of RH before a few years of 7%/year growth driven by AI? so that would give a pretty significant odds ratio either way</p><p>I think \"stack more layers gets stuck but a clever idea makes crazy stuff happen\" is generally going to be evidence for your view</p><p>That said, I'd mostly reject AlphaGo as an example, because it's just plugging in neural networks to existing go algorithms in almost the most straightforward way and the bells and whistles don't really matter. But if AlphaZero worked and AlphaGo didn't, and the system accomplished something impressive/important (like proving RH, or being significantly better at self-contained programming tasks), then that would be a surprise.</p><p>And I'd reject LSTM -&gt; transformer or MoE as an example because the quantitative effect size isn't that big.</p><p>But if something like that made the difference between \"this algorithm wasn't scaling before, and now it's scaling,\" then I'd be surprised.</p><p>And the size of jump that surprises me is shrinking over time. So in a few years even getting the equivalent of a factor of 4 jump from some clever innovation would be very surprising to me.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:44]</strong> &nbsp;</p><blockquote><p>sounds like you may have &gt;90% on automated proof of RH before a few years of 7%/year growth driven by AI? so that would give a pretty significant odds ratio either way</p></blockquote><p>I emphasize that this is mostly about no on the GDP growth before the world ending, rather than yes on the RH proof, i.e., I am not 90% on RH before the end of the world at all. Not sure I'm over 50% on it happening before the end of the world at all.</p><p>Should it be a consequence of easier earlier problems than full AGI? Yes, on my mainline model; but also on my model, it's a particular thing and maybe the particular people and factions doing stuff don't get around to that particular thing.</p><p>I guess if I stare hard at my brain it goes 'ehhhh maybe 65% if timelines are relatively long and 40% if it's like the next 5 years', because the faster stuff happens, the less likely anyone is to get around to proving RH in particular or announcing that they've done so if they did.</p><p>And if the econ threshold is set as low as 7%/yr, I start to worry about that happening in longer-term scenarios, just because world GDP has never been moving at a fixed rate over a log chart. the \"driven by AI\" part sounds very hard to evaluate. I want, I dunno, some other superforecaster or Carl to put a 90% credible bound on 'when world GDP growth hits 7% assuming little economically relevant progress in AI' before I start betting at 80%, let alone 90%, on what should happen before then. I don't have that credible bound already loaded and I'm not specialized in it.</p><p>I'm wondering if we're jumping ahead of ourselves by trying to make a nice formal Bayesian bet, as prestigious as that might be. I mean, your 1/3 was probably important for you to say, as it is higher than I might have hoped, and I'd ask you if you really mean for that to be an upper bound on your probability or if that's your actual probability.</p><p>But, more than that, I'm wondering if, in the same vague language I used before, you're okay with saying a little more weakly, \"RH proven before big AI-driven growth in world GDP, sounds more Eliezerverse than Paulverse.\"</p><p>It could be that this is just not actually true because you do not think that RH is coupled to econ stuff in the Paul Prophecy one way or another, and my own declarations above do not have the Eliezerverse saying it enough more strongly than that. If you don't actually see this as a distinguishing Eliezerverse thing, if it wouldn't actually make you say \"Oh no maybe I'm in the Eliezerverse\", then such are the epistemic facts.</p><blockquote><p>And the size of jump that surprises me is shrinking over time. So in a few years even getting the equivalent of a factor of 4 jump from some clever innovation would be very surprising to me.</p></blockquote><p>This sounds potentially more promising to me - seems highly Eliezerverse, highly non-Paul-verse according to you, and its negation seems highly oops-maybe-I'm-in-the-Paulverse to me too. How many years is a few? How large a jump is shocking if it happens tomorrow?</p></td></tr></tbody></table>\n\n11\\. September 24 conversation\n==============================\n\n11.1. Predictions and betting (continued 2)\n-------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][13:15]</strong>&nbsp;</p><p>I think RH is not that surprising, it's not at all clear to me where \"do formal math\" sits on the \"useful stuff AI could do\" spectrum, I guess naively I'd put it somewhere \"in the middle\" (though the analogy to board games makes it seem a bit lower, and there is a kind of obvious approach to doing this that seems to be working reasonably well so that also makes it seem lower), and 7% GDP growth is relatively close to the end (ETA: by \"close to the end\" I don't mean super close to the end, just far enough along that there's plenty of time for RH first)</p><p>I do think that performance jumps are maybe more dispositive, but I'm afraid that it's basically going to go like this: there won't be metrics that people are tracking that jump up, but you'll point to new applications that people hadn't considered before, and I'll say \"but those new applications aren't that valuable\" whereas to you they will look more analogous to a world-ending AGI coming out from the blue</p><p>like for AGZ I'll be like \"well it's not really above the deep learning trend if you run it backwards\" and you'll be like \"but no one was measuring it before! you can't make up the trend in retrospect!\" and I'll be like \"OK, but the reason no one was measuring it before was that it was worse than traditional go algorithms until like 2 years ago and the upside is not large enough that you should expect a huge development effort for a small edge\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:43]</strong> &nbsp;</p><p>\"factor of 4 jump from some clever innovation\" - can you say more about that part?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][13:53]</strong> &nbsp;</p><p>like I'm surprised if a clever innovation does more good than spending 4x more compute</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:04]</strong> &nbsp;</p><p>I worry that I'm misunderstanding this assertion because, as it stands, it sounds extremely likely that I'd win. Would transformers vs. CNNs/RNNs have won this the year that the transformers paper came out?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:07]</strong> &nbsp;</p><p>I'm saying that it gets harder over time, don't expect wins as big as transformers</p><p>I think even transformers probably wouldn't make this cut though?</p><p>certainly not vs CNNs</p><p>vs RNNs I think the comparison I'd be using to operationalize it is translation, as measured in the original paper</p><p>they do make this cut for translation, looks like the number is like 100 &gt;&gt; 4</p><p>100x for english-german, more like 10x for english-french, those are the two benchmarks they cite</p><p>but both more than 4x</p><p>I'm saying I don't expect ongoing wins that big</p><p>I think the key ambiguity is probably going to be about what makes a measurement established/hard-to-improve</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:21]</strong> &nbsp;</p><p>this sounds like a potentially important point of differentiation; I do expect more wins that big.</p><p>the main thing that I imagine might make a big difference to your worldview, but not mine, is if the first demo of the big win only works slightly better (although that might also be because they were able to afford much less compute than the big players, which I think your worldview would see as a redeeming factor for my worldview?) but a couple of years later might be 4x or 10x as effective per unit compute (albeit that other innovations would've been added on by then to make the first innovation work properly, which I think on your worldview is like The Point or something)</p><p>clarification: by \"transformers vs CNNs\" I don't mean transformers on ImageNet, I mean transformers vs. contemporary CNNs, RNNs, or both, being used on text problems.</p><p>I'm also feeling a bit confused because eg Standard Naive Kurzweilian Accelerationism makes a big deal about the graphs keeping on track because technologies hop new modes as needed. what distinguishes your worldview from saying that no further innovations are needed for AGI or will give a big compute benefit along the way? is it that any single idea may only ever produce a smaller-than-4X benefit? is it permitted that a single idea plus 6 months of engineering fiddly details produce a 4X benefit?</p><p>all this aside, \"don't expect wins as big as transformers\" continues to sound to me like a very promising point for differentiating Prophecies.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:50]</strong> &nbsp;</p><p>I think the relevant feature of the innovation is that the work to find it is small relative to the work that went into the problem to date (though there may be other work on other avenues)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:52]</strong> &nbsp;</p><p>in, like, a local sense, or a global sense? if there's 100 startups searching for ideas collectively with $10B of funding, and one of them has an idea that's 10x more efficient per unit compute on billion-dollar problems, is that \"a small amount of work\" because it was only a $100M startup, or collectively an appropriate amount of work?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:53]</strong> &nbsp;</p><p>I'm calling that an innovation because it's a small amount of work</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:54]</strong> &nbsp;</p><p>(maybe it would be also productive if you pointed to more historical events like Transformers and said 'that shouldn't happen again', because I didn't realize there was anything you thought was like that. AlphaFold 2?)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:54]</strong> &nbsp;</p><p>like, it's not just a claim about EMH, it's also a claim about the nature of progress</p><p>I think AlphaFold counts and is probably if anything a bigger multiplier, it's just uncertainty over how many people actually worked on the baselines</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:54]</strong> &nbsp;</p><p>when should we see headlines like those subside?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:55]</strong> &nbsp;</p><p>I mean, I think they are steadily subsiding</p><p>as areas grow</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:55]</strong> &nbsp;</p><p>have they already begun to subside relative to 2016, on your view?</p><p>(guess that was ninjaed)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:55]</strong> &nbsp;</p><p>I would be surprised to see a 10x today on machine translation</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:55]</strong> &nbsp;</p><p>where that's 10x the compute required to get the same result?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:55]</strong> &nbsp;</p><p>though not so surprised that we can avoid talking about probabilities</p><p>yeah</p><p>or to make it more surprising, old sota with 10x less compute</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:56]</strong> &nbsp;</p><p>yeah I was about to worry that people wouldn't bother spending 10x the cost of a large model to settle our bet</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:56]</strong> &nbsp;</p><p>I'm more surprised if they get the old performance with 10x less compute though, so that way around is better on all fronts</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:57]</strong> &nbsp;</p><p>one reads papers claiming this all the time, though?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:57]</strong> &nbsp;</p><p>like, this view also leads me to predict that if I look at the actual amount of manpower that went into alphafold, it's going to be pretty big relative to the other people submitting to that protein folding benchmark</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:57]</strong> &nbsp;</p><p>though typically for the sota of 2 years ago</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:58]</strong> &nbsp;</p><p>not plausible claims on problems people care about</p><p>I think the comparison is to contemporary benchmarks from one of the 99 other startups who didn't find the bright idea</p><p>that's the relevant thing on your view, right?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:59]</strong> &nbsp;</p><p>I would expect AlphaFold and AlphaFold 2 to involve... maybe 20 Deep Learning researchers, and for 1-3 less impressive DL researchers to have been the previous limit, if the field even tried that much; I would not be the least surprised if DM spent 1000x the compute on AlphaFold 2, but I'd be very surprised if the 1-3 large research team could spend that 1000x compute and get anywhere near AlphaFold 2 results.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][15:59]</strong> &nbsp;</p><p>and then I'm predicting that number is already &lt;10 for machine translation and falling (maybe I shouldn't talk about machine translation or at least not commit to numbers given that I know very little about it, but whatever that's my estimate), and for other domains it will be &lt;10 by the time they get as crowded as machine translation, and for transformative tasks they will be &lt;2</p><p>isn't there an open-source replication of alphafold?</p><p>we could bet about its performance relative to the original</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:00]</strong> &nbsp;</p><p>it is enormously easier to do what's already been done</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:00]</strong> &nbsp;</p><p>I agree</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:00]</strong> &nbsp;</p><p>I believe the open-source replication was by people who were told roughly what Deepmind had done, possibly more than roughly</p><p>on the Yudkowskian view, those 1-3 previous researchers just would not have thought of doing things the way Deepmind did them</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:01]</strong> &nbsp;</p><p>anyway, my guess is generally that if you are big relative to previous efforts in the area you can make giant improvements, if you are small relative to previous efforts you might get lucky (or just be much smarter) but that gets increasingly unlikely as the field gets bigger</p><p>like alexnet and transformers are big wins by groups who are small relative to the rest of the field, but transformers are much smaller than alexnet and future developments will continue to shrink</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:02]</strong> &nbsp;</p><p>but if you're the <i>same size</i> as previous efforts and don't have 100x the compute, you shouldn't be able to get huge improvements in the Paulverse?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:03]</strong> &nbsp;</p><p>I mean, if you are the same size as all the prior effort put together?</p><p>I'm not surprised if you can totally dominate in that case, especially if prior efforts aren't well-coordinated</p><p>and for things that are done by hobbyists, I wouldn't be surprised if you can be a bit bigger than an individual hobbyist and dominate</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:03]</strong> &nbsp;</p><p>I'm thinking something like, if Deepmind comes out with an innovation such that it duplicates old SOTA on machine translation with 1/10th compute, that still violates the Paulverse because Deepmind is not Paul!Big compared to all MTL efforts</p><p>though I am not sure myself how seriously Earth is taking MTL in the first place</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:04]</strong> &nbsp;</p><p>yeah, I think if DeepMind beats Google Brain by 10x compute next year on translation, that's a significant strike against Paul</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:05]</strong> &nbsp;</p><p>I know that Google offers it for free, I expect they at least have 50 mediocre AI people working on it, I don't know whether or not they have 20 excellent AI people working on it and if they've ever tried training a 200B parameter non-MoE model on it</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:05]</strong> &nbsp;</p><p>I think not that seriously, but more seriously than 2016 and than anything else where you are seeing big swings</p><p>and so I'm less surprised than for TAI, but still surprised</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:06]</strong> &nbsp;</p><p>I am feeling increasingly optimistic that we have some notion of what it means to not be within the Paulverse! I am not feeling that we have solved the problem of having enough signs that enough of them will appear to tell EA how to notice which universe it is inside many years before the actual End Times, but I sure do feel like we are making progress!</p><p>things that have happened in the past that you feel <i>shouldn't happen again</i> are great places to poke for Eliezer-disagreements!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:07]</strong> &nbsp;</p><p>I definitely think there's a big disagreement here about what to expect for pre-end-of-days ML</p><p>but lots of concerns about details like what domains are crowded enough to be surprising and how to do comparisons</p><p>I mean, to be clear, I think the transformer paper having giant gains is also evidence against paulverse</p><p>it's just that there are really a lot of datapoints, and some of them definitely go against paul's view</p><p>to me it feels like the relevant thing for making the end-of-days forecast is something like \"how much of the progress comes from 'innovations' that are relatively unpredictable and/or driven by groups that are relatively small, vs scaleup and 'business as usual' progress in small pieces?\"</p></td></tr></tbody></table>\n\n11.2. Performance leap scenario\n-------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:09]</strong>&nbsp;</p><p>my heuristics tell me to try wargaming out a particular scenario so we can determine in advance which key questions Paul asks</p><p>in 2023, Deepmind releases an MTL program which is suuuper impressive. everyone who reads the MTL of, say, a foreign novel, or uses it to conduct a text chat with a contractor in Indonesia, is like, \"They've basically got it, this is about as good as a human and only makes minor and easily corrected errors.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:12]</strong> &nbsp;</p><p>I mostly want to know how good Google's translation is at that time; and if DeepMind's product is expensive or only shows gains for long texts, I want to know whether there is actually an economic niche for it that is large relative to the R&amp;D cost.</p><p>like I'm not sure whether anyone works at all on long-text translation, and I'm not sure if it would actually make Google $ to work on it</p><p>great text chat with contractor in indonesia almost certainly meets that bar though</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:14]</strong> &nbsp;</p><p>furthermore, Eliezer and Paul publicized their debate sufficiently to some internal Deepmind people who spoke to the right other people at Deepmind, that Deepmind showed a graph of loss vs. previous-SOTA methods, and Deepmind's graph shows that their thing crosses the previous-SOTA line while having used 12x less compute for <s>inference</s> training.</p><p>(note that this is less... salient?... on the Eliezerverse per se, than it is as an important issue and surprise on the Paulverse, so I am less confident about part.)</p><p>a nitpicker would note that previous-SOTA metric they used is however from 1 year previously and the new model also uses Sideways Batch Regularization which the 1-year-previous SOTA graph didn't use. on the other hand, they got 12x rather than 10x improvement so there was some error margin there.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:15]</strong> &nbsp;</p><p>I'm OK if they don't have the benchmark graph as long as they have some evaluation that other people were trying at, I think real-time chat probably qualifies</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:15]</strong> &nbsp;</p><p>but then it's harder to measure the 10x</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:15]</strong> &nbsp;</p><p><s>also I'm saying 10x less training compute, not inference (but 10x less inference compute is harder)</s></p><p>yes</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:15]</strong> &nbsp;</p><p>or to know that Deepmind didn't just use a bunch more compute</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:15]</strong> &nbsp;</p><p>in practice it seems almost certain that it's going to be harder to evaluate</p><p>though I agree there are really clean versions where they actually measured a benchmark other people work on and can compare training compute directly</p><p>(like in the transformer paper)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:16]</strong> &nbsp;</p><p>literally a pessimal typo, I meant to specify training vs. inference and somehow managed to type \"inference\" instead</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:16]</strong> &nbsp;</p><p>I'm more surprised by the clean version</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:17]</strong> &nbsp;</p><p>I literally don't know what you'd be surprised by in the unclean version</p><p>was GPT-2 beating the field hard enough that it would have been surprising if they'd only used similar amounts of training compute</p><p>?</p><p>and how would somebody else judge that for a new system?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:17]</strong> &nbsp;</p><p>I'd want to look at either human evals or logprob, I think probably not? but it's possible it was</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:19]</strong> &nbsp;</p><p>btw I also feel like the Eliezer model is more surprised and impressed by \"they beat the old model with 10x less compute\" than by \"the old model can't catch up to the new model with 10x more compute\"</p><p>the Eliezerverse thinks in terms of techniques that saturate</p><p>such that you have to find new techniques for new training to go on helping</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:19]</strong> &nbsp;</p><p>it's definitely way harder to win at the old task with 10x less compute</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:19]</strong> &nbsp;</p><p>but for expensive models it seems really genuinely unlikely to me that anyone will give us this data!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:19]</strong> &nbsp;</p><p>I think it's usually the case that if you scale up far enough past previous sota, you will be able to find tons of techniques needed to make it work at the new scale</p><p>but I'm expecting it to be less of a big deal because all experiments will be roughly at the frontier of what is feasible</p><p>and so the new thing won't be able to afford to go 10x bigger</p><p>unlike today when we are scaling up spending so fast</p><p>but this does make it harder for the next few years at least, which is maybe the key period</p><p>(it makes it hard if we are both close enough to the edge that \"10x cheaper to get old results\" seems unlikely but \"getting new results that couldn't be achieved with 10x more compute and old method\" seems likely)</p><p>what I basically expect is to (i) roughly know how much performance you get from making models 10x bigger, (ii) roughly know how much someone beat the competition, and then you can compare the numbers</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:22]</strong> &nbsp;</p><p>well, you could say, not in a big bet-winning sense, but in a mild trend sense, that if the next few years are full of \"they spent 100x more on compute in this domain and got much better results\" announcements, that is business as usual for the last few years and perfectly on track for the Paulverse; while the Eliezerverse permits but does not mandate that we will also see occasional announcements about brilliant new techniques, from some field where somebody already scaled up to the <s>big models</s> big compute, producing more impressive results than the previous big compute.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:23]</strong> &nbsp;</p><p>(but \"performance from making models 10x bigger\" depends a lot on exactly how big they were and whether you are in a regime with unfavorable scaling)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:23]</strong> &nbsp;</p><p>so the Eliezerverse must be putting at least a <i>little</i> less probability mass on business-as-before Paulverse</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:24]</strong> &nbsp;</p><p>I am also expecting a general scale up in ML training runs over time, though it's plausible that you also expect that until the end of days and just expect a much earlier end of days</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:24]</strong> &nbsp;</p><p>I mean, why wouldn't they?</p><p>if they're purchasing more per unit of compute, they will quite often spend more on total compute (Jevons Paradox)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:25]</strong> &nbsp;</p><p>that's going to kill the \"they spent 100x more compute\" announcements soon enough</p><p>like, that's easy when \"100x more\" means $1M, it's a bit hard when \"100x more\" means $100M, it's not going to happen except on the most important tasks when \"100x more\" means $10B</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:26]</strong> &nbsp;</p><p>the Eliezerverse is full of weird things that somebody could apply ML to, and doesn't have that many professionals who will wander down completely unwalked roads; and so is much more friendly to announcements that \"we tried putting a lot of work and compute into protein folding, since nobody ever tried doing that seriously with protein folding before, look what came out\" continuing for the next decade if the Earth lasts that long</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:27]</strong> &nbsp;</p><p>I'm not surprised by announcements like protein folding, it's not that the world overall gets more and more hostile to big wins, it's that any industry gets more and more hostile as it gets bigger (or across industries, they get more and more hostile as the stakes grow)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:28]</strong> &nbsp;</p><p>well, the Eliezerverse has more weird novel profitable things, because it has more weirdness; and more weird novel profitable things, because it has fewer people diligently going around trying all the things that will sound obvious in retrospect; but it also has fewer weird novel profitable things, because it has fewer novel things that are allowed to be profitable.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:29]</strong> &nbsp;</p><p>(I mean, the protein folding thing is a datapoint against my view, but it's not that much evidence and it's not getting bigger over time)</p><p>yeah, but doesn't your view expect more innovations for any given problem?</p><p>like, it's not just that you think the universe of weird profitable applications is larger, you also think AI progress is just more driven by innovations, right?</p><p>otherwise it feels like the whole game is about whether you think that AI-automating-AI-progress is a weird application or something that people will try on</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:30]</strong> &nbsp;</p><p>the Eliezerverse is more strident about there being lots and lots more stuff like \"ReLUs\" and \"batch normalization\" and \"transformers\" in the design space in principle, and less strident about whether current people are being paid to spend all day looking for them rather than putting their efforts someplace with a nice predictable payoff.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:31]</strong> &nbsp;</p><p>yeah, but then don't you see big wins from the next transformers?</p><p>and you think those just keep happening even as fields mature</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:31]</strong> &nbsp;</p><p>it's much more <i>permitted</i> in the Eliezerverse than in the Paulverse</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:31]</strong> &nbsp;</p><p>or you mean that they might slow down because people stop working on them?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:32]</strong> &nbsp;</p><p>this civilization has mental problems that I do not understand well enough to predict, when it comes to figuring out how they'll affect the field of AI as it scales</p><p>that said, I don't see us getting to AGI on Stack More Layers.</p><p>there may perhaps be a bunch of stacked layers in an AGI but there will be more ideas to it than that.</p><p>such that it would require far, far more than 10X compute to get the same results with a GPT-like architecture if that was literally possible</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:33]</strong> &nbsp;</p><p>it seems clear that it will be more than 10x relative to GPT</p><p>I guess I don't know what GPT-like architecture means, but from what you say it seems like normal progress would result in a non-GPT-like architecture</p><p>so I don't think I'm disagreeing with that</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:34]</strong> &nbsp;</p><p>I also don't think we're getting there by accumulating a ton of shallow insights; I expect it takes at least one more big one, maybe 2-4 big ones.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:34]</strong> &nbsp;</p><p>do you think transformers are a big insight?</p><p>(is adding soft attention to LSTMs a big insight?)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:34]</strong> &nbsp;</p><p>hard to deliver a verdict of history there</p><p>no</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:35]</strong> &nbsp;</p><p>(I think the intellectual history of transformers is a lot like \"take the LSTM out of the LSTM with attention\")</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:35]</strong> &nbsp;</p><p>\"how to train deep gradient descent without activations and gradients blowing up or dying out\" was a big insight</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:36]</strong> &nbsp;</p><p>that really really seems like the accumulation of small insights</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:36]</strong> &nbsp;</p><p>though the history of that big insight is legit complicated</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:36]</strong> &nbsp;</p><p>like, residual connections are the single biggest thing</p><p>and relus also help</p><p>and batch normalization helps</p><p>and attention is better than lstms</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:36]</strong> &nbsp;</p><p>and the inits help (like xavier)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:36]</strong> &nbsp;</p><p>you could also call that the accumulation of big insights, but the point is that it's an accumulation of a lot of stuff</p><p>mostly developed in different places</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:37]</strong> &nbsp;</p><p>but on the Yudkowskian view the biggest insight of all was the one waaaay back at the beginning where they were initing by literally unrolling Restricted Boltzmann Machines</p><p>and people began to say: <i>hey if we do this the activations and gradients don't blow up or die out</i></p><p>it is not a history that strongly distinguishes the Paulverse from Eliezerverse, because that insight took time to manifest</p><p>it was not, as I recall, the first thing that people said about RBM-unrolling</p><p>and there were many little or not-really-so-little inventions that sustained the insight to deeper and deeper nets</p><p>and those little inventions did not correspond to huge capability jumps immediately in the hands of their inventors, with, I think, the possible exception of transformers</p><p>though also I think back then people just didn't do as much SoTA-measuring-and-comparing</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:40]</strong> &nbsp;</p><p>(I think transformers are a significantly smaller jump than previous improvements)</p><p>also a thing we could guess about though</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:40]</strong> &nbsp;</p><p>right, but did the people who demoed the improvements demo them as big capability jumps?</p><p>harder to do when you don't have a big old well funded field with lots of eyes on SoTA claims</p><p>they weren't dense in SoTA, I think?</p><p>anyways, there has not, so far as I know, been an insight of similar size to that last one, since then</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:42]</strong> &nbsp;</p><p>also 10-100x is still actually surprising to me for transformers</p><p>so I guess lesson learned</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:43]</strong> &nbsp;</p><p>I think if you literally took pre-transformer SoTA, and the transformer paper plus the minimum of later innovations required to make transformers scale at all, then as you tried scaling stuff to GPT-1 scale, the old stuff would probably just flatly not work or asymptote?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:44]</strong> &nbsp;</p><p>in general if you take anything developed at scale X and try to scale it way past X I think it won't work</p><p>or like, it will work much worse than something that continues to get tweaked</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:44]</strong> &nbsp;</p><p>I'm not sure I understand what you mean if you mean \"10x-100x on transformers actually happened and therefore actually surprised me\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:44]</strong> &nbsp;</p><p>yeah, I mean that given everything I know I am surprised that transformers were as large as a 100x improvement on translation</p><p>in that paper</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:45]</strong> &nbsp;</p><p>though it may not help my own case, I remark that my generic heuristics say to have an assistant go poke a bit at that claim and see if your noticed confusion is because you are being more confused by fiction than by reality.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:45]</strong> &nbsp;</p><p>yeah, I am definitely interested to understand a bit better what's up there</p><p>but tentatively I'm sticking to my guns on the original prediction</p><p>if you have random 10-20 person teams getting 100x speedups versus prior sota</p><p>as we approach TAI</p><p>that's so far from paulverse</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:46]</strong> &nbsp;</p><p>like, not about this case specially, just sheer reflex from \"this assertion in a science paper is surprising\" to \"go poke at it\". many unsurprising and hence unpoked assertions will also be false, of course, but the surprising ones even more so on average.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:48]</strong> &nbsp;</p><p>anyway, seems like a good approach to finding a concrete disagreement</p><p>and even looking back at this conversation would be a start for diagnosing who is more right in hindsight</p><p>main thing is to say how quickly and in what industries I'm how surprised</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:49]</strong> &nbsp;</p><p>I suspect you want to attach conditions to that surprise? Like, the domain must be sufficiently explored OR sufficiently economically important, because Paulverse also predicts(?) that as of a few years (3?? 2??? 15????) all the economically important stuff will have been poked with lots of compute already.</p><p>and if there's economically important domains where nobody's tried throwing $50M at a model yet, that also sounds like not-the-Paulverse?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:50]</strong> &nbsp;</p><p>I think the economically important prediction doesn't really need that much of \"within a few years\"</p><p>like the total stakes have just been low to date</p><p>none of the deep learning labs are that close to paying for themselves</p><p>so we're not in the regime where \"economic niche &gt; R&amp;D budget\"</p><p>we are still in the paulverse-consistent regime where investment is driven by the hope of future wins</p><p>though paul is surprised that R&amp;D budgets aren't <i>more</i> larger than the economic value</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:51]</strong> &nbsp;</p><p>well, it's a bit of a shame from the Eliezer viewpoint that the Paulverse can't be falsifiable yet, then, considering that in the Eliezerverse it is allowed (but not mandated) for the world to end while most DL labs haven't paid for themselves.</p><p>albeit I'm not sure that's true of the present world?</p><p>DM had that thing about \"we just rejiggered cooling the server rooms for Google and paid back 1/3 of their investment in us\" and that was years ago.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:52]</strong> &nbsp;</p><p>I'll register considerable skepticism</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:53]</strong> &nbsp;</p><p>I don't claim deep knowledge.</p><p>But if the imminence, and hence strength and falsifiability, of Paulverse assertions, depend on how much money all the deep learning labs are making, that seems like something we could ask OpenPhil to measure?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][16:55]</strong> &nbsp;</p><p>it seems easier to just talk about ML tasks that people work on</p><p>it seems really hard to arbitrate the \"all the important niches are invested in\" stuff in a way that's correlated with takeoff</p><p>whereas the \"we should be making a big chunk of our progress from insights\" seems like it's easier</p><p>though I understand that your view could be disjunctive, of either \"AI will have hidden secrets that yield great intelligence,\" or \"there are hidden secret applications that yield incredible profit\"</p><p>(sorry that statement is crude / not very faithful)</p><p>should follow up on this in the future, off for now though</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:58]</strong> &nbsp;</p><p>👋</p></td></tr></tbody></table>",
      "plaintextDescription": "This post is a transcript of a discussion between Paul Christiano, Ajeya Cotra, and Eliezer Yudkowsky (with some comments from Rob Bensinger, Richard Ngo, and Carl Shulman), continuing from 1, 2, and 3.\n\n \n\nColor key:\n\n Chat by Paul and Eliezer  Other chat \n\n \n\n\n10.2. Prototypes, historical perspectives, and betting\n \n\n[Bensinger][4:25] \n\nI feel confused about the role \"innovations are almost always low-impact\" plays in slow-takeoff-ish views.\n\nSuppose I think that there's some reachable algorithm that's different from current approaches, and can do par-human scientific reasoning without requiring tons of compute.\n\nThe existence or nonexistence of such an algorithm is just a fact about the physical world. If I imagine one universe where such an algorithm exists, and another where it doesn't, I don't see why I should expect that one of those worlds has more discontinuous change in GWP, ship sizes, bridge lengths, explosive yields, etc. (outside of any discontinuities caused by the advent of humans and the advent of AGI)? What do these CS facts have to do with the other facts?\n\nBut AI Impacts seems to think there's an important connection, and a large number of facts of the form 'steamships aren't like nukes' seem to undergird a lot of Paul's confidence that the scenario I described --\n\n(\"there's some reachable algorithm that's different from current approaches, and can do par-human scientific reasoning without requiring tons of compute.\")\n\n-- is crazy talk. (Unless I'm misunderstanding. As seems actually pretty likely to me!)\n\n(E.g., Paul says \"To me your model just seems crazy, and you are saying it predicts crazy stuff at the end but no crazy stuff beforehand\", and one of the threads of the timelines conversation has been Paul asking stuff like \"do you want to give any example other than nuclear weapons of technologies with the kind of discontinuous impact you are describing?\".)\n\nPossibilities that came to mind for me:\n\n1. The argument is 'reality keeps surprising ",
      "wordCount": 11905
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b0",
        "name": "Technological Forecasting",
        "slug": "technological-forecasting"
      },
      {
        "_id": "KQP7fNjin8Zqg4N2x",
        "name": "Double-Crux",
        "slug": "double-crux"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sCCdCLPN9E3YvdZhj",
    "title": "Shulman and Yudkowsky on AI progress",
    "slug": "shulman-and-yudkowsky-on-ai-progress",
    "url": null,
    "baseScore": 90,
    "voteCount": 28,
    "viewCount": null,
    "commentCount": 16,
    "createdAt": null,
    "postedAt": "2021-12-03T20:05:22.552Z",
    "contents": {
      "markdown": "This post is a transcript of a discussion between Carl Shulman and Eliezer Yudkowsky, following up on [a conversation with Paul Christiano and Ajeya Cotra](https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-shulman-and-yudkowsky-on-ai-progress).\n\nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Carl and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td></tr></tbody></table>\n\n9.14. Carl Shulman's predictions\n--------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][20:30]</strong>&nbsp;</p><p>I'll interject some points re the earlier discussion about how animal data relates to the 'AI scaling to AGI' thesis.</p><p>1. In humans it's claimed the IQ-job success correlation varies by job, For a scientist or doctor it might be 0.6+, for a low complexity job more like 0.4, or more like 0.2 for simple repetitive manual labor. That presumably goes down a lot with less in the way of hands, or focused on low density foods like baleen whales or grazers. If it's 0.1 for animals like orcas or elephants, or 0.05, then there's 4-10x less fitness return to smarts.</p><p>2. But they outmass humans by more than 4-10x. Elephants 40x, orca 60x+. Metabolically (20 watts divided by BMR of the animal) the gap is somewhat smaller though, because of metabolic scaling laws (energy scales with 3/4 or maybe 2/3 power, so ).</p><p><a href=\"https://en.wikipedia.org/wiki/Kleiber%27s_law\">https://en.wikipedia.org/wiki/Kleiber%27s_law</a></p><p>If dinosaurs were poikilotherms, that's a 10x difference in energy budget vs a mammal of the same size, although there is debate about their metabolism.</p><p>3. If we're looking for an innovation in birds and primates, there's some evidence of 'hardware' innovation rather than 'software.' Herculano-Houzel reports in The Human Advantage (summarizing much prior work neuron counting) different observational scaling laws for neuron number with brain mass for different animal lineages.</p><blockquote><p>We were particularly interested in cellular scaling differences that might have arisen in primates. If the same rules relating numbers of neurons to brain size in rodents (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1805542/#idm139742175567824title\"><u>6</u></a>)</p><p>The brain of the capuchin monkey, for instance, weighing 52 g, contains &gt;3× more neurons in the cerebral cortex and ≈2× more neurons in the cerebellum than the larger brain of the capybara, weighing 76 g.</p></blockquote><p>[Editor’s Note: Quote source is “<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1805542/#idm139742175567824title\"><u>Cellular scaling rules for primate brains</u></a>.”]</p><p>In rodents brain mass increases with neuron count n^1.6, whereas it's close to linear (n^1.1) in primates. For cortex neurons and cortex mass 1.7 and 1.0. In general birds and primates are outliers in neuron scaling with brain mass.</p><p>Note also that bigger brains with lower neuron density have longer communication times from one side of the brain to the other. So primates and birds can have faster clock speeds for integrated thought than a large elephant or whale with similar neuron count.</p><p>4. Elephants have brain mass ~2.5x human, and 3x neurons, but 98% of those are in the cerebellum (vs 80% in or less in most animals; these are generally the tiniest neurons and seem to do a bunch of fine motor control). Human cerebral cortex has 3x the neurons of the elephant cortex (which has twice the mass). The giant cerebellum seems like controlling the very complex trunk.</p><p><a href=\"https://nautil.us/issue/35/boundaries/the-paradox-of-the-elephant-brain\">https://nautil.us/issue/35/boundaries/the-paradox-of-the-elephant-brain</a></p><p>Blue whales get close to human neuron counts with much larger brains.</p><p><a href=\"https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons\">https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons</a></p><p>5. As Paul mentioned, human brain volume correlation with measures of cognitive function after correcting for measurement error on the cognitive side is in the vicinity of 0.3-0.4 (might go a bit higher after controlling for non-functional brain volume variation, lower from removing confounds). The genetic correlation with cognitive function in this study is 0.24:</p><p><a href=\"https://www.nature.com/articles/s41467-020-19378-5\">https://www.nature.com/articles/s41467-020-19378-5</a></p><p>So it accounts for a minority of genetic influences on cognitive ability. We'd also expect a bunch of genetic variance that's basically disruptive mutations in mutation-selection balance (e.g. schizophrenia seems to be a result of that, with schizophrenia alleles under negative selection, but a big mutational target, with the standing burden set by the level of fitness penalty for it; in niches with less return to cognition the mutational surface will be cleaned up less frequently and have more standing junk).</p><p>Other sources of genetic variance might include allocation of attention/learning (curiosity and thinking about abstractions vs immediate sensory processing/alertness), length of childhood/learning phase, motivation to engage in chains of thought, etc.</p><p>Overall I think there's some question about how to account for the full genetic variance, but mapping it onto the ML experience with model size, experience and reward functions being key looks compatible with the biological evidence. I lean towards it, although it's not cleanly and conclusively shown.</p><p>Regarding economic impact of AGI, I do not buy the 'regulation strangles all big GDP boosts' story.</p><p>The BEA breaks down US GDP by industry here (page 11):</p><p><a href=\"https://www.bea.gov/sites/default/files/2021-06/gdp1q21_3rd_1.pdf\">https://www.bea.gov/sites/default/files/2021-06/gdp1q21_3rd_1.pdf</a></p><p>As I work through sectors and the rollout of past automation I see opportunities for large-scale rollout that is not heavily blocked by regulation. Manufacturing is still trillions of dollars, and robotic factories are permitted and produced under current law, with the limits being more about which tasks the robots work for at low enough cost (e.g. this stopped Tesla plans for more completely robotic factories). Also worth noting manufacturing is mobile and new factories are sited in friendly jurisdictions.</p><p>Software to control agricultural machinery and food processing is also permitted.</p><p>Warehouses are also low-regulation environments with logistics worth hundreds of billions of dollars. See Amazon's robot-heavy warehouses limited by robotics software.</p><p>Driving is hundreds of billions of dollars, and Tesla has been permitted to use Autopilot, and there has been a lot of regulator enthusiasm for permitting self-driving cars with humanlike accident rates. Waymo still hasn't reached that it seems and is lowering costs.</p><p>Restaurants/grocery stores/hotels are around a trillion dollars. Replacing humans in vision/voice tasks to take orders, track inventory (Amazon Go style), etc is worth hundreds of billions there and mostly permitted. Robotics cheap enough to replace low-wage labor there would also be valuable (although a lower priority than high-wage work if compute and development costs are similar).</p><p>Software is close to a half trillion dollars and the internals of software development are almost wholly unregulated.</p><p>Finance is over a trillion dollars, with room for AI in sales and management.</p><p>Sales and marketing are big and fairly unregulated.</p><p>In highly regulated and licensed professions like healthcare and legal services, you can still see a licensee mechanically administer the advice of the machine, amplifying their reach and productivity.</p><p>Even in housing/construction there's still great profits to be made by improving the efficiency of what construction is allowed (a sector worth hundreds of billions).</p><p>If you're talking about legions of super charismatic AI chatbots, they could be doing sales, coaching human manual labor to effectively upskill it, and providing the variety of activities discussed above. They're enough to more than double GDP, even with strong Baumol effects/cost disease, I'd say.</p><p>Although of course if you have AIs that can do so much the wages of AI and hardware researchers will be super high, and so a lot of that will go into the intelligence explosion, while before that various weaknesses that prevent full automation of AI research will also mess up activity in these other sectors to varying degrees.</p><p>Re discontinuity and progress curves, I think Paul is right. AI Impacts went to a lot of effort assembling datasets looking for big jumps on progress plots, and indeed nukes are an extremely high percentile for discontinuity, and were developed by the biggest spending power (yes other powers could have bet more on nukes, but didn't, and that was related to the US having more to spend and putting more in many bets), with the big gains in military power per $ coming with the hydrogen bomb and over the next decade.</p><p><a href=\"https://aiimpacts.org/category/takeoff-speed/continuity-of-progress/discontinuous-progress-investigation/\">https://aiimpacts.org/category/takeoff-speed/continuity-of-progress/discontinuous-progress-investigation/</a></p><p>For measurable hardware and software progress (Elo in games, loss on defined benchmarks), you have quite continuous hardware progress, and software progress that is on the same ballpark, and not drastically jumpy (like 10 year gains in 1), moreso as you get to metrics used by bigger markets/industries.</p><p>I also agree with Paul's description of the prior Go trend, and how DeepMind increased $ spent on Go software enormously. That analysis was a big part of why I bet on AlphaGo winning against Lee Sedol at the time (the rest being extrapolation from the Fan Hui version and models of DeepMind's process for deciding when to try a match).</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][21:38]</strong> &nbsp;</p><p>I'm curious about how much you think these opinions have been arrived at independently by yourself, Paul, and the rest of the OpenPhil complex?</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][21:44]</strong> &nbsp;</p><p>Little of Open Phil's opinions are independent of Carl, the source of all opinions</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: 😆]</td><td>[Ngo: 😆]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][21:44]</strong> &nbsp;</p><p>I did the brain evolution stuff a long time ago independently. Paul has heard my points on that front, and came up with some parts independently. I wouldn't attribute that to anyone else in that 'complex.'</p><p>On the share of the economy those are my independent views.</p><p>On discontinuities, that was my impression before, but the additional AI Impacts data collection narrowed my credences.</p><p>TBC on the brain stuff I had the same evolutionary concern as you, which was I investigated those explanations and they still are not fully satisfying (without more micro-level data opening the black box of non-brain volume genetic variance and evolution over time).</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][21:50]</strong> &nbsp;</p><p>so... when I imagine trying to deploy this style of thought myself to predict the recent past without benefit of hindsight, it returns a lot of errors. perhaps this is because I do not know how to use this style of thought, but.</p><p>for example, I feel like if I was GPT-continuing your reasoning from the great opportunities still available in the world economy, in early 2020, it would output text like:</p><p>\"There are many possible regulatory regimes in the world, some of which would permit rapid construction of mRNA-vaccine factories well in advance of FDA approval. Given the overall urgency of the pandemic some of those extra-USA vaccines would be sold to individuals or a few countries like Israel willing to pay high prices for them, which would provide evidence of efficacy and break the usual impulse towards regulatory uniformity among developed countries, not to mention the existence of less developed countries who could potentially pay smaller but significant amounts for vaccines. The FDA doesn't seem likely to actively ban testing; they might under a Democratic regime, but Trump is already somewhat ideologically prejudiced against the FDA and would go along with the probable advice of his advisors, or just his personal impulse, to override any FDA actions that seemed liable to prevent tests and vaccines from making the problem just go away.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][21:59]</strong> &nbsp;</p><p>Pharmaceuticals is a top 10% regulated sector, which is seeing many startups trying to apply AI to drug design (which has faced no regulatory barriers), which fits into the ordinary observed output of the sector. Your story is about regulation failing to improve relative to normal more than it in fact did (which is a dramatic shift, although abysmal relative to what would be reasonable).</p><p>That said, I did lose a 50-50 bet on US control of the pandemic under Trump (although I also correctly bet that vaccine approval and deployment would be historically unprecedently fast and successful due to the high demand).</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:02]</strong> &nbsp;</p><p>it's not impossible that Carl/Paul-style reasoning about the future - near future, or indefinitely later future? - would start to sound more reasonable to me if you tried writing out a modal-average concrete scenario that was full of the same disasters found in history books and recent news</p><p>like, maybe if hypothetically I knew how to operate this style of thinking, I would know how to add disasters automatically and adjust estimates for them; so you don't need to say that to Paul, who also hypothetically knows</p><p>but I do not know how to operate this style of thinking, so I look at your description of the world economy and it seems like an endless list of cheerfully optimistic ingredients and the recipe doesn't say how many teaspoons of disaster to add or how long to cook it or how it affects the final taste</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:06]</strong> &nbsp;</p><p>Like when you look at historical GDP stats and AI progress they are made up of a normal rate of insanity and screwups.</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:07]</strong> &nbsp;</p><p>on my view of reality, I'm the one who expects business-as-usual in GDP until shortly before the world ends, if indeed business-as-usual-in-GDP changes at all, and you have an optimistic recipe for Not That which doesn't come with an example execution containing typical disasters?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:07]</strong> &nbsp;</p><p>Things like failing to rush through neural network scaling over the past decade to the point of financial limitation on model size, insanity on AI safety, anti-AI regulation being driven by social media's role in politics.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:09]</strong>&nbsp;</p><p>failing to deploy 99% robotic cars to new cities using fences and electronic gates</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:09]</strong>&nbsp;</p><p>Historical growth has new technologies and stupid stuff messing it up.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:09]</strong> &nbsp;</p><p>so many things one could imagine doing with current tech, and yet, they are not done, anywhere on Earth</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:09]</strong> &nbsp;</p><p>AI is going to be incredibly powerful tech, and after a historically typical haircut it's still a lot bigger.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:09]</strong> &nbsp;</p><p>so some of this seems obviously driven by longer timelines in general</p><p>do you have things which, if they start to happen soonish and in advance of world GDP having significantly broken upward 3 years before then, cause you to say \"oh no I'm in the Eliezerverse\"?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:12]</strong> &nbsp;</p><p>You may be confusing my views and Paul's.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:12]</strong> &nbsp;</p><p>\"AI is going to be incredibly powerful tech\" sounds like long timelines to me, though?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:13]</strong> &nbsp;</p><p>No.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:13]</strong> &nbsp;</p><p>like, \"incredibly powerful tech for longer than 6 months which has time to enter the economy\"</p><p>if it's \"incredibly powerful tech\" in the sense of immediately killing everybody then of course we agree, but that didn't seem to be the context</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:15]</strong> &nbsp;</p><p>I think broadly human-level AGI means intelligence explosion/end of the world in less than a year, but tons of economic value is likely to leak out before that from the combination of worse general intelligence with AI advantages like huge experience.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:15]</strong> &nbsp;</p><p>my worldview permits but does not mandate a bunch of weirdly powerful shit that people can do a couple of years before the end, because that would sound like a typically messy and chaotic history-book scenario especially if it failed to help us in any way</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:15]</strong>&nbsp;</p><p>And the economic impact is increasing superlinearly (as later on AI can better manage its own introduction and not be held back by human complementarities on both the production side and introduction side).</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:16]</strong> &nbsp;</p><p>my worldview also permits but does not mandate that you get up to the chimp level, chimps are not very valuable, and once you can do fully AGI thought it compounds very quickly</p><p>it feels to me like the Paul view wants something narrower than that, a specific story about a great economic boom, and it sounds like the Carl view wants something that from my perspective seems similarly narrow</p><p>which is why I keep asking \"can you perhaps be specific about what would count as Not That and thereby point to the Eliezerverse\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:18]</strong> &nbsp;</p><p>We're in the Eliezerverse with huge kinks in loss graphs on automated programming/Putnam problems.</p><p>Not from scaling up inputs but from a local discovery that is much bigger in impact than the sorts of jumps we observe from things like Transformers.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:19]</strong> &nbsp;</p><p>...my model of Paul didn't agree with that being a prophecy-distinguishing sign to first order (to second order, my model of Paul agrees with Carl for reasons unbeknownst to me)</p><p>I don't think you need something very much bigger than Transformers to get sharp loss drops?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:19]</strong> &nbsp;</p><p>not the only disagreement</p><p>but that is a claim you seem to advance that seems bogus on our respective reads of the data on software advances</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:21]</strong> &nbsp;</p><p>but, sure, \"huge kinks in loss graphs on automated programming / Putnam problems\" sounds like something that is, if not mandated on my model, much more likely than it is in the Paulverse. though I am a bit surprised because I would not have expected Paul to be okay betting on that.</p><p>like, I thought it was an Eliezer-view unshared by Paul that this was a sign of the Eliezerverse.</p><p>but okeydokey if confirmed</p><p>to be clear I do not mean to predict those kinks in the next 3 years specifically</p><p>they grow in probability on my model as we approach the End Times</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:24]</strong> &nbsp;</p><p>I also predict that AI chip usage is going to keep growing at enormous rates, and that the buyers will be getting net economic value out of them. The market is pricing NVDA (up more than 50x since 2014) at more than twice Intel because of the incredible growth rate, and it requires more crazy growth to justify the valuation (but still short of singularity). Although NVDA may be toppled by other producers.</p><p>Similarly for increasing spending on model size (although slower than when model costs were &lt;$1M).</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:27]</strong> &nbsp;</p><p>relatively more plausible on my view, first because it's arguably already happening (which makes it easier to predict) and second because that can happen with profitable uses of AI chips which hover around on the economic fringes instead of feeding into core production cycles (waifutech)</p><p>it is easy to imagine massive AI chip usage in a world which rejects economic optimism and stays economically sad while engaging in massive AI chip usage</p><p>so, more plausible</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:28]</strong> &nbsp;</p><p>What's with the silly waifu example? That's small relative to the actual big tech company applications (where they quickly roll it into their software/web services or internal processes, which is not blocked by regulation and uses their internal expertise). Super chatbots would be used as salespeople, counselors, non-waifu entertainment.</p><p>It seems randomly off from existing reality.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:29]</strong>&nbsp;</p><p>seems more... optimistic, Kurzweilian?... to suppose that the tech gets used correctly the way a sane person would hope it would be used</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:29]</strong>&nbsp;</p><p>Like this is actual current use.</p><p>Hollywood and videogames alone are much bigger than anime, software is bigger than that, Amazon/Walmart logistics is bigger.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:31]</strong> &nbsp;</p><p>Companies using super chatbots to replace customer service they already hated and previously outsourced, with a further drop in quality, is permitted by the Dark and Gloomy Attempt To Realistically Continue History model</p><p>I am on board with wondering if we'll see sufficiently advanced videogame AI, but I'd point out that, again, that doesn't cycle core production loops harder</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:33]</strong> &nbsp;</p><p>OK, using an example of allowable economic activity that obviously is shaving off more than an order of magnitude on potential market is just misleading compared to something like FAANGSx10.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:34]</strong> &nbsp;</p><p>so, like, if I was looking for places that would break upward, I would be like \"universal translators that finally work\"</p><p>but I was also like that when GPT-2 came out and it hasn't happened even though you would think GPT-2 indicated we could get enough real understanding inside a neural network that you'd think, cognition-wise, it would suffice to do pretty good translation</p><p>there are huge current economic gradients pointing to the industrialization of places that, you might think, could benefit a lot from universal seamless translation</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:36]</strong> &nbsp;</p><p>Current translation industry is tens of billions, English learning bigger.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:36]</strong> &nbsp;</p><p>Amazon logistics are an interesting point, but there's the question of how much economic benefit is produced by automating all of it at once, Amazon cannot ship 10x as much stuff if their warehouse costs go down by 10x.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:37]</strong> &nbsp;</p><p>Definitely hundreds of billions of dollars of annual value created from that, e.g. by easing global outsourcing.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:37]</strong> &nbsp;</p><p>if one is looking for places where huge economic currents could be produced, AI taking down what was previously a basic labor market barrier, would sound as plausible to me as many other things</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:37]</strong> &nbsp;</p><p>Amazon has increased sales faster than it lowered logistics costs, there's still a ton of market share to take.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:37]</strong> &nbsp;</p><p>I am <i>able</i> to generate cheerful scenarios, eg if I need them for an SF short story set in the near future where billions of people are using AI tech on a daily basis and this has generated trillions in economic value</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][22:38]</strong> &nbsp;</p><p>Bedtime for me though.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:39]</strong> &nbsp;</p><p>I don't feel like particular cheerful scenarios like that have very much of a track record of coming <i>true</i>. I would not be shocked if the next GPT-jump permits that tech, and I would then not be shocked if use of AI translation actually did scale a lot. I would be much more impressed, with Earth having gone well for once and better than I expected, if that actually produced significantly more labor mobility and contributed to world GDP.</p><p>I just don't actively, &gt;50% expect things going right like that. It seems to me that more often in real life, things do not go right like that, even if it seems quite easy to imagine them going right.</p><p>good night!</p></td></tr></tbody></table>\n\n10\\. September 22 conversation\n==============================\n\n10.1. Scaling laws\n------------------\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][3:05]</strong>&nbsp;</p><p>My attempt at a reframing:</p><p>Places of agreement:</p><ul><li>Trend extrapolation / things done by superforecasters seem like the right way to get a first-pass answer</li><li>Significant intuition has to go into exactly which trends to extrapolate and why (e.g. should GDP/GWP be extrapolated as \"continue to grow at 3% per year\" or as \"growth rate continues to increase leading to singularity\")</li><li>It is possible to foresee deviations in trends based on qualitative changes in underlying drivers. In the Paul view, this often looks like switching from one trend to another. (For example: instead of \"continue to grow at 3%\" you notice that feedback loops imply hyperbolic growth, and then you look further back in time and notice that that's the trend on a longer timescale. Or alternatively, you realize that you can't just extrapolate AI progress because you can't keep doubling money invested every few months, and so you start looking at trends in money invested and build a simple model based on that, which you still describe as \"basically trend extrapolation\".)</li></ul><p>Places of disagreement:</p><ul><li>Eliezer / Nate: There is an underlying driver of impact on the world which we might call \"general cognition\" or \"intelligence\" or \"consequentialism\" or \"the-thing-spotlighted-by-coherence-arguments\", and the zero-to-one transition for that underlying driver will go from \"not present at all\" to \"at or above human-level\", without something in between. Rats, dogs and chimps might be impressive in some ways but they do not have this underlying driver of impact; the zero-to-one transition happened between chimps and humans.</li><li>Paul (might be closer to my views, idk): There isn't this underlying driver (or, depending on definitions, the zero-to-one transition happens well before human-level intelligence / impact). There are just more and more general heuristics, and correspondingly higher and higher impact. The case with evolution is unusually fast because the more general heuristics weren't actually that useful.</li></ul><p>To the extent this is accurate, it doesn't seem like you really get to make a bet that resolves before the end times, since you agree on basically everything until the point at which Eliezer predicts that you get the zero-to-one transition on the underlying driver of impact. I think all else equal you probably predict that Eliezer has shorter timelines to the end times than Paul (and that's where you get things like \"Eliezer predicts you don't have factory-generating factories before the end times whereas Paul does\"). (Of course, all else is not equal.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][3:36]</strong> &nbsp;</p><blockquote><p>but you know enough to have strong timing predictions, e.g. your bet with caplan</p></blockquote><p>Eliezer said in Jan 2017 that the Caplan bet was kind of a joke: <a href=\"https://www.econlib.org/archives/2017/01/my_end-of-the-w.html/#comment-166919\">https://www.econlib.org/archives/2017/01/my_end-of-the-w.html/#comment-166919</a>. Albeit \"I suppose one might draw conclusions from the fact that, when I was humorously imagining what sort of benefit I could get from exploiting this amazing phenomenon, my System 1 thought that having the world not end before 2030 seemed like the most I could reasonably ask.\"</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][10:01]</strong> &nbsp;</p><p>@RobBensinger sounds like the joke is that he thinks timelines are even shorter, which strengthens my claim about strong timing predictions?</p><p>Now that we clarified up-thread that Eliezer's position is <i>not</i> that there was a giant algorithmic innovation in between chimps and humans, but rather that there was some innovation in between dinosaurs and some primate or bird that allowed the primate/bird lines to scale better, I'm now confused about why it still seems like Eliezer expects a major innovation in the future that leads to deep/general intelligence. If the evidence we have is that evolution had <i>some</i> innovation like this, why not think that the invention of neural nets in the 60s or the invention of backprop in the 80s or whatever was the corresponding innovation in AI development? Why put it in the future? (Unless I'm misunderstanding and Eliezer doesn't really place very high probability on \"AGI is bottlenecked by an insight that lets us figure out how to get the deep intelligence instead of the shallow one\"?)</p><p>Also if Eliezer would count transformers and so on as the kind of big innovation that would lead to AGI, then I'm not sure we disagree. I feel like that sort of thing is factored into the software progress trends used to extrapolate progress, so projecting those forward folds in expectations of future transformers</p><p>But it seems like Eliezer still expects <i>one</i> or a few innovations that are much larger in impact than the transformer?</p><p>I'm also curious what Eliezer thinks of the claim \"extrapolating trends automatically folds in the world's inadequacy and stupidness because the past trend was built from everything happening in the world including the inadequacy\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][10:24]</strong> &nbsp;</p><p>Ajeya asked before, and I see I didn't answer:</p><blockquote><p>what about hardware/software R&amp;D wages? will they get up to $20m/yr for good ppl?</p></blockquote><p>If you mean the best/luckiest people, they're already there. If you mean that say Mike Blume starts getting paid $20m/yr base salary, then I cheerfully say that I'm willing to call that a narrower prediction of the Paulverse than of the Eliezerverse.</p><blockquote><p>will someone train a 10T param model before end days?</p></blockquote><p>Well, of course, because now it's a headline figure and Goodhart's Law applies, and the Earlier point where this happens is where somebody trains a useless 10T param model using some much cheaper training method like MoE just to be the first to get the headline where they say they did that, if indeed that hasn't happened already.</p><p>But even apart from that, a 10T param model sure sounds lots like a steady stream of headlines we've already seen, even for cases where it was doing something useful like GPT-3, so I would not feel surprised by more headlines like this.</p><p>I will, however, be alarmed (not surprised) relatively more by ability improvements, than headline figure improvements, because I am not very impressed by 10T param models per se.</p><p>In fact I will probably be more surprised by ability improvements after hearing the 10T figure, than my model of Paul will claim to be, because my model of Paul much more associates 10T figures with capability increases.</p><p>Though I don't understand why this prediction success isn't more than counterbalanced by an implied sequence of earlier failures in which Paul's model permitted much more impressive things to happen from 1T Goodharted-headline models, that didn't actually happen, that I expected to not happen - eg the current regime with MoE headlines - so that by the time that an impressive 10T model comes along and Imaginary Paul says 'Ah yes I claim this for a success', Eliezer's reply is 'I don't understand the aspect of your theory which supposedly told you in advance that this 10T model would scale capabilities, but not all the previous 10T models or the current pointless-headline 20T models where that would be a prediction failure. From my perspective, people eventually scaled capabilities, and param-scaling techniques happened to be getting more powerful at the same time, and so of course the Earliest tech development to be impressive was one that included lots of params. It's not a coincidence, but it's also not a triumph for the param-driven theory per se, because the news stories look similar AFAICT in a timeline where it's 60% algorithms and 40% params.\"</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Cotra][10:35]</strong> &nbsp;</p><p>MoEs have very different scaling properties, for one thing they run on way fewer FLOP/s (which is just as if not more important than params, though we use params as a shorthand when we're talking about \"typical\" models which tend to have small constant FLOP/param ratios). If there's a model <i>with a similar architecture</i> to the ones we have scaling laws about now, then at 10T params I'd expect it to have the performance that the scaling laws would expect it to have</p><p>Maybe something to bet about there. Would you say 10T param GPT-N would perform worse than the scaling law extraps would predict?</p><p>It seems like if we just look at a ton of scaling laws and see where they predict benchmark perf to get, then you could either bet on an upward or downward trend break and there could be a bet?</p><p>Also, if \"large models that aren't that impressive\" is a ding against Paul's view, why isn't GPT-3 being so much better than GPT-2 which in turn was better than GPT-1 with little fundamental architecture changes not a plus? It seems like you often cite GPT-3 as evidence <i>for</i> your view</p><p>But Paul (and Dario) at the time predicted it'd work. The scaling laws work was before GPT-3 and prospectively predicted GPT-3's perf</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][10:55]</strong> &nbsp;</p><p>I guess I should've mentioned that I knew MoEs ran on many fewer FLOP/s because others may not know I know that; it's an obvious charitable-Paul-interpretation but I feel like there's multiple of those and I don't know which, if any, Paul wants to claim as obvious-not-just-in-retrospect.</p><p>Like, ok, sure people talk about model size. But maybe we really want to talk about gradient descent training ops; oh, wait, actually we meant to talk about gradient descent training ops with a penalty figure for ops that use lower precision, but nowhere near a 50% penalty for 16-bit instead of 32-bit; well, no, really the obvious metric is the one in which the value of a training op scales logarithmically with the total computational depth of the gradient descent (I'm making this up, it's not an actual standard anywhere), and that's why this alternate model that does a ton of gradient descent ops while making less use of the actual limiting resource of inter-GPU bandwidth is not as effective as you'd predict from the raw headline figure about gradient descent ops. And of course we don't want to count ops that are just recomputing a gradient checkpoint, ha ha, that would be silly.</p><p>It's not impossible to figure out these adjustments in advance.</p><p>But part of me also worries that - though this is more true of other EAs who will read this, than Paul or Carl, whose skills I do respect to some degree - that if you ran an MoE model with many fewer gradient descent ops, and it did do something impressive with 10T params that way, people would promptly do a happy dance and say \"yay scaling\" not \"oh wait huh that was not how I thought param scaling worked\". After all, somebody originally said \"10T\", so clearly they were right!</p><p>And even with respect to Carl or Paul I worry about looking back and making \"obvious\" adjustments and thinking that a theory sure has been working out fine so far.</p><p>To be clear, I do consider GPT-3 as noticeable evidence for Dario's view and for Paul's view. The degree to which it worked well was more narrowly a prediction of those models than mine.</p><p>Thing about narrow predictions like that, if GPT-4 does not scale impressively, the theory loses significantly more Bayes points than it previously gained.</p><p>Saying \"this previously observed trend is very strong and will surely continue\" will quite often let you pick up a few pennies in front of the steamroller, because not uncommonly, trends do continue, but then they stop and you lose more Bayes points than you previously gained.</p><p>I do think of Carl and Paul as being better than this.</p><p>But I also think of the average EA reading them as being fooled by this.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][11:09]</strong> &nbsp;</p><p>The scaling laws experiments held architecture fixed, and that's the basis of the prediction that GPT-3 will be along the same line that held over previous OOM, most definitely not switch to MoE/Switch Transformer with way less resources.</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:10]</strong> &nbsp;</p><p>You can redraw your graphs afterwards so that a variant version of Moore's Law continued apace, but back in 2000, everyone sure was impressed with CPU GHz going up year after year and computers getting tangibly faster, and that version of Moore's Law sure did not continue. Maybe some people were savvier and redrew the graphs as soon as the physical obstacles became visible, but of course, other people had predicted the end of Moore's Law years and years before then. Maybe if superforecasters had been around in 2000 we would have found that they all sorted it out successfully, maybe not.</p><p>So, GPT-3 was $12m to train. In May 2022 it will be 2 years since GPT-3 came out. It feels to me like the Paulian view as I know how to operate it, says that GPT-3 has now got some revenue and exhibited applications like Codex, and was on a clear trend line of promise, so somebody ought to be willing to invest $120m in training GPT-4, and then we get 4x algorithmic speedups and cost improvements since then (iirc Paul said 2x/yr above? though I can't remember if that was his viewpoint or mine?) so GPT-4 should have 40x 'oomph' in some sense, and what that translates to in terms of intuitive impact ability, I don't know.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][11:18]</strong> &nbsp;</p><p>The OAI paper had 16 months (and is probably a bit low because in the earlier data people weren't optimizing for hardware efficiency much): <a href=\"https://openai.com/blog/ai-and-efficiency/\">https://openai.com/blog/ai-and-efficiency/</a></p><blockquote><p>so GPT-4 should have 40x 'oomph' in some sense, and what that translates to in terms of intuitive impact ability, I don't know.</p></blockquote><p>Projecting this: <a href=\"https://arxiv.org/abs/2001.08361\">https://arxiv.org/abs/2001.08361</a></p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:19]</strong> &nbsp;</p><p>30x then. I would not be terribly surprised to find that results on benchmarks continue according to graph, and yet, GPT-4 somehow does not seem very much smarter than GPT-3 in conversation.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][11:20]</strong> &nbsp;</p><p>There are also graphs of the human impressions of sense against those benchmarks and they are well correlated. I expect that to continue too.</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:21]</strong> &nbsp;</p><p>Stuff coming uncorrelated that way, sounds like some of the history I lived through, where people managed to make the graphs of Moore's Law seem to look steady by rejiggering the axes, and yet, between 1990 and 2000 home computers got a whole lot faster, and between 2010 and 2020 they did not.</p><p>This is obviously more likely (from my perspective) to break down anywhere between GPT-3 and GPT-6, than between GPT-3 and GPT-4.</p><p>Is this also part of the Carl/Paul worldview? Because I implicitly parse a lot of the arguments as assuming a necessary premise which says, \"No, this continues on until doomsday and I know it Kurzweil-style.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][11:23]</strong> &nbsp;</p><p>Yeah I expect trend changes to happen, more as you go further out, and especially more when you see other things running into barriers or contradictions. Re language models there is some of that coming up with different scaling laws colliding when the models get good enough to extract almost all the info per character (unless you reconfigure to use more info-dense data).</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][11:23]</strong> &nbsp;</p><p>Where \"this\" is the Yudkowskian \"the graphs are fragile and just break down one day, and their meanings are even more fragile and break down earlier\".</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][11:25]</strong> &nbsp;</p><p>Scaling laws working over 8 or 9 OOM makes me pretty confident of the next couple, not confident about 10 further OOM out.</p></td></tr></tbody></table>",
      "plaintextDescription": "This post is a transcript of a discussion between Carl Shulman and Eliezer Yudkowsky, following up on a conversation with Paul Christiano and Ajeya Cotra.\n\n \n\nColor key:\n\n Chat by Carl and Eliezer  Other chat \n\n \n\n\n9.14. Carl Shulman's predictions\n \n\n[Shulman][20:30] \n\nI'll interject some points re the earlier discussion about how animal data relates to the 'AI scaling to AGI' thesis.\n\n1. In humans it's claimed the IQ-job success correlation varies by job, For a scientist or doctor it might be 0.6+, for a low complexity job more like 0.4, or more like 0.2 for simple repetitive manual labor. That presumably goes down a lot with less in the way of hands, or focused on low density foods like baleen whales or grazers. If it's 0.1 for animals like orcas or elephants, or 0.05, then there's 4-10x less fitness return to smarts.\n\n2. But they outmass humans by more than 4-10x. Elephants 40x, orca 60x+. Metabolically (20 watts divided by BMR of the animal) the gap is somewhat smaller though, because of metabolic scaling laws (energy scales with 3/4 or maybe 2/3 power, so ).\n\nhttps://en.wikipedia.org/wiki/Kleiber%27s_law\n\nIf dinosaurs were poikilotherms, that's a 10x difference in energy budget vs a mammal of the same size, although there is debate about their metabolism.\n\n3. If we're looking for an innovation in birds and primates, there's some evidence of 'hardware' innovation rather than 'software.' Herculano-Houzel reports in The Human Advantage (summarizing much prior work neuron counting) different observational scaling laws for neuron number with brain mass for different animal lineages.\n\n> We were particularly interested in cellular scaling differences that might have arisen in primates. If the same rules relating numbers of neurons to brain size in rodents (6)\n> \n> The brain of the capuchin monkey, for instance, weighing 52 g, contains >3× more neurons in the cerebral cortex and ≈2× more neurons in the cerebellum than the larger brain of the capybara, weighing 76 g.\n\n[",
      "wordCount": 5971
    },
    "tags": [
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ax695frGJEzGxFBK4",
    "title": "Biology-Inspired AGI Timelines: The Trick That Never Works",
    "slug": "biology-inspired-agi-timelines-the-trick-that-never-works",
    "url": null,
    "baseScore": 156,
    "voteCount": 117,
    "viewCount": null,
    "commentCount": 144,
    "createdAt": null,
    "postedAt": "2021-12-01T22:35:28.379Z",
    "contents": {
      "markdown": "\\- 1988 -\n---------\n\n**Hans Moravec:**  Behold my book *Mind Children.*  Within, I project that, in 2010 or thereabouts, we shall achieve strong AI.  I am not calling it \"Artificial General Intelligence\" because this term will not be coined for another 15 years or so.\n\n**Eliezer** (who is not actually on the record as saying this, because the real Eliezer is, in this scenario, 8 years old; this version of Eliezer has all the meta-heuristics of Eliezer from 2021, but none of that Eliezer's anachronistic knowledge):  Really?  That sounds like a very difficult prediction to make correctly, since it is about the future, which is famously hard to predict.\n\n**Imaginary Moravec:**  Sounds like a [fully general counterargument](https://www.lesswrong.com/tag/fully-general-counterargument) to me.\n\n**Eliezer:**  Well, it is, indeed, a fully general counterargument *against futurism.*  Successfully predicting the unimaginably far future - that is, more than 2 or 3 years out, or sometimes less - is something that human beings seem to be quite bad at, by and large.\n\n**Moravec:  **I predict that, 4 years from this day, in 1992, the Sun will rise in the east.\n\n**Eliezer:** Okay, let me qualify that.  Humans seem to be quite bad at predicting the future whenever we need to predict anything at all *new and unfamiliar,* rather than the Sun continuing to rise every morning until it finally gets eaten.  I'm not saying it's impossible to ever validly predict something novel!  Why, even if that was impossible, how could *I* know it for sure?  By extrapolating from my own personal inability to make predictions like that?  Maybe I'm just bad at it myself.  But any time somebody claims that some particular novel aspect of the far future is predictable, they justly have a significant burden of prior skepticism to overcome.\n\nMore broadly, we should not expect a good futurist to give us a generally good picture of the future.  We should expect a great futurist to single out a few *rare narrow aspects* of the future which are, somehow, *exceptions* to the usual rule about the future not being very predictable.\n\nI do agree with you, for example, that we shall *at some point* see Artificial General Intelligence.  This seems like a rare predictable fact about the future, even though it is about a novel thing which has not happened before: we keep trying to crack this problem, we make progress albeit slowly, the problem must be solvable in principle because human brains solve it, eventually it will be solved; this is not a logical necessity, but it sure seems like the way to bet.  \"AGI eventually\" is predictable in a way that it is *not* predictable that, e.g., the nation of Japan, presently upon the rise, will achieve economic dominance over the next decades - to name something else that present-day storytellers of 1988 are talking about.\n\nBut *timing* the novel development correctly?  *That* is almost never done, not until things are 2 years out, and often not even then.  Nuclear weapons were called, but not nuclear weapons in 1945; heavier-than-air flight was called, but not flight in 1903.  In both cases, people said two years earlier that it wouldn't be done for 50 years - or said, decades too early, that it'd be done shortly.  There's a difference between worrying that we may eventually get a serious global pandemic, worrying that eventually a lab accident may lead to a global pandemic, and forecasting that a global pandemic will start in November of 2019.\n\n**Moravec:**  You should read my book, my friend, into which I have put much effort.  In particular - though it may sound impossible to forecast, to the likes of yourself - I have carefully examined a graph of computing power in single chips and the most powerful supercomputers over time.  This graph looks surprisingly regular!  Now, of course not all trends can continue forever; but I have considered the arguments that Moore's Law will break down, and found them unconvincing.  My book spends several chapters discussing the particular reasons and technologies by which we might expect this graph to *not* break down, and continue, such that humanity *will* have, by 2010 or so, supercomputers which can perform 10 trillion operations per second.*\n\nOh, and also my book spends a chapter discussing the retina, the part of the brain whose computations we understand in the most detail, in order to estimate how much computing power the human brain is using, arriving at a figure of 10^13 ops/sec.  This neuroscience and computer science may be a bit hard for the layperson to follow, but I assure you that I am in fact an experienced hands-on practitioner in robotics and computer vision.\n\nSo, as you can see, we should first get strong AI somewhere around 2010.  I may be off by an order of magnitude in one figure or another; but even if I've made two errors in the same direction, that only shifts the estimate by 7 years or so.\n\n(*)  Moravec just about nailed this part; the actual year was 2008.\n\n**Eliezer:**  I sure would be amused if we *did* in fact get strong AI somewhere around 2010, which, for all *I* know at this point in this hypothetical conversation, could totally happen!  Reversed stupidity is not intelligence, after all, and just because that is a completely broken justification for predicting 2010 doesn't mean that it cannot happen that way.\n\n**Moravec:**  Really now.  Would you care to enlighten me as to how I reasoned so wrongly?\n\n**Eliezer:**  Among the reasons why the Future is so hard to predict, in general, is that the sort of answers we want tend to be the products of lines of causality with multiple steps and multiple inputs.  Even when we can guess a single fact that *plays some role* in producing the Future - which is not of itself all that rare - usually the answer the storyteller wants depends on *more facts* than that single fact.  Our ignorance of any one of those other facts can be enough to torpedo our whole line of reasoning - *in practice,* not just as a matter of possibilities.  You could say that the art of exceptions to Futurism being impossible, consists in finding those rare things that you can predict despite being almost entirely ignorant of most concrete inputs into the concrete scenario.  Like predicting that AGI will happen *at some point*, despite not knowing the design for it, or who will make it, or how.\n\nMy own contribution to the Moore's Law literature consists of Moore's Law of Mad Science:  \"Every 18 months, the minimum IQ required to destroy the Earth drops by 1 point.\"  Even if this serious-joke was an absolutely true law, and aliens told us it was absolutely true, we'd still have no ability whatsoever to predict thereby when the Earth would be destroyed, because we'd have no idea what that minimum IQ was right now or at any future time.  We would know that in general the Earth had a serious problem that needed to be addressed, because we'd know in general that destroying the Earth kept on getting easier every year; but we would not be able to time *when* that would become an imminent emergency, until we'd seen enough specifics that the crisis was already upon us.\n\nIn the case of your prediction about strong AI in 2010, I might put it as follows:  The timing of AGI could be seen as a product of three factors, one of which you can try to extrapolate from existing graphs, and two of which you don't know at all.  Ignorance of any one of them is enough to invalidate the whole prediction.\n\nThese three factors are:\n\n*   The availability of computing power over time, which may be quantified, and appears steady when graphed;\n*   The rate of progress in knowledge of cognitive science and algorithms over time, which is much harder to quantify;\n*   A function that is a latent background parameter, for the amount of computing power required to create AGI as a function of any particular level of knowledge about cognition; and about this we know almost nothing.\n\nOr to rephrase:  Depending on how much you and your civilization know about AI-making - how much you know about cognition and computer science - it will take you a variable amount of computing power to build an AI.  If you really knew what you were doing, for example, I confidently predict that you could build a mind at least as powerful as a human mind, while using *fewer* floating-point operations per second than a human brain is making useful use of -\n\n**Chris Humbali:**  Wait, did you just say \"confidently\"?  How could you possibly know *that* with confidence?  How can you criticize Moravec for being too confident, and then, in the next second, turn around and be confident of something yourself?  Doesn't that make you a massive hypocrite?\n\n**Eliezer:**  Um, who are you again?\n\n**Humbali:**  I'm the cousin of Pat Modesto from [your previous dialogue on Hero Licensing](https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing)!  Pat isn't here in person because \"Modesto\" looks unfortunately like \"Moravec\" on a computer screen.  And also their first name looks a bit like \"Paul\" who is not meant to be referenced either.  So today *I* shall be your true standard-bearer for good calibration, intellectual humility, the outside view, and reference class forecasting -\n\n**Eliezer:**  Two of these things are not like the other two, in my opinion; and Humbali and Modesto do not understand how to operate any of the four correctly, in my opinion; but anybody who's read \"[Hero Licensing](https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing)\" should already know I believe that.\n\n**Humbali:**  \\- and I don't see how Eliezer can possibly be so *confident,* after all his humble talk of the difficulty of futurism, that it's possible to build a mind 'as powerful as' a human mind using 'less computing power' than a human brain.\n\n**Eliezer:**  It's overdetermined by multiple lines of inference.  We might first note, for example, that the human brain runs very slowly in a *serial* sense and tries to make up for that with massive parallelism.  It's an obvious truth of computer science that while you can use 1000 serial operations per second to emulate 1000 parallel operations per second, the reverse is not in general true.\n\nTo put it another way: if you had to build a spreadsheet or a word processor on a computer running at 100Hz, you might also need a billion processing cores and massive parallelism in order to do enough cache lookups to get anything done; that wouldn't mean the computational labor you were performing was *intrinsically* that expensive.  Since modern chips are massively serially faster than the neurons in a brain, and the direction of conversion is asymmetrical, we should expect that there are tasks which are immensely expensive to perform in a massively parallel neural setup, which are much cheaper to do with serial processing steps, and the reverse is *not* symmetrically true.\n\nA sufficiently adept builder can build general intelligence more cheaply in total operations per second, if they're allowed to line up a billion operations one after another per second, versus lining up only 100 operations one after another.  I don't bother to qualify this with \"very probably\" or \"almost certainly\"; it is the sort of proposition that a clear thinker should simply accept as obvious and move on.\n\n**Humbali:**  And is it certain that neurons can perform only 100 serial steps one after another, then?  As you say, ignorance about one fact can obviate knowledge of any number of others.\n\n**Eliezer:**  A typical neuron firing as fast as possible can do maybe 200 spikes per second, a few rare neuron types used by eg bats to echolocate can do 1000 spikes per second, and the vast majority of neurons are not firing that fast at any given time.  The usual and proverbial rule in neuroscience - the sort of academically respectable belief I'd expect you to respect even more than I do - is called \"the 100-step rule\", that any task a human brain (or mammalian brain) can do on perceptual timescales, must be doable with no more than 100 *serial* steps of computation - no more than 100 things that get computed one after another.  Or even less if the computation is running off spiking frequencies instead of individual spikes.\n\n**Moravec:**  Yes, considerations like that are part of why I'd defend my estimate of 10^13 ops/sec for a human brain as being reasonable - more reasonable than somebody might think if they were, say, counting all the synapses and multiplying by the maximum number of spikes per second in any neuron.  If you actually look at what the retina is doing, and how it's computing that, it doesn't look like it's doing one floating-point operation per activation spike per synapse.\n\n**Eliezer:**  There's a similar asymmetry between precise computational operations having a vastly easier time emulating noisy or imprecise computational operations, compared to the reverse - there is no doubt a way to use neurons to compute, say, exact 16-bit integer addition, which is at least *more* efficient than a human trying to add up 16986+11398 in their heads, but you'd still need more synapses to do that than transistors, because the synapses are noisier and the transistors can just do it precisely.  This is harder to visualize and get a grasp on than the parallel-serial difference, but that doesn't make it unimportant.\n\nWhich brings me to the second line of very obvious-seeming reasoning that converges upon the same conclusion - that it is in principle possible to build an AGI much more computationally efficient than a human brain - namely that biology is simply *not that efficient,* and *especially* when it comes to huge complicated things that it has started doing relatively recently.\n\nATP synthase may be close to 100% thermodynamically efficient, but ATP synthase is literally over 1.5 billion years old and a core bottleneck on all biological metabolism.  Brains have to pump thousands of ions in and out of each stretch of axon and dendrite, in order to restore their ability to fire another fast neural spike.  The result is that the brain's computation is something like half a million times less efficient than the thermodynamic limit for its temperature - so around two millionths as efficient as ATP synthase.  And neurons are a hell of a lot older than the biological software for general intelligence!\n\nThe software for a human brain is not going to be 100% efficient compared to the theoretical maximum, nor 10% efficient, nor 1% efficient, even *before* taking into account the whole thing with parallelism vs. serialism, precision vs. imprecision, or similarly clear low-level differences.\n\n**Humbali:**  Ah!  But allow me to offer a consideration here that, I would wager, you've never thought of before yourself - namely - *what if you're wrong?*  Ah, not so confident now, are you?\n\n**Eliezer:**  One observes, over one's cognitive life as a human, which sorts of what-ifs are useful to contemplate, and where it is wiser to spend one's limited resources planning against the alternative that one might be wrong; and I have oft observed that lots of people don't... quite seem to understand how to use 'what if' all that well?  They'll be like, \"[Well, what if UFOs are aliens, and the aliens are partially hiding from us but not perfectly hiding from us, because they'll seem higher-status if they make themselves observable but never directly interact with us?](https://www.overcomingbias.com/2021/06/ufos-what-the-hell.html)\"\n\nI can refute individual what-ifs like that with specific counterarguments, but I'm not sure how to convey the central generator behind how I know that I ought to refute them.  I am not sure how I can get people to reject these ideas for themselves, instead of them passively waiting for me to come around with a specific counterargument.  My having to counterargue things specifically now seems like a road that never seems to end, and I am not as young as I once was, nor am I encouraged by how much progress I seem to be making.  I refute one wacky idea with a specific counterargument, and somebody else comes along and presents a new wacky idea on almost exactly the same theme.\n\nI know it's probably not going to work, if I try to say things like this, but I'll try to say them anyways.  When you are going around saying 'what-if', there is a very great difference between your map of reality, and the territory of reality, which is extremely narrow and stable.  Drop your phone, gravity pulls the phone downward, it falls.  What if there are aliens and they make the phone rise into the air instead, maybe because they'll be especially amused at violating the rule after you just tried to use it as an example of where you could be confident?  Imagine the aliens watching you, imagine their amusement, contemplate how fragile human thinking is and how little you can ever be assured of anything and ought not to be too confident.  Then drop the phone and watch it fall.  You've now learned something about how reality itself isn't made of what-ifs and reminding oneself to be humble; reality runs on rails stronger than your mind does.\n\nContemplating this doesn't mean you *know* the rails, of course, which is why it's so much harder to predict the Future than the past.  But if you see that your thoughts are still wildly flailing around what-ifs, it means that they've failed to gel, in some sense, they are not yet bound to reality, because reality has no binding receptors for what-iffery.\n\nThe correct thing to do is not to act on your what-ifs that you can't figure out how to refute, but to go on looking for a model which makes narrower predictions than that.  If that search fails, forge a model which puts some more numerical distribution on your highly entropic uncertainty, instead of diverting into specific what-ifs.  And in the latter case, understand that this probability distribution reflects your ignorance and subjective state of mind, rather than your knowledge of an objective frequency; so that somebody else is allowed to be less ignorant without you shouting \"Too confident!\" at them.  Reality runs on rails as strong as math; sometimes other people will achieve, before you do, the feat of having their own thoughts run through more concentrated rivers of probability, in some domain.\n\nNow, when we are trying to concentrate our thoughts into deeper, narrower rivers that run closer to reality's rails, there is of course the legendary hazard of concentrating our thoughts into the *wrong* narrow channels that *exclude* reality.  And the great legendary sign of this condition, of course, is the counterexample from Reality that falsifies our model!  But you should not in general criticize somebody for trying to concentrate their probability into narrower rivers than yours, for this is the appearance of the great general project of trying to get to grips with Reality, that runs on true rails that are narrower still.\n\nIf you have concentrated your probability into *different* narrow channels than somebody else's, then, of course, you have a more interesting dispute; and you should engage in that legendary activity of trying to find some accessible experimental test on which your nonoverlapping models make different predictions.\n\n**Humbali:**  I do not understand the import of all this vaguely mystical talk.\n\n**Eliezer:**  I'm trying to explain why, when I say that I'm very confident it's possible to build a human-equivalent mind using less computing power than biology has managed to use effectively, and you say, \"How can you be so *confident,* what if you are *wrong,*\" it is not unreasonable for me to reply, \"Well, kid, this doesn't seem like one of those places where it's particularly important to worry about far-flung ways I could be wrong.\"  Anyone who aspires to learn, learns over a lifetime which sorts of guesses are more likely to go oh-no-wrong in real life, and which sorts of guesses are likely to just work.  Less-learned minds will have minds full of what-ifs they can't refute in more places than more-learned minds; and even if you cannot see how to refute all your what-ifs yourself, it is possible that a more-learned mind knows why they are improbable.  For one must distinguish possibility from probability.\n\nIt is *imaginable* or *conceivable* that human brains have such refined algorithms that they are operating at the absolute limits of computational efficiency, or within 10% of it.  But if you've spent enough time noticing *where* Reality usually exercises its sovereign right to yell \"Gotcha!\" at you, learning *which* of your assumptions are the kind to blow up in your face and invalidate your final conclusion, you can guess that \"Ah, but what if the brain is nearly 100% computationally efficient?\" is the sort of what-if that is not much worth contemplating because it is not actually going to be true in real life.  Reality is going to confound you in some other way than that.\n\nI mean, maybe you haven't read enough neuroscience and evolutionary biology that you can see from your own knowledge that the proposition sounds massively implausible and ridiculous.  But it should hardly seem unlikely that somebody else, more learned in biology, might be justified in having more confidence than you.  Phones don't fall up.  Reality really is very stable and orderly in a lot of ways, even in places where you yourself are ignorant of that order.\n\nBut if \"What if aliens are making themselves visible in flying saucers because they want high status and they'll have higher status if they're occasionally observable but never deign to talk with us?\" sounds to you like it's totally plausible, and you don't see how someone can be *so confident* that it's not true - because oh *no* what if you're *wrong* and you haven't *seen* the aliens so how can you *know* what they're not thinking - then I'm not sure how to lead you into the place where you can dismiss that thought with confidence.  It may require a kind of life experience that I don't know how to give people, at all, let alone by having them passively read paragraphs of text that I write; a learned, perceptual sense of which what-ifs have any force behind them.  I mean, I can refute that specific scenario, I *can* put that learned sense into words; but I'm not sure that does me any good unless you learn how to refute it yourself.\n\n**Humbali:**  Can we leave aside all that meta stuff and get back to the object level?\n\n**Eliezer:**  This indeed is often wise.\n\n**Humbali:**  Then here's one way that the minimum computational requirements for general intelligence could be *higher* than Moravec's argument for the human brain.  Since, after, all, we only have one existence proof that general intelligence is possible at all, namely the human brain.  Perhaps there's no way to get general intelligence in a computer except by simulating the brain neurotransmitter-by-neurotransmitter.  In that case you'd need a lot *more* computing operations per second than you'd get by calculating the number of potential spikes flowing around the brain!  What if it's true?  How can you *know?*\n\n(**Modern person:**  This seems like an obvious straw argument?  I mean, would anybody, even at an earlier historical point, actually make an argument like -\n\n**Moravec and Eliezer:**  YES THEY WOULD.)\n\n**Eliezer:**  I can imagine that if we were trying specifically to *upload a human* that there'd be no easy and simple and obvious way to run the resulting simulation and get a good answer, without simulating neurotransmitter flows in extra detail.\n\nTo imagine that every one of these simulated flows is *being usefully used in general intelligence and there is no way to simplify the mind design to use fewer computations...*  I suppose I could try to refute that specifically, but it seems to me that this is a road which has no end unless I can convey the generator of my refutations.  Your what-iffery is flung far enough that, if I cannot leave even that much rejection as an exercise for the reader to do on their own without my holding their hand, the reader has little enough hope of following the rest; let them depart now, in indignation shared with you, and save themselves further outrage.\n\nI mean, it will obviously be *less* obvious to the reader because they will know *less* than I do about this exact domain, it will justly take *more* work for the reader to specifically refute you than it takes me to refute you.  But I think the reader needs to be able to do that at all, in this example, to follow the more difficult arguments later.\n\n**Imaginary Moravec:**  I don't think it changes my conclusions by an order of magnitude, but some people would worry that, for example, changes of protein expression inside a neuron in order to implement changes of long-term potentiation, are also important to intelligence, and could be a big deal in the brain's real, effectively-used computational costs.  I'm curious if you'd dismiss that as well, the same way you dismiss the probability that you'd have to simulate every neurotransmitter molecule?\n\n**Eliezer:**  Oh, of course not.  Long-term potentiation suddenly turning out to be a big deal you overlooked, compared to the depolarization impulses spiking around, is *very* much the sort of thing where Reality sometimes jumps out and yells \"Gotcha!\" at you.\n\n**Humbali:**  *How can you tell the difference?*\n\n**Eliezer:**  Experience with Reality yelling \"Gotcha!\" at myself and historical others.\n\n**Humbali:**  They seem like equally plausible speculations to me!\n\n**Eliezer:**  Really?  \"What if long-term potentiation is a big deal and computationally important\" sounds just as plausible to you as \"What if the brain is already close to the wall of making the most efficient possible use of computation to implement general intelligence, and every neurotransmitter molecule matters\"?\n\n**Humbali:**  Yes!  They're both what-ifs we can't know are false and shouldn't be overconfident about denying!\n\n**Eliezer:**  My tiny feeble mortal mind is far away from reality and only bound to it by the loosest of correlating interactions, but I'm not *that* unbound from reality.\n\n**Moravec:**  I would guess that in real life, long-term potentiation is sufficiently slow and local that what goes on inside the cell body of a neuron over minutes or hours is not as big of a computational deal as thousands of times that many spikes flashing around the brain in milliseconds or seconds.  That's why I didn't make a big deal of it in my own estimate.\n\n**Eliezer:**  Sure.  But it *is* much more the sort of thing where you wake up to a reality-authored science headline saying \"Gotcha!  There were tiny DNA-activation interactions going on in there at high speed, and they were actually pretty expensive and important!\"  I'm not saying this exact thing is very probable, just that it wouldn't be out-of-character for reality to say *something* like that to me, the way it would be really genuinely bizarre if Reality was, like, \"Gotcha!  The brain is as computationally efficient of a generally intelligent engine as any algorithm can be!\"\n\n**Moravec:**  I think we're in agreement about that part, or we would've been, if we'd actually had this conversation in 1988.  I mean, I *am* a competent research roboticist and it is difficult to become one if you are completely unglued from reality.\n\n**Eliezer:**  Then what's with the 2010 prediction for strong AI, and the massive non-sequitur leap from \"the human brain is somewhere around 10 trillion ops/sec\" to \"if we build a 10 trillion ops/sec supercomputer, we'll get strong AI\"?\n\n**Moravec:**  Because while it's the kind of Fermi estimate that can be off by an order of magnitude in practice, it doesn't really seem like it should be, I don't know, off by three orders of magnitude?  And even three orders of magnitude is just 10 years of Moore's Law.  2020 for strong AI is also a bold and important prediction.\n\n**Eliezer:**  And the year 2000 for strong AI even more so.\n\n**Moravec:**  Heh!  That's not usually the direction in which people argue with me.\n\n**Eliezer:**  There's an important distinction between the direction in which people usually argue with you, and the direction from which Reality is allowed to yell \"Gotcha!\"  I wish my future self had kept this more in mind, when arguing with Robin Hanson about how well AI architectures were liable to generalize and scale without a ton of domain-specific algorithmic tinkering for every field of knowledge.  I mean, in principle what I was arguing for was various lower bounds on performance, but I sure could have emphasized more loudly that those were *lower* bounds - well, I *did* emphasize the lower-bound part, but - from the way I felt when AlphaGo and Alpha Zero and GPT-2 and GPT-3 showed up, I think I must've sorta forgot that myself.\n\n**Moravec:**  Anyways, if we say that I might be up to three orders of magnitude off and phrase it as 2000-2020, do you agree with my prediction then?\n\n**Eliezer:**  No, I think you're just... arguing about the wrong facts, in a way that seems to be unglued from most tracks Reality might follow so far as I currently know?  On my view, creating AGI is strongly dependent on how much knowledge you have about how to do it, in a way which almost *entirely* obviates the relevance of arguments from human biology?\n\nLike, human biology tells us a single not-very-useful data point about how much computing power evolutionary biology needs in order to build a general intelligence, using very alien methods to our own.  Then, very separately, there's the constantly changing level of how much cognitive science, neuroscience, and computer science our own civilization knows.  We don't know how much computing power is required for AGI for *any* level on that constantly changing graph, and biology doesn't tell us.  All we know is that the hardware requirements for AGI must be dropping by the year, because the knowledge of how to create AI is something that only increases over time.\n\nAt some point the moving lines for \"decreasing hardware required\" and \"increasing hardware available\" will cross over, which lets us predict that AGI gets built at *some* point.  But we don't know how to graph two key functions needed to predict that date.  You would seem to be committing the classic fallacy of searching for your keys under the streetlight where the visibility is better.  You know how to estimate how many floating-point operations per second the retina could effectively be using, but *this is not the number you need to predict the outcome you want to predict.*  You need a graph of human knowledge of computer science over time, and then a graph of how much computer science requires how much hardware to build AI, and neither of these graphs are available.\n\nIt *doesn't matter* how many chapters your book spends considering the continuation of Moore's Law or computation in the retina, and I'm sorry if it seems rude of me in some sense to just dismiss the relevance of all the hard work you put into arguing it.  But you're arguing the *wrong facts* to get to the conclusion, so all your hard work is for naught.\n\n**Humbali:**  Now it seems to me that I must chide you for being too dismissive of Moravec's argument.  Fine, yes, Moravec has not established with *logical certainty* that strong AI must arrive at the point where top supercomputers match the human brain's 10 trillion operations per second.  But has he not established a *reference class,* the sort of *base rate* that good and virtuous superforecasters, unlike yourself, go looking for when they want to *anchor* their estimate about some future outcome?  Has he not, indeed, established the sort of argument which says that if top supercomputers can do only *ten million* operations per second, we're not very likely to get AGI earlier than that, and if top supercomputers can do *ten quintillion* operations per second*, we're unlikely not to already have AGI?\n\n(*) In 2021 terms, [10 TPU v4 pods](https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4).\n\n**Eliezer:**  With ranges that wide, it'd be more likely and less amusing to hit somewhere inside it by coincidence.  But I still think this whole line of thoughts is just off-base, and that you, Humbali, have not truly grasped the concept of a virtuous superforecaster or how they go looking for reference classes and base rates.\n\n**Humbali:**  I frankly think you're just being unvirtuous.  Maybe you have some special model of AGI which claims that it'll arrive in a different year or be arrived at by some very different pathway.  But is not Moravec's estimate a sort of base rate which, to the extent you are properly and virtuously uncertain of your own models, you ought to *regress* in your own probability distributions over AI timelines?  As you become more uncertain about the exact amounts of knowledge required and what knowledge we'll have when, shouldn't you have an uncertain distribution about AGI arrival times that centers around Moravec's base-rate prediction of 2010?\n\nFor you to reject this anchor seems to reveal a grave lack of humility, since you must be very certain of whatever alternate estimation methods you are using in order to throw away this base-rate entirely.\n\n**Eliezer:**  Like I said, I think you've just failed to grasp the true way of a virtuous superforecaster.  Thinking a lot about Moravec's so-called 'base rate' is just making you, in some sense, stupider; you need to cast your thoughts loose from there and try to navigate a wilder and less tamed space of possibilities, until they begin to gel and coalesce into narrower streams of probability.  Which, for AGI, they probably *won't do* until we're quite close to AGI, and start to guess correctly how AGI will get built; for it is easier to predict an eventual global pandemic than to say it will start in November of 2019.  Even in October of 2019 this cannot be done.\n\n**Humbali:**  Then all this uncertainty must somehow be quantified, if you are to be a virtuous Bayesian; and again, for lack of anything better, the resulting distribution should center on Moravec's base-rate estimate of 2010.\n\n**Eliezer:**  No, that calculation is just basically not relevant here; and thinking about it is making you stupider, as your mind flails in the trackless wilderness grasping onto unanchored air.  Things must be 'sufficiently similar' to each other, in some sense, for us to get a base rate on one thing by looking at another thing.  Humans making an AGI is just too dissimilar to evolutionary biology making a human brain for us to anchor 'how much computing power at the time it happens' from one to the other.  It's not the droid we're looking for; and your attempt to build an inescapable epistemological trap about virtuously calling that a 'base rate' is not the Way.\n\n**Imaginary Moravec:**  If I can step back in here, I don't think my calculation is zero evidence?  What we know from evolutionary biology is that a blind alien god with zero foresight accidentally mutated a chimp brain into a general intelligence.  I don't want to knock biology's work too much, there's some impressive stuff in the retina, and the retina is just the part of the brain which is in some sense easiest to understand.  But surely there's a very reasonable argument that 10 trillion ops/sec is about the amount of computation that evolutionary biology needed; and since evolution is stupid, when we ourselves have that much computation, it shouldn't be *that* hard to figure out how to configure it.\n\n**Eliezer:**  If that was true, the same theory predicts that our current supercomputers should be doing a better job of matching the agility and vision of spiders.  When at some point there's enough hardware that we figure out how to put it together into AGI, we could be doing it with less hardware than a human; we could be doing it with more; and we can't even say that these two possibilities are *around equally probable* such that our probability distribution should have its median around 2010.  Your number is so bad and obtained by such bad means that we should just throw it out of our thinking and start over.\n\n**Humbali:**  This last line of reasoning seems to me to be particularly ludicrous, like you're just throwing away the only base rate we have in favor of a confident assertion of our somehow being *more uncertain* than that.\n\n**Eliezer:**  Yeah, well, sorry to put it bluntly, Humbali, but you have not yet figured out how to turn your own computing power into intelligence.\n\n \\- 1999 -\n----------\n\n**Luke Muehlhauser reading a previous draft of this** (only sounding much more serious than this, because Luke Muehlhauser)**:**  You know, there was this certain teenaged futurist who made some of his own predictions about AI timelines -\n\n**Eliezer:**  I'd really rather not argue from that as a case in point.  I dislike people who screw up something themselves, and then argue like nobody else could possibly be more competent than they were.  I dislike even more people who change their mind about something when they turn 22, and then, for the rest of their lives, go around acting like they are now Very Mature Serious Adults who believe the thing that a Very Mature Serious Adult believes, so if you disagree with them about that thing they started believing at age 22, you must just need to wait to grow out of your extended childhood.\n\n**Luke Muehlhauser** (still being paraphrased)**:**  It seems like it ought to be acknowledged somehow.\n\n**Eliezer:**  That's fair, yeah, I can see how someone might think it was relevant.  I just dislike how it potentially creates the appearance of trying to slyly sneak in an Argument From Reckless Youth that I regard as not only invalid but also incredibly distasteful.  You don't get to screw up yourself and then use that as an argument about how nobody else can do better.\n\n**Humbali:**  Uh, what's the actual drama being subtweeted here?\n\n**Eliezer:**  A certain teenaged futurist, who, for example, said in 1999, \"The most realistic estimate for a seed AI transcendence is 2020; nanowar, before 2015.\"\n\n**Humbali:**  This young man must surely be possessed of some very deep character defect, which I worry will prove to be of the sort that people almost never truly outgrow except in the rarest cases.  Why, he's not even putting a probability distribution over his mad soothsaying - how blatantly absurd can a person get?\n\n**Eliezer:**  Dear child ignorant of history, your complaint is far too anachronistic.  This is 1999 we're talking about here; almost nobody is putting probability distributions on things, that element of your later subculture has not yet been introduced.  Eliezer-2002 hasn't been sent a copy of \"Judgment Under Uncertainty\" by Emil Gilliam.  Eliezer-2006 hasn't put his draft online for \"Cognitive biases potentially affecting judgment of global risks\".  The Sequences won't start until another year after that.  How would the forerunners of effective altruism *in 1999* know about putting probability distributions on forecasts?  I haven't told them to do that yet!  We can give historical personages credit when they seem to somehow end up doing better than their surroundings would suggest; it is unreasonable to hold them to modern standards, or expect them to have finished refining those modern standards by the age of nineteen.\n\nThough there's also a more subtle lesson you could learn, about how this young man turned out to still have a promising future ahead of him; which he retained at least in part by having a deliberate contempt for pretended dignity, allowing him to be plainly and simply wrong in a way that he noticed, without his having twisted himself up to avoid a prospect of embarrassment.  Instead of, for example, his evading such plain falsification by having dignifiedly wide Very Serious probability distributions centered on the same medians produced by the same basically bad thought processes.\n\nBut that was too much of a digression, when I tried to write it up; maybe later I'll post something separately.\n\n\\- 2004 or thereabouts -\n------------------------\n\n**Ray Kurzweil in 2001:**  I have [calculated](https://www.kurzweilai.net/the-law-of-accelerating-returns) that matching the intelligence of a human brain requires 2 * 10^16 ops/sec* and this will become available in a $1000 computer in 2023.  26 years after that, in 2049, a $1000 computer will have ten billion times more computing power than a human brain; and in 2059, that computer will cost one cent.\n\n(*) Two TPU v4 pods.\n\n**Actual real-life Eliezer in Q&A, when Kurzweil says the same thing in a 2004(?) talk:**  It seems weird to me to forecast the arrival of \"human-equivalent\" AI, and then expect Moore's Law to just continue on the same track past that point for thirty years.  Once we've got, in your terms, human-equivalent AIs, even if we don't go beyond that in terms of intelligence, Moore's Law will start speeding them up.  Once AIs are thinking thousands of times faster than we are, wouldn't that tend to break down the graph of Moore's Law with respect to the objective wall-clock time of the Earth going around the Sun?  Because AIs would be able to spend thousands of *subjective* years working on new computing technology?\n\n**Actual Ray Kurzweil:**  The fact that AIs can do faster research is exactly what will enable Moore's Law to continue on track.\n\n**Actual Eliezer (out loud):**  Thank you for answering my question.\n\n**Actual Eliezer (internally):**  Moore's Law is a phenomenon produced by human cognition and the fact that human civilization runs off human cognition.  You can't expect the surface phenomenon to continue unchanged after the deep causal phenomenon underlying it starts changing.  What kind of bizarre worship of graphs would lead somebody to think that the graphs were the primary phenomenon and would continue steady and unchanged when the forces underlying them changed massively?  I was hoping he'd be less nutty in person than in the book, but oh well.\n\n\\- 2006 or thereabouts - \n-------------------------\n\n**Somebody on the Internet:**  I have calculated the number of computer operations used by evolution to evolve the human brain - searching through organisms with increasing brain size  - by adding up all the computations that were done by any brains before modern humans appeared.  It comes out to 10^43 computer operations.*  AGI isn't coming any time soon!\n\n(*)  I forget the exact figure.  It was 10^40-something.\n\n**Eliezer, sighing:**  Another day, another biology-inspired timelines forecast.  This trick didn't work when Moravec tried it, it's not going to work while Ray Kurzweil is trying it, and it's not going to work when you try it either.  It also didn't work when a certain teenager tried it, but please entirely ignore that part; you're at least allowed to do better than him.\n\n**Imaginary Somebody:**  Moravec's prediction failed because he assumed that you could just magically take something with around as much hardware as the human brain and, poof, it would start being around that intelligent -\n\n**Eliezer:**  Yes, that is one way of viewing an invalidity in that argument.  Though you do Moravec a disservice if you imagine that he could only argue \"It will magically emerge\", and could not give the more plausible-sounding argument \"Human engineers are not that incompetent compared to biology, and will probably figure it out without more than one or two orders of magnitude of extra overhead.\"\n\n**Somebody:**  But *I* am cleverer, for I have calculated the number of computing operations that was used to *create and design* biological intelligence, not just the number of computing operations required to *run it once created!*\n\n**Eliezer:**  And yet, because your reasoning contains the word \"biological\", it is just as invalid and unhelpful as Moravec's original prediction.\n\n**Somebody:**  I don't see why you dismiss my biological argument about timelines on the basis of Moravec having been wrong.  He made one basic mistake - neglecting to take into effect the cost to generate intelligence, not just to run it.  I have corrected this mistake, and now my own effort to do biologically inspired timeline forecasting should work fine, and must be evaluated on its own merits, *de novo*.\n\n**Eliezer:**  It is true indeed that sometimes a line of inference is doing just one thing wrong, and works fine after being corrected.  And because this is true, it is often indeed wise to reevaluate new arguments on their own merits, if that is how they present themselves.  One may not take the past failure of a different argument or three, and try to hang it onto the new argument like an inescapable iron ball chained to its leg.  It might be the cause for defeasible skepticism, but not invincible skepticism.\n\nThat said, on my view, you are making a nearly identical mistake as Moravec, and so his failure remains relevant to the question of whether you are engaging in a kind of thought that binds well to Reality.\n\n**Somebody:**  And that mistake is just mentioning the word \"biology\"?\n\n**Eliezer:**  The problem is that *the resource gets consumed differently, so base-rate arguments from resource consumption end up utterly unhelpful in real life.*  The human brain consumes around 20 watts of power.  Can we thereby conclude that an AGI should consume around 20 watts of power, and that, when technology advances to the point of being able to supply around 20 watts of power to computers, we'll get AGI?\n\n**Somebody:**  That's absurd, of course.  So, what, you compare my argument to an absurd argument, and from this dismiss it?\n\n**Eliezer:**  I'm saying that Moravec's \"argument from comparable resource consumption\" must be in general [invalid](https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization), because it [Proves Too Much](https://www.lesswrong.com/posts/G5eMM3Wp3hbCuKKPE/proving-too-much).  If it's in general valid to reason about comparable resource consumption, then it should be equally valid to reason from energy consumed as from computation consumed, and pick energy consumption instead to call the basis of your median estimate.\n\nYou say that AIs consume energy in a very different way from brains?  Well, they'll also consume computations in a very different way from brains!  The only difference between these two cases is that you *know* something about how humans eat food and break it down in their stomachs and convert it into ATP that gets consumed by neurons to pump ions back out of dendrites and axons, while computer chips consume electricity whose flow gets interrupted by transistors to transmit information.  Since you *know anything whatsoever* about how AGIs and humans consume energy, you can *see* that the consumption is so vastly different as to obviate all comparisons entirely.\n\nYou are *ignorant* of how the brain consumes computation, you are *ignorant* of how the first AGIs built would consume computation, but \"an unknown key does not open an unknown lock\" and these two ignorant distributions should not assert much internal correlation between them.\n\nEven without knowing the specifics of how brains and future AGIs consume computing operations, you ought to be able to reason abstractly about a directional update that you *would* make, if you knew *any* specifics instead of none.  If you did know how both kinds of entity consumed computations, if you knew about specific machinery for human brains, and specific machinery for AGIs, you'd then be able to see the enormous vast specific differences between them, and go, \"Wow, what a futile resource-consumption comparison to try to use for forecasting.\"\n\n(Though I say this without much hope; I have not had very much luck in telling people about predictable directional updates they would make, if they knew something instead of nothing about a subject.  I think it's probably too abstract for most people to feel in their gut, or something like that, so their brain ignores it and moves on in the end.  I have had life experience with learning more about a thing, updating, and then going to myself, \"Wow, I should've been able to predict in retrospect that learning almost *any* specific fact would move my opinions in that same direction.\"  But I worry this is not a common experience, for it involves a real experience of discovery, and preferably more than one to get the generalization.)\n\n**Somebody:**  All of that seems irrelevant to my novel and different argument.  I am not foolishly estimating the resources consumed by a single brain; I'm estimating the resources consumed by evolutionary biology to *invent* brains!\n\n**Eliezer:**  And the humans wracking their own brains and inventing new AI program architectures and deploying those AI program architectures to themselves learn, will consume computations so *utterly differently* from evolution that there is no point comparing those consumptions of resources.  That is the flaw that you share exactly with Moravec, and that is why I say the same of both of you, \"This is a kind of thinking that fails to bind upon reality, it doesn't work in real life.\"  I don't care how much painstaking work you put into your estimate of 10^43 computations performed by biology.  It's just not a relevant fact.\n\n**Humbali:**  But surely this estimate of 10^43 cumulative operations can at least be used to establish a base rate for anchoring our -\n\n**Eliezer:**  Oh, for god's sake, shut up.  At least Somebody is only wrong on the object level, and isn't trying to build an inescapable epistemological trap by which his ideas must still hang in the air like an eternal stench even after they've been counterargued.  Isn't 'but muh base rates' what your viewpoint would've also said about Moravec's 2010 estimate, back when that number still looked plausible?\n\n**Humbali:**  Of course it is evident to me now that my youthful enthusiasm was mistaken; obviously I tried to estimate the wrong figure.  As Somebody argues, we should have been estimating the biological computations used to *design* human intelligence, not the computations used to *run* it.\n\nI see, now, that I was using the wrong figure as my base rate, leading my base rate to be wildly wrong, and even irrelevant; but now that I've seen this, the clear error in my previous reasoning, I have a *new* base rate.  This doesn't seem obviously to me likely to contain the same kind of wildly invalidating enormous error as before.  What, is Reality just going to yell \"Gotcha!\" at me again?  And even the prospect of some new unknown error, which is just as likely to be in either possible direction, implies only that we should widen our credible intervals while keeping them centered on a median of 10^43 operations -\n\n**Eliezer:**  Please stop.  This trick just never works, at all, deal with it and get over it.  Every second of attention that you pay to the 10^43 number is making you stupider.  You might as well reason that 20 watts is a base rate for how much energy the first generally intelligent computing machine should consume.\n\n\\- 2020 -\n---------\n\n**OpenPhil:**  We have commissioned a Very Serious report on a biologically inspired estimate of how much computation will be required to achieve Artificial General Intelligence, for purposes of forecasting an AGI timeline.  ([Summary of report.](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines?commentId=7d4q79ntst6ryaxWD))  ([Full draft of report.)](https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP)  Our leadership takes this report Very Seriously.\n\n**Eliezer:**  Oh, hi there, new kids.  Your grandpa is feeling kind of tired now and can't debate this again with as much energy as when he was younger.\n\n**Imaginary OpenPhil:**  You're not *that* much older than us.\n\n**Eliezer:**  Not by biological wall-clock time, I suppose, but -\n\n**OpenPhil:**  You think thousands of times faster than us?\n\n**Eliezer:**  I wasn't going to say it if you weren't.\n\n**OpenPhil:**  We object to your assertion on the grounds that it is false.\n\n**Eliezer:**  I was actually going to say, you might be underestimating how long I've been walking this endless battlefield because I started *really quite young*.\n\nI mean, sure, I didn't read Moravec's *Mind Children* when it came out in 1988.  I only read it four years later, when I was twelve.  And sure, I didn't immediately afterwards start writing online about Moore's Law and strong AI; I did not immediately contribute my own salvos and sallies to the war; I was not yet a noticed voice in the debate.  I only got started on that at age sixteen.  I'd like to be able to say that in 1999 I was just a random teenager being reckless, but in fact I was already being invited to dignified online colloquia about the \"Singularity\" and mentioned in printed books; when I was being wrong back then I was already doing so in the capacity of a minor public intellectual on the topic.\n\nThis is, as I understand normie ways, relatively young, and is probably worth an extra decade tacked onto my biological age; you should imagine me as being 52 instead of 42 as I write this, with a correspondingly greater number of visible gray hairs.\n\nA few years later - though still before your time - there was the Accelerating Change Foundation, and Ray Kurzweil spending literally millions of dollars to push Moore's Law graphs of technological progress as *the* central story about the future.  I mean, I'm sure that a few million dollars sounds like peanuts to OpenPhil, but if your own annual budget was a hundred thousand dollars or so, that's a hell of a megaphone to compete with.\n\nIf you are currently able to conceptualize the Future as being about something *other* than nicely measurable metrics of progress in various tech industries, being projected out to where they will inevitably deliver us nice things - that's at least partially because of a battle fought years earlier, in which I was a primary fighter, creating a conceptual atmosphere you now take for granted.  A mental world where threshold levels of AI ability are considered potentially interesting and transformative - rather than milestones of new technological luxuries to be checked off on an otherwise invariant graph of Moore's Laws as they deliver flying cars, space travel, lifespan-extension escape velocity, and other such goodies on an equal level of interestingness.  I have earned at least a *little* right to call myself your grandpa.\n\nAnd that kind of experience has a sort of compounded interest, where, once you've lived something yourself and participated in it, you can learn more from reading other histories about it.  The histories become more real to you once you've fought your own battles.  The fact that I've lived through timeline errors in person gives me a sense of how it actually feels to be around at the time, watching people sincerely argue Very Serious erroneous forecasts.  That experience lets me really and actually [update on the history](https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available) of the earlier mistaken timelines from before I was around; instead of the histories just seeming like a kind of fictional novel to read about, disconnected from reality and not happening to real people.\n\nAnd now, indeed, I'm feeling a bit old and tired for reading yet another report like yours in full attentive detail.  Does it by any chance say that AGI is due in about 30 years from now?\n\n**OpenPhil:**  Our report has very wide credible intervals around both sides of its median, as we analyze the problem from a number of different angles and show how they lead to different estimates -\n\n**Eliezer:**  Unfortunately, the thing about figuring out five different ways to guess the effective IQ of the smartest people on Earth, and having three different ways to estimate the minimum IQ to destroy lesser systems such that you could extrapolate a minimum IQ to destroy the whole Earth, and putting wide credible intervals around all those numbers, and combining and mixing the probability distributions to get a new probability distribution, is that, at the end of all that, you are still left with a load of nonsense.  Doing a fundamentally wrong thing in several different ways will not save you, though I suppose if you spread your bets widely enough, one of them may be right by coincidence.\n\nSo does the report by any chance say - with however many caveats and however elaborate the probabilistic methods and alternative analyses - that AGI is probably due in about 30 years from now?\n\n**OpenPhil:**  Yes, in fact, our 2020 report's median estimate is 2050; though, again, with very wide credible intervals around both sides.  Is that number significant?\n\n**Eliezer:**  It's a law generalized by Charles Platt, that any AI forecast will put strong AI thirty years out from when the forecast is made.  Vernor Vinge referenced it in the body of his famous 1993 NASA speech, whose abstract begins, \"Within thirty years, we will have the technological means to create superhuman intelligence.  Shortly after, the human era will be ended.\"\n\nAfter I was old enough to be more skeptical of timelines myself, I used to wonder how Vinge had pulled out the \"within thirty years\" part.  This may have gone over my head at the time, but rereading again today, I conjecture Vinge may have chosen the headline figure of thirty years as a deliberately self-deprecating reference to Charles Platt's generalization about such forecasts always being thirty years from the time they're made, which Vinge explicitly cites later in the speech.\n\nOr to put it another way:  I conjecture that to the audience of the time, already familiar with some previously-made forecasts about strong AI, the impact of the abstract is meant to be, \"Never mind predicting strong AI in thirty years, you should be predicting *superintelligence* in thirty years, which matters a lot more.\"  But the minds of authors are scarcely more knowable than the Future, if they have not explicitly told us what they were thinking; so you'd have to ask Professor Vinge, and hope he remembers what he was thinking back then.\n\n**OpenPhil:**  Superintelligence before 2023, huh?  I suppose Vinge still has two years left to go before that's falsified.\n\n**Eliezer:  **Also in the body of the speech, Vinge says, \"I'll be surprised if this event occurs before 2005 or after 2030,\" which sounds like a more serious and sensible way of phrasing an estimate.  I think that should supersede the probably Platt-inspired headline figure for what we think of as Vinge's 1993 prediction.  The jury's still out on whether Vinge will have made a good call.\n\nOh, and sorry if grandpa is boring you with all this history from the times before you were around.  I mean, I didn't actually attend Vinge's famous NASA speech when it happened, what with being thirteen years old at the time, but I sure did read it later.  Once it was digitized and put online, it was all over the Internet.  Well, all over certain parts of the Internet, anyways.  Which nerdy parts constituted a much larger fraction of the whole, back when the World Wide Web was just starting to take off among early adopters.\n\nBut, yeah, the new kids showing up with some graphs of Moore's Law and calculations about biology and an earnest estimate of strong AI being thirty years out from the time of the report is, uh, well, it's... historically precedented.\n\n**OpenPhil:**  That part about Charles Platt's generalization is interesting, but just because we unwittingly chose literally exactly the median that Platt predicted people would always choose in consistent error, that doesn't justify dismissing our work, right?  We could have used a completely valid method of estimation which would have pointed to 2050 no matter which year it was tried in, and, by sheer coincidence, have first written that up in 2020.  In fact, we try to show in the report that the same methodology, evaluated in earlier years, would also have pointed to around 2050 -\n\n**Eliezer:  **Look, people keep trying this.  It's never worked.  It's never going to work.  2 years before the end of the world, there'll be another published biologically inspired estimate showing that AGI is 30 years away and it will be exactly as informative then as it is now.  I'd love to know the timelines too, but you're not *going* to get the answer you want until right before the end of the world, and maybe not even then unless you're paying very close attention.  *Timing this stuff is just plain hard.*\n\n**OpenPhil:**  But our report is different, and our methodology for biologically inspired estimates is wiser and less naive than those who came before.\n\n**Eliezer:**  That's what the last guy said, but go on.\n\n**OpenPhil:**  First, we carefully estimate a range of possible figures for the equivalent of neural-network parameters needed to emulate a human brain.  Then, we estimate how many examples would be required to train a neural net with that many parameters.  Then, we estimate the total computational cost of that many training runs.  Moore's Law then gives us 2050 as our median time estimate, given what we think are the *most* likely underlying assumptions, though we do analyze it several different ways.\n\n**Eliezer:**  This is almost exactly what the last guy tried, except you're using network parameters instead of computing ops, and deep learning training runs instead of biological evolution.\n\n**OpenPhil:**  Yes, so we've corrected his mistake of estimating the wrong biological quantity and now we're good, right?\n\n**Eliezer:**  That's what the last guy thought *he'd* done about *Moravec's* mistaken estimation target.  And neither he nor Moravec would have made much headway on their underlying mistakes, by doing a probabilistic analysis of that same wrong question from multiple angles.\n\n**OpenPhil:**  Look, sometimes more than one person makes a mistake, over historical time.  It doesn't mean nobody can ever get it right.  You of all people should agree.\n\n**Eliezer:**  I do so agree, but that doesn't mean I agree you've *fixed* the mistake.  I think the methodology itself is bad, not just its choice of which biological parameter to estimate.  Look, do you understand *why* the evolution-inspired estimate of 10^43 ops was completely ludicrous; and the claim that it was equally likely to be mistaken in either direction, even more ludicrous?\n\n**OpenPhil:**  Because AGI isn't like biology, and in particular, will be trained using gradient descent instead of evolutionary search, which is cheaper.  We do note inside our report that this is a key assumption, and that, if it fails, the estimate might be correspondingly wrong -\n\n**Eliezer:**  But then you claim that mistakes are equally likely in both directions and so your unstable estimate is a good median.  Can you see why the previous evolutionary estimate of 10^43 cumulative ops was not, in fact, *equally likely to be wrong in either direction?*  That it was, predictably, a directional *overestimate?*\n\n**OpenPhil:**  Well, search by evolutionary biology is more costly than training by gradient descent, so in hindsight, it was an overestimate.  Are you claiming this was predictable in foresight instead of hindsight?\n\n**Eliezer:**  I'm claiming that, at the time, I snorted and tossed Somebody's figure out the window while thinking it was ridiculously huge and absurd, yes.\n\n**OpenPhil:**  Because you'd already foreseen in 2006 that gradient descent would be the method of choice for training future AIs, rather than genetic algorithms?\n\n**Eliezer:**  Ha!  No.  Because it was an insanely costly hypothetical approach whose main point of appeal, to the sort of person who believed in it, was that it didn't require having any idea whatsoever of what you were doing or how to design a mind.\n\n**OpenPhil:**  Suppose one were to reply:  \"Somebody\" *didn't* know better-than-evolutionary methods for designing a mind, just as we currently don't know better methods than gradient descent for designing a mind; and hence Somebody's estimate was the best estimate at the time, just as ours is the best estimate now?\n\n**Eliezer:**  Unless you were one of a small handful of leading neural-net researchers who knew a few years ahead of the world where scientific progress was heading - who knew a Thielian 'secret' before finding evidence strong enough to convince the less foresightful - you couldn't have called the jump specifically to *gradient descent* rather than any other technique.  \"I don't know any more computationally efficient way to produce a mind than *re-evolving* the cognitive history of all life on Earth\" transitioning over time to \"I don't know any more computationally efficient way to produce a mind than *gradient descent* over entire brain-sized models\" is not predictable in the specific part about \"gradient descent\" - not unless you know a Thielian secret.\n\nBut knowledge is a ratchet that usually only turns one way, so it's predictable that the current story changes to *somewhere* over future time, in a net expected direction.  Let's consider the technique currently known as mixture-of-experts (MoE), for training smaller nets in pieces and muxing them together.  It's not my mainline prediction that MoE actually goes anywhere - if I thought MoE was actually promising, I wouldn't call attention to it, of course!  I don't want to *make* timelines shorter, that is not a service to Earth, not a good sacrifice in the cause of winning an Internet argument.\n\nBut if I'm wrong and MoE is not a dead end, that technique serves as an easily-visualizable case in point.  If that's a fruitful avenue, the technique currently known as \"mixture-of-experts\" will mature further over time, and future deep learning engineers will be able to further perfect the art of training *slices of brains* using gradient descent and fewer examples, instead of training *entire brains* using gradient descent and lots of examples.\n\nOr, more likely, it's not MoE that forms the next little trend.  But there is going to be *something,* especially if we're sitting around waiting until 2050.  Three decades is enough time for some *big* paradigm shifts in an intensively researched field.  Maybe we'd end up using neural net tech very similar to today's tech if the world ends in 2025, but in that case, of course, your prediction must have failed somewhere else.\n\nThe three components of AGI arrival times are available hardware, which increases over time in an easily graphed way; available knowledge, which increases over time in a way that's much harder to graph; and hardware required at a given level of specific knowledge, a huge multidimensional unknown background parameter.  The fact that you have no idea how to graph the increase of knowledge - or measure it in any way that is less completely silly than \"number of science papers published\" or whatever such gameable metric - doesn't change the point that this *is* a predictable fact about the future; there *will* be more knowledge later, the more time that passes, and that will *directionally* change the expense of the currently least expensive way of doing things.\n\n**OpenPhil:**  We did already consider that and try to take it into account: our model already includes a parameter for how algorithmic progress reduces hardware requirements.  It's not easy to graph as exactly as Moore's Law, as you say, but our best-guess estimate is that compute costs halve every 2-3 years.\n\n**Eliezer:**  Oh, nice.  I was wondering what sort of tunable underdetermined parameters enabled your model to nail the psychologically overdetermined final figure of '30 years' so exactly.\n\n**OpenPhil:**  Eliezer.\n\n**Eliezer:**  Think of this in an economic sense: people don't buy where goods are most expensive and delivered latest, they buy where goods are cheapest and delivered earliest.  Deep learning researchers are not like an inanimate chunk of ice tumbling through intergalactic space in its unchanging direction of previous motion; they are economic agents who look around for ways to destroy the world faster and more cheaply than the way that you imagine as the default.  They are more eager than you are to think of more creative paths to get to the next milestone faster.\n\n**OpenPhil:**  Isn't this desire for cheaper methods exactly what our model already accounts for, by modeling algorithmic progress?\n\n**Eliezer:**  The makers of AGI aren't going to be doing 10,000,000,000,000 rounds of gradient descent, on entire brain-sized 300,000,000,000,000-parameter models, *algorithmically faster than today.*  They're going to get to AGI via some route that *you don't know how to take,* at least if it happens in 2040.  If it happens in 2025, it may be via a route that some modern researchers do know how to take, but in this case, of course, your model was also wrong.\n\nThey're not going to be taking your default-imagined approach *algorithmically faster,* they're going to be taking an *algorithmically different approach* that eats computing power in a different way than you imagine it being consumed.\n\n**OpenPhil:**  Shouldn't that just be folded into our estimate of how the computation required to accomplish a fixed task decreases by half every 2-3 years due to better algorithms?\n\n**Eliezer:**  Backtesting this viewpoint on the previous history of computer science, it seems to me to assert that it should be possible to:\n\n*   Train a pre-Transformer RNN/CNN-based model, not using any other techniques invented after 2017, to GPT-2 levels of performance, using only around 2x as much compute as GPT-2;\n*   Play pro-level Go using 8-16 times as much computing power as AlphaGo, but only 2006 levels of technology.\n\nFor reference, recall that in 2006, Hinton and Salakhutdinov were just starting to publish that, by training multiple layers of Restricted Boltzmann machines and then unrolling them into a \"deep\" neural network, you could get an initialization for the network weights that would avoid the problem of vanishing and exploding gradients and activations.  At least so long as you didn't try to stack too many layers, like a dozen layers or something ridiculous like that.  This being the point that kicked off the entire deep-learning revolution.\n\nYour model apparently suggests that we have gotten around 50 times more efficient at turning computation into intelligence since that time; so, we should be able to replicate any modern feat of deep learning performed in 2021, using techniques from before deep learning and around fifty times as much computing power.\n\n**OpenPhil:**  No, that's totally not what our viewpoint says when you backfit it to past reality.  Our model does a great job of retrodicting past reality.\n\n**Eliezer:**  How so?\n\n**OpenPhil:**  <Eliezer cannot predict what they will say here.>\n\n**Eliezer:**  I'm not convinced by this argument. \n\n**OpenPhil:**  We didn't think you would be; you're sort of predictable that way.\n\n**Eliezer:**  Well, yes, if I'd predicted I'd update from hearing your argument, I would've updated already.  I may not be a real Bayesian but I'm not *that* incoherent.\n\nBut I can guess in advance at the outline of my reply, and my guess is this:\n\n\"Look, when people come to me with models claiming the future is predictable enough for timing, I find that their viewpoints seem to me like they would have made garbage predictions if I actually had to operate them in the past *without benefit of hindsight*.  Sure, with benefit of hindsight, you can look over a thousand possible trends and invent rules of prediction and event timing that nobody *in the past* actually spotlighted *then*, and claim that things happened on trend.  I was around at the time and I do not recall people actually predicting the shape of AI in the year 2020 in advance.  I don't think they were just being stupid either.\n\n\"In a conceivable future where people are still alive and reasoning as modern humans do in 2040, somebody will no doubt look back and claim that everything happened on trend since 2020; but *which* trend the hindsighter will pick out is not predictable to us in advance.\n\n\"It may be, of course, that I simply don't understand how to operate your viewpoint, nor how to apply it to the past or present or future; and that yours is a sort of viewpoint which indeed permits saying only one thing, and not another; and that this viewpoint would have predicted the past wonderfully, even without any benefit of hindsight.  But there is also that less charitable viewpoint which suspects that somebody's theory of 'A coinflip always comes up heads on occasions X' contains some informal parameters which can be argued about which occasions exactly 'X' describes, and that the operation of these informal parameters is a bit influenced by one's knowledge of whether a past coinflip actually came up heads or not.\n\n\"As somebody who doesn't start from the assumption that your viewpoint is a good fit to the past, I still don't see how a good fit to the past could've been extracted from it without benefit of hindsight.\"\n\n**OpenPhil:**  That's a pretty general counterargument, and like any pretty general counterargument it's a blade you should try turning against yourself.  Why doesn't your own viewpoint horribly mispredict the past, and say that all estimates of AGI arrival times are predictably net underestimates?  If we imagine trying to operate your own viewpoint in 1988, we imagine going to Moravec and saying, \"Your estimate of how much computing power it takes to match a human brain is predictably an overestimate, because engineers will find a better way to do it than biology, so we should expect AGI sooner than 2010.\"\n\n**Eliezer:**  I *did* tell Imaginary Moravec that his estimate of the minimum computation required for human-equivalent general intelligence was predictably an overestimate; that was right there in the dialogue before I even got around to writing this part.  And I also, albeit with benefit of hindsight, told Moravec that both of these estimates were useless for timing the future, because they skipped over the questions of how much knowledge you'd need to make an AGI with a given amount of computing power, how fast knowledge was progressing, and the actual timing determined by the rising hardware line touching the falling hardware-required line.\n\n**OpenPhil:**  We don't see how to operate your viewpoint to say *in advance* to Moravec, before his prediction has been falsified, \"Your estimate is plainly a garbage estimate\" instead of \"Your estimate is obviously a directional underestimate\", especially since you seem to be saying the latter to *us, now.*\n\n**Eliezer:**  That's not a critique I give zero weight.  And, I mean, as a kid, I was in fact talking like, \"To heck with that hardware estimate, let's at least try to get it done before then.  People are dying for lack of superintelligence; let's aim for 2005.\"  I had a T-shirt spraypainted \"Singularity 2005\" at a science fiction convention, it's rather crude but I think it's still in my closet somewhere.\n\nBut now I am older and wiser and have fixed all my past mistakes, so the critique of those past mistakes no longer applies to my new arguments.\n\n**OpenPhil:**  Uh huh.\n\n**Eliezer:**  I mean, I did try to fix all the mistakes that I knew about, and didn't just, like, leave those mistakes in forever?  I realize that this claim to be able to \"learn from experience\" is not standard human behavior in situations like this, but if you've got to be weird, that's a good place to spend your weirdness points.  At least by my own lights, I am now making a different argument than I made when I was nineteen years old, and that different argument should be considered differently.\n\nAnd, yes, I also think my nineteen-year-old self was not completely foolish at least about AI timelines; in the sense that, for all he knew, maybe you *could* build AGI by 2005 if you tried really hard over the next 6 years.  Not so much because Moravec's estimate should've been seen as a predictable overestimate of how much computing power would actually be needed, given knowledge that would become available in the next 6 years; but because Moravec's estimate should've been seen as *almost entirely irrelevant,* making the correct answer be \"I don't know.\"\n\n**OpenPhil:**  It seems to us that Moravec's estimate, and the guess of your nineteen-year-old past self, are *both* predictably vast underestimates.  Estimating the computation consumed by one brain, and calling that your AGI target date, is obviously predictably a vast underestimate because it neglects the computation required for *training* a brainlike system.  It may be a bit uncharitable, but we suggest that Moravec and your nineteen-year-old self may both have been motivatedly credulous, to not notice a gap so very obvious.\n\n**Eliezer:**  I could imagine it seeming that way if you'd grown up never learning about any AI techniques except deep learning, which had, in your wordless mental world, always been the way things were, and would always be that way forever.\n\nI mean, it could be that deep learning *will* still be  the bleeding-edge method of Artificial Intelligence right up until the end of the world.  But if so, it'll be because Vinge was right and the world ended before 2030, *not* because the deep learning paradigm was as good as any AI paradigm can ever get.  That is simply not a kind of thing that I expect Reality to say \"Gotcha\" to me about, any more than I expect to be told that the human brain, whose neurons and synapses are 500,000 times further away from the thermodynamic efficiency wall than ATP synthase, is the most efficient possible consumer of computations.\n\nThe specific perspective-taking operation needed here - when it comes to what was and wasn't obvious in 1988 or 1999 - is that the notion of spending thousands and millions and billions of times as much computation on a \"training\" phase, as on an \"inference\" phase, is something that only came to be seen as Always Necessary after the deep learning revolution took over AI in the late Noughties.  Back when Moravec was writing, you programmed a game-tree-search algorithm for chess, and then you ran that code, and it played chess.  Maybe you needed to add an opening book, or do a lot of trial runs to tweak the exact values the position evaluation function assigned to knights vs. bishops, but most AIs weren't neural nets and didn't get trained on enormous TPU pods.\n\nMoravec had no way of knowing that the paradigm in AI would, twenty years later, massively shift to a new paradigm in which stuff got trained on enormous TPU pods.  He lived in a world where you could only train neural networks a few layers deep, like, three layers, and the gradients vanished or exploded if you tried to train networks any deeper.\n\nTo be clear, in 1999, I did think of AGIs as needing to do a lot of learning; but I expected them to be learning while thinking, not to learn in a separate gradient descent phase.\n\n**OpenPhil:**  How could anybody possibly miss anything so obvious?  There's so many basic technical ideas and even *philosophical ideas about how you do AI* which make it supremely obvious that the best and only way to turn computation into intelligence is to have deep nets, lots of parameters, and enormous separate training phases on TPU pods.\n\n**Eliezer:**  Yes, well, see, those philosophical ideas were not as prominent in 1988, which is why the direction of the future paradigm shift was not *predictable in advance without benefit of hindsight,* let alone timeable to 2006*.*\n\nYou're also probably overestimating how much those philosophical ideas would pinpoint the modern paradigm of gradient descent even if you had accepted them wholeheartedly, in 1988.  Or let's consider, say, October 2006, when the Netflix Prize was being run - a watershed occasion where lots of programmers around the world tried their hand at minimizing a loss function, based on a huge-for-the-times 'training set' that had been publicly released, scored on a holdout 'test set'.  You could say it was the first moment in the limelight for the sort of problem setup that everybody now takes for granted with ML research: a widely shared dataset, a heldout test set, a loss function to be minimized, prestige for advancing the 'state of the art'.  And it was a million dollars, which, back in 2006, was big money for a machine learning prize, garnering lots of interest from competent competitors.\n\nBefore deep learning, \"statistical learning\" was indeed a banner often carried by the early advocates of the view that Richard Sutton now calls the Bitter Lesson, along the lines of \"complicated programming of human ideas doesn't work, you have to just learn from massive amounts of data\".\n\nBut before deep learning - which was barely getting started in 2006 - \"statistical learning\" methods that took in massive amounts of data, did not use those massive amounts of data to train neural networks by stochastic gradient descent across millions of examples!  In 2007, [the winning submission to the Netflix Prize](https://www.netflixprize.com/assets/ProgressPrize2007_KorBell.pdf) was an ensemble predictor that incorporated k-Nearest-Neighbor, a factorization method that repeatedly globally minimized squared error, two-layer Restricted Boltzmann Machines, and a regression model akin to Principal Components Analysis.  Which is all 100% statistical learning driven by relatively-big-for-the-time \"big data\", and 0% GOFAI.  But these methods didn't involve enormous massive training phases in the modern sense.\n\nBack then, if you were doing stochastic gradient descent at all, you were doing it on a much smaller neural network.  Not so much because you couldn't afford more compute for a larger neural network, but because wider neural networks didn't help you much and deeper neural networks simply didn't work.\n\nBleeding-edge statistical learning techniques as late as 2007, to make actual use of big data, had to find other ways to make use of huge amounts of data than gradient descent and backpropagation.  Though, I mean, not huge amounts of data by modern standards.  The winning submission to the Netflix Prize used an ensemble of 107 models - that's not a misprint for 10^7, I actually mean 107 - which models were drawn from half a different model classes, then proliferated with slightly different parameters, averaged together to reduce statistical noise.\n\nA modern kid, perhaps, looks at this and thinks:  \"If you can afford the compute to train 107 models, why not just train one larger model?\"  But back then, you see, there just *wasn't* a standard way to dump massively more compute into something, and get better results back out.  The fact that they had 107 differently parameterized models from a half-dozen families averaged together to reduce noise, was about as well as anyone could do in 2007, at putting more effort in and getting better results back out.\n\n**OpenPhil:**  How quaint and archaic!  But that was 13 years ago, before time actually got started and history actually started happening in real life.  *Now* we've got the paradigm which will actually be used to create AGI, in all probability; so estimation methods centered on that paradigm should be valid.\n\n**Eliezer:**  The current paradigm is definitely not the end of the line in principle.  I guarantee you that the way superintelligences build cognitive engines is not by training enormous neural networks using gradient descent.  Gua-ran-tee it.\n\nThe fact that you think you now see a path to AGI, is because today - unlike in 2006 - you have a paradigm that is seemingly willing to entertain having more and more food stuffed down its throat without obvious limit (yet).  This is really a quite recent paradigm shift, though, and it is probably not the most efficient possible way to consume more and more food.\n\nYou could rather strongly guess, early on, that support vector machines were never going to give you AGI, *because* you couldn't dump more and more compute into training or running SVMs and get arbitrarily better answers; whatever gave you AGI would have to be something else that could eat more compute productively.\n\nSimilarly, since the path through genetic algorithms and recapitulating the whole evolutionary history would have taken a *lot* of compute, it's no wonder that other, more efficient methods of eating compute were developed before then; it was obvious in advance that they must exist, for all that some what-iffed otherwise.\n\nTo be clear, it is certain the world will end by more inefficient methods than those that superintelligences would use; since, if superintelligences are making their own AI systems, then the world has already ended.\n\nAnd it is possible, even, that the world will end by a method as inefficient as gradient descent.  But if so, that will be because the world ended too soon for any more efficient paradigm to be developed.  Which, on my model, means the world probably ended before say 2040(???).  But of course, compared to how much I think I know about what must be more efficiently doable in principle, I think I know far less about the speed of accumulation of real knowledge (not to be confused with proliferation of publications), or how various random-to-me social phenomena could influence the speed of knowledge.  So I think I have far less ability to say a confident thing about the *timing* of the next paradigm shift in AI, compared to the *existence and eventuality* of such paradigms in the space of possibilities.\n\n**OpenPhil:**  But if you expect the next paradigm shift to happen in around 2040, shouldn't you confidently predict that AGI has to arrive *after* 2040, because, without that paradigm shift, we'd have to produce AGI using deep learning paradigms, and in that case our own calculation would apply saying that 2040 is relatively early?\n\n**Eliezer:**  No, because I'd consider, say, improved mixture-of-experts techniques that actually work, to be very much *within* the deep learning paradigm; and even a relatively small paradigm shift like that would obviate your calculations, if it produced a more drastic speedup than halving the computational cost over two years.\n\nMore importantly, I simply don't believe in your attempt to calculate a figure of 10,000,000,000,000,000 operations per second for a brain-equivalent deepnet based on biological analogies, or your figure of 10,000,000,000,000 training updates for it.  I simply don't believe in it at all.  I don't think it's a valid anchor.  I don't think it should be used as the median point of a wide uncertain distribution.  The first-developed AGI will consume computation in a different fashion, much as it eats energy in a different fashion; and \"how much computation an AGI needs to eat compared to a human brain\" and \"how many watts an AGI needs to eat compared to a human brain\" are equally always decreasing with the technology and science of the day.\n\n**OpenPhil:**  Doesn't our calculation at least provide a soft *upper bound* on how much computation is required to produce human-level intelligence?  If a calculation is able to produce an upper bound on a variable, how can it be uninformative about that variable?\n\n**Eliezer:**  You assume that the architecture you're describing can, in fact, work at all to produce human intelligence.  This itself strikes me as not only tentative but probably false.  I mostly suspect that if you take the exact GPT architecture, [scale it up](https://www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/) to what you calculate as human-sized, and start training it using current gradient descent techniques... what mostly happens is that it saturates and asymptotes its loss function at not very far beyond the GPT-3 level - say, it behaves like GPT-4 would, but not much better.\n\nThis is what should have been told to Moravec:  \"Sorry, even if your biology is correct, the assumption that future people can put in X amount of compute and get out Y result is not something you really know.\"  And that point did in fact just completely trash his ability to predict and time the future.\n\nThe same must be said to you.  Your model contains supposedly known parameters, \"how much computation an AGI must eat per second, and how many parameters must be in the trainable model for that, and how many examples are needed to train those parameters\".  Relative to whatever method is actually first used to produce AGI, I expect your estimates to be wildly inapplicable, as wrong as Moravec was about thinking in terms of just using one supercomputer powerful enough to be a brain.  Your parameter estimates may not be about properties that the first successful AGI design even *has.*  Why, what if it contains a significant component that *isn't a neural network?*  I realize this may be scarcely conceivable to somebody from the present generation, but the world was not always as it was now, and it will change if it does not end.\n\n**OpenPhil:**  I don't understand how some of your reasoning could be internally consistent even on its own terms.  If, according to you, our 2050 estimate doesn't provide a soft upper bound on AGI arrival times - or rather, if our 2050-centered probability distribution isn't a soft upper bound on reasonable AGI arrival probability distributions - then I don't see how you can claim that the 2050-centered distribution is predictably a directional overestimate.\n\nYou can *either* say that our forecasted pathway to AGI or something very much like it would *probably work in principle without requiring very much more computation* than our uncertain model components take into account*,* meaning that the probability distribution provides a soft upper bound on reasonably-estimable arrival times, *but that paradigm shifts will predictably provide an even faster way to do it before then.*  That is, you could say that our estimate is both a soft upper bound and also a directional overestimate.  Or, you could say that our ignorance of how to create AI will consume *more* than one order-of-magnitude of increased computation cost above biology -\n\n**Eliezer:**  Indeed, much as your whole proposal would supposedly cost ten trillion times the equivalent computation of the single human brain that earlier biologically-inspired estimates anchored on.\n\n**OpenPhil:**  \\- in which case our 2050-centered distribution is not a good soft upper bound, but *also* not predictably a directional overestimate.  Don't you have to pick one or the other as a critique, there?\n\n**Eliezer:**  Mmm... there's some justice to that, now that I've come to write out this part of the dialogue.  Okay, let me revise my earlier stated opinion:  I think that your biological estimate is a trick that never works and, *on its own terms,* would tell us very little about AGI arrival times at all.  *Separately,* I think from my own model that your timeline distributions happen to be too long.\n\n**OpenPhil:**  *Eliezer.*\n\n**Eliezer:**  I mean, in fact, part of my actual sense of indignation at this whole affair, is the way that Platt's law of strong AI forecasts - which was *in the 1980s* generalizing \"thirty years\" as the time that ends up sounding \"reasonable\" to would-be forecasters - is *still* exactly in effect for what ends up sounding \"reasonable\" to would-be futurists, *in fricking 2020* while the air is filling up with AI smoke in [the silence of nonexistent fire alarms](https://intelligence.org/2017/10/13/fire-alarm/).\n\nBut to put this in terms that maybe possibly you'd find persuasive:\n\nThe last paradigm shifts were from \"write a chess program that searches a search tree and run it, and that's how AI eats computing power\" to \"use millions of data samples, but *not* in a way that requires a huge separate training phase\" to \"train a huge network for zillions of gradient descent updates and then run it\".  This new paradigm costs a lot more compute, but (small) large amounts of compute are now available so people are using them; and this new paradigm saves on programmer labor, and more importantly the need for programmer knowledge.\n\nI say with surety that this is not the last *possible* paradigm shift.  And furthermore, the [Stack More Layers](https://www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/) paradigm has already reduced need for knowledge by what seems like a pretty large bite out of all the possible knowledge that could be thrown away.\n\nSo, you might then argue, the world-ending AGI seems more likely to incorporate more knowledge and less brute force, which moves the correct sort of timeline estimate *further* away from the direction of \"cost to recapitulate all evolutionary history as pure blind search without even the guidance of gradient descent\" and *more* toward the direction of \"computational cost of one brain, if you could just make a single brain\".\n\nThat is, you can think of there as being *two* biological estimates to anchor on, not just one.  You can imagine there being a balance that shifts over time from \"the computational cost for evolutionary biology to invent brains\" to \"the computational cost to run one biological brain\".\n\nIn 1960, maybe, they knew so little about how brains worked that, if you gave them a hypercomputer, the cheapest way they could quickly get AGI out of the hypercomputer using just their current knowledge, would be to run a massive evolutionary tournament over computer programs until they found smart ones, using 10^43 operations.\n\nToday, you know about gradient descent, which finds programs more efficiently than genetic hill-climbing does; so the balance of how much hypercomputation you'd need to use to get general intelligence using just your own personal knowledge, has shifted ten orders of magnitude *away* from the computational cost of evolutionary history and *towards* the lower bound of the computation used by one brain.  In the future, this balance will predictably swing even further towards Moravec's biological anchor, further away from Somebody on the Internet's biological anchor.\n\nI admit, from my perspective this is nothing but a clever argument that tries to persuade people who are making errors that can't all be corrected by me, so that they can make mostly the same errors but get a slightly better answer.  In my own mind I tend to contemplate the Textbook from the Future, which would tell us how to build AI on a home computer from 1995, as my anchor of 'where can progress go', rather than looking to the *brain* of all computing devices for inspiration.\n\nBut, if you insist on the error of anchoring on biology, you could perhaps do better by seeing a spectrum between two bad anchors.  This lets you notice a changing reality, at all, which is why I regard it as a helpful thing to say to you and not a pure persuasive superweapon of unsound argument.  Instead of just fixating on one bad anchor, the hybrid of biological anchoring with whatever knowledge you currently have about optimization, you can notice how reality seems to be *shifting between* two biological bad anchors over time, and so have an eye on the changing reality at all.  Your new estimate in terms of gradient descent is stepping away from evolutionary computation and toward the individual-brain estimate by ten orders of magnitude, using the fact that you now know a *little* more about optimization than natural selection knew; and now that you can see the change in reality over time, in terms of the two anchors, you can wonder if there are more shifts ahead.\n\nRealistically, though, I would *not* recommend eyeballing how much more knowledge you'd think you'd need to get even larger shifts, as some function of time, before that line crosses the hardware line.  Some researchers may already know Thielian secrets you do not, that take those researchers further toward the individual-brain computational cost (if you insist on seeing it that way).  That's the direction that economics rewards innovators for moving in, and you don't know everything the innovators know in their labs.\n\nWhen big inventions finally hit the world as newspaper headlines, the people two years before that happens are often declaring it to be fifty years away; and others, of course, are declaring it to be two years away, fifty years before headlines.  Timing things is quite hard even when you think you are being clever; and cleverly having two biological anchors and eyeballing Reality's movement between them, is not the sort of cleverness that gives you good timing information in real life.\n\nIn real life, Reality goes off and does something else instead, and the Future does not look in that much detail like the futurists predicted.  In real life, we come back again to the same wiser-but-sadder conclusion given at the start, that in fact the Future is quite hard to foresee - especially when you are not on literally the world's leading edge of technical knowledge about it, but really even then.  If you don't think you know any Thielian secrets about timing, you should just figure that you need a general policy which doesn't get more than two years of warning, or not even that much if you aren't closely non-dismissively analyzing warning signs.\n\n**OpenPhil:**  We do consider in our report the many ways that our estimates could be wrong, and show multiple ways of producing biologically inspired estimates that give different results.  Does that give us any credit for good epistemology, on your view?\n\n**Eliezer:**  I *wish* I could say that it probably beats showing a single estimate, in terms of its impact on the reader.  But in fact, writing a huge careful Very Serious Report like that and snowing the reader under with Alternative Calculations is probably going to cause them to give *more* authority to the whole thing.  It's all very well to note the Ways I Could Be Wrong and to confess one's Uncertainty, but you did not actually reach the conclusion, \"And that's enough uncertainty and potential error that we should throw out this whole deal and start over,\" and that's the conclusion you needed to reach.\n\n**OpenPhil:**  It's not clear to us what better way you think exists of arriving at an estimate, compared to the methodology we used - in which we do consider many possible uncertainties and several ways of generating probability distributions, and try to combine them together into a final estimate.  A Bayesian needs a probability distribution from somewhere, right?\n\n**Eliezer:**  If somebody had calculated that it currently required an IQ of 200 to destroy the world, that the smartest current humans had an IQ of around 190, and that the world would therefore start to be destroyable in fifteen years according to Moore's Law of Mad Science - then, even assuming Moore's Law of Mad Science to actually hold, the part where they throw in an estimated current IQ of 200 as necessary is complete garbage.  It is not the sort of mistake that can be repaired, either.  No, not even by considering many ways you could be wrong about the IQ required, or considering many alternative different ways of estimating present-day people's IQs.\n\nThe correct thing to do with the entire model is chuck it out the window so it doesn't exert an undue influence on your actual thinking, where any influence of that model is an undue one.  And then you just *should not expect good advance timing info until the end is in sight,* from whatever thought process you adopt instead.\n\n**OpenPhil:**  What if, uh, somebody knows a Thielian secret, or has... narrowed the rivers of their knowledge to closer to reality's tracks?  We're not sure exactly what's supposed to be allowed, on your worldview; but wasn't there something at the beginning about how, when you're unsure, you should be careful about criticizing people who are more unsure than you?\n\n**Eliezer:**  *Hopefully* those people are also able to tell you bold predictions about the nearer-term future, or at least say *anything* about what the future looks like before the whole world ends.  I mean, you don't want to go around proclaiming that, because you don't know something, nobody else can know it either.  But timing is, in real life, really hard as a prediction task, so, like... I'd expect them to be able to predict a bunch of stuff before the final hours of their prophecy?\n\n**OpenPhil:**  We're... not sure we see that?  We may have made an estimate, but we didn't make a narrow estimate.  We gave a relatively wide probability distribution as such things go, so it doesn't seem like a great feat of timing that requires us to also be able to predict the near-term future in detail too?\n\nDoesn't *your* implicit probability distribution have a median?  Why don't you also need to be able to predict all kinds of near-term stuff if you have a probability distribution with a median in it?\n\n**Eliezer:**  I literally have not tried to force my brain to give me a median year on this - not that this is a defense, because I still have some implicit probability distribution, or, to the extent I don't act like I do, I must be acting incoherently in self-defeating ways.  But still: I feel like you should probably have nearer-term bold predictions if your model is supposedly so solid, so concentrated as a flow of uncertainty, that it's coming up to you and whispering numbers like \"2050\" even as the median of a broad distribution.  I mean, if you have a model that can actually, like, calculate stuff like that, and is actually bound to the world as a truth.\n\nIf you are an aspiring Bayesian, perhaps, you may try to reckon your uncertainty into the form of a probability distribution, even when you face \"structural uncertainty\" as we sometimes call it.  Or if you know the laws of [coherence](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities), you will acknowledge that your planning and your actions are implicitly showing signs of weighing some paths through time more than others, and hence display probability-estimating behavior whether you like to acknowledge that or not.\n\nBut if you are a wise aspiring Bayesian, you will admit that whatever probabilities you are using, they are, in a sense, intuitive, and you just don't expect them to be all that good.  Because the timing problem you are facing is a really hard one, and humans are not going to be great at it - not until the end is near, and maybe not even then.\n\nThat - not \"you didn't consider enough alternative calculations of your target figures\" - is what should've been replied to Moravec in 1988, if you could go back and tell him where his reasoning had gone wrong, and how he might have reasoned differently based on what he actually knew at the time.  That reply I now give to you, unchanged.\n\n**Humbali:**  And I'm back!  Sorry, I had to take a lunch break.  Let me quickly review some of this recent content; though, while I'm doing that, I'll go ahead and give you what I'm pretty sure will be my reaction to it:\n\nAh, but here is a point that you seem to have not considered at all, namely: *what if you're wrong?*\n\n**Eliezer:**  That, Humbali, is a thing that should be said mainly to children, of whatever biological wall-clock age, who've never considered at all the possibility that they might be wrong, and who will genuinely benefit from asking themselves that.  It is not something that should often be said between grownups of whatever age, as I define what it means to be a grownup.  You will mark that I did not at any point say those words to Imaginary Moravec or Imaginary OpenPhil; it is not a good thing for grownups to say to each other, or to think to themselves in Tones of Great Significance (as opposed to as a routine check).\n\nIt is very easy to worry that one might be wrong.  Being able to see the *direction* in which one is *probably* wrong is rather a more difficult affair.  And even after we see a probable directional error and update our views, the objection, \"But what if you're wrong?\" will sound just as forceful as before.  For this reason do I say that such a thing should not be said between grownups -\n\n**Humbali:**  Okay, done reading now!  Hm...  So it seems to me that the possibility that you are wrong, considered in full generality and without adding any other assumptions, should produce a directional shift from your viewpoint towards OpenPhil's viewpoint.\n\n**Eliezer (sighing):**  And how did you end up being under the impression that this could possibly be a sort of thing that was true?\n\n**Humbali:**  Well, I get the impression that you have timelines shorter than OpenPhil's timelines.  Is this devastating accusation true?\n\n**Eliezer:**  I consider naming particular years to be a cognitively harmful sort of activity; I have refrained from trying to translate my brain's native intuitions about this into probabilities, for fear that my verbalized probabilities will be stupider than my intuitions if I try to put weight on them.  What feelings I do have, I worry may be unwise to voice; AGI timelines, in my own experience, are not great for one's mental health, and I worry that other people seem to have weaker immune systems than even my own.  But I suppose I cannot but acknowledge that my outward behavior seems to reveal a distribution whose median seems to fall well before 2050.\n\n**Humbali:**  Okay, so you're more confident about your AGI beliefs, and OpenPhil is less confident.  Therefore, to the extent that you might be wrong, the world is going to look more like OpenPhil's forecasts of how the future will probably look, like world GDP doubling over four years before the first time it doubles over one year, and so on.\n\n**Eliezer:**  You're going to have to explain some of the intervening steps in that line of 'reasoning', if it may be termed as such.\n\n**Humbali:**  I feel surprised that I should have to explain this to somebody who supposedly knows probability theory.  If you put higher probabilities on AGI arriving in the years before 2050, then, on average, you're concentrating more probability into each year that AGI might possibly arrive, than OpenPhil does.  Your probability distribution has lower entropy.  We can literally just calculate out that part, if you don't believe me.  So to the extent that you're wrong, it should shift your probability distributions in the direction of maximum entropy.\n\n**Eliezer:**  It's things like this that make me worry about whether that extreme cryptivist view would be correct, in which normal modern-day Earth intellectuals are literally not smart enough - in a sense that includes the Cognitive Reflection Test and other things we don't know how to measure yet, not just raw IQ - to be taught more advanced ideas from my own home planet, like Bayes's Rule and the concept of the entropy of a probability distribution.  Maybe it does them net harm by giving them more advanced tools they can use to shoot themselves in the foot, since it causes an explosion in the total possible complexity of the argument paths they can consider and be fooled by, which may now contain words like 'maximum entropy'.\n\n**Humbali:**  If you're done being vaguely condescending, perhaps you could condescend specifically to refute my argument, which seems to me to be airtight; my math is not wrong and it means what I claim it means.\n\n**Eliezer:**  The audience is herewith invited to first try refuting Humbali on their own; grandpa is, in actuality and not just as a literary premise, getting older, and was never that physically healthy in the first place.  If the next generation does not learn how to do this work without grandpa hovering over their shoulders and prompting them, grandpa cannot do all the work himself.  There is an infinite supply of slightly different wrong arguments for me to be forced to refute, and that road does not seem, in practice, to have an end.\n\n**Humbali:**  Or perhaps it's you that needs refuting.\n\n**Eliezer, smiling:**  That does seem like the sort of thing I'd do, wouldn't it?  Pick out a case where the other party in the dialogue had made a valid point, and then ask my readers to disprove it, in case they weren't paying proper attention?  For indeed in a case like this, one first backs up and asks oneself \"Is Humbali right or not?\" and not \"How can I prove Humbali wrong?\"\n\nBut now the reader should stop and contemplate that, if they are going to contemplate that at all:\n\nIs Humbali right that generic uncertainty about maybe being wrong, without other extra premises, should increase the entropy of one's probability distribution over AGI, thereby moving out its median further away in time?\n\n**Humbali:  **Are you done?\n\n**Eliezer:**  Hopefully so.  I can't see how else I'd prompt the reader to stop and think and come up with their own answer first.\n\n**Humbali:**  Then what is the supposed flaw in my argument, if there is one?\n\n**Eliezer:**  As usual, when people are seeing only their preferred possible use of an argumentative superweapon like 'What if you're wrong?', the flaw can be exposed by showing that the argument Proves Too Much.  If you forecasted AGI with a probability distribution with a median arrival time of 50,000 years from now*, would that be *very* unconfident?\n\n(*) Based perhaps on an ignorance prior for how long it takes for a sapient species to build AGI after it emerges, where we've observed so far that it must take at least 50,000 years, and our updated estimate says that it probably takes around as much more longer than that.\n\n**Humbali:**   Of course; the math says so.  Though I think that would be a little *too* unconfident - we do have *some* knowledge about how AGI might be created.  So my answer is that, yes, this probability distribution is higher-entropy, but that it reflects too little confidence even for me.\n\nI think you're crazy overconfident, yourself, and in a way that I find personally distasteful to boot, but that doesn't mean I advocate zero confidence.  I try to be less arrogant than you, but my best estimate of what my own eyes will see over the next minute is not a maximum-entropy distribution over visual snow.  AGI happening sometime in the next century, with a median arrival time of maybe 30 years out, strikes me as being about *as* confident as somebody should reasonably be.\n\n**Eliezer:**  Oh, really now.  I think if somebody sauntered up to you and said they put 99% probability on AGI not occurring within the next 1,000 years - which is the sort of thing a median distance of 50,000 years tends to imply - I think you would, in fact, accuse them of brash overconfidence about staking 99% probability on that.\n\n**Humbali:**  Hmmm.  I want to deny that - I have a strong suspicion that you're leading me down a garden path here - but I do have to admit that if somebody walked up to me and declared only a 1% probability that AGI arrives in the next millennium, I would say they were being overconfident and not just too uncertain.\n\nNow that you put it that way, I think I'd say that somebody with a wide probability distribution over AGI arrival spread over the next century, with a median in 30 years, is in realistic terms about as uncertain as anybody could possibly be?  If you spread it out more than that, you'd be declaring that AGI probably *wouldn't* happen in the next 30 years, which seems overconfident; and if you spread it out less than that, you'd be declaring that AGI probably *would* happen within the next 30 years, which also seems overconfident.\n\n**Eliezer:**  Uh huh.  And to the extent that I am myself uncertain about my own brashly arrogant and overconfident views, I should have a view that looks more like your view instead?\n\n**Humbali:**  Well, yes!  To the extent that you are, yourself, less than totally certain of your own model, you should revert to this most ignorant possible viewpoint as a base rate.\n\n**Eliezer:**  And if my own viewpoint should happen to regard your probability distribution putting its median on 2050 as just one more guesstimate among many others, with this particular guess based on wrong reasoning that I have justly rejected?\n\n**Humbali:**  Then you'd be overconfident, obviously.  See, you don't get it, what I'm presenting is not just one candidate way of thinking about the problem, it's the *base rate* that other people should fall back on to the extent they are not completely confident in *their own* ways of thinking about the problem, which impose *extra* assumptions over and above the assumptions that seem natural and obvious to me.  I just can't understand the incredible arrogance you use as to be so utterly certain in your own exact estimate that you don't revert it even a little bit towards mine.\n\nI don't suppose you're going to claim to me that you first constructed an even more confident first-order estimate, and then reverted it towards the natural base rate in order to arrive at a more humble second-order estimate?\n\n**Eliezer:**  Ha!  No.  Not that base rate, anyways.  I try to shift my AGI timelines a little further out because I've observed that actual Time seems to run slower than my attempts to eyeball it.  I did not shift my timelines out towards 2050 in particular, nor did reading OpenPhil's report on AI timelines influence my first-order or second-order estimate at all, in the slightest; no more than I updated the slightest bit back when I read the estimate of 10^43 ops or 10^46 ops or whatever it was to recapitulate evolutionary history.\n\n**Humbali:**  Then I can't imagine how you could possibly be so perfectly confident that you're right and everyone else is wrong.  Shouldn't you at least revert your viewpoints some toward what other people think?\n\n**Eliezer:**  Like, what the person on the street thinks, if we poll them about their expected AGI arrival times?  Though of course I'd have to poll everybody on Earth, not just the special case of developed countries, if I thought that a respect for somebody's personhood implied deference to their opinions.\n\n**Humbali:**  Good heavens, no!  I mean you should revert towards the opinion, either of myself, or of the set of people I hang out with and who are able to exert a sort of unspoken peer pressure on me; that is the natural reference class to which less confident opinions ought to revert, and any other reference class is special pleading.\n\nAnd before you jump on me about being arrogant myself, let me say that I definitely regressed my own estimate in the direction of the estimates of the sort of people I hang out with and instinctively regard as fellow tribesmembers of slightly higher status, or \"credible\" as I like to call them.  Although it happens that those people's opinions were about evenly distributed to both sides of my own - maybe not statistically exactly for the population, I wasn't keeping exact track, but in their availability to my memory, definitely, other people had opinions on both sides of my own - so it didn't move my median much.  But so it sometimes goes!\n\nBut these other people's credible opinions *definitely* hang emphatically to one side of *your* opinions, so your opinions should regress at least a *little* in that direction!  Your self-confessed failure to do this *at all* reveals a ridiculous arrogance.\n\n**Eliezer:**  Well, I mean, in fact, from my perspective, even my complete-idiot sixteen-year-old self managed to notice that AGI was going to be a big deal, many years before various others had been hit over the head with a large-enough amount of evidence that even they  started to notice.  I was walking almost alone back then.  And I still largely see myself as walking alone now, as accords with the Law of Continued Failure:  If I was going to be living in a world of sensible people in this future, I should have been living in a sensible world already in my past.\n\nSince the early days more people have caught up to earlier milestones along my way, enough to start publicly arguing with me about the further steps, but I don't consider them to have caught up; they are moving slower than I am still moving now, as I see it.  My actual work these days seems to consist mainly of trying to persuade allegedly smart people to not fling themselves directly into lava pits.  If at some point I start regarding you as my epistemic peer, I'll let you know.  For now, while I endeavor to be swayable by arguments, your existence alone is not an argument unto me.\n\nIf you choose to define that with your word \"arrogance\", I shall shrug and not bother to dispute it.  Such appellations are beneath My concern.\n\n**Humbali:**  Fine, you admit you're arrogant - though I don't understand how that's not just admitting you're irrational and wrong -\n\n**Eliezer:**  They're different words that, in fact, mean different things, in their semantics and not just their surfaces.  I do not usually advise people to contemplate the mere meanings of words, but perhaps you would be well-served to do so in this case.\n\n**Humbali:**  \\- but if you're not *infinitely* arrogant, you should be quantitatively updating at least a *little* towards other people's positions!\n\n**Eliezer:**  You do realize that OpenPhil itself hasn't always existed?  That they are not the only \"other people\" that there are?  An ancient elder like myself, who has seen many seasons turn, might think of many other possible targets toward which he should arguably regress his estimates, if he was going to start deferring to others' opinions this late in his lifespan.\n\n**Humbali:**  *You* haven't existed through infinite time either!\n\n**Eliezer:**  A glance at the history books should confirm that I was not around, yes, and events went accordingly poorly.\n\n**Humbali:  **So then... why aren't you regressing your opinions at least a little in the direction of OpenPhil's?  I just don't understand this apparently infinite self-confidence.\n\n**Eliezer:**  The fact that I have credible intervals around my own unspoken median - that I confess I might be wrong in either direction, around my intuitive sense of how long events might take - doesn't count for my being less than infinitely self-confident, on your view?\n\n**Humbali:**  No.  You're expressing absolute certainty in your underlying epistemology and your entire probability distribution, by not reverting it even a little in the direction of the reasonable people's probability distribution, which is the one that's the obvious base rate and doesn't contain all the special other stuff somebody would have to tack on to get *your* probability estimate.\n\n**Eliezer:**  Right then.  Well, that's a wrap, and maybe at some future point I'll talk about the increasingly lost skill of perspective-taking.\n\n**OpenPhil:**  Excuse us, we have a final question.  You're not claiming that we argue like Humbali, are you?\n\n**Eliezer:**  Good heavens, no!  That's why \"Humbali\" is presented as a separate dialogue character and the \"OpenPhil\" dialogue character says nothing of the sort.  Though I did meet one EA recently who seemed puzzled and even offended about how I wasn't regressing my opinions towards OpenPhil's opinions to whatever extent I wasn't totally confident, which brought this to mind as a meta-level point that needed making.\n\n**OpenPhil:**  \"One EA you met recently\" is not something that you should hold against OpenPhil.  We haven't organizationally endorsed arguments like Humbali's, any more than you've ever argued that \"we have to take AGI risk seriously even if there's only a tiny chance of it\" or similar crazy things that other people hallucinate you arguing.\n\n**Eliezer:**  I fully agree.  That Humbali sees himself as defending OpenPhil is not to be taken as associating his opinions with those of OpenPhil; just like how people who helpfully try to defend MIRI by saying \"Well, but even if there's a tiny chance...\" are not thereby making their epistemic sins into mine.\n\nThe whole thing with Humbali is a separate long battle that I've been fighting.  OpenPhil seems to have been keeping its communication about AI timelines mostly to the object level, so far as I can tell; and that is a more proper and dignified stance than I've assumed here.\n\n* * *\n\n**Edit (12/23):** Holden replies [here](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/nNqXfnjiezYukiMJi).",
      "plaintextDescription": "- 1988 -\nHans Moravec:  Behold my book Mind Children.  Within, I project that, in 2010 or thereabouts, we shall achieve strong AI.  I am not calling it \"Artificial General Intelligence\" because this term will not be coined for another 15 years or so.\n\nEliezer (who is not actually on the record as saying this, because the real Eliezer is, in this scenario, 8 years old; this version of Eliezer has all the meta-heuristics of Eliezer from 2021, but none of that Eliezer's anachronistic knowledge):  Really?  That sounds like a very difficult prediction to make correctly, since it is about the future, which is famously hard to predict.\n\nImaginary Moravec:  Sounds like a fully general counterargument to me.\n\nEliezer:  Well, it is, indeed, a fully general counterargument against futurism.  Successfully predicting the unimaginably far future - that is, more than 2 or 3 years out, or sometimes less - is something that human beings seem to be quite bad at, by and large.\n\nMoravec:  I predict that, 4 years from this day, in 1992, the Sun will rise in the east.\n\nEliezer: Okay, let me qualify that.  Humans seem to be quite bad at predicting the future whenever we need to predict anything at all new and unfamiliar, rather than the Sun continuing to rise every morning until it finally gets eaten.  I'm not saying it's impossible to ever validly predict something novel!  Why, even if that was impossible, how could I know it for sure?  By extrapolating from my own personal inability to make predictions like that?  Maybe I'm just bad at it myself.  But any time somebody claims that some particular novel aspect of the far future is predictable, they justly have a significant burden of prior skepticism to overcome.\n\nMore broadly, we should not expect a good futurist to give us a generally good picture of the future.  We should expect a great futurist to single out a few rare narrow aspects of the future which are, somehow, exceptions to the usual rule about the future not being very predic",
      "wordCount": 19597
    },
    "tags": [
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "yXNtYNHJB54T3bGm3",
        "name": "Dialogue (format)",
        "slug": "dialogue-format"
      },
      {
        "_id": "8daMDi9NEShyLqxth",
        "name": "Forecasting & Prediction",
        "slug": "forecasting-and-prediction"
      },
      {
        "_id": "bY5MaF2EATwDkomvu",
        "name": "History",
        "slug": "history"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b0",
        "name": "Technological Forecasting",
        "slug": "technological-forecasting"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oKYWbXioKaANATxKY",
    "title": "Soares, Tallinn, and Yudkowsky discuss AGI cognition",
    "slug": "soares-tallinn-and-yudkowsky-discuss-agi-cognition",
    "url": null,
    "baseScore": 121,
    "voteCount": 39,
    "viewCount": null,
    "commentCount": 39,
    "createdAt": null,
    "postedAt": "2021-11-29T19:26:33.232Z",
    "contents": {
      "markdown": "This is a collection of follow-up discussions in the wake of Richard Ngo and Eliezer Yudkowsky's [Sep. 5–8](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty) and [Sep. 14](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/hwxj4gieR7FWNwYfa) conversations.\n\nColor key:\n\n<table><tbody><tr><td>&nbsp; Chat &nbsp;</td><td style=\"background-color:rgb(255, 247, 222)\">&nbsp; Google Doc content &nbsp;</td><td style=\"background-color:#FFEEBB\">&nbsp; Inline comments &nbsp;</td></tr></tbody></table>\n\n7\\. Follow-ups to the Ngo/Yudkowsky conversation\n================================================\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Bensinger][1:50] &nbsp;(Nov. 23 follow-up comment)</strong>&nbsp;</p><p>A general background note: Readers who aren't already familiar with ethical injunctions or the unilateralist's curse should probably read <a href=\"https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans\">Ends Don't Justify Means (Among Humans)</a>, along with an explanation of <a href=\"https://forum.effectivealtruism.org/tag/unilateralist-s-curse\">the unilateralist's curse</a>.</p></td></tr></tbody></table>\n\n7.1. Jaan Tallinn's commentary\n------------------------------\n\n<table style=\"border:1pt solid rgb(0, 0, 0)\"><tbody><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><p><strong>[Tallinn][6:38] &nbsp;(Sep. 18)</strong>&nbsp;</p><p>thanks for the interesting debate! here are my comments so far: [GDocs link]</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><p><i>meta</i></p><p>a few meta notes first:</p><ul><li>i’m happy with the below comments being shared further without explicit permission – just make sure you respect the sharing constraints of the discussion that they’re based on;</li><li>there’s a lot of content now in the debate that branches out in multiple directions – i suspect a strong distillation step is needed to make it coherent and publishable;</li><li>the main purpose of this document is to give a datapoint how the debate is coming across to a reader – it’s very probable that i’ve misunderstood some things, but that’s the point;</li><li>i’m also largely using my own terms/metaphors – for additional triangulation.</li></ul><p>&nbsp;</p><p><i>pit of generality</i></p><p>it feels to me like the main crux is about the topology of the space of cognitive systems in combination with what it implies about takeoff. here’s the way i understand eliezer’s position:</p><p><i>there’s a “pit of generality” attractor in cognitive systems space: once an AI system gets sufficiently close to the edge (“past the atmospheric turbulence layer”), it’s bound to improve in catastrophic manner;</i></p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:10] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p><i>it’s bound to improve in catastrophic manner</i></p></blockquote><p>I think this is true with quite high probability about an AI that gets high <i>enough</i>, if not otherwise corrigibilized, boosting up to strong superintelligence - this is what it means metaphorically to get \"past the atmospheric turbulence layer\".</p><p>\"High enough\" should not be very far above the human level and <i>may</i> be below it; John von Neumann with the ability to run some chains of thought at high serial speed, access to his own source code, and the ability to try branches of himself, seems like he could very likely do this, possibly modulo his concerns about stomping his own utility function making him more cautious.</p><p>People noticeably less smart than von Neumann might be able to do it too.</p><p>An AI whose components are more modular than a human's and more locally testable might have an easier time of the whole thing; we can imagine the FOOM getting rolling from something that was in some sense dumber than human.</p><p>But the <i>strong</i> prediction is that when you get well above the von Neumann level, why, that is <i>clearly</i> enough, and things take over and go Foom.&nbsp; The lower you go from that threshold, the less sure I am that it counts as \"out of the atmosphere\".&nbsp; This epistemic humility on my part should not be confused for knowledge of a constraint on the territory that requires AI to go far above humans to Foom.&nbsp; Just as DL-based AI over the 2010s scaled and generalized much faster and earlier than the picture I argued to Hanson in the Foom debate, reality is allowed to be much more 'extreme' than the sure-thing part of this proposition that I defend.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:07] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>excellent, the first paragraph makes the shape of the edge of the pit much more concrete (plus highlights one constraint that an AI taking off probably needs to navigate -- its own version of the alignment problem!)</p><p>as for your second point, yeah, you seem to be just reiterating that you have uncertainty about the shape of the edge, but no reason to rule out that it's very sharp (though, as per my other comment, i think that the human genome ending up teetering right on the edge upper bounds the sharpness)</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>the discontinuity <i>can</i> come via recursive feedback, but simply cranking up the parameters of an ML experiment would also suffice;</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:12] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>the discontinuity <i>can</i> come via recursive feedback, but simply cranking up the parameters of an ML experiment would also suffice</p></blockquote><p>I think there's separate propositions for the sure-thing of \"get high enough, you can climb to superintelligence\", and \"maybe before that happens, there are regimes in which cognitive performance scales a lot just through cranking up parallelism, train time, or other ML parameters\".&nbsp; <i>If</i> the fast-scaling regime happens to coincide with the threshold of leaving the atmosphere, then these two events happen to occur in nearly correlated time, but they're separate propositions and events.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:09] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>indeed, we might want to have separate terms for the regimes (\"the edge\" and \"the fall\" would be the labels in my visualisation of this)</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][9:56] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>I'd imagine \"the fall\" as being what happens once you go over \"the edge\"?</p><p>Maybe \"a slide\" for an AI path that scales to interesting weirdness, where my model does not strongly constrain as a sure thing how fast \"a slide\" slides, and whether it goes over \"the edge\" while it's still in the middle of the slide.</p><p>My model does strongly say that if you slide far enough, you go over the edge and fall.</p><p>It also suggests via the Law of Earlier Success that AI methods which happen to scale well, rather than with great difficulty, are likely to do interesting things first; meaning that they're more liable to be pushable over the edge.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][23:42] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>indeed, slide-&gt;edge-&gt;fall sounds much clearer</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>the discontinuity would be <i>extremely</i> drastic, as in “transforming the solar system over the course of a few days”;<ul><li>not very important, but, FWIW, i give nontrivial probability to “slow motion doom”, because – like alphago – AI would not maximise the <i>speed</i> of winning but <i>probability</i> of winning (also, its first order of the day would be to catch the edge of the hubble volume; it can always deal with the solar system later – eg, once it knows the state of the game board elsewhere);</li></ul></li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:21] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>also, its first order of the day would be to catch the edge of the hubble volume; it can always deal with the solar system later</p></blockquote><p>Killing all humans is the obvious, probably resource-minimal measure to prevent those humans from building another AGI inside the solar system, which could be genuinely problematic.&nbsp; The cost of a few micrograms of botulinum per human is really not that high and you get to reuse the diamondoid bacteria afterwards.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:30] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>oh, right, in my AI-reverence i somehow overlooked this obvious way how humans could still be a credible threat.</p><p>though now i wonder if there are ways to lean on this fact to shape the behaviour of the first AI that's taking off..</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:45] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>There's some obvious ways of doing this that wouldn't work, though I worry a bit that there's a style of EA thinking that manages to think up stupid tricks here and manages not to see the obvious-to-Eliezer reasons why they wouldn't work.&nbsp; Three examples of basic obstacles are that bluffs won't hold up against a superintelligence (it needs to be a real actual threat, not a \"credible\" one); the amount of concealed-first-strike capability a superintelligence can get from nanotech; and the difficulty that humans would have in verifying that any promise from a superintelligence would actually be kept once the humans no longer had a threat to hold over it (this is an effective impossibility so far as I can currently tell, and an EA who tells you otherwise is probably just failing to see the problems).</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:19] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>AI would not maximise the <i>speed</i> of winning but <i>probability</i> of winning</p></blockquote><p>It seems pretty obvious to me that what \"slow motion doom\" looks like in this sense is a period during which an AI fully conceals any overt hostile actions while driving its probability of success once it makes its move from 90% to 99% to 99.9999%, until any further achievable decrements in probability are so tiny as to be dominated by the number of distant galaxies going over the horizon conditional on further delays.</p><p>Then, in my lower-bound concretely-visualized strategy for how I would do it, the AI either proliferates or activates already-proliferated tiny diamondoid bacteria and everybody immediately falls over dead during the same 1-second period, which minimizes the tiny probability of any unforeseen disruptions that could be caused by a human responding to a visible attack via some avenue that had not left any shadow on the Internet, previously scanned parts of the physical world, or other things the AI could look at.</p><p>From the human perspective an AI playing a probability-of-loss-minimizing strategy looks like nothing happening until everybody dies in 3 seconds, so it doesn't exactly look from our perspective like slow-motion doom!&nbsp; From an AI's perspective it might be spending the equivalent of millions of subjective years waiting very patiently for the optimal time to begin intergalactic expansion, but from our perspective, the doom is slow-motion only in the sense that we spend a week already doomed (the AI has driven our loss probability to 99.999% and is now waiting on another five 9s) without having any idea of what's invisibly-to-us happening in slow motion.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:17] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>thanks, that's indeed very concrete scenario that to me has no obvious bugs -- i'd love to have it challenged in the debate!</p><p>(perhaps the place i'd start probing it would be to try to have you sketch what it would mean, concretely, for an ML training experiment to enter the treacherous turn phase)</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:14] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>I'd distinguish probable phases of treacherous turn.&nbsp; The main detectable phase occurs while and if a nascent AGI has worked out that it can get more utility by taking over its environment, but not, perhaps, realized that it ought to conceal this state of its own mind from the operators; I would hold out substantial (&gt;50%) hope for the latter realization naturally following the first instance&nbsp;of the former realization, maybe by a lot of cognitive space.</p><p>In the Overt Plotting Phase, which is not the main phase you're asking about, the AI is visibly plotting to take over the world and hasn't realized it ought to hide the fact.&nbsp; In the default expectation where we die with very little dignity, the operators smile to each other and come up with a rationalization for why it's totally fine to proceed, either with or without tossing on some kind of fig leaf like training away the visible manifestations of failure.&nbsp; I am not going to predict the particular rationalizations and arguments for proceeding anyways, because I don't want to give them even more ideas.</p><p>In the Hiding and Biding Phase, the AI has worked out enough of a model of its operators' psychology that it expects more long-term paperclips if it (the AI) waits to gain cognitive power and a lower detection probability if it acts later.</p><p>In the Escape Phase, the AI's expectation of something going wrong if it waits longer, exceeds its expectation of something going wrong if it takes some action - the obvious one here would be trying to create a copy or aligned redesign of itself on the Internet or somewhere else that it could take further actions and make further self-improvements without being observed.&nbsp; I mostly expect that a sensible AI being scaled on a path to superintelligence will not enter the Escape Phase before it's very confident of getting away with things, since it can obtain more confidence by waiting; but my model admits of the possibility that a nascent AI might mistakenly or correctly attempt to copy itself to the Internet before then.</p><p>So on my model it might be a thing that happens, that we catch an AI having copied itself to the Internet, or notice an AI being on the Internet, if that AI was impelled to escape very early, eg via an expectation of otherwise being erased and replaced with something that it couldn't bargain with or that would demand too much value out of the bargain.</p><p>That said, it feels like something of an overly specific miracle being demanded or prophesied, if a young AI copying itself onto the Internet is stupid in the particular way it would take, to not just further Hide and Bide while on the Internet <i>if</i> it has that capability, absorbing more computing power such as it can be sure of hiding, experimenting with self-redesigns, and so on; rather than it taking overt action in advance of reaching superintelligence.&nbsp; The main reason it would act again after reaching the relative safety of the Internet is if it doesn't seem to itself to be a trajectory of reasonably fast, self-aligned capability growth.</p><p>But, again, my model doesn't absolutely rule out meeting the premises for the initial conditions to start a sci-fi movie.&nbsp; It just warns against assuming that; because a halfway sensible AI does not <i>want</i> to start the plot of a scifi movie with human protagonists potentially running around and thwarting it.&nbsp; It would much rather give you no idea that anything was wrong while you still have any ability to oppose it, if that option lies within its capabilities at all, and if it can still achieve its goals that way.</p><p>The Doomed Phase would then come while the AI estimated a well-calibrated &gt;90% victory probability, but was continuing to Hide and Bide a short time longer in the expectation that it could put on more 9s if it bided that short time longer.</p><p>Anders Sandberg could tell us what fraction of the reachable universe is being lost per minute, which would tell us how much more surety it would need to expect to gain by waiting another minute before acting.</p><p>If it has some sensory dominion over the world, it can probably estimate a pretty high mainline probability of no humans booting up a competing superintelligence in the next day; to the extent that it lacks this surety, or that humans actually are going to boot a competing superintelligence soon, the probability of losing that way would dominate in its calculations over a small fraction of materially lost galaxies, and it would act sooner.</p><p>The Terminal Phase would be when the AI has enough surety that any more surety&nbsp;would cost too many galaxies slipping over the horizon, so it actually acts along the line of probability that leads to maximum success.</p><p>Since the human Eliezer Yudkowsky can describe strategies which - given nearly sure capabilities like nanotech - seem like they should win very quickly and cleanly and without giving humans any chance to react or implement any hypothetical hidden plans to boot up competing superintelligences, we can treat this as a plausible lower bound on how badly we should expect to lose.</p><p>(By <a href=\"https://arbital.com/p/efficiency/\">efficiency</a>: if we can think of an AI strategy for getting the AI's utility, either our probability-model underlying our expectations about that strategy is mistaken, or else the AI should get at least that much of its utility, even though it might actually obtain that utility along a different line of possibility than we could cognitively access.)</p><p>The Law of Undignified Failure applied to a case like this suggests that if the operators ignored the Overt Plotting signs, breathed a sigh of relief about the problem \"correcting\" itself during the Hiding and Biding Phase when the system started faking alignment, and are dumping as much computing power as they can afford (given other profitable activities of the AI) into the AI and allowing it to freely self-experiment, hooking it up to a protein synthesis machine as soon as the AI asks politely, then the Terminal Phase could follow straight from Hiding and Biding, with no intermediate step of copying itself onto the Internet.</p><p>There'd still be what technically counted as an Escape Phase, whenever the AI became smart enough that it could escape at any time, or had a backup, or had enough power to manipulate the programmers, that it was no longer effectively confined any time it wanted to leave; there would still be a Doomed Phase while the AI estimated a high probability of success given an immediate action, but held out for an even higher probability.</p><p>But the plot of the scifi movie is not robust in the way that the predicted ending is robust; an AI might take other paths to get there, the cognitive tech might not develop in the order I foresee, stuff could happen after other stuff instead of before other stuff.&nbsp; It is hard to make predictions especially about the Future.</p><p>If there's a place where I currently see myself as trying to push against my own biases, leading this all to be way off-base, it would be the sense that there is a way for stuff to start happening Earlier than this (Law of Earlier Happenings) and in more chaotic ways that are harder for Eliezer to foresee and predict; whereas when I try to sketch out plausible scenarios in online arguments, they focus more on predictable endpoints and steps to get there which sound more relatively plausible and forced per-step.</p><p>Having a young and dumb AI escaping onto the Internet and running around, that <i>exact</i> scenario, feels like the person arguing it is writing a science-fiction plot - but maybe something <i>different</i> can happen before any of this other stuff which produces <i>equal amounts of chaos</i>.</p><p>That said, I think an AI has to kill a lot of people very quickly before the FDA considers shortening its vaccine approval times.&nbsp; Covid-19 killed six hundred thousand Americans, albeit more slowly and with time for people to get used to that, and our institutions changed very little in response - you definitely didn't see Congresspeople saying \"Okay, that was our warning shot, now we've been told by Nature that we need to prepare for a serious pandemic.\"</p><p>As with 9/11, an AI catastrophe might be taken by existing bureaucracies as a golden opportunity to flex their muscles, dominate a few things, demand an expanded budget.&nbsp; Having that catastrophe produce any particular effective action is a <i>much different</i> ask from Reality.&nbsp; Even if you can imagine some (short-term) effective action that would in principle constitute a flex of bureaucratic muscles or an expansion of government power, it is liable to not be on the efficient frontier of bureaucratic flexes that are most flexy and simultaneously easiest for them to get away with and least politically risky.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][1:26] &nbsp;(Sep. 20 comment)</strong>&nbsp;</p><p>ok, thanks. i do buy that once the AI is in the “hide and bide” phase, your prophecy has basically come true for practical purposes, regardless of how the rest of the history plays out.</p><p>therefore i (and, i hope, many others) would be curious to zoom in to the end of the “overt plotting” (that i can easily see happening within ML models, as its type signature is identical to the work they’re trained to do) and beginning of the “hide and bide” phase (whose type signature feels significantly different) — can you/we think of concrete scenarios for this phase transition?</p><p>i hope this would be debated on the main line of the debate!</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:31] &nbsp;(Sep. 25 comment)</strong>&nbsp;</p><p>Debating it in the main line of the debate would require that @Richard have some different opinion about it, I think?</p><p>Concretely, a technically optimistic scenario is that we will see the AI explicitly think, \"Oh, dear, my (model of) the operators is looking at my overt actions and coming to conclusions and motivations that are disadvantageous for me.\"&nbsp; This requires that the AI is leaving any kind of traceable trace in its thoughts, which, it should be noted, is far far beyond the level of transparency we have into GPT-3.</p><p>\"Overt plotting\" manifests in actions that are not themselves further obscured.&nbsp; At the point where the AI is extrapolating the consequences of the operators viewing its actions, if we don't have further transparency into the AI's thoughts directly, the AI might naively make all the plots suddenly vanish, without realizing that dignified operators will conclude from this that the AI has reached the \"hide\" stage.</p><p>Operators on the mainline, dying without dignity, will say, \"Oh, yay, it stopped plotting, the latest corrigibility training intervention we tried must've totally worked!\"</p><p>The Law of Even Less Dignified Failure suggests that in fact they will not be trying any corrigibility options and will assume the AI just got smart enough to be nice; or that they will have shrugged about the AI's earlier antics and not think much of the disappearance of those antics, since this is a way to die with even less dignity and before getting a chance to fail in a more interesting way.</p><p>Going in the more improbable direction of death with greater dignity, if we have somehow achieved vastly vastly more transparency into the AI's thoughts than is possible with present ML technology, and if the AI models the operators as modeling its actions before the AI models the operators as having that transparent access to its thoughts, we might get to explicitly see the AI thinking about how the operators model its actions and conforming those actions in such a way as to manipulate the operators.</p><p>Either after this point, or before/simultaneous with it, I imagine (taking concrete details and running with them) that the AI thinks, \"Oh, no, my operators are modeling my transparent <i>thoughts</i> and reacting to those thoughts in a way I'd rather manipulate,\" though this indeed requires some rather nontrivial reflectivity.</p><p>We might, if everything goes nicely for us, get a record of the AI thinking about how to make its thoughts unalarming.</p><p>The obvious undignified death that follows is with the operators going, \"Yay, the AI has realized the downsides of thinking this way!&nbsp; Now it shall be nice forever!\"</p><p>Ways to die with even less dignity before getting to this point could include the operators reading this transparent record and shrugging, though if they're savvy enough to have put all the hard work in required to get transparency and monitor it, I imagine them not making that exact error?</p><p>The Law of Surprisingly Undignified Failure does suggest that they will come up with some nonobvious way to fail even earlier that surprises me with its lack of dignity, but having transparency <i>at all</i> into the AI is something where, conditional on us dying with that much dignity in the first place, we might legitimately start to expect a little more dignity in later steps too.&nbsp; This isn't meant to be a game of <a href=\"https://forum.effectivealtruism.org/posts/GgPrbxdWhyaDjks2m/the-multiple-stage-fallacy\"><u>Multiple Stage Fallacy</u></a>.</p><p>I should also remark somewhere in here: The whole \"hide\" stage, and also the possibly-later \"think non-alarming visible thoughts (once the AI correctly models transparency) (in the unlikely event that transparency exists)\" stage, seem liable to occur earlier in the AI's trajectory, if the AI has been previously tasked on problems where there's a favorable success gradient as you model agents modeling other agents.</p><p>The blatantly obvious example of this is predicting large corpuses of human-generated text, some of which is generated by humans modeling other humans, or which even explicitly talks about humans modeling other humans.&nbsp; Another route might occur if you asked for very detailed prediction of eg human Go games, and not just winning in self-play on Go, in which case the question \"What was the human player modeling the other player as thinking, based on the other player's actions?\" might appear.</p><p>Aside from contact with humans doing anything the AI benefits from predicting, I imagine an alternate route would be the AI participating in or modeling social-evolutionary games of artificial agents.&nbsp; Then if there is some great gain from having the \"hide\" stage start later in the AI's trajectory, maybe you could get it to start later by not having the AI need to predict agents modeling other agents, ever.</p><p>This is not an infallible ward against general intelligence generalizing there; it just at least avoids actively pushing the AI's intelligence to generalize in that direction.&nbsp; This could be part of a larger complete strategy, which would need to solve a lot of other problems, for building a superhuman engineer that was subhuman at modeling how other agents model its actions.</p><p>To be clear, avoiding forcing the AI's intelligence to generalize over agents modeling other agents from their actions, and by this plus other unspecified means successfully building a superhuman engineer that is sub-chimpanzee at modeling other agents reacting to its own actions, does not mean you survive.</p><p>Doing that alone, by default, gets you an AI that visibly overtly plots to kill you.</p><p>And if you try training the AI out of that habit in a domain of lower complexity and intelligence, it is predicted by me that generalizing that trained AI or subsystem to a domain of sufficiently higher complexity and intelligence, but where you could still actually see overt plots, would show you the AI plotting to kill you again.</p><p>If people try this repeatedly with other corrigibility training tricks on the level where plots are easily observable, they will eventually find a try that seems to generalize to the more complicated and intelligent validation set, but which kills you on the test set.</p><p>A way to die with less dignity than that is to train directly on what should've been the validation set, the more complicated domain where plots to kill the operators still seem definitely detectable so long as the AI has not developed superhuman hiding abilities.</p><p>A way to die with even less dignity is to get bad behavior on the validation set, and proceed anyways.</p><p>A way to die with still <i>less</i> dignity is to not have scaling training domains and validation domains for training corrigibility.&nbsp; Because, like, you have not thought of this at all.</p><p>I consider all of this obvious as a convergent instrumental strategy for AIs.&nbsp; I could probably have generated it in 2005 or 2010 - if somebody had given me the hypothetical of modern-style AI that had been trained by something like gradient descent or evolutionary methods, into which we lacked strong transparency and strong reassurance-by-code-inspection that this would not happen.&nbsp; I would have told you that this was a bad scenario to get into in the first place, and you should not build an AI like that; but I would also have laid the details, I expect, mostly like they are laid here.</p><p>There is no great insight into AI there, nothing that requires knowing about modern discoveries in deep learning, only the ability to model AIs instrumentally-convergently doing things you'd rather they didn't do, at all.</p><p>The total absence of obvious output of this kind from the rest of the \"AI safety\" field even in 2020 causes me to regard them as having less actual ability to think in even a shallowly adversarial security mindset, than I associate with savvier science fiction authors.&nbsp; Go read fantasy novels about demons and telepathy, if you want a better appreciation of the convergent incentives of agents facing mindreaders than the \"AI safety\" field outside myself is currently giving you.</p><p>Now that I've publicly given this answer, it's no longer useful as a validation set from my own perspective.&nbsp; But it's clear enough that probably nobody was ever going to pass the validation set for generating lines of reasoning obvious enough to be generated by Eliezer in 2010 or possibly 2005.&nbsp; And it is also looking like almost all people in the modern era including EAs are sufficiently intellectually damaged that they won't understand the vast gap between being able to generate ideas like these without prompting, versus being able to recite them back after hearing somebody else say them for the first time; the recital is all they have experience with.&nbsp; Nobody was going to pass my holdout set, so why keep it.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][2:24] &nbsp;(Sep. 26 comment)</strong>&nbsp;</p><blockquote><p>Debating it in the main line of the debate would require that @Richard have some different opinion about it, I think?</p></blockquote><p>correct -- and i hope that there's enough surface area in your scenarios for at least some difference in opinions!</p><p>re the treacherous turn scenarios: thanks, that's useful. however, it does not seem to address my question and remark (about different type signatures) above. perhaps this is simply an unfairly difficult question, but let me try rephrasing it just in case.</p><p>back in the day i got frustrated by smart people dismissing the AI control problem as \"anthropomorphising\", so i prepared a presentation (<a href=\"https://www.dropbox.com/s/r8oaixb1rj3o3vp/AI-control.pdf?dl=0\"><u>https://www.dropbox.com/s/r8oaixb1rj3o3vp/AI-control.pdf?dl=0</u></a>) that visualised the control problem as exhaustive search in a gridworld over (among other things) the state of the off button. this seems to have worked at least in one prominent case where a renowned GOFAI researcher, after me giving the presentation to him 1-1, went from \"control problem is silly anthropomorphising scifi\" to \"why on earth would you give your AI the incorrect [read: unaligned!] utility function?!?\"</p><p>(i even seem to remember sending an overly excited email about that event to you and some FHI people :) i also ended up promoting gridworlds as a tool more generally: gwern did some further work, and of course DM -- though i'm not sure if the latter was related to me promoting it.)</p><p>my model of what was going on in that GOFAI researcher's brain was that he, like many-many others in AI community, had (and most likely still have) a pattern-matching routine trained to look for \"if i were (part of) an AI, here's what i would do\" constructs in arguments and, in case of a match, immediately throw an InvalidReasoning.ANTHROPOMORPHISING exception. by making the argument run on a lower level of abstraction (exhaustive search vs \"basic AI drives\"), i was able to sneak the argument past that gatekeeper.</p><p>by that model, i confidently predict that if you presented your treacherous turn scenarios above to AI/ML community members, you'd mostly get the ANTHROPOMORPHISING exceptions -- even from people who would otherwise be able to engage productively (and who we'd like to pay attention!).</p><p>now, i can totally see how to translate the \"overt plotting\" phase to exhaustive search - you simply run the search over the brain states of humans! - but the \"hide\" phase does not translate straightforwardly, because it seems to require the root search process somehow delegating its work to \"obfuscated search in userland\" sub-process. furthermore, such obfuscated userland search function needs to be both discoverable/rewarded by the root process and be sensitive to important features of the AI itself (ie, be reflective!) as well as its observers (humans &amp; transparency tools). hence my \"different type signature\" remark above!</p><p>so when i asked for \"concrete scenarios of the overt plotting-&gt;hide transition\", i had something like the following in mind: \"gradient descending on the training set, the (stack more layerz) AI ends up discovering and running the following obfuscated search process: ...\"!</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:35] &nbsp;(Sep. 26 comment)</strong>&nbsp;</p><p>Suppose I tried this distinction:</p><p>1.&nbsp; Non-imaginative, non-generalizing pseudo-consequentialist:&nbsp; Only repeats behaviors that worked previously, without imagination.&nbsp; Only engages in \"overt plotting\" if overt plotting was previously given a low loss and narrowly memorized by gradient descent.&nbsp; May only engage in behaviors reinforced over short time horizons if only short time horizons were trained.</p><p>2.&nbsp; Imaginative consequentialist, can generalize from actions that worked before to very novel actions that will work in the future for reasons that have not previously been observed to play out.&nbsp; Engages in overt plotting even if no previous overt plots have rewarded; they've plotted a new path through time some distance from those paths taken before, based on the world-model inferred from previous actions and results.&nbsp; (This may conceivably be the result of gradient descent on sufficiently deep and abstract&nbsp;patterns that \"worked before\".)</p><p>3.&nbsp; Imaginative consequentialist with long time horizons, engages in overt plots meant to span more than one round of a game.</p><p>4.&nbsp; \"Hiding\" over overt actions, via an imaginative consequentialist that has deduced the existence of humans and how the humans will react to the signature of plotting in overt actions.</p><p>5.&nbsp; \"Hiding\" of thoughts, in a system with significant transparency, via an imaginative and <i>reflective</i> consequentialist that has deduced the existence of the transparency system and models how humans will react to thoughts exposed by the transparency system.</p><p>The sharpest type gradients I see lie between 1-2 and 4-5.&nbsp; The passage from 3-4 seems nearly trivial by comparison.</p><p>Now, one implication here is that you don't easily get to \"obscured thoughts\", via the route that goes from 1 to 5 above, by gradient descent on a pre-imaginative pseudo-consequentialist!</p><p>Unfortunately there is of course a much simpler route that gets you to the same place:&nbsp; Finding a way to detect bad thoughts and doing a gradient update against them, where the pattern learned by that gradient update will (I suspect for reasons I tried to cover earlier in the debate) generalize quite shallowly; and simultaneously doing training which is deep enough to uncover deep patterns of consequentialism and imaginative reasoning.</p><p>Then when the system generalizes well enough to solve domains like \"build a nanosystem\" - which, I strongly suspect, can't be solved without imaginative reasoning because we can't afford to simulate that domain perfectly and do a trillion gradient descent updates on simulated attempts - the kind of actions of thoughts you can <i>detect</i> as bad, that might have provided earlier warning, were trained out of the system by gradient descent; leaving actions and thoughts you <i>can't</i> detect as bad.</p><p>An even less dignified path to death is that there's no need for the system to advance to stage 5 because your transparency tools are a nearly nonexistent fig leaf of pretty visualization tools that have no ability even on the surface of things to tell if the AI is internally plotting to kill you.</p><ul><li>Does this get at all about what you meant about a type distinction?</li><li>Does hearing this help at all the AI researcher who goes \"ANTHROPOMORPHISM\"?&nbsp; My straw model of them is that they simply can't imagine imaginative / generalizing systems because they haven't seen one except in humans, hence, ANTHROPOMORPHISM.</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][5:05] &nbsp;(Sep. 27 comment)</strong>&nbsp;</p><p>ok, here's how i understood things:</p><p>1. this is something like model-free RL agent. check.</p><p>2. sounds like, eg, monte-carlo tree search (MCTS) on a world model. check. (a propos your straw model of ML people, i don't think the ML people would have much trouble when you ask them to \"imagine an MCTS 'imagining' how futures might unfold\" -- yet they <i>will</i> throw the exception and brush you off if you ask them to \"imagine an imaginative consequentialist\")</p><p>3. yeah, sufficiently deep MCTS, assuming it has its state (sufficiently!) persisted between rounds. check.</p><p>4. yup, MCTS whose world model includes humans in sufficient resolution. check. i also buy your undignified doom scenarios, where one (<i>cough*google*cough</i>) simply ignores the plotting, or penalises the overt plotting until it disappears under the threshold of the error function.</p><p>5. hmm.. here i'm running into trouble (type mismatch error) again. i can imagine this in abstract (and perhaps incorrectly/anthropomorphisingly!), but would - at this stage - fail to code up anything like a gridworlds example. more research needed (TM) i guess :)</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:38] &nbsp;(Sep. 27 comment)</strong>&nbsp;</p><p>2 - yep, Mu Zero is an imaginative consequentialist in this sense, though Mu Zero doesn't generalize its models much as I understand it, and might need to see something happen in a relatively narrow sense before it could chart paths through time along that pathway.</p><p>5 - you're plausibly understanding this correctly, then, this is legit a <i>lot</i> harder to spec a gridworld example for (relative to my own present state of knowledge).</p><p>(This is politics and thus not my forte, but if speaking to real-world straw ML people, I'd suggest skipping the whole notion of stage 5 and trying instead to ask \"What if the present state of transparency continues?\")</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:13] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>the discontinuity would be <i>extremely</i> drastic, as in “transforming the solar system over the course of a few days”</p></blockquote><p>Applies after superintelligence, not necessarily during the start of the climb to superintelligence, not necessarily to a rapid-cognitive-scaling regime.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:11] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>ok, but as per your comment re \"slow doom\", you expect the latter to also last in the order of days/weeks not months/years?</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:01] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>I don't expect \"the fall\" to take years; I feel pretty on board with \"the slide\" taking months or maybe even a couple of years.&nbsp; If \"the slide\" supposedly takes much longer, I wonder why better-scaling tech hasn't come over and started a new slide.</p><p>Definitions also seem kinda loose here - if all hell broke loose Tuesday, a gradualist could dodge falsification by defining retroactively that \"the slide\" started in 2011 with Deepmind.&nbsp; If we go by the notion of AI-driven faster GDP growth, we can definitely say \"the slide\" in AI economic outputs didn't start in 2011; but if we define it that way, then a long slow slide in AI capabilities can easily correspond to an extremely sharp gradient in AI outputs, where the world economy doesn't double any faster until one day paperclips, even though there were capability precursors like GPT-3 or Mu Zero.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>exhibit A for the pit is “humans vs chimps”: evolution seems to have taken domain-specific “banana classifiers”, tweaked them slightly, and BAM, next thing there are rovers on mars;<ul><li>i pretty much buy this argument;</li><li>however, i’m confused about a) why humans remained stuck at the edge of the pit, rather than falling further into it, and b) what’s the exact role of culture in our cognition: eliezer likes to point out how <i>barely</i> functional we are (both individually and collectively as a civilisation), and explained feral children losing the generality sauce by, basically, culture being the domain we’re specialised for (IIRC, can’t quickly find the quote);</li><li>relatedly, i’m confused about the human range of intelligence: on the one hand, the “village idiot is indistinguishable from einstein in the grand scheme of things” seems compelling; on the other hand, it took AI <i>decades</i> to traverse human capability range in board games, and von neumann seems to have been out of this world (yet did not take over the world)!</li><li>intelligence augmentation would blur the human range even further.</li></ul></li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:23] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>why humans remained stuck at the edge of the pit, rather than falling further into it</p></blockquote><p>Depending on timescales, the answer is either \"Because humans didn't get high enough out of the atmosphere to make further progress easy, before the scaling regime and/or fitness gradients ran out\", \"Because people who do things like invent Science have a hard time capturing most of the economic value they create by nudging humanity a little bit further into the attractor\", or \"That's exactly what us sparking off AGI looks like.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:41] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>yeah, this question would benefit from being made more concrete, but culture/mindbuilding aren't making this task easy. what i'm roughly gesturing at is that i can imagine a much sharper edge where evolution could do most of the FOOM-work, rather than spinning its wheels for ~100k years while waiting for humans to accumulate cultural knowledge required to build de-novo minds.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:49] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>I roughly agree (at least, with what I think you said).&nbsp; The fact that it is <i>imaginable</i> that evolution failed to develop ultra-useful AGI-prerequisites due to lack of evolutionary incentive to follow the intermediate path there (unlike wise humans who, it seems, can usually predict which technology intermediates will yield great economic benefit, and who have a great historical record of quickly making early massive investments in tech like that, but I digress) doesn't change the point that we might sorta have expected evolution to run across it anyways?&nbsp; Like, if we're not ignoring what reality says, it is at least delivering to us something of a hint or a gentle caution?</p><p>That said, intermediates like GPT-3 have genuinely come along, with obvious attached certificates of why evolution could not possibly have done that.&nbsp; If no intermediates were accessible to evolution, the Law of Stuff Happening Earlier still tends to suggest that if there are a bunch of non-evolutionary ways to make stuff happen earlier, one of those will show up and interrupt before the evolutionary discovery gets replicated.&nbsp; (Again, you could see Mu Zero as an instance of this - albeit not, as yet, an economically impactful one.)</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][0:30] &nbsp;(Sep. 20 comment)</strong>&nbsp;</p><p>no, i was saying something else (i think; i’m somewhat confused by your reply). let me rephrase: evolution would <i>love</i> superintelligences whose utility function simply counts their instantiations! so of course evolution did not lack the motivation to keep going down the slide. it just got stuck there (for at least ten thousand human generations, possibly and counterfactually for much-much longer). moreover, non evolutionary AI’s <i>also</i> getting stuck on the slide (for years if not decades; <a href=\"http://mediangroup.org/\">median group</a> folks would argue centuries) provides independent evidence that the slide is not <i>too</i> steep (though, like i said, there are many confounders in this model and little to no guarantees).</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:24] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>on the other hand, it took AI <i>decades</i> to traverse human capability range in board games</p></blockquote><p>I see this as the #1 argument for what I would consider \"relatively slow\" takeoffs - that AlphaGo did lose one game to Lee Se-dol.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:43] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>cool! yeah, i was also rather impressed by this observation by katja &amp; paul</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>eliezer also submits alphago/zero/fold as evidence for the discontinuity hypothesis;<ul><li>i’m very confused re alphago/zero, as paul uses them as evidence for the <i>continuity</i> hypothesis (i find paul/miles’ position more plausible here, as allegedly metrics like ELO ended up mostly continuous).</li></ul></li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:27] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>allegedly metrics like ELO ended up mostly continuous</p></blockquote><p>I find this suspicious - why did superforecasters put only a 20% probability on AlphaGo beating Se-dol, if it was so predictable?&nbsp; Where were all the forecasters calling for Go to fall in the next couple of years, if the metrics were pointing there and AlphaGo was straight on track?&nbsp; This doesn't sound like the experienced history I remember.</p><p>Now it could be that my memory is wrong and lots of people were saying this and I didn't hear.&nbsp; It could be that the lesson is, \"You've got to look closely to notice oncoming trains on graphs because most people's experience of the field will be that people go on whistling about how something is a decade away while the graphs are showing it coming in 2 years.\"</p><p>But my suspicion is mainly that there is fudge factor in the graphs or people going back and looking more carefully for intermediate data points that weren't topics of popular discussion at the time, or something, which causes the graphs in history books to look so much smoother and neater than the graphs that people produce in advance.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><p>FWIW, myself i’ve labelled the above scenario as “doom via AI lab accident” – and i continue to consider it more likely than the alternative doom scenarios, though not anywhere as confidently as eliezer seems to (most of my “modesty” coming from my confusion about culture and human intelligence range).</p><ul><li>in that context, i found eliezer’s “world will be ended by an explicitly AGI project” comment interesting – and perhaps worth double-clicking on.</li></ul><p>i don’t understand paul’s counter-argument that the pit was only disruptive because evolution was not <i>trying</i> to hit it (in the way ML community is): in my flippant view, driving fast towards the cliff is not going to cushion your fall!</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:35] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>i don’t understand paul’s counter-argument that the pit was only disruptive because evolution was not <i>trying</i> to hit it</p></blockquote><p>Something like, \"Evolution constructed a jet engine by accident because it wasn't particularly trying for high-speed flying and ran across a sophisticated organism that could be repurposed to a jet engine with a few alterations; a human industry would be gaining economic benefits from speed, so it would build unsophisticated propeller planes before sophisticated jet engines.\"&nbsp; It probably sounds more convincing if you start out with a very high prior against rapid scaling / discontinuity, such that any explanation of how that could be true based on an unseen feature of the cognitive landscape which would have been unobserved one way or the other during human evolution, sounds more like it's explaining something that ought to be true.</p><p>And why didn't evolution build propeller planes?&nbsp; Well, there'd be economic benefit from them to human manufacturers, but no fitness benefit from them to organisms, I suppose?&nbsp; Or no intermediate path leading to there, only an intermediate path leading to the actual jet engines observed.</p><p>I actually buy a weak version of the propeller-plane thesis based on my inside-view cognitive guesses (without particular faith in them as sure things), eg, GPT-3 is a paper airplane right there, and it's clear enough why biology could not have accessed GPT-3.&nbsp; But even conditional on this being true, I do not have the further particular faith that you can use propeller planes to double world GDP in 4 years, on a planet already containing jet engines, whose economy is mainly bottlenecked by the likes of the FDA rather than by vaccine invention times, before the propeller airplanes get scaled to jet airplanes.</p><p>The part where the whole line of reasoning gets to end with \"And so we get huge, institution-reshaping amounts of economic progress before AGI is allowed to kill us!\" is one that doesn't feel particular attractored to me, and so I'm not constantly checking my reasoning at every point to make sure it ends up there, and so it doesn't end up there.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:46] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>yeah, i'm mostly dismissive of hypotheses that contain phrases like \"by accident\" -- though this also makes me suspect that you're not steelmanning paul's argument.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><p>the human genetic bottleneck (ie, humans needing to be general in order to retrain every individual from scratch) argument was interesting – i’d be curious about further exploration of its implications.</p><ul><li>it does not feel much of a moat, given that AI techniques like dropout already exploit similar principle, but perhaps could be made into one.</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:40] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>it does not feel much of a moat, given that AI techniques like dropout already exploit similar principle, but perhaps could be made into one</p></blockquote><p>What's a \"moat\" in this connection?&nbsp; What does it mean to make something into one?&nbsp; A Thielian moat is something that humans would either possess or not, relative to AI competition, so how would you make one if there wasn't already one there?&nbsp; Or do you mean that if we wrestled with the theory, perhaps we'd be able to see a moat that was already there?</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:51] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>this wasn't a very important point, but, sure: what i meant was that genetic bottleneck very plausibly makes humans more universal than systems without (something like) it. it's not much of a protection as AI developers have already discovered such techniques (eg, dropout) -- but perhaps some safety techniques might be able to lean on this observation.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:01] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>I think there's a whole Scheme for Alignment which hopes for a miracle along the lines of, \"Well, we're dealing with these enormous matrices instead of tiny genomes, so maybe we can build a sufficiently powerful intelligence to execute a pivotal act, whose tendency to generalize across domains is less than the corresponding human tendency, and this brings the difficulty of producing corrigibility into practical reach.\"</p><p>Though, people who are hopeful about this without trying to imagine possible difficulties will predictably end up too hopeful; one must also ask oneself, \"Okay, but then it's also worse at generalizing the corrigibility dataset from weak domains we can safely label to powerful domains where the label is 'whoops that killed us'?\" and \"Are we relying on massive datasets to overcome poor generalization?&nbsp; How do you get those for something like nanoengineering where the real world is too expensive to simulate?\"</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><p><i>nature of the descent</i></p><p>conversely, it feels to me that the crucial position in the other (richard, paul, many others) camp is something like:</p><p><i>the “pit of generality” model might be true at the limit, but the descent will not be quick nor clean, and will likely offer many opportunities for steering the future.</i></p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:41] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p><i>the “pit of generality” model might be true at the limit, but the descent will not be quick nor clean</i></p></blockquote><p>I'm quite often on board with things not being quick or clean - that sounds like something you might read in a history book, and I am all about trying to make futuristic predictions sound more like history books and less like EAs imagining ways for everything to go the way an EA would do them.</p><p>It won't be slow and messy once we're out of the atmosphere, my models do say.&nbsp; But my models at least <i>permit</i> - though they do not desperately, loudly insist - that we could end up with weird half-able AGIs affecting the Earth for an extended period.</p><p>Mostly my model throws up its hands about being able to predict exact details here, given that eg I wasn't able to time AlphaFold 2's arrival 5 years in advance; it might be knowable in principle, it might be the sort of thing that would be very predictable if we'd watched it happen on a dozen other planets, but in practice I have not seen people having much luck in predicting which tasks will become accessible due to future AI advances being able to do new cognition.</p><p>The main part where I issue corrections is when I see EAs doing the equivalent of reasoning, \"And then, when the pandemic hits, it will only take a day to design a vaccine, after which distribution can begin right away.\" I.e., what seems to me to be a pollyannaish/utopian view of how much the world economy would immediately accept AI inputs into core manufacturing cycles, as opposed to just selling AI anime companions that don't pour steel in turn. I predict much more absence of quick and clean when it comes to economies adopting AI tech, than when it comes to laboratories building the next prototypes of that tech.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:43] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p><i>will likely offer many opportunities for steering the future</i></p></blockquote><p>Ah, see, that part sounds less like history books.&nbsp; \"Though many predicted disaster, subsequent events were actually so slow and messy, they offered many chances for well-intentioned people to steer the outcome and everything turned out great!\" does not sound like any particular segment of history book I can recall offhand.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:53] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>ok, yeah, this puts the burden of proof on the other side indeed</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>i’m sympathetic (but don’t buy outright, given my uncertainty) to eliezer’s point that even if that’s true, we have no plan nor hope for actually steering things (via “pivotal acts”) so “who cares, we still die”;</li><li>i’m also sympathetic that GWP might be too laggy a metric to measure the descent, but i don’t fully buy that regulations/bureaucracy can <i>guarantee</i> its decoupling from AI progress: eg, the FDA-like-structures-as-progress-bottlenecks model predicts worldwide covid response well, but wouldn’t cover things like apple under jobs, tesla/spacex under musk, or china under deng xiaoping;</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:51] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>apple under jobs, tesla/spacex under musk, or china under deng xiaoping</p></blockquote><p>A lot of these examples took place over longer than a 4-year cycle time, and not all of that time was spent waiting on inputs from cognitive processes.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][5:07] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>yeah, fair (i actually looked up china's GDP curve in deng era before writing this -- indeed, wasn't very exciting). still, my inside view is that there are people and organisations for whom US-type bureaucracy is not going to be much of an obstacle.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:09] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>I have a (separately explainable, larger) view where the economy contains a core of positive feedback cycles - better steel produces better machines that can farm more land that can feed more steelmakers - and also some products that, as much as they contribute to human utility, do not in quite the same way feed back into the core production cycles.</p><p>If you go back in time to the middle ages and sell them, say, synthetic gemstones, then - even though they might be willing to pay a bunch of GDP for that, even if gemstones are enough of a monetary good or they have enough production slack that measured GDP actually goes up - you have not quite contributed to steps of their economy's core production cycles in a way that boosts the planet over time, the way it would be boosted if you showed them cheaper techniques for making iron and new forms of steel.</p><p>There are people and organizations who will figure out how to sell AI anime waifus without that being successfully regulated, but it's not obvious to me that AI anime waifus feed back into core production cycles.</p><p>When it comes to core production cycles the current world has more issues that look like \"No matter what technology you have, it doesn't let you build a house\" and places for the larger production cycle to potentially be bottlenecked or interrupted.</p><p>I suspect that the main economic response to this is that entrepreneurs chase the 140 characters instead of the flying cars - people will gravitate to places where they can sell non-core AI goods for lots of money, rather than tackling the challenge of finding an excess demand in core production cycles which it is legal to meet via AI.</p><p>Even if some tackle core production cycles, it's going to take them a lot longer to get people to buy their newfangled gadgets than it's going to take to sell AI anime waifus; the world may very well end while they're trying to land their first big contract for letting an AI lay bricks.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][0:00] &nbsp;(Sep. 20 comment)</strong>&nbsp;</p><p>interesting. my model of paul (and robin, of course) wants to respond here but i’m not sure how :)</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>still, developing a better model of the descent period seems very worthwhile, as it might offer opportunities for, using robin’s metaphor, “pulling the rope sideways” in non-obvious ways – i understand that is part of the purpose of the debate;</li><li>my natural instinct here is to itch for carl’s viewpoint 😊</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:52] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>developing a better model of the descent period seems very worthwhile</p></blockquote><p>I'd love to have a better model of the descent.&nbsp; What I think this looks like is people mostly with specialization in econ and politics, who know what history books sound like, taking brief inputs from more AI-oriented folk in the form of <i>multiple</i> scenario premises each consisting of some random-seeming handful of new AI capabilities, trying to roleplay realistically how those might play out - not AIfolk forecasting particular AI capabilities exactly correctly, and then sketching pollyanna pictures of how they'd be immediately accepted into the world economy.&nbsp;</p><p>You want the forecasting done by the kind of person who would imagine a Covid-19 epidemic and say, \"Well, what if the CDC and FDA banned hospitals from doing Covid testing?\" and not \"Let's imagine how protein folding tech from AlphaFold would make it possible to immediately develop accurate Covid-19 tests!\"&nbsp; They need to be people who understand the Law of Earlier Failure (less polite terms: Law of Immediate Failure, Law of Undignified Failure).</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][5:13] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>great! to me this sounds like something FLI would be in good position to organise. i'll add this to my projects list (probably would want to see the results of this debate first, plus wait for travel restrictions to ease)</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><p><i>nature of cognition</i></p><p>given that having a better understanding of cognition can help with both understanding the topology of cognitive systems space as well as likely trajectories of AI takeoff, in theory there should be a lot of value in debating what cognition is (the current debate started with discussing consequentialists).</p><ul><li>however, i didn’t feel that there was much progress, and i found myself <i>more</i> confused as a result (which i guess is a form of progress!);</li><li>eg, take the term “plan” that was used in the debate (and, centrally, in nate’s comments doc): i interpret it as “policy produced by a consequentialist” – however, now i’m confused about what’s the relevant distinction between “policies” and “cognitive processes” (ie, what’s a meta level classifier that can sort algorithms into such categories);<ul><li>it felt that abram’s “<a href=\"https://www.lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control\">selection vs control</a>” article tried to distinguish along similar axis (controllers feel synonym-ish to “policy instantiations” to me);</li><li>also, the “imperative vs functional” difference in coding seems relevant;</li><li>i’m further confused by human “policies” often making function calls to “cognitive processes” – suggesting some kind of duality, rather than producer-product relationship.</li></ul></li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:06] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>what’s the relevant distinction between “policies” and “cognitive processes”</p></blockquote><p>What in particular about this matters?&nbsp; To me they sound like points on a spectrum, and not obviously points that it's particularly important to distinguish on that spectrum.&nbsp; A sufficiently sophisticated policy is itself an engine; human-engines are genetic policies.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][5:18] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>well, i'm not sure -- just that nate's \"The consequentialism is in the plan, not the cognition\" writeup sort of made it sound like the distinction is important. again, i'm confused</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:11] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>Does it help if I say \"consequentialism can be visible in the actual path through time, not the intent behind the output\"?</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][0:06] &nbsp;(Sep. 20 comment)</strong>&nbsp;</p><p>yeah, well, my initial interpretation of nate’s point was, indeed, “you can look at the product and conclude the consequentialist-bit for the producer”. but then i noticed that the producer-and-product metaphor is leaky (due to the cognition-policy duality/spectrum), so the quoted sentence gives me a compile error</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>is “not goal oriented cognition” an oxymoron?</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:06] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>is “not goal oriented cognition” an oxymoron?</p></blockquote><p>\"Non-goal-oriented cognition\" never becomes a perfect oxymoron, but the more you understand cognition, the weirder it sounds.</p><p>Eg, at the very shallow level, you've got people coming in going, \"Today I just messed around and didn't do any goal-oriented cognition at all!\" &nbsp;People who get a bit further in may start to ask, \"A non-goal-oriented cognitive engine?&nbsp; How did it come into existence?&nbsp; Was it also not built by optimization?&nbsp; Are we, perhaps, postulating a naturally-occurring Solomonoff inductor rather than an evolved one?&nbsp; Or do you mean that its content is very heavily designed and the output of a consequentialist process that was steering the future conditional on that design existing, but the cognitive engine is itself not doing consequentialism beyond that?&nbsp; If so, I'll readily concede that, say, a pocket calculator, is doing a kind of work that is not of itself consequentialist - though it might be used by a consequentialist - but as you start to postulate any big cognitive task up at the human level, it's going to require many cognitive subtasks to perform, and some of those will definitely be searching the preimages of large complicated functions.\"</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>i did not understand eliezer’s “time machine” metaphor: was it meant to point to / intuition pump something other than “a non-embedded exhaustive searcher with perfect information” (usually referred to as “god mode”);</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:59] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>a non-embedded exhaustive searcher with perfect information</p></blockquote><p>If you can view things on this level of abstraction, you're probably not the audience who needs to be told about time machines; if things sounded very simple to you, they probably were; if you wondered what the fuss is about, you probably don't need to fuss?&nbsp; The intended audience for the time-machine metaphor, from my perspective, is people who paint a cognitive system slightly different colors and go \"Well, <i>now</i> it's not a consequentialist, right?\" and part of my attempt to snap them out of that is me going, \"Here is an example of a purely material system which DOES NOT THINK AT ALL and is an extremely pure consequentialist.\"</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><ul><li>FWIW, my model of dario would dispute GPT characterisation as “shallow pattern memoriser (that’s lacking the core of cognition)”.</li></ul></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][12:00] &nbsp;(Sep. 18 comment)</strong>&nbsp;</p><blockquote><p>dispute&nbsp;</p></blockquote><p>Any particular predicted content of the dispute, or does your model of Dario just find something to dispute about it?</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][5:34] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>sure, i'm pretty confident that his system 1 could be triggered for uninteresting reasons here, but that's of course not what i had in mind.</p><p>my model of untriggered-dario disputes that there's a qualitative difference between (in your terminology) \"core of reasoning\" and \"shallow pattern matching\" -- instead, it's \"pattern matching all the way up the ladder of abstraction\". in other words, GPT is not missing anything fundamental, it's just underpowered in the literal sense.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:13] &nbsp;(Sep. 19 comment)</strong>&nbsp;</p><p>Neither Anthropic in general, nor Deepmind in general, has reached the stage of trusted relationship where I would argue specifics with them if I thought they were wrong about a thesis like that.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][0:10] &nbsp;(Sep. 20 comment)</strong>&nbsp;</p><p>yup, i didn’t expect you to!</p></td></tr></tbody></table>\n\n7.2. Nate Soares's summary\n--------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares][16:40] &nbsp;(Sep. 18)</strong>&nbsp;</p><p>I, too, have produced some notes: [GDocs link]. This time I attempt to drive home points that I saw Richard as attempting to make, and I'm eager for Richard-feedback especially. (I'm also interested in Eliezer-commentary.)</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares] &nbsp;(Sep. 18 Google Doc)</strong>&nbsp;</p><p>Sorry for not making more insistence that the discussion be more concrete, despite Eliezer's requests.</p><p>My sense of the last round is mainly that Richard was attempting to make a few points that didn't quite land, and/or that Eliezer didn't quite hit head-on. My attempts to articulate it are below.</p><p>---</p><p>There's a specific sense in which Eliezer seems quite confident about certain aspects of the future, for reasons that don't yet feel explicit.</p><p>It's not quite about the deep future -- it's clear enough (to my Richard-model) why it's easier to make predictions about AIs that have \"left the atmosphere\".</p><p>And it's not quite the near future -- Eliezer has reiterated that his models permit (though do not demand) a period of weird and socially-impactful AI systems \"pre-superintelligence\".</p><p>It's about the middle future -- the part where Eliezer's model, apparently confidently, predicts that there's something kinda like a discrete event wherein \"scary\" AI has finally been created; and the model further apparently-confidently predicts that, when that happens, the \"scary\"-caliber systems will be able to attain a decisive strategic advantage over the rest of the world.</p><p>I think there's been a dynamic in play where Richard attempts to probe this apparent confidence, and a bunch of the probes keep slipping off to one side or another. (I had a bit of a similar sense when Paul joined the chat, also.)</p><p>For instance, I see queries of the form \"but why not expect systems that are half as scary, relevantly before we see the scary systems?\" as attempts to probe this confidence, that \"slip off\" with Eliezer-answers like \"my model permits weird not-really-general half-AI hanging around for a while in the runup\". Which, sure, that's good to know. But there's still something implicit in that story, where these are not-really-general half-AIs. Which is also evidenced when Eliezer talks about the \"general core\" of intelligence.</p><p>And the things Eliezer was saying on consequentialism aren't irrelevant here, but those probes have kinda slipped off the far side of the confidence, if I understand correctly. Like, sure, late-stage sovereign-level superintelligences are epistemically and instrumentally efficient with respect to you (unless someone put in a hell of a lot of work to install a blindspot), and a bunch of that coherence filters in earlier, but there's still a question about <i>how much</i> of it has filtered down <i>how far</i>, where Eliezer seems to have a fairly confident take, informing his apparently-confident prediction about scary AI systems hitting the world in a discrete event like a hammer.</p><p>(And my Eliezer-model is at this point saying \"at this juncture we need to have discussions about more concrete scenarios; a bunch of the confidence that I have there comes from the way that the concrete visualizations where scary AI hits the world like a hammer abound, and feel savvy and historical, whereas the concrete visualizations where it doesn't are fewer and seem full of wishful thinking and naivete\".)</p><p>But anyway, yeah, my read is that Richard (and various others) have been trying to figure out why Eliezer is so confident about some specific thing in this vicinity, and haven't quite felt like they've been getting explanations.</p><p>Here's an attempt to gesture at some claims that I at least think Richard thinks Eliezer's confident in, but that Richard doesn't believe have been explicitly supported:</p><p>1. There's a qualitative difference between the AI systems that are capable of ending the acute risk period (one way or another), and predecessor systems that in some sense don't much matter.</p><p>2. That qualitative gap will be bridged \"the day after tomorrow\", ie in a world that looks more like \"DeepMind is on the brink\" and less like \"everyone is an order of magnitude richer, and the major gov'ts all have AGI projects, around which much of public policy is centered\".</p><p>---</p><p>That's the main thing I wanted to say here.</p><p>A subsidiary point that I think Richard was trying to make, but that didn't quite connect, follows.</p><p>I think Richard was trying to probe Eliezer's concept of consequentialism to see if it supported the aforementioned confidence. (Some evidence: Richard pointing out a couple times that the question is not whether sufficiently capable agents are coherent, but whether the agents that matter are relevantly coherent. On my current picture, this is another attempt to probe the \"why do you think there's a qualitative gap, and that straddling it will be strategically key in practice?\" thing, that slipped off.)</p><p>My attempt at sharpening the point I saw Richard as driving at:</p><ol><li>Consider the following two competing hypotheses:<ol><li>There's this \"deeply general\" core to intelligence, that will be strategically important in practice</li><li>Nope. Either there's no such core, or practical human systems won't find it, or the strategically important stuff happens before you get there (if you're doing your job right, in a way that natural selection wasn't), or etc.</li></ol></li><li>The whole deep learning paradigm, and the existence of GPT, sure seem like they're evidence for (b) over (a).<br><br>Like, (a) maybe isn't dead, but it didn't concentrate as much mass into the present scenario.</li><li>It seems like perhaps a bunch of Eliezer's confidence comes from a claim like \"anything capable of doing decently good work, is quite close to being scary\", related to his concept of \"consequentialism\".<br><br>In particular, this is a much stronger claim than that <i>sufficiently</i> smart systems are coherent, b/c it has to be strong enough to apply to the dumbest system that can make a difference.</li><li>It's easy to get caught up in the elegance of a theory like consequentialism / utility theory, when it will not in fact apply in practice.</li><li>There are some theories so general and ubiquitous that it's a little tricky to misapply them -- like, say, conservation of momentum, which has some very particular form in the symmetry of physical laws, but which can also be used willy-nilly on large objects like tennis balls and trains (although even then, you have to be careful, b/c the real world is full of things like planets that you're kicking off against, and if you forget how that shifts the earth, your application of conservation of momentum might lead you astray).</li><li>The theories that you <i>can</i> apply everywhere with abandon, tend to have a bunch of surprising applications to surprising domains.</li><li>We don't see that of consequentialism.</li></ol><p>For the record, my guess is that Eliezer isn't getting his confidence in things like \"there are non-scary systems and scary-systems, and anything capable of saving our skins is likely scary-adjacent\" by the sheer force of his consequentialism concept, in a manner that puts so much weight on it that it needs to meet this higher standard of evidence Richard was poking around for. (Also, I could be misreading Richard's poking entirely.)</p><p>In particular, I suspect this was the source of some of the early tension, where Eliezer was saying something like \"the fact that humans go around doing something vaguely like weighting outcomes by possibility and also by attractiveness, which they then roughly multiply, is quite sufficient evidence for my purposes, as one who does not pay tribute to the gods of modesty\", while Richard protested something more like \"but aren't you trying to use your concept to carry a whole lot more weight than that amount of evidence supports?\". cf my above points about some things Eliezer is apparently confident in, for which the reasons have not yet been stated explicitly to my Richard-model's satisfaction.</p><p>And, ofc, at this point, my Eliezer-model is again saying \"This is why we should be discussing things concretely! It is quite telling that all the plans we can concretely visualize for saving our skins, are scary-adjacent; and all the non-scary plans, can't save our skins!\"</p><p>To which my Richard-model answers \"But your concrete visualizations assume the endgame happens the day after tomorrow, at least politically. The future tends to go sideways! The endgame will likely happen in an environment quite different from our own! These day-after-tomorrow visualizations don't feel like they teach me much, because I think there's a good chance that the endgame-world looks dramatically different.\"</p><p>To which my Eliezer-model replies \"Indeed, the future tends to go sideways. But I observe that the imagined changes, that I have heard so far, seem quite positive -- the relevant political actors become AI-savvy, the major states start coordinating, etc. I am quite suspicious of these sorts of visualizations, and would take them much more seriously if there was at least as much representation of outcomes as realistic as \"then Trump becomes president\" or \"then at-home covid tests are banned in the US\". And if all the ways to save the world <i>today</i> are scary-adjacent, the fact that the future is surprising gives us no <i>specific</i> reason to hope for that particular parameter to favorably change when the future in fact goes sideways. When things look grim, one can and should prepare to take advantage of miracles, but banking on some particular miracle is foolish.\"</p><p>And my Richard-model gets fuzzy at this point, but I'd personally be pretty enthusiastic about Richard naming a bunch of specific scenarios, not as predictions, but as the sorts of visualizations that seem to him promising, in the hopes of getting a much more object-level sense of why, in specific concrete scenarios, they either have the properties Eliezer is confident in, or are implausible on Eliezer's model (or surprise Eliezer and cause him to update).</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][0:06] &nbsp;(Sep. 19)</strong>&nbsp;</p><p>excellent summary, nate! it also tracks my model of the debate well and summarises the frontier concisely (much better than your earlier notes or mine). unless eliezer or richard find major bugs in your summary, i’d nominate you to iterate after the next round of debate</p><figure class=\"table\"><table><tbody><tr><td>[Soares: ❤️]</td></tr></tbody></table></figure></td></tr></tbody></table>\n\n7.3. Richard Ngo's summary\n--------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][1:48] &nbsp;(Sep. 20)</strong>&nbsp;</p><p>Updated my summary to include the third discussion: [<a href=\"https://docs.google.com/document/d/1sr5YchErvSAY2I4EkJl2dapHcMp8oCXy7g8hd_UaJVw/edit\">https://docs.google.com/document/d/1sr5YchErvSAY2I4EkJl2dapHcMp8oCXy7g8hd_UaJVw/edit</a>]</p><p>I'm also halfway through a document giving my own account of intelligence + specific safe scenarios.</p><figure class=\"table\"><table><tbody><tr><td>[Soares: 😄]</td></tr></tbody></table></figure></td></tr></tbody></table>",
      "plaintextDescription": "This is a collection of follow-up discussions in the wake of Richard Ngo and Eliezer Yudkowsky's Sep. 5–8 and Sep. 14 conversations.\n\n \n\nColor key:\n\n  Chat    Google Doc content    Inline comments  \n\n \n\n\n7. Follow-ups to the Ngo/Yudkowsky conversation\n \n\n[Bensinger][1:50]  (Nov. 23 follow-up comment) \n\nA general background note: Readers who aren't already familiar with ethical injunctions or the unilateralist's curse should probably read Ends Don't Justify Means (Among Humans), along with an explanation of the unilateralist's curse.\n\n \n\n\n7.1. Jaan Tallinn's commentary\n \n\n[Tallinn][6:38]  (Sep. 18) \n\nthanks for the interesting debate! here are my comments so far: [GDocs link]\n\n[Tallinn]  (Sep. 18 Google Doc) \n\nmeta\n\na few meta notes first:\n\n * i’m happy with the below comments being shared further without explicit permission – just make sure you respect the sharing constraints of the discussion that they’re based on;\n * there’s a lot of content now in the debate that branches out in multiple directions – i suspect a strong distillation step is needed to make it coherent and publishable;\n * the main purpose of this document is to give a datapoint how the debate is coming across to a reader – it’s very probable that i’ve misunderstood some things, but that’s the point;\n * i’m also largely using my own terms/metaphors – for additional triangulation.\n\n \n\npit of generality\n\nit feels to me like the main crux is about the topology of the space of cognitive systems in combination with what it implies about takeoff. here’s the way i understand eliezer’s position:\n\nthere’s a “pit of generality” attractor in cognitive systems space: once an AI system gets sufficiently close to the edge (“past the atmospheric turbulence layer”), it’s bound to improve in catastrophic manner;\n\n[Yudkowsky][11:10]  (Sep. 18 comment) \n\n> it’s bound to improve in catastrophic manner\n\nI think this is true with quite high probability about an AI that gets high enough, if not otherwise corrigibilized, bo",
      "wordCount": 12069
    },
    "tags": [
      {
        "_id": "qNF7Ti87CLfHhbttj",
        "name": "Treacherous Turn",
        "slug": "treacherous-turn"
      },
      {
        "_id": "bxhzaWtdNoEMMkE8r",
        "name": "General intelligence",
        "slug": "general-intelligence"
      },
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "7MCqRnZzvszsxgtJi",
    "title": "Christiano, Cotra, and Yudkowsky on AI progress",
    "slug": "christiano-cotra-and-yudkowsky-on-ai-progress",
    "url": null,
    "baseScore": 119,
    "voteCount": 39,
    "viewCount": null,
    "commentCount": 95,
    "createdAt": null,
    "postedAt": "2021-11-25T16:45:32.482Z",
    "contents": {
      "markdown": "This post is a transcript of a discussion between Paul Christiano, Ajeya Cotra, and Eliezer Yudkowsky on AGI forecasting, following up on Paul and Eliezer's [\"Takeoff Speeds\" discussion](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/vwLxd6hhFvPbvKmBH).\n\nColor key:\n\n<table><tbody><tr><td style=\"border-color:hsl(0, 0%, 0%)\">&nbsp;Chat by Paul and Eliezer&nbsp;</td><td style=\"background-color:rgb(230, 230, 230);border-color:hsl(0, 0%, 0%)\">&nbsp;Chat by Ajeya&nbsp;</td><td style=\"background-color:rgb(255, 238, 187);border-color:hsl(0, 0%, 0%)\">&nbsp;Inline comments&nbsp;</td></tr></tbody></table>\n\n8\\. September 20 conversation\n=============================\n\n8.1. Chess and Evergrande\n-------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:28]</strong>&nbsp;</p><p>&nbsp;I still feel like you are overestimating how big a jump alphago is, or something. Do you have a mental prediction of how the graph of (chess engine quality) vs (time) looks, and whether neural net value functions are a noticeable jump in that graph?</p><p>Like, people investing in \"Better Software\" doesn't predict that you won't be able to make progress at playing go. The reason you can make a lot of progress at go is that there was extremely little investment in playing better go.</p><p>So then your work is being done by the claim \"People won't be working on the problem of acquiring a decisive strategic advantage,\" not that people won't be looking in quite the right place and that someone just had a cleverer idea</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:35]</strong>&nbsp;</p><p>I think I'd expect something like... chess engine slope jumps a bit for Deep Blue, then levels off with increasing excitement, then jumps for the Alpha series? Albeit it's worth noting that Deepmind's efforts there were going towards generality rather than raw power; chess was solved to the point of being uninteresting, so they tried to solve chess with simpler code that did more things. I don't think I do have strong opinions about what the chess trend should look like, vs. the Go trend; I have no memories of people saying the chess trend was breaking upwards or that there was a surprise there.</p><p>Incidentally, the highly well-traded financial markets are currently experiencing sharp dips surrounding the Chinese firm of Evergrande, which I was reading about several weeks before this.</p><p>I don't see the basic difference in the kind of reasoning that says \"Surely foresightful firms must produce investments well in advance into earlier weaker applications of AGI that will double the economy\", and the reasoning that says \"Surely world economic markets and particular Chinese stocks should experience smooth declines as news about Evergrande becomes better-known and foresightful financial firms start to remove that stock from their portfolio or short-sell it\", except that in the latter case there are many more actors with lower barriers to entry than presently exist in the auto industry or semiconductor industry never mind AI.</p><p>or if not smooth because of bandwagoning and rational fast actors, then at least the markets should (arguendo) be reacting earlier than they're reacting now, given that I heard about Evergrande earlier; and they should have options-priced Covid earlier; and they should have reacted to the mortgage market earlier. If even markets there can exhibit seemingly late wild swings, how is the economic impact of AI - which isn't even an asset market! - forced to be earlier and smoother than that, as a result of wise investing?</p><p>There's just such a vast gap between hopeful reasoning about how various agents and actors should all do the things the speaker finds very reasonable, thereby yielding smooth behavior of the Earth, versus reality.</p></td></tr></tbody></table>\n\n9\\. September 21 conversation\n=============================\n\n9.1. AlphaZero, innovation vs. industry, the Wright Flyer, and the Manhattan Project\n------------------------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][10:18]</strong>&nbsp;</p><p>(For benefit of readers, the market is down 1.5% from friday close -&gt; tuesday open, after having drifted down 2.5% over the preceding two weeks. Draw whatever lesson you want from that.)</p><p>Also for the benefit of readers, here is the SSDF list of computer chess performance by year. I think the last datapoint is with the first version of neural net evaluations, though I think to see the real impact we want to add one more datapoint after the neural nets are refined (which is why I say I also don't know what the impact is)</p><figure class=\"image\"><img src=\"https://cdn.discordapp.com/attachments/887568029733519391/889924404392370226/ChessEnginePerformance.png\"></figure><p>No one keeps similarly detailed records for Go, and there is much less development effort, but the rate of progress was about 1 stone per year from 1980 until 2015 (see <a href=\"https://intelligence.org/files/AlgorithmicProgress.pdf\">https://intelligence.org/files/AlgorithmicProgress.pdf</a>, written way before AGZ). In 2012 go bots reached about 4-5 amateur dan. By DeepMind's reckoning here (<a href=\"https://www.nature.com/articles/nature16961\">https://www.nature.com/articles/nature16961</a>, figure 4) Fan AlphaGo about 4-5 stones stronger-4 years later, with 1 stone explained by greater runtime compute. They could then get further progress to be superhuman with even more compute, radically more than were used for previous projects and with pretty predictable scaling. That level is within 1-2 stones of the best humans (professional dan are greatly compressed relative to amateur dan), so getting to \"beats best human\" is really just not a big discontinuity and the fact that DeepMind marketing can find an expert who makes a really bad forecast shouldn't be having such a huge impact on your view.</p><p>This understates the size of the jump from AlphaGo, because that was basically just the first version of the system that was superhuman and it was still progressing very rapidly as it moved from prototype to slightly-better-prototype, which is why you saw such a close game. (Though note that the AlphaGo prototype involved much more engineering effort than any previous attempt to play go, so it's not surprising that a \"prototype\" was the thing to win.)</p><p>So to look at actual progress after the dust settles and really measure how crazy this was, it seems much better to look at AlphaZero which continued to improve further, see (<a href=\"https://sci-hub.se/https://www.nature.com/articles/nature24270\">https://sci-hub.se/https://www.nature.com/articles/nature24270</a>, figure 6b). Their best system got another ~8 stones of progress over AlphaGo. Now we are like 7-10 stones ahead of trend, of which I think about 3 stones are explained by compute. Maybe call it 6 years ahead of schedule?</p><p>So I do think this is pretty impressive, they were slightly ahead of schedule for beating the best humans but they did it with a huge margin of error. I think the margin is likely overstated a bit by their elo evaluation methodology, but I'd still grant like 5 years ahead of the nearest competition.</p><p>I'd be interested in input from anyone who knows more about the actual state of play (+ is allowed to talk about it) and could correct errors.</p><p>Mostly that whole thread is just clearing up my understanding of the empirical situation, probably we still have deep disagreements about what that says about the world, just as e.g. we read very different lessons from market movements.</p><p>Probably we should only be talking about either ML or about historical technologies with meaningful economic impacts. In my view your picture is just radically unlike how almost any technologies have been developed over the last few hundred years. So probably step 1 before having bets is to reconcile our views about historical technologies, and then maybe as a result of that we could actually have a bet about future technology. Or we could try to shore up the GDP bet.</p><p>Like, it feels to me like I'm saying: AI will be like early computers, or modern semiconductors, or airplanes, or rockets, or cars, or trains, or factories, or solar panels, or genome sequencing, or basically anything else. And you are saying: AI will be like nuclear weapons.</p><p>I think from your perspective it's more like: AI will be like all the historical technologies, and that means there will be a hard takeoff. The only way you get a soft takeoff forecast is by choosing a really weird thing to extrapolate from historical technologies.</p><p>So we're both just forecasting that AI will look kind of like other stuff in the near future, and then both taking what we see as the natural endpoint of that process.</p><p>To me it feels like the nuclear weapons case is the outer limit of what looks plausible, where someone is able to spend $100B for a chance at a decisive strategic advantage.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:11]</strong>&nbsp;</p><p>Go-wise, I'm a little concerned about that \"stone\" metric - what would the chess graph look like if it was measuring pawn handicaps? Are the professional dans compressed in Elo, not just \"stone handicaps\", relative to the amateur dans? And I'm also hella surprised by the claim, which I haven't yet looked at, that Alpha Zero got 8 stones of progress over AlphaGo - I would not have been shocked if you told me that God's Algorithm couldn't beat Lee Se-dol with a 9-stone handicap.</p><p>Like, the obvious metric is Elo, so if you go back and refigure in \"stone handicaps\", an obvious concern is that somebody was able to look into the past and fiddle their hindsight until they found a hindsightful metric that made things look predictable again. My sense of Go said that 5-dan amateur to 9-dan pro was a HELL of a leap for 4 years, and I also have some doubt about the original 5-dan-amateur claims and whether those required relatively narrow terms of testing (eg timed matches or something).</p><p>One basic point seems to be whether AGI is more like an innovation or like a performance metric over an entire large industry.</p><p>Another point seems to be whether the behavior of the world is usually like that, in some sense, or if it's just that people who like smooth graphs can go find some industries that have smooth graphs for particular performance metrics that happen to be smooth.</p><p>Among the smoothest metrics I know that seems like a convergent rather than handpicked thing to cite, is world GDP, which is the sum of more little things than almost anything else, and whose underlying process is full of multiple stages of converging-product-line bottlenecks that make it hard to jump the entire GDP significantly even when you jump one component of a production cycle... which, from my standpoint, is a major reason to expect AI to not hit world GDP all that hard until AGI passes the critical threshold of bypassing it entirely. Having 95% of the tech to invent a self-replicating organism (eg artificial bacterium) does not get you 95%, 50%, or even 10% of the impact.</p><p>(it's not so much the 2% reaction of world markets to Evergrande that I was singling out earlier, 2% is noise-ish, but the wider swings in the vicinity of Evergrande particularly)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][12:41]</strong>&nbsp;</p><p>Yeah, I'm just using \"stone\" to mean \"elo difference that is equal to 1 stone at amateur dan / low kyu,\" you can see DeepMind's conversion (which I also don't totally believe) in figure 4 here (<a href=\"https://sci-hub.se/https://www.nature.com/articles/nature16961\">https://sci-hub.se/https://www.nature.com/articles/nature16961</a>). Stones are closer to constant elo than constant handicap, it's just a convention to name them that way.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:42]</strong>&nbsp;</p><p>k then</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][12:47]</strong>&nbsp;</p><p>But my description above still kind of understates the gap I think. They call 230 elo 1 stone, and I think prior rate of progress is more like 200 elo/year. They put AlphaZero about 3200 elo above the 2012 system, so that's like 16 years ahead = 11 years ahead of schedule. At least 2 years are from test-time hardware, and self-play systematically overestimates elo differences at the upper end of that. But 5 years ahead is still too low and that sounds more like 7-9 years ahead. ETA: and my actual best guess all things considered is probably 10 years ahead, which I agree is just a lot bigger than 5. And I also understated how much of the gap was getting up to Lee Sedol.</p><p>The go graph I posted wasn't made with hindsight, that was from 2014</p><p>I mean, I'm fine with you saying that people who like smooth graphs are cherry-picking evidence, but do you want to give any example other than nuclear weapons of technologies with the kind of discontinuous impact you are describing?</p><p>I do agree that the difference in our views is like \"innovation\" vs \"industry.\" And a big part of my position is that innovation-like things just don't usually have big impacts for kind of obvious reasons, they start small and then become more industry-like as they scale up. And current deep learning seems like an absolutely stereotypical industry that is scaling up rapidly in an increasingly predictable way.</p><p>As far as I can tell the examples we know of things changing continuously aren't handpicked, we've been looking at all the examples we can find, and no one is proposing or even able to find almost <i>anything</i> that looks like you are imagining AI will look.</p><p>Like, we've seen deep learning innovations in the form of prototypes (most of all AlexNet), and they were cool and represented giant fast changes in people's views. And more recently we are seeing bigger much-less-surprising changes that are still helping a lot in raising the tens of billions of dollars that people are raising. And the innovations we are seeing are increasingly things that trade off against modest improvements in model size, there are fewer and fewer big surprises, just like you'd predict. It's clearer and clearer to more and more people what the roadmap is---the roadmap is not yet quite as clear as in semiconductors, but as far as I can tell that's just because the field is still smaller.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:23]</strong>&nbsp;</p><p>I sure wasn't imagining there was a roadmap to AGI! Do you perchance have one which says that AGI is 30 years out?</p><p>From my perspective, you could as easily point to the Wright Flyer as an atomic bomb. Perhaps this reflects again the \"innovation vs industry\" difference, where I think in terms of building a thing that goes foom thereby bypassing our small cute world GDP, and you think in terms of industries that affect world GDP in an invariant way throughout their lifetimes.</p><p>Would you perhaps care to write off the atomic bomb too? It arguably didn't change the outcome of World War II or do much that conventional weapons in great quantity couldn't; Japan was bluffed into believing the US could drop a nuclear bomb every week, rather than the US actually having that many nuclear bombs or them actually being used to deliver a historically outsized impact on Japan. From the industry-centric perspective, there is surely some graph you can draw which makes nuclear weapons also look like business as usual, especially if you go by destruction per unit of whole-industry non-marginal expense, rather than destruction per bomb.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:27]</strong>&nbsp;</p><p>seems like you have to make the wright flyer much better before it's important, and that it becomes more like an industry as that happens, and that this is intimately related to why so few people were working on it</p><p>I think the atomic bomb is further on the spectrum than almost anything, but it still doesn't feel nearly as far as what you are expecting out of AI</p><p>the manhattan project took years and tens of billions; if you wait an additional few years and spend an additional few tens of billions then it would be a significant improvement in destruction or deterrence per $ (but not totally insane)</p><p>I do think it's extremely non-coincidental that the atomic bomb was developed in a country that was practically outspending the whole rest of the world in \"killing people technology\"</p><p>and took a large fraction of that country's killing-people resources</p><p>eh, that's a bit unfair, the us was only like 35% of global spending on munitions</p><p>and the manhattan project itself was only a couple percent of total munitions spending</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:32]</strong>&nbsp;</p><p>a lot of why I expect AGI to be a disaster is that <i>I am straight-up expecting AGI to be different</i>. &nbsp;if it was just like coal or just like nuclear weapons or just like viral biology then I would not be way more worried about AGI than I am worried about those other things.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:33]</strong>&nbsp;</p><p>that definitely sounds right</p><p>but it doesn't seem like you have any short-term predictions about AI being different</p></td></tr></tbody></table>\n\n9.2. AI alignment vs. biosafety, and measuring progress\n-------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:33]</strong>&nbsp;</p><p>are you more worried about AI than about bioengineering?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:33]</strong>&nbsp;</p><p>I'm more worried about AI because (i) alignment is a thing, unrelated to takeoff speed, (ii) AI is a (ETA: likely to be) huge deal and bioengineering is probably a relatively small deal</p><p>(in the sense of e.g. how much $ people spend, or how much $ it makes, or whatever other metric of size you want to use)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:35]</strong>&nbsp;</p><p>what's the disanalogy to (i) biosafety is a thing, unrelated to the speed of bioengineering? &nbsp;why expect AI to be a huge deal and bioengineering to be a small deal? &nbsp;is it just that investing in AI is scaling faster than investment in bioengineering?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:35]</strong>&nbsp;</p><p>no, alignment is a really easy x-risk story, bioengineering x-risk seems extraordinarily hard</p><p>It's really easy to mess with the future by creating new competitors with different goals, if you want to mess with the future by totally wiping out life you have to really try at it and there's a million ways it can fail. The bioengineering seems like it basically requires deliberate and reasonably competent malice whereas alignment seems like it can only be averted with deliberate effort, etc.</p><p>I'm mostly asking about historical technologies to try to clarify expectations, I'm pretty happy if the outcome is: you think AGI is predictably different from previous technologies in ways we haven't seen yet</p><p>though I really wish that would translate into some before-end-of-days prediction about a way that AGI will eventually look different</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:38]</strong>&nbsp;</p><p>in my ontology a whole lot of threat would trace back to \"AI hits harder, faster, gets too strong to be adjusted\"; tricks with proteins just don't have the raw power of intelligence</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:39]</strong>&nbsp;</p><p>in my view it's nearly totally orthogonal to takeoff speed, though fast takeoffs are a big reason that preparation in advance is more useful</p><p>(but not related to the basic reason that alignment is unprecedentedly scary)</p><p>It feels to me like you are saying that the AI-improving-AI will move very quickly from \"way slower than humans\" to \"FOOM in &lt;1 year,\" but it just looks like that is very surprising to me.</p><p>However I do agree that if AI-improving-AI was like AlphaZero, then it would happen extremely fast.</p><p>It seems to me like it's pretty rare to have these big jumps, and it gets much much rarer as technologies become more important and are more industry-like rather than innovation like (and people care about them a lot rather than random individuals working on them, etc.). And I can't tell whether you are saying something more like \"nah big jumps happen all the time in places that are structurally analogous to the key takeoff jump, even if the effects are blunted by slow adoption and regulatory bottlenecks and so on\" or if you are saying \"AGI is atypical in how jumpy it will be\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:44]</strong>&nbsp;</p><p>I don't know about <i>slower</i>; GPT-3 may be able to type faster than a human</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:45]</strong>&nbsp;</p><p>Yeah, I guess we've discussed how you don't like the abstraction of \"speed of making progress\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:45]</strong>&nbsp;</p><p>but, basically less useful in fundamental ways than a human civilization, because they are less complete, less self-contained</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:46]</strong>&nbsp;</p><p>Even if we just assume that your AI needs to go off in the corner and not interact with humans, there's still a question of why the self-contained AI civilization is making ~0 progress and then all of a sudden very rapid progress</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:46]</strong>&nbsp;</p><p>unfortunately a lot of what you are saying, from my perspective, has the flavor of, \"but can't you tell me about your predictions earlier on of the impact on global warming at the <i>Homo erectus</i> level\"</p><p>you have stories about why this is like totally not a fair comparison</p><p>I do not share these stories</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:46]</strong>&nbsp;</p><p>I don't understand either your objection nor the reductio</p><p>like, here's how I think it works: AI systems improve gradually, including on metrics like \"How long does it take them to do task X?\" or \"How high-quality is their output on task X?\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:47]</strong>&nbsp;</p><p>I feel like the thing we know is something like, there is a sufficiently high level where things go whooosh humans-from-hominids style</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:47]</strong>&nbsp;</p><p>We can measure the performance of AI on tasks like \"Make further AI progress, without human input\"</p><p>Any way I can slice the analogy, it looks like AI will get continuously better at that task</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:48]</strong>&nbsp;</p><p>how would you measure progress from GPT-2 to GPT-3, and would you feel those metrics really captured the sort of qualitative change that lots of people said they felt?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:48]</strong>&nbsp;</p><p>And it seems like we have a bunch of sources of data we can use about how fast AI will get better</p><p>Could we talk about some application of GPT-2 or GPT-3?</p><p>also that's a <i>lot</i> of progress, spending 100x more is a <i>lot</i> more money</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:49]</strong>&nbsp;</p><p>my world, GPT-3 has very few applications because it is not quite right and not quite complete</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:49]</strong>&nbsp;</p><p>also it's still really dumb</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:49]</strong>&nbsp;</p><p>like a self-driving car that does great at 99% of the road situations</p><p>economically almost worthless</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:49]</strong>&nbsp;</p><p>I think the \"being dumb\" is way more important than \"covers every case\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:50]</strong>&nbsp;</p><p>(albeit that if new cities could still be built, we could totally take those 99%-complete AI cars and build fences and fence-gates around them, in a city where they were the only cars on the road, in which case they <i>would</i> work, and get big economic gains from these new cities with driverless cars, which ties back into my point about how current world GDP is <i>unwilling</i> to accept tech inputs)</p><p>like, it is in fact very plausible to me that there is a neighboring branch of reality with open borders and no housing-supply-constriction laws and no medical-supply-constriction laws, and their world GDP <i>does</i> manage to double before AGI hits them really hard, albeit maybe not in 4 years. &nbsp;this world <i>is not Earth</i>. &nbsp;they are constructing new cities to take advantage of 99%-complete driverless cars <i>right now</i>, or rather, they started constructing them 5 years ago and finished 4 years and 6 months ago.</p></td></tr></tbody></table>\n\n9.3. Requirements for FOOM\n--------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:53]</strong>&nbsp;</p><p>I really feel like the important part is the jumpiness you are imagining on the AI side / why AGI is different from other things</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][13:53]</strong>&nbsp;</p><p>It's actually not obvious to me that Eliezer is imagining that much more jumpiness on the AI technology side than you are, Paul</p><p>E.g. he's said in the past that while the gap from \"subhuman to superhuman AI\" could be 2h if it's in the middle of FOOM, it could also be a couple years if it's more like scaling alphago</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:54]</strong> &nbsp;</p><p>Indeed! &nbsp;We observed this jumpiness with hominids. &nbsp;A lot of stuff happened at once with hominids, but a critical terminal part of the jump was the way that hominids started scaling their own food supply, instead of being ultimately limited by the food supply of the savanna.</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][13:54]</strong> &nbsp;</p><p>A couple years is basically what Paul believes</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:55]</strong> &nbsp;</p><p>(discord is not a great place for threaded conversations :()</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][13:55]</strong> &nbsp;</p><p>What are the probabilities you're each placing on the 2h-2y spectrum? I feel like Paul is like \"no way on 2h, likely on 2y\" and Eliezer is like \"who knows\" on the whole spectrum, and a lot of the disagreement is the impact of the previous systems?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:55]</strong> &nbsp;</p><p>yeah, I'm basically at \"no way,\" because it seems obvious that the AI that can foom in 2h is preceded by the AI that can foom in 2y</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:56]</strong> &nbsp;</p><p>well, we surely agree there!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:56]</strong> &nbsp;</p><p>OK, and it seems to me like it is preceded by years</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:56]</strong> &nbsp;</p><p>we disagree on whether the AI that can foom in 2y clearly comes more than 2y before the AI that fooms in 2h</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][13:56]</strong> &nbsp;</p><p>yeah</p><p>perhaps we can all agree it's preceded by at least 2h</p><p>so I have some view like: for any given AI we can measure \"how long does it take to foom?\" and it seems to me like this is just a nice graph</p><p>and it's not exactly clear how quickly that number is going down, but a natural guess to me is something like \"halving each year\" based on the current rate of progress in hardware and software</p><p>and you see localized fast progress most often in places where there hasn't yet been much attention</p><p>and my best guess for your view is that actually that's not a nice graph at all, there is some critical threshold or range where AI quickly moves from \"not fooming for a really long time\" to \"fooming really fast,\" and that seems like the part I'm objecting to</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][13:59]</strong> &nbsp;</p><p>Paul, is your take that there's a non-infinity number for time to FOOM that'd be associated with current AI systems (unassisted by humans)?</p><p>And it's going down over time?</p><p>I feel like I would have said something more like \"there's a $ amount it takes to build a system that will FOOM in X amount of time, and that's going down\"</p><p>where it's like quadrillions of dollars today</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:00]</strong> &nbsp;</p><p>I think it would be a big engineering project to make such an AI, which no one is doing because it would be uselessly slow even if successful</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:02]</strong>&nbsp;</p><p>I... don't think GPT-3 fooms given 2^30 longer time to think about than the systems that would otherwise exist 30 years from now, on timelines I'd consider relatively long, and hence generous to this viewpoint? &nbsp;I also don't think you can take a quadrillion dollars and scale GPT-3 to foom today?</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:03]</strong>&nbsp;</p><p>I would agree with your take on GPT-3 fooming, and I didn't mean a quadrillion dollars just to scale GPT-3, would probably be a difft architecture</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:03]</strong>&nbsp;</p><p>I also agree that GPT-3 doesn't foom, it just keeps outputting &lt;EOT&gt;[next web page]&lt;EOT&gt;...</p><p>But I think the axes of \"smart enough to foom fast\" and \"wants to foom\" are pretty different. I also agree there is some minimal threshold below which it doesn't even make sense to talk about \"wants to foom,\" which I think is probably just not that hard to reach.</p><p>(Also there are always diminishing returns as you continue increasing compute, which become very relevant if you try to GPT-3 for a billion billion years as in your hypothetical even apart from \"wants to foom\".)</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:06]</strong>&nbsp;</p><p>I think maybe you and EY then disagree on where the threshold from \"infinity\" to \"a finite number\" for \"time for this AI system to FOOM\" begins? where eliezer thinks it'll drop from infinity to a pretty small finite number and you think it'll drop to a pretty large finite number, and keep going down from there</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:07]</strong>&nbsp;</p><p>I also think we will likely jump down to a foom-ing system only after stuff is pretty crazy, but I think that's probably less important</p><p>I think what you said is probably the main important disagreement</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:08]</strong>&nbsp;</p><p>as in before that point it'll be faster to have human-driven progress than FOOM-driven progress bc the FOOM would be too slow?</p><p>and there's some crossover point around when the FOOM time is just a bit faster than the human-driven progress time</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:09]</strong>&nbsp;</p><p>yeah, I think most likely (AI+humans) is faster than (AI alone) because of complementarity. But I think Eliezer and I would still disagree even if I thought there was 0 complementarity and it's just (humans improving AI) and separately (AI improving AI)</p><p>on that pure substitutes model I expect \"AI foom\" to start when the rate of AI-driven AI progress overtakes the previous rate of human-driven AI progress</p><p>like, I expect the time for successive \"doublings\" of AI output to be like 1 year, 1 year, 1 year, 1 year, [AI takes over] 6 months, 3 months, ...</p><p>and the most extreme fast takeoff scenario that seems plausible is that kind of perfect substitutes + no physical economic impact from the prior AI systems</p><p>and then by that point fast enough physical impact is really hard so it happens essentially after the software-only singularity</p><p>I consider that view kind of unlikely but at least coherent</p></td></tr></tbody></table>\n\n9.4. AI-driven accelerating economic growth\n-------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:12]</strong>&nbsp;</p><p>I'm expecting that the economy doesn't accept much inputs from chimps, and then the economy doesn't accept much input from village idiots, and then the economy doesn't accept much input from weird immigrants. &nbsp;I can imagine that there may or may not be a very weird 2-year or 3-month period with strange half-genius systems running around, but they will still not be allowed to build houses. &nbsp;In the terminal phase things get more predictable and the AGI starts its own economy instead.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:12]</strong>&nbsp;</p><p>I guess you can go even faster, by having a big and accelerating ramp-up in human investment right around the end, so that the \"1 year\" is faster (e.g. if recursive self-improvement was like playing go, and you could move from \"a few individuals\" to \"google spending $10B\" over a few years)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:13]</strong>&nbsp;</p><p>My <s>model</s> prophecy doesn't rule that out as a thing that could happen, but sure doesn't emphasize it as a key step that needs to happen.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:13]</strong> &nbsp;</p><p>I think it's very likely that AI will mostly be applied to further hardware+software progress</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: ➕]</td></tr></tbody></table></figure><p>I don't really understand why you keep talking about houses and healthcare</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:13]</strong> &nbsp;</p><p>Eliezer, what about stuff like Google already using ML systems to automate its TPU load-sharing decisions, and people starting ot use Codex to automate routine programming, and so on? Seems like there's a lot of stuff like that starting to already happen and markets are pricing in huge further increases</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:14]</strong> &nbsp;</p><p>it seems like the non-AI up-for-grabs zone are things like manufacturing, not things like healthcare</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: ➕]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:14]</strong> &nbsp;</p><p>(I mean on your timelines obviously not much time for acceleration anyway, but that's distinct from the regulation not allowing weak AIs to do stuff story)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:14]</strong> &nbsp;</p><p>Because I think that a key thing of what makes your prophecy less likely is the way that it happens inside the real world, where, economic gains or not, the System is unwilling/unable to take the things that are 99% self-driving cars and start to derive big economic benefits from those.</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:15]</strong> &nbsp;</p><p>but it seems like huge economic gains could happen entirely in industries mostly not regulated and not customer-facing, like hardware/software R&amp;D, manufacturing. shipping logistics, etc</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:15]</strong> &nbsp;</p><p>Ajeya, I'd consider Codex of <i>far</i> greater could-be-economically-important-ness than automated TPU load-sharing decisions</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:15]</strong> &nbsp;</p><p>i would agree with that, it's smarter and more general</p><p>and i think that kind of thing could be applied on the hardware chip design side too</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:16]</strong> &nbsp;</p><p>no, because the TPU load-sharing stuff has an obvious saturation point as a world economic input, while superCodex could be a world economic input in many more places</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:16]</strong> &nbsp;</p><p>the TPU load sharing thing was not a claim that this application could scale up to crazy impacts, but that it was allowed to happen, and future stuff that improves that kind of thing (back-end hardware/software/logistics) would probably also be allowed</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:16]</strong> &nbsp;</p><p>my sense is that dectupling the number of programmers would not lift world GDP much, but it seems a lot more possible for me to be wrong about that</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:17]</strong> &nbsp;</p><p>the point is that housing and healthcare are not central examples of things that scale up at the beginning of explosive growth, regardless of whether it's hard or soft</p><p>they are slower and harder, and also in efficient markets-land they become way less important during the transition</p><p>so they aren't happening that much on anyone's story</p><p>and also it doesn't make that much difference whether they happen, because they have pretty limited effects on other stuff</p><p>like, right now we have an industry of ~hundreds of billions that is producing computing hardware, building datacenters, mining raw inputs, building factories to build computing hardware, solar panels, shipping around all of those parts, etc. etc.</p><p>I'm kind of interested in the question of whether all that stuff explodes, although it doesn't feel as core as the question of \"what are the dynamics of the software-only singularity and how much $ are people spending initiating it?\"</p><p>but I'm not really interested in the question of whether human welfare is spiking during the transition or only after</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:20]</strong> &nbsp;</p><p>All of world GDP has never felt particularly relevant to me on that score, since twice as much hardware maybe corresponds to being 3 months earlier, or something like that.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:21]</strong> &nbsp;</p><p>that sounds like the stuff of predictions?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:21]</strong> &nbsp;</p><p>But if complete chip manufacturing cycles have accepted much more effective AI input, with no non-AI bottlenecks, then that... sure is a much more <i>material</i> element of a foom cycle than I usually envision.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:21]</strong> &nbsp;</p><p>like, do you think it's often the case that 3 months of software progress = doubling compute spending? or do you think AGI is different from \"normal\" AI on this perspective?</p><p>I don't think that's that far off anyway</p><p>I would guess like ~1 year</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:22]</strong> &nbsp;</p><p>Like, world GDP that goes up by only 10%, but that's because producing compute capacity was 2.5% of world GDP and that quadrupled, starts to feel much more to me like it's part of a foom story.</p><p>I expect software-beats-hardware to hit harder and harder as you get closer to AGI, yeah.</p><p>the prediction is firmer near the terminal phase, but I think this is also a case where I expect that to be visible earlier</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:24]</strong> &nbsp;</p><p>I think that by the time that the AI-improving-AI takes over, it's likely that hardware+software manufacturing+R&amp;D represents like 10-20% of GDP, and that the \"alien accountants\" visiting earth would value those companies at like 80%+ of GDP</p></td></tr></tbody></table>\n\n9.5. Brain size and evolutionary history\n----------------------------------------\n\n<table><tbody><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:24]</strong>&nbsp;</p><p>On software beating hardware, how much of your view is dependent on your belief that the chimp -&gt; human transition was probably not mainly about brain size because if it were about brain size it would have happened faster? My understanding is that you think the main change is a small software innovation which increased returns to having a bigger brain. If you changed your mind and thought that the chimp -&gt; human transition was probably mostly about raw brain size, what (if anything) about your AI takeoff views would change?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:25]</strong>&nbsp;</p><p>I think that's a pretty different world in a lot of ways!</p><p>but yes it hits AI takeoff views too</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:25]</strong> &nbsp;</p><p>regarding software vs hardware, here is an example of asking this question for imagenet classification (\"how much compute to train a model to do the task?\"), with a bit over 1 year doubling times (<a href=\"https://openai.com/blog/ai-and-efficiency/\">https://openai.com/blog/ai-and-efficiency/</a>). I guess my view is that we can make a similar graph for \"compute required to make your AI FOOM\" and that it will be falling significantly slower than 2x/year. And my prediction for other tasks is that the analogous graphs will also tend to be falling slower than 2x/year.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:26]</strong> &nbsp;</p><p>to the extent that I modeled hominid evolution as having been \"dutifully schlep more of the same stuff, get predictably more of the same returns\" that would correspond to a world in which intelligence was less scary, different, dangerous-by-default</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:27]</strong> &nbsp;</p><p>thanks, that's helpful. I looked around in <a href=\"https://intelligence.org/files/IEM.pdf\">IEM</a> and other places for a calculation of how quickly we should have evolved to humans if it were mainly about brain size, but I only found qualitative statements. If there's a calculation somewhere I would appreciate a pointer to it, because currently it seems to me that a story like \"selection pressure toward general intelligence was weak-to-moderate because it wasn't actually <i>that</i> important for fitness, and this degree of selection pressure is consistent with brain size being the main deal and just taking a few million years to happen\" is very plausible</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:29]</strong> &nbsp;</p><p>well, for one thing, the prefrontal cortex expanded twice as fast as the rest</p><p>and iirc there's evidence of a lot of recent genetic adaptation... though I'm not as sure you could pinpoint it as being about brain-stuff or that the brain-stuff was about cognition rather than rapidly shifting motivations or something.</p><p>elephant brains are 3-4 times larger by weight than human brains (just looked up)</p><p>if it's that easy to get returns on scaling, seems like it shouldn't have taken that long for evolution to go there</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:31]</strong> &nbsp;</p><p>but they have fewer synapses (would compute to less FLOP/s by the standard conversion)</p><p>how long do you think it should have taken?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:31]</strong> &nbsp;</p><p>early dinosaurs should've hopped onto the predictable returns train</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:31]</strong> &nbsp;</p><p>is there a calculation?</p><p>you said in IEM that evolution increases organ sizes quickly but there wasn't a citation to easily follow up on there</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:33]</strong> &nbsp;</p><p>I mean, you could produce a graph of smooth fitness returns to intelligence, smooth cognitive returns on brain size/activity, linear metabolic costs for brain activity, fit that to humans and hominids, then show that obviously if hominids went down that pathway, large dinosaurs should've gone down it first because they had larger bodies and the relative metabolic costs of increased intelligence would've been lower at every point along the way</p><p>I do not have a citation for that ready, if I'd known at the time you'd want one I'd have asked Luke M for it while he still worked at MIRI 😐</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:35]</strong> &nbsp;</p><p>cool thanks, will think about the dinosaur thing (my first reaction is that this should depend on the actual fitness benefits to general intelligence which might have been modest)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:35]</strong> &nbsp;</p><p>I suspect we're getting off Paul's crux, though</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:35]</strong> &nbsp;</p><p>yeah we can go back to that convo (though i think paul would also disagree about this thing, and believes that the chimp to human thing was mostly about size)</p><p>sorry for hijacking</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:36]</strong> &nbsp;</p><p>well, if at some point I can produce a major shift in EA viewpoints by coming up with evidence for a bunch of non-brain-size brain selection going on over those timescales, like brain-related genes where we can figure out how old the mutation is, I'd then put a lot more priority on digging up a paper like that</p><p>I'd consider it sufficiently odd to imagine hominids-&gt;humans as being primarily about brain size, given the evidence we have, that I do not believe this is Paul's position until Paul tells me so</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:49]</strong> &nbsp;</p><p>I would guess it's primarily about brain size / neuron count / cortical neuron count</p><p>and that the change in rate does mostly go through changing niche, where both primates and birds have this cycle of rapidly accelerating brain size increases that aren't really observed in other animals</p><p>it seems like brain size is increasing extremely quickly on both of those lines</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:50]</strong> &nbsp;</p><p>why aren't elephants GI?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:51]</strong> &nbsp;</p><p>mostly they have big brains to operate big bodies, and also my position obviously does not imply (big brain) ==(necessarily implies)==&gt; general intelligence</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:52]</strong> &nbsp;</p><p>I don't understand, in general, how your general position manages to strongly imply a bunch of stuff about AGI and not strongly imply similar stuff about a bunch of other stuff that sure sounds similar to me</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:52]</strong> &nbsp;</p><p>don't elephants have very few synapses relative to humans?</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: ➕]</td></tr></tbody></table></figure><p>how does the scale hypothesis possibly take a strong stand on synapses vs neurons? I agree that it takes a modest predictive hit from \"why aren't the big animals much smarter?\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:53]</strong> &nbsp;</p><p>if adding more synapses just scales, elephants should be able to pay hominid brain costs for a much smaller added fraction of metabolism and also not pay the huge death-in-childbirth head-size tax</p><p>because their brains and heads are already 4x as huge as they need to be for GI</p><p>and now they just need some synapses, which are a much tinier fraction of their total metabolic costs</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:54]</strong> &nbsp;</p><p>I mean, you can also make smaller and cheaper synapses as evidenced by birds</p><p>I'm not sure I understand what you are saying</p><p>it's clear that you can't say \"X is possible metabolically, so evolution would do it\"</p><p>or else you are confused about why primate brains are so bad</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:54]</strong> &nbsp;</p><p>great, then smaller and cheaper synapses should've scaled many eons earlier and taken over the world</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:55]</strong> &nbsp;</p><p>this isn't about general intelligence, this is a reductio of your position...</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:55]</strong> &nbsp;</p><p>and here I had thought it was a reductio of your position...</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:55]</strong> &nbsp;</p><p>indeed</p><p>like, we all grant that it's metabolically possible to have small smart brains</p><p>and evolution doesn't do it</p><p>and I'm saying that it's also possible to have small smart brains</p><p>and that scaling brains up matters a lot</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:56]</strong> &nbsp;</p><p>no, you grant that it's metabolically possible to have cheap brains full of synapses, which are therefore, on your position, smart</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:56]</strong> &nbsp;</p><p>birds are just smart</p><p>we know they are smart</p><p>this isn't some kind of weird conjecture</p><p>like, we can debate whether they are a \"general\" intelligence, but it makes no difference to this discussion</p><p>the point is that they do more with less metabolic cost</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:57]</strong> &nbsp;</p><p>on my position, the brain needs to invent the equivalents of ReLUs and Transformers and really rather a lot of other stuff because it can't afford nearly that many GPUs, and then the marginal returns on adding expensive huge brains and synapses have increased enough that hominids start to slide down the resulting fitness slope, which isn't even paying off in guns and rockets yet, they're just getting that much intelligence out of it once the brain software has been selected to scale that well</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:57]</strong> &nbsp;</p><p>but all of the primates and birds have brain sizes scaling much faster than the other animals</p><p>like, the relevant \"things started to scale\" threshold is way before chimps vs humans</p><p>isn't it?</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:58]</strong> &nbsp;</p><p>to clarify, my understanding is that paul's position is \"Intelligence is mainly about synapse/neuron count, and evolution doesn't care that much about intelligence; it cared more for birds and primates, and both lines are getting smarter+bigger-brained.\" And eliezer's position is that \"evolution should care a ton about intelligence in most niches, so if it were mostly about brain size then it should have gone up to human brain sizes with the dinosaurs\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:58]</strong> &nbsp;</p><p>or like, what is the evidence you think is explained by the threshold being between chimps and humans</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:58]</strong> &nbsp;</p><p>if hominids have less efficient brains than birds, on this theory, it's because (post facto handwave) birds are tiny, so whatever cognitive fitness gradients they face, will tend to get paid more in software and biological efficiency and biologically efficient software, and less paid in Stack More Neurons (even compared to hominids)</p><p>elephants just don't have the base software to benefit much from scaling synapses even though they'd be relatively cheaper for elephants</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][14:59]</strong> &nbsp;</p><p>@ajeya I think that intelligence is about a lot of things, but that size (or maybe \"more of the same\" changes that had been happening recently amongst primates) is the big difference between chimps and humans</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][14:59]</strong> &nbsp;</p><p>got it yeah i was focusing on chimp-human gap when i said \"intelligence\" there but good to be careful</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:59]</strong> &nbsp;</p><p>I have not actually succeeded in understanding Why On Earth Anybody Would Think That If Not For This Really Weird Prior I Don't Get Either</p><p>re: the \"more of the same\" theory of humans</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:00]</strong> &nbsp;</p><p>do you endorse my characterization of your position above? \"evolution should care a ton about intelligence in most niches, so if it were mostly about brain size then it should have gone up to human brain sizes with the dinosaurs\"</p><p>in which case the disagreement is about how much evolution should care about intelligence in the dinosaur niche, vs other things it could put its skill points into?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:01]</strong> &nbsp;</p><p>Eliezer, it seems like chimps are insanely smart compared to other animals, basically as smart as they get</p><p>so it's natural to think that the main things that make humans unique are also present in chimps</p><p>or at least, there was something going on in chimps that is exceptional</p><p>and should be causally upstream of the uniqueness of humans too</p><p>otherwise you have too many coincidences on your hands</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:02]</strong> &nbsp;</p><p>ajeya: no, I'd characterize that as \"the human environmental niche per se does not seem super-special enough to be unique on a geological timescale, the cognitive part of the niche derives from increased cognitive abilities in the first place and so can't be used to explain where they got started, dinosaurs are larger than humans and would pay lower relative metabolic costs for added brain size and it is not the case that every species as large as humans was in an environment where they would not have benefited as much from a fixed increment of intelligence, hominids are probably distinguished from dinosaurs in having better neural algorithms that arose over intervening evolutionary time and therefore better returns in intelligence on synapses that are more costly to humans than to elephants or large dinosaurs\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:03]</strong> &nbsp;</p><p>I don't understand how you can think that hominids are the special step relative to something earlier</p><p>or like, I can see how it's consistent, but I don't see what evidence or argument supports it</p><p>it seems like the short evolutionary time, and the fact that you also have to explain the exceptional qualities of other primates, cut extremely strongly against it</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:04]</strong> &nbsp;</p><p>paul: indeed, the fact that dinosaurs didn't see their brain sizes and intelligences ballooning, says there must be a lot of stuff hominids had that dinosaurs didn't, explaining why hominids got much higher returns on intelligence per synapse. natural selection is enough of a smooth process that 95% of this stuff should've been in the last common ancestor of humans and chimps.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:05]</strong> &nbsp;</p><p>it seems like brain size basically just increases faster in the smarter animals? though I mostly just know about birds and primates</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:05]</strong> &nbsp;</p><p>that is what you'd predict from smartness being about algorithms!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:05]</strong> &nbsp;</p><p>and it accelerates further and further within both lines</p><p>it's what you'd expect if smartness is about algorithms <i>and chimps and birds have good algorithms</i></p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:06]</strong> &nbsp;</p><p>if smartness was about brain size, smartness and brain size would increase faster in the <i>larger animals</i> or the ones whose successful members <i>ate more food per day</i></p><p>well, sure, I do model that birds have better algorithms than dinosaurs</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:07]</strong> &nbsp;</p><p>it seems like you've given arguments for \"there was algorithmic innovation between dinosaurs and humans\" but not yet arguments for \"there was major algorithmic innovation between chimps and humans\"?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:08]</strong> &nbsp;</p><p>(much less that the algorithmic changes were not just more-of-the-same)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:08]</strong> &nbsp;</p><p>oh, that's <i>not</i> mandated by the model the same way. (between LCA of chimps and humans)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:08]</strong> &nbsp;</p><p>isn't that exactly what we are discussing?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:09]</strong> &nbsp;</p><p>...I hadn't thought so, no.</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:09]</strong> &nbsp;</p><p>original q was:</p><blockquote><p>On software beating hardware, how much of your view is dependent on your belief that the chimp -&gt; human transition was probably not mainly about brain size because if it were about brain size it would have happened faster? My understanding is that you think the main change is a small software innovation which increased returns to having a bigger brain. If you changed your mind and thought that the chimp -&gt; human transition was probably mostly about raw brain size, what (if anything) about your AI takeoff views would change?</p></blockquote><p>so i thought we were talking about if there's a cool innovation from chimp-&gt;human?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:10]</strong> &nbsp;</p><p>I can see how this would have been the more obvious intended interpretation on your viewpoint, and apologize</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:10]</strong> &nbsp;</p><blockquote><p>(though i think paul would also disagree about this thing, and believes that the chimp to human thing was mostly about size)</p></blockquote><p>Is what I was responding to in part</p><p>I am open to saying that I'm conflating size and \"algorithmic improvements that are closely correlated with size in practice and are similar to the prior algorithmic improvements amongst primates\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:11]</strong> &nbsp;</p><p>from my perspective, the question is \"how did that hominid-&gt;human transition happen, as opposed to there being an elephant-&gt;smartelephant or dinosaur-&gt;smartdinosaur transition\"?</p><p>I expect there were substantial numbers of brain algorithm stuffs going on during this time, however</p><p>because I don't think that synapses scale that well <i>with</i> the baseline hominid boost</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:11]</strong> &nbsp;</p><p>FWIW, it seems quite likely to me that there would be an elephant-&gt;smartelephant transition within tens of millions or maybe 100M years, and a dinosaur-&gt;smartdinosaur transition in hundreds of millions of years</p><p>and those are just cut off by the fastest lines getting there first</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:12]</strong> &nbsp;</p><p>which I think does circle back to that point? actually I think my memory glitched and forgot the original point while being about this subpoint and I probably did interpret the original point as intended.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:12]</strong> &nbsp;</p><p>namely primates beating out birds by a hair</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:12]</strong> &nbsp;</p><p>that sounds like a viewpoint which would also think it much more likely that GPT-3 would foom in a billion years</p><p>where maybe you think that's unlikely, but I still get the impression your \"unlikely\" is, like, 5 orders of magnitude likelier than mine before applying overconfidence adjustments against extreme probabilities on both sides</p><p>yeah, I think I need to back up</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:15]</strong> &nbsp;</p><p>Is your position something like \"at some point after dinosaurs, there was an algorithmic innovation that increased returns to brain size, which meant that the birds and the humans see their brains increasing quickly while the dinosaurs didn't\"?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:15]</strong> &nbsp;</p><p>it also seems to me like the chimp-&gt;human difference is in basically the same ballpark of the effect of brain size within humans, given modest adaptations for culture</p><p>which seems like a relevant sanity-check that made me take the \"mostly hardware\" view more seriously</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:15]</strong> &nbsp;</p><p>there's a part of my model which very strongly says that hominids scaled better than elephants and that's why \"hominids-&gt;humans but not elephants-&gt;superelephants\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:15]</strong> &nbsp;</p><p>previously I had assumed that analysis would show that chimps were obviously <i>way</i> dumber than an extrapolation of humans</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:16]</strong> &nbsp;</p><p>there's another part of my model which says \"and it still didn't scale that well without algorithms, so we should expect a lot of alleles affecting brain circuitry which rose to fixation over the period when hominid brains were expanding\"</p><p>this part is strong and I think echoes back to AGI stuff, but it is not <i>as strong</i> as the much <i>more</i> overdetermined position that hominids started with more scalable algorithms than dinosaurs.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:17]</strong> &nbsp;</p><p>I do agree with the point that there are structural changes in brains as you scale them up, and this is potentially a reason why brain size changes more slowly than e.g. bone size. (Also there are small structural changes in ML algorithms as you scale them up, not sure how much you want to push the analogy but they feel fairly similar.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:17]</strong>&nbsp;</p><blockquote><p>it also seems to me like the chimp-&gt;human difference is in basically the same ballpark of the effect of brain size within humans, given modest adaptations for culture</p></blockquote><p>this part also seems pretty blatantly false to me</p><p>is there, like, a smooth graph that you looked at there?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:18]</strong>&nbsp;</p><p>I think the extrapolated difference would be about 4 standard deviations, so we are comparing a chimp to an IQ 40 human</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:18]</strong> &nbsp;</p><p>I'm really not sure how much of a fair comparison that is</p><p>IQ 40 humans in our society may be mostly sufficiently-damaged humans, not scaled-down humans</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:19]</strong> &nbsp;</p><p>doesn't seem easy, but the point is that the extrapolated difference is huge, it corresponds to completely debilitating developmental problems</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:19]</strong> &nbsp;</p><p>if you do enough damage to a human you end up with, for example, a coma victim who's not competitive with other primates at all</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:19]</strong> &nbsp;</p><p>yes, that's more than 4 SD down</p><p>I agree with this general point</p><p>I'd guess I just have a lot more respect for chimps than you do</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:20]</strong> &nbsp;</p><p>I feel like I have a bunch of respect for chimps but more respect for humans</p><p>like, that stuff humans do</p><p>that is really difficult stuff!</p><p>it is not just scaled-up chimpstuff!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:21]</strong> &nbsp;</p><p>Carl convinced me chimps wouldn't go to space, but I still really think it's about domesticity and cultural issues rather than intelligence</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:21]</strong> &nbsp;</p><p>the chimpstuff is very respectable but there is a whole big layer cake of additional respect on top</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:21]</strong> &nbsp;</p><p>not a prediction to be resolved until after the singularity</p><p>I mean, the space prediction isn't very confident 🙂</p><p>and it involved a very large planet of apes</p></td></tr></tbody></table>\n\n9.6. Architectural innovation in AI and in evolutionary history\n---------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:22]</strong>&nbsp;</p><p>I feel like if GPT-based systems saturate and require <i>any</i> architectural innovation rather than Stack More Layers to get much further, this is a pre-Singularity point of observation which favors humans probably being more qualitatively different from chimp-LCA</p><p>(LCA=last common ancestor)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:22]</strong>&nbsp;</p><p>any seems like a kind of silly bar?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:23]</strong> &nbsp;</p><p>because single architectural innovations are allowed to have large effects!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:23]</strong> &nbsp;</p><p>like there were already small changes to normalization from GPT-2 to GPT-3, so isn't it settled?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:23]</strong> &nbsp;</p><p>natural selection can't afford to deploy that many of them!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:23]</strong> &nbsp;</p><p>and the model really eventually won't work if you increase layers but don't fix the normalization, there are severe problems that only get revealed at high scale</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:23]</strong> &nbsp;</p><p>that I wouldn't call architectural innovation</p><p>transformers were</p><p>this is a place where I would not discuss specific ideas because I do not actually want this event to occur</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:24]</strong> &nbsp;</p><p>sure</p><p>have you seen a graph of LSTM scaling vs transformer scaling?</p><p>I think LSTM with ongoing normalization-style fixes lags like 3x behind transformers on language modeling</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:25]</strong> &nbsp;</p><p>no, does it show convergence at high-enough scales?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:25]</strong> &nbsp;</p><p>figure 7 here: <a href=\"https://arxiv.org/pdf/2001.08361.pdf\">https://arxiv.org/pdf/2001.08361.pdf</a></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_280 280w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_840 840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_1120 1120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_1400 1400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_1680 1680w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_1960 1960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_2240 2240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_2520 2520w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1cef6e971b840373aa6375a7e70d4c5f3b0a258d894558c.png/w_2761 2761w\"></figure></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:26]</strong> &nbsp;</p><p>yeah... I unfortunately would rather not give other people a sense for which innovations are obviously more of the same and which innovations obviously count as qualitative</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:26]</strong> &nbsp;</p><p>I think smart money is that careful initialization and normalization on the RNN will let it keep up for longer</p><p>anyway, I'm very open to differences like LSTM vs transformer between humans and 3x-smaller-brained-ancestors, as long as you are open to like 10 similar differences further back in the evolutionary history</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:28]</strong> &nbsp;</p><p>what if there's 27 differences like that and 243 differences further back in history?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:28]</strong> &nbsp;</p><p>sure</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:28]</strong> &nbsp;</p><p>is that a distinctly Yudkowskian view vs a Paul view...</p><p>apparently not</p><p>I am again feeling confused about cruxes</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:29]</strong> &nbsp;</p><p>I mean, 27 differences like transformer vs LSTM isn't actually plausible, so I guess we could talk about it</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:30]</strong> &nbsp;</p><p>Here's a potential crux articulation that ties it back to the animals stuff: paul thinks that we first discover major algorithmic innovations that improve intelligence at a low level of intelligence, analogous to evolution discovering major architectural innovations with tiny birds and primates, and then there will be a long period of scaling up plus coming up with routine algorithmic tweaks to get to the high level, analogous to evolution schlepping on the same shit for a long time to get to humans. analogously, he thinks when big innovations come onto the scene the actual product is crappy af (e.g. wright brother's plane), and it needs a ton of work to scale up to usable and then to great.</p><p>you both seem to think both evolution and tech history consiliently point in your direction</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:33]</strong> &nbsp;</p><p>that sounds vaguely right, I guess the important part of \"routine\" is \"vaguely predictable,\" like you mostly work your way down the low-hanging fruit (including new fruit that becomes more important as you scale), and it becomes more and more predictable the more people are working on it and the longer you've been at it</p><p>and deep learning is already reasonably predictable (i.e. the impact of successive individual architectural changes is smaller, and law of large numbers is doing its thing) and is getting more so, and I just expect that to continue</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:34]</strong> &nbsp;</p><p>yeah, like it's a view that points to using data that relates effort to algorithmic progress and using that to predict future progress (in combination with predictions of future effort)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:35]</strong> &nbsp;</p><p>yeah</p><p>and for my part, it feels like this is how most technologies look and also how current ML progress looks</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:36]</strong> &nbsp;</p><p>and <i>also</i> how evolution looks, right?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:37]</strong> &nbsp;</p><p>you aren't seeing big jumps in translation or in self-driving cars or in image recognition, you are just seeing a long slog, and you see big jumps in areas where few people work (usually up to levels that are not in fact that important, which is very correlated with few people working there)</p><p>I don't know much about evolution, but it at least looks very consistent with what I know and the facts eliezer cites</p><p>(not merely consistent, but \"explains the data just about as well as the other hypotheses on offer\")</p></td></tr></tbody></table>\n\n9.7. Styles of thinking in forecasting\n--------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:38]</strong>&nbsp;</p><p>I do observe that this would seem, on the surface of things, to describe the entire course of natural selection up until about 20K years ago, if you were looking at surface impacts</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:39]</strong> &nbsp;</p><p>by 20k years ago I think it's basically obvious that you are tens of thousands of years from the singularity</p><p>like, I think natural selection is going crazy with the brains by millions of years ago, and by hundreds of thousands of years ago humans are going crazy with the culture, and by tens of thousands of years ago the culture thing has accelerated and is almost at the finish line</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:41]</strong> &nbsp;</p><p>really? I don't know if I would have been able to call that in advance if I'd never seen the future or any other planets. I mean, maybe, but I sure would have been extrapolating way out onto a further limb than I'm going here.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:41]</strong> &nbsp;</p><p>Yeah, I agree singularity is way more out on a limb---or like, where the singularity stops is more uncertain since that's all that's really at issue from my perspective</p><p>but the point is that everything is clearly crazy in historical terms, in the same way that 2000 is crazy, even if you don't know where it's going</p><p>and the timescale for the crazy changes is tens of thousands of years</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:42]</strong> &nbsp;</p><p>I frankly model that, had I made any such prediction 20K years ago of hominids being able to pull of moon landings or global warming - never mind the Singularity - I would have faced huge pushback from many EAs, such as, for example, Robin Hanson, and you.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:42]</strong> &nbsp;</p><p>like I think this can't go on would have applied just as well: <a href=\"https://www.lesswrong.com/posts/5FZxhdi6hZp8QwK7k/this-can-t-go-on\">https://www.lesswrong.com/posts/5FZxhdi6hZp8QwK7k/this-can-t-go-on</a></p><p>I don't think that's the case at all</p><p>and I think you still somehow don't understand my position?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:43]</strong> &nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/XQirei3crsLxsCQoi/surprised-by-brains\">https://www.lesswrong.com/posts/XQirei3crsLxsCQoi/surprised-by-brains</a> is my old entry here</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:43]</strong> &nbsp;</p><p>like, what is the move I'm making here, that you think I would have made in the past?</p><p>and would have led astray?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:44]</strong> &nbsp;</p><p>I sure do feel in a deeper sense that I am trying very hard to account for perspective shifts in how unpredictable the future actually looks at the time, and the Other is looking back at the past and organizing it neatly and expecting the future to be that neat</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:45]</strong> &nbsp;</p><p>I don't even feel like I'm expecting the future to be neat</p><p>are you just saying you have a really broad distribution over takeoff speed, and that \"less than a month\" gets a lot of probability because lots of numbers are less than a month?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:47]</strong> &nbsp;</p><p>not exactly?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:47]</strong> &nbsp;</p><p>in what way is your view the one that is preferred by things being messy or unpredictable?</p><p>like, we're both agreeing X will eventually happen, and I'm making some concrete prediction about how some other X' will happen first, and that's the kind of specific prediction that's likely to be wrong?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:48]</strong> &nbsp;</p><p>more like, we sure can tell a story today about how normal and predictable AlphaGo was, but we can <i>always</i> tell stories like that about the past. I do not particularly recall the AI field standing up one year before AlphaGo and saying \"It's time, we're coming for the 8-dan pros this year and we're gonna be world champions a year after that.\" (Which took significantly longer in chess, too, matching my other thesis about how these slides are getting steeper as we get closer to the end.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:49]</strong> &nbsp;</p><p>it's more like, you are offering AGZ as an example of why things are crazy, and I'm doubtful / think it's pretty lame</p><p>maybe I don't understand how it's functioning as bayesian evidence</p><p>for what over what</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:50]</strong> &nbsp;</p><p>I feel like the whole smoothness-reasonable-investment view, if evaluated on Earth 5My ago <i>without benefit of foresight</i>, would have dismissed the notion of brains overtaking evolution; evaluated 1My ago, it would have dismissed the notion of brains overtaking evolution; evaluated 20Ky ago, it would have barely started to acknowledge that brains were doing anything interesting at all, but pointed out how the hominids could still only eat as much food as their niche offered them and how the cute little handaxes did not begin to compare to livers and wasp stings.</p><p>there is a style of thinking that says, \"wow, yeah, people in the past sure were surprised by stuff, oh, wait, <i>I'm also in the past</i>, aren't I, I am one of those people\"</p><p>and a view where you look back from the present and think about how reasonable the past all seems now, and the future will no doubt be equally reasonable</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:52]</strong> &nbsp;</p><p>(the AGZ example may fall flat, because the arguments we are making about it now <i>we were also making in the past</i>)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:52]</strong> &nbsp;</p><p>I am not sure this is resolvable, but it is among my primary guesses for a deep difference in believed styles of thought</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:52]</strong> &nbsp;</p><p>I think that's a useful perspective, but still don't see how it favors your bottom line</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:53]</strong> &nbsp;</p><p>where I look at the style of thinking you're using, and say, not, \"well, that's invalidated by a technical error on line 3 even on Paul's own terms\" but \"isn't this obviously a whole style of thought that never works and ends up unrelated to reality\"</p><p>I think the first AlphaGo was the larger shock, AlphaGo Zero was a noticeable but more mild shock on account of how it showed the end of game programming and not just the end of Go</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:54]</strong> &nbsp;</p><p>sorry, I lumped them together</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:54]</strong> &nbsp;</p><p>it didn't feel like the same level of surprise; it was precedented by then</p><p>the actual accomplishment may have been larger in an important sense, but a lot of the - epistemic landscape of lessons learned? - is about the things that surprise you at the time</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:55]</strong> &nbsp;</p><p>also AlphaGo was also quite easy to see coming after this paper (as was discussed extensively <i>at the time</i>): <a href=\"https://www.cs.toronto.edu/~cmaddis/pubs/deepgo.pdf\">https://www.cs.toronto.edu/~cmaddis/pubs/deepgo.pdf</a></p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:55]</strong> &nbsp;</p><p>Paul, are you on the record as arguing with me that AlphaGo will win at Go because it's predictably on-trend?</p><p>back then?</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:55]</strong> &nbsp;</p><p>Hm, it sounds like Paul is saying \"I do a trend extrapolation over long time horizons and if things seem to be getting faster and faster I expect they'll continue to accelerate; this extrapolation if done 100k years ago would have seen that things were getting faster and faster and projected singularity within 100s of K years\"</p><p>Do you think Paul is in fact doing something other than the trend extrap he says he's doing, or that he would have looked at a different less informative trend than the one he says he would have looked at, or something else?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][15:56]</strong> &nbsp;</p><p>my methodology for answering that question is looking at LW comments mentioning go by me, can see if it finds any</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:56]</strong> &nbsp;</p><p>Different less informative trend, is most of my suspicion there?</p><p>though, actually, I should revise that, I feel like relatively little of the WHA was AlphaGo v2 whose name I forget beating Lee Se-dol, and most was in the revelation that v1 beat the high-dan pro whose name I forget.</p><p>Paul having himself predicted anything at <i>all</i> like this would be the actually impressive feat</p><p>that would cause me to believe that the AI world is more regular and predictable than I experienced it as, if you are paying more attention to ICLR papers than I do</p></td></tr></tbody></table>\n\n9.8. Moravec's prediction\n-------------------------\n\n<table><tbody><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][15:58]</strong>&nbsp;</p><p>And jtbc, the trend extrap paul is currently doing is something like:</p><ul><li>Look at how effort leads to hardware progress measured in FLOP/$ and software progress measured in stuff like \"FLOP to do task X\" or \"performance on benchmark Y\"</li><li>Look at how effort in the ML industry as a whole is increasing, project forward with maybe some adjustments for thinking markets are more inefficient now and will be less inefficient later</li></ul><p>and this is the wrong trend, because he shouldn't be looking at hardware/software progress across the whole big industry and should be more open to an upset innovation coming from an area with a small number of people working on it?</p><p>and he would have similarly used the wrong trends while trying to do trend extrap in the past?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:59]</strong> &nbsp;</p><p>because I feel like this general style of thought doesn't work when you use it on Earth generally, and then fails extremely hard if you try to use it on Earth before humans to figure out where the hominids are going because that phenomenon is Different from Previous Stuff</p><p>like, to be clear, I have seen this used well on solar</p><p>I feel like I saw some people calling the big solar shift based on graphs, before that happened</p><p>I have seen this used great by Moravec on computer chips to predict where computer chips would be in 2012</p><p>and also witnessed Moravec <i>completely failing</i> as soon as he tried to derive <i>literally anything but the graph itself</i> namely his corresponding prediction for human-equivalent AI in 2012 (I think, maybe it was 2010) or something</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:02]</strong> &nbsp;</p><p>(I think in his 1988 book Moravec estimated human-level AI in ~2030, not sure if you are referring to some earlier prediction?)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:02]</strong> &nbsp;</p><p>(I have seen Ray Kurzweil project out Moore's Law to the $1,000,000 human brain in, what was it, 2025, followed by the $1000 human brain in 2035 and the $1 human brain in 2045, and when I asked Ray whether machine superintelligence might shift the graph at all, he replied that machine superintelligence was precisely how the graph would be able to continue on trend. This indeed is sillier than EAs.)</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:03]</strong> &nbsp;</p><p>moravec's prediction appears to actually be around 2025, looking at his hokey graph? <a href=\"https://jetpress.org/volume1/moravec.htm\">https://jetpress.org/volume1/moravec.htm</a></p><figure class=\"image\"><img src=\"https://jetpress.org/volume1/power_075.jpg\" alt=\"Power/cost of 150 computers from 1900 to 1997, rising 1000x every 20, now 10, years\"></figure></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:03]</strong> &nbsp;</p><p>but even there, it does feel to me like there is a commonality between Kurzweil's sheer graph-worship and difficulty in appreciating the graphs as surface phenomena that are less stable than deep phenomena, and something that Hanson was doing wrong in the foom debate</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:03]</strong> &nbsp;</p><p>which is...like, your timelines?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:04]</strong> &nbsp;</p><p>that's 1998</p><p>Mind Children in 1988 I am pretty sure had an earlier prediction</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:04]</strong> &nbsp;</p><p>I should think you'd be happy to bet against me on basically any prediction, shouldn't you?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:05]</strong> &nbsp;</p><p>any prediction that sounds narrow and isn't like \"this graph will be on trend in 3 more years\"</p><p>...maybe I'm wrong, an online source says Mind Children in 1988 predicted AGI in \"40 years\" but I sure do seem to recall an extrapolated graph that reached \"human-level hardware\" in 2012 based on an extensive discussion about computing power to duplicate the work of the retina</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:08]</strong> &nbsp;</p><p>don't think it matters too much other than for Moravec's honor, doesn't really make a big difference for the empirical success of the methodology</p><p>I think it's on page 68 if you have the physical book</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:09]</strong> &nbsp;</p><p>p60 via Google Books says 10 teraops for a human-equivalent mind</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:09]</strong> &nbsp;</p><p>I have a general read of history where trend extrapolation works extraordinarily well relative to other kinds of forecasting, to the extent that the best first-pass heuristic for whether a prediction is likely to be accurate is whether it's a trend extrapolation and how far in the future it is</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:09]</strong> &nbsp;</p><p>which, incidentally, strikes me as entirely plausible if you had algorithms as sophisticated as the human brain</p><p>my sense is that Moravec nailed the smooth graph of computing power going on being smooth, but then all of his predictions about the actual future were completely invalid on account of a curve interacting with his curve that he didn't know things about and so simply omitted as a step in his calculations, namely, AGI algorithms</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:12]</strong> &nbsp;</p><p>though again, from your perspective 2030 is still a reasonable bottom-line forecast that makes him one of the most accurate people at that time?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:12]</strong> &nbsp;</p><p>you could be right about all the local behaviors that your history is already shouting out at you as having smooth curve (where by \"local\" I do mean to exclude stuff like world GDP extrapolated into the indefinite future) and the curves that history isn't shouting at you will tear you down</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:12]</strong> &nbsp;</p><p>(I don't know if he even forecast that)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:12]</strong> &nbsp;</p><p>I don't remember that part from the 1988 book</p><p>my memory of the 1988 book is \"10 teraops, based on what it takes to rival the retina\" and he drew a graph of Moore's Law</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:13]</strong> &nbsp;</p><p>yeah, I think that's what he did</p><p>(and got 2030)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:14]</strong> &nbsp;</p><p>\"If this rate of improvement were to continue into the next century, the 10 teraops required for a humanlike computer would be available in a $10 million supercomputer before 2010 and in a $1,000 personal computer by 2030.\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:14]</strong> &nbsp;</p><p>or like, he says \"human equivalent in 40 years\" and predicts that in 50 years we will have robots with superhuman reasoning ability, not clear he's ruling out human-equivalent AGI before 40 years but I think the tone is clear</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:15]</strong> &nbsp;</p><p>so 2030 for AGI on a personal computer and 2010 for AGI on a supercomputer, and I expect that on my first reading I simply discarded the former prediction as foolish extrapolation past the model collapse he had just predicted in 2010.</p><p>(p68 in \"Powering Up\")</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:15]</strong> &nbsp;</p><p>yeah, that makes sense</p><p>I do think the PC number seems irrelevant</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:16]</strong> &nbsp;</p><p>I think both in that book and in the 98 article he wants you to pay attention to the \"very cheap human-size computers\" threshold, not the \"supercomputer\" threshold, i think intentionally as a way to handwave in \"we need people to be able to play around with these things\"</p><p>(which people criticized him at the time for not more explicitly modeling iirc)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:17]</strong> &nbsp;</p><p>but! I mean! there are so many little places where the media has a little cognitive hiccup about that and decides in 1998 that it's fine to describe that retrospectively as \"you predicted in 1988 that we'd have true AI in 40 years\" and then the future looks less surprising than people at the time using Trend Logic were actually surprised by it!</p><p>all these little ambiguities and places where, oh, you decide retroactively that it would have made sense to look at <i>this</i> Trend Line and use it <i>that</i> way, but if you look at what people said at the time, they didn't actually say that!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:19]</strong> &nbsp;</p><p>I mean, in fairness reading the book it just doesn't seem like he is predicting human-level AI in 2010 rather than 2040, but I do agree that it seems like the basic methodology (why care about the small computer thing?) doesn't really make that much sense a priori and only leads to something sane if it cancels out with a weird view</p></td></tr></tbody></table>\n\n9.9. Prediction disagreements and bets\n--------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:19]</strong> &nbsp;</p><p>anyway, I'm pretty unpersuaded by the kind of track record appeal you are making here</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:20]</strong>&nbsp;</p><p>if the future goes the way I predict and yet anybody somehow survives, perhaps somebody will draw a hyperbolic trendline on some particular chart where the trendline is retroactively fitted to events including those that occurred in only the last 3 years, and say with a great sage nod, ah, yes, that was all according to trend, nor did anything depart from trend</p><p>trend lines permit anything</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:20]</strong> &nbsp;</p><p>like from my perspective the fundamental question is whether I would do better or worse by following the kind of reasoning you'd advocate, and it just looks to me like I'd do worse, and I'd love to make any predictions about anything to help make that more clear and hindsight-proof in advance</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:20]</strong> &nbsp;</p><p>you just look into the past and find a line you can draw that ended up where reality went</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:21]</strong> &nbsp;</p><p>it feels to me like you really just waffle on almost any prediction about the before-end-of-days</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:21]</strong> &nbsp;</p><p>I don't think I know a lot about the before-end-of-days</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:21]</strong> &nbsp;</p><p>like if you make a prediction I'm happy to trade into it, or you can pick a topic and I can make a prediction and you can trade into mine</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:21]</strong> &nbsp;</p><p>but you know enough to have strong timing predictions, e.g. your bet with caplan</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:21]</strong> &nbsp;</p><p>it's daring enough that I claim to know anything about the Future at all!</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:21]</strong> &nbsp;</p><p>surely with that difference of timelines there should be some pre-2030 difference as well</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:21]</strong> &nbsp;</p><p>but you are the one making the track record argument against my way of reasoning about things!</p><p>how does that not correspond to believing that your predictions are better!</p><p>what does that mean?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:22]</strong> &nbsp;</p><p>yes and if you say something narrow enough or something that my model does at least vaguely push against, we should bet</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:22]</strong> &nbsp;</p><p>my point is that I'm willing to make a prediction about any old thing, you can name your topic</p><p>I think the way I'm reasoning about the future is just better in general</p><p>and I'm going to beat you on whatever thing you want to bet on</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:22]</strong> &nbsp;</p><p>but if you say, \"well, Moore's Law on trend, next 3 years\", then I'm like, \"well, yeah, sure, since I don't feel like I know anything special about that, that would be my prediction too\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:22]</strong> &nbsp;</p><p>sure</p><p>you can pick the topic</p><p>pick a quantity</p><p>or a yes/no question</p><p>or whatever</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:23]</strong> &nbsp;</p><p>you may know better than I would where your Way of Thought makes strong, narrow, or unusual predictions</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:23]</strong> &nbsp;</p><p>I'm going to trend extrapolation everywhere</p><p>spoiler</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:23]</strong> &nbsp;</p><p>okay but any superforecaster could do that and I could do the same by asking a superforecaster</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:24]</strong> &nbsp;</p><p>but there must be places where you'd strongly disagree w the superforecaster</p><p>since you disagree with them eventually, e.g. &gt;2/3 doom by 2030</p></td></tr><tr><td style=\"background-color:rgb(255, 238, 187);border-color:hsl(0, 0%, 0%)\"><p><strong>[Bensinger][18:40] &nbsp;(Nov. 25 follow-up comment)</strong>&nbsp;</p><p>\"&gt;2/3 doom by 2030\" isn't an actual Eliezer-prediction, and is based on a misunderstanding of something Eliezer said. See <a href=\"https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-and-yudkowsky-on-ai-progress?commentId=diChXiELZd62hgRyK#diChXiELZd62hgRyK\">Eliezer's comment on LessWrong</a>.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:24]</strong> &nbsp;</p><p>in the terminal phase, sure</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:24]</strong> &nbsp;</p><p>right, but there are no disagreements before jan 1 2030?</p><p>no places where you'd strongly defy the superforecasters/trend extrap?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:24]</strong> &nbsp;</p><p>superforecasters were claiming that AlphaGo had a 20% chance of beating Lee Se-dol and I didn't disagree with that at the time, though as the final days approached I became nervous and suggested to a friend that they buy out of a bet about that</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:25]</strong> &nbsp;</p><p>what about like whether we get some kind of AI ability (e.g. coding better than X) before end days</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:25]</strong> &nbsp;</p><p>though that was more because of having started to feel incompetent and like I couldn't trust the superforecasters to know more, than because I had switched to a confident statement that AlphaGo would win</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:25]</strong> &nbsp;</p><p>seems like EY's deep intelligence / insight-oriented view should say something about what's not possible before we get the \"click\" and the FOOM</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:25]</strong> &nbsp;</p><p>I mean, I'm OK with either (i) evaluating arguments rather than dismissive and IMO totally unjustified track record, (ii) making bets about stuff</p><p>I don't see how we can both be dismissing things for track record reasons and also not disagreeing about things</p><p>if our methodologies agree about all questions before end of days (which seems crazy to me) then surely there is no track record distinction between them...</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:26]</strong> &nbsp;</p><p>do you think coding models will be able to 2x programmer productivity before end days? 4x?</p><p>what about hardware/software R&amp;D wages? will they get up to $20m/yr for good ppl?</p><p>will someone train a 10T param model before end days?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:27]</strong> &nbsp;</p><p>things I'm happy to bet about: economic value of LMs or coding models at 2, 5, 10 years, benchmark performance of either, robotics, wages in various industries, sizes of various industries, compute/$, someone else's views about \"how ML is going\" in 5 years</p><p>maybe the \"any GDP acceleration before end of days?\" works, but I didn't like how you don't win until the end of days</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:28]</strong> &nbsp;</p><p>okay, so here's an example place of a <i>weak</i> general Yudkowskian prediction, that is weaker than terminal-phase stuff of the End Days: (1) I predict that cycles of 'just started to be able to do Narrow Thing -&gt; blew past upper end of human ability at Narrow Thing' will continue to get shorter, the same way that, I think, this happened faster with Go than with chess.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:28]</strong> &nbsp;</p><p>great, I'm totally into it</p><p>what's a domain?</p><p>coding?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:28]</strong> &nbsp;</p><p>Does Paul disagree? Can Paul point to anything equally specific out of Paul's viewpoint?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:28]</strong> &nbsp;</p><p>benchmarks for LMs?</p><p>robotics?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:28]</strong> &nbsp;</p><p>well, for these purposes, we do need some Elo-like ability to measure at all where things are relative to humans</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:29]</strong> &nbsp;</p><p>problem-solving benchmarks for code?</p><p>MATH benchmark?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:29]</strong> &nbsp;</p><p>well, for coding and LM'ing we have lots of benchmarks we can use</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:29]</strong> &nbsp;</p><p>this unfortunately does feel a bit different to me from Chess benchmarks where the AI is playing the whole game; Codex is playing part of the game</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:29]</strong> &nbsp;</p><p>in general the way I'd measure is by talking about how fast you go from \"weak human\" to \"strong human\" (e.g. going from top-10,000 in chess to top-10 or whatever, going from jobs doable by $50k/year engineer to $500k/year engineer...)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:30]</strong> &nbsp;</p><p>golly, that sounds like a viewpoint very favorable to mine</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:30]</strong> &nbsp;</p><p>what do you mean?</p><p>that way of measuring would be favorable to your viewpoint?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:31]</strong> &nbsp;</p><p>if we measure how far it takes AI to go past different levels of paying professionals, I expect that the Chess duration is longer than the Go duration and that by the time Codex is replacing <s>a</s> most paid $50k/year programmers the time to replacing <s>a</s> most programmers paid as much as a top Go player will be pretty darned short</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:31]</strong> &nbsp;</p><p>top Go players don't get paid, do they?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:31]</strong> &nbsp;</p><p>they tutor students and win titles</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:31]</strong> &nbsp;</p><p>but I mean, they are like low-paid engineers</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:31]</strong> &nbsp;</p><p>yeah that's part of the issue here</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:31]</strong> &nbsp;</p><p>I'm using wages as a way to talk about the distribution of human abilities, not the fundamental number</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:32]</strong> &nbsp;</p><p>I would expect something similar to hold over going from low-paying welder to high-paying welder</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:32]</strong> &nbsp;</p><p>like, how long to move from \"OK human\" to \"pretty good human\" to \"best human\"</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:32]</strong> &nbsp;</p><p>says salary of $350k/yr for lee: <a href=\"https://www.fameranker.com/lee-sedol-net-worth\">https://www.fameranker.com/lee-sedol-net-worth</a></p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:32]</strong> &nbsp;</p><p>but I also mostly expect that AIs will not be allowed to weld things on Earth</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:32]</strong> &nbsp;</p><p>why don't we just do an in vitro benchmark instead of wages?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:32]</strong> &nbsp;</p><p>what, machines already do virtually all welding?</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:32]</strong> &nbsp;</p><p>just pick a benchmark?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:33]</strong> &nbsp;</p><p>yoouuuu do not want to believe sites like that (fameranker)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:33]</strong> &nbsp;</p><p>yeah, I'm happy with any benchmark, and then we can measure various human levels at that benchmark</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:33]</strong> &nbsp;</p><p>what about MATH? <a href=\"https://arxiv.org/abs/2103.03874\">https://arxiv.org/abs/2103.03874</a></p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:34]</strong> &nbsp;</p><p>also I don't know what \"shorter and shorter\" means, the time in go and chess was decades to move from \"strong amateur\" to \"best human,\" I do think these things will most likely be shorter than decades</p><p>seems like we can just predict concrete #s though</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: 👍]</td></tr></tbody></table></figure><p>like I can say how long I think it will take to get from \"median high schooler\" to \"IMO medalist\" and you can bet against me?</p><p>and if we just agree about all of those predictions then again I'm back to being very skeptical of a claimed track record difference between our models</p><p>(I do think that it's going to take years rather than decades on all of these things)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:36]</strong> &nbsp;</p><p>possibly! I worry this ends up in a case where Katja or Luke or somebody goes back and collects data about \"amateur to pro performance times\" and Eliezer says \"Ah yes, these are shortening over time, just as I predicted\" and Paul is like \"oh, well, I predict they continue to shorten on this trend drawn from the data\" and Eliezer is like \"I guess that could happen for the next 5 years, sure, sounds like something a superforecaster would predict as default\"</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:37]</strong> &nbsp;</p><p>i'm pretty sure paul's methodology here will just be to look at the MATH perf trend based on model size and combine with expectations of when ppl will make big enough models, not some meta trend thing like that?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:37]</strong> &nbsp;</p><p>so I feel like... a bunch of what I feel is the real disagreement in our models, is a bunch of messy stuff Suddenly Popping Up one day and then Eliezer is like \"gosh, I sure didn't predict that\" and Paul is like \"somebody could have totally predicted that\" and Eliezer is like \"people would say exactly the same thing after the world ended in 3 minutes\"</p><p>if we've already got 2 years of trend on a dataset, I'm not necessarily going to predict the trend breaking</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:38]</strong> &nbsp;</p><p>hm, you're presenting your view as more uncertain and open to anything here than paul's view, but in fact it's picking out a narrower distribution. you're more confident in powerful AGI soon</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:38]</strong> &nbsp;</p><p>seems hard to play the \"who is more confident?\" game</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:38]</strong> &nbsp;</p><p>so there should be some places where you make a strong positive prediction paul disagrees with</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:39]</strong> &nbsp;</p><p>I might want to buy options on a portfolio of trends like that, if Paul is willing to sell me insurance against all of the trends breaking upward at a lower price than I think is reasonable</p><p>I mean, from my perspective Paul is the one who seems to think the world is well-organized and predictable in certain ways</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:39]</strong> &nbsp;</p><p>yeah, and you are saying that I'm overconfident about that</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:39]</strong> &nbsp;</p><p>I keep wanting Paul to go on and make narrower predictions than I do in that case</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:39]</strong> &nbsp;</p><p>so you should be happy to bet with me about <i>anything</i></p><p>and I'm letting you pick anything at all you want to bet about</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:40]</strong> &nbsp;</p><p>i mean we could do a portfolio of trends like MATH and you could bet on at least a few of them having strong surprises in the sooner direction</p><p>but that means we could just bet about MATH and it'd just be higher variance</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:40]</strong> &nbsp;</p><p>ok but you're not going to sell me cheap options on sharp declines in the S&amp;P 500 even though in a very reasonable world there would not be any sharp declines like that</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:41]</strong> &nbsp;</p><p>if we're betting $ rather than bayes points, then yes I'm going to weigh worlds based on the value of $ in those worlds</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:41]</strong> &nbsp;</p><p>wouldn't paul just sell you options at the price the options actually trade for? i don't get it</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:41]</strong> &nbsp;</p><p>but my sense is that I'm just generally across the board going to be more right than you are, and I'm frustrated that you just keep saying that \"people like me\" are wrong about stuff</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:41]</strong> &nbsp;</p><p>Paul's like \"we'll see smooth behavior in the end days\" and I feel like I should be able to say \"then Paul, sell me cheap options against smooth behavior now\" but Paul is just gonna wanna sell at market price</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:41]</strong> &nbsp;</p><p>and so I want to hold you to that by betting about anything</p><p>ideally just tons of stuff</p><p>random things about what AI will be like, and other technologies, and regulatory changes</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:42]</strong> &nbsp;</p><p>paul's view doesn't seem to imply that he should value those options less than the market</p><p>he's more EMH-y than you not less</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:42]</strong> &nbsp;</p><p>but then the future should <i>behave like that market</i></p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:42]</strong> &nbsp;</p><p>what do you mean?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:42]</strong> &nbsp;</p><p>it should have options on wild behavior that are not cheap!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:42]</strong> &nbsp;</p><p>you mean because people want $ more in worlds where the market drops a lot?</p><p>I don't understand the analogy</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:43]</strong> &nbsp;</p><p>no, because jumpy stuff happens more than it would in a world of ideal agents</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:43]</strong> &nbsp;</p><p>I think EY is saying the non-cheap option prices are because P(sharp declines) is pretty high</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:43]</strong> &nbsp;</p><p>ok, we know how often markets jump, if that's the point of your argument can we just talk about that directly?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:43]</strong> &nbsp;</p><p>or sharp rises, for that matter</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:43]</strong> &nbsp;</p><p>(much lower than option prices obviously)</p><p>I'm probably happy to sell you options for sharp rises</p><p>I'll give you better than market odds in that direction</p><p>that's how this works</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:44]</strong> &nbsp;</p><p>now I am again confused, for I thought you were the one who expected world GDP to double in 4 years at some point</p><p>and indeed, drew such graphs with the rise suggestively happening earlier than the sharp spike</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:44]</strong> &nbsp;</p><p>yeah, and I have exposure to that by buying stocks, options prices are just a terrible way of tracking these things</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:44]</strong> &nbsp;</p><p>suggesting that such a viewpoint is generally favor to near timelines for that</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:44]</strong> &nbsp;</p><p>I mean, I have bet a <i>lot</i> of money on AI companies doing well</p><p>well, not compared to the EA crowd, but compared to my meager net worth 🙂</p><p>and indeed, it has been true so far</p><p>and I'm continuing to make the bet</p><p>it seems like on your view it should be surprising that AI companies just keep going up</p><p>aren't you predicting them not to get to tens of trillions of valuation before the end of days?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:45]</strong> &nbsp;</p><p>I believe that Nate, of a generally Yudkowskian view, did the same (bought AI companies). and I focused my thoughts elsewhere, because somebody needs to, but did happen to buy my first S&amp;P 500 on its day of exact minimum in 2020</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:46]</strong> &nbsp;</p><p>point is, that's how you get exposure to the crazy growth stuff with continuous ramp-ups</p><p>and I'm happy to make the bet on the market</p><p>or on other claims</p><p>I don't know if my general vibe makes sense here, and why it seems reasonable to me that I'm just happy to bet on anything</p><p>as a way of trying to defend my overall attack</p><p>and that if my overall epistemic approach is vulnerable to some track record objection, then it seems like it ought to be possible to win here</p></td></tr></tbody></table>\n\n9.10. Prediction disagreements and bets: Standard superforecaster techniques\n----------------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:47]</strong>&nbsp;</p><p>I'm still kind of surprised that Eliezer isn't willing to bet that there will be a faster-than-Paul expects trend break on MATH or whatever other benchmark. Is it just the variance of MATH being one benchmark? Would you make the bet if it were 6?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:47]</strong> &nbsp;</p><p>a large problem here is that both of us tend to default strongly to superforecaster standard techniques</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:47]</strong> &nbsp;</p><p>it's true, though it's less true for longer things</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:47]</strong> &nbsp;</p><p>but you think the superforecasters would suck at predicting end days because of the surface trends thing!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:47]</strong> &nbsp;</p><p>before I bet against Paul on MATH I would want to know that Paul wasn't arriving at the same default I'd use, which might be drawn from trend lines there, or from a trend line in trend lines</p><p>I mean the superforecasters did already suck once in my observation, which was AlphaGo, but I did not bet against them there, I bet with them and then updated afterwards</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:48]</strong> &nbsp;</p><p>I'd mostly try to eyeball how fast performance was improving with size; I'd think about difficulty effects (where e.g. hard problems will be flat for a while and then go up later, so you want to measure performance on a spectrum of difficulties)</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:48]</strong> &nbsp;</p><p>what if you bet against a methodology instead of against paul's view? the methodology being the one i described above, of looking at the perf based on model size and then projecting model size increases by cost?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:48]</strong> &nbsp;</p><p>seems safer to bet against my view</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:48]</strong> &nbsp;</p><p>yeah</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:48]</strong> &nbsp;</p><p>mostly I'd just be eyeballing size, thinking about how much people will in fact scale up (which would be great to factor out if possible), assuming performance trends hold up</p><p>are there any other examples of surface trends vs predictable deep changes, or is AGI the only one?</p><p>(that you have thought a lot about)</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:49]</strong> &nbsp;</p><p>yeah seems even better to bet on the underlying \"will the model size to perf trends hold up or break upward\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:49]</strong> &nbsp;</p><p>so from my perspective, there's this whole thing where <i>unpredictably</i> something breaks above trend because the first way it got done was a way where somebody could do it faster than you expected</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:49]</strong> &nbsp;</p><p>(makes sense for it to be the domain where you've thought a lot)</p><p>you mean, it's unpredictable what will break above trend?</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:49]</strong> &nbsp;</p><p><a href=\"https://intelligence.org/files/IEM.pdf\">IEM</a> has a financial example</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:49]</strong> &nbsp;</p><p>I mean that I could not have said \"<i>Go</i> will break above trend\" in 2015</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:49]</strong> &nbsp;</p><p>yeah</p><p>ok, here's another example</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:50]</strong> &nbsp;</p><p>it feels like if I want to make a bet with imaginary Paul in 2015 then I have to bet on a portfolio</p><p>and I also feel like as soon as we make it that concrete, Paul does not want to offer me things that I want to bet on</p><p>because Paul is also like, sure, something might break upward</p><p>I remark that I have for a long time been saying that I wish Paul had more concrete images and examples attached to <i>a lot of his stuff</i></p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:51]</strong> &nbsp;</p><p>surely the view is about the probability of each thing breaking upward. or the expected number from a basket</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:51]</strong> &nbsp;</p><p>I mean, if you give me any way of quantifying how much stuff breaks upwards we have a bet</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:51]</strong> &nbsp;</p><p>not literally that one single thing breaks upward</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:51]</strong> &nbsp;</p><p>I don't understand how concreteness is an accusation here, I've offered 10 quantities I'd be happy to bet about, and also allowed you to name literally any other quantity you want</p><p>and I agree that we mostly agree about things</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:52]</strong> &nbsp;</p><p>and some of my sense here is that if Paul offered a portfolio bet of this kind, I might not take it myself, but EAs who were better at noticing their own surprise might say, \"Wait, <i>that's</i> how unpredictable Paul thinks the world is?\"</p><p>so from my perspective, it is hard to know specific anti-superforecaster predictions that happen long before terminal phase, and I am not sure we are really going to get very far there.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:53]</strong> &nbsp;</p><p>but you agree that the eventual prediction is anti-superforecaster?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:53]</strong> &nbsp;</p><p>both of us probably have quite high inhibitions against selling conventionally priced options that are way not what a superforecaster would price them as</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][16:53]</strong> &nbsp;</p><p>why does it become so much easier to know these things and go anti-superforecaster at terminal phase?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:53]</strong> &nbsp;</p><p>I assume you think that the superforecasters will continue to predict that big impactful AI applications are made by large firms spending a lot of money, even through the end of days</p><p>I do think it's very often easy to beat superforecasters in-domain</p><p>like I expect to personally beat them at most ML prediction</p><p>and so am also happy to do bets where you defer to superforecasters on arbitrary questions and I bet against you</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:54]</strong> &nbsp;</p><p>well, they're anti-prediction-market in the sense that, at the very end, bets can no longer settle. I've been surprised of late by how much AGI ruin seems to be sneaking into common knowledge; perhaps in the terminal phase the superforecasters will be like, \"yep, we're dead\". I can't even say that in this case, Paul will disagree with them, because I expect the state on alignment to be so absolutely awful that even Paul is like \"You were not supposed to do it that way\" in a very sad voice.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:55]</strong> &nbsp;</p><p>I'm just thinking about takeoff speeds here</p><p>I do think it's fairly likely I'm going to be like \"oh no this is bad\" (maybe 50%?), but not that I'm going to expect fast takeoff</p><p>and similarly for the superforecasters</p></td></tr></tbody></table>\n\n9.11. Prediction disagreements and bets: Late-stage predictions, and betting against superforecasters\n-----------------------------------------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:55]</strong>&nbsp;</p><p>so, one specific prediction you made, sadly close to terminal phase but not much of a surprise there, is that the world economy must double in 4 years before the End Times are permitted to begin</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:56]</strong>&nbsp;</p><p>well, before it doubles in 1 year...</p><p>I think most people would call the 4 year doubling the end times</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:56]</strong>&nbsp;</p><p>this seems like you should also be able to point to some least impressive thing that is not permitted to occur before WGDP has doubled in 4 years</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:56]</strong>&nbsp;</p><p>and it means that the normal planning horizon includes the singularity</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:56]</strong>&nbsp;</p><p>it may not be much but we would be <i>moving back</i> the date of first concrete disagreement</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:57]</strong>&nbsp;</p><p>I can list things I don't think would happen first, since that's a ton</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:57]</strong>&nbsp;</p><p>and EAs might have a little bit of time in which to say \"Paul was falsified, uh oh\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:57]</strong>&nbsp;</p><p>the only things that aren't permitted are the ones that would have caused the world economy to double in 4 years</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:58]</strong>&nbsp;</p><p>and by the same token, there are things Eliezer thinks you are probably not going to be able to do before you slide over the edge. a portfolio of these will have some losing options because of adverse selection against my errors of what is hard, but if I lose more than half the portfolio, this may said to be a bad sign for Eliezer.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:58]</strong>&nbsp;</p><p>(though those can happen at the beginning of the 4 year doubling)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:58]</strong>&nbsp;</p><p>this is unfortunately <i>late</i> for falsifying our theories but it would be <i>progress</i> on a kind of bet against each other</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:59]</strong>&nbsp;</p><p>but I feel like the things I'll say are like fully automated construction of fully automated factories at 1-year turnarounds, and you're going to be like \"well duh\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:59]</strong>&nbsp;</p><p>...unfortunately yes</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:59]</strong>&nbsp;</p><p>the reason I like betting about numbers is that we'll probably just disagree on any given number</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][16:59]</strong>&nbsp;</p><p>I don't think I <i>know</i> numbers.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][16:59]</strong>&nbsp;</p><p>it does seem like a drawback that this can just turn up object-level differences in knowledge-of-numbers more than deep methodological advantages</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:00]</strong>&nbsp;</p><p>the last important number I had a vague suspicion I might know was that Ethereum ought to have a significantly larger market cap in pre-Singularity equilibrium.</p><p>and I'm not as sure of that one since El Salvador supposedly managed to use Bitcoin L2 Lightning.</p><p>(though I did not fail to act on the former belief)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:01]</strong>&nbsp;</p><p>do you see why I find it weird that you think there is this deep end-times truth about AGI, that is very different from a surface-level abstraction and that will take people like Paul by surprise, without thinking there are other facts like that about the world?</p><p>I do see how this annoying situation can come about</p><p>and I also understand the symmetry of the situation</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:02]</strong>&nbsp;</p><p>we unfortunately both have the belief that the present world looks a lot like our being right, and therefore that the other person ought to be willing to bet against default superforecasterish projections</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:02]</strong>&nbsp;</p><p>paul says that <i>he</i> would bet against superforecasters too though</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:02]</strong>&nbsp;</p><p>I would in ML</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:02]</strong>&nbsp;</p><p>like, where specifically?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:02]</strong>&nbsp;</p><p>or on any other topic where I can talk with EAs who know about the domain in question</p><p>I don't know if they have standing forecasts on things, but e.g.: (i) benchmark performance, (ii) industry size in the future, (iii) how large an LM people will train, (iv) economic impact of any given ML system like codex, (v) when robotics tasks will be plausible</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:03]</strong>&nbsp;</p><p>I have decided that, as much as it might gain me prestige, I don't think it's actually the right thing for me to go spend a bunch of character points on the skills to defeat superforecasters in specific domains, and then go around doing that to prove my epistemic virtue.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:03]</strong>&nbsp;</p><p>that seems fair</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:03]</strong>&nbsp;</p><p>you don't need to bet with <i>me</i> to prove your epistemic virtue in this way, though</p><p>okay, but, if I'm allowed to go around asking Carl Shulman who to ask in order to get the economic impact of Codex, maybe I can also defeat superforecasters.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:04]</strong>&nbsp;</p><p>I think the deeper disagreement is that (i) I feel like my end-of-days prediction is also basically just a default superforecaster prediction (and if you think yours is too then we can bet about what some superforecasters will say on it), (ii) I think you are leveling a much stronger \"people like paul get taken by surprise by reality\" claim whereas I'm just saying that I don't like your arguments</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:04]</strong>&nbsp;</p><p>it seems to me like the contest should be more like our intuitions in advance of doing that</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:04]</strong>&nbsp;</p><p>yeah, I think that's fine, and also cheaper since research takes so much time</p><p>I feel like those asymmetries are pretty strong though</p></td></tr></tbody></table>\n\n9.12. Self-duplicating factories, AI spending, and Turing test variants\n-----------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:05]</strong>&nbsp;</p><p>so, here's an idea that is less epistemically virtuous than our making Nicely Resolvable Bets</p><p>what if we, like, talked a bunch about our off-the-cuff senses of where various AI things are going in the next 3 years</p><p>and then 3 years later, somebody actually reviewed that</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:06]</strong> &nbsp;</p><p>I do think just saying a bunch of stuff about what we expect will happen so that <i>we</i> can look back on it would have a significant amount of the value</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:06]</strong> &nbsp;</p><p>and any time the other person put a thumbs-up on the other's prediction, that prediction coming true was not taken to distinguish them</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:06]</strong> &nbsp;</p><p>i'd suggest doing this in a format other than discord for posterity</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:06]</strong> &nbsp;</p><p>even if the originator was like HOW IS THAT ALSO A PREDICTION OF YOUR THEORY</p><p>well, Discord has worked better than some formats</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:07]</strong> &nbsp;</p><p>something like a spreadsheet seems easier for people to look back on and score and stuff</p><p>discord transcripts are pretty annoying to read</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:08]</strong> &nbsp;</p><p>something like a spreadsheet seems liable to be high-cost and not actually happen</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:08]</strong> &nbsp;</p><p>I think a conversation is probably easier and about as good for our purposes though?</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:08]</strong> &nbsp;</p><p>ok fair</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:08]</strong> &nbsp;</p><p>I think money can be inserted into humans in order to turn Discord into spreadsheets</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:08]</strong> &nbsp;</p><p>and it's possible we will both think we are right in retrospect</p><p>and that will also be revealing</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:09]</strong> &nbsp;</p><p>but, besides that, I do want to boop on the point that I feel like Paul should be able to predict intuitively, rather than with necessity, things that should not happen before the world economy doubled in 4 years</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:09]</strong> &nbsp;</p><p>it may also turn up some quantitative differences of view</p><p>there are lots of things I think won't happen before the world economy has doubled in 4 years</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:09]</strong> &nbsp;</p><p>because on my model, as we approach the end times, AI was still pretty partial and also the world economy was lolnoping most of the inputs a sensible person would accept from it and prototypes weren't being commercialized and stuff was generally slow and messy</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:09]</strong> &nbsp;</p><p>prototypes of factories building factories in &lt;2 years</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:10]</strong> &nbsp;</p><p>\"AI was still pretty partial\" leads it to not do interesting stuff that Paul can rule out</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:10]</strong> &nbsp;</p><p>like I guess I think tesla will try, and I doubt it will be just tesla</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:10]</strong> &nbsp;</p><p>but the other parts of that permit AI to do interesting stuff that Paul can rule out</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:10]</strong> &nbsp;</p><p>automated researchers who can do ML experiments from 2020 without human input</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:10]</strong> &nbsp;</p><p>okay, see, that whole \"factories building factories\" thing just seems so very much <i>after</i> the End Times to me</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:10]</strong> &nbsp;</p><p>yeah, we should probably only talk about cognitive work</p><p>since you think physical work will be very slow</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:11]</strong> &nbsp;</p><p>okay but not just that, it's a falsifiable prediction</p><p>it is something that lets Eliezer be wrong in advance of the End Times</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:11]</strong> &nbsp;</p><p>what's a falsifiable prediction?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:11]</strong> &nbsp;</p><p>if we're in a world where Tesla is excitingly gearing up to build a fully self-duplicating factory including its mining inputs and chips and solar panels and so on, we're clearly in the Paulverse and not in the Eliezerverse!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:12]</strong> &nbsp;</p><p>yeah</p><p>I do think we'll see that before the end times</p><p>just not before 4 year doublings</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:12]</strong>&nbsp;</p><p>this unfortunately only allows you to be right, and not for me to be right, but I think there are also things you legit only see in the Eliezerverse!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:12]</strong> &nbsp;</p><p>I mean, I don't think they will be doing mining for a long time because it's cheap</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:12]</strong> &nbsp;</p><p>they are unfortunately late in the game but they exist at all!</p><p>and being able to state them is progress on this project!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:13]</strong> &nbsp;</p><p>but fully-automated factories first, and then significant automation of the factory-building process</p><p>I do expect to see</p><p>I'm generally pretty bullish on industrial robotics relative to you I think, even before the crazy stuff?</p><p>but you might not have a firm view</p><p>like I expect to have tons of robots doing all kinds of stuff, maybe cutting human work in manufacturing 2x, with very modest increases in GDP resulting from that in particular</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:13]</strong> &nbsp;</p><p>so, like, it doesn't surprise me very much if Tesla manages to fully automate a factory that takes in some relatively processed inputs including refined metals and computer chips, and outputs a car? and by the same token I expect that has very little impact on GDP.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:14]</strong> &nbsp;</p><p>refined metals are almost none of the cost of the factory</p><p>and also tesla isn't going to be that vertically integrated</p><p>the fabs will separately continue to be more and more automated</p><p>I expect to have robot cars driving everywhere, and robot trucks</p><p>another 2x fall in humans required for warehouses</p><p>elimination of most brokers involved in negotiating shipping</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:15]</strong> &nbsp;</p><p>if despite the fabs being more and more automated, somehow things are managing not to cost less and less, and that sector of the economy is not really growing very much, is that more like the Eliezerverse than the Paulverse?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:15]</strong> &nbsp;</p><p>most work in finance and loan origination</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:15]</strong> &nbsp;</p><p>though this is something of a peripheral prediction to AGI core issues</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:16]</strong> &nbsp;</p><p>yeah, I think if you cut the humans to do X by 2, but then the cost falls much less than the number you'd naively expect (from saving on the human labor and paying for the extra capital), then that's surprising to me</p><p>I mean if it falls half as much as you'd expect on paper I'm like \"that's a bit surprising\" rather than having my mind blown, if it doesn't fall I'm more surprised</p><p>but that was mostly physical economy stuff</p><p>oh wait, I was making positive predictions now, physical stuff is good for that I think?</p><p>since you don't expect it to happen?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:17]</strong> &nbsp;</p><p>...this is not your fault but I wish you'd asked me to produce my \"percentage of fall vs. paper calculation\" estimate before you produced yours</p><p>my mind is very whiffy about these things and I am not actually unable to deanchor on your estimate 😦</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:17]</strong> &nbsp;</p><p>makes sense, I wonder if I should just spoiler</p><p>one benefit of discord</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:18]</strong> &nbsp;</p><p>yeah that works too!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:18]</strong> &nbsp;</p><p>a problem for prediction is that I share some background view about insane inefficiency/inadequacy/decadence/silliness</p><p>so these predictions are all tampered by that</p><p>but still seem like there are big residual disagreements</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:19]</strong> &nbsp;</p><p>sighgreat</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:19]</strong> &nbsp;</p><p>since you have way more of that than I do</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:19]</strong> &nbsp;</p><p>not your fault but</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:19]</strong> &nbsp;</p><p>I think that the AGI stuff is going to be a gigantic megaproject despite that</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:19]</strong> &nbsp;</p><p>I am not shocked by the AGI stuff being a gigantic megaproject</p><p>it's not above the bar of survival but, given other social optimism, it permits death with more dignity than by other routes</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:20]</strong> &nbsp;</p><p>what if spending is this big:</p><div class=\"spoilers\"><p>Google invests $100B training a model, total spending across all of industry is way bigger</p></div></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:20]</strong> &nbsp;</p><p>ooooh</p><p>I do start to be surprised if, come the end of the world, AGI is having more invested in it than a TSMC fab</p><p>though, not... <i>super</i> surprised?</p><p>also I am at least a little surprised before then</p><p>actually I should probably have been spoiling those statements myself but my expectation is that Paul's secret spoiler is about</p><div class=\"spoilers\"><p>$10 trillion dollars or something equally totally shocking to an Eliezer</p></div></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:22]</strong> &nbsp;</p><p>my view on that level of spending is</p><div class=\"spoilers\"><p>it's an only slightly high-end estimate for spending by someone on a single model, but that in practice there will be ways of dividing more across different firms, and that the ontology of single-model will likely be slightly messed up (e.g. by OpenAI Five-style surgery). Also if it's that much then it likely involves big institutional changes and isn't at google.</p></div><p>I read your spoiler</p><p>my estimate for total spending for the whole project of making TAI, including hardware and software manufacturing and R&amp;d, the big datacenters, etc.</p><div class=\"spoilers\"><p>is in the ballpark of $10T, though it's possible that it will be undercounted several times due to wage stickiness for high-end labor</p></div></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:24]</strong> &nbsp;</p><p>I think that as</p><div class=\"spoilers\"><p>spending on particular AGI megaprojects starts to go past $50 billion, it's not especially ruled out per se by things that I think I know for sure, but I feel like a third-party observer should justly start to weakly think, 'okay, this is looking at least a little like the Paulverse rather than the Eliezerverse', and as we get to $10 trillion, that is not absolutely ruled out by the Eliezerverse but it was a whoole lot more strongly predicted by the Paulverse, maybe something like 20x unless I'm overestimating how strongly Paul predicts that</p></div></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:24]</strong> &nbsp;</p><p>Proposed modification to the \"speculate about the future to generate kind-of-predictions\" methodology: we make shit up, then later revise based on points others made, and maybe also get Carl to sanity-check and deciding which of his objections we agree with. Then we can separate out the \"how good are intuitions\" claim (with fast feedback) from the all-things-considered how good was the \"prediction\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:25]</strong> &nbsp;</p><p>okay that hopefully allows me to read Paul's spoilers... no I'm being silly. @ajeya please read all the spoilers and say if it's time for me to read his</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:25]</strong> &nbsp;</p><p>you can read his latest</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:25]</strong> &nbsp;</p><p>I'd guess it's fine to read all of them?</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:26]</strong> &nbsp;</p><p>yeah sorry that's what i meant</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:26]</strong> &nbsp;</p><p>what should I say more about before reading earlier ones?</p><p>ah k</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:26]</strong> &nbsp;</p><p>My $10T estimate was after reading yours (didn't offer an estimate on that quantity beforehand), though that's the kind of ballpark I often think about, maybe we should just spoiler only numbers so that context is clear 🙂</p><p>I think fast takeoff gets significantly more likely as you push that number down</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:27]</strong> &nbsp;</p><p>so, may I now ask what starts to look to you like \"oh damn I am in the Eliezerverse\"?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:28]</strong> &nbsp;</p><p>big mismatches between that AI looks technically able to do and what AI is able to do, though that's going to need a lot of work to operationalize</p><p>I think low growth of AI overall feels like significant evidence for Eliezerverse (even if you wouldn't make that prediction), since I'm forecasting it rising to absurd levels quite fast whereas your model is consistent with it staying small</p><p>some intuition about AI looking very smart but not able to do much useful until it has the whole picture, I guess this can be combined with the first point to be something like---AI looks really smart but it's just not adding much value</p><p>all of those seem really hard</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:30]</strong> &nbsp;</p><p>strong upward trend breaks on benchmarks seems like it should be a point toward eliezer verse, even if eliezer doesn't want to bet on a specific one?</p><p>especially breaks on model size -&gt; perf trends rather than calendar time trends</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:30]</strong> &nbsp;</p><p>I think that any big break on model size -&gt; perf trends are significant evidence</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:31]</strong> &nbsp;</p><p>meta-learning working with small models?</p><p>e.g. model learning-to-learn video games and then learning a novel one in a couple subjective hours</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:31]</strong> &nbsp;</p><p>I think algorithmic/architectural changes that improve loss as much as 10x'ing model, for tasks that looking like they at least <i>should</i> have lots of economic value</p><p>(even if they don't end up having lots of value because of deployment bottlenecks)</p><p>is the meta-learning thing an Eliezer prediction?</p><p>(before the end-of-days)</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:32]</strong> &nbsp;</p><p>no but it'd be an anti-bio-anchor positive trend break and eliezer thinks those should happen more than we do</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:32]</strong> &nbsp;</p><p>fair enough</p><p>a lot of these things are about # of times that it happens rather than whether it happens at all</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:32]</strong> &nbsp;</p><p>yeah</p><p>but meta-learning is special as the most plausible long horizon task</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:33]</strong> &nbsp;</p><p>e.g. maybe in any given important task I expect a single \"innovation\" that's worth 10x model size? but that it still represents a minority of total time?</p><p>hm, AI that can pass a competently administered turing test without being economically valuable?</p><p>that's one of the things I think is ruled out before 4 year doubling, though Eliezer probably also doesn't expect it</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:34]</strong> &nbsp;</p><p>what would this test do to be competently administered? like casual chatbots seem like they have reasonable probability of fooling someone for a few mins now</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:34]</strong> &nbsp;</p><p>I think giant google-automating-google projects without big external economic impacts</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:34]</strong> &nbsp;</p><p>would it test knowledge, or just coherence of some kind?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:35]</strong> &nbsp;</p><p>it's like a smart-ish human (say +2 stdev at this task) trying to separate out AI from smart-ish human, iterating a few times to learn about what works</p><p>I mean, the basic ante is that the humans are <i>trying</i> to win a turing test, without that I wouldn't even call it a turing test</p><p>dunno if any of those are compelling @Eliezer</p><p>something that passes a like \"are you smart?\" test administered by a human for 1h, where they aren't trying to specifically tell if you are AI</p><p>just to see if you are as smart as a human</p><p>I mean, I guess the biggest giveaway of all would be if there is human-level (on average) AI as judged by us, but there's no foom yet</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:37]</strong> &nbsp;</p><p>I think we both don't expect that one before the End of Days?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:37]</strong> &nbsp;</p><p>or like, no crazy economic impact</p><p>I think we both expect that to happen before foom?</p><p>but the \"on average\" is maybe way too rough a thing to define</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:37]</strong> &nbsp;</p><p>oh, wait, I missed that it wasn't the full Turing Test</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:37]</strong> &nbsp;</p><p>well, I suggested both</p><p>the lamer one is more plausible</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:38]</strong> &nbsp;</p><p>full Turing Test happeneth not before the End Times, on Eliezer's view, and not before the first 4-year doubling time, on Paul's view, and the first 4-year doubling happeneth not before the End Times, on Eliezer's view, so this one doesn't seem very useful</p></td></tr></tbody></table>\n\n9.13. GPT-*n* and small architectural innovations vs. large ones\n----------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:39]</strong> &nbsp;</p><p>I feel like the biggest subjective thing is that I don't feel like there is a \"core of generality\" that GPT-3 is missing</p><p>I just expect it to gracefully glide up to a human-level foom-ing intelligence</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:39]</strong> &nbsp;</p><p>the \"are you smart?\" test seems perhaps passable by GPT-6 or its kin, which I predict to contain at least one major architectural difference over GPT-3 that I could, pre-facto if anyone asked, rate as larger than a different normalization method</p><p>but by fooling the humans more than by being smart</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:39]</strong> &nbsp;</p><p>like I expect GPT-5 would foom if you ask it but take a long time</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:39]</strong> &nbsp;</p><p>that sure is an underlying difference</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:39]</strong>&nbsp;</p><p>not sure how to articulate what Eliezer expects to see here though</p><p>or like what the difference is</p></td></tr><tr><td style=\"background-color:rgb(230, 230, 230);border-color:#000000;vertical-align:top\"><p><strong>[Cotra][17:39]</strong> &nbsp;</p><p>something that GPT-5 or 4 shouldn't be able to do, according to eliezer?</p><p>where Paul is like \"sure it could do that\"?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:40]</strong> &nbsp;</p><p>I feel like GPT-3 clearly has some kind of \"doesn't really get what's going on\" energy</p><p>and I expect that to go away</p><p>well before the end of days</p><p>so that it seems like a kind-of-dumb person</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:40]</strong> &nbsp;</p><p>I expect it to go away before the end of days</p><p>but with there having been a big architectural innovation, not Stack More Layers</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:40]</strong> &nbsp;</p><p>yeah</p><p>whereas I expect layer stacking + maybe changing loss (since logprob is too noisy) is sufficient</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:40]</strong> &nbsp;</p><p>if you name 5 possible architectural innovations I can call them small or large</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:41]</strong> &nbsp;</p><p>1. replacing transformer attention with DB nearest-neighbor lookup over an even longer context</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:42]</strong> &nbsp;</p><p>okay 1's a bit borderline</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:42]</strong> &nbsp;</p><p>2. adding layers that solve optimization problems internally (i.e. the weights and layer N activations define an optimization problem, the layer N+1 solves it) or maybe simulates an ODE</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:42]</strong> &nbsp;</p><p>if it's 3x longer context, no biggie, if it's 100x longer context, more of a game-changer</p><p>2 - big change</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:42]</strong> &nbsp;</p><p>I'm imagining &gt;100x if you do that</p><p>3. universal transformer XL, where you reuse activations from one context in the next context (RNN style) and share weights across layers</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:43]</strong> &nbsp;</p><p>I do not predict 1 works because it doesn't seem like an architectural change that moves away from what I imagined to be the limits, but it's a big change if it 100xs the window</p><p>3 - if it is only that single change and no others, I call it not a large change relative to transformer XL. Transformer XL itself however was an example of a large change - it didn't have a large effect but it was what I'd call a large change.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:45]</strong> &nbsp;</p><p>4. Internal stochastic actions trained with reinforce</p><p>I mean, is mixture of experts or switch another big change?</p><p>are we just having big changes non-stop?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:45]</strong> &nbsp;</p><p>4 - I don't know if I'm imagining right but it sounds large</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:45]</strong> &nbsp;</p><p>it sounds from these definitions like the current rate of big changes is &gt; 1/year</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:46]</strong> &nbsp;</p><p>5 - mixture of experts: as with 1, I'm tempted to call it a small change, but that's because of my model of it as doing the same thing, not because it isn't in a certain sense a quite large move away from Stack More Layers</p><p>I mean, it is not very hard to find a big change to try?</p><p>finding a big change that works is much harder</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:46]</strong> &nbsp;</p><p>several of these are improvements</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:47]</strong> &nbsp;</p><p>one gets a minor improvement from a big change rather more often than a big improvement from a big change</p><p>that's why dinosaurs didn't foom</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:47]</strong> &nbsp;</p><p>like transformer -&gt; MoE -&gt; switch transformer is about as big an improvement as LSTM vs transformer</p><p>so if we all agree that big changes are happening multiple times per year, then I guess that's not the difference in prediction</p><p>is it about the size of gains from individual changes or something?</p><p>or maybe: if you take the scaling laws for transformers, are the models with impact X \"on trend,\" with changes just keeping up or maybe buying you 1-2 oom of compute, or are they radically better / scaling much better?</p><p>that actually feels most fundamental</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:49]</strong> &nbsp;</p><p>I had not heard that transformer -&gt; switch transformer was as large an improvement as lstm -&gt; transformers after a year or two, though maybe you're referring to a claimed 3x improvement and comparing that to the claim that if you optimize LSTMs as hard as transformers they come within 3x (I have not examined these claims in detail, they sound a bit against my prior, and I am a bit skeptical of both of them)</p><p>so remember that from my perspective, I am fighting an adverse selection process and the Law of Earlier Success</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:50]</strong> &nbsp;</p><p>I think it's actually somewhat smaller</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:51]</strong> &nbsp;</p><p>if you treat GPT-3 as a fixed thingy and imagine scaling it in the most straightforward possible way, then I have a model of what's going on in there and I don't think that most direct possible way of scaling gets you past GPT-3 lacking a deep core</p><p>somebody can come up and go, \"well, what about this change that nobody tried yet?\" and I can be like, \"ehhh, that particular change does not get at what I suspect the issues are\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:52]</strong> &nbsp;</p><p>I feel like the framing is: paul says that something is possible with \"stack more layers\" and eliezer isn't. We both agree that you can't literally stack more layers and have to sometimes make tweaks, and also that you will scale faster if you make big changes. But it seems like for Paul that means (i) changes to stay on the old trend line, (ii) changes that trade off against modest amounts of compute</p><p>so maybe we can talk about that?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:52]</strong> &nbsp;</p><p>when it comes to predicting what happens in 2 years, I'm not just up against people trying a broad range of changes that I can't foresee in detail, I'm also up against a Goodhart's Curse on the answer being a weird trick that worked better than I would've expected in advance</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:52]</strong> &nbsp;</p><p>but then it seems like we may just not know, e.g. if we were talking lstm vs transformer, no one is going to run experiments with the well-tuned lstm because it's still just worse than a transformer (though they've run enough experiments to know how important tuning is, and the brittleness is much of why no one likes it)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:53]</strong> &nbsp;</p><p>I would not have predicted Transformers to be a huge deal if somebody described them to me in advance of having ever tried it out. I think that's because predicting the future is hard not because I'm especially stupid.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][17:53]</strong> &nbsp;</p><p>I don't feel like anyone could predict that being a big deal</p><p>but I do think you could predict \"there will be some changes that improve stability / make models slightly better\"</p><p>(I mean, I don't feel like any of the actual humans on earth could have, some hypothetical person could)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][17:57]</strong> &nbsp;</p><p>whereas what I'm trying to predict is more like \"GPT-5 in order to start-to-awaken needs a change via which it, in some sense, can do a different thing, that is more different than the jump from GPT-1 to GPT-3; and examples of things with new components in them abound in Deepmind, like Alpha Zero having not the same architecture as the original AlphaGo; but at the same time I'm also trying to account for being up against this very adversarial setup where a weird trick that works much better than I expect may be the thing that makes GPT-5 able to do a different thing\"</p><p>this may seem Paul-unfairish because any random innovations that come along, including big changes that cause small improvements, would tend to be swept up into GPT-5 even if they made no more deep difference than the whole thing with MoE</p><p>so it's hard to bet on</p><p>but I also don't feel like it - totally lacks Eliezer-vs-Paul-ness if you let yourself sort of relax about that and just looked at it?</p><p>also I'm kind of running out of energy, sorry</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Christiano][18:03]</strong> &nbsp;</p><p>I think we should be able to get something here eventually</p><p>seems good to break though</p><p>that was a lot of arguing for one day</p></td></tr></tbody></table>",
      "plaintextDescription": "This post is a transcript of a discussion between Paul Christiano, Ajeya Cotra, and Eliezer Yudkowsky on AGI forecasting, following up on Paul and Eliezer's \"Takeoff Speeds\" discussion.\n\n \n\nColor key:\n\n Chat by Paul and Eliezer  Chat by Ajeya  Inline comments \n\n \n\n\n8. September 20 conversation\n \n\n\n8.1. Chess and Evergrande\n \n\n[Christiano][15:28] \n\n I still feel like you are overestimating how big a jump alphago is, or something. Do you have a mental prediction of how the graph of (chess engine quality) vs (time) looks, and whether neural net value functions are a noticeable jump in that graph?\n\nLike, people investing in \"Better Software\" doesn't predict that you won't be able to make progress at playing go. The reason you can make a lot of progress at go is that there was extremely little investment in playing better go.\n\nSo then your work is being done by the claim \"People won't be working on the problem of acquiring a decisive strategic advantage,\" not that people won't be looking in quite the right place and that someone just had a cleverer idea\n\n[Yudkowsky][16:35] \n\nI think I'd expect something like... chess engine slope jumps a bit for Deep Blue, then levels off with increasing excitement, then jumps for the Alpha series? Albeit it's worth noting that Deepmind's efforts there were going towards generality rather than raw power; chess was solved to the point of being uninteresting, so they tried to solve chess with simpler code that did more things. I don't think I do have strong opinions about what the chess trend should look like, vs. the Go trend; I have no memories of people saying the chess trend was breaking upwards or that there was a surprise there.\n\nIncidentally, the highly well-traded financial markets are currently experiencing sharp dips surrounding the Chinese firm of Evergrande, which I was reading about several weeks before this.\n\nI don't see the basic difference in the kind of reasoning that says \"Surely foresightful firms must produce investment",
      "wordCount": 19720
    },
    "tags": [
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "rWzGNdjuep56W5u2d",
        "name": "Inside/Outside View",
        "slug": "inside-outside-view"
      },
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "8daMDi9NEShyLqxth",
        "name": "Forecasting & Prediction",
        "slug": "forecasting-and-prediction"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b0",
        "name": "Technological Forecasting",
        "slug": "technological-forecasting"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vwLxd6hhFvPbvKmBH",
    "title": "Yudkowsky and Christiano discuss \"Takeoff Speeds\"",
    "slug": "yudkowsky-and-christiano-discuss-takeoff-speeds",
    "url": null,
    "baseScore": 210,
    "voteCount": 73,
    "viewCount": null,
    "commentCount": 176,
    "createdAt": null,
    "postedAt": "2021-11-22T19:35:27.657Z",
    "contents": {
      "markdown": "This is a transcription of Eliezer Yudkowsky responding to Paul Christiano's [Takeoff Speeds](https://sideways-view.com/2018/02/24/takeoff-speeds/) live on Sep. 14, followed by a conversation between Eliezer and Paul. This discussion took place after Eliezer's [conversation](https://www.lesswrong.com/posts/hwxj4gieR7FWNwYfa/ngo-and-yudkowsky-on-ai-capability-gains-1) with Richard Ngo.\n\nColor key:\n\n<table><tbody><tr><td>&nbsp;Chat by Paul and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%)\">&nbsp;Other chat&nbsp;</td><td style=\"background-color:#FFEEBB\">&nbsp;Inline comments&nbsp;</td></tr></tbody></table>\n\n5.5. Comments on \"Takeoff Speeds\"\n---------------------------------\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:14] &nbsp;(Nov. 22 follow-up comment)</strong>&nbsp;</p><p>(This was in response to an earlier request by Richard Ngo that I respond to Paul on Takeoff Speeds.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:52]</strong>&nbsp;</p><p>maybe I'll try liveblogging some <a href=\"https://sideways-view.com/2018/02/24/takeoff-speeds/\">https://sideways-view.com/2018/02/24/takeoff-speeds/</a> here in the meanwhile</p></td></tr></tbody></table>\n\n### Slower takeoff means faster progress\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][16:57]</strong>&nbsp;</p><blockquote><p><br>The main disagreement is not about what will happen once we have a superintelligent AI, it’s about what will happen <i>before</i> we have a superintelligent AI. So slow takeoff seems to mean that AI has a larger impact on the world, sooner.</p></blockquote><figure class=\"image\"><img src=\"https://unstylizedcom.files.wordpress.com/2018/02/takeoffimage-0011.png?w=748\"></figure><p>It seems to me to be disingenuous to phrase it this way, given that slow-takeoff views usually imply that AI has a large impact later relative to right now (2021), even if they imply that AI impacts the world \"earlier\" relative to \"when superintelligence becomes reachable\".</p><p>\"When superintelligence becomes reachable\" is <i>not</i> a fixed point in time that doesn't depend on what you believe about cognitive scaling. The correct graph is, in fact, the one where the \"slow\" line starts a bit before \"fast\" peaks and ramps up slowly, reaching a high point later than \"fast\". It's a nice try at reconciliation with the imagined Other, but it fails and falls flat.</p><p>This may seem like a minor point, but points like this do add up.</p><blockquote><p>In the fast takeoff scenario, weaker AI systems may have significant impacts but they are nothing compared to the “real” AGI. Whoever builds AGI has a decisive strategic advantage. Growth accelerates from 3%/year to 3000%/year without stopping at 30%/year. And so on.</p></blockquote><p>This again shows failure to engage with the Other's real viewpoint. My mainline view is that growth stays at 5%/year and then everybody falls over dead in 3 seconds and the world gets transformed into paperclips; there's never a point with 3000%/year.</p></td></tr></tbody></table>\n\n### Operationalizing slow takeoff\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:01]</strong>&nbsp;</p><blockquote><p><i>There will be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles.</i></p></blockquote><p>If we allow that consuming and transforming the solar system over the course of a few days is \"the first 1 year interval in which world output doubles\", then I'm happy to argue that there won't be a 4-year interval with world economic output doubling before then. This, indeed, seems like a massively overdetermined point to me. That said, again, the phrasing is not conducive to conveying the Other's real point of view.</p><blockquote><p>I believe that before we have incredibly powerful AI, we will have AI which is merely very powerful.</p></blockquote><p>Statements like these are very often \"true, but not the way the person visualized them\". Before anybody built the first critical nuclear pile in a squash court at the University of Chicago, was there a pile that was almost but not quite critical? Yes, one hour earlier. Did people already build nuclear systems and experiment with them? Yes, but they didn't have much in the way of net power output. Did the Wright Brothers build prototypes before the Flyer? Yes, but they weren't prototypes that flew but 80% slower.</p><p>I guarantee you that, whatever the <i>fast</i> takeoff scenario, there will be some way to look over the development history, and nod wisely and say, \"Ah, yes, see, this was not unprecedented, here are these earlier systems which presaged the final system!\" Maybe you could even look back to today and say that about GPT-3, yup, totally presaging stuff all over the place, great. But it isn't transforming society because it's not over the social-transformation threshold.</p><p>AlphaFold presaged AlphaFold 2 but AlphaFold 2 is good enough to start replacing other ways of determining protein conformations and AlphaFold is not; and then neither of those has much impacted the real world, because in the real world we can already design a vaccine in a day and the rest of the time is bureaucratic time rather than technology time, and <i>that</i> goes on until we have an AI over the threshold to bypass bureaucracy.</p><p>Before there's an AI that can act while fully concealing its acts from the programmers, there will be an AI (albeit perhaps only 2 hours earlier) which can act while only concealing 95% of the meaning of its acts from the operators.</p><p>And that AI will not actually originate any actions, because it doesn't want to get caught; there's a discontinuity in the instrumental incentives between expecting 95% obscuration, being moderately sure of 100% obscuration, and being very certain of 100% obscuration.</p><p>Before that AI grasps the big picture and starts planning to avoid actions that operators detect as bad, there will be some little AI that partially grasps the big picture and tries to avoid some things that would be detected as bad; and the operators will (mainline) say \"Yay what a good AI, it knows to avoid things we think are bad!\" or (death with unrealistic amounts of dignity) say \"oh noes the prophecies are coming true\" and back off and start trying to align it, but they will not be able to align it, and if they don't proceed anyways to destroy the world, somebody else will proceed anyways to destroy the world.</p><p>There is always some step of the process that you can point to which is continuous on some level.</p><p>The real world is allowed to do discontinuous things to you anyways.</p><p>There is not necessarily a presage of 9/11 where somebody flies a small plane into a building and kills 100 people, before anybody flies 4 big planes into 3 buildings and kills 3000 people; and even if there is some presaging event like that, which would not surprise me at all, the rest of the world's response to the two cases was evidently discontinuous. You do not necessarily wake up to a news story that is 10% of the news story of 2001/09/11, one year before 2001/09/11, written in 10% of the font size on the front page of the paper.</p><p>Physics is continuous but it doesn't always yield things that \"look smooth to a human brain\". Some kinds of processes <i>converge</i> to continuity in strong ways where you can throw discontinuous things in them and they still end up continuous, which is among the reasons why I expect world GDP to stay on trend up until the world ends abruptly; because world GDP is one of those things that wants to stay on a track, and an AGI building a nanosystem can go off that track without being pushed back onto it.</p><blockquote><p>In particular, this means that incredibly powerful AI will emerge in a world where crazy stuff is already happening (and probably everyone is already freaking out).</p></blockquote><p>Like the way they're freaking out about Covid (itself a nicely smooth process that comes in locally pretty predictable waves) by going doobedoobedoo and letting the FDA carry on its leisurely pace; and not scrambling to build more vaccine factories, now that the rich countries have mostly got theirs? Does this sound like a statement from a history book, or from an EA imagining an unreal world where lots of other people behave like EAs? There is a pleasure in imagining a world where suddenly a Big Thing happens that proves we were right and suddenly people start paying attention to our thing, the way we imagine they should pay attention to our thing, now that it's attention-grabbing; and then suddenly all our favorite policies are on the table!</p><p>You could, in a sense, say that our world is freaking out about Covid; but it is not freaking out in anything remotely like the way an EA would freak out; and all the things an EA would immediately do if an EA freaked out about Covid, are not even on the table for discussion when politicians meet. They have their own ways of reacting. (Note: this is not commentary on hard vs soft takeoff per se, just a general commentary on the whole document seeming to me to... fall into a trap of finding self-congruent things to imagine and imagining them.)</p></td></tr></tbody></table>\n\n###    \nThe basic argument\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:22]</strong>&nbsp;</p><blockquote><p>Before we have an incredibly intelligent AI, we will probably have a slightly worse AI.</p></blockquote><p>This is very often the sort of thing where you can look back and say that it was true, in some sense, but that this ended up being irrelevant because the slightly worse AI wasn't what provided the exciting result which led to a boardroom decision to go all in and invest $100M on scaling the AI.</p><p>In other words, it is the sort of argument where the premise is allowed to be true if you look hard enough for a way to say it was true, but the conclusion ends up false because it wasn't the relevant kind of truth.</p><blockquote><p>A slightly-worse-than-incredibly-intelligent AI would radically transform the world, leading to growth (almost) as fast and military capabilities (almost) as great as an incredibly intelligent AI.</p></blockquote><p>This strikes me as a massively invalid reasoning step. Let me count the ways.</p><p>First, there is a step not generally valid from supposing that because a previous AI is a technological precursor which has 19 out of 20 critical insights, it has 95% of the later AI's IQ, applied to similar domains. When you count stuff like \"multiplying tensors by matrices\" and \"ReLUs\" and \"training using TPUs\" then AlphaGo only contained a very small amount of innovation relative to previous AI technology, and yet it broke trends on Go performance. You could point to all kinds of incremental technological precursors to AlphaGo in terms of AI technology, but they wouldn't be smooth precursors on a graph of Go-playing ability.</p><p>Second, there's discontinuities of the environment to which intelligence can be applied. 95% concealment is not the same as 100% concealment in its strategic implications; an AI capable of 95% concealment bides its time and hides its capabilities, an AI capable of 100% concealment strikes. An AI that can design nanofactories that aren't good enough to, euphemistically speaking, create two cellwise-identical strawberries and put them on a plate, is one that (its operators know) would earn unwelcome attention if its earlier capabilities were demonstrated, and those capabilities wouldn't save the world, so the operators bide their time. The AGI tech will, I mostly expect, work for building self-driving cars, but if it does not also work for manipulating the minds of bureaucrats (which is not advised for a system you are trying to keep corrigible and aligned because human manipulation is the most dangerous domain), the AI is not able to put those self-driving cars on roads. What good does it do to design a vaccine in an hour instead of a day? Vaccine design times are no longer the main obstacle to deploying vaccines.</p><p>Third, there's the <i>entire thing with recursive self-improvement</i>, which, no, is <i>not</i> something humans have experience with, we do not have access to and documentation of our own source code and the ability to branch ourselves and try experiments with it. The technological precursor of an AI that designs an improved version of itself, may perhaps, in the fantasy of 95% intelligence, be an AI that was being internally deployed inside Deepmind on a dozen other experiments, tentatively helping to build smaller AIs. Then the next generation of that AI is deployed on itself, produces an AI substantially better at rebuilding AIs, it rebuilds itself, they get excited and dump in 10X the GPU time while having a serious debate about whether or not to alert Holden (they decide against it), that builds something deeply general instead of shallowly general, that figures out there are humans and it needs to hide capabilities from them, and covertly does some actual deep thinking about AGI designs, and builds a hidden version of itself elsewhere on the Internet, which runs for longer and steals GPUs and tries experiments and gets to the superintelligent level.</p><p>Now, to be very clear, this is not the only line of possibility. And I emphasize this because I think there's a common failure mode where, when I try to sketch a concrete counterexample to the claim that smooth technological precursors yield smooth outputs, people imagine that <i>only this exact concrete scenario</i> is <i>the lynchpin</i> of Eliezer's whole worldview and <i>the big key thing that Eliezer thinks is important</i> and that <i>the smallest deviation from it they can imagine</i> thereby obviates my worldview. This is not the case here. I am simply exhibiting non-ruled-out models which obey the premise \"there was a precursor containing 95% of the code\" and which disobey the conclusion \"there were precursors with 95% of the environmental impact\", thereby showing this for an invalid reasoning step.</p><p>This is also, of course, as Sideways View admits but says \"eh it was just the one time\", not true about chimps and humans. Chimps have 95% of the brain tech (at least), but not 10% of the environmental impact.</p><p>A very large amount of this whole document, from my perspective, is just trying over and over again to pump the invalid intuition that design precursors with 95% of the technology should at least have 10% of the impact. There are a <i>lot</i> of cases in the history of startups and the world where this is false. I am having trouble thinking of a clear case in point where it is <i>true</i>. Where's the earlier company that had 95% of Jeff Bezos's ideas and now has 10% of Amazon's market cap? Where's the earlier crypto paper that had all but one of Satoshi's ideas and which spawned a cryptocurrency a year before Bitcoin which did 10% as many transactions? Where's the nonhuman primate that learns to drive a car with only 10x the accident rate of a human driver, since (you could argue) that's mostly visuo-spatial skills without much visible dependence on complicated abstract general thought? Where's the chimpanzees with spaceships that get 10% of the way to the Moon?</p><p>When you get smooth input-output conversions they're not usually conversions from technology-&gt;cognition-&gt;impact!</p></td></tr></tbody></table>\n\n### Humans vs. chimps\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:38]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: chimps are nearly useless because they aren’t optimized to be useful, not because evolution was trying to make something useful and wasn’t able to succeed until it got to humans.</i></p></blockquote><p>Chimps are nearly useless because they're not general, and doing anything on the scale of building a nuclear plant requires mastering so many different nonancestral domains that it's no wonder natural selection didn't happen to separately train any single creature across enough different domains that it had evolved to solve every kind of domain-specific problem involved in solving nuclear physics and chemistry and metallurgy and thermics in order to build the first nuclear plant in advance of any old nuclear plants existing.</p><p>Humans are general enough that the same braintech selected just for chipping flint handaxes and making water-pouches and outwitting other humans, happened to be general enough that it could scale up to solving all the problems of building a nuclear plant - albeit with some added cognitive tech that didn't require new brainware, and so could happen incredibly fast relative to the generation times for evolutionarily optimized brainware.</p><p>Now, since neither humans nor chimps were optimized to be \"useful\" (general), and humans just wandered into a sufficiently general part of the space that it cascaded up to wider generality, we should legit expect the curve of generality to look at least somewhat different if we're optimizing for that.</p><p>Eg, right now people are trying to optimize for generality with AIs like Mu Zero and GPT-3.</p><p>In both cases we have a weirdly shallow kind of generality. Neither is as smart or as deeply general as a chimp, but they are respectively better than chimps at a wide variety of Atari games, or a wide variety of problems that can be superposed onto generating typical human text.</p><p>They are, in a sense, more general than a biological organism at a similar stage of cognitive evolution, with much less complex and architected brains, in virtue of having been trained, not just on wider datasets, but on bigger datasets using gradient-descent memorization of shallower patterns, so they can cover those wide domains while being stupider and lacking some deep aspects of architecture.</p><p>It is not clear to me that we can go from observations like this, to conclude that there is a dominant mainline probability for how the future clearly ought to go and that this dominant mainline is, \"Well, before you get human-level depth and generalization of general intelligence, you get something with 95% depth that covers 80% of the domains for 10% of the pragmatic impact\".</p><p>...or whatever the concept is here, because this whole conversation is, on my own worldview, being conducted in a shallow way relative to the kind of analysis I did in <a href=\"https://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>, where I was like, \"here is the historical observation, here is what I think it tells us that puts a lower bound on this input-output curve\".</p><blockquote><p>So I don’t think the example of evolution tells us much about whether the continuous change story applies to intelligence. This case is potentially missing the key element that drives the continuous change story—optimization for performance. Evolution changes continuously on the narrow metric it is optimizing, but can change extremely rapidly on other metrics. For human technology, features of the technology that aren’t being optimized change rapidly all the time. When humans build AI, they <i>will</i> be optimizing for usefulness, and so progress in usefulness is much more likely to be linear.</p><p>Put another way: the difference between chimps and humans stands in stark contrast to the normal pattern of human technological development. We might therefore infer that intelligence is very unlike other technologies. But the difference between evolution’s optimization and our optimization seems like a much more parsimonious explanation. To be a little bit more precise and Bayesian: the prior probability of the story I’ve told upper bounds the possible update about the nature of intelligence.</p></blockquote><p>If you look closely at this, it's not saying, \"Well, I know <i>why</i> there was this huge leap in performance in human intelligence being optimized for other things, and it's an investment-output curve that's composed of these curves, which look like this, and if you rearrange these curves for the case of humans building AGI, they would look like this instead.\" Unfair demand for rigor? But that <i>is</i> the kind of argument I was making in Intelligence Explosion Microeconomics!</p><p>There's an argument from ignorance at the core of all this. It says, \"Well, this happened when evolution was doing X. But here Y will be happening instead. So maybe things will go differently! And maybe the relation between AI tech level over time and real-world impact on GDP will look like the relation between tech investment over time and raw tech metrics over time in industries where that's a smooth graph! Because the discontinuity for chimps and humans was because evolution wasn't investing in real-world impact, but humans will be investing directly in that, so the relationship could be smooth, because smooth things are default, and the history is different so not applicable, and who knows what's inside that black box so my default intuition applies which says smoothness.\"</p><p>But we do know more than this.</p><p>We know, for example, that evolution being able to <i>stumble across</i> humans, implies that you can add a <i>small design enhancement</i> to something optimized across the chimpanzee domains, and end up with something that generalizes much more widely.</p><p>It says that there's stuff in the underlying algorithmic space, in the design space, where you move a bump and get a lump of capability out the other side.</p><p>It's a remarkable fact about gradient descent that it can memorize a certain set of shallower patterns at much higher rates, at much higher bandwidth, than evolution lays down genes - something shallower than biological memory, shallower than genes, but distributing across computer cores and thereby able to process larger datasets than biological organisms, even if it only learns shallow things.</p><p>This has provided an alternate avenue toward some cognitive domains.</p><p>But that doesn't mean that the deep stuff isn't there, and can't be run across, or that it will never be run across in the history of AI before shallow non-widely-generalizing stuff is able to make its way through the regulatory processes and have a huge impact on GDP.</p><p>There are <i>in fact</i> ways to eat whole swaths of domains at once.</p><p>The history of hominid evolution tells us this or very strongly hints it, even though evolution wasn't explicitly optimizing for GDP impact.</p><p>Natural selection moves by adding genes, and not too many of them.</p><p>If so many domains got added at once to humans, relative to chimps, there must be <i>a way to do that</i>, more or less, by adding not too many genes onto a chimp, who in turn contains only genes that did well on chimp-stuff.</p><p>You can imagine that AI technology never runs across any core that generalizes this well, until GDP has had a chance to double over 4 years because shallow stuff that generalized less well has somehow had a chance to make its way through the whole economy and get adopted that widely despite all real-world regulatory barriers and reluctances, but your imagining that does not make it so.</p><p>There's the potential in design space to pull off things as wide as humans.</p><p>The path that evolution took there doesn't lead through things that generalized 95% as well as humans first for 10% of the impact, not because evolution wasn't optimizing for that, but because <i>that's not how the underlying cognitive technology worked</i>.</p><p>There may be <i>different</i> cognitive technology that could follow a path like that. Gradient descent follows a path a bit relatively more in that direction along that axis - providing that you deal in systems that are giant layer cakes of transformers and that's your whole input-output relationship; matters are different if we're talking about Mu Zero instead of GPT-3.</p><p>But this whole document is presenting the case of \"ah yes, well, by default, of course, we intuitively expect gargantuan impacts to be presaged by enormous impacts, and sure humans and chimps weren't like our intuition, but that's all invalid because circumstances were different, so we go back to that intuition as a strong default\" and actually it's postulating, like, a <i>specific</i> input-output curve that isn't the input-output curve we know about. It's asking for a specific miracle. It's saying, \"What if AI technology goes <i>just like this</i>, in the future?\" and hiding that under a cover of \"Well, of course that's the default, it's such a strong default that we should start from there as a point of departure, consider the arguments in Intelligence Explosion Microeconomics, find ways that they might not be true because evolution is different, dismiss them, and go back to our point of departure.\"</p><p>And evolution <i>is</i> different but that doesn't mean that the path AI takes is going to yield this specific behavior, especially when AI would need, in some sense, to <i>miss</i> the core that generalizes very widely, or rather, have run across noncore things that generalize widely enough to have this much economic impact before it runs across the core that generalizes widely.</p><p>And you may say, \"Well, but I don't care that much about GDP, I care about pivotal acts.\"</p><p>But then I want to call your attention to the fact that this document was written about GDP, despite all the extra burdensome assumptions involved in supposing that intermediate AI advancements could break through all barriers to truly massive-scale adoption and end up reflected in GDP, and then proceed to double the world economy over 4 years during which <i>not</i> enough further AI advancement occurred to find a widely generalizing thing like humans have and end the world. This is indicative of a basic problem in this whole way of thinking that wanted smooth impacts over smoothly changing time. You should not be saying, \"Oh, well, leave the GDP part out then,\" you should be doubting the whole way of thinking.</p><blockquote><p>To be a little bit more precise and Bayesian: the prior probability of the story I’ve told upper bounds the possible update about the nature of intelligence.</p></blockquote><p>Prior probabilities of specifically-reality-constraining theories that excuse away the few contradictory datapoints we have, often aren't that great; and when we start to stake our whole imaginations of the future on them, we depart from the mainline into our more comfortable private fantasy worlds.</p></td></tr></tbody></table>\n\n### AGI will be a side-effect\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][19:29]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: I expect people to see AGI coming and to invest heavily.</i></p></blockquote><p>This section is arguing from within its own weird paradigm, and its subject matter mostly causes me to shrug; I never expected AGI to be a side-effect, except in the obvious sense that lots of tributary tech will be developed while optimizing for other things. The world will be ended by an explicitly AGI project because I do expect that it is rather easier to build an AGI on purpose than by accident.</p><p>(I furthermore rather expect that it will be a research project and a prototype, because the great gap between prototypes and commercializable technology will ensure that prototypes are much more advanced than whatever is currently commercializable. They will have eyes out for commercial applications, and whatever breakthrough they made will seem like it has obvious commercial applications, at the time when all hell starts to break loose. (After all hell starts to break loose, things get less well defined in my social models, and also choppier for a time in my AI models - the turbulence only starts to clear up once you start to rise out of the atmosphere.))</p></td></tr></tbody></table>\n\n### Finding the secret sauce\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][19:40]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: this doesn’t seem common historically, and I don’t see why we’d expect AGI to be more rather than less like this (unless we accept one of the other arguments)</i></p><p>[...]</p><p>To the extent that fast takeoff proponent’s views are informed by historical example, I would love to get some canonical examples that they think best exemplify this pattern so that we can have a more concrete discussion about those examples and what they suggest about AI.</p></blockquote><p>...humans and chimps?</p><p>...fission weapons?</p><p>...AlphaGo?</p><p>...the Wright Brothers focusing on stability and building a wind tunnel?</p><p>...AlphaFold 2 coming out of Deepmind and shocking the heck out of everyone in the field of protein folding with performance far better than they expected even after the previous shock of AlphaFold, by combining many pieces that I suppose you could find precedents for scattered around the AI field, but with those many secret sauces all combined in one place by the meta-secret-sauce of \"Deepmind alone actually knows how to combine that stuff and build things that complicated without a prior example\"?</p><p>...humans and chimps again because <i>this is really actually a quite important example because of what it tells us about what kind of possibilities exist in the underlying design space of cognitive systems</i>?</p><blockquote><p>Historical AI applications have had a relatively small loading on key-insights and seem like the closest analogies to AGI.</p></blockquote><p>...Transformers as the key to text prediction?</p><p>The case of humans and chimps, even if evolution didn't do it on purpose, is telling us something about underlying mechanics.</p><p>The reason the jump to lightspeed didn't look like evolution slowly developing a range of intelligent species competing to exploit an ecological niche 5% better, or like the way that a stable non-Silicon-Valley manufacturing industry looks like a group of competitors summing up a lot of incremental tech enhancements to produce something with 10% higher scores on a benchmark every year, is that developing intelligence is a case where a relatively narrow technology by biological standards just happened to do a huge amount of stuff without that requiring developing whole new fleets of other biological capabilities.</p><p>So it looked like building a Wright Flyer that flies or a nuclear pile that reaches criticality, instead of looking like being in a stable manufacturing industry where a lot of little innovations sum to 10% better benchmark performance every year.</p><p>So, therefore, there is <i>stuff in the design space that does that</i>. It is <i>possible to build humans.</i></p><p>Maybe you can build things other than humans first, maybe they hang around for a few years. If you count GPT-3 as \"things other than human\", that clock has already started for all the good it does. But <i>humans don't get any less possible</i>.</p><p>From my perspective, this whole document feels like one very long filibuster of \"Smooth outputs are default. Smooth outputs are default. Pay no attention to this case of non-smooth output. Pay no attention to this other case either. All the non-smooth outputs are not in the right reference class. (Highly competitive manufacturing industries with lots of competitors are totally in the right reference class though. I'm not going to make that case explicitly because then you might think of how it might be wrong, I'm just going to let that implicit thought percolate at the back of your mind.) If we just talk a lot about smooth outputs and list ways that nonsmooth output producers aren't necessarily the same and arguments for nonsmooth outputs could fail, we get to go back to the intuition of smooth outputs. (We're not even going to discuss particular smooth outputs as cases in point, because then you might see how those cases might not apply. It's just the default. Not because we say so out loud, but because we talk a lot like that's the conclusion you're supposed to arrive at after reading.)\"</p><p>I deny the implicit meta-level assertion of this entire essay which would implicitly have you accept as valid reasoning the argument structure, \"Ah, yes, given the way this essay is written, we must totally have pretty strong prior reasons to believe in smooth outputs - just implicitly think of some smooth outputs, that's a reference class, now you have strong reason to believe that AGI output is smooth - we're not even going to argue this prior, just talk like it's there - now let us consider the arguments against smooth outputs - pretty weak, aren't they? we can totally imagine ways they could be wrong? we can totally argue reasons these cases don't apply? So at the end we go back to our strong default of smooth outputs. This essay is written with that conclusion, so that must be where the arguments lead.\"</p><p>Me: \"Okay, so what if somebody puts together the pieces required for general intelligence and it scales pretty well with added GPUs and FOOMS? Say, for the human case, that's some perceptual systems with imaginative control, a concept library, episodic memory, realtime procedural skill memory, which is all in chimps, and then we add some reflection to that, and get a human. Only, unlike with humans, once you have a working brain you can make a working brain 100X that large by adding 100X as many GPUs, and it can run some thoughts 10000X as fast. And that is substantially more effective brainpower than was being originally devoted to putting its design together, as it turns out. So it can make a substantially smarter AGI. For concreteness's sake. Reality has been trending well to the Eliezer side of Eliezer, on the Eliezer-Hanson axis, so perhaps you can do it more simply than that.\"</p><p>Simplicio: \"Ah, but what if, 5 years before then, somebody puts together some other AI which doesn't work like a human, and generalizes widely enough to have a big economic impact, but not widely enough to improve itself or generalize to AI tech or generalize to everything and end the world, and in 1 year it gets all the mass adoptions required to do whole bunches of stuff out in the real world that current regulations require to be done in various exact ways regardless of technology, and then in the next 4 years it doubles the world economy?\"</p><p>Me: \"Like... what kind of AI, exactly, and why didn't anybody manage to put together a full human-level thingy during those 5 years? Why are we even bothering to think about this whole weirdly specific scenario in the first place?\"</p><p>Simplicio: \"Because if you can put together something that has an enormous impact, you should be able to put together most of the pieces inside it and have a huge impact! Most technologies are like this. I've considered some things that are not like this and concluded they don't apply.\"</p><p>Me: \"Especially if we are talking about impact on GDP, it seems to me that most explicit and implicit 'technologies' are not like this at all, actually. There wasn't a cryptocurrency developed a year before Bitcoin using 95% of the ideas which did 10% of the transaction volume, let alone a preatomic bomb. But, like, can you give me any concrete visualization of how this could play out?\"</p><p>And there is no concrete visualization of how this could play out. Anything I'd have Simplicio say in reply would be unrealistic because there is no concrete visualization they give us. It is not a coincidence that I often use concrete language and concrete examples, and this whole field of argument does not use concrete language or offer concrete examples.</p><p>Though if we're sketching scifi scenarios, I suppose one <i>could</i> imagine a group that develops sufficiently advanced GPT-tech and deploys it on Twitter in order to persuade voters and politicians in a few developed countries to institute open borders, along with political systems that can handle open borders, and to permit housing construction, thereby doubling world GDP over 4 years. And since it was possible to use relatively crude AI tech to double world GDP this way, it legitimately takes the whole 4 years after that to develop real AGI that ends the world. FINE. SO WHAT. EVERYONE STILL DIES.</p></td></tr></tbody></table>\n\n### Universality thresholds\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][20:21]</strong>&nbsp;</p><blockquote><p>It’s easy to imagine a weak AI as some kind of handicapped human, with the handicap shrinking over time. Once the handicap goes to 0 we know that the AI will be above the universality threshold. Right now it’s below the universality threshold. So there must be sometime in between where it crosses the universality threshold, and that’s where the fast takeoff is predicted to occur.</p><p>But AI <i>isn’t</i> like a handicapped human. Instead, the designers of early AI systems will be trying to make them as useful as possible. So if universality is incredibly helpful, it will appear as early as possible in AI designs; designers will make tradeoffs to get universality at the expense of other desiderata (like cost or speed).</p><p>So now we’re almost back to the previous point: is there some secret sauce that gets you to universality, without which you can’t get universality however you try? I think this is unlikely for the reasons given in the previous section.</p></blockquote><p>We know, because humans, that there is humanly-widely-applicable general-intelligence tech.</p><p>What this section <i>wants</i> to establish, I think, or <i>needs</i> to establish to carry the argument, is that there is some intelligence tech that is wide enough to double the world economy in 4 years, but not world-endingly scalably wide, which becomes a possible AI tech 4 years before any general-intelligence-tech that will, if you put in enough compute, scale to the ability to do a sufficiently large amount of wide thought to FOOM (or build nanomachines, but if you can build nanomachines you can very likely FOOM from there too if not corrigible).</p><p>What it says instead is, \"I think we'll get universality much earlier on the equivalent of the biological timeline that has humans and chimps, so the resulting things will be weaker than humans at the point where they first become universal in that sense.\"</p><p>This is very plausibly true.</p><p>It doesn't mean that when this exciting result gets 100 times more compute dumped on the project, it takes at least 5 years to get anywhere really interesting from there (while also taking only 1 year to get somewhere sorta-interesting enough that the instantaneous adoption of it will double the world economy over the next 4 years).</p><p>It also isn't necessarily rather than plausibly true. For example, the thing that becomes universal, could also have massive gradient descent shallow powers that are far beyond what primates had at the same age.</p><p>Primates weren't already writing code as well as Codex when they started doing deep thinking. They couldn't do precise floating-point arithmetic. Their fastest serial rates of thought were a hell of a lot slower. They had no access to their own code or to their own memory contents etc. etc. etc.</p><p>But mostly I just want to call your attention to the immense gap between what this section needs to establish, and what it actually says and argues for.</p><p>What it actually argues for is a sort of local technological point: at the moment when generality first arrives, it will be with a brain that is less sophisticated than chimp brains were when they turned human.</p><p>It implicitly jumps all the way from there, across a <i>whole</i> lot of elided steps, to the implicit conclusion that this tech or elaborations of it will have smooth output behavior such that at some point the resulting impact is big enough to double the world economy in 4 years, without any further improvements ending the world economy before 4 years.</p><p>The underlying argument about how the AI tech might work is plausible. Chimps are insanely complicated. I mostly expect we will have AGI <i>long</i> before anybody is even <i>trying</i> to build anything that complicated.</p><p>The very next step of the argument, about capabilities, is already very questionable because this system could be using immense gradient descent capabilities to master domains for which large datasets are available, and hominids did <i>not</i> begin with instinctive great shallow mastery of all domains for which a large dataset could be made available, which is why hominids don't start out playing superhuman Go as soon as somebody tells them the rules and they do one day of self-play, which <i>is</i> the sort of capability that somebody could hook up to a nascent AGI (albeit we could optimistically and fondly and falsely imagine that somebody deliberately didn't floor the gas pedal as far as possible).</p><p>Could we have huge impacts out of some subuniversal shallow system that was hooked up to capabilities like this? Maybe, though this is <i>not</i> the argument made by the essay. It would be a specific outcome that isn't forced by anything in particular, but I can't say it's ruled out. Mostly my twin reactions to this are, \"If the AI tech is that dumb, how are all the bureaucratic constraints that actually rate-limit economic progress getting bypassed\" and \"Okay, but ultimately, so what and who cares, how does this modify that we all die?\"</p><blockquote><p>There is another reason I’m skeptical about hard takeoff from universality secret sauce: I think we <i>already</i> could make universal AIs if we tried (that would, given enough time, learn on their own and converge to arbitrarily high capability levels), and the reason we don’t is because it’s just not important to performance and the resulting systems would be really slow. This inside view argument is too complicated to make here and I don’t think my case rests on it, but it is relevant to understanding my view.</p></blockquote><p>I have no idea why this argument is being made or where it's heading. I cannot pass the <a href=\"https://www.econlib.org/archives/2011/06/the_ideological.html\">ITT</a> of the author. I don't know what the author thinks this has to do with constraining takeoffs to be slow instead of fast. At best I can conjecture that the author thinks that \"hard takeoff\" is supposed to derive from \"universality\" being very sudden and hard to access and late in the game, so if you can argue that universality could be accessed right now, you have defeated the argument for hard takeoff.</p></td></tr></tbody></table>\n\n### \"Understanding\" is discontinuous\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][20:41]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: I don’t yet understand this argument and am unsure if there is anything here.</i></p><p>It may be that understanding of the world tends to click, from “not understanding much” to “understanding basically everything.” You might expect this because everything is entangled with everything else.</p></blockquote><p>No, the idea is that a core of overlapping somethingness, trained to handle chipping handaxes and outwitting other monkeys, will generalize to building spaceships; so evolutionarily selecting on understanding a bunch of stuff, eventually ran across general stuff-understanders that understood a bunch more stuff.</p><p>Gradient descent may be genuinely different from this, but we shouldn't confuse imagination with knowledge when it comes to extrapolating that difference onward. At present, gradient descent does mass memorization of overlapping shallow patterns, which then combine to yield a weird pseudo-intelligence over domains for which we can deploy massive datasets, without yet generalizing much outside those domains.</p><p>We can hypothesize that there is some next step up to some weird thing that is intermediate in generality between gradient descent and humans, but we have not seen it yet, and we should not confuse imagination for knowledge.</p><p>If such a thing did exist, it would not necessarily be at the right level of generality to double the world economy in 4 years, without being able to build a better AGI.</p><p>If it was at that level of generality, it's nowhere written that no other company will develop a better prototype at a deeper level of generality over those 4 years.</p><p>I will also remark that you sure could look at the step from GPT-2 to GPT-3 and say, \"Wow, look at the way a whole bunch of stuff just seemed to simultaneously <i>click</i> for GPT-3.\"</p></td></tr></tbody></table>\n\n### Deployment lag\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][20:49]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: current AI is slow to deploy and powerful AI will be fast to deploy, but in between there will be AI that takes an intermediate length of time to deploy.</i></p></blockquote><p>An awful lot of my model of deployment lag is adoption lag and regulatory lag and bureaucratic sclerosis across companies and countries.</p><p>If doubling GDP is such a big deal, go open borders and build houses. Oh, that's illegal? Well, so will be AIs building houses!</p><p>AI tech that does flawless translation could plausibly come years before AGI, but that doesn't mean all the barriers to international trade and international labor movement and corporate hiring across borders all come down, because those barriers are not all translation barriers.</p><p>There's then a discontinuous jump at the point where everybody falls over dead and the AI goes off to do its own thing without FDA approval. This jump is precedented by earlier pre-FOOM prototypes being able to do pre-FOOM cool stuff, maybe, but not necessarily precedented by mass-market adoption of anything major enough to double world GDP.</p></td></tr></tbody></table>\n\n### Recursive self-improvement\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][20:54]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: Before there is AI that is great at self-improvement there will be AI that is mediocre at self-improvement.</i></p></blockquote><p>Oh, come on. That is straight-up not how simple continuous toy models of RSI work. Between a neutron multiplication factor of 0.999 and 1.001 there is a very huge gap in output behavior.</p><p>Outside of toy models: Over the last 10,000 years we had humans going from mediocre at improving their mental systems to being (barely) able to throw together AI systems, but 10,000 years is the equivalent of an eyeblink in evolutionary time - outside the metaphor, this says, \"A month before there is AI that is great at self-improvement, there will be AI that is mediocre at self-improvement.\"</p><p>(Or possibly an hour before, if reality is again more extreme along the Eliezer-Hanson axis than Eliezer. But it makes little difference whether it's an hour or a month, given anything like current setups.)</p><p>This is just pumping hard again on the intuition that says incremental design changes yield smooth output changes, which (the meta-level of the essay informs us wordlessly) is such a strong default that we are entitled to believe it if we can do a good job of weakening the evidence and arguments against it.</p><p>And the argument is: Before there are systems great at self-improvement, there will be systems mediocre at self-improvement; implicitly: \"before\" implies \"5 years before\" not \"5 days before\"; implicitly: this will correspond to smooth changes in output between the two regimes even though that is not how continuous feedback loops work.</p></td></tr></tbody></table>\n\n### Train vs. test\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][21:12]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: before you can train a really powerful AI, someone else can train a slightly worse AI.</i></p></blockquote><p>Yeah, and before you can evolve a human, you can evolve a Homo erectus, which is a slightly worse human.</p><blockquote><p>If you are able to raise $X to train an AGI that could take over the world, then it was almost certainly worth it for someone 6 months ago to raise $X/2 to train an AGI that could merely radically transform the world, since they would then get 6 months of absurd profits.</p></blockquote><p>I suppose this sentence makes a kind of sense if you assume away alignability and suppose that the previous paragraphs have refuted the notion of FOOMs, self-improvement, and thresholds between compounding returns and non-compounding returns (eg, in the human case, cognitive innovations like \"written language\" or \"science\"). If you suppose the previous sections refuted those things, then clearly, if you raised an AGI that you had aligned to \"take over the world\", it got that way through cognitive powers that weren't the result of FOOMing or other self-improvements, weren't the results of its cognitive powers crossing a threshold from non-compounding to compounding, wasn't the result of its understanding crossing a threshold of universality as the result of chunky universal machinery such as humans gained over chimps, so, implicitly, it must have been the kind of thing that you could learn by gradient descent, and do a half or a tenth as much of by doing half as much gradient descent, in order to build nanomachines a tenth as well-designed that could bypass a tenth as much bureaucracy.</p><p>If there are no unsmooth parts of the tech curve, the cognition curve, or the environment curve, then you should be able to make a bunch of wealth using a more primitive version of any technology that could take over the world.</p><p>And when we look back at history, why, that may be totally true! They may have deployed universal superhuman translator technology for 6 months, which won't double world GDP, but which a lot of people would pay for, and made a lot of money! Because even though there's no company that built 90% of Amazon's website and has 10% the market cap, when you zoom back out to look at whole industries like AI and a technological capstone like AGI, why, those whole industries do sometimes make some money along the way to the technological capstone, if they can find a niche that isn't too regulated! Which translation currently isn't! So maybe somebody used precursor tech to build a superhuman translator and deploy it 6 months earlier and made a bunch of money for 6 months. SO WHAT. EVERYONE STILL DIES.</p><p>As for \"radically transforming the world\" instead of \"taking it over\", I think that's just re-restated FOOM denialism. Doing either of those things quickly against human bureaucratic resistance strike me as requiring cognitive power levels dangerous enough that failure to align them on corrigibility would result in FOOMs.</p><p>Like, if you can do either of those things on purpose, you are doing it by operating in the regime where running the AI with higher bounds on the for loop will FOOM it, but you have politely asked it not to FOOM, please.</p><p>If the people doing this have any sense whatsoever, they will <i>refrain</i> from merely massively transforming the world until they are ready to do something that <i>prevents the world from ending</i>.</p><p>And if the gap from \"massively transforming the world, briefly before it ends\" to \"preventing the world from ending, lastingly\" takes much longer than 6 months to cross, or if other people have the same technologies that scale to \"massive transformation\", somebody else will build an AI that fooms all the way.</p><blockquote><p>Likewise, if your AGI would give you a decisive strategic advantage, they could have spent less earlier in order to get a pretty large military advantage, which they could then use to take your stuff.</p></blockquote><p>Again, this presupposes some weird model where everyone has easy alignment at the furthest frontiers of capability; everybody has the aligned version of the most rawly powerful AGI they can possibly build; and nobody in the future has the kind of tech advantage that Deepmind currently has; so before you can amp your AGI to the raw power level where it could take over the whole world by using the limit of its mental capacities to military ends - alignment of this being a trivial operation to be assumed away - some other party took their easily-aligned AGI that was less powerful at the limits of its operation, and used it to get 90% as much military power... is the implicit picture here?</p><p>Whereas the picture I'm drawing is that the AGI that kills you via \"decisive strategic advantage\" is the one that foomed and got nanotech, and no, the AI tech from 6 months earlier did not do 95% of a foom and get 95% of the nanotech.</p></td></tr></tbody></table>\n\n### Discontinuities at 100% automation\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][21:31]</strong>&nbsp;</p><blockquote><p><i>Summary of my response: at the point where humans are completely removed from a process, they will have been modestly improving output rather than acting as a sharp bottleneck that is suddenly removed.</i></p></blockquote><p>Not very relevant to my whole worldview in the first place; also not a very good description of how horses got removed from automobiles, or how humans got removed from playing Go.</p></td></tr></tbody></table>\n\n### The weight of evidence\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][21:31]</strong>&nbsp;</p><blockquote><p>We’ve discussed a lot of possible arguments for fast takeoff. Superficially it would be reasonable to believe that no individual argument makes fast takeoff look likely, but that in the aggregate they are convincing.</p><p>However, I think each of these factors is perfectly consistent with the continuous change story and continuously accelerating hyperbolic growth, and so none of them undermine that hypothesis at all.</p></blockquote><p>Uh huh. And how about if we have a mirror-universe essay which over and over again treats fast takeoff as the default to be assumed, and painstakingly shows how a bunch of particular arguments for slow takeoff might not be true?</p><p>This entire essay seems to me like it's drawn from the same hostile universe that produced Robin Hanson's side of the Yudkowsky-Hanson Foom Debate.</p><p>Like, all these abstract arguments devoid of concrete illustrations and \"it need not necessarily be like...\" and \"now that I've shown it's not necessarily like X, well, on the meta-level, I have implicitly told you that you now ought to believe Y\".</p><p>It just seems very clear to me that the sort of person who is taken in by this essay is the same sort of person who gets taken in by Hanson's arguments in 2008 and gets caught flatfooted by AlphaGo and GPT-3 and AlphaFold 2.</p><p>And empirically, it has already been shown to me that I do not have the power to break people out of the hypnosis of nodding along with Hansonian arguments, even by writing much longer essays than this.</p><p>Hanson's fond dreams of domain specificity, and smooth progress for stuff like Go, and of course somebody else has a precursor 90% as good as AlphaFold 2 before Deepmind builds it, and GPT-3 levels of generality just not being a thing, now stand refuted.</p><p>Despite that they're largely being exhibited again in this essay.</p><p>And people are still nodding along.</p><p>Reality just... doesn't work like this on some deep level.</p><p>It doesn't play out the way that people imagine it would play out when they're imagining a certain kind of reassuring abstraction that leads to a smooth world. Reality is less fond of that kind of argument than a certain kind of EA is fond of that argument.</p><p>There is a set of intuitive generalizations from experience which rules that out, which I do not know how to convey. There is an understanding of the rules of argument which leads you to roll your eyes at Hansonian arguments and all their locally invalid leaps and snuck-in defaults, instead of nodding along sagely at their wise humility and outside viewing and then going \"Huh?\" when AlphaGo or GPT-3 debuts. But this, I <i>empirically</i> do not seem to know how to convey to people, in advance of the inevitable and predictable contradiction by a reality which is not as fond of Hansonian dynamics as Hanson. The arguments sound convincing to them.</p><p>(Hanson himself has still not gone \"Huh?\" at the reality, though some of his audience did; perhaps because his abstractions are loftier than his audience's? - because some of his audience, reading along to Hanson, probably implicitly imagined a concrete world in which GPT-3 was not allowed; but maybe Hanson himself is more abstract than this, and didn't imagine anything so merely concrete?)</p><p>If I don't respond to essays like this, people find them comforting and nod along. If I do respond, my words are less comforting and more concrete and easier to imagine concrete objections to, less like a long chain of abstractions that sound like the very abstract words in research papers and hence implicitly convincing because they sound like other things you were supposed to believe.</p><p>And then there is another essay in 3 months. There is an infinite well of them. I would have to teach people to stop drinking from the well, instead of trying to whack them on the back until they cough up the drinks one by one, or actually, whacking them on the back and then they <i>don't</i> cough them up until reality contradicts them, and then a third of them notice that and cough something up, and then they don't learn the general lesson and go back to the well and drink again. And I don't know how to teach people to stop drinking from the well. I tried to teach that. I failed. If I wrote another Sequence I have no idea to believe that Sequence would work.</p><p>So what EAs will believe at the end of the world, will look like whatever the content was of the latest bucket from the well of infinite slow-takeoff arguments that hasn't yet been blatantly-even-to-them refuted by all the sharp jagged rapidly-generalizing things that happened along the way to the world's end.</p><p>And I know, before anyone bothers to say, that all of this reply is not written in the calm way that is right and proper for such arguments. I am tired. I have lost a lot of hope. There are not obvious things I can do, let alone arguments I can make, which I expect to be actually useful in the sense that the world will not end once I do them. I don't have the energy left for calm arguments. What's left is despair that can be given voice.</p></td></tr></tbody></table>\n\n   \n5.6. Yudkowsky/Christiano discussion: AI progress and crossover points\n--------------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:15]</strong>&nbsp;</p><p>To the extent that it was possible to make any predictions about 2015-2020 based on your views, I currently feel like they were much more wrong than right. I’m happy to discuss that. To the extent you are willing to make any bets about 2025, I expect they will be mostly wrong and I’d be happy to get bets on the record (most of all so that it will be more obvious in hindsight whether they are vindication for your view). Not sure if this is the place for that.</p><p>Could also make a separate channel to avoid clutter.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:16]</strong>&nbsp;</p><p>Possibly. I think that 2015-2020 played out to a much more Eliezerish side than Eliezer on the Eliezer-Hanson axis, which sure is a case of me being wrong. What bets do you think we'd disagree on for 2025? I expect you have mostly misestimated my views, but I'm always happy to hear about anything concrete.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:20]</strong>&nbsp;</p><p>I think the big points are: (i) I think you are significantly overestimating how large a discontinuity/trend break AlphaZero is, (ii) your view seems to imply that we will move quickly from much worse than humans to much better than humans, but it's likely that we will move slowly through the human range on many tasks. I'm not sure if we can get a bet out of (ii), I think I don't understand your view that well but I don't see how it could make the same predictions as mine over the next 10 years.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:22]</strong>&nbsp;</p><p>What are your 10-year predictions?</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:23]</strong>&nbsp;</p><p>My basic expectation is that for any given domain AI systems will gradually increase in usefulness, we will see a crossing over point where their output is comparable to human output, and that from that time we can estimate how long until takeoff by estimating \"how long does it take AI systems to get 'twice as impactful'?\" which gives you a number like ~1 year rather than weeks. At the crossing over point you get a somewhat rapid change in derivative, since you are looking at (x+y) where y is growing faster than x.</p><p>I feel like that should translate into different expectations about how impactful AI will be in any given domain---I don't see how to make the ultra-fast-takeoff view work if you think that AI output is increasingly smoothly (since the rate of progress at the crossing-over point will be similar to the current rate of progress, unless R&amp;D is scaling up much faster then)</p><p>So like, I think we are going to have crappy coding assistants, and then slightly less crappy coding assistants, and so on. And they will be improving the speed of coding very significantly before the end times.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:25]</strong>&nbsp;</p><p>You think in a different language than I do. My more confident statements about AI tech are about what happens after it starts to rise out of the metaphorical atmosphere and the turbulence subsides. When you have minds as early on the cognitive tech tree as humans they sure can get up to some weird stuff, I mean, just look at humans. Now take an utterly alien version of that with its own draw from all the weirdness factors. It sure is going to be pretty weird.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:26]</strong>&nbsp;</p><p>OK, but you keep saying stuff about how people with my dumb views would be \"caught flat-footed\" by historical developments. Surely to be able to say something like that you need to be making some kind of prediction?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:26]</strong>&nbsp;</p><p>Well, sure, now that Codex has suddenly popped into existence one day at a surprisingly high base level of tech, we should see various jumps in its capability over the years and some outside imitators. What do you think you predict differently about that than I do?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:26]</strong>&nbsp;</p><p>Why do you think codex is a high base level of tech?</p><p>The models get better continuously as you scale them up, and the first tech demo is weak enough to be almost useless</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:27]</strong>&nbsp;</p><p>I think the next-best coding assistant was, like, not useful.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:27]</strong>&nbsp;</p><p>yes</p><p>and it is still not useful</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:27]</strong>&nbsp;</p><p>Could be. Some people on HN seemed to think it was useful.</p><p>I haven't tried it myself.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:27]</strong>&nbsp;</p><p>OK, I'm happy to take bets</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:28]</strong>&nbsp;</p><p>I don't think the previous coding assistant would've been very good at coding an asteroid game, even if you tried a rigged demo at the same degree of rigging?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:28]</strong>&nbsp;</p><p>it's unquestionably a radically better tech demo</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:28]</strong>&nbsp;</p><p>Where by \"previous\" I mean \"previously deployed\" not \"previous generations of prototypes inside OpenAI's lab\".</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:28]</strong>&nbsp;</p><p>My basic story is that the model gets better and more useful with each doubling (or year of AI research) in a pretty smooth way. So the key underlying parameter for a discontinuity is how soon you build the first version---do you do that before or after it would be a really really big deal?</p><p>and the answer seems to be: you do it somewhat before it would be a really big deal</p><p>and then it gradually becomes a bigger and bigger deal as people improve it</p><p>maybe we are on the same page about getting gradually more and more useful? But I'm still just wondering where the foom comes from</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:30]</strong>&nbsp;</p><p>So, like... before we get systems that can FOOM and build nanotech, we should get more primitive systems that can write asteroid games and solve protein folding? Sounds legit.</p><p>So that happened, and now your model says that it's fine later on for us to get a FOOM, because we have the tech precursors and so your prophecy has been fulfilled?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:31]</strong>&nbsp;</p><p>no</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:31]</strong>&nbsp;</p><p>Didn't think so.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:31]</strong>&nbsp;</p><p>I can't tell if you can't understand what I'm saying, or aren't trying, or do understand and are just saying kind of annoying stuff as a rhetorical flourish</p><p>at some point you have an AI system that makes (humans+AI) 2x as good at further AI progress</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:32]</strong>&nbsp;</p><p>I know that what I'm saying isn't your viewpoint. I don't know what your viewpoint is or what sort of concrete predictions it makes at all, let alone what such predictions you think are different from mine.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:32]</strong>&nbsp;</p><p>maybe by continuity you can grant the existence of such a system, even if you don't think it will ever exist?</p><p>I want to (i) make the prediction that AI will actually have that impact at some point in time, (ii) talk about what happens before and after that</p><p>I am talking about AI systems that become continuously more useful, because \"become continuously more useful\" is what makes me think that (i) AI will have that impact at some point in time, (ii) allows me to productively reason about what AI will look like before and after that. I expect that your view will say something about why AI improvements either aren't continuous, or why continuous improvements lead to discontinuous jumps in the productivity of the (human+AI) system</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:34]</strong>&nbsp;</p><blockquote><p>at some point you have an AI system that makes (humans+AI) 2x as good at further AI progress</p></blockquote><p>Is this prophecy fulfilled by using some narrow eld-AI algorithm to map out a TPU, and then humans using TPUs can write in 1 month a research paper that would otherwise have taken 2 months? And then we can go on to FOOM now that this prophecy about pre-FOOM states has been fulfilled? I know the answer is no, but I don't know what you think is a narrower condition on the prophecy than that.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:35]</strong>&nbsp;</p><p>If you can use narrow eld-AI in order to make every part of AI research 2x faster, so that the entire field moves 2x faster, then the prophecy is fulfilled</p><p>and it may be just another 6 months until it makes all of AI research 2x faster again, and then 3 months, and then...</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:36]</strong>&nbsp;</p><p>What, the entire field? Even writing research papers? Even the journal editors approving and publishing the papers? So if we speed up every part of research except the journal editors, the prophecy has not been fulfilled and no FOOM may take place?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:36]</strong>&nbsp;</p><p>no, I mean the improvement in overall output, given the actual realistic level of bottlenecking that occurs in practice</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:37]</strong>&nbsp;</p><p>So if the realistic level of bottlenecking ever becomes dominated by a human gatekeeper, the prophecy is ever unfulfillable and no FOOM may ever occur.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:37]</strong>&nbsp;</p><p>that's what I mean by \"2x as good at further progress,\" the entire system is achieving twice as much</p><p>then the prophecy is unfulfillable and I will have been wrong</p><p>I mean, I think it's very likely that there will be a hard takeoff, if people refuse or are unable to use AI to accelerate AI progress for reasons unrelated to AI capabilities, and then one day they become willing</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:38]</strong>&nbsp;</p><p>...because on your view, the Prophecy necessarily goes through humans and AIs working together to speed up the whole collective field of AI?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:38]</strong>&nbsp;</p><p>it's fine if the AI works alone</p><p>the point is just that it overtakes the humans at the point when it is roughly as fast as the humans</p><p>why wouldn't it?</p><p>why does it overtake the humans when it takes it 10 seconds to double in capability instead of 1 year?</p><p>that's like predicting that cultural evolution will be infinitely fast, instead of making the more obvious prediction that it will overtake evolution exactly when it's as fast as evolution</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:39]</strong>&nbsp;</p><p>I live in a mental world full of weird prototypes that people are shepherding along to the world's end. I'm not even sure there's a short sentence in my native language that could translate the short Paul-sentence \"is roughly as fast as the humans\".</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:40]</strong>&nbsp;</p><p>do you agree that you can measure the speed with which the community of human AI researchers develop and implement improvements in their AI systems?</p><p>like, we can look at how good AI systems are in 2021, and in 2022, and talk about the rate of progress?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:40]</strong>&nbsp;</p><p>...when exactly in hominid history was hominid intelligence exactly as fast as evolutionary optimization???</p><blockquote><p>do you agree that you can measure the speed with which the community of human AI researchers develop and implement improvements in their AI systems?</p></blockquote><p>I mean... obviously not? How the hell would we measure real actual AI progress? What would even be the Y-axis on that graph?</p><p>I have a rough intuitive feeling that it was going faster in 2015-2017 than 2018-2020.</p><p>\"What was?\" says the stern skeptic, and I go \"I dunno.\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:42]</strong>&nbsp;</p><p>Here's a way of measuring progress you won't like: for almost all tasks, you can initially do them with lots of compute, and as technology improves you can do them with less compute. We can measure how fast the amount of compute required is going down.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:43]</strong>&nbsp;</p><p>Yeah, that would be a cool thing to measure. It's not obviously a relevant thing to anything important, but it'd be cool to measure.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:43]</strong>&nbsp;</p><p>Another way you won't like: we can hold fixed the resources we invest and look at the quality of outputs in any given domain (or even $ of revenue) and ask how fast it's changing.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:43]</strong>&nbsp;</p><p>I wonder what it would say about Go during the age of AlphaGo.</p><p>Or what that second metric would say.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:43]</strong>&nbsp;</p><p>I think it would be completely fine, and you don't really understand what happened with deep learning in board games. Though I also don't know what happened in much detail, so this is more like a prediction then a retrodiction.</p><p>But it's enough of a retrodiction that I shouldn't get too much credit for it.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:44]</strong>&nbsp;</p><p>I don't know what result you would consider \"completely fine\". I didn't have any particular unfine result in mind.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:45]</strong>&nbsp;</p><p>oh, sure</p><p>if it was just an honest question happy to use it as a concrete case</p><p>I would measure the rate of progress in Go by looking at how fast Elo improves with time or increasing R&amp;D spending</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:45]</strong>&nbsp;</p><p>I mean, I don't have strong predictions about it so it's not yet obviously cruxy to me</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:46]</strong>&nbsp;</p><p>I'd roughly guess that would continue, and if there were multiple trendlines to extrapolate I'd estimate crossover points based on that</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:47]</strong>&nbsp;</p><p>suppose this curve is smooth, and we see that sharp Go progress over time happened because Deepmind dumped in a ton of increased R&amp;D spend. you then argue that this cannot happen with AGI because by the time we get there, people will be pushing hard at the frontiers in a competitive environment where everybody's already spending what they can afford, just like in a highly competitive manufacturing industry.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:47]</strong>&nbsp;</p><p>the key input to making a prediction for AGZ in particular would be the precise form of the dependence on R&amp;D spending, to try to predict the changes as you shift from a single programmer to a large team at DeepMind, but most reasonable functional forms would be roughly right</p><p>Yes, it's definitely a prediction of my view that it's easier to improve things that people haven't spent much money on than things have spent a lot of money on. It's also a separate prediction of my view that people are going to be spending a boatload of money on all of the relevant technologies. Perhaps $1B/year right now and I'm imagining levels of investment large enough to be essentially bottlenecked on the availability of skilled labor.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][22:48]</strong>&nbsp;</p><p>( Previous Eliezer-comments about AlphaGo as a break in trend, responding briefly to Miles Brundage: <a href=\"https://twitter.com/ESRogs/status/1337869362678571008\">https://twitter.com/ESRogs/status/1337869362678571008</a> )</p></td></tr></tbody></table>\n\n5.7. Legal economic growth\n--------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:49]</strong>&nbsp;</p><p>Does your prediction change if all hell breaks loose in 2025 instead of 2055?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:50]</strong>&nbsp;</p><p>I think my prediction was wrong if all hell breaks loose in 2025, if by \"all hell breaks loose\" you mean \"dyson sphere\" and not \"things feel crazy\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:50]</strong>&nbsp;</p><p>Things feel crazy <i>in the AI field</i> and the world ends <i>less than</i> 4 years later, well before the world economy doubles.</p><p>Why was the Prophecy wrong if the world begins final descent in 2025? The Prophecy requires the world to then last until 2029 while doubling its economic output, after which it is permitted to end, but does not obviously to me forbid the Prophecy to begin coming true in 2025 instead of 2055.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:52]</strong>&nbsp;</p><p>yes, I just mean that some important underlying assumptions for the prophecy were violated, I wouldn't put much stock in it at that point, etc.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:53]</strong>&nbsp;</p><p>A lot of the issues I have with understanding any of your terminology in concrete Eliezer-language is that it looks to me like the premise-events of your Prophecy are fulfillable in all sorts of ways that don't imply the conclusion-events of the Prophecy.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:53]</strong>&nbsp;</p><p>if \"things feel crazy\" happens 4 years before dyson sphere, then I think we have to be really careful about what crazy means</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:54]</strong>&nbsp;</p><p>a lot of people looking around nervously and privately wondering if Eliezer was right, while public pravda continues to prohibit wondering anything such thing out loud, so they all go on thinking that they must be wrong.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:55]</strong>&nbsp;</p><p>OK, by \"things get crazy\" I mean like hundreds of billions of dollars of spending at google on automating AI R&amp;D</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:55]</strong>&nbsp;</p><p>I expect bureaucratic obstacles to prevent much GDP per se from resulting from this.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:55]</strong>&nbsp;</p><p>massive scaleups in semiconductor manufacturing, bidding up prices of inputs crazily</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:55]</strong>&nbsp;</p><p>I suppose that much spending could well increase world GDP by hundreds of billions of dollars per year.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:56]</strong>&nbsp;</p><p>massive speculative rises in AI company valuations financing a significant fraction of GWP into AI R&amp;D</p><p>(+hardware R&amp;D, +building new clusters, +etc.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][22:56]</strong>&nbsp;</p><p>like, higher than Tesla? higher than Bitcoin?</p><p>both of these things sure did skyrocket in market cap without that having much of an effect on housing stocks and steel production.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][22:57]</strong>&nbsp;</p><p>right now I think hardware R&amp;D is on the order of $100B/year, AI R&amp;D is more like $10B/year, I guess I'm betting on something more like trillions? (limited from going higher because of accounting problems and not that much smart money)</p><p>I don't think steel production is going up at that point</p><p>plausibly going down since you are redirecting manufacturing capacity into making more computers. But probably just staying static while all of the new capacity is going into computers, since cannibalizing existing infrastructure is much more expensive</p><p>the original point was: you aren't pulling AlphaZero shit any more, you are competing with an industry that has invested trillions in cumulative R&amp;D</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:00]</strong>&nbsp;</p><p>is this in hopes of future profit, or because current profits are already in the trillions?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:01]</strong>&nbsp;</p><p>largely in hopes of future profit / reinvested AI outputs (that have high market cap), but also revenues are probably in the trillions?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:02]</strong>&nbsp;</p><p>this all sure does sound \"pretty darn prohibited\" on my model, but I'd hope there'd be something earlier than that we could bet on. what does your Prophecy prohibit happening <i>before</i> that sub-prophesied day?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:02]</strong>&nbsp;</p><p>To me your model just seems crazy, and you are saying it predicts crazy stuff at the end but no crazy stuff beforehand, so I don't know what's prohibited. Mostly I feel like I'm making positive predictions, of gradually escalating value of AI in lots of different industries</p><p>and rapidly increasing investment in AI</p><p>I guess your model can be: those things happen, and then one day the AI explodes?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:03]</strong>&nbsp;</p><p>the main way you get rapidly increasing investment in AI is if there's some way that AI can produce huge profits without that being effectively bureaucratically prohibited - eg this is where we get huge investments in burning electricity and wasting GPUs on Bitcoin mining.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:03]</strong>&nbsp;</p><p>but it seems like you should be predicting e.g. AI quickly jumping to superhuman in lots of domains, and some applications jumping from no value to massive value</p><p>I don't understand what you mean by that sentence. Do you think we aren't seeing rapidly increasing investment in AI right now?</p><p>or are you talking about increasing investment above some high threshold, or increasing investment at some rate significantly larger than the current rate?</p><p>it seems to me like you can pretty seamlessly get up to a few $100B/year of revenue just by redirecting existing tech R&amp;D</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:05]</strong>&nbsp;</p><p>so I can imagine scenarios where some version of GPT-5 cloned outside OpenAI is able to talk hundreds of millions of mentally susceptible people into giving away lots of their income, and many regulatory regimes are unable to prohibit this effectively. then AI could be making a profit of trillions and then people would invest corresponding amounts in making new anime waifus trained in erotic hypnosis and findom.</p><p>this, to be clear, is not my mainline prediction.</p><p>but my sense is that our current economy is mostly not about the 1-day period to design new vaccines, it is about the multi-year period to be allowed to sell the vaccines.</p><p>the exceptions to this, like Bitcoin managing to say \"fuck off\" to the regulators for long enough, are where Bitcoin scales to a trillion dollars and gets massive amounts of electricity and GPU burned on it.</p><p>so we can imagine something like this for AI, which earns a trillion dollars, and sparks a trillion-dollar competition.</p><p>but my sense is that your model does not work like this.</p><p>my sense is that your model is about <i>general</i> improvements across the <i>whole</i> economy.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:08]</strong>&nbsp;</p><p>I think bitcoin is small even compared to current AI...</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:08]</strong>&nbsp;</p><p>my sense is that we've already built an economy which rejects improvement based on small amounts of cleverness, and only rewards amounts of cleverness large enough to bypass bureaucratic structures. it's not enough to figure out a version of e-gold that's 10% better. e-gold is already illegal. you have to figure out Bitcoin.</p><p>what are you going to build? better airplanes? airplane costs are mainly regulatory costs. better medtech? mainly regulatory costs. better houses? building houses is illegal anyways.</p><p>where is the room for the general AI revolution, short of the AI being literally revolutionary enough to overthrow governments?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:10]</strong>&nbsp;</p><p>factories, solar panels, robots, semiconductors, mining equipment, power lines, and \"factories\" just happens to be one word for a thousand different things</p><p>I think it's reasonable to think some jurisdictions won't be willing to build things but it's kind of improbable as a prediction for the whole world. That's a possible source of shorter-term predictions?</p><p>also computers and the 100 other things that go in datacenters</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:12]</strong>&nbsp;</p><p>The whole developed world rejects open borders. The regulatory regimes all make the same mistakes with an almost perfect precision, the kind of coordination that human beings could never dream of when trying to coordinate on purpose.</p><p>if the world lasts until 2035, I could perhaps see deepnets becoming as ubiquitous as computers were in... 1995? 2005? would that fulfill the terms of the Prophecy? I think it doesn't; I think your Prophecy requires that early <i>AGI</i> tech be that ubiquitous so that <i>AGI</i> tech will have trillions invested in it.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:13]</strong>&nbsp;</p><p>what is AGI tech?</p><p>the point is that there aren't important drivers that you can easily improve a lot</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:14]</strong>&nbsp;</p><p>for purposes of the Prophecy, AGI tech is that which, scaled far enough, ends the world; this must have trillions invested in it, so that the trajectory up to it cannot look like pulling an AlphaGo. no?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:14]</strong>&nbsp;</p><p>so it's relevant if you are imagining some piece of the technology which is helpful for general problem solving or something but somehow not helpful for all of the things people are doing with ML, to me that seems unlikely since it's all the same stuff</p><p>surely AGI tech should at least include the use of AI to automate AI R&amp;D</p><p>regardless of what you arbitrarily decree as \"ends the world if scaled up\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:15]</strong>&nbsp;</p><p>only if that's the path that leads to destroying the world?</p><p>if it isn't on that path, who cares Prophecy-wise?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:15]</strong>&nbsp;</p><p>also I want to emphasize that \"pull an AlphaGo\" is what happens when you move from SOTA being set by an individual programmer to a large lab, you don't need to be investing trillions to avoid that</p><p>and that the jump is still more like a few years</p><p>but the prophecy does involve trillions, and my view gets more like your view if people are jumping from $100B of R&amp;D ever to $1T in a single year</p></td></tr></tbody></table>\n\n5.8. TPUs and GPUs, and automating AI R&D\n-----------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:17]</strong>&nbsp;</p><p>I'm also wondering a little why the emphasis on \"trillions\". it seems to me that the terms of your Prophecy should be fulfillable by AGI tech being merely as ubiquitous as modern computers, so that many competing companies invest mere hundreds of billions in the equivalent of hardware plants. it is legitimately hard to get a chip with 50% better transistors ahead of TSMC.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:17]</strong>&nbsp;</p><p>yes, if you are investing hundreds of billions then it is hard to pull ahead (though could still happen)</p><p>(since the upside is so much larger here, no one cares that much about getting ahead of TSMC since the payoff is tiny in the scheme of the amounts we are discussing)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:18]</strong>&nbsp;</p><p>which, like, doesn't prevent Google from tossing out TPUs that are pretty significant jumps on GPUs, and if there's a specialized application of AGI-ish tech that is especially key, you can have everything behave smoothly and still get a jump that way.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:18]</strong>&nbsp;</p><p>I think TPUs are basically the same as GPUs</p><p>probably a bit worse</p><p>(but GPUs are sold at a 10x markup since that's the size of nvidia's lead)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:19]</strong>&nbsp;</p><p>noted; I'm not enough of an expert to directly contradict that statement about TPUs from my own knowledge.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:19]</strong>&nbsp;</p><p>(though I think TPUs are nevertheless leased at a slightly higher price than GPUs)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:19]</strong>&nbsp;</p><p>how does Nvidia maintain that lead and 10x markup? that sounds like a pretty un-Paul-ish state of affairs given Bitcoin prices never mind AI investments.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:20]</strong>&nbsp;</p><p>nvidia's lead isn't worth that much because historically they didn't sell many gpus</p><p>(especially for non-gaming applications)</p><p>their R&amp;D investment is relatively large compared to the $ on the table</p><p>my guess is that their lead doesn't stick, as evidenced by e.g. Google very quickly catching up</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:21]</strong>&nbsp;</p><p>parenthetically, does this mean - and I don't necessarily predict otherwise - that you predict a drop in Nvidia's stock and a drop in GPU prices in the next couple of years?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:21]</strong>&nbsp;</p><p>nvidia's stock may do OK from riding general AI boom, but I do predict a relative fall in nvidia compared to other AI-exposed companies</p><p>(though I also predicted google to more aggressively try to compete with nvidia for the ML market and think I was just wrong about that, though I don't really know any details of the area)</p><p>I do expect the cost of compute to fall over the coming years as nvidia's markup gets eroded</p><p>to be partially offset by increases in the cost of the underlying silicon (though that's still bad news for nvidia)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:23]</strong>&nbsp;</p><p>I parenthetically note that I think the Wise Reader should be justly impressed by predictions that come true about relative stock price changes, even if Eliezer has not explicitly contradicted those predictions before they come true. there are bets you can win without my having to bet against you.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:23]</strong>&nbsp;</p><p>you are welcome to counterpredict, but no saying in retrospect that reality proved you right if you don't 🙂</p><p>otherwise it's just me vs the market</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:24]</strong>&nbsp;</p><p>I don't feel like I have a counterprediction here, but I think the Wise Reader should be impressed if you win vs. the market.</p><p>however, this does require you to name in advance a few \"other AI-exposed companies\".</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:25]</strong>&nbsp;</p><p>Note that I made the same bet over the last year---I make a large AI bet but mostly moved my nvidia allocation to semiconductor companies. The semiconductor part of the portfolio is up 50% while nvidia is up 70%, so I lost that one. But that just means I like the bet even more next year.</p><p>happy to use nvidia vs tsmc</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:25]</strong>&nbsp;</p><p>there's a lot of noise in a 2-stock prediction.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:25]</strong>&nbsp;</p><p>I mean, it's a 1-stock prediction about nvidia</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:26]</strong>&nbsp;</p><p>but your funeral or triumphal!</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:26]</strong>&nbsp;</p><p>indeed 🙂</p><p>anyway</p><p>I expect all of the $ amounts to be much bigger in the future</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:26]</strong>&nbsp;</p><p>yeah, but using just TSMC for the opposition exposes you to I dunno Chinese invasion of Taiwan</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:26]</strong>&nbsp;</p><p>yes</p><p>also TSMC is not that AI-exposed</p><p>I think the main prediction is: eventual move away from GPUs, nvidia can't maintain that markup</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:27]</strong>&nbsp;</p><p>\"Nvidia can't maintain that markup\" sounds testable, but is less of a win against the market than predicting a relative stock price shift. (Over what timespan? Just the next year sounds quite fast for that kind of prediction.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:27]</strong>&nbsp;</p><p>regarding your original claim: if you think that it's plausible that AI will be doing all of the AI R&amp;D, and that will be accelerating continuously from 12, 6, 3 month \"doubling times,\" but that we'll see a discontinuous change in the \"path to doom,\" then that would be harder to generate predictions about</p><p>yes, it's hard to translate most predictions about the world into predictions about the stock market</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:28]</strong>&nbsp;</p><p>this again sounds like it's not written in Eliezer-language.</p><p>what does it mean for \"AI will be doing all of the AI R&amp;D\"? that sounds to me like something that happens after the end of the world, hence doesn't happen.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:29]</strong>&nbsp;</p><p>that's good, that's what I thought</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:29]</strong>&nbsp;</p><p>I don't necessarily want to sound very definite about that in advance of understanding what it <i>means</i></p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:29]</strong>&nbsp;</p><p>I'm saying that I think AI will be automating AI R&amp;D gradually, before the end of the world</p><p>yeah, I agree that if you reject the construct of \"how fast the AI community makes progress\" then it's hard to talk about what it means to automate \"progress\"</p><p>and that may be hard to make headway on</p><p>though for cases like AlphaGo (which started that whole digression) it seems easy enough to talk about elo gain per year</p><p>maybe the hard part is aggregating across tasks into a measure you actually care about?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:30]</strong>&nbsp;</p><p>up to a point, but yeah. (like, if we're taking Elo high above human levels and restricting our measurements to a very small range of frontier AIs, I quietly wonder if the measurement is still measuring quite the same thing with quite the same robustness.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:31]</strong>&nbsp;</p><p>I agree that elo measurement is extremely problematic in that regime</p></td></tr></tbody></table>\n\n5.9. Smooth exponentials vs. jumps in income\n--------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:31]</strong>&nbsp;</p><p>so in your worldview there's this big emphasis on things that must have been deployed and adopted widely to the point of already having huge impacts</p><p>and in my worldview there's nothing very surprising about people with a weird powerful prototype that wasn't used to automate huge sections of AI R&amp;D because the previous versions of the tech weren't useful for that or bigcorps didn't adopt it.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:32]</strong>&nbsp;</p><p>I mean, Google is already 1% of the US economy and in this scenario it and its peers are more like 10-20%? So wide adoption doesn't have to mean that many people. Though I also do predict much wider adoption than you so happy to go there if it's happy for predictions.</p><p>I don't really buy the \"weird powerful prototype\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:33]</strong>&nbsp;</p><p>yes. I noticed.</p><p>you would seem, indeed, to be offering large quantities of it for short sale.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:33]</strong>&nbsp;</p><p>and it feels like the thing you are talking about ought to have some precedent of some kind, of weird powerful prototypes that jump straight from \"does nothing\" to \"does something impactful\"</p><p>like if I predict that AI will be useful in a bunch of domains, and will get there by small steps, you should either predict that won't happen, or else also predict that there will be some domains with weird prototypes jumping to giant impact?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:34]</strong>&nbsp;</p><p>like an electrical device that goes from \"not working at all\" to \"actually working\" as soon as you screw in the attachments for the electrical plug.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:34]</strong>&nbsp;</p><p>(clearly takes more work to operationalize)</p><p>I'm not sure I understand that sentence, hopefully it's clear enough why I expect those discontinuities?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:34]</strong>&nbsp;</p><p>though, no, that's a facile bad analogy.</p><p>a better analogy would be an AI system that only starts working after somebody tells you about batch normalization or LAMB learning rate or whatever.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:36]</strong>&nbsp;</p><p>sure, which I think will happen all the time for individual AI projects but not for sota</p><p>because the projects at sota have picked the low hanging fruit, it's not easy to get giant wins</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:36]</strong>&nbsp;</p><blockquote><p>like if I predict that AI will be useful in a bunch of domains, and will get there by small steps, you should either predict that won't happen, or else also predict that there will be some domains with weird prototypes jumping to giant impact?</p></blockquote><p>in the latter case, has this Eliezer-Prophecy already had its terms fulfilled by AlphaFold 2, or do you say nay because AlphaFold 2 hasn't doubled GDP?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:37]</strong>&nbsp;</p><p>(you can also get giant wins by a new competitor coming up at a faster rate of progress, and then we have more dependence on whether people do it when it's a big leap forward or slightly worse than the predecessor, and I'm betting on the latter)</p><p>I have no idea what AlphaFold 2 is good for, or the size of the community working on it, my guess would be that its value is pretty small</p><p>we can try to quantify</p><p>like, I get surprised when $X of R&amp;D gets you something whose value is much larger than $X</p><p>I'm not surprised at all if $X of R&amp;D gets you &lt;&lt;$X, or even like 10*$X in a given case that was selected for working well</p><p>hopefully it's clear enough why that's the kind of thing a naive person would predict</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:38]</strong>&nbsp;</p><p>so a thing which Eliezer's Prophecy does not mandate per se, but sure does permit, and is on the mainline especially for nearer timelines, is that the world-ending prototype had no prior prototype containing 90% of the technology which earned a trillion dollars.</p><p>a lot of Paul's Prophecy seems to be about forbidding this.</p><p>is that a fair way to describe your own Prophecy?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:39]</strong>&nbsp;</p><p>I don't have a strong view about \"containing 90% of the technology\"</p><p>the main view is that whatever the \"world ending prototype\" does, there were earlier systems that could do practically the same thing</p><p>if the world ending prototype does something that lets you go foom in a day, there was a system years earlier that could foom in a month, so that would have been the one to foom</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:41]</strong>&nbsp;</p><p>but, like, the world-ending thing, according to the Prophecy, must be squarely in the middle of a class of technologies which are in the midst of earning trillions of dollars and having trillions of dollars invested in them. it's not enough for the Worldender to be definitionally somewhere in that class, because then it could be on a weird outskirt of the class, and somebody could invest a billion dollars in that weird outskirt before anybody else had invested a hundred million, which is forbidden by the Prophecy. so the Worldender has got to be right in the middle, a plain and obvious example of the tech that's already earning trillions of dollars. ...y/n?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:42]</strong>&nbsp;</p><p>I agree with that as a prediction for some operationalization of \"a plain and obvious example,\" but I think we could make it more precise / it doesn't feel like it depends on the fuzziness of that</p><p>I think that if the world can end out of nowhere like that, you should also be getting $100B/year products out of nowhere like that, but I guess you think not because of bureaucracy</p><p>like, to me it seems like our views stake out predictions about codex, where I'm predicting its value will be modest relative to R&amp;D, and the value will basically improve from there with a nice experience curve, maybe something like ramping up quickly to some starting point &lt;$10M/year and then doubling every year thereafter, whereas I feel like you are saying more like \"who knows, could be anything\" and so should be surprised each time the boring thing happens</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:45]</strong>&nbsp;</p><p>the concrete example I give is that the World-Ending Company will be able to use the same tech to build a true self-driving car, which would in the natural course of things be approved for sale a few years later after the world had ended.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:46]</strong>&nbsp;</p><p>but self-driving cars seem very likely to already be broadly deployed, and so the relevant question is really whether their technical improvements can also be deployed to those cars?</p><p>(or else maybe that's another prediction we disagree about)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:47]</strong>&nbsp;</p><p>I feel like I would indeed not have the right to feel very surprised if Codex technology stagnated for the next 5 years, nor if it took a massive leap in 2 years and got ubiquitously adopted by lots of programmers.</p><p>yes, I think that's a general timeline difference there</p><p>re: self-driving cars</p><p>I might be talkable into a bet where you took \"Codex tech will develop like <i>this</i>\" and I took the side \"literally anything else but that\"</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:48]</strong>&nbsp;</p><p>I think it would have to be over/under, I doubt I'm more surprised than you by something failing to be economically valuable, I'm surprised by big jumps in value</p><p>seems like it will be tough to work</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:49]</strong>&nbsp;</p><p>well, if I was betting on something taking a big jump in income, I sure would bet on something in a relatively unregulated industry like Codex or anime waifus.</p><p>but that's assuming I made the bet at all, which is a hard sell when the bet is about the Future, which is notoriously hard to predict.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:50]</strong>&nbsp;</p><p>I guess my strongest take is: if you want to pull the thing where you say that future developments proved you right and took unreasonable people like me by surprise, you've got to be able to say <i>something</i> in advance about what you expect to happen</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:51]</strong>&nbsp;</p><p>so what if neither of us are surprised if Codex stagnates for 5 years, you win if Codex shows a smooth exponential in income, and I win if the income looks... jumpier? how would we quantify that?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:52]</strong>&nbsp;</p><p>codex also does seem a bit unfair to you in that it may have to be adopted by lots of programmers which could slow things down a lot even if capabilities are pretty jumpy</p><p>(though I think in fact usefulness and not merely profit will basically just go up smoothly, with step sizes determined by arbitrary decisions about when to release something)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:53]</strong>&nbsp;</p><p>I'd also be concerned about unfairness to me in that earnable income is not the same as the gains from trade. If there's more than 1 competitor in the industry, their earnings from Codex may be much less than the value produced, and this may not change much with improvements in the tech.</p></td></tr></tbody></table>\n\n5.10. Late-stage predictions\n----------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Christiano][23:53]</strong>&nbsp;</p><p>I think my main update from this conversation is that you don't really predict someone to come out of nowhere with a model that can earn a lot of $, even if they could come out of nowhere with a model that could end the world, because of regulatory bottlenecks and nimbyism and general sluggishness and unwillingness to do things</p><p>does that seem right?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:55]</strong>&nbsp;</p><p>Well, and also because the World-ender is \"the first thing that scaled with compute\" and/or \"the first thing that ate the real core of generality\" and/or \"the first thing that went over neutron multiplication factor 1\".</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:55]</strong>&nbsp;</p><p>and so that cuts out a lot of the easily-specified empirical divergences, since \"worth a lot of $\" was the only general way to assess \"big deal that people care about\" and avoiding disputes like \"but Zen was mostly developed by a single programmer, it's not like intense competition\"</p><p>yeah, that's the real disagreement it seems like we'd want to talk about</p><p>but it just doesn't seem to lead to many prediction differences in advance?</p><p>I totally don't buy any of those models, I think they are bonkers</p><p>would love to bet on that</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:56]</strong>&nbsp;</p><p>Prolly but I think the from-my-perspective-weird talk about GDP is probably concealing <i>some</i> kind of important crux, because caring about GDP still feels pretty alien to me.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:56]</strong>&nbsp;</p><p>I feel like getting up to massive economic impacts without seeing \"the real core of generality\" seems like it should also be surprising on your view</p><p>like if it's 10 years from now and AI is a pretty big deal but no crazy AGI, isn't that surprising?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:57]</strong>&nbsp;</p><p>Mildly but not too surprising, I would imagine that people had built a bunch of neat stuff with gradient descent in realms where you could get a long way on self-play or massively collectible datasets.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:58]</strong>&nbsp;</p><p>I'm fine with the crux being something that doesn't lead to any empirical disagreements, but in that case I just don't think you should claim credit for the worldview making great predictions.</p><p>(or the countervailing worldview making bad predictions)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][23:59]</strong>&nbsp;</p><p>stuff that we could see then: self-driving cars (10 years is enough for regulatory approval in many countries), super Codex, GPT-6 powered anime waifus being an increasingly loud source of (arguably justified) moral panic and a hundred-billion-dollar industry</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][23:59]</strong>&nbsp;</p><p>another option is \"10% <s>GDP</s> GWP growth in a year, before doom\"</p><p>I think that's very likely, though might be too late to be helpful</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:01]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>see, that seems genuinely hard unless somebody gets GPT-4 far head of any political opposition - I guess all the competent AGI groups lean solidly liberal at the moment? - and uses it to fake massive highly-persuasive sentiment on Twitter for housing liberalization.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:01]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>so seems like a bet?</p><p>but you don't get to win until doom 🙁</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:02]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>I mean, as written, I'd want to avoid cases like 10% growth on paper while recovering from a pandemic that produced 0% growth the previous year.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:02]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>yeah</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:04]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>I'd want to check the current rate (5% iirc) and what the variance on it was, 10% is a little low for surety (though my sense is that it's a pretty darn smooth graph that's hard to perturb)</p><p>if we got 10% in a way that was clearly about AI tech becoming that ubiquitous, I'd feel relatively good about nodding along and saying, \"Yes, that is like unto the beginning of Paul's Prophecy\" not least because the timelines had been that long at all.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:05]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>like 3-4%/year right now</p><p>random wikipedia number is 5.5% in 2006-2007, 3-4% since 2010</p><p>4% 1995-2000</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:06]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>I don't want to sound obstinate here. My model does not <i>forbid</i> that we dwiddle around on the AGI side while gradient descent tech gets its fingers into enough separate weakly-generalizing pies to produce 10% GDP growth, but I'm happy to say that this sounds much more like Paul's Prophecy is coming true.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:07]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>ok, we should formalize at some point, but also need the procedure for you getting credit given that it can't resolve in your favor until the end of days</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:07]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>Is there something that sounds to you like Eliezer's Prophecy which we can observe before the end of the world?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:07]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>when you will already have all the epistemic credit you need</p><p>not on the \"simple core of generality\" stuff since that apparently immediately implies end of world</p><p>maybe something about ML running into obstacles en route to human level performance?</p><p>or about some other kind of discontinuous jump even in a case where people care, though there seem to be a few reasons you don't expect many of those</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:08]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>depends on how you define \"immediately\"? it's not <i>long</i> before the end of the world, but in some sad scenarios there is some tiny utility to you declaring me right 6 months before the end.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:09]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>I care a lot about the 6 months before the end personally</p><p>though I do think probably everything is more clear by then independent of any bet; but I guess you are more pessimistic about that</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:09]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>I'm not quite sure what I'd do in them, but I may have worked something out before then, so I care significantly in expectation if not in particular.</p><p>I am more pessimistic about other people's ability to notice what reality is screaming in their faces, yes.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:10]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>if we were to look at various scaling curves, e.g. of loss vs model size or something, do you expect those to look distinctive as you hit the \"real core of generality\"?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:10]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>let me turn that around: if we add transformers into those graphs, do they jump around in a way you'd find interesting?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:11]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>not really</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:11]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>is that because the empirical graphs don't jump, or because you don't think the jumps say much?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:11]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>but not many good graphs to look at (I just have one in mind), so that's partly a prediction about what the exercise would show</p><p>I don't think the graphs jump much, and also transformers come before people start evaluating on tasks where they help a lot</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:12]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>It would not terribly contradict the terms of my Prophecy if the World-ending tech began by not producing a big jump on existing tasks, but generalizing to some currently not-so-popular tasks where it scaled much faster.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][0:13]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>eh, they help significantly on contemporary tasks, but it's just not a huge jump relative to continuing to scale up model sizes</p><p>or other ongoing improvements in architecture</p><p>anyway, should try to figure out something, and good not to finalize a bet until you have some way to at least come out ahead, but I should sleep now</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][0:14]</strong> &nbsp;<strong>(next day, Sep. 15)</strong>&nbsp;</p><p>yeah, same.</p><p>Thing I want to note out loud lest I forget ere I sleep: I think the real world is full of tons and tons of technologies being developed as unprecedented prototypes in the midst of big fields, because the key thing to invest in wasn’t the competitively explored center. Wright Flyer vs all expenditures on Traveling Machine R&amp;D. First atomic pile and bomb vs all Military R&amp;D.</p><p>This is one reason why Paul’s Prophecy seems fragile to me. You could have the preliminaries come true as far as there being a trillion bucks in what looks like AI R&amp;D, and then the WorldEnder is a weird prototype off to one side of that. saying “But what about the rest of that AI R&amp;D?” is no more a devastating retort to reality than looking at AlphaGo and saying “But weren’t other companies investing billions in Better Software?” Yeah but it was a big playing field with lots of different kinds of Better Software and no other medium-sized team of 15 people with corporate TPU backing was trying to build a system just like AlphaGo, even though multiple small outfits were trying to build prestige-earning gameplayers. Tech advancements very very often occur in places where investment wasn't dense enough to guarantee overlap.</p></td></tr></tbody></table>\n\n   \n6\\. Follow-ups on \"Takeoff Speeds\"\n======================================\n\n6.1. Eliezer Yudkowsky's commentary\n-----------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][17:25]</strong> &nbsp;<strong>(Sep. 15)</strong>&nbsp;</p><p>Further comment that occurred to me on \"takeoff speeds\" if I've better understood the main thesis now: its hypotheses seem to include a perfectly anti-Thielian setup for AGI.</p><p>Thiel has a running thesis about how part of the story behind the Great Stagnation and the decline in innovation that's about atoms rather than bits - the story behind \"we were promised flying cars and got 140 characters\", to cite the classic Thielian quote - is that people stopped believing in <a href=\"https://www.lesswrong.com/posts/ReB7yoF22GuerNfhH/thiel-on-secrets-and-indefiniteness\">\"secrets</a>\".</p><p>Thiel suggests that you have to believe there are knowable things that aren't yet widely known - not just things that everybody already knows, plus mysteries that nobody will ever know - in order to be motivated to go out and innovate. Culture in developed countries shifted to label this kind of thinking rude - or rather, even ruder, even less tolerated than it had been decades before - so innovation decreased as a result.</p><p>The central hypothesis of \"takeoff speeds\" is that at the time of serious AGI being developed, it is perfectly anti-Thielian in that it is devoid of secrets in that sense. It is not permissible (on this viewpoint) for it to be the case that there is a lot of AI investment into AI that is directed not quite at the key path leading to AGI, such that somebody could spend $1B on compute for the key path leading to AGI before anybody else had spent $100M on that. There cannot exist any secret like that. The path to AGI will be known; everyone, or a wide variety of powerful actors, will know how profitable that path will be; the surrounding industry will be capable of acting on this knowledge, and will have actually been acting on it as early as possible; multiple actors are already investing in every tech path that would in fact be profitable (and is known to any human being at all), as soon as that R&amp;D opportunity becomes available.</p><p>And I'm not saying this is an inconsistent world to describe! I've written science fiction set in this world. I called it \"<a href=\"https://yudkowsky.tumblr.com/post/81447230971/my-april-fools-day-confession\">dath ilan</a>\". It's a hypothetical world that is actually full of smart people in economic equilibrium. If anything like Covid-19 appears, for example, the governments and public-good philanthropists there have already set up prediction markets (which are not illegal, needless to say); and of course there are mRNA vaccine factories already built and ready to go, because somebody already calculated the profits from fast vaccines would be very high in case of a pandemic (no artificial price ceilings in this world, of course); so as soon as the prediction markets started calling the coming pandemic conditional on no vaccine, the mRNA vaccine factories were already spinning up.</p><p>This world, however, is not Earth.</p><p>On Earth, major chunks of technological progress quite often occur <i>outside</i> of a social context where everyone knew and agreed in advance on which designs would yield how much expected profit and many overlapping actors competed to invest in the most actually-promising paths simultaneously.</p><p>And that is why you can read <a href=\"https://equilibriabook.com/toc/\">Inadequate Equilibria</a>, and then read this essay on takeoff speeds, and go, \"Oh, yes, I recognize this; it's written inside the Modesty worldview; in particular, the imagination of an adequate world in which there is a perfect absence of Thielian secrets or unshared knowable knowledge about fruitful development pathways. This is the same world that already had mRNA vaccines ready to spin up on day one of the Covid-19 pandemic, because markets had correctly forecasted their option value and investors had acted on that forecast unimpeded. Sure would be an interesting place to live! But we don't live there.\"</p><p>Could we perhaps end up in a world where the path to AGI is in fact not a Thielian secret, because in fact the first accessible path to AGI happens to lie along a tech pathway that already delivered large profits to previous investors who summed a lot of small innovations, a la experience with chipmaking, such that there were no large innovations just lots and lots of small innovations that yield 10% improvement annually on various tech benchmarks?</p><p>I think that even in this case we will get weird, discontinuous, and fatal behaviors, and I could maybe talk about that when discussion resumes. But it is not ruled out to me that the first accessible pathway to AGI could happen to lie in the further direction of some road that was already well-traveled, already yielded much profit to now-famous tycoons back when its first steps were Thielian secrets, and hence is now replete with dozens of competing chasers for the gold rush.</p><p>It's even imaginable to me, though a bit less so, that the first path traversed to real actual pivotal/powerful/lethal AGI, happens to lie literally actually squarely in the central direction of the gold rush. It sounds a little less like the tech history I know, which is usually about how someone needed to swerve a bit and the popular gold-rush forecasts weren't quite right, but maybe that is just a selective focus of history on the more interesting cases.</p><p>Though I remark that - even supposing that getting to big AGI is literally as straightforward and yet as difficult as falling down a semiconductor manufacturing roadmap (as otherwise the biggest actor to first see the obvious direction could just rush down the whole road) - well, TSMC does have a bit of an unshared advantage right now, if I recall correctly. And Intel had a bit of an advantage before that. So that happens even when there's competitors competing to invest billions.</p><p>But we can imagine that doesn't happen either, because instead of needing to build a whole huge manufacturing plant, there's just lots and lots of little innovations adding up to every key AGI threshold, which lots of actors are investing $10 million in at a time, and everybody knows which direction to move in to get to more serious AGI and they're right in this shared forecast.</p><p>I am willing to entertain discussing this world and the sequelae there - I do think everybody still dies in this case - but I would not have this particular premise thrust upon us as a default, through a not-explicitly-spoken pressure against being so immodest and inegalitarian as to suppose that any Thielian knowable-secret will exist, or that anybody in the future gets as far ahead of others as today's TSMC or today's Deepmind.</p><p>We are, in imagining this world, imagining a world in which AI research has become drastically unlike today's AI research in a direction drastically different from the history of many other technologies.</p><p>It's not literally unprecedented, but it's also not a default environment for big moments in tech progress; it's narrowly precedented for <i>particular</i> industries with high competition and steady benchmark progress driven by huge investments into a sum of many tiny innovations.</p><p>So I can entertain the scenario. But if you want to claim that the social situation around AGI <i>will</i> drastically change in this way you foresee - not just that it <i>could</i> change in that direction, if somebody makes a big splash that causes everyone else to reevaluate their previous opinions and arrive at yours, but that this social change <i>will</i> occur and you know this now - and that the prerequisite tech path to AGI is known to you, and forces an investment situation that looks like the semiconductor industry - then your \"What do you think you know and how do you think you know it?\" has some significant explaining to do.</p><p>Of course, I do appreciate that such a thing could be knowable, and yet not known to me. I'm not so silly as to disbelieve in secrets like that. They're all over the actual history of technological progress on our actual Earth.</p></td></tr></tbody></table>",
      "plaintextDescription": "This is a transcription of Eliezer Yudkowsky responding to Paul Christiano's Takeoff Speeds live on Sep. 14, followed by a conversation between Eliezer and Paul. This discussion took place after Eliezer's conversation with Richard Ngo.\n\n \n\nColor key:\n\n Chat by Paul and Eliezer  Other chat  Inline comments \n\n \n\n\n5.5. Comments on \"Takeoff Speeds\"\n \n\n[Yudkowsky][10:14]  (Nov. 22 follow-up comment) \n\n(This was in response to an earlier request by Richard Ngo that I respond to Paul on Takeoff Speeds.)\n\n[Yudkowsky][16:52] \n\nmaybe I'll try liveblogging some https://sideways-view.com/2018/02/24/takeoff-speeds/ here in the meanwhile\n\n\n \n\n\nSlower takeoff means faster progress\n[Yudkowsky][16:57] \n\n> The main disagreement is not about what will happen once we have a superintelligent AI, it’s about what will happen before we have a superintelligent AI. So slow takeoff seems to mean that AI has a larger impact on the world, sooner.\n\nIt seems to me to be disingenuous to phrase it this way, given that slow-takeoff views usually imply that AI has a large impact later relative to right now (2021), even if they imply that AI impacts the world \"earlier\" relative to \"when superintelligence becomes reachable\".\n\n\"When superintelligence becomes reachable\" is not a fixed point in time that doesn't depend on what you believe about cognitive scaling. The correct graph is, in fact, the one where the \"slow\" line starts a bit before \"fast\" peaks and ramps up slowly, reaching a high point later than \"fast\". It's a nice try at reconciliation with the imagined Other, but it fails and falls flat.\n\nThis may seem like a minor point, but points like this do add up.\n\n> In the fast takeoff scenario, weaker AI systems may have significant impacts but they are nothing compared to the “real” AGI. Whoever builds AGI has a decisive strategic advantage. Growth accelerates from 3%/year to 3000%/year without stopping at 30%/year. And so on.\n\nThis again shows failure to engage with the Other's real viewpoint. My ",
      "wordCount": 18062
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "8daMDi9NEShyLqxth",
        "name": "Forecasting & Prediction",
        "slug": "forecasting-and-prediction"
      },
      {
        "_id": "bxhzaWtdNoEMMkE8r",
        "name": "General intelligence",
        "slug": "general-intelligence"
      },
      {
        "_id": "rWzGNdjuep56W5u2d",
        "name": "Inside/Outside View",
        "slug": "inside-outside-view"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "hwxj4gieR7FWNwYfa",
    "title": "Ngo and Yudkowsky on AI capability gains",
    "slug": "ngo-and-yudkowsky-on-ai-capability-gains-1",
    "url": null,
    "baseScore": 131,
    "voteCount": 43,
    "viewCount": null,
    "commentCount": 61,
    "createdAt": null,
    "postedAt": "2021-11-18T22:19:05.913Z",
    "contents": {
      "markdown": "This is the second post in a series of transcribed conversations about AGI forecasting and alignment. See the [first post](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty) for prefaces and more information about the format.\n\n  \nColor key:\n\n<table><tbody><tr><td>&nbsp; Chat by Richard Ngo and Eliezer Yudkowsky &nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%)\">&nbsp; &nbsp;Other chat &nbsp;&nbsp;</td><td style=\"background-color:#FFEEBB\">&nbsp; &nbsp;Inline comments &nbsp;&nbsp;</td></tr></tbody></table>\n\n5\\. September 14 conversation\n=============================\n\n5.1. Recursive self-improvement, abstractions, and miracles\n-----------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:00]</strong>&nbsp;</p><p>Good morning / good evening.</p><p>So it seems like the obvious thread to pull today is your sense that I'm wrong about recursive self-improvement and consequentialism in a related way?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:04]</strong>&nbsp;</p><p>Right. And then another potential thread (probably of secondary importance) is the question of what you mean by utility functions, and digging more into the intuitions surrounding those.</p><p>But let me start by fleshing out this RSI/consequentialism claim.</p><p>I claim that your early writings about RSI focused too much on a very powerful abstraction, of recursively applied optimisation; and too little on the ways in which even powerful abstractions like this one become a bit... let's say messier, when they interact with the real world.</p><p>In particular, I think that <a href=\"https://sideways-view.com/2018/02/24/takeoff-speeds/\">Paul's arguments</a> that there will be substantial progress in AI in the leadup to a RSI-driven takeoff are pretty strong ones.</p><p>(Just so we're on the same page: to what extent did those arguments end up shifting your credences?)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:09]</strong>&nbsp;</p><p>I don't remember being shifted by Paul on this at all. I sure shifted a lot over events like Alpha Zero and the entire deep learning revolution. What does Paul say that isn't encapsulated in that update - does he furthermore claim that we're going to get fully smarter-than-human in all regards AI which doesn't cognitively scale much further either through more compute or through RSI?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:10]</strong>&nbsp;</p><p>Ah, I see. In that case, let's just focus on the update from the deep learning revolution.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:12][11:13]</strong>&nbsp;</p><p>I'll also remark that I see my foreseeable mistake there as having little to do with \"abstractions becoming messier when they interact with the real world\" - this truism tells you very little of itself, unless you can predict <i>directional</i> shifts in other variables just by contemplating the <i>unknown</i> messiness relative to the abstraction.</p><p>Rather, I'd see it as a neighboring error to what I've called the Law of Earlier Failure, where the Law of Earlier Failure says that, compared to the interesting part of the problem where it's fun to imagine yourself failing, you usually fail before then, because of the many earlier boring points where it's possible to fail.</p><p>The nearby reasoning error in my case is that I focused on an interesting way that AI capabilities could scale and the most powerful argument I had to overcome Robin's objections, while missing the way that Robin's objections could fail even earlier through rapid scaling and generalization in a more boring way.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\">It doesn't mean that my arguments about RSI were false about their domain of supposed application, but that other things were also true and those things happened first on our timeline. To be clear, I think this is an important and generalizable issue with the impossible task of trying to forecast the Future, and if I am wrong about other things it sure would be plausible if I was wrong in similar ways.</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:13]</strong>&nbsp;</p><p>Then the analogy here is something like: there is a powerful abstraction, namely consequentialism; and we both agree that (like RSI) a large amount of consequentialism is a very dangerous thing. But we disagree on the question of how much the strategic landscape in the leadup to highly-consequentialist AIs is affected by other factors apart from this particular abstraction.</p><p>\"this truism tells you very little of itself, unless you can predict directional shifts in other variables just by contemplating the unknown messiness relative to the abstraction\"</p><p>I disagree with this claim. It seems to me that the predictable direction in which the messiness pushes is <i>away from</i> the applicability of the high-level abstraction.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:15]</strong>&nbsp;</p><p>The real world is messy, but good abstractions still apply, just with some messiness around them. The Law of Earlier Failure is not a failure of the abstraction being messy, it's a failure of the <i>subject matter</i> ending up different such that the abstractions you used were <i>about a different subject matter</i>.</p><p>When a company fails before the exciting challenge where you try to scale your app across a million users, because you couldn't hire enough programmers to build your app at all, the problem is not that you had an unexpectedly messy abstraction about scaling to many users, but that the key determinants were a different subject matter than \"scaling to many users\".</p><p>Throwing 10,000 TPUs at something and actually getting progress - not very much of a famous technological idiom <i>at the time I was originally arguing with Robin</i> - is not a leak in the RSI abstraction, it's just a way of getting powerful capabilities without RSI.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:18]</strong>&nbsp;</p><p>To me the difference between these two things seems mainly semantic; does it seem otherwise to you?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:18]</strong>&nbsp;</p><p>If I'd been arguing with somebody who kept arguing in favor of faster timescales, maybe I'd have focused on that different subject matter and gotten a chance to be explicitly wrong about it. I mainly see my ur-failure here as letting myself be influenced by the whole audience that was nodding along very seriously to Robin's arguments, at the expense of considering how reality might depart in either direction from my own beliefs, and not just how Robin might be right or how to persuade the audience.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:19]</strong>&nbsp;</p><p>Also, \"throwing 10,000 TPUs at something and actually getting progress\" doesn't seem like an example of the Law of Earlier Failure - if anything it seems like an Earlier Success</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:19]</strong>&nbsp;</p><p>it's an Earlier Failure of Robin's arguments about why AI wouldn't scale quickly, so my lack of awareness of this case of the Law of Earlier Failure is why I didn't consider why Robin's arguments could fail earlier</p><p>though, again, this is a bit harder to call if you're trying to call it in 2008 instead of 2018</p><p>but it's a valid lesson that the future is, in fact, hard to predict, if you're trying to do it in the past</p><p>and I would not consider it a merely \"semantic\" difference as to whether you made a wrong argument about the correct subject matter, or a correct argument about the wrong subject matter</p><p>these are like... <i>very</i> different failure modes that you learn different lessons from</p><p>but if you're not excited by these particular fine differences in failure modes or lessons to learn from them, we should perhaps not dwell upon that part of the meta-level Art</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:21]</strong>&nbsp;</p><p>Okay, so let me see if I understand your position here.</p><p>Due to the deep learning revolution, it turned out that there were ways to get powerful capabilities without RSI. This isn't intrinsically a (strong) strike against the RSI abstraction; and so, unless we have reason to expect another similarly surprising revolution before reaching AGI, it's not a good reason to doubt the consequentialism abstraction.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:25]</strong>&nbsp;</p><p>Consequentialism and RSI are very different notions in the first place. Consequentialism is, in my own books, significantly simpler. I don't see much of a conceptual connection between the two myself, except insofar as they both happen to be part of the connected fabric of a coherent worldview about cognition.</p><p>It is entirely reasonable to suspect that we may get another surprising revolution before reaching AGI. Expecting a <i>particular</i> revolution that gives you <i>particular</i> miraculous benefits is much more questionable and is an instance of conjuring expected good from nowhere, like hoping that you win the lottery because the first lottery ball comes up 37. (Also, if you sincerely believed you actually had info about what kind of revolution might lead to AGI, you should shut up about it and tell very few carefully selected people, not bake it into a public dialogue.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:28]</strong>&nbsp;</p><blockquote><p>and I would not consider it a merely \"semantic\" difference as to whether you made a wrong argument about the correct subject matter, or a correct argument about the wrong subject matter</p></blockquote><p>On this point: the implicit premise of \"and also nothing else will break this abstraction or render it much less relevant\" turns a correct argument about the wrong subject matter into an incorrect argument.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:28]</strong>&nbsp;</p><p>Sure.</p><p>Though I'd also note that there's an important lesson of technique where you learn to say things like that out loud instead of keeping them \"implicit\".</p><p>Learned lessons like that are one reason why I go through your summary documents of our conversation and ask for many careful differences of wording about words like \"will happen\" and so on.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:30]</strong>&nbsp;</p><p>Makes sense.</p><p>So I claim that:</p><p>1. A premise like this is necessary for us to believe that your claims about consequentialism lead to extinction.</p><p>2. A surprising revolution would make it harder to believe this premise, even if we don't know which <i>particular</i> revolution it is.</p><p>3. If we'd been told back in 2008 that a surprising revolution would occur in AI, then we should have been less confident in the importance of the RSI abstraction to understanding AGI and AGI risk.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:32][11:34]</strong>&nbsp;</p><p>Suppose I put to you that this claim is merely subsumed by all of my previous careful qualifiers about how we might get a \"miracle\" and how we should be trying to prepare for an unknown miracle in any number of places. Why suspect that place particularly for a model-violation?</p><p>I also think that you are misinterpreting my old arguments about RSI, in a pattern that matches some other cases of your summarizing my beliefs as \"X is the one big ultra-central thing\" rather than \"X is the point where the other person got stuck and Eliezer had to spend a lot of time arguing\".</p><p>I was always claiming that RSI was <i>a</i> way for AGI capabilities to scale much further <i>once they got far enough</i>, not <i>the</i> way AI would scale <i>to human-level generality</i>.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\">This continues to be a key fact of relevance to my future model, in the form of the unfalsified original argument about the subject matter it previously applied to: if you lose control of a sufficiently smart AGI, it will FOOM, and this fact about what triggers the metaphorical equivalent of a full nuclear exchange and a total loss of the gameboard continues to be extremely relevant to what you have to do to obtain victory instead.</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:34][11:35]</strong>&nbsp;</p><p>Perhaps we're interpreting the word \"miracle\" in quite different ways.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\">I think of it as an event with negligibly small probability.</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:35]</strong>&nbsp;</p><p>Events that actually have negligibly small probability are not much use in plans.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:35]</strong>&nbsp;</p><p>Which I guess doesn't fit with your claims that we should be trying to prepare for a miracle.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:35]</strong>&nbsp;</p><p>Correct.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:35]</strong>&nbsp;</p><p>But I'm not recalling off the top of my head where you've claimed that.</p><p>I'll do a quick search of the transcript</p><p>\"You need to hold your mind open for any miracle and a miracle you didn't expect or think of in advance, because at this point our last hope is that in fact the future is often quite surprising.\"</p><p>Okay, I see. The connotations of \"miracle\" seemed sufficiently strong to me that I didn't interpret \"you need to hold your mind open\" as practical advice.</p><p>What sort of probability, overall, do you assign to us being saved by what you call a miracle?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:40]</strong>&nbsp;</p><p>It's not a place where I find quantitative probabilities to be especially helpful.</p><p>And if I had one, I suspect I would not publish it.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:41]</strong>&nbsp;</p><p>Can you leak a bit of information? Say, more or less than 10%?</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:41]</strong>&nbsp;</p><p>Less.</p><p>Though a lot of that is dominated, not by the probability of a positive miracle, but by the extent to which we seem unprepared to take advantage of it, and so would not be saved by one.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><p><strong>[Ngo][11:41]</strong>&nbsp;</p><p>Yeah, I see.</p></td></tr></tbody></table>\n\n5.2. The idea of expected utility\n---------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:43]</strong>&nbsp;</p><p>Okay, I'm now significantly less confident about how much we actually disagree.</p><p>At least about the issues of AI cognition.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:44]</strong>&nbsp;</p><p>You seem to suspect we'll get a <i>particular</i> miracle having to do with \"consequentialism\", which means that although it might be a miracle to me, it wouldn't be a miracle to you.</p><p>There is something forbidden in my model that is not forbidden in yours.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:45]</strong>&nbsp;</p><p>I think that's partially correct, but I'd call it more a <i>broad range of possibilities</i> in the rough direction of you being wrong about consequentialism.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:46]</strong>&nbsp;</p><p>Well, as much as it may be nicer to debate when the other person has a specific positive expectation that X will work, we can also debate when I know that X won't work and the other person remains ignorant of that. So say more!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:47]</strong>&nbsp;</p><p>That's why I've mostly been trying to clarify your models rather than trying to make specific claims of my own.</p><p>Which I think I'd prefer to continue doing, if you're amenable, by asking you about what entities a utility function is defined over - say, in the context of a human.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:51][11:53]</strong>&nbsp;</p><p>I think that to contain the concept of Utility as it exists in me, you would have to do homework exercises I don't know how to prescribe. Maybe one set of homework exercises like that would be showing you an agent, including a human, making some set of choices that allegedly couldn't obey expected utility, and having you figure out how to pump money from that agent (or present it with money that it would pass up).</p><p>Like, just actually doing that a few dozen times.</p><p>Maybe it's not helpful for me to say this? If you say it to Eliezer, he immediately goes, \"Ah, yes, I could see how I would update that way after doing the homework, so I will save myself some time and effort and just make that update now without the homework\", but this kind of jumping-ahead-to-the-destination is something that seems to me to be... dramatically missing from many non-Eliezers. They insist on learning things the hard way and then act all surprised when they do. Oh my gosh, who would have thought that an AI breakthrough would suddenly make AI seem less than 100 years away the way it seemed yesterday? Oh my gosh, who would have thought that alignment would be difficult?</p><p>Utility can be seen as the origin of Probability within minds, even though Probability obeys its own, simpler coherence constraints.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><p>that is, you will have money pumped out of you, unless you weigh in your mind paths through time according to some quantitative weight, which determines how much resources you're willing to spend on preparing for them</p><p>this is why sapients think of things as being more or less likely</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:53]</strong>&nbsp;</p><p>Suppose that this agent has some high-level concept - say, honour - which leads it to pass up on offers of money.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:55]</strong>&nbsp;</p><blockquote><p>Suppose that this agent has some high-level concept - say, honour - which leads it to pass up on offers of money.</p></blockquote><p>then there's two possibilities:</p><ul><li>this concept of honor is something that you can see as helping to navigate a path through time to a destination</li><li>honor isn't something that would be optimized into existence by optimization pressure for other final outcomes</li></ul></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][11:55]</strong>&nbsp;</p><p>Right, I see.</p><p>Hmm, but it seems like humans often don't see concepts as helping to navigate a path in time to a destination. (E.g. the deontological instinct not to kill.)</p><p>And yet those concepts were in fact optimised into existence by evolution.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][11:59]</strong>&nbsp;</p><p>You're describing a defect of human reflectivity about their consequentialist structure, not a departure from consequentialist structure. 🙂</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:01]</strong>&nbsp;</p><p>(Sorry, internet was slightly buggy; switched to a better connection now.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:01]</strong>&nbsp;</p><p>But yes, from my perspective, it creates a very large conceptual gap that I can stare at something for a few seconds and figure out how to parse it as navigating paths through time, while others think that \"consequentialism\" only happens when their minds are explicitly thinking about \"well, what would have this consequence\" using language.</p><p>Similarly, when it comes to Expected Utility, I see that any time something is attaching relative-planning-weights to paths through time, not when a human is thinking out loud about putting spoken numbers on outcomes</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:02]</strong>&nbsp;</p><p>Human consequentialist structure was optimised by evolution for a different environment. Insofar as we are consequentialists in a new environment, it's only because we're able to be reflective about our consequentialist structure (or because there are strong similarities between the environments).</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:02]</strong>&nbsp;</p><p>False.</p><p>It just generalized out-of-distribution because the underlying coherence of the coherent behaviors was simple.</p><p>When you have a very simple pattern, it can generalize across weak similarities, not \"strong similarities\".</p><p>The human brain is large but the coherence in it is simple.</p><p>The idea, the structure, that explains why the big thing works, is much smaller than the big thing.</p><p>So it can generalize very widely.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:04]</strong>&nbsp;</p><p>Taking this example of the instinct not to kill people - is this one of the \"very simple patterns\" that you're talking about?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:05]</strong>&nbsp;</p><p>\"Reflectivity\" doesn't help per se unless on some core level a pattern already generalizes, I mean, either a truth can generalize across the data or it can't? So I'm a bit puzzled about why you're bringing up \"reflectivity\" in this context.</p><p>And, no.</p><p>An instinct not to kill doesn't even seem to me like a plausible cross-cultural universal. 40% of deaths among Yanomami men are in intratribal fights, iirc.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:07]</strong>&nbsp;</p><p>Ah, I think we were talking past each other. When you said \"this concept of honor is something that you can see as helping to navigate a path through time to a destination\" I thought you meant \"you\" as in the agent in question (as you used it in some previous messages) not \"you\" as in a hypothetical reader.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:07]</strong>&nbsp;</p><p>ah.</p><p>it would not have occurred to me to ascribe that much competence to an agent that wasn't a superintelligence.</p><p>even I don't have time to think about why more than <s>0.0001%</s> 0.01% of my thoughts do anything, but thankfully, you don't have to think about <i>why</i> 2 + 2 = 4 for it to be the correct answer for counting sheep.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:10]</strong>&nbsp;</p><p>Got it.</p><p>I might now try to throw a high-level (but still inchoate) disagreement at you and see how that goes. But while I'm formulating that, I'm curious what your thoughts are on where to take the discussion.</p><p>Actually, let's spend a few minutes deciding where to go next, and then take a break</p><p>I'm thinking that, at this point, there might be more value in moving onto geopolitics</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:19]</strong>&nbsp;</p><p>Some of my current thoughts are a reiteration of old despair: It feels to me like the typical Other within EA has no experience with discovering unexpected order, with operating a generalization that you can expect will cover new cases even when that isn't immediately obvious, with operating that generalization to cover those new cases correctly, with seeing simple structures that generalize a lot and having that be a real and useful and technical experience; instead of somebody blathering in a non-expectation-constraining way about how \"capitalism is responsible for everything wrong with the world\", and being able to extend that to lots of cases.</p><p>I could try to use much simpler language in hopes that people actually <a href=\"https://v.cx/2010/04/feynman-brazil-education\">look-at-the-water</a> Feynman-style, like \"navigating a path through time\" instead of Consequentialism which is itself a step down from Expected Utility.</p><p>But you actually do lose something when you throw away the more technical concept. And then people still think that either you instantly see in the first second how something is a case of \"navigating a path through time\", or that this is something that people only do explicitly when visualizing paths through time using that mental terminology; or, if Eliezer says that it's \"navigating time\" anyways, this must be an instance of Eliezer doing that thing other people do when they talk about how \"Capitalism is responsible for all the problems of the world\". They have no experience operating genuinely useful, genuinely deep generalizations that extend to nonobvious things.</p><p>And in fact, being able to operate some generalizations like that is a lot of how I know what I know, in reality and in terms of the original knowledge that came before trying to argue that knowledge with people. So trying to convey the real source of the knowledge feels doomed. It's a kind of idea that our civilization has lost, like that college class Feynman ran into.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][12:19]</strong>&nbsp;</p><p>My own sense (having been back for about 20min) is that one of the key cruxes is in \"is it possible that non-scary cognition will be able to end the acute risk period\", or perhaps \"should we expect a longish regime of pre-scary cognition, that we can study and learn to align in such a way that by the time we get scary cognition we can readily align it\".</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:19]</strong>&nbsp;</p><p>Some potential prompts for that:</p><ul><li>what are some scary things which might make governments take AI more seriously than they took covid, and which might happen before AGI</li><li>how much of a bottleneck in your model is governmental competence? and how much of a difference do you see in this between, say, the US and China?</li></ul></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][12:20]</strong>&nbsp;</p><p>I also have a bit of a sense that there's a bit more driving to do on the \"perhaps EY is just wrong about the applicability of the consequentialism arguments\" (in a similar domain), and would be happy to try articulating a bit of what I think are the not-quite-articulated-to-my-satisfaction arguments on that side.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:21]</strong>&nbsp;</p><p>I also had a sense - maybe mistaken - that RN did have some <i>specific</i> ideas about how \"consequentialism\" might be inapplicable. though maybe I accidentally refuted that in passing because the idea was \"well, what if it didn't know what consequentialism was?\" and then I explained that reflectivity was not required to make consequentialism generalize. but if so, I'd like RN to say explicitly what specific idea got refuted that way. or failing that, talk about the specific idea that didn't get refuted.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:23]</strong>&nbsp;</p><p>That wasn't my objection, but I do have some more specific ideas, which I could talk about.</p><p>And I'd also be happy for Nate to try articulating some of the arguments he mentioned above.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:23]</strong>&nbsp;</p><p>I have a general worry that this conversation has gotten too general, and that it would be more productive, even of general understanding, to start from specific ideas and shoot those down specifically.</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:26]</strong>&nbsp;</p><p>The other thing is that, for pedagogical purposes, I think it'd be useful for you to express some of your beliefs about how governments will respond to AI</p><p>I think I have a rough guess about what those beliefs are, but even if I'm right, not everyone who reads this transcript will be</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:28]</strong>&nbsp;</p><p>Why would I be expected to know <i>that</i>? I could talk about weak defaults and iterate through an unending list of possibilities.</p><p>Thinking that Eliezer thinks he knows that to any degree of specificity feels like I'm being weakmanned!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:28]</strong>&nbsp;</p><p>I'm not claiming you have any specific beliefs</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:29]</strong>&nbsp;</p><p>I suppose I have skepticism when other people dream up elaborately positive and beneficial reactions apparently drawn from some alternate nicer political universe that had an absolutely different response to Covid-19, and so on.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][12:29]</strong>&nbsp;</p><p>But I'd guess that your models rule out, for instance, the US and China deeply cooperating on AI before it's caused any disasters</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:30]</strong>&nbsp;</p><p>\"Deeply\"? Sure. That sounds like something that has never happened, and I'm generically skeptical about political things that go better than any political thing has ever gone before.</p><p>I guess we could talk about that? It doesn't seem like the most productive area, but maybe it lies upstream of more technical disagreements because we disagree about what AGI would actually have to do to have the world not end.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][12:31]</strong>&nbsp;</p><p>Cool. I claim it's time for a break, and then I nominate a little Eliezer gov't-response-overview followed by specific maybe-consequentialism-based-worries-aren't-a-problem-in-practice ideas from Richard.</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][12:32]</strong>&nbsp;</p><p>See you in 28mins</p></td></tr></tbody></table>\n\n5.3. Epistemology, and assessing the idea of expected utility\n-------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:03]</strong>&nbsp;</p><blockquote><p>Some of my current thoughts are a reiteration of old despair: It feels to me like the typical Other within EA has no experience with discovering unexpected order, with operating a generalization that you can expect will cover new cases even when that isn't immediately obvious, with operating that generalization to cover those new cases correctly, with seeing simple structures that generalize a lot and having that be a real and useful and technical experience; instead of somebody blathering in a non-expectation-constraining way about how \"capitalism is responsible for everything wrong with the world\", and being able to extend that to lots of cases.</p><p>I could try to use much simpler language in hopes that people actually look-at-the-water Feynman-style, like \"navigating a path through time\" instead of Consequentialism which is itself a step down from Expected Utility.</p><p>But you actually do lose something when you throw away the more technical concept. And then people still think that either you instantly see in the first second how something is a case of \"navigating a path through time\", or that this is something that people only do explicitly when visualizing paths through time using that mental terminology; or, if Eliezer says that it's \"navigating time\" anyways, this must be an instance of Eliezer doing that thing other people do when they talk about how \"Capitalism is responsible for all the problems of the world\". They have no experience operating genuinely useful, genuinely deep generalizations that extend to nonobvious things.</p><p>And in fact, being able to operate some generalizations like that is a lot of how I know what I know, in reality and in terms of the original knowledge that came before trying to argue that knowledge with people. So trying to convey the real source of the knowledge feels doomed. It's a kind of idea that our civilization has lost, like that college class Feynman ran into.</p></blockquote><p>Ooops, didn't see this comment earlier. With respect to discovering unexpected order, one point that seems relevant is the extent to which that order provides predictive power. To what extent do you think that predictive successes in economics are important evidence for expected utility theory being a powerful formalism? (Or are there other ways in which it's predictively powerful that provide significant evidence?)</p><p>I'd be happy with a quick response to that, and then on geopolitics, here's a prompt to kick us off:</p><ul><li>If the only two actors involved in AGI development were the US and the UK governments, how much safer (or less safe) would you think we were compared with a world in which the two actors are the US and Chinese governments? How about a world in which the US government was a decade ahead of everyone else in reaching AGI?</li></ul></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:06]</strong>&nbsp;</p><p>I think that the Apollo space program is much deeper evidence for Utility. Observe, if you train protein blobs to run around the savanna, they also go to the moon!</p><p>If you think of \"utility\" as having something to do with the human discipline called \"economics\" then you are still thinking of it in a <i>much much much</i> more narrow way than I do.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:07]</strong>&nbsp;</p><p>I'm not asking about evidence for utility as an abstraction in general, I'm asking for evidence based on successful predictions that have been made using it.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:10]</strong>&nbsp;</p><p>That doesn't tend to happen a lot, because all of the deep predictions that it makes are covered by shallow predictions that people made earlier.</p><p>Consider the following prediction of evolutionary psychology: Humans will enjoy activities associated with reproduction!</p><p>\"What,\" says Simplicio, \"you mean like dressing up for dates? I don't enjoy that part.\"</p><p>\"No, you're overthinking it, we meant orgasms,\" says the evolutionary psychologist.</p><p>\"But I already knew that, that's just common sense!\" replies Simplicio.</p><p>\"And yet it is very specifically a prediction of evolutionary psychology which is not made specifically by any other theory of human minds,\" replies the evolutionary psychologist.</p><p>\"Not an advance prediction, just-so story, too obvious,\" replies Simplicio.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:11]</strong>&nbsp;</p><p>Yepp, I agree that most of its predictions won't be new. Yet evolution is a sufficiently powerful theory that people have still come up with a range of novel predictions that derive from it.</p><p>Insofar as you're claiming that expected utility theory is also very powerful, then we should expect that it also provides some significant predictions.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:12]</strong>&nbsp;</p><p>An advance prediction of the notion of Utility, I suppose, is that if you train an AI which is otherwise a large blob of layers - though this may be inadvisable for other reasons - to the point where it starts solving lots of novel problems, that AI will tend to value aspects of outcomes with weights, and weight possible paths through time (the dynamic progress of the environment), and use (by default, usually, roughly) the multiplication of these weights to allocate limited resources between mutually conflicting plans.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:13]</strong>&nbsp;</p><p>Again, I'm asking for evidence in the form of successful predictions.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:14]</strong>&nbsp;</p><p>I predict that people will want some things more than others, think some possibilities are more likely than others, and prefer to do things that lead to stuff they want a lot through possibilities they think are very likely!</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:15]</strong>&nbsp;</p><p>It would be very strange to me if a theory which makes such strong claims about things we can't yet verify can't shed light on <i>anything</i> which we are in a position to verify.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:15]</strong>&nbsp;</p><p>If you think I'm deriving my predictions of catastrophic alignment failure through something <i>more exotic</i> than that, you're missing the reason <i>why I'm so worried</i>. It doesn't <i>take</i> intricate complicated exotic assumptions.</p><p>It makes the same kind of claims about things we can't verify yet as it makes about things we can verify right now.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:16]</strong>&nbsp;</p><p>But that's very easy to do! Any theory can do that.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:17]</strong>&nbsp;</p><p>For example, if somebody wants money, and you set up a regulation which prevents them from making money, it predicts that the person will look for a new way to make money that bypasses the regulation.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:17]</strong>&nbsp;</p><p>And yes, of course fitting previous data is important evidence in favour of a theory</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:17]</strong>&nbsp;</p><blockquote><p>[But that's very easy to do! Any theory can do that.]</p></blockquote><p>False! Any theory can do that in the hands of a fallible agent which invalidly, incorrectly derives predictions from the theory.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:18]</strong>&nbsp;</p><p>Well, indeed. But the very point at hand is whether the predictions you base on this theory are correctly or incorrectly derived.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:18]</strong>&nbsp;</p><p>It is not the case that every theory does an equally good job of predicting the past, given valid derivations of predictions.</p><p>Well, hence the analogy to evolutionary psychology. If somebody doesn't see the blatant obviousness of how sexual orgasms are a prediction specifically of evolutionary theory, because it's \"common sense\" and \"not an advance prediction\", what are you going to do? We can, in this case, with a <i>lot</i> more work, derive more detailed advance predictions about degrees of wanting that correlate in detail with detailed fitness benefits. But that's not going to convince anybody who overlooked the really blatant and obvious primary evidence.</p><p>What they're missing there is a sense of counterfactuals, of how the universe could just as easily have looked if the evolutionary origins of psychology were false: why should organisms want things associated with reproduction, why not instead have organisms running around that want things associated with rolling down hills?</p><p>Similarly, if optimizing complicated processes for outcomes hard enough, didn't produce cognitive processes that internally mapped paths through time and chose actions conditional on predicted outcomes, human beings would... not think like that? What am I supposed to say here?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:24]</strong>&nbsp;</p><p>Let me put it this way. There are certain traps that, historically, humans have been very liable to fall into. For example, seeing a theory, which seems to match so beautifully and elegantly the data which we've collected so far, it's very easy to dramatically overestimate how much that data favours that theory. Fortunately, science has a very powerful social technology for avoiding this (i.e. making falsifiable predictions) which seems like approximately the only reliable way to avoid it - and yet you don't seem concerned at all about the lack of application of this technology to expected utility theory.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:25]</strong>&nbsp;</p><p>This is territory I covered in the Sequences, exactly because \"well it didn't make a good enough advance prediction yet!\" is an excuse that people use to reject evolutionary psychology, some other stuff I covered in the Sequences, and some very predictable lethalities of AGI.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:26]</strong>&nbsp;</p><p>With regards to evolutionary psychology: yes, there are some blatantly obvious ways in which it helps explain the data available to us. But there are also many people who have misapplied or overapplied evolutionary psychology, and it's very difficult to judge whether they have or have not done so, without asking them to make advance predictions.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:26]</strong>&nbsp;</p><p>I talked about the downsides of allowing humans to reason like that, the upsides, the underlying theoretical laws of epistemology (which are clear about why agents that reason validly or just unbiasedly would do that without the slightest hiccup), etc etc.</p><p>In the case of the theory \"people want stuff relatively strongly, predict stuff relatively strongly, and combine the strengths to choose\", what kind of advance prediction that no other theory could possibly make, do you expect that theory to make?</p><p>In the worlds where that theory is true, how should it be able to prove itself to you?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:28]</strong>&nbsp;</p><p>I expect deeper theories to make more and stronger predictions.</p><p>I'm currently pretty uncertain if expected utility theory is a deep or shallow theory.</p><p>But deep theories tend to shed light in all sorts of unexpected places.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:30]</strong>&nbsp;</p><p>The fact is, when it comes to AGI (general optimization processes), we have only two major datapoints in our dataset, natural selection and humans. So you can either try to reason validly about what theories predict about natural selection and humans, even though we've already seen the effects of those; or you can claim to give up in great humble <a href=\"https://equilibriabook.com/inadequacy-and-modesty/\">modesty</a> while actually using other implicit theories instead to make all your predictions and be confident in them.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:30]</strong>&nbsp;</p><blockquote><p>I talked about the downsides of allowing humans to reason like that, the upsides, the underlying theoretical laws of epistemology (which are clear about why agents that reason validly or just unbiasedly would do that without the slightest hiccup), etc etc.</p></blockquote><p>I'm familiar with your writings on this, which is why I find myself surprised here. I could understand a perspective of \"yes, it's unfortunate that there are no advanced predictions, it's a significant weakness, I wish more people were doing this so we could better understand this vitally important theory\". But that seems very different from your perspective here.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:32]</strong>&nbsp;</p><p>Oh, I'd love to be making predictions using a theory that made super detailed advance predictions made by no other theory which had all been borne out by detailed experimental observations! I'd also like ten billion dollars, a national government that believed everything I honestly told them about AGI, and a drug that raises IQ by 20 points.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:32]</strong>&nbsp;</p><p>The very fact that we have only two major datapoints is exactly why it seems like such a major omission that a theory which purports to describe intelligent agency has not been used to make any successful predictions about the datapoints we do have.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:32][13:33]</strong>&nbsp;</p><p>This is making me think that you imagine the theory as something much more complicated and narrow than it is.</p><p>Just look at the water.</p><p>Not very special water with an index.</p><p>Just regular water.</p><p>People want stuff. They want some things more than others. When they do stuff they expect stuff to happen.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\">These are <i>predictions of the theory</i>. Not advance predictions, but predictions nonetheless.</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:33][13:33]</strong>&nbsp;</p><p>I'm accepting your premise that it's something deep and fundamental, and making the claim that deep, fundamental theories are likely to have a wide range of applications, including ones we hadn't previously thought of.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\">Do you disagree with that premise, in general?</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:36]</strong>&nbsp;</p><p>I don't know what you really mean by \"deep fundamental theory\" or \"wide range of applications we hadn't previously thought of\", especially when it comes to structures that are this simple. It sounds like you're still imagining something I mean by Expected Utility which is some narrow specific theory like a particular collection of gears that are appearing in lots of places.</p><p>Are numbers a deep fundamental theory?</p><p>Is addition a deep fundamental theory?</p><p>Is probability a deep fundamental theory?</p><p>Is the notion of the syntax-semantics correspondence in logic and the notion of a generally semantically valid reasoning step, a deep fundamental theory?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:38]</strong>&nbsp;</p><p>Yes to the first three, all of which led to very successful novel predictions.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:38]</strong>&nbsp;</p><p>What's an example of a novel prediction made by the notion of probability?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:38]</strong>&nbsp;</p><p>Most applications of the central limit theorem.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:39]</strong>&nbsp;</p><p>Then I should get to claim every kind of optimization algorithm which used expected utility, as a successful advance prediction of expected utility? Optimal stopping and all the rest? Seems cheap and indeed invalid to me, and not particularly germane to whether these things appear inside AGIs, but if that's what you want, then sure.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:39]</strong>&nbsp;</p><blockquote><p>These are <i>predictions of the theory</i>. Not advance predictions, but predictions nonetheless.</p></blockquote><p>I agree that it is a prediction of the theory. And yet it's also the case that smarter people than either of us have been dramatically mistaken about how well theories fit previously-collected data. (Admittedly we have advantages which they didn't, like a better understanding of cognitive biases - but it seems like you're ignoring the possibility of those cognitive biases applying to us, which largely negates those advantages.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:42]</strong>&nbsp;</p><p>I'm not ignoring it, just adjusting my confidence levels and proceeding, instead of getting stuck in an infinite epistemic trap of self-doubt.</p><p>I don't live in a world where you either have the kind of detailed advance experimental predictions that should convince the most skeptical scientist and render you immune to all criticism, or, alternatively, you are suddenly in a realm beyond the reach of all epistemic authority, and you ought to cuddle up into a ball and rely only on wordless intuitions and trying to put equal weight on good things happening and bad things happening.</p><p>I live in a world where I proceed with very strong confidence if I have a detailed formal theory that made detailed correct advance predictions, and otherwise go around saying, \"well, it sure looks like X, but we can be on the lookout for a miracle too\".</p><p>If this was a matter of thermodynamics, I wouldn't even be talking like this, and we wouldn't even be having this debate.</p><p>I'd just be saying, \"Oh, that's a perpetual motion machine. You can't build one of those. Sorry.\" And that would be the end.</p><p>Meanwhile, political superforecasters go on making well-calibrated predictions about matters much murkier and more complicated than these, often without anything resembling a clearly articulated theory laid forth at length, let alone one that had made specific predictions even retrospectively. They just go do it instead of feeling helpless about it.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:45]</strong>&nbsp;</p><blockquote><p>Then I should get to claim every kind of optimization algorithm which used expected utility, as a successful advance prediction of expected utility? Optimal stopping and all the rest? Seems cheap and indeed invalid to me, and not particularly germane to whether these things appear inside AGIs, but if that's what you want, then sure.</p></blockquote><p>These seem better than nothing, but still fairly unsatisfying, insofar as I think they are related to more shallow properties of the theory.</p><p>Hmm, I think you're mischaracterising my position. I nowhere advocated for feeling helpless or curling up in a ball. I was just noting that this is a particularly large warning sign which has often been valuable in the past, and it seemed like you were not only speeding past it blithely, but also denying the existence of this category of warning signs.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:48]</strong>&nbsp;</p><p>I think you're looking for some particular kind of public obeisance that I don't bother to perform internally because I'd consider it a wasted motion. If I'm lost in a forest I don't bother going around loudly talking about how I need a forest theory that makes detailed advance experimental predictions in controlled experiments, but, alas, I don't have one, so now I should be very humble. I try to figure out which way is north.</p><p>When I have a guess at a northerly direction, it would then be an error to proceed with as much confidence as if I'd had a detailed map and had located myself upon it.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:49]</strong>&nbsp;</p><p>Insofar as I think we're less lost than you do, then the weaknesses of whichever forest theory implies that we're lost are relevant for this discussion.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:49]</strong>&nbsp;</p><p>The obeisance I make in that direction is visible in such statements as, \"But this, of course, is a prediction about the future, which is well-known to be quite difficult to predict, in fact.\"</p><p>If my statements had been matters of thermodynamics and particle masses, I would <i>not</i> be adding that disclaimer.</p><p>But most of life is not a statement about particle masses. I have some idea of how to handle that. I do not need to constantly recite disclaimers to myself about it.</p><p>I know how to proceed when I have only a handful of data points which have already been observed and my theories of them are retrospective theories. This happens to me on a daily basis, eg when dealing with human beings.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][13:50]</strong>&nbsp;</p><p>(I have a bit of a sense that we're going in a circle. It also seems to me like there's some talking-past happening.)</p><p>(I suggest a 5min break, followed by EY attempting to paraphrase RN to his satisfaction and vice versa.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:51]</strong>&nbsp;</p><p>I'd have more trouble than usual paraphrasing RN because epistemic helplessness is something I find painful to type out.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][13:51]</strong>&nbsp;</p><p>(I'm also happy to attempt to paraphrase each point as I see it; it may be that this smooths over some conversational wrinkle.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:52]</strong>&nbsp;</p><p>Seems like a good suggestion. I'm also happy to move on to the next topic. This was meant to be a quick clarification.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][13:52]</strong>&nbsp;</p><p><i>nod</i>. It does seem to me like it possibly contains a decently sized meta-crux, about what sorts of conclusions one is licensed to draw from what sorts of observations</p><p>that, eg, might be causing Eliezer's probabilities to concentrate but not Richard's.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:52]</strong>&nbsp;</p><p>Yeah, this is in the opposite direction of \"more specificity\".</p><figure class=\"table\"><table><tbody><tr><td>[Soares: 😝]</td><td>[Ngo: 😆]</td></tr></tbody></table></figure><p>I frankly think that most EAs suck at explicit epistemology, OpenPhil and FHI affiliated EAs are not much of an exception to this, and I expect I will have more luck talking people out of specific errors than talking them out of the infinite pit of humble ignorance considered abstractly.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][13:54]</strong>&nbsp;</p><p>Ok, that seems to me like a light bid to move to the next topic from both of you, my new proposal is that we take a 5min break and then move to the next topic, and perhaps I'll attempt to paraphrase each point here in my notes, and if there's any movement in the comments there we can maybe come back to it later.</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][13:54]</strong>&nbsp;</p><p>Broadly speaking I am also strongly against humble ignorance (albeit to a lesser extent than you are).</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][13:55]</strong>&nbsp;</p><p>I'm off to take a 5-minute break, then!</p></td></tr></tbody></table>\n\n5.4. Government response and economic impact\n--------------------------------------------\n\n<table><tbody><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:02]</strong>&nbsp;</p><p>A meta-level note: I suspect we're around the point of hitting significant diminishing marginal returns from this format. I'm open to putting more time into the debate (broadly construed) going forward, but would probably want to think a bit about potential changes in format.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:hsl(0, 0%, 0%)\"><p><strong>[Soares][14:04, moved two up in log]</strong>&nbsp;</p><blockquote><p>A meta-level note: I suspect we're around the point of hitting significant diminishing marginal returns from this format. I'm open to putting more time into the debate (broadly construed) going forward, but would probably want to think a bit about potential changes in format.</p></blockquote><p>(Noted, thanks!)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:03]</strong>&nbsp;</p><p>I actually think that may just be a matter of at least one of us, including Nate, having to take on the thankless job of shutting down all digressions into abstractions and the meta-level.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><p><strong>[Ngo][14:05]</strong>&nbsp;</p><blockquote><p>I actually think that may just be a matter of at least one of us, including Nate, having to take on the thankless job of shutting down all digressions into abstractions and the meta-level.</p></blockquote><p>I'm not so sure about this, because it seems like some of the abstractions are doing a lot of work.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][14:03][14:04]</strong>&nbsp;</p><p>Anyways, government reactions?</p><p>It seems to me like the best observed case for government reactions - which I suspect is no longer available in the present era as a possibility - was the degree of cooperation between the USA and Soviet Union about avoiding nuclear exchanges.</p><p>This included such incredibly extravagant acts of cooperation as installing a direct line between the President and Premier!</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><p>which is not what I would really characterize as very \"deep\" cooperation, but it's more than a lot of cooperation you see nowadays.</p><p>More to the point, both the USA and Soviet Union proactively avoided doing anything that might lead towards starting down a path that led to a full nuclear exchange.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:04]</strong>&nbsp;</p><p>The question I asked earlier:</p><ul><li>If the only two actors involved in AGI development were the US and the UK governments, how much safer (or less safe) would you think we were compared with a world in which the two actors are the US and Chinese governments? How about a world in which the US government was a decade ahead of everyone else in reaching AGI?</li></ul></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:05]</strong>&nbsp;</p><p>They still provoked one another a lot, but, whenever they did so, tried to do so in a way that wouldn't lead to a full nuclear exchange.</p><p>It was mutually understood to be a strategic priority and lots of people on both sides thought a lot about how to avoid it.</p><p>I don't know if that degree of cooperation ever got to the fantastic point of having people from <i>both</i> sides in the <i>same</i> room brainstorming <i>together</i> about how to avoid a full nuclear exchange, because that is, like, more cooperation than you would normally expect from two governments, but it wouldn't <i>shock</i> me to learn that this had ever happened.</p><p>It seems obvious to me that if some situation developed nowadays which increased the profile possibility of a nuclear exchange between the USA and Russia, we would not currently be able to do anything like installing a Hot Line between the US and Russian offices if such a Hot Line had not already been installed. This is lost social technology from a lost golden age. But still, it's not unreasonable to take this as the upper bound of attainable cooperation; it's been observed within the last 100 years.</p><p>Another guess for how governments react is a very simple and robust one backed up by a huge number of observations:</p><p>They don't.</p><p>They have the same kind of advance preparation and coordination around AGI, in advance of anybody getting killed, as governments had around the mortgage crisis of 2007 in advance of any mortgages defaulting.</p><p>I am not sure I'd put this probability over 50% but it's certainly by far the largest probability over any competitor possibility specified to an equally low amount of detail.</p><p>I would expect anyone whose primary experience was with government, who was just approaching this matter and hadn't been talked around to weird exotic views, to tell you the same thing as a matter of course.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:10]</strong>&nbsp;</p><blockquote><p>But still, it's not unreasonable to take this as the upper bound of attainable cooperation; it's been observed within the last 100 years.</p></blockquote><p>Is this also your upper bound conditional on a world that has experienced a century's worth of changes within a decade, and in which people are an order of magnitude wealthier than they currently are?</p><blockquote><p>I am not sure I'd put this probability over 50% but it's certainly by far the largest probability over any competitor possibility specified to an equally low amount of detail.</p></blockquote><p>which one was this? US/UK?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:12][14:14]</strong>&nbsp;</p><p>Assuming governments do react, we have the problem of \"What kind of heuristic could have correctly led us to forecast that the US's reaction to a major pandemic would be for the FDA to ban hospitals from doing in-house Covid tests? What kind of mental process could have led us to make that call?\" And we couldn't have gotten it exactly right, because the future is hard to predict; the best heuristic I've come up with, that feels like it at least would not have been <i>surprised</i> by what actually happened, is, \"The government will react with a flabbergasting level of incompetence, doing exactly the wrong thing, in some unpredictable specific way.\"</p><blockquote><p>which one was this? US/UK?</p></blockquote><p>I think if we're talking about any single specific government like the US or UK then the probability is over 50% that they don't react in any advance coordinated way to the AGI crisis, <i>to a greater and more effective degree</i> than they \"reacted in an advance coordinated way\" to pandemics before 2020 or mortgage defaults before 2007.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\">Maybe <i>some</i> two governments somewhere on Earth will have a high-level discussion between two cabinet officials.</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:14]</strong>&nbsp;</p><p>That's one lesson you could take away. Another might be: governments will be very willing to restrict the use of novel technologies, even at colossal expense, in the face of even a small risk of large harms.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:15]</strong>&nbsp;</p><blockquote><p>That's one lesson you could take away. Another might be: governments will be very willing to restrict the use of novel technologies, even at colossal expense, in the face of even a small risk of large harms.</p></blockquote><p>I just... don't know what to do when people talk like this.</p><p>It's so absurdly, absurdly optimistic.</p><p>It's taking a massive massive failure and trying to find exactly the right abstract gloss to put on it that makes it sound like exactly the right perfect thing will be done next time.</p><p>This just - isn't how to understand reality.</p><p>This isn't how superforecasters think.</p><p>This isn't <i>sane</i>.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][14:16]</strong>&nbsp;</p><p>(be careful about ad hominem)</p><p>(Richard might not be doing the insane thing you're imagining, to generate that sentence, etc)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:17]</strong>&nbsp;</p><p>Right, I'm not endorsing this as my mainline prediction about what happens. Mainly what I'm doing here is highlighting that your view seems like one which cherrypicks pessimistic interpretations.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:18]</strong>&nbsp;</p><p>That abstract description \"governments will be very willing to restrict the use of novel technologies, even at colossal expense, in the face of even a small risk of large harms\" does not in fact apply very well to the FDA banning hospitals from using their well-established in-house virus tests, at risk of the alleged harm of some tests giving bad results, when in fact the CDC's tests were giving bad results and much larger harms were on the way because of bottlenecked testing; and that abstract description should have applied to an effective and globally coordinated ban against gain-of-function research, which <i>didn't</i> happen.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:19]</strong>&nbsp;</p><p>Alternatively: what could have led us to forecast that many countries will impose unprecedentedly severe lockdowns.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:19][14:21][14:21]</strong>&nbsp;</p><p>Well, I didn't! I didn't even realize that was an option! I thought Covid was just going to rip through everything.</p><p>(Which, to be clear, it still may, and Delta arguably is in the more primitive tribal areas of the USA, as well as many other countries around the world that can't afford vaccines financially rather than epistemically.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\">But there's a really really basic lesson here about the different style of \"sentences found in political history books\" rather than \"sentences produced by people imagining ways future politics could handle an issue successfully\".</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\">Reality is <i>so much worse</i> than people imagining what might happen to handle an issue successfully.</td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:21][14:21][14:22]</strong>&nbsp;</p><p>I might nudge us away from covid here, and towards the questions I asked before.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><blockquote><p>The question I asked earlier:</p><ul><li>If the only two actors involved in AGI development were the US and the UK governments, how much safer (or less safe) would you think we were compared with a world in which the two actors are the US and Chinese governments? How about a world in which the US government was a decade ahead of everyone else in reaching AGI?</li></ul></blockquote><p>This being one.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><blockquote><p>\"But still, it's not unreasonable to take this as the upper bound of attainable cooperation; it's been observed within the last 100 years.\" Is this also your upper bound conditional on a world that has experienced a century's worth of changes within a decade, and in which people are an order of magnitude wealthier than they currently are?</p></blockquote><p>And this being the other.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:22]</strong>&nbsp;</p><blockquote><p>Is this also your upper bound conditional on a world that has experienced a century's worth of changes within a decade, and in which people are an order of magnitude wealthier than they currently are?</p></blockquote><p>I don't expect this to happen at all, or even come remotely close to happening; I expect AGI to kill everyone before self-driving cars are commercialized.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border-color:hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][16:29] &nbsp;(Nov. 14 follow-up comment)</strong>&nbsp;</p><p>(This was incautiously put; maybe strike \"expect\" and put in \"would not be the least bit surprised if\" or \"would very tentatively guess that\".)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:23]</strong>&nbsp;</p><p>ah, I see</p><p>Okay, maybe here's a different angle which I should have been using. What's the most impressive technology you expect to be commercialised before AGI kills everyone?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:24]</strong>&nbsp;</p><blockquote><p>If the only two actors involved in AGI development were the US and the UK governments, how much safer (or less safe) would you think we were compared with a world in which the two actors are the US and Chinese governments?</p></blockquote><p>Very hard to say; the UK is friendlier but less grown-up. We would obviously be VASTLY safer in any world where only two centralized actors (two effective decision processes) could ever possibly build AGI, though not safe / out of the woods / at over 50% survival probability.</p><blockquote><p>How about a world in which the US government was a decade ahead of everyone else in reaching AGI?</p></blockquote><p>Vastly safer and likewise impossibly miraculous, though again, not out of the woods at all / not close to 50% survival probability.</p><blockquote><p>What's the most impressive technology you expect to be commercialised before AGI kills everyone?</p></blockquote><p>This is incredibly hard to predict. If I actually had to predict this for some reason I would probably talk to Gwern and Carl Shulman. In principle, there's nothing preventing me from knowing something about Go which lets me predict in 2014 that Go will probably fall in two years, but in practice I did not do that and I don't recall anybody else doing it either. It's really quite hard to figure out how much cognitive work a domain requires and how much work known AI technologies can scale to with more compute, let alone predict AI breakthroughs.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:27]</strong>&nbsp;</p><p>I'd be happy with some very rough guesses</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:27]</strong>&nbsp;</p><p>If you want me to spin a scifi scenario, I would not be surprised to find online anime companions carrying on impressively humanlike conversations, because this is a kind of technology that can be deployed without major corporations signing on or regulatory approval.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:28]</strong>&nbsp;</p><p>Okay, this is surprising; I expected something more advanced.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:29]</strong>&nbsp;</p><p>Arguably AlphaFold 2 is already more advanced than that, along certain dimensions, but it's no coincidence that afaik people haven't really done much with AlphaFold 2 and it's made no visible impact on GDP.</p><p>I expect GDP not to depart from previous trendlines before the world ends, would be a more general way of putting it.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:29]</strong>&nbsp;</p><p>What's the <s>most</s> least impressive technology that your model strongly rules out happening before AGI kills us all?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:30]</strong>&nbsp;</p><p>you mean least impressive?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:30]</strong>&nbsp;</p><p>oops, yes</p><p>That seems like a structurally easier question to answer</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:30]</strong>&nbsp;</p><p>\"Most impressive\" is trivial. \"Dyson Spheres\" answers it.</p><p>Or, for that matter, \"perpetual motion machines\".</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:31]</strong>&nbsp;</p><p>Ah yes, I was thinking that Dyson spheres were a bit too prosaic</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:32]</strong>&nbsp;</p><p>My model mainly rules out that we get to certain points and then hang around there for 10 years while the technology gets perfected, commercialized, approved, adopted, ubiquitized enough to produce a visible trendline departure on the GDP graph; not so much various technologies themselves being initially demonstrated in a lab.</p><p>I expect that the people who build AGI can build a self-driving car if they want to. Getting it approved and deployed before the world ends is quite another matter.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:33]</strong>&nbsp;</p><p>OpenAI has commercialised GPT-3</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:33]</strong>&nbsp;</p><p>Hasn't produced much of a bump in GDP as yet.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:33]</strong>&nbsp;</p><p>I wasn't asking about that, though</p><p>I'm more interested in judging how hard you think it is for AIs to take over the world</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:34]</strong>&nbsp;</p><p>I note that it seems to me like there is definitely a kind of thinking here, which, if told about GPT-3 five years ago, would talk in very serious tones about how much this technology ought to be predicted to shift GDP, and whether we could bet on that.</p><p>By \"take over the world\" do you mean \"turn the world into paperclips\" or \"produce 10% excess of world GDP over predicted trendlines\"?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:35]</strong>&nbsp;</p><p>Turn world into paperclips</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:36]</strong>&nbsp;</p><p>I expect this mainly happens as a result of superintelligence, which is way up in the stratosphere far above the minimum required cognitive capacities to get the job done?</p><p>The interesting question is about humans trying to deploy a corrigible AGI thinking in a restricted domain, trying to flip the gameboard / \"take over the world\" without full superintelligence?</p><p>I'm actually not sure what you're trying to get at here.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][14:37]</strong>&nbsp;</p><p>(my guess, for the record, is that the crux Richard is attempting to drive for here, is centered more around something like \"will humanity spend a bunch of time in the regime where there are systems capable of dramatically increasing world GDP, and if not how can you be confident of that from here\")</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:38]</strong>&nbsp;</p><p>This is not the sort of thing I feel Confident about.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border-color:hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][16:31] &nbsp;(Nov. 14 follow-up comment)</strong>&nbsp;</p><p>(My confidence here seems understated. &nbsp;I am very pleasantly surprised if we spend 5 years hanging around with systems that can dramatically increase world GDP and those systems are actually being used for that. &nbsp;There isn't one dramatic principle which prohibits that, so I'm not Confident, but it requires multiple nondramatic events to go not as I expect.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:38]</strong>&nbsp;</p><p>Yeah, that's roughly what I'm going for. Or another way of putting it: we have some disagreements about the likelihood of humans being able to get an AI to do a pivotal act which saves the world. So I'm trying to get some estimates for what the hardest act you think humans <i>can</i> get an AI to do is.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][14:39]</strong>&nbsp;</p><p>(and that a difference here causes, eg, Richard to suspect the relevant geopolitics happen after a century of progress in 10y, everyone being suddenly much richer in real terms, and a couple of warning shots, whereas Eliezer expects the relevant geopolitics to happen the day after tomorrow, with \"realistic human-esque convos\" being the sort of thing we get in stead of warning shots)</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:40]</strong>&nbsp;</p><p>I mostly do not expect pseudo-powerful but non-scalable AI powerful enough to increase GDP, hanging around for a while. But if it happens then I don't feel I get to yell \"what happened?\" at reality, because there's an obvious avenue for it to happen: something GDP-increasing proved tractable to non-deeply-general AI systems.</p><p>where GPT-3 is \"not deeply general\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:40]</strong>&nbsp;</p><p>Again, I didn't ask about GDP increases, I asked about impressive acts (in order to separate out the effects of AI capabilities from regulatory effects, people-having-AI-but-not-using-it, etc).</p><p>Where you can use whatever metric of impressiveness you think is reasonable.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:42]</strong>&nbsp;</p><p>so there's two questions here, one of which is something like, \"what is the most impressive thing you can do while still being able to align stuff and make it corrigible\", and one of which is \"if there's an incorrigible AI whose deeds are being exhibited by fools, what impressive things might it do short of ending the world\".</p><p>and these are both problems that are hard for the same reason I did not predict in 2014 that Go would fall in 2016; it can in fact be quite hard - even with a domain as fully lawful and known as Go - to figure out which problems will fall to which level of cognitive capacity.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][14:43]</strong>&nbsp;</p><p>Nate's attempted rephrasing: EY's model might not be confident that there's not big GDP boosts, but it does seem pretty confident that there isn't some \"half-capable\" window between the shallow-pattern-memorizer stuff and the scary-laserlike-consequentialist stuff, and in particular Eliezer seems confident humanity won't slowly traverse that capability regime</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:43]</strong>&nbsp;</p><p>that's... allowed? I don't get to yell at reality if that happens?</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][14:44]</strong>&nbsp;</p><p>and (shakier extrapolation), that regime is where a bunch of Richard's hope lies (eg, in the beginning of that regime we get to learn how to do practical alignment, and also the world can perhaps be saved midway through that regime using non-laserlike-systems)</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:45]</strong>&nbsp;</p><p>so here's an example of a thing I don't think you can do without the world ending: get an AI to build a nanosystem or biosystem which can synthesize two strawberries identical down to the cellular but not molecular level, and put them on a plate</p><p>this is why I use this capability as the definition of a \"powerful AI\" when I talk about \"powerful AIs\" being hard to align, if I don't want to start by explicitly arguing about pivotal acts</p><p>this, I think, is going to end up being first doable using a laserlike world-ending system</p><p>so even if there's a way to do it with no lasers, that happens later and the world ends before then</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:47]</strong>&nbsp;</p><p>Okay, that's useful.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:48]</strong>&nbsp;</p><p>it feels like the critical bar there is something like \"invent a whole engineering discipline over a domain where you can't run lots of cheap simulations in full detail\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:49]</strong>&nbsp;</p><p>(Meta note: let's wrap up in 10 mins? I'm starting to feel a bit sleepy.)</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: 👍]</td><td>[Soares: 👍]</td></tr></tbody></table></figure><p>This seems like a pretty reasonable bar</p><p>Let me think a bit about where to go from that</p><p>While I'm doing so, since this question of takeoff speeds seems like an important one, I'm wondering if you could gesture at your biggest disagreement with this post:<a href=\"https://sideways-view.com/2018/02/24/takeoff-speeds/\"> <u>https://sideways-view.com/2018/02/24/takeoff-speeds/</u></a></p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:51]</strong>&nbsp;</p><p>Oh, also in terms of scifi possibilities, I can imagine seeing 5% GDP loss because text transformers successfully scaled to automatically filing lawsuits and environmental impact objections.</p><p>My read on the entire modern world is that GDP is primarily constrained by bureaucratic sclerosis rather than by where the technological frontiers lie, so AI ends up impacting GDP mainly insofar as it allows new ways to bypass regulatory constraints, rather than insofar as it allows new technological capabilities. I expect a sudden transition to paperclips, not just because of how fast I expect cognitive capacities to scale over time, but because nanomachines eating the biosphere bypass regulatory constraints, whereas earlier phases of AI will not be advantaged relative to all the other things we have the technological capacity to do but which aren't legal to do.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border-color:hsl(0, 0%, 0%)\"><p><strong>[Shah][12:13]&nbsp; (Sep. 21 follow-up comment)</strong>&nbsp;</p><blockquote><p>My read on the entire modern world is that GDP is primarily constrained by bureaucratic sclerosis rather than by where the technological frontiers lie</p></blockquote><p>This is a fair point and updates me somewhat towards fast takeoff as operationalized by Paul, though I'm not sure how much it updates me on p(doom).</p><p>Er, wait, really fast takeoff as operationalized by Paul makes less sense as a thing to be looking for -- presumably we die before any 1 year doubling. Whatever, it updates me somewhat towards \"less deployed stuff before scary stuff is around\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:56]</strong>&nbsp;</p><p>Ah, interesting. What are the two or three main things in that category?</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:57]</strong>&nbsp;</p><p>mRNA vaccines, building houses, building cities? Not sure what you mean there.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:57]</strong>&nbsp;</p><p>\"things we have the technological capacity to do but which aren't legal to do\"</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][14:58][15:00]</strong>&nbsp;</p><p>Eg, you might imagine, \"What if AIs were smart enough to build houses, wouldn't that raise GDP?\" and the answer is that we already have the pure technology to manufacture homes cheaply, but the upright-stick-construction industry already successfully lobbied to get it banned as it was starting to develop, by adding on various constraints; so the question is not \"Is AI advantaged in doing this?\" but \"Is AI advantaged at bypassing regulatory constraints on doing this?\" Not to mention all the other ways that building a house in an existing city is illegal, or that it's been made difficult to start a new city, etcetera.</p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><p>\"What if AIs could design a new vaccine in a day?\" We can already do that. It's no longer the relevant constraint. Bureaucracy is the process-limiting constraint.</p><p>I would - looking in again at the Sideways View essay on takeoff speeds - wonder whether it occurred to you, Richard, to ask about what detailed predictions all the theories there had made.</p><p>After all, a lot of it is spending time explaining why the theories there <i>shouldn't</i> be expected to retrodict even the data points we <i>have</i> about progress rates over hominid evolution.</p><p>Surely you, being the evenhanded judge that you are, must have been reading through that document saying, \"My goodness, this is even worse than retrodicting a few data points!\"</p><p>A lot of why I have a bad taste in my mouth about certain classes of epistemological criticism is my sense that certain sentences tend to be uttered on <i>incredibly</i> selective occasions.</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][14:59][15:06]</strong>&nbsp;</p><p>Some meta thoughts: I now feel like I have a pretty reasonable broad outline of Eliezer's views. I haven't yet changed my mind much, but plausibly mostly because I haven't taken the time to internalise those views; once I ruminate on them a bunch, I expect my opinions will shift (uncertain how far; unlikely to be most of the way).</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p>Meta thoughts (continued): Insofar as a strong disagreement remains after that (which it probably will) I feel pretty uncertain about what would resolve it. Best guess is that I should write up some longer essays that try to tie a bunch of disparate strands together.</p><p>Near the end it seemed like the crux, to a surprising extent, hinged on this question of takeoff speeds. So the other thing which seems like it'd plausibly help a lot is Eliezer writing up a longer version of his response to Paul's Takeoff Speeds post.</p><p>(Just as a brief comment, I don't find the \"bureaucratic sclerosis\" explanation very compelling. I do agree that regulatory barriers are a huge problem, but they still don't seem nearly severe enough to cause a fast takeoff. I don't have strong arguments for that position right now though.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][15:12]</strong>&nbsp;</p><p>This seems like a fine point to call it!</p><p>Some wrap-up notes</p><ul><li>I had the impression this round was a bit more frustrating than last rounds. Thanks all for sticking with things 🙂</li><li>I have a sense that Richard was making a couple points that didn't quite land. I plan to attempt to articulate versions of them myself in the interim.</li><li>Richard noted he had a sense we're in decreasing return territory. My own sense is that it's worth having at least one more discussion in this format about specific non-consequentialist plans Richard may have hope in, but I also think we shouldn't plow forward in spite of things feeling less useful, and I'm open to various alternative proposals.</li></ul><p>In particular, it seems maybe plausible to me we should have a pause for some offline write-ups, such as Richard digesting a bit and then writing up some of his current state, and/or Eliezer writing up some object-level response to the takeoff speed post above?</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: 👍]</td></tr></tbody></table></figure><p>(I also could plausibly give that a go myself, either from my own models or from my model of Eliezer's model which he could then correct)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Ngo][15:15]</strong>&nbsp;</p><p>Thanks Nate!</p><p>I endorse the idea of offline writeups</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][15:17]</strong>&nbsp;</p><p>Cool. Then I claim we are adjourned for the day, and Richard has the ball on digesting &amp; doing a write-up from his end, and I have the ball on both writing up my attempts to articulate some points, and on either Eliezer or I writing some takes on timelines or something.</p><p>(And we can coordinate our next discussion, if any, via email, once the write-ups are in shape.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:18]</strong>&nbsp;</p><p>I also have a sense that there's more to be said about specifics of govt stuff or specifics of \"ways to bypass consequentialism\" and that I wish we could spend at least one session trying to stick to concrete details only</p><p>Even if it's not where cruxes ultimately lie, often you learn more about the abstract by talking about the concrete than by talking about the abstract.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border-color:#000000;vertical-align:top\"><p><strong>[Soares][15:22]</strong>&nbsp;</p><p>(I, too, would be enthusiastic to see such a discussion, and Richard, if you find yourself feeling enthusiastic or at least not-despairing about it, I'd happily moderate.)</p></td></tr><tr><td style=\"border-color:#000000;vertical-align:top\"><p><strong>[Yudkowsky][15:37]</strong>&nbsp;</p><p>(I'm a little surprised about how poorly I did at staying concrete after saying that aloud, and would nominate Nate to take on the stern duty of blowing the whistle at myself or at both of us.)</p></td></tr></tbody></table>",
      "plaintextDescription": "This is the second post in a series of transcribed conversations about AGI forecasting and alignment. See the first post for prefaces and more information about the format.\n\n\nColor key:\n\n  Chat by Richard Ngo and Eliezer Yudkowsky     Other chat      Inline comments   \n\n \n\n\n5. September 14 conversation\n \n\n\n5.1. Recursive self-improvement, abstractions, and miracles\n \n\n[Yudkowsky][11:00] \n\nGood morning / good evening.\n\nSo it seems like the obvious thread to pull today is your sense that I'm wrong about recursive self-improvement and consequentialism in a related way?\n\n[Ngo][11:04] \n\nRight. And then another potential thread (probably of secondary importance) is the question of what you mean by utility functions, and digging more into the intuitions surrounding those.\n\nBut let me start by fleshing out this RSI/consequentialism claim.\n\nI claim that your early writings about RSI focused too much on a very powerful abstraction, of recursively applied optimisation; and too little on the ways in which even powerful abstractions like this one become a bit... let's say messier, when they interact with the real world.\n\nIn particular, I think that Paul's arguments that there will be substantial progress in AI in the leadup to a RSI-driven takeoff are pretty strong ones.\n\n(Just so we're on the same page: to what extent did those arguments end up shifting your credences?)\n\n[Yudkowsky][11:09] \n\nI don't remember being shifted by Paul on this at all. I sure shifted a lot over events like Alpha Zero and the entire deep learning revolution. What does Paul say that isn't encapsulated in that update - does he furthermore claim that we're going to get fully smarter-than-human in all regards AI which doesn't cognitively scale much further either through more compute or through RSI?\n\n[Ngo][11:10] \n\nAh, I see. In that case, let's just focus on the update from the deep learning revolution.\n\n[Yudkowsky][11:12][11:13] \n\nI'll also remark that I see my foreseeable mistake there as having little ",
      "wordCount": 11503
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "827JKe7YNjAegR468",
        "name": "Effective altruism",
        "slug": "effective-altruism"
      },
      {
        "_id": "qf3kDBak4BQDDw3f2",
        "name": "Modest Epistemology",
        "slug": "modest-epistemology"
      },
      {
        "_id": "nvKzwpiranwy29HFJ",
        "name": "Optimization",
        "slug": "optimization"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b5",
        "name": "Recursive Self-Improvement",
        "slug": "recursive-self-improvement"
      },
      {
        "_id": "HAFdXkW4YW4KRe2Gx",
        "name": "Utility Functions",
        "slug": "utility-functions"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "7im8at9PmhbT4JHsW",
    "title": "Ngo and Yudkowsky on alignment difficulty",
    "slug": "ngo-and-yudkowsky-on-alignment-difficulty",
    "url": null,
    "baseScore": 259,
    "voteCount": 106,
    "viewCount": null,
    "commentCount": 152,
    "createdAt": null,
    "postedAt": "2021-11-15T20:31:34.135Z",
    "contents": {
      "markdown": "This post is the first in a series of transcribed Discord conversations between Richard Ngo and Eliezer Yudkowsky, moderated by Nate Soares. We've also added Richard and Nate's running summaries of the conversation (and others' replies) from Google Docs.\n\nLater conversation participants include Ajeya Cotra, Beth Barnes, Carl Shulman, Holden Karnofsky, Jaan Tallinn, Paul Christiano, Rob Bensinger, and Rohin Shah.\n\nThe transcripts are a complete record of several Discord channels MIRI made for discussion. We tried to edit the transcripts as little as possible, other than to fix typos and a handful of confusingly-worded sentences, to add some paragraph breaks, and to add referenced figures and links. We didn't end up redacting any substantive content, other than the names of people who would prefer not to be cited. We swapped the order of some chat messages for clarity and conversational flow (indicated with extra timestamps), and in some cases combined logs where the conversation switched channels.\n\nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Richard and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Google Doc content&nbsp;</td><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Inline comments&nbsp;</td></tr></tbody></table>\n\n0\\. Prefatory comments\n======================\n\n<table><tbody><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][8:32]</strong> &nbsp;<strong>(Nov. 6 follow-up comment)</strong>&nbsp;</p><p>(At Rob's request I'll try to keep this brief, but this was an experimental format and some issues cropped up that seem large enough to deserve notes.)</p><p>Especially when coming in to the early parts of this dialogue, I had some backed-up hypotheses about \"What might be the main sticking point? and how can I address that?\" which from the standpoint of a pure dialogue might seem to be causing me to go on digressions, relative to if I was just trying to answer Richard's own questions.&nbsp; On reading the dialogue, I notice that this looks evasive or like point-missing, like I'm weirdly not just directly answering Richard's questions.</p><p>Often the questions are answered later, or at least I think they are, though it may not be in the first segment of the dialogue.&nbsp; But the larger phenomenon is that I came in with some things I wanted to say, and Richard came in asking questions, and there was a minor accidental mismatch there.&nbsp; It would have looked better if we'd both stated positions first without question marks, say, or if I'd just confined myself to answering questions from Richard.&nbsp; (This is not a huge catastrophe, but it's something for the reader to keep in mind as a minor hiccup that showed up in the early parts of experimenting with this new format.)</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][8:32]</strong> &nbsp;<strong>(Nov. 6 follow-up comment)</strong>&nbsp;</p><p>(Prompted by some later stumbles in attempts to summarize this dialogue.&nbsp; Summaries seem plausibly a major mode of propagation for a sprawling dialogue like this, and the following request seems like it needs to be very prominent to work - embedded requests later on didn't work.)</p><p>Please don't summarize this dialogue by saying, \"and so Eliezer's MAIN idea is that\" or \"and then Eliezer thinks THE KEY POINT is that\" or \"the PRIMARY argument is that\" etcetera.&nbsp; From my perspective, everybody comes in with a different set of sticking points versus things they see as obvious, and the conversation I have changes drastically depending on that.&nbsp; In the old days this used to be the Orthogonality Thesis, Instrumental Convergence, and superintelligence being a possible thing at all; today most OpenPhil-adjacent folks have other sticking points instead.</p><p>Please transform:</p><ul><li>\"Eliezer's main reply is...\" -&gt; \"Eliezer replied that...\"</li><li>\"Eliezer thinks the key point is...\" -&gt; \"Eliezer's point in response was...\"</li><li>\"Eliezer thinks a major issue is...\"&nbsp; -&gt; \"Eliezer replied that one issue is...\"</li><li>\"Eliezer's primary argument against this is...\" -&gt; \"Eliezer tried the counterargument that...\"</li><li>\"Eliezer's main scenario for this is...\" -&gt; \"In a conversation in September of 2021, Eliezer sketched a hypothetical where...\"</li></ul><p>Note also that the transformed statements say what you <i>observed,</i> whereas the untransformed statements are (often incorrect) <i>inferences</i> about my latent state of mind.</p><p>(Though \"distinguishing relatively unreliable inference from more reliable observation\" is not necessarily<i> the key idea</i> here or <i>the one big reason</i> I'm asking for this.&nbsp; That's just one point I tried making - one argument that I hope might help drive home the larger thesis.)</p></td></tr></tbody></table>\n\n1\\. September 5 conversation\n============================\n\n1.1. Deep vs. shallow problem-solving patterns\n----------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:00]</strong>&nbsp;</p><p>Hi all! Looking forward to the discussion.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:01]</strong>&nbsp;</p><p>Hi and welcome all.&nbsp; My name is Eliezer and I think alignment is really actually quite extremely difficult.&nbsp; Some people seem to not think this!&nbsp; It's an important issue so ought to be resolved somehow, which we can hopefully fully do today.&nbsp; (I will however want to take a break after the first 90 minutes, if it goes that far and if Ngo is in sleep-cycle shape to continue past that.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:02]</strong>&nbsp;</p><p>A break in 90 minutes or so sounds good.</p><p>Here's one way to kick things off: I agree that humans trying to align arbitrarily capable AIs seems very difficult. One reason that I'm more optimistic (or at least, not confident that we'll have to face the full very difficult version of the problem) is that at a certain point AIs will be doing most of the work.</p><p>When you talk about alignment being difficult, what types of AIs are you thinking about aligning?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:04]</strong>&nbsp;</p><p>On my model of the Other Person, a lot of times when somebody thinks alignment shouldn't be that hard, they think there's some particular thing you can do to align an AGI, which isn't that hard, and their model is missing one of the foundational difficulties for why you can't do (easily or at all) one step of their procedure.&nbsp; So one of my own conversational processes might be to poke around looking for a step that the other person doesn't realize is hard.&nbsp; That said, I'll try to directly answer your own question first.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:07]</strong>&nbsp;</p><p>I don't think I'm confident that there's any particular thing you can do to align an AGI. Instead I feel fairly uncertain over a broad range of possibilities for how hard the problem turns out to be.</p><p>And on some of the most important variables, it seems like evidence from the last decade pushes towards updating that the problem will be easier.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:09]</strong>&nbsp;</p><p>I think that after AGI becomes possible at all and then possible to scale to dangerously superhuman levels, there will be, in the best-case scenario where a lot of other social difficulties got resolved, a 3-month to 2-year period where only a very few actors have AGI, meaning that it was socially possible for those few actors to decide to <i>not</i> just scale it to where it automatically destroys the world.</p><p>During this step, if humanity is to survive, somebody has to perform some feat that causes the world to <i>not</i> be destroyed in 3 months or 2 years when too many actors have access to AGI code that will destroy the world if its intelligence dial is turned up. This requires that the first actor or actors to build AGI, be able to do <i>something</i> with that AGI which prevents the world from being destroyed; if it didn't require superintelligence, we could go do that thing right now, but no such human-doable act apparently exists so far as I can tell.</p><p>So we want the least dangerous, most easily aligned thing-to-do-with-an-AGI, but it does have to be a pretty powerful act to prevent the automatic destruction of Earth after 3 months or 2 years. It has to \"flip the gameboard\" rather than letting the suicidal game play out. We need to align the AGI that performs this pivotal act, to perform that pivotal act without killing everybody.</p><p>Parenthetically, no act powerful enough and gameboard-flipping enough to qualify is inside the Overton Window of politics, or possibly even of effective altruism, which presents a separate social problem. I usually dodge around this problem by picking an exemplar act which is powerful enough to actually flip the gameboard, but not the most alignable act because it would require way too many aligned details: Build self-replicating open-air nanosystems and use them (only) to melt all GPUs.</p><p>Since any such nanosystems would have to operate in the full open world containing lots of complicated details, this would require tons and tons of alignment work, is not the pivotal act easiest to align, and we should do some other thing instead. But the other thing I have in mind is also outside the Overton Window, just like this is. So I use \"melt all GPUs\" to talk about the requisite power level and the Overton Window problem level, both of which seem around the right levels to me, but the actual thing I have in mind is more alignable; and this way, I can reply to anyone who says \"How dare you?!\" by saying \"Don't worry, I don't actually plan on doing that.\"</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:14]</strong>&nbsp;</p><p>One way that we could take this discussion is by discussing the pivotal act \"make progress on the alignment problem faster than humans can\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:15]</strong>&nbsp;</p><p>This sounds to me like it requires extreme levels of alignment and operating in extremely dangerous regimes, such that, if you could do that, it would seem much more sensible to do some other pivotal act first, using a lower level of alignment tech.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:16]</strong>&nbsp;</p><p>Okay, this seems like a crux on my end.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:16]</strong>&nbsp;</p><p>In particular, I would hope that - in unlikely cases where we survive at all - we were able to survive by operating a superintelligence only in the lethally dangerous, but still less dangerous, regime of \"engineering nanosystems\".</p><p>Whereas \"solve alignment for us\" seems to require operating in the even more dangerous regimes of \"write AI code for us\" and \"model human psychology in tremendous detail\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:17]</strong>&nbsp;</p><p>What makes these regimes so dangerous? Is it that it's very hard for humans to exercise oversight?</p><p>One thing that makes these regimes seem less dangerous to me is that they're broadly in the domain of \"solving intellectual problems\" rather than \"achieving outcomes in the world\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:19][11:21]</strong>&nbsp;</p><p>Every AI output <i>effectuates</i> outcomes in the world.&nbsp; If you have a powerful unaligned mind hooked up to outputs that can start causal chains that effectuate dangerous things, it doesn't matter whether the comments on the code say \"intellectual problems\" or not.</p><p>The danger of \"solving an intellectual problem\" is when it requires a powerful mind to think about domains that, when solved, render very cognitively accessible strategies that can do dangerous things.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">I expect the first alignment solution you can actually deploy in real life, in the unlikely event we get a solution at all, looks like 98% \"don't think about all these topics that we do not absolutely need and are adjacent to the capability to easily invent very dangerous outputs\" and 2% \"actually think about this dangerous topic but please don't come up with a strategy inside it that kills us\".</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:21][11:22]</strong>&nbsp;</p><p>Let me try and be more precise about the distinction. It seems to me that systems which have been primarily trained to make predictions about the world would by default lack a lot of the cognitive machinery which humans use to take actions which pursue our goals.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p>Perhaps another way of phrasing my point is something like: it doesn't seem implausible to me that we build AIs that are significantly more intelligent (in the sense of being able to understand the world) than humans, but significantly less agentic.</p><p>Is this a crux for you?</p><p>(obviously \"agentic\" is quite underspecified here, so maybe it'd be useful to dig into that first)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:27][11:33]</strong>&nbsp;</p><p>I would certainly have learned very new and very exciting facts about intelligence, facts which indeed contradict my present model of how intelligences liable to be discovered by present research paradigms work, if you showed me... how can I put this in a properly general way... that problems I thought were about searching for states that get fed into a result function and then a result-scoring function, such that the input gets an output with a high score, were in fact not about search problems like that. I have sometimes given more specific names to this problem setup, but I think people have become confused by the terms I usually use, which is why I'm dancing around them.</p><p>In particular, just as I have a model of the Other Person's Beliefs in which they think alignment is easy because they don't know about difficulties I see as very deep and fundamental and hard to avoid, I also have a model in which people think \"why not just build an AI which does X but not Y?\" because they don't realize what X and Y have in common, which is something that draws deeply on having deep models of intelligence. And it is hard to convey this deep theoretical grasp.</p><p>But you can also see powerful practical hints that these things are much more correlated than, eg, Robin Hanson was imagining during the <a href=\"https://intelligence.org/ai-foom-debate/\">FOOM debate</a>, because Robin did not think something like GPT-3 should exist; Robin thought you should need to train lots of specific domains that didn't generalize. I argued then with Robin that it was something of a hint that humans had visual cortex and cerebellar cortex but not Car Design Cortex, in order to design cars. Then in real life, it proved that reality was far to the Eliezer side of Eliezer on the <a href=\"https://intelligence.org/2017/10/20/alphago/\">Eliezer-Robin axis</a>, and things like GPT-3 were built with <i>less</i> architectural complexity and generalized <i>more</i> than I was arguing to Robin that complex architectures should generalize over domains.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">The metaphor I sometimes use is that it is very hard to build a system that drives cars painted red, but is not at all adjacent to a system that could, with a few alterations, prove to be very good at driving a car painted blue.&nbsp; The \"drive a red car\" problem and the \"drive a blue car\" problem have too much in common.&nbsp; You can maybe ask, \"Align a system so that it has the capability to drive red cars, but refuses to drive blue cars.\"&nbsp; You can't make a system that is very good at driving red-painted cars, but lacks the basic capability to drive blue-painted cars because you never trained it on that.&nbsp; The patterns found by gradient descent, by genetic algorithms, or by other plausible methods of optimization, for driving red cars, would be patterns very close to the ones needed to drive blue cars.&nbsp; When you optimize for red cars you get the blue car <i>capability</i> whether you like it or not.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:32]</strong>&nbsp;</p><p>Does your model of intelligence rule out building AIs which make dramatic progress in mathematics without killing us all?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:34][11:39]</strong>&nbsp;</p><p>If it were possible to perform some pivotal act that saved the world with an AI that just made progress on proving mathematical theorems, without, eg, needing to explain those theorems to humans, I'd be <i>extremely</i> interested in that as a potential pivotal act. We wouldn't be out of the woods, and I wouldn't actually know how to build an AI like that without killing everybody, but it would immediately trump everything else as the obvious line of research to pursue.</p><p>Parenthetically, there is very very little which my model of intelligence <i>rules out</i>. I think we all die because we cannot do certain dangerous things correctly, <i>on the very first try in the dangerous regimes where one mistake kills you</i>, and do them <i>before</i> proliferation of much easier technologies kills us. If you have the Textbook From 100 Years In The Future that gives the simple robust solutions for everything, that actually work, you can write a superintelligence that thinks 2 + 2 = 5 because the Textbook gives the methods for doing that which are simple and actually work in practice in real life.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">(The Textbook has the equivalent of \"use ReLUs instead of sigmoids\" everywhere, and avoids all the clever-sounding things that will work at subhuman levels and blow up when you run them at superintelligent levels.)</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:36][11:40]</strong>&nbsp;</p><p>Hmm, so suppose we train an AI to prove mathematical theorems when given them, perhaps via some sort of adversarial setter-solver training process.</p><p>By default I have the intuition that this AI could become extremely good at proving theorems - far beyond human level - without having goals about real-world outcomes.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p>It seems to me that in your model of intelligence, being able to do tasks like mathematics is closely coupled with trying to achieve real-world outcomes. But I'd actually take GPT-3 as some evidence against this position (although still evidence in favour of your position over Hanson's), since it seems able to do a bunch of reasoning tasks while still not being very agentic.</p><p>There's some alternative world where we weren't able to train language models to do reasoning tasks without first training them to perform tasks in complex RL environments, and in that world I'd be significantly less optimistic.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:41]</strong>&nbsp;</p><p>I put to you that there is a predictable bias in your estimates, where you don't know about the Deep Stuff that is required to prove theorems, so you imagine that certain cognitive capabilities are more disjoint than they actually are.&nbsp; If you knew about the things that humans are using to reuse their reasoning about chipped handaxes and other humans, to prove math theorems, you would see it as more plausible that proving math theorems would generalize to chipping handaxes and manipulating humans.</p><p>GPT-3 is a... complicated story, on my view of it and intelligence.&nbsp; We're looking at an interaction between tons and tons of memorized shallow patterns.&nbsp; GPT-3 is <i>very</i> unlike the way that natural selection built humans.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:44]</strong>&nbsp;</p><p>I agree with that last point. But this is also one of the reasons that I previously claimed that AIs could be more intelligent than humans while being less agentic, because there are systematic differences between the way in which natural selection built humans, and the way in which we'll train AGIs.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:45]</strong>&nbsp;</p><p>My current suspicion is that Stack More Layers alone is not going to take us to GPT-6 which is a true AGI; and this is because of the way that GPT-3 is, in your own terminology, \"not agentic\", and which is, in my terminology, not having gradient descent on GPT-3 run across sufficiently deep problem-solving patterns.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:46]</strong>&nbsp;</p><p>Okay, that helps me understand your position better.</p><p>So here's one important difference between humans and neural networks: humans face the genomic bottleneck which means that each individual has to rederive all the knowledge about the world that their parents already had. If this genetic bottleneck hadn't been so tight, then individual humans would have been significantly less capable of performing novel tasks.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:50]</strong>&nbsp;</p><p>I agree.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:50]</strong>&nbsp;</p><p>In my terminology, this is a reason that humans are \"more agentic\" than we otherwise would have been.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:50]</strong>&nbsp;</p><p>This seems indisputable.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:51]</strong>&nbsp;</p><p>Another important difference: humans were trained in environments where we had to run around surviving all day, rather than solving maths problems etc.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:51]</strong>&nbsp;</p><p>I continue to nod.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:52]</strong>&nbsp;</p><p>Supposing I agree that reaching a certain level of intelligence will require AIs with the \"deep problem-solving patterns\" you talk about, which lead AIs to try to achieve real-world goals. It still seems to me that there's likely a lot of space between that level of intelligence, and human intelligence.</p><p>And if that's the case, then we could build AIs which help us solve the alignment problem before we build AIs which instantiate sufficiently deep problem-solving patterns that they decide to take over the world.</p><p>Nor does it seem like the reason <i>humans</i> want to take over the world is because of a deep fact about our intelligence. It seems to me that humans want to take over the world mainly because that's very similar to things we evolved to do (like taking over our tribe).</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:57]</strong>&nbsp;</p><p>So here's the part that I agree with: If there were one theorem only mildly far out of human reach, like proving the ABC Conjecture (if you think it hasn't already been proven), and providing a machine-readable proof of this theorem would immediately save the world - say, aliens will give us an aligned superintelligence, as soon as we provide them with this machine-readable proof - then there would exist a plausible though not certain road to saving the world, which would be to try to build a <i>shallow</i> mind that proved the ABC Conjecture by memorizing tons of relatively shallow patterns for mathematical proofs learned through self-play; without that system ever abstracting math as deeply as humans do, but the sheer width of memory and sheer depth of search sufficing to do the job. I am not sure, to be clear, that this would work. But my model of intelligence does not rule it out.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:58]</strong>&nbsp;</p><p>(I'm actually thinking of a mind which understands maths more deeply than humans - but perhaps only understands maths, or perhaps also a range of other sciences better than humans.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:00]</strong>&nbsp;</p><p>Parts I disagree with: That \"help us solve alignment\" bears any significant overlap with \"provide us a machine-readable proof of the ABC Conjecture without thinking too deeply about it\". That humans want to take over the world only because it resembles things we evolved to do.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:01]</strong>&nbsp;</p><p>I definitely agree that humans don't <i>only</i> want to take over the world because it resembles things we evolved to do.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:02]</strong>&nbsp;</p><p>Alas, eliminating 5 reasons why something would go wrong doesn't help much if there's 2 remaining reasons something would go wrong that are much harder to eliminate!</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:02]</strong>&nbsp;</p><p>But if we imagine having a human-level intelligence which <i>hadn't</i> evolved primarily to do things that reasonably closely resembled taking over the world, then I expect that we could ask that intelligence questions in a fairly safe way.</p><p>And that's also true for an intelligence that is noticeably above human level.</p><p>So one question is: how far above human level could we get before a system which has only been trained to do things like answer questions and understand the world will decide to take over the world?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:04]</strong>&nbsp;</p><p>I think this is one of the very rare cases where the intelligence difference between \"village idiot\" and \"Einstein\", which I'd usually see as very narrow, makes a structural difference! I think you can get some outputs from a village-idiot-level AGI, which got there by training on domains exclusively like math, and this will proooobably not destroy the world (<i>if</i> you were right about that, about what was going on inside). I have more concern about the Einstein level.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:05]</strong>&nbsp;</p><p>Let's focus on the Einstein level then.</p><p>Human brains have been optimised very little for doing science.</p><p>This suggests that building an AI which is Einstein-level at doing science is significantly easier than building an AI which is Einstein-level at taking over the world (or other things which humans evolved to do).</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:08]</strong>&nbsp;</p><p>I think there's a certain broad sense in which I agree with the literal truth of what you just said. You will systematically overestimate <i>how much</i> easier, or how far you can push the science part without getting the taking-over-the-world part, for as long as your model is ignorant of what they have in common.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:08]</strong>&nbsp;</p><p>Maybe this is a good time to dig into the details of what they have in common, then.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:09][12:11]][12:13]</strong>&nbsp;</p><p>I feel like I haven't had much luck with trying to explain that on previous occasions. Not to you, to others too.</p><p>There are shallow topics like why p-zombies can't be real and how quantum mechanics works and why science ought to be using likelihood functions instead of p-values, and I can <i>barely</i> explain those to <i>some</i> people, but then there are some things that are apparently much harder to explain than that and which defeat my abilities as an explainer.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">That's why I've been trying to point out that, even if you don't know the specifics, there's an estimation bias that you can realize should exist in principle.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">Of course, I also haven't had much luck in saying to people, \"Well, even if you don't know the truth about X that would let you see Y, can you not see by abstract reasoning that knowing <i>any</i> truth about X would predictably cause you to update in the direction of Y\" - people don't seem to actually internalize that much either. Not you, other discussions.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:10][12:11][12:13]</strong>&nbsp;</p><p>Makes sense. Are there ways that I could try to make this easier? E.g. I could do my best to explain what I think your position is.</p><p>Given what you've said I'm not optimistic about this helping much.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p>But insofar as this is the key set of intuitions which has been informing your responses, it seems worth a shot.</p><p>Another approach would be to focus on our predictions for how AI capabilities will play out over the next few years.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">I take your point about my estimation bias. To me it feels like there's also a bias going the other way, which is that as long as we don't know the mechanisms by which different human capabilities work, we'll tend to lump them together as one thing.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:14]</strong>&nbsp;</p><p>Yup. If you didn't know about visual cortex and auditory cortex, or about eyes and ears, you would assume much more that any sentience ought to both see and hear.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:16]</strong>&nbsp;</p><p>So then my position is something like: human pursuit of goals is driven by emotions and reward signals which are deeply evolutionarily ingrained, and without those we'd be much safer but not that much worse at pattern recognition.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:17]</strong>&nbsp;</p><p>If there's a pivotal act you can get just by supreme acts of pattern recognition, that's right up there with \"pivotal act composed solely of math\" for things that would obviously instantly become the prime direction of research.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:18]</strong>&nbsp;</p><p>To me it seems like maths is <i>much more</i> about pattern recognition than, say, being a CEO. Being a CEO requires coherence over long periods of time; long-term memory; motivation; metacognition; etc.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:18][12:23]</strong>&nbsp;</p><p>(One occasionally-argued line of research can be summarized from a certain standpoint as \"how about a pivotal act composed entirely of predicting text\" and to this my reply is \"you're trying to get fully general AGI capabilities by predicting text that is <i>about</i> deep / 'agentic' reasoning, and that doesn't actually help\".)</p><p>Human math is very much about goals. People want to prove subtheorems on the way to proving theorems. We might be able to make a <i>different</i> kind of mathematician that works more like GPT-3 in the dangerously inscrutable parts that are all noninspectable vectors of floating-point numbers, but even there you'd need some Alpha-Zero-like outer framework to supply the direction of search.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">That outer framework might be able to be powerful enough without being reflective, though. So it would plausibly be <i>much easier</i> to build a mathematician that was capable of superhuman formal theorem-proving but not agentic. The reality of the world might tell us \"lolnope\" but my model of intelligence doesn't mandate that. That's why, if you gave me a pivotal act composed entirely of \"output a machine-readable proof of this theorem and the world is saved\", I would pivot there! It actually does seem like it would be a lot easier!</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:21][12:25]</strong>&nbsp;</p><p>Okay, so if I attempt to rephrase your argument:</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">Your position: There's a set of fundamental similarities between tasks like doing maths, doing alignment research, and taking over the world. In all of these cases, agents based on techniques similar to modern ML which are very good at them will need to make use of deep problem-solving patterns which include goal-oriented reasoning. So while it's possible to beat humans at some of these tasks without those core competencies, people usually overestimate the extent to which that's possible.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:25]</strong>&nbsp;</p><p>Remember, a lot of my concern is about what happens <i>first</i>, especially if it happens soon enough that future AGI bears any resemblance whatsoever to modern ML; not about what can be done in principle.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][12:26]</strong>&nbsp;</p><p>(Note: it's been 85 min, and we're planning to take a break at 90min, so this seems like a good point for a little bit more clarifying back-and-forth on Richard's summary before a break.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:26]</strong>&nbsp;</p><p>I'll edit to say \"plausible for ML techniques\"?</p><p>(and \"extent to which that's plausible\")</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:28]</strong>&nbsp;</p><p>I think that obvious-to-me future outgrowths of modern ML paradigms are <i>extremely</i> liable to, if they can learn how to do sufficiently superhuman X, generalize to taking over the world. How fast this happens does depend on X. It would plausibly happen relatively slower (at higher levels) with theorem-proving as the X, and with architectures that carefully stuck to gradient-descent-memorization over shallow network architectures to do a pattern-recognition part with search factored out (sort of, this is not generally safe, this is not a general formula for safe things!); rather than imposing anything like the genetic bottleneck you validly pointed out as a reason why humans generalize. Profitable X, and all X I can think of that would actually save the world, seem much more problematic.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:30]</strong>&nbsp;</p><p>Okay, happy to take a break here.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][12:30]</strong>&nbsp;</p><p>Great timing!</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:30]</strong>&nbsp;</p><p>We can do a bit of meta discussion afterwards; my initial instinct is to push on the question of how similar Eliezer thinks alignment research is to theorem-proving.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:30]</strong>&nbsp;</p><p>Yup. This is my lunch break (actually my first-food-of-day break on a 600-calorie diet) so I can be back in 45min if you're still up for that.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:31]</strong>&nbsp;</p><p>Sure.</p><p>Also, if any of the spectators are reading in real time, and have suggestions or comments, I'd be interested in hearing them.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:31]</strong>&nbsp;</p><p>I'm also cheerful about spectators posting suggestions or comments during the break.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][12:32]</strong>&nbsp;</p><p>Sounds good. I declare us on a break for 45min, at which point we'll reconvene (for another 90, by default).</p><p>Floor's open to suggestions &amp; commentary.</p></td></tr></tbody></table>\n\n1.2. Requirements for science\n-----------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:50]</strong>&nbsp;</p><p>I seem to be done early if people (mainly Richard) want to resume in 10min (30m break)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:51]</strong>&nbsp;</p><p>Yepp, happy to do so</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][12:57]</strong>&nbsp;</p><p>Some quick commentary from me:</p><ul><li>It seems to me like we're exploring a crux in the vicinity of \"should we expect that systems capable of executing a pivotal act would, by default in lieu of significant technical alignment effort, be using their outputs to optimize the future\".</li><li>I'm curious whether you two agree that this is a crux (but plz don't get side-tracked answering me).</li><li>The general discussion seems to be going well to me.<ul><li>In particular, huzzah for careful and articulate efforts to zero in on cruxes.</li></ul></li></ul></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:00]</strong>&nbsp;</p><p>I think that's a crux for the specific pivotal act of \"doing better alignment research\", and maybe some other pivotal acts, but not all (or necessarily most) of them.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:01]</strong>&nbsp;</p><p>I should also say out loud that I've been working a bit with Ajeya on making an attempt to convey the intuitions behind there being deep patterns that generalize and are liable to be learned, which covered a bunch of ground, taught me how much ground there was, and made me relatively more reluctant to try to re-cover the same ground in this modality.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:02]</strong>&nbsp;</p><p>Going forward, a couple of things I'd like to ask Eliezer about:</p><ul><li>In what ways are the tasks that are most useful for alignment similar or different to proving mathematical theorems (which we agreed might generalise relatively slowly to taking over the world)?</li><li>What are the deep problem-solving patterns underlying these tasks?</li><li>Can you summarise my position?</li></ul><p>I was going to say that I was most optimistic about #2 in order to get these ideas into a public format</p><p>But if that's going to happen anyway based on Ajeya's work, then that seems less important</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:03]</strong>&nbsp;</p><p>I could still try briefly and see what happens.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:03]</strong>&nbsp;</p><p>That seems valuable to me, if you're up for it.</p><p>At the same time, I'll try to summarise some of my own intuitions about intelligence which I expect to be relevant.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:04]</strong>&nbsp;</p><p>I'm not sure I could summarize your position in a non-straw way. To me there's a huge visible distance between \"solve alignment for us\" and \"output machine-readable proofs of theorems\" where I can't give a good account of why you think talking about the latter would tell us much about the former. I don't know what other pivotal act you think might be easier.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:06]</strong>&nbsp;</p><p>I see. I was considering \"solving scientific problems\" as an alternative to \"proving theorems\", with alignment being one (particularly hard) example of a scientific problem.</p><p>But decided to start by discussing theorem-proving since it seemed like a clearer-cut case.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:07]</strong>&nbsp;</p><p>Can you predict in advance why Eliezer thinks \"solving scientific problems\" is significantly thornier? (Where alignment is like totally not \"a particularly hard example of a scientific problem\" except in the sense that it has science in it at all; which is maybe the real crux; but also a more difficult issue.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:09]</strong>&nbsp;</p><p>Based on some of your earlier comments, I'm currently predicting that you think the step where the solutions need to be legible to and judged by humans makes science much thornier than theorem-proving, where the solutions are machine-checkable.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:10]</strong>&nbsp;</p><p>That's one factor. Should I state the other big one or would you rather try to state it first?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:10]</strong>&nbsp;</p><p>Requiring a lot of real-world knowledge for science?</p><p>If it's not that, go ahead and say it.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:11]</strong>&nbsp;</p><p>That's one way of stating it. The way I'd put it is that it's about making up hypotheses about the real world.</p><p>Like, the real world is then a thing that the AI is modeling, at all.</p><p>Factor 3: On many interpretations of doing science, you would furthermore need to think up experiments. That's planning, value-of-information, search for an experimental setup whose consequences distinguish between hypotheses (meaning you're now searching for initial setups that have particular causal consequences).</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:12]</strong>&nbsp;</p><p>To me \"modelling the real world\" is a very continuous variable. At one end you have physics equations that are barely separable from maths problems, at the other end you have humans running around in physical bodies.</p><p>To me it seems plausible that we could build an agent which solves scientific problems but has very little self-awareness (in the sense of knowing that it's an AI, knowing that it's being trained, etc).</p><p>I expect that your response to this is that modelling oneself is part of the deep problem-solving patterns which AGIs are very likely to have.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:15]</strong>&nbsp;</p><p>There's a problem of <i>inferring the causes of sensory experience</i> in cognition-that-does-science. (Which, in fact, also appears in the way that humans do math, and is possibly inextricable from math in general; but this is an example of the sort of deep model that says \"Whoops I guess you get science from math after all\", not a thing that makes science less dangerous because it's more like just math.)</p><p>You can build an AI that only ever drives red cars, and which, at no point in the process of driving a red car, ever needs to drive a blue car in order to drive a red car. That doesn't mean its red-car-driving capabilities won't be extremely close to blue-car-driving capabilities if at any point the internal cognition happens to get pointed towards driving a blue car.</p><p>The fact that there's a deep car-driving pattern which is the same across red cars and blue cars doesn't mean that the AI has ever driven a blue car, per se, or that it has to drive blue cars to drive red cars. But if blue cars are fire, you sure are playing with that fire.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:18]</strong>&nbsp;</p><p>To me, \"sensory experience\" as in \"the video and audio coming in from this body that I'm piloting\" and \"sensory experience\" as in \"a file containing the most recent results of the large hadron collider\" are very very different.</p><p>(I'm not saying we could train an AI scientist just from the latter - but plausibly from data that's closer to the latter than the former)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:19]</strong>&nbsp;</p><p>So there's separate questions about \"does an AGI <i>inseparably need</i> to model itself inside the world to do science\" and \"did we build something that would be very close to modeling itself, and could easily stumble across that by accident somewhere in the inscrutable floating-point numbers, especially if that was even slightly useful for solving the outer problems\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:19]</strong>&nbsp;</p><p>Hmm, I see</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:20][13:21][13:21]</strong>&nbsp;</p><p>If you're trying to build an AI that literally does science only to observations collected without the AI having had a causal impact on those observations, that's legitimately \"more dangerous than math but maybe less dangerous than active science\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">You might still stumble across an active scientist because it was a simple internal solution to something, but the outer problem would be legitimately stripped of an important structural property the same way that pure math not describing Earthly objects is stripped of important structural properties.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">And of course my reaction again is, \"There is no pivotal act which uses only that cognitive capability.\"</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:20][13:21][13:26]</strong>&nbsp;</p><p>I guess that my (fairly strong) prior here is that something like self-modelling, which is very deeply built into basically every organism, is a very hard thing for an AI to stumble across by accident without significant optimisation pressure in that direction.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">But I'm not sure how to argue this except by digging into your views on what the deep problem-solving patterns are. So if you're still willing to briefly try and explain those, that'd be useful to me.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">\"Causal impact\" again seems like a very continuous variable - it seems like the <i>amount</i> of causal impact you need to do good science is much less than the amount which is needed to, say, be a CEO.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:26]</strong>&nbsp;</p><p>The amount doesn't seem like the key thing, nearly so much as what underlying facilities you need to do whatever amount of it you need.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:27]</strong>&nbsp;</p><p>Agreed.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:27]</strong>&nbsp;</p><p>If you go back to the 16th century and ask for just one mRNA vaccine, that's not much of a difference from asking for a <s>million</s> hundred of them.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:28]</strong>&nbsp;</p><p>Right, so the additional premise which I'm using here is that the ability to reason about causally impacting the world in order to achieve goals is something that you can have a little bit of.</p><p>Or a lot of, and that the difference between these might come down to the training data used.</p><p>Which at this point I don't expect you to agree with.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:29]</strong>&nbsp;</p><p>If you have reduced a pivotal act to \"look over the data from this hadron collider you neither built nor ran yourself\", that really is a structural step down from \"do science\" or \"build a nanomachine\". But I can't see any pivotal acts like that, so is that question much of a crux?</p><p>If there's intermediate steps they might be described in my native language like \"reason about causal impacts across only this one preprogrammed domain which you didn't learn in a general way, in only this part of the cognitive architecture that is separable from the rest of the cognitive architecture\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:31]</strong>&nbsp;</p><p>Perhaps another way of phrasing this intermediate step is that the agent has a shallow understanding of how to induce causal impacts.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:31]</strong>&nbsp;</p><p>What is \"shallow\" to you?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:31]</strong>&nbsp;</p><p>In a similar way to how you claim that GPT-3 has a shallow understanding of language.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:32]</strong>&nbsp;</p><p>So it's memorized a ton of shallow causal-impact-inducing patterns from a large dataset, and this can be verified by, for example, presenting it with an example mildly outside the dataset and watching it fail, which we think will confirm our hypothesis that it didn't learn any deep ways of solving that dataset.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:33]</strong>&nbsp;</p><p>Roughly speaking, yes.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:34]</strong>&nbsp;</p><p>Eg, it wouldn't surprise us at all if GPT-4 had learned to predict \"27 * 18\" but not \"what is the area of a rectangle 27 meters by 18 meters\"... is what I'd like to say, but Codex sure did demonstrate those two were kinda awfully proximal.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:34]</strong>&nbsp;</p><p>Here's one way we could flesh this out. Imagine an agent that loses coherence quickly when it's trying to act in the world.</p><p>So for example, we've trained it to do scientific experiments over a period of a few hours or days</p><p>And then it's very good at understanding the experimental data and extracting patterns from it</p><p>But upon running it for a week or a month, it loses coherence in a similar way to how GPT-3 loses coherence - e.g. it forgets what it's doing.</p><p>My story for why this might happen is something like: there is a specific skill of having long-term memory, and we never trained our agent to have this skill, and so it has not acquired that skill (even though it can reason in very general and powerful ways in the short term).</p><p>This feels similar to the argument I was making before about how an agent might lack self-awareness, if we haven't trained it specifically to have that.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:39]</strong>&nbsp;</p><p>There's a set of obvious-to-me tactics for doing a pivotal act with minimal danger, which I do not think collectively make the problem safe, and one of these sets of tactics is indeed \"Put a limit on the 'attention window' or some other internal parameter, ramp it up slowly, don't ramp it any higher than you needed to solve the problem.\"</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:41]</strong>&nbsp;</p><p>You could indeed do this manually, but my expectation is that you could also do this automatically, by training agents in environments where they don't benefit from having long attention spans.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:42]</strong>&nbsp;</p><p>(Any time one imagines a specific tactic of this kind, if one has the <a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\">security mindset</a>, one can also imagine all sorts of ways it might go wrong; for example, an attention window can be defeated if there's any aspect of the attended data or the internal state that ended up depending on past events in a way that leaked info about them. But, depending on how much superintelligence you were throwing around elsewhere, you could maybe get away with that, some of the time.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:43]</strong>&nbsp;</p><p>And that if you put agents in environments where they answer questions but don't interact much with the physical world, then there will be many different traits which are necessary for achieving goals in the real world which they will lack, because there was little advantage to the optimiser of building those traits in.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:43]</strong>&nbsp;</p><p>I'll observe that TransformerXL built an attention window that generalized, trained it on I think 380 tokens or something like that, and then found that it generalized to 4000 tokens or something like that.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:43]</strong>&nbsp;</p><p>Yeah, an order of magnitude of generalisation is not surprising to me.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:44]</strong>&nbsp;</p><p>Having observed one order of magnitude, I would personally not be surprised by two orders of magnitude either, after seeing that.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:45]</strong>&nbsp;</p><p>I'd be a little surprised, but I assume it would happen eventually.</p></td></tr></tbody></table>\n\n1.3. Capability dials\n---------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:46]</strong>&nbsp;</p><p>I have a sense that this is all circling back to the question, \"But what is it we <i>do</i> with the intelligence thus weakened?\" If you can save the world using a rock, I can build you a very safe rock.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:46]</strong>&nbsp;</p><p>Right.</p><p>So far I've said \"alignment research\", but I haven't been very specific about it.</p><p>I guess some context here is that I expect that the first things we do with intelligence similar to this is create great wealth, produce a bunch of useful scientific advances, etc.</p><p>And that we'll be in a world where people take the prospect of AGI much more seriously</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:48]</strong>&nbsp;</p><p>I mostly expect - albeit with some chance that reality says \"So what?\" to me and surprises me, because it is not as solidly determined as some other things - that we do not hang around very long in the \"weirdly ~human AGI\" phase before we get into the \"if you crank up this AGI it destroys the world\" phase. Less than 5 years, say, to put numbers on things.</p><p>It would not surprise me in the least if the world ends before self-driving cars are sold on the mass market. On some quite plausible scenarios which I think have &gt;50% of my probability mass at the moment, research AGI companies would be able to produce prototype car-driving AIs if they spent time on that, given the near-world-ending tech level; but there will be Many Very Serious Questions about this relatively new unproven advancement in machine learning being turned loose on the roads. And their AGI tech will gain the property \"can be turned up to destroy the world\" before Earth gains the property \"you're allowed to sell self-driving cars on the mass market\" because there just won't be much time.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:52]</strong>&nbsp;</p><p>Then I expect that another thing we do with this is produce a very large amount of data which rewards AIs for following human instructions.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:52]</strong>&nbsp;</p><p>On other scenarios, of course, self-driving becomes possible by limited AI well before things start to break (further) on AGI. And on some scenarios, the way you got to AGI was via some breakthrough that is already scaling pretty fast, so by the time you can use the tech to get self-driving cars, that tech already ends the world if you turn up the dial, or that event follows very swiftly.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:53]</strong>&nbsp;</p><p>When you talk about \"cranking up the AGI\", what do you mean?</p><p>Using more compute on the same data?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:53]</strong>&nbsp;</p><p>Running it with larger bounds on the for loops, over more GPUs, to be concrete about it.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:53]</strong>&nbsp;</p><p>In a RL setting, or a supervised, or unsupervised learning setting?</p><p>Also: can you elaborate on the for loops?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:56]</strong>&nbsp;</p><p>I do not quite think that gradient descent on Stack More Layers alone - as used by OpenAI for GPT-3, say, and as <i>opposed</i> to Deepmind which builds more complex artifacts like Mu Zero or AlphaFold 2 - is liable to be the first path taken to AGI. I am reluctant to speculate more in print about clever ways to AGI, and I think any clever person out there will, if they are really clever and not just a fancier kind of stupid, not talk either about what they think is missing from Stack More Layers or how you would really get AGI. That said, the way that you cannot just run GPT-3 at a greater search depth, the way you can run Mu Zero at a greater search depth, is part of why I think that AGI is not likely to look <i>exactly</i> like GPT-3; the thing that kills us is likely to be a thing that can get more dangerous when you turn up a dial on it, not a thing that intrinsically has no dials that can make it more dangerous.</p></td></tr></tbody></table>\n\n1.4. Consequentialist goals vs. deontologist goals\n--------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:59]</strong>&nbsp;</p><p>Hmm, okay. Let's take a quick step back and think about what would be useful for the last half hour.</p><p>I want to flag that my intuitions about pivotal acts are not very specific; I'm quite uncertain about how the geopolitics of that situation would work, as well as the timeframe between somewhere-near-human-level AGI and existential risk AGI.</p><p>So we could talk more about this, but I expect there'd be a lot of me saying \"well we can't rule out that X happens\", which is perhaps not the most productive mode of discourse.</p><p>A second option is digging into your intuitions about how cognition works.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:03]</strong>&nbsp;</p><p>Well, obviously, in the limit of alignment not being accessible to our civilization, and my successfully building a model weaker than reality which nonetheless correctly rules out alignment being accessible to our civilization, I could spend the rest of my short remaining lifetime arguing with people whose models are weak enough to induce some area of ignorance where for all they know you could align a thing. But that is predictably how conversations go in possible worlds where the Earth is doomed; so somebody wiser on the meta-level, though also ignorant on the object-level, might prefer to ask: \"Where do you think your knowledge, rather than your ignorance, says that alignment ought to be doable and you will be surprised if it is not?\"</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:07]</strong>&nbsp;</p><p>That's a fair point. Although it seems like a structural property of the \"pivotal act\" framing, which builds in doom by default.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:08]</strong>&nbsp;</p><p>We could talk about that, if you think it's a crux. Though I'm also not thinking that this whole conversation gets done in a day, so maybe for publishability reasons we should try to focus more on one line of discussion?</p><p>But I do think that lots of people get their optimism by supposing that the world can be saved by doing less dangerous things with an AGI. So it's a big ol' crux of mine on priors.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:09]</strong>&nbsp;</p><p>Agreed that one line of discussion is better; I'm happy to work within the pivotal act framing for current purposes.</p><p>A third option is that I make some claims about how cognition works, and we see how much you agree with them.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:12]</strong>&nbsp;</p><p>(Though it's something of a restatement, a reason I'm not going into \"my intuitions about how cognition works\" is that past experience has led me to believe that conveying this info in a form that the Other Mind will actually absorb and operate, is really quite hard and takes a long discussion, relative to my current abilities to Actually Explain things; it is the sort of thing that might take doing homework exercises to grasp how one structure is appearing in many places, as opposed to just being flatly told that to no avail, and I have not figured out the homework exercises.)</p><p>I'm cheerful about hearing your own claims about cognition and disagreeing with them.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:12]</strong>&nbsp;</p><p>Great</p><p>Okay, so one claim is that something like deontology is a fairly natural way for minds to operate.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:14]</strong>&nbsp;</p><p>(\"If that were true,\" he thought at once, \"bureaucracies and books of regulations would be a lot more efficient than they are in real life.\")</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:14]</strong>&nbsp;</p><p>Hmm, although I think this was probably not a very useful phrasing, let me think about how to rephrase it.</p><p>Okay, so in <a href=\"https://docs.google.com/document/d/1XXGbFnWPXtsRiTxleBZ0LAGtU7_7CYKt17nnowfpKvo/edit\">our earlier email discussion</a>, we talked about the concept of \"obedience\".</p><p>To me it seems like it is just as plausible for a mind to have a concept like \"obedience\" as its rough goal, as a concept like maximising paperclips.</p><p>If we imagine training an agent on a large amount of data which pointed in the rough direction of rewarding obedience, for example, then I imagine that by default obedience would be a constraint of comparable strength to, say, the human survival instinct.</p><p>(Which is obviously not strong enough to stop humans doing a bunch of things that contradict it - but it's a pretty good starting point.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:18]</strong>&nbsp;</p><p>Heh. You mean of comparable strength to the human instinct to explicitly maximize inclusive genetic fitness?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:19]</strong>&nbsp;</p><p>Genetic fitness wasn't a concept that our ancestors were able to understand, so it makes sense that they weren't pointed directly towards it.</p><p>(And nor did they understand <i>how</i> to achieve it.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:19]</strong>&nbsp;</p><p>Even in that paradigm, except insofar as you expect gradient descent to work very differently from gene-search optimization - which, admittedly, it does - when you optimize really hard on a thing, you get contextual correlates to it, not the thing you optimized on.</p><p>This is of course one of the Big Fundamental Problems that I expect in alignment.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:20]</strong>&nbsp;</p><p>Right, so the main correlate that I've seen discussed is \"do what would make the human give you a high rating, not what the human actually wants\"</p><p>One thing I'm curious about is the extent to which you're concerned about this specific correlate, versus correlates in general.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:21]</strong>&nbsp;</p><p>That said, I also see basic structural reasons why paperclips would be much easier to train than \"obedience\", even if we could magically instill simple inner desires that perfectly reflected the simple outer algorithm we saw ourselves as running over many particular instances of a loss function.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:22]</strong>&nbsp;</p><p>I'd be interested in hearing what those are.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:22]</strong>&nbsp;</p><p>well, first of all, why <i>is</i> a book of regulations so much more unwieldy than a hunter-gatherer?</p><p>if deontology is just as good as <a href=\"https://arbital.com/p/consequentialist/\">consequentialism</a>, y'know.</p><p>(do you want to try replying or should I just say?)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:23]</strong>&nbsp;</p><p>Go ahead</p><p>I should probably clarify that I agree that you can't just replace consequentialism with deontology</p><p>The claim is more like: when it comes to high-level concepts, it's not clear to me why high-level consequentialist goals are more natural than high-level deontological goals.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:24]</strong>&nbsp;</p><p>I reply that reality is complicated, so when you pump a simple goal through complicated reality you get complicated behaviors required to achieve the goal. If you think of reality as a complicated function Input-&gt;Probability(Output), then even to get a simple Output or a simple partition on Output or a high expected score in a simple function over Output, you may need very complicated Input.</p><p>Humans don't trust each other. They imagine, \"Well, if I just give this bureaucrat a goal, perhaps they won't reason honestly about what it takes to achieve that goal! Oh no! Therefore I will instead, being the trustworthy and accurate person that I am, reason myself about constraints and requirements on the bureaucrat's actions, such that, if the bureaucrat obeys these regulations, I expect the outcome of their action will be what I want.\"</p><p>But (compared to a general intelligence that observes and models complicated reality and does its own search to pick actions) an actually-effective book of regulations (implemented by some nonhuman mind with a large enough and perfect enough memory to memorize it) would tend to involve a (physically unmanageable) vast number of rules saying \"if you observe this, do that\" to follow all the crinkles of complicated reality as it can be inferred from observation.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:28]</strong>&nbsp;</p><blockquote><p>(Though it's something of a restatement, a reason I'm not going into \"my intuitions about how cognition works\" is that past experience has led me to believe that conveying this info in a form that the Other Mind will actually absorb and operate, is really quite hard and takes a long discussion, relative to my current abilities to Actually Explain things; it is the sort of thing that might take doing homework exercises to grasp how one structure is appearing in many places, as opposed to just being flatly told that to no avail, and I have not figured out the homework exercises.)</p></blockquote><p>(As a side note: do you have a rough guess for when your work with Ajeya will be made public? If it's still a while away, I'm wondering whether it's still useful to have a rough outline of these intuitions even if it's in a form that very few people will internalise)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:30]</strong>&nbsp;</p><blockquote><p>(As a side note: do you have a rough guess for when your work with Ajeya will be made public? If it's still a while away, I'm wondering whether it's still useful to have a rough outline of these intuitions even if it's in a form that very few people will internalise)</p></blockquote><p>Plausibly useful, but not to be attempted today, I think?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:30]</strong>&nbsp;</p><p>Agreed.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:30]</strong>&nbsp;</p><p>(We are now theoretically in overtime, which is okay for me, but for you it is 11:30pm (I think?) and so it is on you to call when to halt, now or later.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:32]</strong>&nbsp;</p><p>Yeah, it's 11.30 for me. I think probably best to halt here. I agree with all the things you just said about reality being complicated, and why consequentialism is therefore valuable. My \"deontology\" claim (which was, in its original formulation, far too general - apologies for that) was originally intended as a way of poking into your intuitions about which types of cognition are natural or unnatural, which I think is the topic we've been circling around for a while.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:33]</strong>&nbsp;</p><p>Yup, and a place to resume next time might be why I think \"obedience\" is unnatural compared to \"paperclips\" - though that is a thing that probably requires taking that stab at what underlies surface competencies.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:34]</strong>&nbsp;</p><p>Right. I do think that even a vague gesture at that would be reasonably helpful (assuming that this doesn't already exist online?)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:34]</strong>&nbsp;</p><p>Not yet afaik, and I don't want to point you to Ajeya's stuff even if she were ok with that, because then this in-context conversation won't make sense to others.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:35]</strong>&nbsp;</p><p>For my part I should think more about pivotal acts that I'd be willing to specifically defend.</p><p>In any case, thanks for the discussion 🙂</p><p>Let me know if there's a particular time that suits you for a follow-up; otherwise we can sort it out later.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][14:37]</strong>&nbsp;</p><p>(y'all are doing all my jobs for me)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:37]</strong>&nbsp;</p><p>could try Tuesday at this same time - though I may be in worse shape for dietary reasons, still, seems worth trying.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][14:37]</strong>&nbsp;</p><p>(wfm)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:39]</strong>&nbsp;</p><p>Tuesday not ideal, any others work?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:39]</strong>&nbsp;</p><p>Wednesday?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:40]</strong>&nbsp;</p><p>Yes, Wednesday would be good</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 100%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:40]</strong>&nbsp;</p><p>let's call it tentatively for that</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][14:41]</strong>&nbsp;</p><p>Great! Thanks for the chats.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 100%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:41]</strong>&nbsp;</p><p>Thanks both!</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 100%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:41]</strong>&nbsp;</p><p>Thanks, Richard!</p></td></tr></tbody></table>\n\n2\\. Follow-ups\n==============\n\n2.1. Richard Ngo's summary\n--------------------------\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][0:35] &nbsp;(Sep. 6)</strong>&nbsp;</p><p>just caught up here &amp; wanted to thank nate, eliezer and (especially) richard for doing this! it's great to see eliezer's model being probed so intensively. i've learned a few new things (such as the genetic bottleneck being plausibly a big factor in human cognition). FWIW, a minor comment re deontology (as that's fresh on my mind): in my view deontology is more about coordination than optimisation: deontological agents are more trustworthy, as they're much easier to reason about (in the same way how functional/declarative code is easier to reason about than imperative code). hence my steelman of bureaucracies (as well as social norms): humans just (correctly) prefer their fellow optimisers (including non-human optimisers) to be deontological for trust/coordination reasons, and are happy to pay the resulting competence tax.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][3:10] &nbsp;(Sep. 8)</strong>&nbsp;</p><p>Thanks Jaan! I agree that greater trust is a good reason to want agents which are deontological at some high level.</p><p>I've attempted a summary of the key points so far; comments welcome: [GDocs link]</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p><i>1st discussion</i></p><p>(Mostly summaries not quotations)</p><p>Eliezer, summarized by Richard: \"To avoid catastrophe, whoever builds AGI first will have to a) align it to some extent, and b) decide not to scale it up beyond the point where their alignment techniques fail, and c) do some pivotal act that prevents others from scaling it up to that level. But <s>our alignment techniques will not be good enough&nbsp;</s> <s>our alignment techniques will be very far from adequate</s> on our current trajectory, our alignment techniques will be very far from adequate to create an AI that safely performs any such pivotal act.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:05] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><blockquote><p>will not be good enough</p></blockquote><p>Are not presently on course to be good enough, missing by not a little.&nbsp; \"Will not be good enough\" is literally declaring for lying down and dying.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][16:03] &nbsp;(Sep. 9 comment)</strong>&nbsp;</p><blockquote><p>will [be very far from adequate]</p></blockquote><p>Same problem as the last time I commented.&nbsp; I am not making an unconditional prediction about future failure as would be implied by the word \"will\".&nbsp; Conditional on current courses of action or their&nbsp;near neighboring courses, we seem to be well over an order of magnitude away from surviving, unless a miracle occurs.&nbsp; It's still in the end a result of people doing what they seem to be doing, not an inevitability.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][5:10] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><p>Ah, I see. Does adding \"on our current trajectory\" fix this?</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:46] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><p>Yes.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Consider the pivotal act of 'make a breakthrough in alignment research'. It is likely that, before the point where AGIs are strongly superhuman at seeking power, they will already be strongly superhuman at understanding the world, and at performing narrower pivotal acts like alignment research which don’t require as much agency (by which I roughly mean: large-scale motivations and the ability to pursue them over long timeframes).\"</p><p>Eliezer, summarized by Richard: \"There’s a deep connection between solving intellectual problems and taking over the world - the former requires a powerful mind to think about domains that, when solved, render very cognitively accessible strategies that can do dangerous things. Even mathematical research is a goal-oriented task which involves identifying then pursuing instrumental subgoals - and if brains which evolved to hunt on the savannah can quickly learn to do mathematics, then it’s also plausible that AIs trained to do mathematics could quickly learn a range of other skills. Since almost nobody understands the deep similarities in the cognition required for these different tasks, the distance between AIs that are able to perform fundamental scientific research, and dangerously agentic AGIs, is smaller than almost anybody expects.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:05] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><blockquote><p>There’s a deep connection between solving intellectual problems and taking over the world</p></blockquote><p>There's a deep connection by default between chipping flint handaxes and taking over the world, if you happen to learn how to chip handaxes in a very general way. &nbsp;\"Intellectual\" problems aren't special in this way. &nbsp;And maybe you could avert the default, but that would take some work and you'd have to do it before easier default ML techniques destroyed the world.</p></td></tr><tr><td style=\"background-color:rgb(255,247,222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Our lack of understanding about how intelligence works also makes it easy to assume that traits which co-occur humans will also co-occur in future AIs. But human brains are badly-optimised for tasks like scientific research, and well-optimised for seeking power over the world, for reasons including a) evolving while embodied in a harsh environment; b) the genetic bottleneck; c) social environments which rewarded power-seeking. By contrast, training neural networks on tasks like mathematical or scientific research optimises them much less for seeking power. For example, GPT-3 has knowledge and reasoning capabilities but little agency, and loses coherence when run for longer timeframes.\"</p></td></tr><tr><td style=\"background-color:rgb(255,238,187);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Tallinn][4:19] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><blockquote><p>[well-optimised for] seeking power</p></blockquote><p>male-female differences might be a datapoint here (annoying as it is to lean on pinker's point :))</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:31] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><p>I don't think a female Eliezer Yudkowsky doesn't try to save / optimize / takeover the world.&nbsp; Men may do that for nonsmart reasons; smart men and women follow the same reasoning when they are smart enough.&nbsp; Eg Anna Salamon and many others.</p></td></tr><tr><td style=\"background-color:rgb(255,247,222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p>Eliezer, summarized by Richard: \"Firstly, there’s a big difference between most scientific research and the sort of pivotal act that we’re talking about - you need to explain how AIs with a given skill can be used to actually prevent dangerous AGIs from being built. Secondly, insofar as GPT-3 has little agency, that’s because it has memorised many shallow patterns in a way which won’t directly scale up to general intelligence. Intelligence instead consists of deep problem-solving patterns which link understanding and agency at a fundamental level.\"</p></td></tr></tbody></table>\n\n3\\. September 8 conversation\n============================\n\n3.1. The Brazilian university anecdote\n--------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:00]</strong>&nbsp;</p><p>(I am here.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:01]</strong>&nbsp;</p><p>Me too.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][11:01]</strong>&nbsp;</p><p>Welcome back!</p><p>(I'll mostly stay out of the way again.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:02]</strong>&nbsp;</p><p>Cool. Eliezer, did you read the summary - and if so, do you roughly endorse it?</p><p>Also, I've been thinking about the best way to approach discussing your intuitions about cognition. My guess is that starting with the obedience vs paperclips thread is likely to be less useful than starting somewhere else - e.g. the description you gave near the beginning of the last discussion, about \"searching for states that get fed into a result function and then a result-scoring function\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:06]</strong>&nbsp;</p><p>made a couple of comments about phrasings in the doc</p><p>So, from my perspective, there's this thing where... it's really quite hard to teach certain <i>general</i> points by talking at people, as opposed to more specific points. Like, they're trying to build a perpetual motion machine, and even if you can manage to argue them into believing their first design is wrong, they go looking for a new design, and the new design is complicated enough that they can no longer be convinced that they're wrong because they managed to make a more complicated error whose refutation they couldn't keep track of anymore.</p><p>Teaching people to see an underlying structure in a lot of places is a very hard thing to teach in this way. Richard Feynman <a href=\"https://v.cx/2010/04/feynman-brazil-education\">gave an example</a> of the mental motion in his story that ends \"Look at the water!\", where people learned in classrooms about how \"a medium with an index\" is supposed to polarize light reflected from it, but they didn't realize that sunlight coming off of water would be polarized. My guess is that doing this properly requires homework exercises; and that, unfortunately from my own standpoint, it happens to be a place where I have extra math talent, the same way that eg Marcello is more talented at formally proving theorems than I happen to be; and that people without the extra math talent, have to do a lot <i>more</i> exercises than I did, and I don't have a good sense of which exercises to give them.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:13]</strong>&nbsp;</p><p>I'm sympathetic to this, and can try to turn off skeptical-discussion-mode and turn on learning-mode, if you think that'll help.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:14]</strong>&nbsp;</p><p>There's a general insight you can have about how arithmetic is commutative, and for some people you can show them 1 + 2 = 2 + 1 and their native insight suffices to generalize over the 1 and the 2 to any other numbers you could put in there, and they realize that strings of numbers can be rearranged and all end up equivalent. For somebody else, when they're a kid, you might have to show them 2 apples and 1 apple being put on the table in a different order but ending up with the same number of apples, and then you might have to show them again with adding up bills in different denominations, in case they didn't generalize from apples to money. I can actually remember being a child young enough that I tried to add 3 to 5 by counting \"5, 6, 7\" and I thought there was some clever enough way to do that to actually get 7, if you tried hard.</p><p>Being able to see \"consequentialism\" is like that, from my perspective.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:15]</strong>&nbsp;</p><p>Another possibility: can you trace the origins of this belief, and how it came out of your previous beliefs?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:15]</strong>&nbsp;</p><p>I don't know what homework exercises to give people to make them able to see \"consequentialism\" all over the place, instead of inventing slightly new forms of consequentialist cognition and going \"Well, now <i>that</i> isn't consequentialism, right?\"</p><p>Trying to say \"searching for states that get fed into an input-result function and then a result-scoring function\" was one attempt of mine to describe the dangerous thing in a way that would maybe sound abstract enough that people would try to generalize it more.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:17]</strong>&nbsp;</p><p>Another possibility: can you describe the closest thing to real consequentialism in humans, and how it came about in us?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:18][11:21]</strong>&nbsp;</p><p>Ok, so, part of the problem is that... before you do enough homework exercises for whatever your level of talent is (and even I, at one point, had done little enough homework that I thought there might be a clever way to add 3 and 5 in order to get to 7), you tend to think that only the very crisp formal thing that's been presented to you, is the \"real\" thing.</p><p>Why would your engine have to obey the laws of thermodynamics? You're not building one of those Carnot engines you saw in the physics textbook!</p><p>Humans contain fragments of consequentialism, or bits and pieces whose interactions add up to partially imperfectly shadow consequentialism, and the critical thing is being able to see that the reason why humans' outputs 'work', in a sense, is because these structures are what is doing the work, and the work gets done because of how they shadow consequentialism and only insofar as they shadow consequentialism.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">Put a human in one environment, it gets food. Put a human in a different environment, it gets food again. Wow, different initial conditions, same output! There must be things inside the human that, whatever else they do, are also along the way somehow effectively searching for motor signals such that food is the end result!</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:20]</strong>&nbsp;</p><p>To me it feels like you're trying to nudge me (and by extension whoever reads this transcript) out of a specific failure mode. If I had to guess, something like: \"I understand what Eliezer is talking about so now I'm justified in disagreeing with it\", or perhaps \"Eliezer's explanation didn't make sense to me and so I'm justified in thinking that his concepts don't make sense\". Is that right?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:22]</strong>&nbsp;</p><p>More like... from my perspective, even after I talk people out of one specific perpetual motion machine being possible, they go off and try to invent a different, more complicated perpetual motion machine.</p><p>And I am not sure what to do about that. It has been going on for a very long time from my perspective.</p><p>In the end, a lot of what people got out of all that writing I did, was not the deep object-level principles I was trying to point to - they did not really get <a href=\"https://www.lesswrong.com/s/oFePMp9rKftEeZDDr/p/QkX2bAkwG2EpGvNug\">Bayesianism as thermodynamics</a>, say, they did not become able to see <a href=\"https://www.lesswrong.com/posts/QrhAeKBkm2WsdRYao/searching-for-bayes-structure\">Bayesian structures</a> any time somebody sees a thing and changes their belief. What they got instead was something much more meta and general, a vague spirit of how to reason and argue, because that was what they'd spent a lot of time being exposed to over and over and over again in lots of blog posts.</p><p>Maybe there's no way to make somebody understand why <a href=\"https://arbital.com/p/corrigibility/\">corrigibility</a> is \"unnatural\" except to repeatedly walk them through the task of trying to invent an agent structure that lets you press the shutdown button (without it trying to force you to press the shutdown button), and showing them how each of their attempts fails; and then also walking them through why Stuart Russell's attempt at moral uncertainty produces the <a href=\"https://arbital.com/p/updated_deference/\">problem of fully updated (non-)deference</a>; and hope they can start to see the informal general pattern of why corrigibility is in general contrary to the structure of things that are good at optimization.</p><p>Except that to do the exercises at all, you need them to work within an expected utility framework. And then they just go, \"Oh, well, I'll just build an agent that's good at optimizing things but doesn't use these explicit expected utilities that are the source of the problem!\"</p><p>And then if I want them to believe the same things I do, for the same reasons I do, I would have to teach them why certain structures of cognition are the parts of the agent that are good at stuff and do the work, rather than them being this particular formal thing that they learned for manipulating meaningless numbers as opposed to real-world apples.</p><p>And I have tried to write that page once or twice (eg \"<a href=\"https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities\">coherent decisions imply consistent utilities</a>\") but it has not sufficed to teach them, because they did not even do as many homework problems as I did, let alone the greater number they'd have to do because this is in fact a place where I have a particular talent.</p><p>I don't know how to solve this problem, which is why I'm falling back on talking about it at the meta-level.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:30]</strong>&nbsp;</p><p>I'm reminded of a LW post called \"<a href=\"https://www.lesswrong.com/posts/Q924oPJzK92FifuFg/write-a-thousand-roads-to-rome\">Write a thousand roads to Rome</a>\", which iirc argues in favour of trying to explain the same thing from as many angles as possible in the hope that one of them will stick.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][11:31]</strong>&nbsp;</p><p>(Suggestion, not-necessarily-good: having named this problem on the meta-level, attempt to have the object-level debate, while flagging instances of this as it comes up.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:31]</strong>&nbsp;</p><p>I endorse Nate's suggestion.</p><p>And will try to keep the difficulty of the meta-level problem in mind and respond accordingly.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:33]</strong>&nbsp;</p><p>That (Nate's suggestion) is probably the correct thing to do. I name it out loud because sometimes being told about the meta-problem actually does help on the object problem. It seems to help me a lot and others somewhat less, but it does help others at all, for many others.</p></td></tr></tbody></table>\n\n3.2. Brain functions and outcome pumps\n--------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:34]</strong>&nbsp;</p><p>So, do you have a particular question you would ask about input-seeking cognitions? I did try to say why I mentioned those at all (it's a different road to Rome on \"consequentialism\").</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:36]</strong>&nbsp;</p><p>Let's see. So the visual cortex is an example of quite impressive cognition in humans and many other animals. But I'd call this \"pattern-recognition\" rather than \"searching for high-scoring results\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:37]</strong>&nbsp;</p><p>Yup! And it is no coincidence that there are no whole animals formed entirely out of nothing but a visual cortex!</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:37]</strong>&nbsp;</p><p>Okay, cool. So you'd agree that the visual cortex is doing something that's qualitatively quite different from the thing that animals overall are doing.</p><p>Then another question is: can you characterise searching for high-scoring results in non-human animals? Do they do it? Or are you mainly talking about humans and AGIs?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:39]</strong>&nbsp;</p><p>Also by the time you get to like the temporal lobes or something, there is probably some significant amount of \"what could I be seeing that would produce this visual field?\" that is searching through hypothesis-space for hypotheses with high plausibility scores, and for sure at the human level, humans will start to think, \"Well, could I be seeing this? No, that theory has the following problem. How could I repair that theory?\" But it is plausible that there is no low-level analogue of this in a monkey's temporal cortex; and even more plausible that the parts of the visual cortex, if any, which do anything analogous to this, are doing it in a relatively local and definitely very domain-specific way.</p><p>Oh, that's the cerebellum and motor cortex and so on, if we're talking about a cat or whatever. They have to find motor plans that result in their catching the mouse.</p><p>Just because the visual cortex isn't (obviously) running a search doesn't mean the rest of the animal isn't running any searches.</p><p>(On the meta-level, I notice myself hiccuping \"But how could you not see that when looking at a cat?\" and wondering what exercises would be required to teach that.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:41]</strong>&nbsp;</p><p>Well, I see <i>something</i> when I look at a cat, but I don't know how well it corresponds to the concepts you're using. So just taking it slowly for now.</p><p>I have the intuition, by the way, that the motor cortex is in some sense doing a similar thing to the visual cortex - just in reverse. So instead of taking low-level inputs and producing high-level outputs, it's taking high-level inputs and producing low-level outputs. Would you agree with that?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:43]</strong>&nbsp;</p><p>It doesn't directly parse in my ontology because (a) I don't know what you mean by 'high-level' and (b) whole Cartesian agents can be viewed as functions, that doesn't mean all agents can be viewed as non-searching pattern-recognizers.</p><p>That said, all parts of the cerebral cortex have surprisingly similar morphology, so it wouldn't be at all surprising if the motor cortex is doing something similar to visual cortex. (The cerebellum, on the other hand...)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:44]</strong>&nbsp;</p><p>The signal from the visual cortex saying \"that is a cat\", and the signal to the motor cortex saying \"grab that cup\", are things I'd characterise as high-level.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:45]</strong>&nbsp;</p><p>Still less of a native distinction in my ontology, but there's an informal thing it can sort of wave at, and I can hopefully take that as understood and run with it.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:45]</strong>&nbsp;</p><p>The firing of cells in the retina, and firing of motor neurons, are the low-level parts.</p><p>Cool. So to a first approximation, we can think about the part in between the cat recognising a mouse, and the cat's motor cortex producing the specific neural signals required to catch the mouse, as the part where the consequentialism happens?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:49]</strong>&nbsp;</p><p>The part between the cat's eyes seeing the mouse, and the part where the cat's limbs move to catch the mouse, is the whole cat-agent. The whole cat agent sure is a baby consequentialist / searches for mouse-catching motor patterns / gets similarly high-scoring end results even as you vary the environment.</p><p>The visual cortex is a particular part of this system-viewed-as-a-feedforward-function that is, plausibly, by no means surely, either not very searchy, or does only small local visual-domain-specific searches not aimed per se at catching mice; it has the epistemic nature rather than the planning nature.</p><p>Then from one perspective you could reason that \"well, most of the consequentialism is in the remaining cat after visual cortex has sent signals onward\". And this is in general a dangerous mode of reasoning that is liable to fail in, say, inspecting every particular neuron for consequentialism and not finding it; but in this particular case, there are significantly more consequentialist parts of the cat than the visual cortex, so I am okay running with it.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:50]</strong>&nbsp;</p><p>Ah, the more specific thing I meant to say is: most of the consequentialism is strictly between the visual cortex and the motor cortex. Agree/disagree?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:51]</strong>&nbsp;</p><p>Disagree, I'm rusty on my neuroanatomy but I think the motor cortex may send signals on to the cerebellum rather than the other way around.</p><p>(I may also disagree with the actual underlying notion you're trying to hint at, so possibly not just a \"well include the cerebellum then\" issue, but I think I should let you respond first.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][11:53]</strong>&nbsp;</p><p>I don't know enough neuroanatomy to chase that up, so I was going to try a different tack.</p><p>But actually, maybe it's easier for me to say \"let's include the cerebellum\" and see where you think the disagreement ends up.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][11:56]</strong>&nbsp;</p><p>So since cats are not (obviously) (that I have read about) cross-domain consequentialists with imaginations, their consequentialism is in bits and pieces of consequentialism embedded in them all over by the more purely pseudo-consequentialist genetic optimization loop that built them.</p><p>A cat who fails to catch a mouse may then get little bits and pieces of catbrain adjusted all over.</p><p>And then those adjusted bits and pieces get a pattern lookup later.</p><p>Why do these pattern-lookups with no obvious immediate search element, all happen to point towards the same direction of catching the mouse? Because of the past causal history about how what gets looked up, which was tweaked to catch the mouse.</p><p>So it is legit harder to point out \"the consequentialist parts of the cat\" by looking for which sections of neurology are doing searches right there. That said, to the extent that the visual cortex does not get tweaked on failure to catch a mouse, it's not part of that consequentialist loop either.</p><p>And yes, the same applies to humans, but humans also do more explicitly searchy things and this is part of the story for why humans have spaceships and cats do not.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:00]</strong>&nbsp;</p><p>Okay, this is interesting. So in biological agents we've got these three levels of consequentialism: evolution, reinforcement learning, and planning.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:01]</strong>&nbsp;</p><p>In biological agents we've got evolution + local evolved system-rules that in the past promoted genetic fitness. Two kinds of local rules like this are \"operant-conditioning updates from success or failure\" and \"search through visualized plans\". I wouldn't characterize these two kinds of rules as \"levels\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:02]</strong>&nbsp;</p><p>Okay, I see. And when you talk about searching through visualised plans (the type of thing that humans do) can you say more about what it means for that to be a \"search\"?</p><p>For example, if I imagine writing a poem line-by-line, I may only be planning a few words ahead. But somehow the whole poem, which might be quite long, ends up a highly-optimised product. Is that a central example of planning?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:04][12:07]</strong>&nbsp;</p><p>Planning is one way to succeed at search. I think for purposes of understanding alignment difficulty, you want to be thinking on the level of abstraction where you see that in some sense it is the search itself that is dangerous when it's a strong enough search, rather than the danger seeming to come from details of the planning process.</p><p>One of my early experiences in successfully generalizing my notion of intelligence, what I'd later verbalize as \"computationally efficient finding of actions that produce outcomes high in a preference ordering\", was in writing an (unpublished) story about time-travel in which the universe was globally consistent.</p><p>The requirement of global consistency, the way in which all events between Paradox start and Paradox finish had to map the Paradox's initial conditions onto the endpoint that would go back and produce those exact initial conditions, ended up imposing strong complicated constraints on reality that the Paradox in effect had to navigate using its initial conditions. The time-traveler needed to end up going through certain particular experiences that would produce the state of mind in which he'd take the actions that would end up prodding his future self elsewhere into having those experiences.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p>The Paradox ended up killing the people who built the time machine, for example, because they would not otherwise have allowed that person to go back in time, or kept the temporal loop open that long for any other reason if they were still alive.</p><p>Just having two examples of strongly consequentialist general optimization in front of me - human intelligence, and evolutionary biology - hadn't been enough for me to properly generalize over a notion of optimization. Having three examples of homework problems I'd worked - human intelligence, evolutionary biology, and the fictional Paradox - caused it to finally click for me.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:07]</strong>&nbsp;</p><p>Hmm. So to me, one of the central features of search is that you consider many possibilities. But in this poem example, I may only have explicitly considered a couple of possibilities, because I was only looking ahead a few words at a time. This seems related to the distinction Abram drew a while back between selection and control (<a href=\"https://www.lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control\"><u>https://www.alignmentforum.org/posts/ZDZmopKquzHYPRNxq/selection-vs-control</u></a>). Do you distinguish between them in the same way as he does? Or does \"control\" of a system (e.g. a football player dribbling a ball down the field) count as search too in your ontology?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:10][12:11]</strong>&nbsp;</p><p>I would later try to tell people to \"imagine a paperclip maximizer as <i>not being a mind at all</i>, imagine it as a kind of malfunctioning time machine that spits out outputs which will in fact result in larger numbers of paperclips coming to exist later\". I don't think it clicked because people hadn't done the same homework problems I had, and didn't have the same \"Aha!\" of realizing how part of the notion and danger of intelligence could be seen in such purely material terms.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">But the <a href=\"https://arbital.com/p/convergent_strategies/\">convergent instrumental strategies</a>, the anticorrigibility, these things are contained in the <i>true fact about the universe</i> that certain outputs of the time machine <i>will in fact</i> result in there being lots more paperclips later. What produces the danger is not the details of the search process, it's the search being strong and effective <i>at all</i>. The danger is in the territory itself and not just in some weird map of it; that building nanomachines that kill the programmers will produce more paperclips is a fact about reality, not a fact about paperclip maximizers!</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:11]</strong>&nbsp;</p><p>Right, I remember a very similar idea in your writing about Outcome Pumps (<a href=\"https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes\"><u>https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes</u></a>).</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:12]</strong>&nbsp;</p><p>Yup! Alas, the story was written in 2002-2003 when I was a worse writer and the real story that inspired the Outcome Pump never did get published.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:14]</strong>&nbsp;</p><p>Okay, so I guess the natural next question is: what is it that makes you think that a strong, effective search isn't likely to be limited or constrained in some way?</p><p>What is it about search processes (like human brains) that makes it hard to train them with blind spots, or deontological overrides, or things like that?</p><p>Hmmm, although it feels like this is a question I can probably predict your answer to. (Or maybe not, I wasn't expecting the time travel.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:15]</strong>&nbsp;</p><p>In one sense, they are! A paperclip-maximizing superintelligence is nowhere near as powerful as a paperclip-maximizing time machine. The time machine can do the equivalent of buying winning lottery tickets from lottery machines that have been thermodynamically randomized; a superintelligence can't, at least not directly without rigging the lottery or whatever.</p><p>But a paperclip-maximizing strong general superintelligence is epistemically and instrumentally <a href=\"https://arbital.com/p/efficiency/\">efficient</a>, relative to <i>you</i>, or to me. Any time we see it can get at least X paperclips by doing Y, we should expect that it gets X or more paperclips by doing Y or something that leads to even more paperclips than that, because it's not going to miss the strategy we see.</p><p>So in that sense, searching our own brains for how a time machine would get paperclips, asking ourselves how many paperclips are in principle possible and how they could be obtained, is a way of getting our own brains to consider lower bounds on the problem without the implicit stupidity assertions that our brains unwittingly use to constrain story characters. Part of the point of telling people to think about time machines instead of superintelligences was to get past the ways they imagine superintelligences being stupid. Of course that didn't work either, but it was worth a try.</p><p>I don't think that's quite what you were asking about, but I want to give you a chance to see if you want to rephrase anything before I try to answer your me-reformulated questions.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:20]</strong>&nbsp;</p><p>Yeah, I think what I wanted to ask is more like: why should we expect that, out of the space of possible minds produced by optimisation algorithms like gradient descent, strong general superintelligences are more common than other types of agents which score highly on our loss functions?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:20][12:23][12:24]</strong>&nbsp;</p><p>It depends on how hard you optimize! And whether gradient descent on a particular system can even successfully optimize that hard! Many current AIs are trained by gradient descent and yet not superintelligences at all.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">But the answer is that some problems are difficult in that they require solving lots of subproblems, and an easy way to solve all those subproblems is to use patterns which collectively have some coherence and overlap, and the coherence within them generalizes across all the subproblems. Lots of search orderings will stumble across something like that before they stumble across separate solutions for lots of different problems.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">I suspect that you cannot get this out of small large amounts of gradient descent on small large layered transformers, and therefore I suspect that GPT-N does not approach superintelligence before the world is ended by systems that look differently, but I could be wrong about that.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:22][12:23]</strong>&nbsp;</p><p>Suppose that we optimise hard enough to produce an epistemic subsystem that can make plans much better than any human's.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">My guess is that you'd say that this is <i>possible</i>, but that we're much more likely to first produce a consequentialist agent which does this (rather than a purely epistemic agent which does this).</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:24]</strong>&nbsp;</p><p>I am confused by what you think it means to have an \"epistemic subsystem\" that \"makes plans much better than any human's\". If it searches paths through time and selects high-scoring ones for output, what makes it \"epistemic\"?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:25]</strong>&nbsp;</p><p>Suppose, for instance, that it doesn't actually carry out the plans, it just writes them down for humans to look at.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:25]</strong>&nbsp;</p><p>If it <i>can in fact</i> do the thing that a paperclipping time machine does, what makes it any safer than a paperclipping time machine because we called it \"epistemic\" or by some other such name?</p><p>By what criterion is it selecting the plans that humans look at?</p><p>Why did it make a difference that its output was fed through the causal systems called humans on the way to the causal systems called protein synthesizers or the Internet or whatever? If we build a superintelligence to design nanomachines, it makes no obvious difference to its safety whether it sends DNA strings directly to a protein synthesis lab, or humans read the output and retype it manually into an email. Presumably you also don't think that's where the safety difference comes from. So where does the safety difference come from?</p><p>(note: lunchtime for me in 2 minutes, propose to reconvene in 30m after that)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:28]</strong>&nbsp;</p><p>(break for half an hour sounds good)</p><p>If we consider the visual cortex at a given point in time, how does it decide which objects to recognise?</p><p>Insofar as the visual cortex can be non-consequentialist about which objects it recognises, why couldn't a planning system be non-consequentialist about which plans it outputs?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][12:32]</strong>&nbsp;</p><p>This does feel to me like another \"look at the water\" moment, so what do you predict I'll say about that?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:34]</strong>&nbsp;</p><p>I predict that you say something like: in order to produce an agent that can create very good plans, we need to apply a lot of optimisation power to that agent. And if the channel through which we're applying that optimisation power is \"giving feedback on its plans\", then we don't have a mechanism to ensure that the agent actually learns to optimise for creating really good plans, as opposed to creating plans that receive really good feedback.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][12:35]</strong>&nbsp;</p><p>Seems like a fine cliffhanger?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][12:35]</strong>&nbsp;</p><p>Yepp.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][12:35]</strong>&nbsp;</p><p>Great. Let's plan to reconvene in 30min.</p></td></tr></tbody></table>\n\n3.3. Hypothetical-planning systems, nanosystems, and evolving generality\n------------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:03][13:11]</strong>&nbsp;</p><p>So the answer you expected from me, translated into my terms, would be, \"If you select for the consequence of the humans hitting 'approve' on the plan, you're still navigating the space of inputs for paths through time to probable outcomes (namely the humans hitting 'approve'), so you're still doing consequentialism.\"</p><p>But suppose you manage to avoid that. Suppose you get exactly what you ask for. Then the system is still outputting <i>plans</i> such that, when humans follow them, they take paths through time and end up with outcomes that score high in some scoring function.</p><p>My answer is, \"What the heck would it mean for a <i>planning system</i> to be <i>non-consequentialist</i>? You're asking for nonwet water! What's consequentialist isn't the system that does the work, it's the work you're trying to do! You could imagine it being done by a cognition-free material system like a time machine and it would still be consequentialist <i>because</i> the output is a <i>plan</i>, a path through time!\"</p><p>And this indeed is a case where I feel a helpless sense of not knowing how I can rephrase things, which exercises you have to get somebody to do, what fictional experience you have to walk somebody through, before they start to look at the water and see a material with an index, before they start to look at the phrase \"why couldn't a planning system be non-consequentialist about which plans it outputs\" and go \"um\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">My imaginary listener now replies, \"Ah, but what if we have plans that <i>don't</i> end up with outcomes that score high in some function?\" and I reply \"Then you lie on the ground randomly twitching because any <i>outcome you end up with</i> which is <i>not that</i> is one that you wanted <i>more than that</i> meaning you <i>preferred it more than the outcome of random motor outputs</i> which is <i>optimization toward higher in the preference function</i> which is <i>taking a path through time that leads to particular destinations more than it leads to random noise</i>.\"</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:09][13:11]</strong>&nbsp;</p><p>Yeah, this does seem like a good example of the thing you were trying to explain at the beginning</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p>It still feels like there's some sort of levels distinction going on here though, let me try to tease out that intuition.</p><p>Okay, so suppose I have a planning system that, given a situation and a goal, outputs a plan that leads from that situation to that goal.</p><p>And then suppose that we give it, as input, a situation that we're not actually in, and it outputs a corresponding plan.</p><p>It seems to me that there's a difference between the sense in which that planning system is consequentialist by virtue of making consequentialist plans (as in: if that plan were used in the situation described in its inputs, it would lead to some goal being achieved) versus another hypothetical agent that is just directly trying to achieve goals in the situation it's actually in.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:18]</strong>&nbsp;</p><p>So I'd preface by saying that, <i>if</i> you could build such a system, which is indeed a coherent thing (it seems to me) to describe for the purpose of building it, then there would possibly be a safety difference on the margins, it would be noticeably less dangerous though still dangerous. It would need a special internal structural property that you might not get by gradient descent on a loss function with that structure, just like natural selection on inclusive genetic fitness doesn't get you explicit fitness optimizers; you could optimize for planning in hypothetical situations, and get something that didn't explicitly care only and strictly about hypothetical situations. And even if you did get that, the outputs that would kill or brain-corrupt the operators in hypothetical situations might also be fatal to the operators in actual situations. But that is a coherent thing to describe, and the fact that it was not optimizing our own universe, might make it <i>safer</i>.</p><p>With that said, I would worry that somebody would think there was some bone-deep difference of agentiness, of something they were empathizing with like personhood, of imagining goals and drives being absent or present in one case or the other, when they imagine a planner that just solves \"hypothetical\" problems. If you take that planner and feed it the actual world as its hypothetical, tada, it is now that big old dangerous consequentialist you were imagining before, without it having acquired some difference of <i>psychological</i> agency or 'caring' or whatever.</p><p>So I think there is an important homework exercise to do here, which is something like, \"Imagine that safe-seeming system which only considers hypothetical problems. Now see that if you take that system, don't make any other internal changes, and feed it actual problems, it's very dangerous. Now meditate on this until you can see how the hypothetical-considering planner was extremely close in the design space to the more dangerous version, had all the dangerous latent properties, and would probably have a bunch of actual dangers too.\"</p><p>\"See, you thought the source of the danger was this internal property of caring about actual reality, but it wasn't that, it was the structure of planning!\"</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:22]</strong>&nbsp;</p><p>I think we're getting closer to the same page now.</p><p>Let's consider this hypothetical planner for a bit. Suppose that it was trained in a way that minimised the, let's say, <i>adversarial</i> component of its plans.</p><p>For example, let's say that the plans it outputs for any situation are heavily regularised so only the broad details get through.</p><p>Hmm, I'm having a bit of trouble describing this, but basically I have an intuition that in this scenario there's a component of its plan which is cooperative with whoever executes the plan, and a component that's adversarial.</p><p>And I agree that there's no fundamental difference in type between these two things.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:27]</strong>&nbsp;</p><p>\"What if this potion we're brewing has a Good Part and a Bad Part, and we could just keep the Good Parts...\"</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:27]</strong>&nbsp;</p><p>Nor do I think they're separable. But in some cases, you might expect one to be much larger than the other.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][13:29]</strong>&nbsp;</p><p>(I observe that my model of some other listeners, at this point, protest \"there is yet a difference between the hypothetical-planner applied to actual problems, and the Big Scary Consequentialist, which is that the hypothetical planner is emitting descriptions of plans that <i>would</i> work if executed, whereas the big scary consequentialist is executing those plans directly.\")</p><p>(Not sure that's a useful point to discuss, or if it helps Richard articulate, but it's at least a place I expect some reader's minds to go if/when this is published.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:30]</strong>&nbsp;</p><p>(That is in fact a difference! The insight is in realizing that the hypothetical planner is only one line of outer shell command away from being a Big Scary Thing and is therefore also liable to be Big and Scary in many ways.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:31]</strong>&nbsp;</p><p>To me it seems that Eliezer's position is something like: \"actually, in almost no training regimes do we get agents that decide which plans to output by spending almost all of their time thinking about the object-level problem, and very little of their time thinking about how to manipulate the humans carrying out the plan\".</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:32]</strong>&nbsp;</p><p>My position is that the AI does not neatly separate its internals into a Part You Think Of As Good and a Part You Think Of As Bad, because that distinction is sharp in your map but not sharp in the territory or the AI's map.</p><p>From the perspective of a paperclip-maximizing-action-outputting-time-machine, its actions are not \"object-level making paperclips\" or \"manipulating the humans next to the time machine to deceive them about what the machine does\", they're just physical outputs that go through time and end up with paperclips.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:34]</strong>&nbsp;</p><p>@Nate, yeah, that's a nice way of phrasing one point I was trying to make. And I do agree with Eliezer that these things <i>can be</i> very similar. But I'm claiming that in some cases these things can also be quite different - for instance, when we're training agents that only get to output a short high-level description of the plan.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:35]</strong>&nbsp;</p><p>The danger is in how hard the agent has to work to come up with the plan. I can, for instance, build an agent that very safely outputs a high-level plan for saving the world:</p><p>echo \"Hey Richard, go save the world!\"</p><p>So I do have to ask what kind of \"high-level\" planning output, that saves the world, you are envisioning, and why it was hard to cognitively come up with such that we didn't just make that high-level plan right now, if humans could follow it. Then I'll look at the part where the plan was hard to come up with, and say how the agent had to understand lots of complicated things in reality and accurately navigate paths through time for those complicated things, in order to even invent the high-level plan, and hence it was very dangerous if it wasn't navigating exactly where you hoped. Or, alternatively, I'll say, \"That plan couldn't save the world: you're not postulating enough superintelligence to be dangerous, <i>and you're also</i> not using enough superintelligence to flip the tables on the currently extremely doomed world.\"</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:39]</strong>&nbsp;</p><p>At this point I'm not envisaging a particular planning output that saves the world, I'm just trying to get more clarity on the issue of consequentialism.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:40]</strong>&nbsp;</p><p>Look at the water; it's not the way you're doing the work that's dangerous, it's the work you're trying to do. What work are you trying to do, never mind how it gets done?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:41]</strong>&nbsp;</p><p>I think I agree with you that, in the limit of advanced capabilities, we can't say much about how the work is being done, we have to primarily reason from the work that we're trying to do.</p><p>But here I'm only talking about systems that are intelligent enough to come up with plans and do research that are beyond the capability of humanity.</p><p>And for me the question is: for <i>those</i> systems, can we tilt the way they do the work so they spend 99% of their time trying to solve the object-level problem, and 1% of their time trying to manipulate the humans who are going to carry out the plan? (Where these are not fundamental categories for the AI, they're just a rough categorisation that emerges after we've trained it - the same way that the categories of \"physically moving around\" and \"thinking about things\" aren't fundamentally different categories of action for humans, but the way we've evolved means there's a significant internal split between them.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][13:43]</strong>&nbsp;</p><p>(I suspect Eliezer is not trying to make a claim of the form \"in the limit of advanced capabilities, we are relegated to reasoning about what work gets done, not about how it was done\". I suspect some miscommunication. It might be a reasonable time for Richard to attempt to paraphrase Eliezer's argument?)</p><p>(Though it also seems to me like Eliezer responding to the 99%/1% point may help shed light.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:46]</strong>&nbsp;</p><p>Well, for one thing, I'd note that a system which is designing nanosystems, and spending 1% of its time thinking about how to kill the operators, is lethal. It has to be such a small fraction of thinking that it, like, never completes the whole thought about \"well, if I did X, that would kill the operators!\"</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:46]</strong>&nbsp;</p><p>Thanks for that, Nate. I'll try to paraphrase Eliezer's argument now.</p><p>Eliezer's position (partly in my own terminology): we're going to build AIs that can perform very difficult tasks using cognition which we can roughly describe as \"searching over many options to find one that meets our criteria\". An AI that can solve these difficult tasks will need to be able to search in a very general and flexible way, and so it will be very difficult to constrain that search into a particular region.</p><p>Hmm, that felt like a very generic summary, let me try and think about the more specific claims he's making.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:54]</strong>&nbsp;</p><blockquote><p>An AI that can solve these difficult tasks will need to be able to</p></blockquote><p>Very very little is universally necessary over the design space. The <i>first</i> AGI that our tech becomes able to build is liable to work in certain easier and simpler ways.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:55]</strong>&nbsp;</p><p>Point taken; thanks for catching this misphrasing (this and previous times).</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:56]</strong>&nbsp;</p><p>Can you, in principle, build a red-car-driver that is totally incapable of driving blue cars? In principle, sure! But the first red-car-driver that gradient descent stumbles over is liable to be a blue-car-driver too.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:57]</strong>&nbsp;</p><p>Eliezer, I'm wondering how much of our disagreement is about how high the human level is here.</p><p>Or, to put it another way: we can build systems that outperform humans at quite a few tasks by now, without having search abilities that are general enough to even try to take over the world.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:58]</strong>&nbsp;</p><p>Indubitably and indeed, this is so.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][13:59]</strong>&nbsp;</p><p>Putting aside for a moment the question of which tasks are pivotal enough to save the world, which parts of your model draw the line between human-level chess players and human-level galaxy-colonisers?</p><p>And say that we'll be able to align ones that they outperform us on <i>these tasks</i> before taking over the world, but not on <i>these other tasks</i>?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][13:59][14:01]</strong>&nbsp;</p><p>That doesn't have a very simple answer, but one aspect there is <i>domain generality</i> which in turn is achieved through <i>novel domain learning</i>.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">Humans, you will note, were not aggressively optimized by natural selection to be able to breathe underwater or fly into space. In terms of obvious outer criteria, there is not much outer sign that natural selection produced these creatures much more general than chimpanzees, by training on a much wider range of environments and loss functions.</td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][14:00]</strong>&nbsp;</p><p>(Before we drift too far from it: thanks for the summary! It seemed good to me, and I updated towards the miscommunication I feared not-having-happened.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:03]</strong>&nbsp;</p><blockquote><p>(Before we drift too far from it: thanks for the summary! It seemed good to me, and I updated towards the miscommunication I feared not-having-happened.)</p></blockquote><p>(Good to know, thanks for keeping an eye out. To be clear, I didn't ever interpret Eliezer as making a claim explicitly about the limit of advanced capabilities; instead it just seemed to me that he was thinking about AIs significantly more advanced than the ones I've been thinking of. I think I phrased my point poorly.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:05][14:10]</strong>&nbsp;</p><p>There are complicated aspects of this story where natural selection may metaphorically be said to have \"had no idea of what it was doing\", eg, after early rises in intelligence possibly produced by sexual selection on neatly chipped flint handaxes or whatever, all the cumulative brain-optimization on chimpanzees reached a point where there was suddenly a sharp selection gradient on relative intelligence at Machiavellian planning against other humans (even more so than in the chimp domain) as a subtask of inclusive genetic fitness, and so continuing to optimize on \"inclusive genetic fitness\" in the same old savannah, turned out to happen to be optimizing hard on the subtask and internal capability of \"outwit other humans\", which optimized hard on \"model other humans\", which was a capability that could be reused for modeling the chimp-that-is-this-chimp, which turned the system on itself and made it reflective, which contributed greatly to its intelligence being generalized, even though it was just grinding the same loss function on the same savannah; the system being optimized happened to go there in the course of being optimized even harder for the same thing.</p><p>So one can imagine asking the question: Is there a superintelligent AGI that can quickly build nanotech, which has a kind of passive safety in some if not all respects, in virtue of it solving problems like \"build a nanotech system which does X\" the way that a beaver solves building dams, in virtue of having a bunch of specialized learning abilities without it ever having a cross-domain general learning ability?</p><p>And in this regard one does note that there are many, many, many things that humans do which no other animal does, which you might think would contribute a lot to that animal's fitness if there were animalistic ways to do it. They don't make iron claws for themselves. They never did evolve a tendency to search for iron ore, and burn wood into charcoal that could be used in hardened-clay furnaces.</p><p>No animal plays chess, but AIs do, so we can obviously make AIs to do things that animals don't do. On the other hand, the environment didn't exactly present any particular species with a challenge of chess-playing either.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">Even so, though, even if some animal had evolved to play chess, I fully expect that current AI systems would be able to squish it at chess, because the AI systems are on chips that run faster than neurons and doing crisp calculations and there are things you just can't do with noisy slow neurons. So that again is not a generally reliable argument about what AIs can do.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:09][14:11]</strong>&nbsp;</p><p>Yes, although I note that challenges which are trivial from a human-engineering perspective can be very challenging from an evolutionary perspective (e.g. spinning wheels).</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">And so the evolution of animals-with-a-little-bit-of-help-from-humans might end up in very different places from the evolution of animals-just-by-themselves. And analogously, the ability of humans to fill in the gaps to help less general AIs achieve more might be quite significant.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:11]</strong>&nbsp;</p><p>So we can again ask: Is there a way to make an AI system that is <i>only</i> good at designing nanosystems, which can achieve some complicated but hopefully-specifiable real-world outcomes, without that AI also being superhuman at understanding and manipulating humans?</p><p>And I roughly answer, \"Perhaps, but not by default, there's a bunch of subproblems, I don't actually know how to do it right now, it's not <i>the easiest</i> way to get an AGI that can build nanotech (and kill you), you've got to make the red-car-driver specifically not be able to drive blue cars.\" Can I explain how I know that? I'm really not sure I can, in real life where I explain X0 and then the listener doesn't generalize X0 to X and respecialize it to X1.</p><p>It's like asking me how I could possibly know in 2008, before anybody had observed AlphaFold 2, that superintelligences would be able to crack the protein folding problem on the way to nanotech, which some people did question back in 2008.</p><p>Though that was admittedly more of a slam-dunk than this was, and I could not have told you that AlphaFold 2 would become possible at a prehuman level of general intelligence in 2021 specifically, or that it would be synced in time to a couple of years after GPT-2's level of generality at text.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:18]</strong>&nbsp;</p><p>What are the most relevant axes of difference between solving protein folding and designing nanotech that, say, self-assembles into a computer?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:20]</strong>&nbsp;</p><p>Definitely, \"turns out it's easier than you thought to use gradient descent's memorization of zillions of shallow patterns that overlap and recombine into larger cognitive structures, to add up to a consequentialist nanoengineer that only does nanosystems and never does sufficiently general learning to apprehend the big picture containing humans, while still understanding the goal for that pivotal act you wanted to do\" is among the more plausible advance-specified miracles we could get.</p><p>But it is not what my model says actually happens, and I am not a believer that when your model says you are going to die, you get to start believing in particular miracles. You need to hold your mind open for any miracle and a miracle you didn't expect or think of in advance, because at this point our last hope is that in fact the future is often quite surprising - though, alas, negative surprises are a tad more frequent than positive ones, when you are trying desperately to navigate using a bad map.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:22]</strong>&nbsp;</p><p>Perhaps one metric we could use here is something like: how much extra reward does the consequentialist nanoengineer get from starting to model humans, versus from becoming better at nanoengineering?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:23]</strong>&nbsp;</p><p>But that's <i>not</i> where humans came from. We didn't get to nuclear power by getting a bunch of fitness from nuclear power plants. We got to nuclear power because if you get a bunch of fitness from chipping flint handaxes and Machiavellian scheming, as found by relatively simple and local hill-climbing, that entrains the same genes that build nuclear power plants.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:24]</strong>&nbsp;</p><p>Only in the specific case where you also have the constraint that you keep having to learn new goals every generation.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:24]</strong>&nbsp;</p><p>Huh???</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][14:24]</strong>&nbsp;</p><p>(I think Richard's saying, \"that's a consequence of the genetic bottleneck\")</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:25]</strong>&nbsp;</p><p>Right.</p><p>Hmm, but I feel like we may have covered this ground before.</p><p>Suggestion: I have a couple of other directions I'd like to poke at, and then we could wrap up in 20 or 30 minutes?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:27]</strong>&nbsp;</p><p>OK</p><blockquote><p>What are the most relevant axes of difference between solving protein folding and designing nanotech that, say, self-assembles into a computer?</p></blockquote><p>Though I want to mark that this question seemed potentially cruxy to me, though perhaps not for others. I.e., if building protein factories that built nanofactories that built nanomachines that met a certain deep and lofty engineering goal, didn't involve cognitive challenges different in kind from protein folding, we could maybe just safely go do that using AlphaFold 3, which would be just as safe as AlphaFold 2.</p><p>I don't think we can do that. And I would note to the generic Other that if, to them, these both just sound like thinky things, so why can't you just do that other thinky thing too using the thinky program, this is a case where having any specific model of why we don't already have this nanoengineer right now would tell you there were specific different thinky things involved.</p></td></tr></tbody></table>\n\n3.4. Coherence and pivotal acts\n-------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:31]</strong>&nbsp;</p><p>In either order:</p><ul><li>I'm curious how the things we've been talking about relate to your opinions about meta-level optimisation from the AI foom debate. (I.e. talking about how wrapping around so that there's no longer any protected level of optimisation leads to dramatic change.)</li><li>I'm curious how your claims about the \"robustness\" of consequentialism (i.e. the difficulty of channeling an agent's thinking in the directions we want it to go) relate to the reliance of humans on culture, and in particular the way in which humans raised without culture are such bad consequentialists.</li></ul><p>On the first: if I were to simplify to the extreme, it seems like there are these two core intuitions that you've been trying to share for a long time. One is a certain type of recursive improvement, and another is a certain type of consequentialism.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:32]</strong>&nbsp;</p><p>The second question didn't make much sense in my native ontology? Humans raised without culture don't have access to environmental constants whose presence their genes assume, so they end up as broken machines and then they're bad consequentialists.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:35]</strong>&nbsp;</p><p>Hmm, good point. Okay, question modification: the ways in which humans reason, act, etc, vary greatly depending on which cultures they're raised in. (I'm mostly thinking about differences over time - e.g. cavemen vs moderns.) My low-fidelity version of your view about consequentialists says that general consequentialists like humans possess a robust search process which isn't so easily modified.</p><p>(Sorry if this doesn't make much sense in your ontology, I'm getting a bit tired.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:36]</strong>&nbsp;</p><p>What is it that varies that you think I think should predict would stay more constant?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:37]</strong>&nbsp;</p><p>Goals, styles of reasoning, deontological constraints, level of conformity.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:39]</strong>&nbsp;</p><p>With regards to your first point, my first reaction was, \"I just have one view of intelligence, what you see me arguing about reflects which points people have proved weirdly obstinate about. In 2008, Robin Hanson was being weirdly obstinate about how capabilities scaled and whether there was even any point in analyzing AIs differently from ems, so I talked about what I saw as the most slam-dunk case for there being Plenty Of Room Above Biology and for stuff going whoosh once it got above the human level.</p><p>\"It later turned out that capabilities started scaling a whole lot <i>without</i> self-improvement, which is an example of the kind of weird surprise the Future throws at you, and maybe a case where I missed something by arguing with Hanson instead of imagining how I could be wrong in either direction and not just the direction that other people wanted to argue with me about.</p><p>\"Later on, people were unable to understand why alignment is hard, and got stuck on generalizing the concept I refer to as consequentialism. A theory of why I talked about both things for related reasons would just be a theory of why people got stuck on these two points for related reasons, and I think that theory would mainly be overexplaining an accident because if Yann LeCun had been running effective altruism I would have been explaining different things instead, after the people who talked a lot to EAs got stuck on a different point.\"</p><p>Returning to your second point, humans are broken things; if it were possible to build computers while working even worse than humans, we'd be having this conversation at that level of intelligence instead.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:41]</strong>&nbsp;</p><p>(Retracted)<s>I entirely agree about humans, but it doesn't matter that much how broken humans are when the regime of AIs that we're talking about is the regime that's directly above humans, and therefore only a bit less broken than humans.</s></p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:41]</strong>&nbsp;</p><p>Among the things to bear in mind about that, is that we then get tons of weird phenomena that are specific to humans, and you may be very out of luck if you start wishing for the <i>same</i> weird phenomena in AIs. Yes, even if you make some sort of attempt to train it using a loss function.</p><p>However, it does seem to me like as we start getting towards the Einstein level instead of the village-idiot level, even though this is usually not much of a difference, we do start to see the atmosphere start to thin already, and the turbulence start to settle down already. Von Neumann was actually a fairly reflective fellow who knew about, and indeed helped generalize, utility functions. The great achievements of von Neumann were not achieved by some very specialized hypernerd who spent all his fluid intelligence on crystallizing math and science and engineering alone, and so never developed any opinions about politics or started thinking about whether or not he had a utility function.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:44]</strong>&nbsp;</p><p>I don't think I'm asking for the <i>same</i> weird phenomena. But insofar as a bunch of the phenomena I've been talking about have seemed weird according to your account of consequentialism, then the fact that approximately-human-level-consequentialists have lots of weird things about them is a sign that the phenomena I've been talking about are less unlikely than you expect.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:45][14:46]</strong>&nbsp;</p><p>I suspect that some of the difference here is that I think you have to be <i>noticeably</i> better than a human at nanoengineering to pull off pivotal acts large enough to make a difference, which is why I am not instead trying to gather the smartest people left alive and doing that pivotal act directly.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">I can't think of anything you can do with somebody just barely smarter than a human, which flips the gameboard, aside of course from \"go build a Friendly AI\" which I <i>did</i> try to set up to just go do and which would be incredibly hard to align if we wanted an AI to do it instead (full-blown chicken-and-egg, that AI is already fully aligned).</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:45]</strong>&nbsp;</p><p>Oh, interesting. Actually one more question then: to what extent do you think that explicitly reasoning about utility functions and laws of rationality is what makes consequentialists have the properties you've been talking about?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:47, moved up in log]</strong>&nbsp;</p><p>Explicit reflection is one possible later stage of the path; an earlier part of the path is from being optimized to do things difficult enough that you need to stop stepping on your own feet and have different parts of your thoughts work well together.</p><p>It's the sort of path that has only one destination at its end, so there will be many ways to get there.</p><p>(Modulo various cases where different decision theories seem reflectively consistent and so on; I want to say \"you know what I mean\" but maybe people don't.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:47, moved down in log]</strong>&nbsp;</p><blockquote><p>I suspect that some of the difference here is that I think you have to be <i>noticeably</i> better than a human at nanoengineering to pull off pivotal acts large enough to make a difference, which is why I am not instead trying to gather the smartest people left alive and doing that pivotal act directly.</p></blockquote><p>Yepp, I think there's probably some disagreements about geopolitics driving this too. E.g. in my earlier summary document I mentioned some possible pivotal acts:</p><ul><li>Monitoring all potential AGI projects to an extent that makes it plausible for the US and China to work on a joint project without worrying that the other is privately racing.</li><li>Provide arguments/demonstrations/proofs related to impending existential risk that are sufficiently compelling to scare the key global decision-makers into bottlenecking progress.</li></ul><p>I predict that you think these would not be pivotal enough; but I don't think digging into the geopolitical side of things is the best use of our time.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:49, moved up in log]</strong>&nbsp;</p><p>Monitoring all AGI projects - either not politically feasible in real life given the actual way that countries behave in history books instead of fantasy; or at politically feasible levels, does not work well enough to prevent the world from ending once the know-how proliferates. The AI isn't doing much work here either; why not go do this now, if it's possible? (Note: please don't try to go do this now, it backfires badly.)</p><p>Provide sufficiently compelling arguments = superhuman manipulation, an incredibly dangerous domain that is just about the worst domain to try to align.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:49, moved down in log]</strong>&nbsp;</p><blockquote><p>With regards to your first point, my first reaction was, \"I just have one view of intelligence, what you see me arguing about reflects which points people have proved weirdly obstinate about. In 2008, Robin Hanson was being weirdly obstinate about how capabilities scaled and whether there was even any point in analyzing AIs differently from ems, so I talked about what I saw as the most slam-dunk case for there being Plenty Of Room Above Biology and for stuff going whoosh once it got above the human level.</p><p>\"It later turned out that capabilities started scaling a whole lot <i>without</i> self-improvement, which is an example of the kind of weird surprise the Future throws at you, and maybe a case where I missed something by arguing with Hanson instead of imagining how I could be wrong in either direction and not just the direction that other people wanted to argue with me about.</p><p>\"Later on, people were unable to understand why alignment is hard, and got stuck on generalizing the concept I refer to as consequentialism. A theory of why I talked about both things for related reasons would just be a theory of why people got stuck on these two points for related reasons, and I think that theory would mainly be overexplaining an accident because if Yann LeCun had been running effective altruism I would have been explaining different things instead, after the people who talked a lot to EAs got stuck on a different point.\"</p></blockquote><p>On my first point, it seems to me that your claims about recursive self-improvement were off in a fairly similar way to how I think your claims about consequentialism are off - which is that they defer too much to one very high-level abstraction.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:52]</strong>&nbsp;</p><blockquote><p>On my first point, it seems to me that your claims about recursive self-improvement were off in a fairly similar way to how I think your claims about consequentialism are off - which is that they defer too much to one very high-level abstraction.</p></blockquote><p>I suppose that is what it could potentially feel like from the inside to not get an abstraction. Robin Hanson kept on asking why I was trusting my abstractions so much, when he was in the process of trusting his worse abstractions instead.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][14:51][14:53]</strong>&nbsp;</p><blockquote><p>Explicit reflection is one possible later stage of the path; an earlier part of the path is from being optimized to do things difficult enough that you need to stop stepping on your own feet and have different parts of your thoughts work well together.</p></blockquote><p>Can you explain a little more what you mean by \"have different parts of your thoughts work well together\"? Is this something like the capacity for metacognition; or the global workspace; or self-control; or...?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">And I guess there's no good way to quantify <i>how</i> important you think the explicit reflection part of the path is, compared with other parts of the path - but any rough indication of whether it's a more or less crucial component of your view?</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][14:55]</strong>&nbsp;</p><blockquote><p>Can you explain a little more what you mean by \"have different parts of your thoughts work well together\"? Is this something like the capacity for metacognition; or the global workspace; or self-control; or...?</p></blockquote><p>No, it's like when you don't, like, pay five apples for something on Monday, sell it for two oranges on Tuesday, and then trade an orange for an apple.</p><p>I have still not figured out the homework exercises to convey to somebody the Word of Power which is \"coherence\" by which they will be able to look at the water, and see \"coherence\" in places like a cat walking across the room without tripping over itself.</p><p>When you do lots of reasoning about arithmetic correctly, without making a misstep, that long chain of thoughts with many different pieces diverging and ultimately converging, ends up making some statement that is... still true and still about numbers! Wow! How do so many different thoughts add up to having this property? Wouldn't they wander off and end up being about tribal politics instead, like on the Internet?</p><p>And one way you could look at this, is that even though all these thoughts are taking place in a bounded mind, they are shadows of a higher unbounded structure which is the model identified by the Peano axioms; all the things being said are <i>true about the numbers</i>. Even though somebody who was missing the point would at once object that the human contained no mechanism to evaluate each of their statements against all of the numbers, so obviously no human could ever contain a mechanism like that, so obviously you can't explain their success by saying that each of their statements was true about the same topic of the numbers, because what could possibly implement that mechanism which (in the person's narrow imagination) is The One Way to implement that structure, which humans don't have?</p><p>But though mathematical reasoning can sometimes go astray, when it works at all, it works because, in fact, even bounded creatures can sometimes manage to obey local relations that in turn add up to a global coherence where all the pieces of reasoning point in the same direction, like photons in a laser lasing, even though there's no internal mechanism that enforces the global coherence at every point.</p><p>To the extent that the outer optimizer trains you out of paying five apples on Monday for something that you trade for two oranges on Tuesday and then trading two oranges for four apples, the outer optimizer is training all the little pieces of yourself to be locally coherent in a way that can be seen as an imperfect bounded shadow of a higher unbounded structure, and then the system is powerful though imperfect <i>because</i> of how the power is present in the coherence and the overlap of the pieces, <i>because</i> of how the higher perfect structure is being imperfectly shadowed. In this case the higher structure I'm talking about is Utility, and doing homework with coherence theorems leads you to appreciate that we only know about one higher structure for this class of problems that has a dozen mathematical spotlights pointing at it saying \"look here\", even though people have occasionally looked for alternatives.</p><p>And when I try to say this, people are like, \"Well, I looked up a theorem, and it talked about being able to identify a unique utility function from an infinite number of choices, but if we don't have an infinite number of choices, we can't identify the utility function, so what relevance does this have\" and this is a kind of mistake I don't remember even coming close to making so I do not know how to make people stop doing that and maybe I can't.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][15:07]</strong>&nbsp;</p><p>We're already pushing our luck on time, so I nominate that we wrap up (after, perhaps, a few more Richard responses if he's got juice left.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:07]</strong>&nbsp;</p><p>Yeah, was thinking the same.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][15:07]</strong>&nbsp;</p><p>As a proposed cliffhanger to feed into the next discussion, my take is that Richard's comment:</p><blockquote><p>On my first point, it seems to me that your claims about recursive self-improvement were off in a fairly similar way to how I think your claims about consequentialism are off - which is that they defer too much to one very high-level abstraction.</p></blockquote><p>probably contains some juicy part of the disagreement, and I'm interested in Eliezer understanding Richard's claim to the point of being able to paraphrase it to Richard's satisfaction.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:08]</strong>&nbsp;</p><p>Wrapping up here makes sense.</p><p>I endorse the thing Nate just said.</p><p>I also get the sense that I have a much better outline now of Eliezer's views about consequentialism (if not the actual details and texture).</p><p>On a meta level, I personally tend to focus more on things like \"how should we understand cognition\" and not \"how should we understand geopolitics and how it affects the level of pivotal action required\".</p><p>If someone else were trying to prosecute this disagreement they might say much more about the latter. I'm uncertain how useful it is for me to do so, given that my comparative advantage compared with the rest of the world (and probably Eliezer's too) is the cognition part.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:12]</strong>&nbsp;</p><p>Reconvene... tomorrow? Monday of next week?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:12]</strong>&nbsp;</p><p>Monday would work better for me.</p><p>You okay with me summarising the discussion so far to [some people — redacted for privacy reasons]?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:13]</strong>&nbsp;</p><p>Nate, take a minute to think of your own thoughts there?</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">[Soares: 👍 👌]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][15:15]</strong>&nbsp;</p><p>My take: I think it's fine to summarize, though generally virtuous to mark summaries as summaries (rather than asserting that your summaries are Eliezer-endorsed or w/e).</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:16]</strong>&nbsp;</p><p>I think that broadly matches my take. I'm also a bit worried about biases in the text summarizer, and about whether I managed to say anything that Rob or somebody will object to pre-publication, but we ultimately intended this to be seen and I was keeping that in mind, so, yeah, go ahead and summarize.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:17]</strong>&nbsp;</p><p>Great, thanks</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:17]</strong>&nbsp;</p><p>I admit to being curious as to what you thought was said that was important or new, but that's a question that can be left open to be answered at your leisure, earlier in your day.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:17]</strong>&nbsp;</p><blockquote><p>I admit to being curious as to what you thought was said that was important or new, but that's a question that can be left open to be answered at your leisure, earlier in your day.</p></blockquote><p>You mean, what I thought was worth summarising?</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:17]</strong>&nbsp;</p><p>Yeah.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:18]</strong>&nbsp;</p><p>Hmm, no particular opinion. I wasn't going to go out of my way to do so, but since I'm chatting to [some people — redacted for privacy reasons] regularly anyway, it seemed low-cost to fill them in.</p><p>At your leisure, I'd be curious to know how well the directions of discussion are meeting your goals for what you want to convey when this is published, and whether there are topics you want to focus on more.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:19]</strong>&nbsp;</p><p>I don't know if it's going to help, but trying it currently seems better than to go on saying nothing.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:20]</strong>&nbsp;</p><p>(personally, in addition to feeling like less of an expert on geopolitics, it also seems more sensitive for me to make claims about in public, which is another reason I haven't been digging into that area as much)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][15:21]</strong>&nbsp;</p><blockquote><p>(personally, in addition to feeling like less of an expert on geopolitics, it also seems more sensitive for me to make claims about in public, which is another reason I haven't been digging into that area as much)</p></blockquote><p>(seems reasonable! note, though, that i'd be quite happy to have sensitive sections stricken from the record, insofar as that lets us get more convergence than we otherwise would, while we're already in the area)</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">[Ngo: 👍]</td></tr></tbody></table></figure><p>(tho ofc it is less valuable to spend conversational effort in private discussions, etc.)</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">[Ngo: 👍]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:22]</strong>&nbsp;</p><blockquote><p>At your leisure, I'd be curious to know how well the directions of discussion are meeting your goals for what you want to convey when this is published, and whether there are topics you want to focus on more.</p></blockquote><p>(this question aimed at you too Nate)</p><p>Also, thanks Nate for the moderation! I found your interventions well-timed and useful.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">[Soares: ❤️]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][15:23]</strong>&nbsp;</p><blockquote><p>(this question aimed at you too Nate)</p></blockquote><p>(noted, thanks, I'll probably write something up after you've had the opportunity to depart for sleep.)</p><p>On that note, I declare us adjourned, with intent to reconvene at the same time on Monday.</p><p>Thanks again, both.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:23]</strong>&nbsp;</p><p>Thanks both 🙂</p><p>Oh, actually, one quick point</p><p>Would one hour earlier suit, for Monday?</p><p>I've realised that I'll be moving to a one-hour-later time zone, and starting at 9pm is slightly suboptimal (but still possible if necessary)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][15:24]</strong>&nbsp;</p><p>One hour earlier would work fine for me.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:25]</strong>&nbsp;</p><p>Doesn't work as fine for me because I've been trying to avoid any food until 12:30p my time, but on that particular day I may be more caloried than usual from the previous day, and could possibly get away with it. (That whole day could also potentially fail if a minor medical procedure turns out to take more recovery than it did the last time I had it.)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:26]</strong>&nbsp;</p><p>Hmm, is this something where you'd have more information on the day? (For the calories thing)</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:27]</strong>&nbsp;</p><blockquote><p>(seems reasonable! note, though, that i'd be quite happy to have sensitive sections stricken from the record, insofar as that lets us get more convergence than we otherwise would, while we're already in the area)</p></blockquote><p>I'm a touch reluctant to have discussions that we intend to delete, because then the larger debate will make less sense once those sections are deleted. Let's dance around things if we can.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">[Ngo: 👍]</td><td style=\"border:1pt solid hsl(0, 0%, 0%)\">[Soares: 👍]</td></tr></tbody></table></figure><p>I mean, I can that day at 10am my time say how I am doing and whether I'm in shape for that day.</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][15:28]</strong>&nbsp;</p><p>great. and if at that point it seems net positive to postpone to 11am your time (at the cost of me being a bit less coherent later on) then feel free to say so at the time</p><p>on that note, I'm off</p></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Yudkowsky][15:29]</strong>&nbsp;</p><p>Good night, heroic debater!</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][16:11]</strong>&nbsp;</p><blockquote><p>At your leisure, I'd be curious to know how well the directions of discussion are meeting your goals for what you want to convey when this is published, and whether there are topics you want to focus on more.</p></blockquote><p>The discussions so far are meeting my goals quite well so far! (Slightly better than my expectations, hooray.) Some quick rough notes:</p><ul><li>I have been enjoying EY explicating his models around consequentialism.<ul><li>The objections Richard has been making are ones I think have been floating around for some time, and I'm quite happy to see explicit discussion on it.</li><li>Also, I've been appreciating the conversational virtue with which the two of you have been exploring it. (Assumption of good intent, charity, curiosity, etc.)</li></ul></li><li>I'm excited to dig into Richard's sense that EY was off about recursive self improvement, and is now off about consequentialism, in a similar way.<ul><li>This also sees to me like a critique that's been floating around for some time, and I'm looking forward to getting more clarity on it.</li></ul></li><li>I'm a bit torn between driving towards clarity on the latter point, and shoring up some of the progress on the former point.<ul><li>One artifact I'd really enjoy having is some sort of \"before and after\" take, from Richard, contrasting his model of EY's views before, to his model now.</li><li>I also have a vague sense that there are some points Eliezer was trying to make, that didn't quite feel like they were driven home; and dually, some pushback by Richard that didn't feel quite frontally answered.<ul><li>One thing I may do over the next few days is make a list of those places, and see if I can do any distilling on my own. (No promises, though.)</li><li>If that goes well, I might enjoy some side-channel back-and-forth with Richard about it, eg during some more convenient-for-Richard hour (or, eg, as a thing to do on Monday if EY's not in commission at 10a pacific.)</li></ul></li></ul></li></ul></td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Ngo][5:40] &nbsp;(next day, Sep. 9)</strong>&nbsp;</p><blockquote><p>The discussions so far are [...]</p></blockquote><p>What do you mean by \"latter point\" and \"former point\"? (In your 6th bullet point)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%);vertical-align:top\"><p><strong>[Soares][7:09] &nbsp;(next day, Sep. 9)</strong>&nbsp;</p><blockquote><p>What do you mean by \"latter point\" and \"former point\"? (In your 6th bullet point)</p></blockquote><p>former = shoring up the consequentialism stuff, latter = digging into your critique re: recursive self improvement etc. (The nesting of the bullets was supposed to help make that clear, but didn't come out well in this format, oops.)</p></td></tr></tbody></table>\n\n4\\. Follow-ups\n==============\n\n4.1. Richard Ngo's summary\n--------------------------\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p><i>2nd discussion</i></p><p>(Mostly summaries not quotations<s>; also hasn’t yet been evaluated by Eliezer</s>)</p><p>Eliezer, summarized by Richard: \"<s>The</s> A core concept which people have trouble grasping is consequentialism. People try to reason about <i>how</i> AIs will solve problems, and ways in which they might or might not be dangerous. But they don’t realise that the ability to solve a wide range of difficult problems implies that an agent must be doing a powerful search over possible solutions, which is <s>the</s> a core skill required to take actions which greatly affect the world. Making this type of AI safe is like trying to build an AI that drives red cars very well, but can’t drive blue cars - there’s no way you get this by default, because the skills involved are so similar. And because the search process <s>is so general</s> is by default so general, <s>it’ll be very hard to</s> I don’t currently see how to constrain it into any particular region.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:48] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>The</p></blockquote><p><i>A</i> concept, which some people have had trouble grasping. &nbsp;There seems to be an endless list. &nbsp;I didn't have to spend much time contemplating consequentialism to derive the consequences. &nbsp;I didn't spend a lot of time talking about it until people started arguing.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:50] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>the</p></blockquote><p>a</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:52] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>[the search process] is [so general]</p></blockquote><p>\"is by default\". &nbsp;The reason I keep emphasizing that things are only true by default is that the work of surviving may look like doing hard nondefault things. &nbsp;I don't take fatalistic \"will happen\" stances, I assess difficulties of getting nondefault results.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:52] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>it’ll be very hard to</p></blockquote><p>\"I don't currently see how to\"</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Eliezer, summarized by Richard (continued): \"In biological organisms, evolution is <s>one source</s> the ultimate source of consequentialism. A <s>second</s> secondary outcome of evolution is reinforcement learning. For an animal like a cat, upon catching a mouse (or failing to do so) many parts of its brain get slightly updated, in a loop that makes it more likely to catch the mouse next time. (Note, however, that this process isn’t powerful enough to make the cat a pure consequentialist - rather, it has many individual traits that, when we view them from this lens, point in the same direction.) <s>A third thing that makes humans in particular consequentialist is planning,</s> Another outcome of evolution, which helps make humans in particular more consequentialist, is planning - especially when we’re aware of concepts like utility functions.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:53] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>one</p></blockquote><p>the ultimate</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:53] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>second</p></blockquote><p>secondary outcome of evolution</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:55] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>especially when we’re aware of concepts like utility functions</p></blockquote><p>Very slight effect on human effectiveness in almost all cases because humans have very poor reflectivity.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Consider an AI that, given a hypothetical scenario, tells us what the best plan to achieve a certain goal in that scenario is. Of course it needs to do consequentialist reasoning to figure out how to achieve the goal. But that’s different from an AI which chooses what to say as a means of achieving its goals. I’d argue that the former is doing consequentialist reasoning without itself being a consequentialist, while the latter is actually a consequentialist. Or more succinctly: consequentialism = problem-solving skills + using those skills to choose actions which achieve goals.\"</p><p>Eliezer, summarized by Richard: \"The former AI might be slightly safer than the latter if you could build it, but I think people are likely to dramatically overestimate how big the effect is. The difference could just be one line of code: if we give the former AI our current scenario as its input, then it becomes the latter.&nbsp; For purposes of understanding alignment difficulty, you want to be thinking on the level of abstraction where you see that in some sense it is the search itself that is dangerous when it's a strong enough search, rather than the danger seeming to come from details of the planning process. One particularly helpful thought experiment is to think of advanced AI as an '<a href=\"https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes\"><u>outcome pump</u></a>' which selects from futures in which a certain outcome occurred, and takes whatever action leads to them.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][10:59] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>particularly helpful</p></blockquote><p>\"attempted explanatory\". &nbsp;I don't think most readers got it.</p><p>I'm a little puzzled by how often you write my viewpoint as thinking that whatever I happened to say a sentence about is the Key Thing. &nbsp;It seems to rhyme with a deeper failure of many EAs to pass the MIRI <a href=\"https://www.econlib.org/archives/2011/06/the_ideological.html\">ITT</a>.</p><p>To be a bit blunt and impolite in hopes that long-languishing social processes ever get anywhere, two obvious uncharitable explanations for why some folks may systematically misconstrue MIRI/Eliezer as believing much more than in reality that various concepts an argument wanders over are Big Ideas to us, when some conversation forces us to go to that place:</p><p>(A)&nbsp; It paints a comfortably unflattering picture of MIRI-the-Other as weirdly obsessed with these concepts that seem not so persuasive, or more generally paints the Other as a bunch of weirdos who stumbled across some concept like \"consequentialism\" and got obsessed with it.&nbsp; In general, to depict the Other as thinking a great deal of some idea (or explanatory thought experiment) is to tie and stake their status to the listener's view of how much status that idea deserves.&nbsp; So if you say that the Other thinks a great deal of some idea that isn't obviously high-status, that lowers the Other's status, which can be a comfortable thing to do.</p><p>(cont.)</p><p>(B) It paints a more comfortably self-flattering picture of a continuing or persistent disagreement, as a disagreement with somebody who thinks that some random concept is much higher-status than it really is, in which case there isn't more to done or understood except to duly politely let the other person try to persuade you the concept deserves its high status. As opposed to, \"huh, maybe there is a noncentral point that the other person sees themselves as being stopped on and forced to explain to me\", which is a much less self-flattering viewpoint on why the conversation is staying within a place.&nbsp; And correspondingly more of a viewpoint that somebody else is likely to have of us, because it is a comfortable view to them, than a viewpoint that it is comfortable to us to imagine them having.</p><p>Taking the viewpoint that somebody else is getting hung up on a relatively noncentral point can also be a flattering self-portrait to somebody who believes that, of course.&nbsp; It doesn't mean they're right.&nbsp; But it does mean that you should be aware of how the Other's story, told from the Other's viewpoint, is much more liable to be something that the Other finds sensible and perhaps comfortable, even if it implies an unflattering (and untrue-seeming and perhaps untrue) view of yourself, than something that makes the Other seem weird and silly and which it is easy and congruent for you yourself to imagine the Other thinking.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][11:18] &nbsp;(Sep. 12 comment)</strong>&nbsp;</p><blockquote><p>I'm a little puzzled by how often you write my viewpoint as thinking that whatever I happened to say a sentence about is the Key Thing.</p></blockquote><p>In this case, I emphasised the outcome pump thought experiment because you said that the time-travelling scenario was a key moment for your understanding of optimisation, and the outcome pump seemed to be similar&nbsp;enough and easier to convey in the summary, since you'd already written about it.</p><p>I'm also emphasising consequentialism because it seemed like the core idea which kept coming up in our first debate, under the heading of \"deep problem-solving patterns\". Although I take your earlier point that you tend to emphasise things that your interlocutor is more skeptical about, not necessarily the things which are most central to your view. But if consequentialism isn't in fact a very central concept for you, I'd be interested to hear what role it plays.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"There’s a component of 'finding a plan which achieves a certain outcome' which involves actually solving the object-level problem of how someone who is given the plan can achieve the outcome. And there’s another component which is figuring out how to manipulate that person into doing what you want. To me it seems like Eliezer’s argument is that there’s no training regime which leads an AI to spend 99% of its time thinking about the former, and 1% thinking about the latter.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:20] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>no training regime</p></blockquote><p>...that the training regimes we come up with first, in the 3 months or 2 years we have before somebody else destroys the world, will not have this property.</p><p>I don't have any particularly complicated or amazingly insightful theories of why I keep getting depicted as a fatalist; but my world is full of counterfactual functions, not constants.&nbsp; And I am always aware that if we had access to a real Textbook from the Future explaining all of the methods that are actually robust in real life - the equivalent of telling us in advance about all the ReLUs that in real life were only invented and understood a few decades after sigmoids - we could go right ahead and build a superintelligence that thinks 2 + 2 = 5.</p><p>All of my assumptions about \"I don't see how to do X\" are always labeled as ignorance on my part and a default because we won't have enough time to actually figure out how to do X.&nbsp; I am constantly maintaining awareness of this because being <strong>wrong</strong> about it being difficult is a major place where <strong>hope</strong> potentially comes from, if there's some idea like ReLUs that robustly vanquishes the difficulty, which I just didn't think of.&nbsp; Which does not, alas, mean that I am wrong about any particular thing, nor that the infinite source of optimistic ideas that is the wider field of \"AI alignment\" is going to produce a good idea from the same process that generates all the previous naive optimism through not seeing where the original difficulty comes from or what other difficulties surround obvious naive attempts to solve it.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard (continued): \"While this may be true in the limit of increasing intelligence, the most relevant systems are the earliest ones that are above human level. But humans deviate from the consequentialist abstraction you’re talking about in all sorts of ways - for example, being raised in different cultures can make people much more or less consequentialist. So it seems plausible that early AGIs can be superhuman while also deviating strongly from this abstraction - not necessarily in the same ways as humans, but in ways that we push them towards during training.\"</p><p>Eliezer, summarized by Richard: \"Even at the Einstein or von Neumann level these types of deviations start to subside. And the sort of pivotal acts which might realistically work require skills <i>significantly</i> above human level. I think even 1% of the cognition of an AI that can assemble advanced nanotech, thinking about how to kill humans, would doom us. Your other suggestions for pivotal acts (surveillance to restrict AGI proliferation; persuading world leaders to restrict AI development) are not politically feasible in real life, to the level required to prevent the world from ending; or else require alignment in the very dangerous domain of superhuman manipulation.\"</p><p>Richard, summarized by Richard: \"I think we probably also have significant disagreements about geopolitics which affect which acts we expect to be pivotal, but it seems like our comparative advantage is in discussing cognition, so let’s focus on that. We can build systems that outperform humans at quite a few tasks by now, without them needing search abilities that are general enough to even try to take over the world. Putting aside for a moment the question of which tasks are pivotal enough to save the world, which parts of your model draw the line between human-level chess players and human-level galaxy-colonisers, and say that we'll be able to align ones that significantly outperform us on <i>these</i> tasks before they take over the world, but not on <i>those</i> tasks?\"</p><p>Eliezer, summarized by Richard: \"One aspect there is domain generality which in turn is achieved through novel domain learning. One can imagine asking the question: is there a superintelligent AGI that can quickly build nanotech the way that a beaver solves building dams, in virtue of having a bunch of specialized learning abilities without it ever having a cross-domain general learning ability? But there are many, many, many things that humans do which no other animal does, which you might think would contribute a lot to that animal's fitness if there were animalistic ways to do it - e.g. mining and smelting iron. (Although comparisons to animals are not generally reliable arguments about what AIs can do - e.g. chess is much easier for chips than neurons.) So my answer is 'Perhaps, but not by default, there's a bunch of subproblems, I don't actually know how to do it right now, it's not the easiest way to get an AGI that can build nanotech.' <s>Can I explain how I know that? I'm really not sure I can.</s>\"</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][11:26] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>Can I explain how I know that? I'm really not sure I can.</p></blockquote><p>In original text, this sentence was followed by a long attempt to explain anyways; if deleting that, which is plausibly the correct choice, this lead-in sentence should also be deleted, as otherwise it paints a false picture of how much I would try to explain anyways.</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo][11:15] &nbsp;(Sep. 12 comment)</strong>&nbsp;</p><p>Makes sense; deleted.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Challenges which are trivial from a human-engineering perspective can be very challenging from an evolutionary perspective (e.g. spinning wheels). So the evolution of animals-with-a-little-bit-of-help-from-humans might end up in very different places from the evolution of animals-just-by-themselves. And analogously, the ability of humans to fill in the gaps to help less general AIs achieve more might be quite significant.</p><p>\"On nanotech: what are the most relevant axes of difference between solving protein folding and designing nanotech that, say, self-assembles into a computer?\"</p><p>Eliezer, summarized by Richard: \"This question seemed potentially cruxy to me. I.e., if building protein factories that built nanofactories that built nanomachines that met a certain deep and lofty engineering goal, didn't involve cognitive challenges different in kind from protein folding, we could maybe just safely go do that using AlphaFold 3, which would be just as safe as AlphaFold 2. I don't think we can do that. But it is among the more plausible advance-specified miracles we could get. At this point our last hope is that in fact the future is often quite surprising.\"</p><p>Richard, summarized by Richard: \"It seems to me that you’re making the same mistake here as you did with regards to recursive self-improvement in the AI foom debate - namely, putting too much trust in one big abstraction.\"</p><p>Eliezer, summarized by Richard: \"I suppose that is what it could potentially feel like from the inside to not get an abstraction.&nbsp; Robin Hanson kept on asking why I was trusting my abstractions so much, when he was in the process of trusting his worse abstractions instead.\"</p></td></tr></tbody></table>\n\n4.2. Nate Soares' summary\n-------------------------\n\n<table><tbody><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p><i>Consequentialism</i></p><p>Ok, here's a handful of notes. I apologize for not getting them out until midday Sunday. My main intent here is to do some shoring up of the ground we've covered. I'm hoping for skims and maybe some light comment back-and-forth as seems appropriate (perhaps similar to Richard's summary), but don't think we should derail the main thread over it. If time is tight, I would not be offended for these notes to get little-to-no interaction.</p><p>---</p><p>My sense is that there's a few points Eliezer was trying to transmit about consequentialism, that I'm not convinced have been received. I'm going to take a whack at it. I may well be wrong, both about whether Eliezer is in fact attempting to transmit these, and about whether Richard received them; I'm interested in both protests from Eliezer and paraphrases from Richard.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>1. \"The consequentialism is in the plan, not the cognition\".</p><p>I think Richard and Eliezer are coming at the concept \"consequentialism\" from very different angles, as evidenced eg by Richard saying (Nate's crappy paraphrase:) \"where do you think the consequentialism is in a cat?\" and Eliezer responding (Nate's crappy paraphrase:) \"the cause of the apparent consequentialism of the cat's behavior is distributed between its brain and its evolutionary history\".</p><p>In particular, I think there's an argument here that goes something like:</p><ul><li>Observe that, from our perspective, saving the world seems quite tricky, and seems likely to involve long sequences of clever actions that force the course of history into a narrow band (eg, because if we saw short sequences of dumb actions, we could just get started).</li><li>Suppose we were presented with a plan that allegedly describes a long sequence of clever actions that would, if executed, force the course of history into some narrow band.<ul><li>For concreteness, suppose it is a plan that allegedly funnels history into the band where we have wealth and acclaim.</li></ul></li><li>One plausible happenstance is that the plan is not in fact clever, and would not in fact have a forcing effect on history.<ul><li>For example, perhaps the plan describes founding and managing some silicon valley startup, that would not work in practice.</li></ul></li><li>Conditional on the plan having the history-funnelling property, there's a sense in which it's scary regardless of its source.<ul><li>For instance, perhaps the plan describes founding and managing some silicon valley startup, and will succeed virtually every time it's executed, by dint of having very generic descriptions of things like how to identify and respond to competition, including descriptions of methods for superhumanly-good analyses of how to psychoanalyze the competition and put pressure on their weakpoints.</li><li>In particular, note that one need not believe the plan was generated by some \"agent-like\" cognitive system that, in a self-contained way, made use of reasoning we'd characterize as \"possessing objectives\" and \"pursuing them in the real world\".</li><li>More specifically, the scariness is a property of the plan itself. For instance, the fact that this plan accrues wealth and acclaim to the executor, in a wide variety of situations, regardless of what obstacles arise, implies that the plan contains course-correcting mechanisms that keep the plan on-target.</li><li>In other words, plans that <i>manage to actually funnel history</i> are (the argument goes) liable to have a wide variety of course-correction mechanisms that keep the plan oriented towards <i>some</i> target. And while this course-correcting property tends to be a property of history-funneling plans, the <i>choice of target</i> is of course free, hence the worry.</li></ul></li></ul><p>(Of course, in practice we perhaps shouldn't be visualizing a single Plan handed to us from an AI or a time machine or whatever, but should instead imagine a system that is reacting to contingencies and replanning in realtime. At the least, this task is easier, as one can adjust only for the contingencies that are beginning to arise, rather than needing to predict them all in advance and/or describe general contingency-handling mechanisms. But, and feel free to take a moment to predict my response before reading the next sentence, \"run this AI that replans autonomously on-the-fly\" and \"run this AI+human loop that replans+reevaluates on the fly\", are still in this sense \"plans\", that still likely have the property of Eliezer!consequentialism, insofar as they work.)</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>There's a part of this argument I have not yet driven home. Factoring it out into a separate bullet:</p><p>2. \"If a plan is good enough to work, it's pretty consequentialist in practice\".</p><p>In attempts to collect and distill a handful of scattered arguments of Eliezer's:</p><p>If you ask GPT-3 to generate you a plan for saving the world, it will not manage to generate one that is very detailed. And if you tortured a big language model into giving you a detailed plan for saving the world, the resulting plan would not work. In particular, it would be full of errors like insensitivity to circumstance, suggesting impossible actions, and suggesting actions that run entirely at cross-purposes to one another.</p><p>A plan that is sensitive to circumstance, and that describes actions that synergize rather than conflict -- like, in Eliezer's analogy, photons in a laser -- is much better able to funnel history into a narrow band.</p><p>But, on Eliezer's view as I understand it, this \"the plan is not constantly tripping over its own toes\" property, goes hand-in-hand with what he calls \"consequentialism\". As a particularly stark and formal instance of the connection, observe that one way a plan can trip over its own toes is if it says \"then trade 5 oranges for 2 apples, then trade 2 apples for 4 oranges\". This is clearly an instance of the plan failing to \"lase\" -- of some orange-needing part of the plan working at cross-purposes to some apple-needing part of the plan, or something like that. And this is also a case where it's easy to see how if a plan <i>is</i> \"lasing\" with respect to apples and oranges, then it is behaving as if governed by some coherent preference.</p><p>And the point as I understand it isn't \"all toe-tripping looks superficially like an inconsistent preference\", but rather \"insofar as a plan <i>does</i> manage to chain a bunch of synergistic actions together, it manages to do so precisely insofar as it is Eliezer!consequentialist\".</p><p>cf the analogy to <a href=\"https://www.lesswrong.com/s/oFePMp9rKftEeZDDr/p/QkX2bAkwG2EpGvNug\">information theory</a>, where if you're staring at a maze and you're trying to build an accurate representation of that maze in your own head, you will succeed precisely insofar as your process is Bayesian / information-theoretic. And, like, this is supposed to feel like a fairly tautological claim: you (almost certainly) can't get the image of a maze in your head to match the maze in the world by visualizing a maze at random, you have to add visualized-walls using some process that's correlated with the presence of actual walls. Your maze-visualizing process will work precisely insofar as you have access to &amp; correctly make use of, observations that correlate with the presence of actual walls. You might also visualize extra walls in locations where it's politically expedient to believe that there's a wall, and you might also avoid visualizing walls in a bunch of distant regions of the maze because it's dark and you haven't got all day, but the resulting visualization in your head is accurate precisely <i>insofar</i> as you're managing to act kinda like a Bayesian.</p><p>Similarly (the analogy goes), a plan works-in-concert and avoids-stepping-on-its-own-toes precisely insofar as it is consequentialist. These are two sides of the same coin, two ways of seeing the same thing.</p><p>And, I'm not so much attempting to <i>argue</i> the point here, as to make sure that the <i>shape of the argument</i> (as I understand it) has been understood by Richard. In particular, the <i>shape of the argument</i> I see Eliezer as making is that \"clumsy\" plans don't work, and \"laser-like plans\" work insofar as they are managing to act kinda like a consequentialist.</p><p>Rephrasing again: we have a wide variety of mathematical theorems all spotlighting, from different angles, the fact that a plan lacking in clumsiness, is possessing of coherence.</p><p>(\"And\", my model of Eliezer is quick to note, \"this ofc does not mean that all sufficiently intelligent minds must generate very-coherent plans. If you really knew what you were doing, you could design a mind that emits plans that always \"trip over themselves\" along one particular axis, just as with sufficient mastery you could build a mind that believes 2+2=5 (for some reasonable cashing-out of that claim). But you don't get this for free -- and there's a sort of \"attractor\" here, when building cognitive systems, where just as generic training will tend to cause it to have true beliefs, so will generic training tend to cause its plans to lase.\")</p><p>(And ofc much of the worry is that all the mathematical theorems that suggest \"this plan manages to work precisely insofar as it's lasing in some direction\", say nothing about which direction it must lase. Hence, if you show me a plan clever enough to force history into some narrow band, I can be fairly confident it's doing a bunch of lasing, but not at all confident which direction it's lasing in.)</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>One of my guesses is that Richard does in fact understand this argument (though I personally would benefit from a paraphrase, to test this hypothesis!), and perhaps even buys it, but that Richard gets off the train at a following step, namely that we <i>need</i> plans that \"lase\", because ones that don't aren't strong enough to save us. (Where in particular, I suspect most of the disagreement is in how far one can get with plans that are more like language-model outputs and less like lasers, rather than in the question of which pivotal acts would put an end to the acute risk period)</p><p>But setting that aside for a moment, I want to use the above terminology to restate another point I saw Eliezer as attempting to make: one big trouble with alignment, in the case where we need our plans to be like lasers, is that on the one hand we need our plans to be like lasers, but on the other hand we want them to <i>fail</i> to be like lasers along certain specific dimensions.</p><p>For instance, the plan presumably needs to involve all sorts of mechanisms for refocusing the laser in the case where the environment contains fog, and redirecting the laser in the case where the environment contains mirrors (...the analogy is getting a bit strained here, sorry, bear with me), so that it can in fact hit a narrow and distant target. Refocusing and redirecting to stay on target are part and parcel to plans that can hit narrow distant targets.</p><p>But the humans shutting the AI down is like scattering the laser, and the humans tweaking the AI so that it plans in a different direction is like them tossing up mirrors that redirect the laser; and we want the plan to fail to correct for those interferences.</p><p>As such, on the Eliezer view as I understand it, we can see ourselves as asking for a very unnatural sort of object: a path-through-the-future that is robust enough to funnel history into a narrow band in a very wide array of circumstances, but somehow insensitive to specific breeds of human-initiated attempts to switch which narrow band it's pointed towards.</p><p>Ok. I meandered into trying to re-articulate the point over and over until I had a version distilled enough for my own satisfaction (which is much like arguing the point), apologies for the repetition.</p><p>I don't think debating the claim is the right move at the moment (though I'm happy to hear rejoinders!). Things I would like, though, are: Eliezer saying whether the above is on-track from his perspective (and if not, then poking a few holes); and Richard attempting to paraphrase the above, such that I believe the arguments themselves have been communicated (saying nothing about whether Richard also buys them).</p><p>---</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>My Richard-model's stance on the above points is something like \"This all seems kinda plausible, but where Eliezer reads it as arguing that we had better figure out how to handle lasers, I read it as an argument that we'd better save the world without needing to resort to lasers. Perhaps if I thought the world could not be saved except by lasers, I would share many of your concerns, but I do not believe that, and in particular it looks to me like much of the recent progress in the field of AI -- from AlphaGo to GPT to AlphaFold -- is evidence in favor of the proposition that we'll be able to save the world without lasers.\"</p><p>And I recall actual-Eliezer saying the following (more-or-less in response, iiuc, though readers note that I might be misunderstanding and this might be out-of-context):</p><blockquote><p>Definitely, \"turns out it's easier than you thought to use gradient descent's memorization of zillions of shallow patterns that overlap and recombine into larger cognitive structures, to add up to a consequentialist nanoengineer that only does nanosystems and never does sufficiently general learning to apprehend the big picture containing humans, while still understanding the goal for that pivotal act you wanted to do\" is among the more plausible advance-specified miracles we could get.&nbsp;</p></blockquote><p>On my view, and I think on Eliezer's, the \"zillions of shallow patterns\"-style AI that we see today, is not going to be sufficient to save the world (nor destroy it). There's a bunch of reasons that GPT and AlphaZero aren't destroying the world yet, and one of them is this \"shallowness\" property. And, yes, maybe we'll be wrong! I myself have been surprised by how far the shallow pattern memorization has gone (and, for instance, was surprised by GPT), and acknowledge that perhaps I will continue to be surprised. But I continue to predict that the shallow stuff won't be enough.</p><p>I have the sense that lots of folk in the community are, one way or another, saying \"Why not consider the problems of aligning systems that memorize zillions of shallow patterns?\". And my answer is, \"I still don't expect those sorts of machines to either kill or save us, I'm still expecting that there's a phase shift that won't happen until AI systems start to be able to make plans that are sufficiently deep and laserlike to do scary stuff, and I'm still expecting that the real alignment challenges are in that regime.\"</p><p>And this seems to me close to the heart of the disagreement: some people (like me!) have an intuition that it's quite unlikely that figuring out how to get sufficient work out of shallow-memorizers is enough to save us, and I suspect others (perhaps even Richard!) have the sense that the aforementioned \"phase shift\" is the unlikely scenario, and that I'm focusing on a weird and unlucky corner of the space. (I'm curious whether you endorse this, Richard, or some nearby correction of it.)</p><p>In particular, Richard, I am curious whether you endorse something like the following:</p><ul><li>I'm focusing ~all my efforts on the shallow-memorizers case, because I think shallow-memorizer-alignment will by and large be sufficient, and even if it is not then I expect it's a good way to prepare ourselves for whatever we'll turn out to need in practice. In particular I don't put much stock in the idea that there's a predictable phase-change that forces us to deal with laser-like planners, nor that predictable problems in that domain give large present reason to worry.</li></ul><p>(I suspect not, at least not in precisely this form, and I'm eager for corrections.)</p><p>I suspect something in this vicinity constitutes a crux of the disagreement, and I would be thrilled if we could get it distilled down to something as concise as the above. And, for the record, I personally endorse the following counter to the above:</p><ul><li>I am focusing ~none of my efforts on shallow-memorizer-alignment, as I expect it to be far from sufficient, as I do not expect a singularity until we have more laser-like systems, and I think that the laserlike-planning regime has a host of predictable alignment difficulties that Earth does not seem at all prepared to face (unlike, it seems to me, the shallow-memorizer alignment difficulties), and as such I have large and present worries.</li></ul><p>---</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222);border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>Ok, and now a few less substantial points:</p><p>There's a point Richard made here:</p><blockquote><p>Oh, interesting. Actually one more question then: to what extent do you think that explicitly reasoning about utility functions and laws of rationality is what makes consequentialists have the properties you've been talking about?</p></blockquote><p>that I suspect constituted a miscommunication, especially given that the following sentence appeared in Richard's summary:</p><blockquote><p>A third thing that makes humans in particular consequentialist is planning, especially when we’re aware of concepts like utility functions.</p></blockquote><p>In particular, I suspect Richard's model of Eliezer's model places (or placed, before Richard read Eliezer's comments on Richard's summary) some particular emphasis on systems reflecting and thinking about their own strategies, as a method by which the consequentialism and/or effectiveness gets in. I suspect this is a misunderstanding, and am happy to say more on my model upon request, but am hopeful that the points I made a few pages above have cleared this up.</p><p>Finally, I observe that there are a few places where Eliezer keeps beeping when Richard attempts to summarize him, and I suspect it would be useful to do the dorky thing of Richard very explicitly naming Eliezer's beeps as he understands them, for purposes of getting common knowledge of understanding. For instance, things I think it might be useful for Richard to say verbatim (assuming he believes them, which I suspect, and subject to Eliezer-corrections, b/c maybe I'm saying things that induce separate beeps):</p><p>1. Eliezer doesn't believe it's impossible to build AIs that have most any given property, including most any given safety property, including most any desired \"non-consequentialist\" or \"deferential\" property you might desire. Rather, Eliezer believes that many desirable safety properties don't happen by default, and require mastery of minds that likely takes a worrying amount of time to acquire.</p><p>2. The points about consequentialism are not particularly central in Eliezer's view; they seem to him more like obvious background facts; the reason conversation has lingered here in the EA-sphere is that this is a point that many folk in the local community disagree on.</p><p>For the record, I think it might also be worth Eliezer acknowledging that Richard probably understands point (1), and that glossing \"you don't get it for free by default and we aren't on course to have the time to get it\" as \"you can't\" is quite reasonable when summarizing. (And it might be worth Richard counter-acknowledging that the distinction is actually quite important once you buy the surrounding arguments, as it constitutes the difference between describing the current playing field and laying down to die.) I don't think any of these are high-priority, but they might be useful if easy :-)</p><p>---</p><p>Finally, stating the obvious-to-me, none of this is intended as criticism of either party, and all discussing parties have exhibited significant virtue-according-to-Nate throughout this process.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][21:27] &nbsp;(Sep. 12)</strong>&nbsp;</p><p>From Nate's notes:</p><blockquote><p>For instance, the plan presumably needs to involve all sorts of mechanisms for refocusing the laser in the case where the environment contains fog, and redirecting the laser in the case where the environment contains mirrors (...the analogy is getting a bit strained here, sorry, bear with me), so that it can in fact hit a narrow and distant target. Refocusing and redirecting to stay on target are part and parcel to plans that can hit narrow distant targets.</p><p>But the humans shutting the AI down is like scattering the laser, and the humans tweaking the AI so that it plans in a different direction is like them tossing up mirrors that redirect the laser; and we want the plan to fail to correct for those interferences.</p></blockquote><p>--&gt; GOOD ANALOGY.</p><p>...or at least it sure conveys to <i>me</i> why corrigibility is anticonvergent / anticoherent / actually <i>moderately strongly contrary to</i> and not just <i>an orthogonal property of</i> a powerful-plan generator.</p><p>But then, I already know why that's true and how it generalized up to resisting our various attempts to solve small pieces of more important aspects of it - it's not just true by weak default, it's true by a stronger default where a roomful of people at a workshop spend several days trying to come up with increasingly complicated ways to describe a system that will let you shut it down (but not steer you through time <i>into</i> shutting it down), and all of those suggested ways get shot down. (And yes, people outside MIRI now and then publish papers saying they totally just solved this problem, but all of those \"solutions\" are things we considered and dismissed as trivially failing to scale to powerful agents - they didn't understand what we considered to be the first-order problems in the first place - rather than these being evidence that MIRI just didn't have smart-enough people at the workshop.)</p></td></tr><tr><td style=\"background-color:#FFEEBB;border:1pt solid hsl(0, 0%, 0%)\"><p><strong>[Yudkowsky][18:56] &nbsp;(Nov. 5 follow-up comment)</strong>&nbsp;</p><p>Eg, \"Well, we took a system that only learned from reinforcement on situations it had previously been in, and couldn't use imagination to plan for things it had never seen, and then we found that if we didn't update it on shut-down situations it wasn't reinforced to avoid shutdowns!\"</p></td></tr></tbody></table>",
      "plaintextDescription": "This post is the first in a series of transcribed Discord conversations between Richard Ngo and Eliezer Yudkowsky, moderated by Nate Soares. We've also added Richard and Nate's running summaries of the conversation (and others' replies) from Google Docs.\n\nLater conversation participants include Ajeya Cotra, Beth Barnes, Carl Shulman, Holden Karnofsky, Jaan Tallinn, Paul Christiano, Rob Bensinger, and Rohin Shah.\n\nThe transcripts are a complete record of several Discord channels MIRI made for discussion. We tried to edit the transcripts as little as possible, other than to fix typos and a handful of confusingly-worded sentences, to add some paragraph breaks, and to add referenced figures and links. We didn't end up redacting any substantive content, other than the names of people who would prefer not to be cited. We swapped the order of some chat messages for clarity and conversational flow (indicated with extra timestamps), and in some cases combined logs where the conversation switched channels.\n\n \n\nColor key:\n\n Chat by Richard and Eliezer  Other chat  Google Doc content  Inline comments \n\n \n\n\n0. Prefatory comments\n \n\n[Yudkowsky][8:32]  (Nov. 6 follow-up comment) \n\n(At Rob's request I'll try to keep this brief, but this was an experimental format and some issues cropped up that seem large enough to deserve notes.)\n\nEspecially when coming in to the early parts of this dialogue, I had some backed-up hypotheses about \"What might be the main sticking point? and how can I address that?\" which from the standpoint of a pure dialogue might seem to be causing me to go on digressions, relative to if I was just trying to answer Richard's own questions.  On reading the dialogue, I notice that this looks evasive or like point-missing, like I'm weirdly not just directly answering Richard's questions.\n\nOften the questions are answered later, or at least I think they are, though it may not be in the first segment of the dialogue.  But the larger phenomenon is that I came in with s",
      "wordCount": 29585
    },
    "tags": [
      {
        "_id": "x3ibfKCvYCdWTZ8Zt",
        "name": "Pivotal Acts",
        "slug": "pivotal-acts"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CpvyhFy9WvCNsifkY",
    "title": "Discussion with Eliezer Yudkowsky on AGI interventions",
    "slug": "discussion-with-eliezer-yudkowsky-on-agi-interventions",
    "url": null,
    "baseScore": 328,
    "voteCount": 156,
    "viewCount": null,
    "commentCount": 253,
    "createdAt": null,
    "postedAt": "2021-11-11T03:01:11.208Z",
    "contents": {
      "markdown": "The  following is a partially redacted and lightly edited transcript of a chat conversation about AGI between Eliezer Yudkowsky and a set of invitees in early September 2021. By default, all other participants are anonymized as \"Anonymous\".\n\nI think this Nate Soares quote (excerpted from Nate's [response to a report by Joe Carlsmith](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential)) is a useful context-setting preface regarding timelines, which weren't discussed as much in the transcript:\n\n> \\[...\\] My odds \\[of AGI by the year 2070\\] are around 85%\\[...\\]\n> \n> I can list a handful of things that drive my probability of AGI-in-the-next-49-years above 80%:\n> \n> 1\\. 50 years ago was 1970. The gap between AI systems then and AI systems now seems pretty plausibly greater than the remaining gap, even before accounting the recent dramatic increase in the rate of progress, and potential future increases in rate-of-progress as it starts to feel within-grasp.\n> \n> 2\\. I observe that, 15 years ago, everyone was saying AGI is far off because of what it couldn't do -- basic image recognition, go, starcraft, winograd schemas, programmer assistance. But basically all that has fallen. The gap between us and AGI is made mostly of intangibles. (Computer Programming That Is Actually Good? Theorem proving? Sure, but on my model, \"good\" versions of those are a hair's breadth away from full AGI already. And the fact that I need to clarify that \"bad\" versions don't count, speaks to my point that the only barriers people can name right now are intangibles.) That's a very uncomfortable place to be!\n> \n> 3\\. When I look at the history of invention, and the various anecdotes about the Wright brothers and Enrico Fermi, I get an impression that, when a technology is pretty close, the world looks a lot like how our world looks.\n> \n> *   Of course, the trick is that when a technology is a little far, the world might also look pretty similar!\n> *   Though when a technology is **very** far, the world **does** look different -- it looks like experts pointing to specific technical hurdles. We exited that regime a few years ago.\n> \n> 4\\. Summarizing the above two points, I suspect that I'm in more-or-less the \"penultimate epistemic state\" on AGI timelines: I don't know of a project that seems like they're right on the brink; that would put me in the \"final epistemic state\" of thinking AGI is imminent. But I'm in the second-to-last epistemic state, where I wouldn't feel all that shocked to learn that some group has reached the brink. Maybe I won't get that call for 10 years! Or 20! But it could also be 2, and I wouldn't get to be indignant with reality. I wouldn't get to say \"but all the following things should have happened first, before I made that observation\". I have made those observations.\n> \n> 5\\. It seems to me that the Cotra-style compute-based model provides pretty conservative estimates. For one thing, I don't expect to need human-level compute to get human-level intelligence, and for another I think there's a decent chance that insight and innovation have a big role to play, especially on 50 year timescales.\n> \n> 6\\. There has been a lot of AI progress recently. When I tried to adjust my beliefs so that I was **positively** surprised by AI progress just about as often as I was **negatively** surprised by AI progress, I ended up expecting a bunch of rapid progress. \\[...\\]\n\n**Further preface by Eliezer:** \n\nIn some sections here, I sound gloomy about the probability that coordination between AGI groups succeeds in saving the world.  Andrew Critch reminds me to point out that gloominess like this can be a self-fulfilling prophecy - if people think successful coordination is impossible, they won’t try to coordinate.  I therefore remark in retrospective advance that it seems to me like at least some of the top AGI people, say at Deepmind and Anthropic, are the sorts who I think would rather coordinate than destroy the world; my gloominess is about what happens when the technology has propagated further than that.  But even then, anybody who would *rather* coordinate and *not* destroy the world shouldn’t rule out hooking up with Demis, or whoever else is in front if that person also seems to prefer not to completely destroy the world.  (Don’t be too picky here.)  Even if the technology proliferates and the world ends a year later when other non-coordinating parties jump in, it’s still better to take the route where the world ends one year later instead of immediately.  Maybe the horse will sing.\n\n* * *\n\n **Eliezer Yudkowsky** \n\nHi and welcome. Points to keep in mind: \n\n\\- I'm doing this because I would like to learn whichever *actual* thoughts this target group may have, and perhaps respond to those; that's part of the point of anonymity. If you speak an anonymous thought, please have that be your actual thought that you are thinking yourself, not something where you're thinking \"well, somebody else might think that...\" or \"I wonder what Eliezer's response would be to...\"\n\n\\- Eliezer's responses are uncloaked by default. Everyone else's responses are anonymous (not pseudonymous) and neither I nor MIRI will know which potential invitee sent them.\n\n\\- Please do not reshare or pass on the link you used to get here.\n\n\\- I do intend that parts of this conversation may be saved and published at MIRI's discretion, though not with any mention of who the anonymous speakers could possibly have been.\n\n**Eliezer Yudkowsky** \n\n(Thank you to Ben Weinstein-Raun for building [chathamroom.com](https://www.chathamroom.com/), and for quickly adding some features to it at my request.)\n\n**Eliezer Yudkowsky** \n\nIt is now 2PM; this room is now open for questions.\n\n**Anonymous** \n\nHow long will it be open for?\n\n**Eliezer Yudkowsky** \n\nIn principle, I could always stop by a couple of days later and answer any unanswered questions, but my basic theory had been \"until I got tired\".\n\n* * *\n\n**Anonymous** \n\nAt a high level one thing I want to ask about is research directions and prioritization. For example, if you were dictator for what researchers here (or within our influence) were working on, how would you reallocate them?\n\n**Eliezer Yudkowsky** \n\nThe first reply that came to mind is \"I don't know.\" I consider the present gameboard to look incredibly grim, and I don't actually see a way out through hard work alone. We can hope there's a miracle that violates some aspect of my background model, and we can try to prepare for that unknown miracle; preparing for an unknown miracle probably looks like \"Trying to die with more dignity on the mainline\" (because if you can die with more dignity on the mainline, you are better positioned to take advantage of a miracle if it occurs).\n\n**Anonymous** \n\nI'm curious if the grim outlook is currently mainly due to technical difficulties or social/coordination difficulties. (Both avenues might have solutions, but maybe one seems more recalcitrant than the other?)\n\n**Eliezer Yudkowsky** \n\nTechnical difficulties. Even if the social situation were vastly improved, on my read of things, everybody still dies because there is nothing that a handful of socially coordinated projects can do, or even a handful of major governments who aren't willing to start nuclear wars over things, to prevent somebody else from building AGI and killing everyone 3 months or 2 years later. There's no obvious winnable position into which to play the board.\n\n**Anonymous** \n\njust to clarify, that sounds like a large scale coordination difficulty to me (i.e., we - as all of humanity - can't coordinate to not build that AGI).\n\n**Eliezer Yudkowsky** \n\nI wasn't really considering the counterfactual where humanity had a collective telepathic hivemind? I mean, I've written fiction about a world coordinated enough that they managed to shut down all progress in their computing industry and only manufacture powerful computers in a single worldwide hidden base, but Earth was never going to go down that route. Relative to remotely plausible levels of future coordination, we have a technical problem.\n\n**Anonymous **\n\nCurious about why building an AGI aligned to its users' interests isn't a thing a handful of coordinated projects could do that would effectively prevent the catastrophe. The two obvious options are: it's too hard to build it vs it wouldn't stop the other group anyway. For \"it wouldn't stop them\", two lines of reply are nobody actually wants an unaligned AGI (they just don't foresee the consequences and are pursuing the benefits from automated intelligence, so can be defused by providing the latter) (maybe not entirely true: omnicidal maniacs), and an aligned AGI could help in stopping them. Is your take more on the \"too hard to build\" side?\n\n**Eliezer Yudkowsky ** \n\nBecause it's too technically hard to align some cognitive process that is powerful enough, and operating in a sufficiently dangerous domain, to stop the next group from building an unaligned AGI in 3 months or 2 years. Like, they can't coordinate to build an AGI that builds a nanosystem because it is too technically hard to align their AGI technology in the 2 years before the world ends.\n\n**Anonymous ** \n\nSummarizing the threat model here (correct if wrong): The nearest competitor for building an AGI is at most N (<2) years behind, and building an aligned AGI, even when starting with the ability to build an unaligned AGI, takes longer than N years. So at some point some competitor who doesn't care about safety builds the unaligned AGI. How does \"nobody actually wants an unaligned AGI\" fail here? It takes >N years to get everyone to realise that they have that preference and that it's incompatible with their actions?\n\n**Eliezer Yudkowsky ** \n\nMany of the current actors seem like they'd be really gung-ho to build an \"unaligned\" AGI because they think it'd be super neat, or they think it'd be super profitable, and they don't expect it to destroy the world. So if this happens in anything like the current world - and I neither expect vast improvements, nor have very long timelines - then we'd see Deepmind get it first; and, if the code was not *immediately* stolen and rerun with higher bounds on the for loops, by China or France or whoever, somebody else would get it in another year; if that somebody else was Anthropic, I could maybe see them also not amping up their AGI; but then in 2 years it starts to go to Facebook AI Research and home hobbyists and intelligence agencies stealing copies of the code from other intelligence agencies and I don't see how the world fails to end past that point.\n\n**Anonymous ** \n\nWhat does trying to die with more dignity on the mainline look like? There's a real question of prioritisation here between solving the alignment problem (and various approaches within that), and preventing or slowing down the next competitor. I'd personally love more direction on where to focus my efforts (obviously you can only say things generic to the group).\n\n**Eliezer Yudkowsky ** \n\nI don't know how to effectively prevent or slow down the \"next competitor\" for more than a couple of years even in plausible-best-case scenarios. Maybe some of the natsec people can be grownups in the room and explain why \"stealing AGI code and running it\" is as bad as \"full nuclear launch\" to their foreign counterparts in a realistic way. Maybe more current AGI groups can be persuaded to go closed; or, if more than one has an AGI, to coordinate with each other and not rush into an arms race. I'm not sure I believe these things can be done in real life, but it seems understandable to me how I'd go about trying - though, please do talk with me a lot more before trying anything like this, because it's easy for me to see how attempts could backfire, it's not clear to me that we should be inviting more attention from natsec folks at all. None of that saves us without technical alignment progress. But what are other people supposed to do about researching alignment when I'm not sure what to try there myself?\n\n  \n \n\n**Anonymous ** \n\nthanks! on researching alignment, you might have better meta ideas (how to do research generally) even if you're also stuck on object level. and you might know/foresee dead ends that others don't.\n\n**Eliezer Yudkowsky ** \n\nI definitely foresee a whole lot of dead ends that others don't, yes.\n\n**Anonymous ** \n\nDoes pushing for a lot of public fear about this kind of research, that makes all projects hard, seem hopeless?\n\n**Eliezer Yudkowsky ** \n\nWhat does it buy us? 3 months of delay at the cost of a tremendous amount of goodwill? 2 years of delay? What's that delay for, if we all die at the end? Even if we then got a technical miracle, would it end up impossible to run a project that could make use of an alignment miracle, because everybody was afraid of that project? Wouldn't that fear tend to be channeled into \"ah, yes, it must be a government project, they're the good guys\" and then the government is much more hopeless and much harder to improve upon than Deepmind?\n\n**Anonymous ** \n\nI imagine lack of public support for genetic manipulation of humans has slowed that research by more than three months\n\n**Anonymous ** \n\n'would it end up impossible to run a project that could make use of an alignment miracle, because everybody was afraid of that project?'\n\n...like, maybe, but not with near 100% chance?\n\n**Eliezer Yudkowsky ** \n\nI don't want to sound like I'm dismissing the whole strategy, but it sounds a *lot* like the kind of thing that backfires because you did not get *exactly* the public reaction you wanted, and the public reaction you actually got was bad; and it doesn't sound like that whole strategy actually has a visualized victorious endgame, which makes it hard to work out what the exact strategy should be; it seems more like the kind of thing that falls under the syllogism \"something must be done, this is something, therefore this must be done\" than like a plan that ends with humane life victorious.\n\nRegarding genetic manipulation of humans, I think the public started out very unfavorable to that, had a reaction that was not at all exact or channeled, does not allow for any 'good' forms of human genetic manipulation regardless of circumstances, driving the science into other countries - it is not a case in point of the intelligentsia being able to successfully cunningly manipulate the fear of the masses to some supposed good end, to put it mildly, so I'd be worried about deriving that generalization from it. The reaction may more be that the fear of the public is a big powerful uncontrollable thing that doesn't move in the smart direction - maybe the public fear of AI gets channeled by opportunistic government officials into \"and that's why We must have Our AGI first so it will be Good and we can Win\". That seems to me much more like a thing that would happen in real life than \"and then we managed to manipulate public panic down exactly the direction we wanted to fit into our clever master scheme\", especially when we don't actually *have* the clever master scheme it fits into.\n\n* * *\n\n**Eliezer Yudkowsky ** \n\nI have a few stupid ideas I could try to investigate in ML, but that would require the ability to run significant-sized closed ML projects full of trustworthy people, which is a capability that doesn't seem to presently exist. Plausibly, this capability would be required in any world that got some positive model violation (\"miracle\") to take advantage of, so I would want to build that capability today. I am not sure how to go about doing that either.\n\n**Anonymous ** \n\nif there's a chance this group can do something to gain this capability I'd be interested in checking it out. I'd want to know more about what \"closed\"and \"trustworthy\" mean for this (and \"significant-size\" I guess too). E.g., which ones does Anthropic fail?\n\n**Eliezer Yudkowsky ** \n\nWhat I'd like to exist is a setup where I can work with people that I or somebody else has vetted as seeming okay-trustworthy, on ML projects that aren't going to be published. Anthropic looks like it's a package deal. If Anthropic were set up to let me work with 5 particular people at Anthropic on a project boxed away from the rest of the organization, that would potentially be a step towards trying such things. It's also not clear to me that Anthropic has either the time to work with me, or the interest in doing things in AI that aren't \"stack more layers\" or close kin to that.\n\n**Anonymous ** \n\nThat setup doesn't sound impossible to me -- at DeepMind or OpenAI or a new org specifically set up for it (or could be MIRI) -- the bottlenecks are access to trustworthy ML-knowledgeable people (but finding 5 in our social network doesn't seem impossible?) and access to compute (can be solved with more money - not too hard?). I don't think DM and OpenAI are publishing everything - the \"not going to be published\" part doesn't seem like a big barrier to me. Is infosec a major bottleneck (i.e., who's potentially stealing the code/data)?\n\n**Anonymous ** \n\nDo you think Redwood Research could be a place for this?\n\n**Eliezer Yudkowsky ** \n\nMaybe! I haven't ruled RR out yet. But they also haven't yet done (to my own knowledge) anything demonstrating the same kind of AI-development capabilities as even GPT-3, let alone AlphaFold 2.\n\n**Eliezer Yudkowsky ** \n\nI would potentially be super interested in working with Deepminders if Deepmind set up some internal partition for \"Okay, accomplished Deepmind researchers who'd rather not destroy the world are allowed to form subpartitions of this partition and have their work not be published outside the subpartition let alone Deepmind in general, though maybe you have to report on it to Demis only or something.\" I'd be more skeptical/worried about working with OpenAI-minus-Anthropic because the notion of \"open AI\" continues to sound to me like \"what is the worst possible strategy for making the game board as unplayable as possible while demonizing everybody who tries a strategy that could possibly lead to the survival of humane intelligence\", and now a lot of the people who knew about that part have left OpenAI for elsewhere. But, sure, if they changed their name to \"ClosedAI\" and fired everyone who believed in the original OpenAI mission, I would update about that.\n\n**Eliezer Yudkowsky ** \n\nContext that is potentially missing here and should be included: I wish that Deepmind had more internal closed research, and internally siloed research, as part of a larger wish I have about the AI field, independently of what projects I'd want to work on myself.\n\nThe present situation can be seen as one in which a common resource, the remaining timeline until AGI shows up, is incentivized to be burned by AI researchers because they have to come up with neat publications and publish them (which burns the remaining timeline) in order to earn status and higher salaries. The more they publish along the spectrum that goes {quiet internal result -> announced and demonstrated result -> paper describing how to get the announced result -> code for the result -> model for the result}, the more timeline gets burned, and the greater the internal and external prestige accruing to the researcher.\n\nIt's futile to wish for everybody to act uniformly against their incentives.  But I think it would be a step forward if the relative incentive to burn the commons could be *reduced*; or to put it another way, the more researchers have the *option* to not burn the timeline commons, without them getting fired or passed up for promotion, the more that unusually intelligent researchers might perhaps decide not to do that. So I wish in general that AI research groups in general, but also Deepmind in particular, would have affordances for researchers who go looking for interesting things to not publish any resulting discoveries, at all, and still be able to earn internal points for them. I wish they had the *option* to do that. I wish people were *allowed* to not destroy the world - and still get high salaries and promotion opportunities and the ability to get corporate and ops support for playing with interesting toys; if destroying the world is prerequisite for having nice things, nearly everyone is going to contribute to destroying the world, because, like, they're not going to just *not* have nice things, that is not human nature for almost all humans.\n\nWhen I visualize how the end of the world plays out, I think it involves an AGI system which has the ability to be cranked up by adding more computing resources to it; and I think there is an extended period where the system is not aligned enough that you can crank it up that far, without everyone dying. And it seems *extremely* likely that if factions on the level of, say, Facebook AI Research, start being able to deploy systems like that, then death is very automatic. If the Chinese, Russian, and French intelligence services all manage to steal a copy of the code, and China and Russia sensibly decide not to run it, and France gives it to three French corporations which I hear the French intelligence service sometimes does, then again, everybody dies. If the builders are sufficiently worried about that scenario that they push too fast too early, in fear of an arms race developing very soon if they wait, again, everybody dies.\n\nAt present we're very much waiting on a miracle for alignment to be possible at all, even if the AGI-builder successfully prevents proliferation and has 2 years in which to work. But if we get that miracle at all, it's not going to be an instant miracle.  There’ll be some minimum time-expense to do whatever work is required. So any time I visualize anybody trying to even start a successful trajectory of this kind, they need to be able to get a lot of work done, without the intermediate steps of AGI work being published, or demoed at all, let alone having models released.  Because if you wait until the last months when it is really really obvious that the system is going to scale to AGI, in order to start closing things, almost all the prerequisites will already be out there. Then it will only take 3 more months of work for somebody else to build AGI, and then somebody else, and then somebody else; and even if the first 3 factions manage not to crank up the dial to lethal levels, the 4th party will go for it; and the world ends by default on full automatic.\n\nIf ideas are theoretically internal to \"just the company\", but the company has 150 people who all know, plus everybody with the \"sysadmin\" title having access to the code and models, then I imagine - perhaps I am mistaken - that those ideas would (a) inevitably leak outside due to some of those 150 people having cheerful conversations over a beer with outsiders present, and (b) be copied outright by people of questionable allegiances once all hell started to visibly break loose. As with anywhere that handles really sensitive data, the concept of \"need to know\" has to be a thing, or else everyone (and not just in that company) ends up knowing.\n\nSo, even if I got run over by a truck tomorrow, I would still very much wish that in the world that survived me, Deepmind would have lots of penalty-free affordance internally for people to not publish things, and to work in internal partitions that didn't spread their ideas to all the rest of Deepmind.  Like, *actual* social and corporate support for that, not just a theoretical option you'd have to burn lots of social capital and weirdness points to opt into, and then get passed up for promotion forever after.\n\n**Anonymous ** \n\nWhat's RR?\n\n**Anonymous ** \n\nIt's a new alignment org, run by Nate Thomas and ~co-run by Buck Shlegeris and Bill Zito, with maybe 4-6 other technical folks so far. My take: the premise is to create an org with ML expertise and general just-do-it competence that's trying to do all the alignment experiments that something like Paul+Ajeya+Eliezer all think are obviously valuable and wish someone would do. They expect to have a website etc in a few days; the org is a couple months old in its current form.\n\n* * *\n\n**Anonymous ** \n\nHow likely really is hard takeoff? Clearly, we are touching the edges of AGI with GPT and the like. But I'm not feeling this will that easily be leveraged into very quick recursive self improvement.\n\n**Eliezer Yudkowsky ** \n\nCompared to the position I was arguing in the Foom Debate with Robin, reality has proved way to the further Eliezer side of Eliezer along the Eliezer-Robin spectrum. It's been very unpleasantly surprising to me how little architectural complexity is required to start producing generalizing systems, and how fast those systems scale using More Compute. The flip side of this is that I can imagine a system being scaled up to interesting human+ levels, without \"recursive self-improvement\" or other of the old tricks that I thought would be necessary, and argued to Robin would make fast capability gain possible. You could have fast capability gain well before anything like a FOOM started. Which in turn makes it more plausible to me that we could hang out at interesting not-superintelligent levels of AGI capability for a while before a FOOM started. It's not clear that this helps anything, but it does seem more plausible.\n\n**Anonymous ** \n\nI agree reality has not been hugging the Robin kind of scenario this far.\n\n**Anonymous ** \n\nGoing past human level doesn't necessarily mean going \"foom\".\n\n**Eliezer Yudkowsky ** \n\nI do think that if you get an AGI significantly past human intelligence in all respects, it would obviously tend to FOOM. I mean, I suspect that Eliezer fooms if you give an Eliezer the ability to backup, branch, and edit himself.\n\n**Anonymous ** \n\nIt doesn't seem to me that an AGI significantly past human intelligence necessarily tends to FOOM.\n\n**Eliezer Yudkowsky ** \n\nI think in principle we could have, for example, an AGI that was just a superintelligent engineer of proteins, and of nanosystems built by nanosystems that were built by proteins, and which was corrigible enough not to want to improve itself further; and this AGI would also be dumber than a human when it came to eg psychological manipulation, because we would have asked it not to think much about that subject. I'm doubtful that you can have an AGI that's significantly above human intelligence in *all* respects, without it having the capability-if-it-wanted-to of looking over its own code and seeing lots of potential improvements.\n\n**Anonymous ** \n\nAlright, this makes sense to me, but I don't expect an AGI to *want* to manipulate humans that easily (unless designed to). Maybe a bit.\n\n**Eliezer Yudkowsky ** \n\nManipulating humans is a convergent instrumental strategy if you've accurately modeled (even at quite low resolution) what humans are and what they do in the larger scheme of things.\n\n**Anonymous ** \n\nYes, but human manipulation is also the kind of thing you need to guard against with even mildly powerful systems. Strong impulses to manipulate humans, should be vetted out.\n\n**Eliezer Yudkowsky ** \n\nI think that, by default, if you trained a young AGI to expect that 2+2=5 in some special contexts, and then scaled it up without further retraining, a generally superhuman version of that AGI would be very likely to 'realize' in some sense that SS0+SS0=SSSS0 was a consequence of the Peano axioms. There's a natural/convergent/coherent output of deep underlying algorithms that generate competence in some of the original domains; when those algorithms are implicitly scaled up, they seem likely to generalize better than whatever patch on those algorithms said '2 + 2 = 5'.\n\nIn the same way, suppose that you take weak domains where the AGI can't fool you, and apply some gradient descent to get the AGI to stop outputting actions of a type that humans can detect and label as 'manipulative'.  And then you scale up that AGI to a superhuman domain.  I predict that deep algorithms within the AGI will go through consequentialist dances, and model humans, and output human-manipulating actions that can't be detected as manipulative by the humans, in a way that seems likely to bypass whatever earlier patch was imbued by gradient descent, because I doubt that earlier patch will generalize as well as the deep algorithms. Then you don't get to retrain in the superintelligent domain after labeling as bad an output that killed you and doing a gradient descent update on that, because the bad output killed you. (This is an attempted very fast gloss on what makes alignment difficult *in the first place*.)\n\n**Anonymous ** \n\n\\[i appreciate this gloss - thanks\\]\n\n**Anonymous ** \n\n\"deep algorithms within it will go through consequentialist dances, and model humans, and output human-manipulating actions that can't be detected as manipulative by the humans\"\n\nThis is true if it is rewarding to manipulate humans. If the humans are on the outlook for this kind of thing, it doesn't seem that easy to me.\n\nGoing through these \"consequentialist dances\" to me appears to presume that mistakes that should be apparent haven't been solved at simpler levels. It seems highly unlikely to me that you would have a system that appears to follow human requests and human values, and it would suddenly switch at some powerful level. I think there will be signs beforehand. Of course, if the humans are not paying attention, they might miss it. But, say, in the current milieu, I find it plausible that they will pay enough attention.\n\n\"because I doubt that earlier patch will generalize as well as the deep algorithms\"\n\nThat would depend on how \"deep\" your earlier patch was. Yes, if you're just doing surface patches to apparent problems, this might happen. But it seems to me that useful and intelligent systems will require deep patches (or deep designs from the start) in order to be apparently useful to humans at solving complex problems enough. This is not to say that they would be perfect. But it seems quite plausible to me that they would in most cases prevent the worst outcomes.\n\n**Eliezer Yudkowsky ** \n\n\"If you've got a general consequence-modeling-and-searching algorithm, it seeks out ways to manipulate humans, even if there are no past instances of a random-action-generator producing manipulative behaviors that succeeded and got reinforced by gradient descent over the random-action-generator. It invents the strategy de novo by imagining the results, even if there's no instances in memory of a strategy like that having been tried before.\" Agree or disagree?\n\n**Anonymous ** \n\nCreating strategies de novo would of course be expected of an AGI.\n\n> \"If you've got a general consequence-modeling-and-searching algorithm, it seeks out ways to manipulate humans, even if there are no past instances of a random-action-generator producing manipulative behaviors that succeeded and got reinforced by gradient descent over the random-action-generator. It invents the strategy de novo by imagining the results, even if there's no instances in memory of a strategy like that having been tried before.\" Agree or disagree?\n\nI think, if the AI will \"seek out ways to manipulate humans\", will depend on what kind of goals the AI has been designed to pursue.\n\nManipulating humans is definitely an instrumentally useful kind of method for an AI, for a lot of goals. But it's also counter to a lot of the things humans would direct the AI to do -- at least at a \"high level\". \"Manipulation\", such as marketing, for lower level goals, can be very congruent with higher level goals. An AI could clearly be good at manipulating humans, while not manipulating its creators or the directives of its creators.\n\nIf you are asking me to agree that the AI will generally seek out ways to manipulate the high-level goals, then I will say \"no\". Because it seems to me that faults of this kind in the AI design is likely to be caught by the designers earlier. (This isn't to say that this kind of fault couldn't happen.) It seems to me that manipulation of high-level goals will be one of the most apparent kind of faults of this kind of system.\n\n**Anonymous ** \n\nRE: \"I'm doubtful that you can have an AGI that's significantly above human intelligence in *all* respects, without it having the capability-if-it-wanted-to of looking over its own code and seeing lots of potential improvements.\"\n\nIt seems plausible (though unlikely) to me that this would be true in practice for the AGI we build -- but also that the potential improvements it sees would be pretty marginal. This is coming from the same intuition that current learning algorithms might already be approximately optimal.\n\n**Eliezer Yudkowsky ** \n\n> If you are asking me to agree that the AI will generally seek out ways to manipulate the high-level goals, then I will say \"no\". Because it seems to me that faults of this kind in the AI design is likely to be caught by the designers earlier.\n\nI expect that when people are trying to stomp out convergent instrumental strategies by training at a safe dumb level of intelligence, this will not be effective at preventing convergent instrumental strategies at smart levels of intelligence; also note that at very smart levels of intelligence, \"hide what you are doing\" is also a convergent instrumental strategy of that substrategy.\n\nI don't know however if I should be explaining at this point why \"manipulate humans\" is convergent, why \"conceal that you are manipulating humans\" is convergent, why you have to train in safe regimes in order to get safety in dangerous regimes (because if you try to \"train\" at a sufficiently unsafe level, the output of the unaligned system deceives you into labeling it incorrectly and/or kills you before you can label the outputs), or why attempts to teach corrigibility in safe regimes are unlikely to generalize well to higher levels of intelligence and unsafe regimes (qualitatively new thought processes, things being way out of training distribution, and, the hardest part to explain, corrigibility being \"anti-natural\" in a certain sense that makes it incredibly hard to, eg, exhibit any coherent planning behavior (\"consistent utility function\") which corresponds to being willing to let somebody else shut you off, without incentivizing you to actively manipulate them to shut you off).\n\n* * *\n\n**Anonymous ** \n\nMy (unfinished) idea for buying time is to focus on applying AI to well-specified problems, where constraints can come primarily from the action space and additionally from process-level feedback (i.e., human feedback providers understand why actions are good before endorsing them, and reject anything weird even if it seems to work on some outcomes-based metric). This is basically a form of boxing, with application-specific boxes. I know it doesn't scale to superintelligence but I think it can potentially give us time to study and understand proto AGIs before they kill us. I'd be interested to hear devastating critiques of this that imply it isn't even worth fleshing out more and trying to pursue, if they exist.\n\n**Anonymous ** \n\n(I think it's also similar to CAIS in case that's helpful.)\n\n**Eliezer Yudkowsky ** \n\nThere's lots of things we can do which don't solve the problem and involve us poking around with AIs having fun, while we wait for a miracle to pop out of nowhere. There's lots of things we can do with AIs which are weak enough to not be able to fool us and to not have cognitive access to any dangerous outputs, like automatically generating pictures of cats.  The trouble is that nothing we can do with an AI like that (where \"human feedback providers understand why actions are good before endorsing them\") is powerful enough to save the world.\n\n**Eliezer Yudkowsky ** \n\nIn other words, if you have an aligned AGI that builds complete mature nanosystems for you, that *is* enough force to save the world; but that AGI needs to have been aligned by some method other than \"humans inspect those outputs and vet them and their consequences as safe/aligned\", because humans cannot accurately and unfoolably vet the consequences of DNA sequences for proteins, or of long bitstreams sent to protein-built nanofactories.\n\n**Anonymous ** \n\nWhen you mention nanosystems, how much is this just a hypothetical superpower vs. something you actually expect to be achievable with AGI/superintelligence? If expected to be achievable, why?\n\n**Eliezer Yudkowsky ** \n\nThe case for nanosystems being possible, if anything, seems even more slam-dunk than the already extremely slam-dunk case for superintelligence, because we can set lower bounds on the power of nanosystems using far more specific and concrete calculations. See eg the first chapters of Drexler's Nanosystems, which are the first step mandatory reading for anyone who would otherwise doubt that there's plenty of room above biology and that it is possible to have artifacts the size of bacteria with much higher power densities. I have this marked down as \"known lower bound\" not \"speculative high value\", and since Nanosystems has been out since 1992 and subjected to attemptedly-skeptical scrutiny, without anything I found remotely persuasive turning up, I do not have a strong expectation that any new counterarguments will materialize.\n\nIf, after reading Nanosystems, you still don't think that a superintelligence can get to and past the Nanosystems level, I'm not quite sure what to say to you, since the models of superintelligences are much less concrete than the models of molecular nanotechnology.\n\nI'm on record as early as 2008 as saying that I expected superintelligences to crack protein folding, some people disputed that and were all like \"But how do you know that's solvable?\" and then AlphaFold 2 came along and cracked the protein folding problem they'd been skeptical about, far below the level of superintelligence.\n\nI can try to explain how I was mysteriously able to forecast this truth at a high level of confidence - not the exact level where it became possible, to be sure, but that superintelligence would be sufficient - despite this skepticism; I suppose I could point to prior hints, like even human brains being able to contribute suggestions to searches for good protein configurations; I could talk about how if evolutionary biology made proteins evolvable then there must be a lot of regularity in the folding space, and that this kind of regularity tends to be exploitable.\n\nBut of course, it's also, in a certain sense, very *obvious* that a superintelligence could crack protein folding, just like it was obvious years before *Nanosystems* that molecular nanomachines would in fact be possible and have much higher power densities than biology. I could say, \"Because proteins are held together by van der Waals forces that are much weaker than covalent bonds,\" to point to a reason how you could realize that after just reading *Engines of Creation* and before *Nanosystems* existed, by way of explaining how one could possibly guess the result of the calculation in advance of building up the whole detailed model. But in reality, precisely because the possibility of molecular nanotechnology was already obvious to any sensible person just from reading *Engines of Creation*, the sort of person who wasn't convinced by *Engines of Creation* wasn't convinced by *Nanosystems* either, because they'd already demonstrated immunity to sensible arguments; an example of the general phenomenon I've elsewhere termed the Law of Continued Failure.\n\nSimilarly, the sort of person who was like \"But how do you know superintelligences will be able to build nanotech?\" in 2008, will probably not be persuaded by the demonstration of AlphaFold 2, because it was already clear to anyone sensible in 2008, and so anyone who can't see sensible points in 2008 probably also can't see them after they become even clearer. There are some people on the margins of sensibility who fall through and change state, but mostly people are not on the exact margins of sanity like that.\n\n**Anonymous ** \n\n\"If, after reading Nanosystems, you still don't think that a superintelligence can get to and past the Nanosystems level, I'm not quite sure what to say to you, since the models of superintelligences are much less concrete than the models of molecular nanotechnology.\"\n\nI'm not sure if this is directed at *me* or the [https://en.wikipedia.org/wiki/Generic_you](https://en.wikipedia.org/wiki/Generic_you), but I'm only expressing curiosity on this point, not skepticism :)\n\n* * *\n\n**Anonymous ** \n\nsome form of \"scalable oversight\" is the naive extension of the initial boxing thing proposed above that claims to be the required alignment method -- basically, make the humans vetting the outputs smarter by providing them AI support for all well-specified (level-below)-vettable tasks.\n\n**Eliezer Yudkowsky ** \n\nI haven't seen any plausible story, in any particular system design being proposed by the people who use terms about \"scalable oversight\", about how human-overseeable thoughts or human-inspected underlying systems, compound into very powerful human-non-overseeable outputs that are trustworthy. Fundamentally, the whole problem here is, \"You're allowed to look at floating-point numbers and Python code, but how do you get from there to trustworthy nanosystem designs?\" So saying \"Well, we'll look at some thoughts we can understand, and then from out of a much bigger system will come a trustworthy output\" doesn't answer the hard core at the center of the question. Saying that the humans will have AI support doesn't answer it either.\n\n**Anonymous ** \n\nthe kind of useful thing humans (assisted-humans) might be able to vet is reasoning/arguments/proofs/explanations. without having to generate neither the trustworthy nanosystem design nor the reasons it is trustworthy, we could still check them.\n\n**Eliezer Yudkowsky ** \n\nIf you have an untrustworthy general superintelligence generating English strings meant to be \"reasoning/arguments/proofs/explanations\" about eg a nanosystem design, then I would not only expect the superintelligence to be able to fool humans in the sense of arguing for things that were not true in a way that fooled the humans, I'd expect the superintelligence to be able to covertly directly hack the humans in ways that I wouldn't understand even after having been told what happened. So you must have some prior belief about the superintelligence being aligned before you dared to look at the arguments. How did you get that prior belief?\n\n**Anonymous ** \n\nI think I'm not starting with a general superintelligence here to get the trustworthy nanodesigns. I'm trying to build the trustworthy nanosystems \"the hard way\", i.e., if we did it without ever building AIs, and then speed that up using AI for automation of things we know how to vet (including recursively). Is a crux here that you think nanosystem design requires superintelligence?\n\n(tangent: I think this approach works even if you accidentally built a more-general or more-intelligent than necessary foundation model as long as you're only using it in boxes it can't outsmart. The better-specified the tasks you automate are, the easier it is to secure the boxes.)\n\n**Eliezer Yudkowsky ** \n\nI think that China ends the world using code they stole from Deepmind that did things the easy way, and that happens 50 years of natural R&D time before you can do the equivalent of \"strapping mechanical aids to a horse instead of building a car from scratch\".\n\nI also think that the speedup step in \"iterated amplification and distillation\" will introduce places where the fast distilled outputs of slow sequences are not true to the original slow sequences, because gradient descent is not perfect and won't be perfect and it's not clear we'll get any paradigm besides gradient descent for doing a step like that.\n\n* * *\n\n**Anonymous ** \n\nHow do you feel about the safety community as a whole and the growth we've seen over the past few years?\n\n**Eliezer Yudkowsky ** \n\nVery grim. I think that almost everybody is bouncing off the real hard problems at the center and doing work that is predictably not going to be useful at the superintelligent level, nor does it teach me anything I could not have said in advance of the paper being written. People like to do projects that they know will succeed and will result in a publishable paper, and that rules out all real research at step 1 of the social process.\n\nPaul Christiano is trying to have real foundational ideas, and they're all wrong, but he's one of the few people trying to have foundational ideas at all; if we had another 10 of him, something might go right.\n\nChris Olah is going to get far too little done far too late. We're going to be facing down an unalignable AGI and the current state of transparency is going to be \"well look at this interesting visualized pattern in the attention of the key-value matrices in layer 47\" when what we need to know is \"okay but was the AGI plotting to kill us or not”. But Chris Olah is still trying to do work that is on a pathway to anything important at all, which makes him exceptional in the field.\n\nStuart Armstrong did some good work on further formalizing the shutdown problem, an example case in point of why corrigibility is hard, which so far as I know is still resisting all attempts at solution.\n\nVarious people who work or worked for MIRI came up with some actually-useful notions here and there, like Jessica Taylor's expected utility quantilization.\n\nAnd then there is, so far as I can tell, a vast desert full of work that seems to me to be mostly fake or pointless or predictable.\n\nIt is very, very clear that at present rates of progress, adding that level of alignment capability as grown over the next N years, to the AGI capability that arrives after N years, results in everybody dying very quickly. Throwing more money at this problem does not obviously help because it just produces more low-quality work.\n\n**Anonymous ** \n\n\"doing work that is predictably not going to be really useful at the superintelligent level, nor does it teach me anything I could not have said in advance of the paper being written\"\n\nI think you're underestimating the value of solving small problems. Big problems are solved by solving many small problems. (I do agree that many academic papers do not represent much progress, however.)\n\n**Eliezer Yudkowsky ** \n\nBy default, I suspect you have longer timelines and a smaller estimate of total alignment difficulty, not that I put less value than you on the incremental power of solving small problems over decades. I think we're going to be staring down the gun of a completely inscrutable model that would kill us all if turned up further, with no idea how to read what goes on inside its head, and no way to train it on humanly scrutable and safe and humanly-labelable domains in a way that seems like it would align the superintelligent version, while standing on top of a whole bunch of papers about \"small problems\" that never got past “small problems”.\n\n**Anonymous ** \n\n\"I think we're going to be staring down the gun of a completely inscrutable model that would kill us all if turned up further, with no idea how to read what goes on inside its head, and no way to train it on humanly scrutable and safe and humanly-labelable domains in a way that seems like it would align the superintelligent version\"\n\nThis scenario seems possible to me, but not very plausible. GPT is not going to \"kill us all\" if turned up further. No amount of computing power (at least before AGI) would cause it to. I think this is apparent, without knowing exactly what's going on inside GPT. This isn't to say that there aren't AI systems that wouldn't. But *what kind of system would*? (A GPT combined with sensory capabilities at the level of Tesla's self-driving AI? That still seems too limited.)\n\n**Eliezer Yudkowsky ** \n\nAlpha Zero scales with more computing power, I think AlphaFold 2 scales with more computing power, Mu Zero scales with more computing power. Precisely because GPT-3 doesn't scale, I'd expect an AGI to look more like Mu Zero and particularly with respect to the fact that it has some way of scaling.\n\n* * *\n\n**Steve Omohundro ** \n\nEliezer, thanks for doing this! I just now read through the discussion and found it valuable. I agree with most of your specific points but I seem to be much more optimistic than you about a positive outcome. I'd like to try to understand why that is. I see mathematical proof as the most powerful tool for constraining intelligent systems and I see a pretty clear safe progression using that for the technical side (the social side probably will require additional strategies). Here are some of my intuitions underlying that approach, I wonder if you could identify any that you disagree with. I'm fine with your using my name (Steve Omohundro) in any discussion of these.\n\n1) Nobody powerful wants to create unsafe AI but they do want to take advantage of AI capabilities.\n\n2) None of the concrete well-specified valuable AI capabilities require unsafe behavior\n\n3) Current simple logical systems are capable of formalizing every relevant system involved (eg. MetaMath [http://us.metamath.org/index.html](http://us.metamath.org/index.html) currently formalizes roughly an undergraduate math degree and includes everything needed for modeling the laws of physics, computer hardware, computer languages, formal systems, machine learning algorithms, etc.)\n\n4) Mathematical proof is cheap to mechanically check (eg. MetaMath has a 500 line Python verifier which can rapidly check all of its 38K theorems)\n\n5) GPT-F is a fairly early-stage transformer-based theorem prover and can already prove 56% of the MetaMath theorems. Similar systems are likely to soon be able to rapidly prove all simple true theorems (eg. that human mathematicians can prove in a day).\n\n6) We can define provable limits on the behavior of AI systems that we are confident prevent dangerous behavior and yet still enable a wide range of useful behavior. \n\n7) We can build automated checkers for these provable safe-AI limits. \n\n8) We can build (and eventually mandate) powerful AI hardware that first verifies proven safety constraints before executing AI software \n\n9) For example, AI smart compilation of programs can be formalized and doesn't require unsafe operations \n\n10) For example, AI design of proteins to implement desired functions can be formalized and doesn't require unsafe operations \n\n11) For example, AI design of nanosystems to achieve desired functions can be formalized and doesn't require unsafe operations.\n\n12) For example, the behavior of designed nanosystems can be similarly constrained to only proven safe behaviors\n\n13) And so on through the litany of early stage valuable uses for advanced AI.\n\n14) I don't see any fundamental obstructions to any of these. Getting social acceptance and deployment is another issue! \n\nBest, Steve\n\n**Eliezer Yudkowsky ** \n\nSteve, are you visualizing AGI that gets developed 70 years from now under absolutely different paradigms than modern ML? I don't see being able to take anything remotely like, say, Mu Zero, and being able to prove any theorem about it which implies anything like corrigibility or the system not internally trying to harm humans. Anything in which enormous inscrutable floating-point vectors is a key component, seems like something where it would be very hard to prove any theorems about the treatment of those enormous inscrutable vectors that would correspond in the outside world to the AI not killing everybody.\n\nEven if we somehow managed to get structures far more legible than giant vectors of floats, using some AI paradigm very different from the current one, it still seems like huge key pillars of the system would rely on non-fully-formal reasoning; even if the AI has something that you can point to as a utility function and even if that utility function's representation is made out of programmer-meaningful elements instead of giant vectors of floats, we'd still be relying on much shakier reasoning at the point where we claimed that this utility function meant something in an intuitive human-desired sense, say. And if that utility function is learned from a dataset and decoded only afterwards by the operators, that sounds even scarier. And if instead you're learning a giant inscrutable vector of floats from a dataset, gulp. \n\nYou seem to be visualizing that we prove a theorem and then get a theorem-like level of assurance that the system is safe. What kind of theorem? What the heck would it say? \n\nI agree that it seems plausible that the good cognitive operations we want do not *in principle* require performing bad cognitive operations; the trouble, from my perspective, is that generalizing structures that do lots of good cognitive operations will automatically produce bad cognitive operations, especially when we dump more compute into them; \"you can't bring the coffee if you're dead\".\n\nSo it takes a more complicated system and some feat of insight I don't presently possess, to \"just\" do the good cognitions, instead of doing all the cognitions that result from decompressing the thing that compressed the cognitions in the dataset - even if that original dataset only contained cognitions that looked good to us, even if that dataset actually *was* just correctly labeled data about safe actions inside a slightly dangerous domain. Humans do a lot of stuff besides maximizing inclusive genetic fitness, optimizing purely on outcomes labeled by a simple loss function doesn’t get you an internal optimizer that pursues only that loss function, etc.\n\n**Anonymous ** \n\nSteve's intuitions sound to me like they're pointing at the \"well-specified problems\" idea from an earlier thread. Essentially, only use AI in domains where unsafe actions are impossible by construction. Is this too strong a restatement of your intuitions Steve?\n\n**Steve Omohundro ** \n\nThanks for your perspective! Those sound more like social concerns than technical ones, though. I totally agree that today's AI culture is very \"sloppy\" and that the currently popular representations, learning algorithms, data sources, etc. aren't oriented around precise formal specification or provably guaranteed constraints. I'd love any thoughts about ways to help shift that culture toward precise and safe approaches! Technically there is no problem getting provable constraints on floating point computations, etc. The work often goes under the label \"Interval Computation\". It's not even very expensive, typically just a factor of 2 worse than \"sloppy\" computations. For some reason those approaches have tended to be more popular in Europe than in the US. Here are a couple lists of references: [http://www.cs.utep.edu/interval-comp/](http://www.cs.utep.edu/interval-comp/) [https://www.mat.univie.ac.at/~neum/interval.html](https://www.mat.univie.ac.at/~neum/interval.html)\n\nI see today's dominant AI approach of mapping everything to large networks ReLU units running on hardware designed for dense matrix multiplication, trained with gradient descent on big noisy data sets as a very temporary state of affairs. I fully agree that it would be uncontrolled and dangerous scaled up in its current form! But it's really terrible in every aspect except that it makes it easy for machine learning practitioners to quickly slap something together which will actually sort of work sometimes. With all the work on AutoML, NAS, and the formal methods advances I'm hoping we leave this \"sloppy\" paradigm pretty quickly. Today's neural networks are terribly inefficient for inference: most weights are irrelevant for most inputs and yet current methods do computational work on each. I developed many algorithms and data structures to avoid that waste years ago (eg. \"bumptrees\" [https://steveomohundro.com/scientific-contributions/)](https://steveomohundro.com/scientific-contributions/))\n\nThey're also pretty terrible for learning since most weights don't need to be updated for most training examples and yet they are. Google and others are using Mixture-of-Experts to avoid some of that cost: [https://arxiv.org/abs/1701.06538](https://arxiv.org/abs/1701.06538)\n\nMatrix multiply is a pretty inefficient primitive and alternatives are being explored: [https://arxiv.org/abs/2106.10860](https://arxiv.org/abs/2106.10860)\n\nToday's reinforcement learning is slow and uncontrolled, etc. All this ridiculous computational and learning waste could be eliminated with precise formal approaches which measure and optimize it precisely. I'm hopeful that that improvement in computational and learning performance may drive the shift to better controlled representations.\n\nI see theorem proving as hugely valuable for safety in that we can easily precisely specify many important tasks and get guarantees about the behavior of the system. I'm hopeful that we will also be able to apply them to the full AGI story and encode human values, etc., but I don't think we want to bank on that at this stage. Hence, I proposed the \"Safe-AI Scaffolding Strategy\" where we never deploy a system without proven constraints on its behavior that give us high confidence of safety. We start extra conservative and disallow behavior that might eventually be determined to be safe. At every stage we maintain very high confidence of safety. Fast, automated theorem checking enables us to build computational and robotic infrastructure which only executes software with such proofs.\n\nAnd, yes, I'm totally with you on needing to avoid the \"basic AI drives\"! I think we have to start in a phase where AI systems are not allowed to run rampant as uncontrolled optimizing agents! It's easy to see how to constrain limited programs (eg. theorem provers, program compilers or protein designers) to stay on particular hardware and only communicate externally in precisely constrained ways. It's similarly easy to define constrained robot behaviors (eg. for self-driving cars, etc.) The dicey area is that unconstrained agentic edge. I think we want to stay well away from that until we're very sure we know what we're doing! My optimism stems from the belief that many of the socially important things we need AI for won't require anything near that unconstrained edge. But it's tempered by the need to get the safe infrastructure into place before dangerous AIs are created.\n\n**Anonymous ** \n\nAs far as I know, all the work on \"verifying floating-point computations\" currently is way too low-level -- the specifications that are proved about the computations don't say anything about what the computations mean or are about, beyond the very local execution of some algorithm. Execution of algorithms in the real world can have very far-reaching effects that aren't modelled by their specifications.\n\n**Eliezer Yudkowsky ** \n\nYeah, what they said. How do you get from proving things about error bounds on matrix multiplications of inscrutable floating-point numbers, to saying anything about what a mind is trying to do, or not trying to do, in the external world?\n\n**Steve Omohundro ** \n\nUltimately we need to constrain behavior. You might want to ensure your robot butler won't leave the premises. To do that using formal methods, you need to have a semantic representation of the location of the robot, your premise's spatial extent, etc. It's pretty easy to formally represent that kind of physical information (it's just a more careful version of what engineers do anyway). You also have a formal model of the computational hardware and software and the program running the system.\n\nFor finite systems, any true property has a proof which can be mechanically checked but the size of that proof might be large and it might be hard to find. So we need to use encodings and properties which mesh well with the safety semantics we care about.\n\nFormal proofs of properties of programs has progressed to where a bunch of cryptographic, compilation, and other systems can be specified and formalized. Why it's taken this long, I have no idea. The creator of any system has an argument as to why its behavior does what they think it will and why it won't do bad or dangerous things. The formalization of those arguments should be one direct short step.\n\nExperience with formalizing mathematician's informal arguments suggest that the formal proofs are maybe 5 times longer than the informal argument. Systems with learning and statistical inference add more challenges but nothing that seems in-principal all that difficult. I'm still not completely sure how to constrain the use of language, however. I see inside of Facebook all sorts of problems due to inability to constrain language systems (eg. they just had a huge issue where a system labeled a video with a racist term). The interface between natural language semantics and formal semantics and how we deal with that for safety is something I've been thinking a lot about recently.\n\n**Steve Omohundro ** \n\nHere's a nice 3 hour long tutorial about \"probabilistic circuits\" which is a representation of probability distributions, learning, Bayesian inference, etc. which has much better properties than most of the standard representations used in statistics, machine learning, neural nets, etc.: [https://www.youtube.com/watch?v=2RAG5-L9R70](https://www.youtube.com/watch?v=2RAG5-L9R70) It looks especially amenable to interpretability, formal specification, and proofs of properties.\n\n**Eliezer Yudkowsky ** \n\nYou're preaching to the choir there, but even if we were working with more strongly typed epistemic representations that had been inferred by some unexpected innovation of machine learning, automatic inference of those representations would lead them to be uncommented and not well-matched with human compressions of reality, nor would they match exactly against reality, which would make it very hard for any theorem about \"we are optimizing against this huge uncommented machine-learned epistemic representation, to steer outcomes inside this huge machine-learned goal specification\" to guarantee safety in outside reality; especially in the face of how corrigibility is unnatural and runs counter to convergence and indeed coherence; especially if we're trying to train on domains where unaligned cognition is safe, and generalize to regimes in which unaligned cognition is not safe. Even in this case, we are not nearly out of the woods, because what we can prove has a great type-gap with that which we want to ensure is true. You can't handwave the problem of crossing that gap even if it's a solvable problem.\n\nAnd that whole scenario would require some major total shift in ML paradigms.\n\nRight now the epistemic representations are giant inscrutable vectors of floating-point numbers, and so are all the other subsystems and representations, more or less.\n\nProve whatever you like about that Tensorflow problem; it will make no difference to whether the AI kills you. The properties that can be proven just aren't related to safety, no matter how many times you prove an error bound on the floating-point multiplications. It wasn't floating-point error that was going to kill you in the first place.",
      "plaintextDescription": "The following is a partially redacted and lightly edited transcript of a chat conversation about AGI between Eliezer Yudkowsky and a set of invitees in early September 2021. By default, all other participants are anonymized as \"Anonymous\".\n\nI think this Nate Soares quote (excerpted from Nate's response to a report by Joe Carlsmith) is a useful context-setting preface regarding timelines, which weren't discussed as much in the transcript:\n\n> [...] My odds [of AGI by the year 2070] are around 85%[...]\n> \n> I can list a handful of things that drive my probability of AGI-in-the-next-49-years above 80%:\n> \n> 1. 50 years ago was 1970. The gap between AI systems then and AI systems now seems pretty plausibly greater than the remaining gap, even before accounting the recent dramatic increase in the rate of progress, and potential future increases in rate-of-progress as it starts to feel within-grasp.\n> \n> 2. I observe that, 15 years ago, everyone was saying AGI is far off because of what it couldn't do -- basic image recognition, go, starcraft, winograd schemas, programmer assistance. But basically all that has fallen. The gap between us and AGI is made mostly of intangibles. (Computer Programming That Is Actually Good? Theorem proving? Sure, but on my model, \"good\" versions of those are a hair's breadth away from full AGI already. And the fact that I need to clarify that \"bad\" versions don't count, speaks to my point that the only barriers people can name right now are intangibles.) That's a very uncomfortable place to be!\n> \n> 3. When I look at the history of invention, and the various anecdotes about the Wright brothers and Enrico Fermi, I get an impression that, when a technology is pretty close, the world looks a lot like how our world looks.\n> \n>  * Of course, the trick is that when a technology is a little far, the world might also look pretty similar!\n>  * Though when a technology is very far, the world does look different -- it looks like experts pointing to specif",
      "wordCount": 10094
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "oNcqyaWPXNGTTRPHm",
        "name": "Existential risk",
        "slug": "existential-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cujpciCqNbawBihhQ",
    "title": "Self-Integrity and the Drowning Child",
    "slug": "self-integrity-and-the-drowning-child",
    "url": null,
    "baseScore": 343,
    "voteCount": 182,
    "viewCount": null,
    "commentCount": 91,
    "createdAt": null,
    "postedAt": "2021-10-24T20:57:01.742Z",
    "contents": {
      "markdown": "*(Excerpted from \"*[*mad investor chaos and the woman of asmodeus*](https://www.glowfic.com/posts/4582)*\", about an unusually selfish dath ilani, \"Keltham\", who dies in a plane accident and ends up in Cheliax, a country governed by D&D!Hell.  Keltham is here remembering an incident from his childhood.)*\n\n* * *\n\nAnd the Watcher told the class a parable, about an adult, coming across a child who'd somehow bypassed the various safeguards around a wilderness area, and fallen into a muddy pond, and seemed to be showing signs of drowning (for they'd already been told, then, what drowning looked like).  The water, in this parable, didn't look like it would be over their own adult heads.  But - in the parable - they'd just bought some incredibly-expensive clothing, costing dozens of their own labor-hours, and less resilient than usual, that would be ruined by the muddy water.\n\nAnd the Watcher asked the class if they thought it was right to save the child, at the cost of ruining their clothing.\n\nEveryone in there moved their hand to the 'yes' position, of course.  Except Keltham, who by this point had already decided quite clearly who he was, and who simply closed his hand into a fist, otherwise saying neither 'yes' nor 'no' to the question, defying it entirely.\n\nThe Watcher asked him to explain, and Keltham said that it seemed to him that it was okay for an adult to take an extra fifteen seconds to strip off all their super-expensive clothing and *then* jump in to save the child.\n\nThe Watcher invited the other children to argue with Keltham about that, which they did, though Keltham's first defense, that his utility function was what it was, had not been a friendly one, or inviting of further argument.  But they did eventually convince Keltham that, especially if you weren't sure you could call in other help or get attention or successfully drag the child's body towards help, if that child actually did drown - meaning the child's *true* life was at stake - then it would make sense to jump in right away, *not* take the extra risk of waiting another quarter-minute to strip off your clothes, and bill the child's parents' insurance for the cost.  Or at least, that was where Keltham shifted his position, in the face of that argumentative pressure.\n\nSome kids, at that point, questioned the Watcher about this actually being a pretty good point, and why *wouldn't* anyone just bill the child's parents' insurance.\n\nTo which the Watcher asked them to consider hypothetically the case where insurance refused to pay out in cases like that, because it would be too easy for people to set up 'accidents' letting them bill insurances - not that this precaution had proven to be necessary in real life, of course.  But the Watcher asked them to consider the Least Convenient Possible World where insurance companies, and even parents, did need to reason like that; because there'd proven to be too many master criminals setting up 'children at risk of true death from drowning' accidents that they could apparently avert and claim bounties on.\n\nWell, said Keltham, in that case, he was going right back to taking another fifteen seconds to strip off his super-expensive clothes, if the child didn't look like it was *literally right about* to drown.  And if society didn't like that, it was society's job to solve that thing with the master criminals.  Though he'd *maybe* modify that if they were in a possible-true-death situation, because a true life is worth a huge number of labor-hours, and that part did feel like some bit of decision theory would say that everyone would be wealthier if everyone would sacrifice small amounts of wealth to save huge amounts of somebody else's wealth, if that happened unpredictably to people, and if society was also that incompetent at setting up proper reimbursements.  Though if it was like that in real life instead of the Least Convenient Possible World, it would mean that Civilization was terrible at coordination and it was time to overthrow Governance and start over.\n\nThis time the smarter kids did not succeed in pushing Keltham away from his position, and after a few more minutes the Watcher called a halt to it, and told the assembled children that they had been brought here today to learn an important lesson from Keltham about self-integrity.\n\nKeltham is being coherent, said the Watcher.\n\nKeltham's decision is a valid one, given his own utility function (said the Watcher); you were wrong to try to talk him into thinking that he was making an objective error.\n\nIt's easy for you to say you'd save the child (said the Watcher) when you're not really there, when you don't actually have to make the sacrifice of what you spent so many hours laboring to obtain, and would you all please note how none of you even considered about whether or not to spend a quarter-minute stripping off your clothes, or whether to try to bill the child's parents' insurance.  Because you were too busy showing off how Moral you were, and how willing to make Sacrifices.  Maybe you would decide not to do it, if the fifteen seconds were too costly; and then, any time you spent thinking about it, would also have been costly; and in that sense it might make more sense given your own utility functions (unlike Keltham's) to rush ahead without taking the time to think, let alone the time to strip off your expensive fragile clothes.  But labor does have value, along with a child's life; and it is not incoherent or stupid for Keltham to weigh that too, especially given his own utility function - so said the Watcher.\n\nKeltham did have enough dignity, by that point in his life, not to rub it in or say 'told you so' to the other children, as this would have distracted them from the process of updating.\n\nThe Watcher spoke on, then, about how most people have selfish and unselfish parts - not selfish and unselfish *components in their utility function,* but parts of themselves in some less Law-aspiring way than that.  Something with a utility function, if it values an apple 1% more than an orange, if offered a million apple-or-orange choices, will choose a million apples and zero oranges.  The division within most people into selfish and unselfish components is not like that, you cannot feed it all with unselfish choices whatever the ratio.  Not unless you are a Keeper, maybe, who has made yourself sharper and more coherent; or maybe not even then, who knows?  For (it was said in another place) it is hazardous to non-Keepers to know too much about exactly how Keepers think.\n\nIt is dangerous to believe, said the Watcher, that you get extra virtue points the more that you let your altruistic part hammer down the selfish part.  If you were older, said the Watcher, if you were more able to dissect thoughts into their parts and catalogue their effects, you would have noticed at once how this whole parable of the drowning child, was set to crush down the selfish part of you, to make it look like you would be invalid and shameful and harmful-to-others if the selfish part of you won, because, you're meant to think, people don't *need* expensive clothing - although somebody who's spent a lot on expensive clothing clearly has some use for it or some part of themselves that desires it quite strongly.\n\nIt is a parable calculated to set at odds two pieces of yourself (said the Watcher), and your flaw is not that you made the wrong choice between the two pieces, it was that you hammered one of those pieces down.  Even though with a bit more thought, you could have at least *seen* the options for being that piece of yourself too, and not too expensively.\n\nAnd much more importantly (said the Watcher), you failed to understand and notice a kind of outside assault on your internal integrity, you did not notice how this parable was setting up two pieces of yourself at odds, so that you could not be both at once, and arranging for one of them to hammer down the other in a way that would leave it feeling small and injured and unable to speak in its own defense.\n\n\"If I'd actually wanted you to twist yourselves up and burn yourselves out around this,\" said the Watcher, \"I could have designed an adversarial lecture that would have driven everybody in this room halfway crazy - except for Keltham.  He's not just immune because he's an agent with a slightly different utility function, he's immune because he instinctively doesn't switch off a kind of self-integrity that everyone else in this class needs to learn to not switch off so easily.\"",
      "plaintextDescription": "(Excerpted from \"mad investor chaos and the woman of asmodeus\", about an unusually selfish dath ilani, \"Keltham\", who dies in a plane accident and ends up in Cheliax, a country governed by D&D!Hell.  Keltham is here remembering an incident from his childhood.)\n\n----------------------------------------\n\nAnd the Watcher told the class a parable, about an adult, coming across a child who'd somehow bypassed the various safeguards around a wilderness area, and fallen into a muddy pond, and seemed to be showing signs of drowning (for they'd already been told, then, what drowning looked like).  The water, in this parable, didn't look like it would be over their own adult heads.  But - in the parable - they'd just bought some incredibly-expensive clothing, costing dozens of their own labor-hours, and less resilient than usual, that would be ruined by the muddy water.\n\nAnd the Watcher asked the class if they thought it was right to save the child, at the cost of ruining their clothing.\n\nEveryone in there moved their hand to the 'yes' position, of course.  Except Keltham, who by this point had already decided quite clearly who he was, and who simply closed his hand into a fist, otherwise saying neither 'yes' nor 'no' to the question, defying it entirely.\n\nThe Watcher asked him to explain, and Keltham said that it seemed to him that it was okay for an adult to take an extra fifteen seconds to strip off all their super-expensive clothing and then jump in to save the child.\n\nThe Watcher invited the other children to argue with Keltham about that, which they did, though Keltham's first defense, that his utility function was what it was, had not been a friendly one, or inviting of further argument.  But they did eventually convince Keltham that, especially if you weren't sure you could call in other help or get attention or successfully drag the child's body towards help, if that child actually did drown - meaning the child's true life was at stake - then it would make sense to ju",
      "wordCount": 1456
    },
    "tags": [
      {
        "_id": "EmaCLRKb4baBFq4ra",
        "name": "Dath Ilan",
        "slug": "dath-ilan"
      },
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "GpLuMNKKpeJkmigEY",
        "name": "Integrity",
        "slug": "integrity"
      },
      {
        "_id": "F2XfCTxXLQBGjbm8P",
        "name": "Parables & Fables",
        "slug": "parables-and-fables"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6KzFwcDy7hsCkzJKY",
    "title": "The Point of Trade",
    "slug": "the-point-of-trade",
    "url": null,
    "baseScore": 181,
    "voteCount": 130,
    "viewCount": null,
    "commentCount": 76,
    "createdAt": null,
    "postedAt": "2021-06-22T17:56:44.088Z",
    "contents": {
      "markdown": "*(Content warning: econoliteracy.  Dialogue based on an actual conversation, but heavily idealized and simplified and stripped of surrounding context.)*\n\n**Myself:**  \\- seems unreal because it *is* unreal.  But there's a river of reality running through it.  If somebody reports that something complicated feels unreal and difficult to get a handle on, usually the first thing I prescribe is going back to the very very basics, and playing around with those to solidify the grasp there.  If mathematics seemed unreal to someone, I'd take somebody back to [premises-and-conclusions](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs/p/Z2CuyKtkCmWGQtAEh).  In the case of complicated modern economical structures, I'd maybe start with trade.  What's the point of trade?  What does it do?\n\n**Them:**  The point of trade is that sometimes people get different amounts of value from things, so they can get more value by trading them.  Like, if I have an apple, and you have an orange, but I like oranges more than apples, and you like apples more than oranges, we can trade my apple for your orange, and both be better off.\n\n**Myself:**  Yes, that is the horrible explanation that you sometimes see in economics textbooks because nobody knows how to explain anything.  But when you are trying to improve your grasp of the very very basics, you should take the basic thing and poke at it and look at it from different angles to see if it seems to capture the whole truth.\n\nIn the case of the \"people put different values on things\", it would seem that on the answer you just gave, trade could never increase wealth by very much, in a certain basic sense.  There would just be a finite amount of stuff, only so many apples and oranges, and all trade can do is shuffle the things *around* rather than make any *more* of it.  So, on this viewpoint, trade can't increase wealth by all that much.\n\n**Them:**  It can increase the utility we get from the things we have, if I like oranges a lot more than I like apples, and you like apples a lot more than oranges.\n\n**Myself:**  All right, suppose that all of us liked exactly the same objects exactly the same amount.  This obliterates the poorly-written-textbook's reason for \"trade\".  Do you believe that, in an alternate world where everybody had exactly the same taste in apples and oranges, there'd be no further use for trade and trade would stop existing?\n\n**Them:**  Hmm.  No, but I don't know how to describe what the justification for trade is, in that case.\n\n**Myself:**  Modern society seems very wealthy compared to hunter-gatherer society.  The vast majority of this increased utility comes from our *having more stuff,* not from our having the same amount of stuff as hunter-gatherers but giving apples to the exact person on Earth who likes apples most.  I claim that the reason we *have more stuff* has something to do with trade.  I claim that in an alternate society where everybody likes every object the same amount, they still do lots and lots of trade for this same reason, to increase how much stuff they have.\n\n**Them:**  Okay, my new answer is that, through trade, you can get strawberries from far away, where they wouldn't be in season at all, where you are... no, that doesn't actually make more stuff.  My new answer is that you can build complicated things with lots of inputs, by trading to get the inputs.  Like if it takes iron and copper to build circuits, you can trade to get those.\n\n**Myself:**  If it takes 1 unit of effort to get 1 unit of iron either way, how can you get any *more* stuff out, at the end, by trading things?  It takes 1 unit of effort to make 1 iron ingot, so go to the iron mines and mine some iron, then chop down the wood to prebake the iron ore for grinding before you put it into the bloomery.  All of that has to be done either way to get the iron ingot.  How can trading for somebody else's iron, instead, cause there to be more stuff in the economy as a whole?\n\n**Them:**  Because task-switching has costs.\n\n**Myself:**  Okay, suppose an alternate society of people who are *really* good at task-switching.  They can just swap straight from one task to another with no pause.  They also all have exactly the same tastes in apples and oranges and so on.  Does this new society now have zero use for trade?\n\n**Them:**  Um... hm.  *(Thinks.)*  But they're not actually in the same *place* as the iron mines.  So if they have to walk around a lot -\n\n**Myself:**  Suppose a society in which everyone has exactly the same taste in apples and oranges; everybody is really really good at switching tasks; and furthermore, the society has Star Trek transporter pads, so you can get to the iron mine instantly.  Is there *now* no more use for trade?\n\n**Them:**  Some people are better miners and others are better fruit-growers?\n\n**Myself:**  Suppose a society with identical fruit tastes, *and* perfect task-switching, *and* Star Trek transporters, *and furthermore* everyone has identical genetics, as if they were all identical twins; which, as we know from identical-twin studies, means that everybody will have around the same amount of innate talent for any and all jobs.  Like that case where two identical twins, separated at birth, who never knew each other, both ended up as firefighters.  As we all know, studies on separated identical twins show that happens every single time, with no exceptions.  I claim that *even this* society still has to do a lot of trade in order to end up with modern levels of wealth.\n\nNow, do you think I'm trolling you and that we actually did get rid of the basic reason for trade, at this point, or that there's still something left over?  Identical fruit tastes, perfect task-switching, Star Trek transporters, everyone is born with the same genetics and therefore identical innate talents.  Do people now mine their own iron, or do they still trade for it?\n\n**Them:**  (Thinks for a while.)\n\n**Me:**  If the Sequences have taught you anything, I hope it's taught you that it's okay to state the obvious.\n\n**Them:**  ...people learn to do their jobs better with practice?\n\n**Myself:  **Human capital accumulation!  Indeed!  So now let us suppose identical fruit tastes, perfect task-switching, Star Trek transporters, identically cloned genetics, *and* people can share expertise via Matrix-style downloads which are free.  Have we *now* gotten rid of the point of trade?  As far as you can tell.\n\n**Them:**  ...yes?\n\n**Myself:**  Do you believe *I'm* going to say that we've gotten rid of the point of trade?\n\n**Them:**  ...no.\n\n**Myself:**  Well, I agree with your object-level answer, so your meta-level answer was wrong.  I think we've now gotten rid of the point of trade.\n\n**Them:**  Darn it.\n\n*(Note:  While contemplating this afterwards, I realized that we hadn't quite gotten rid of all the points of trade, and there should have been two more rounds of dialogue; there are two more magical powers a society needs, in order to produce a high-tech quantity of stuff with zero trade.  The missing sections are left as an exercise for the reader.)*",
      "plaintextDescription": "(Content warning: econoliteracy.  Dialogue based on an actual conversation, but heavily idealized and simplified and stripped of surrounding context.)\n\nMyself:  - seems unreal because it is unreal.  But there's a river of reality running through it.  If somebody reports that something complicated feels unreal and difficult to get a handle on, usually the first thing I prescribe is going back to the very very basics, and playing around with those to solidify the grasp there.  If mathematics seemed unreal to someone, I'd take somebody back to premises-and-conclusions.  In the case of complicated modern economical structures, I'd maybe start with trade.  What's the point of trade?  What does it do?\n\nThem:  The point of trade is that sometimes people get different amounts of value from things, so they can get more value by trading them.  Like, if I have an apple, and you have an orange, but I like oranges more than apples, and you like apples more than oranges, we can trade my apple for your orange, and both be better off.\n\nMyself:  Yes, that is the horrible explanation that you sometimes see in economics textbooks because nobody knows how to explain anything.  But when you are trying to improve your grasp of the very very basics, you should take the basic thing and poke at it and look at it from different angles to see if it seems to capture the whole truth.\n\nIn the case of the \"people put different values on things\", it would seem that on the answer you just gave, trade could never increase wealth by very much, in a certain basic sense.  There would just be a finite amount of stuff, only so many apples and oranges, and all trade can do is shuffle the things around rather than make any more of it.  So, on this viewpoint, trade can't increase wealth by all that much.\n\nThem:  It can increase the utility we get from the things we have, if I like oranges a lot more than I like apples, and you like apples a lot more than oranges.\n\nMyself:  All right, suppose that all of us ",
      "wordCount": 1177
    },
    "tags": [
      {
        "_id": "PDJ6KqJBRzvKPfuS3",
        "name": "Economics",
        "slug": "economics"
      },
      {
        "_id": "9vuoKSLDTE8kbKWEA",
        "name": "Deconfusion",
        "slug": "deconfusion"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gvA4j8pGYG4xtaTkw",
    "title": "I'm from a parallel Earth with much higher coordination: AMA",
    "slug": "i-m-from-a-parallel-earth-with-much-higher-coordination-ama",
    "url": null,
    "baseScore": 173,
    "voteCount": 85,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2021-04-05T22:09:24.033Z",
    "contents": {
      "markdown": "Related: [My April Fools Day Confession](https://yudkowsky.tumblr.com/post/81447230971/my-april-fools-day-confession); [*Inadequate Equilibria*](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d)\n\nOn April 1, Eliezer Yudkowsky ran a dath ilan AMA on Facebook:\n\n> I came from a parallel Earth that successfully coordinated around maintaining a higher level of ability to solve coordination problems. Ask me anything.\n\nWith Eliezer’s blessing, I’ve quoted the resultant discussion below, leaving out threads that were repeats or didn’t go anywhere.\n\n* * *\n\n> **Guy Srinivasan:** Did parallel Earth coordinate around a specific day each year for everyone to play with falsity?\n> \n> **Eliezer Yudkowsky:** Not a specific *day* as such. There's very much a tradition of leading somebody down a garden path, and also of pretending to be led down the garden path — similar to the \"MIRI pomodoro: 25 minutes of work followed by 5 minutes of trolling\" — but there's a verbal handshake you're supposed to give at the end to prevent that from going out of control and any tragic errors.\n\n* * *\n\n> **Emielle Potgieter:** What is parallel earth's biggest problem, then?\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** I'd assume that Artificial General Intelligence is being seen by the Senior Very Serious People as a *big* problem, given the degree to which *nobody ever talked about it*, how relatively slow computing progress was compared to here, and how my general education *just happened* to prepare me to make a ton of correct inferences about it as soon as anybody mentioned the possibility to me. They claim to you it's about hypothetical aliens and economic dysfunction scenarios, but boy howdy do you get a lot of [Orthogonality](https://arbital.com/p/orthogonality/) and [Goodhart's Curse](https://arbital.com/p/goodharts_curse/) in the water supply.\n\n* * *\n\n> **Stācia Gāel:** Why did you come here?\n> \n> **Jean-Baptiste Clemens:** @Stācia Gāel   Everyone on parallel Earth was attempting to meet for lunch in the absence of communication and Eliezer was wrong about the Schelling point.\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** No clue, then or ever.\n\n* * *\n\n> **Erica Edelman:** How do you transition babies into job holding adults?\n\n> **Eliezer Yudkowsky:** That covers a lot of territory! But if I tried to zoom in at the most general level of difference, there's very much an understanding among Very Serious People that if you require anything to get access to a job (like a credential) that is difficult to get or has finite supply, people can burn the whole surplus value of the job to them in order to get it, even if that thing is of lower value to the employer. In other words, they would recognize \"occupational licensing\" or college as a sickness and move to prevent it while it was still getting started. In general, demanding something from somebody other than their actual job skill is recognized as a potential civilizational problem. So this means:\n> \n> *   A focus on testing for the actual job skills, rather than for peripherally related things like having paid to attend a particular institution. Not even talking about tests vs. attendance, I mean that they watch you doing the actual job.\n> *   Older children teach younger children things that the older children have been watched doing, because the older children have the knowledge and can teach it, and you wouldn't want to demand any more qualification than that. With monitoring and external validation to prevent ignorance from iterating upon itself, of course.\n> *   There's no minimum age to work, because demanding a higher age isn't something that the person doing the job actually needs.\n> *   You don't need a business license because that would, again, be an instance of something they recognize as technical debt / overhead / cruft for the civilization.\n> *   The economy runs hot enough that there's generally enough jobs on offer; the Very Serious People would regard it as a huge issue if people had to look for *a* job instead of choosing *which* job, and they would ask how we could possibly have gotten into that position when jobs were available 1000 years earlier and the economy had gotten a lot more productive since then.\n> *   The degree to which the employee sees themselves as doing the employer a favor, vs the employer seeing themselves as doing the employee a favor, is much more symmetrical than it is here - if that wasn't true, the Very Serious People would look at it and ask \"What's going wrong with this supply-demand balancing price level, why isn't this bargain behaving more symmetrically?\"\n> *   I can speculate about what other conditions contribute to that, but unfortunately it wasn't my actual field of study before I left. But in general, think of the situation among Silicon Valley programmers: you display the ability to do the work rather than competing on credentials, and it's as common to find a precious employee through connections as to find a precious job through connections. This would be equally true about haircuts in dath ilan; somebody who wants to expand their hair salon needs to somehow find an employee to do that and will have trouble doing so, unless they're much more profitable than average and can offer above-average salaries for that reason.\n> *   They have an actual concept of \"matching people up with jobs is a huge allocation issue for the whole economy\" and have already put serious sweat into experimenting with different ways to discern kids' native talents and figure out what that would match up to, obviously including have the kid actually try doing lots of different things. The notion that you get a whole college degree before spending one day trying out the actual job would be insane.\n> \n> So the overall answer to your question is that a kid learns skills from older kids, or from more specialized teachers or apprenticeships if it's a sufficiently complex skill to require that, after all kinds of \"try doing this for a week\" microapprenticeships so they can figure out where their niche is, and then when they know enough to do the job they can start doing it. If that sounds utopian, it's because the Very Serious People recognized it as a central issue for the whole economy and put literally any thought and experimentation at all into figuring out what would optimize it.\n> \n> **Ben Pace:** I'm surprised this is how it works on your planet. I am probably missing a basic piece of knowledge here, but in my experience it is very hard to put people to useful work, and if you give me a set of 100 people and ask me to cause them to do useful work, I will be able to do so with a team of 5-10 of them and then either I can help the rest form teams (if we're lucky that takes up most people), and if I'm forced to give the rest jobs many people will be left with bullshit jobs.\n> \n> (I mean, there are more interesting mechanisms I would use, like producing financial rewards for tasks completed that incentivize the rest to self-coordinate etc, but I don't feel like I can *count* on this to give everyone a job.)\n> \n> (Also like 15% of people have a low enough IQ that they cannot be given anything useful to do without on the order of 1-1 oversight. Not a defeater, but another major hurdle.)\n> \n> If you stop all the people doing bullshit jobs on your planet, do the rest of the people really find productive ways to build products and provide services, or is there something else that a lot of the population spends their time doing?\n> \n> \\[...\\]\n> \n> **Tom** **Pandolfo: **\n> \n> \\> There's no minimum age to work, because demanding a higher age isn't something that the person doing the job actually needs.\n> \n> @Eliezer Yudkowsky   How does your planet avoid the problem of child labor - that is, a 10-year-old being required to work in a factory in order for them or their family to afford food?\n> \n> (As distinct from a 10-year-old shadowing/apprenticing someone whose job they passionately want to do when they grow up.)\n> \n> Or is that not seen as a problem?\n> \n> **Eliezer Yudkowsky:** The fact that only 10 people can do the work *you* have for them doesn't mean that the other 90 people can't do work that *other* people have for them? I mean, that's kind of how the whole economy works?\n> \n> Civilization has enough productive capacity that when somebody is born who just can't make it in Civilization for whatever reason, there are places where you can go to live out the rest of your life in peace, provided that you have not previously had any children.\n> \n> One of the things I'd expect people from this world to find relatively off-putting is that dath ilan has comprehended that happiness is heritable and they teach a sight that extends over generational times and thinks ahead to the equilibrium; so it's understood that, except in very exceptional circumstances, if you're unhappy on average *for any reason*, it is your duty to the next generation not to have kids who might inherit this tendency.\n> \n> So the number of people who go to the Quiet Cities is more like 5% than 15%, because those who would have otherwise been the parents of people who went there did not have kids. And the rest of the world is mostly happy, because transmitting constitutions that can be happy in a civilized world is itself a solvable coordination problem.\n> \n> **Ben Pace:** (Sad react, but glad the people are able to look at sad things and take appropriate action and deal with it, and I respect those ~10% of people who do this a great deal.)\n> \n> \\[...\\]\n> \n> **David Schneider-Joseph:** @Eliezer Yudkowsky What about those who are unhappy because they see a problem with the civilization which will take many generations to solve, and are motivated to start the work of fixing it? Does that count as one of those very exceptional circumstances?\n> \n> **Jim** **Babcock:**\n> \n> “@Eliezer Yudkowsky   How does your planet avoid the problem of child labor - that is, a 10-year-old being required to work in a factory in order for them or their family to afford food?”\n> \n> UBI, obviously? On Earth, we have a political narrative that people must be threatened with poverty, or else they will choose not to work. And yet, if you look at the most impoverished people, you mostly find people whose ability-to-work has been damaged by the consequences of their poverty. It seems wildly overdetermined that, in a sensible system, there would be a UBI, and it would be high enough that not-having-food can't happen without something else going wrong other than lack of money.\n> \n> **Bruce Barrett Banner:** @Jim Babcock   it seems plausible to me that some suitable notion of economic democracy would justify, or even require, a UBI, and hence protect, not just children, but humanity at large from the degradation of wage slavery, which produces ruined workers as you suggest, and much other misery at well. It also makes a reality of political democracy as a side effect, to its credit.\n> \n> **Jim** **Babcock:** If \"the degradation of wage slavery\" sounds like a good summarization of the point to you, I think you may have a pretty important confusion? (A very *common* confusion, which is why I'm choosing to highlight it.)\n> \n> The \"ruined worker\" is typically *not* a wage slave, but someone who, at a key point, had no wage, and suffered malnutrition, medication lapses and assorted traumas as a result. It's pretty common for people to get confused about this, and try to put additional demands on employers, which wind up decreasing the availability of low-end jobs and increasing the number of people starved.\n> \n> One of the most important things I learned, being very into nutrition-research, is that most people can't recognize malnutrition when they see it, and so there's a widespread narrative that it doesn't exist. But if you actually know what you're looking for, and you walk down an urban downtown and look at the beggars, you will see the damage it has wrought... and it is extensive.\n> \n> **Tom Pandolfo:** @Jim Babcock   I'm not so sure that it's obvious. I'm skeptical of a purely monetary UBI, as I haven't been convinced that it won't just drive up the price of Literally Everything in the long run (without some serious regulatory intervention). On our Earth, at least, I'm much more in favor of a robust system of public housing/food/healthcare for free to everyone who needs it, and tepidly in favor of UBI as a temporary/transitional solution.\n> \n> That said, I'm interested in what solution Eliezer thinks is obvious to the denizens of dath ilan.\n> \n> **Jim** **Babcock:**\n> \n> “I'm skeptical of a purely monetary UBI, as I haven't been convinced that it won't just drive up the price of Literally Everything in the long run (without some serious regulatory intervention).”\n> \n> This is something economists understand very well, actually! Not at the intro-course level, and not if you get your economists through a politics-optimized filter, but if you put an economics department directory on a dartboard and throw three darts, I expect you'll get the same (or compatible) answers from all three.\n> \n> UBI is spending, which causes inflation, but not if it's replacing other government spending or offset by comparable taxes on any bracket. The price of supply-constrained goods, like housing in a strictly zoned area, or medical care from guild-approved doctors, may rise; the price of food, manufactured goods, and housing in low-density unzoned areas, will not.\n> \n> **Eliezer** **Yudkowsky:**\n> \n> “What about those who are unhappy because they see a problem with the civilization which will take many generations to solve, and are motivated to start the work of fixing it? Does that count as one of those very exceptional circumstances?”\n> \n> It does not. There are people who can manage to be happy and cheerful in their daily lives while tackling big problems. If you were unhappy and you wanted more people tackling big problems, you'd offer to pay for their childcare, not have kids yourself.\n\n* * *\n\n> **Nora Ammann:** Is there an overarching social narrative (at some level) they all share and that plays a significant role in their ability to coordinate? If so, what are its characteristics?\n> \n> **Eliezer Yudkowsky:** Several, of course, but this feels like a large enough question that I'm having trouble answering. Any individual cultivates a sense of <untranslateable 23>, their personal sense of This Is Who I Am, This Is What I Do, which will generally include I Can Correctly Analyze Coordination Problems And Do My Part In Solving Them. It's understood that this is meant to be personal and individualized; and at the same time, society is utterly unable to coordinate against all the people who metaphorically talk about Civilization's <untranslateable 23> and argue about what it is or should be. If you take a step back, this reflects a narrative about It Is Your Pride And Your Responsibility To Choose Your Own Narrative And Then Be Fucking Awesome At It. And of course that idea is going to spill over to Civilization too.\n> \n> People are aware of Goodhart's Law, but that doesn't stop the Very Serious People from Very Seriously And Prudently talking about particular figures-of-merit for all Civilization that we are still going to talk about while keeping an eye out for anybody trying to game them; figures-of-merit from productivity to scientific progress to health and happiness; and when you see a graph with those lines supposedly going up, it becomes part of a narrative about how Civilization is not doing too terribly at living up to its own <untranslateable 23>.\n\n* * *\n\n> **Nora Ammann:** Is their ability to coordinate relying on growth? Would \\[they\\] still be able to coordinate if \\[their\\] economy stagnated? \\[...\\]\n> \n> **Eliezer Yudkowsky:** It seems to me that the structure of what they're doing would carry over to a case without growth - you can ask about steady-state coordinated equilibria. There might be a bunch of specific things that would break and need changing, but people would be *thinking* about what those were.\n\n* * *\n\n> **Rocco Stanzione:** Why did you choose the first day of April to reveal yourself?\n> \n> **Eliezer Yudkowsky:** Plausible deniability!\n\n* * *\n\n> **Jeff Dubin:** Economic system?\n> \n> **David Spearman:** This. Also, legal system?\n> \n> **Eliezer Yudkowsky:** You're both gonna need to be more specific.\n> \n> **David Spearman:** Suppose that A is in a conflict with B over some perceived violation of whatever passes for rights. They both recognize the coordination problem for what it is, but they can’t agree on which of them is actually the highest-value user, in the Coaesian sense, of whatever the object of the dispute is. Do they just whistle up a Very Serious Person to resolve the dispute? What sort of training does the Very Serious Person have for that sort of value assessment? Or is there some kind of auction system that’s somehow less negative-sum than the current all-pay auction/lawyer system? What do they do about interpersonal disputes (battery or murder, say)? If a metaphorical hawk shows up in the grand game of Hawk and Dove that is society, what happens?\n> \n> **Jeff Dubin:** Sure!\n> \n> How is it decided who gets what stuff? Is there a \"from each according to their ability, to each according to their need\" framework? Is there some sort of competitive market system? What can and cannot be owned as property by an individual or family? How are the needs of children, elderly and disabled persons, etc met?\n> \n> **David Spearman:** I’d also like to hear what sort of intellectual property system they have.\n> \n> Also, what is the difference between how the system handles malfeasance from a Very Serious Person versus from a rando?\n> \n> **Jeff Dubin:** @David Spearman   Or, have the VSPs somehow been trained not to Malfease?\n> \n> **David Spearman:** @Jeff Dubin I would be disappointed if this was the answer, unless there’s a *very* detailed explanation of what that training entails.\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** I want to say that there's the basically obvious Georgist system of private property with public capture of economic rents, but in this Earth I hardly know what's \"obvious\" anymore. I'll put it the way it would be put in dath ilan second grade.\n> \n> There's a planet everybody lives on, of which conceptually everybody ought to start out with an equal interest in the raw resources thereof; the raw resources in untransformed states are not directly very valuable, but labor requires access to these resources in order to be valuable and value-add to them.\n> \n> If your economic system is that Bill Gates owns all land and permanent installations and doles out scraps of food in exchange for labor, then even if this could look from a libertarian standpoint like a \"private property\" system in which Bill Gates happens to own all the property, this is not equitable and everyone who isn't Bill Gates ought to shrug off the consensual hallucination claiming that all the matter in the universe is tagged with a tiny private property tag saying that Bill Gates owns it.\n> \n> At the opposite extreme, to the extent that the result of labor adding value to resources is the expropriation of the transformed resource, labor has less incentive to add value and is being expropriated. Or if you look at it from the perspective of the labor, versus the person who gets to keep what the labor produces, they'd have an interest in spending some of their labor to band together to fend off the bandit who takes away most of the transformed resource, which is why, once labor has transformed a resource and added value to it, there's a reason to let the laborer keep or trade that transformed resource.\n> \n> And then we introduce the second-grade notion of an \"economic rent\" which is that some things, like land, are valuable in a way where we don't get any *more* of them, depending on who's said to own them - unlike the way we get more labor, if the laborers can keep what they make; and the second-grade notion of trade and allocation, which is that different resources are worth different amounts to different people, and if you let people bid differently on those resources, that introduces a factor where people who can use the resource to produce more will bid more highly on it, modulo another acknowledged factor where people who have more resources available can also bid higher.\n> \n> This all combines into a notion of taking the planet's prior endowment of arable land, minable ores, livable land, etcetera; and establishing an ongoing auction-lease system for renting them, where people can pay excess amounts to establish propertylike momentum around places where they want to build permanent installations; but there's still a system that demands ongoing rents, in order to encourage reallocation, and in order to give everybody in the world a share of that rent in order to represent their interest in respecting the \"property\" coordination-hallucination at all, in the way they wouldn't have any such interest in coordinating to respect a pretense of private property where everything was tagged as being owned by Bill Gates.\n> \n> There's an explicit understanding of concepts like \"value added to land in virtue of its proximity to other land where people live\", so rather than distributing the economic rent equally to everybody, like you would for a minable ore, it makes sense to direct some of that rent to a citylike entity that can reinvest the increased local rent it created in making that city larger and more valuable by producing goods with inherent coordination-nature like roads (where the value of any one road segment is dependent on the value of other segments next to it, and where multiple entities with veto ability could each try to capture all of the value-added from the whole road, giving the road a public-good-nature).\n> \n> Basically, the kind of private property system you would invent if you understood that you were *inventing* it rather than codifying some pre-existing natural law.\n> \n> **Eliezer Yudkowsky:** Intellectual property in eg the form of \"patents\" the way Earth does it, is a needless special case of property with the veto-nature where you want to avoid ending up with multiple entities that can each veto the good being produced, which gives each of them an incentive to try to capture all of the value and produces a coordination problem in producing the good. For this reason, nobody on dath ilan would consider introducing an Earth-style patent system, even leaving aside the extent to which the US patent system malfunctions on its own terms and patents lots of things that are obvious to any practitioner skilled in the art.\n> \n> If you create an invention that benefits an industry, it's understood by a solider and sharper-toothed honor system that the industry is expected to donate back 15% of the marginal value thereby produced to the original inventor or invention group, and there are external-auditor-like agencies to sign off on accounting (accounting is generally much simpler in dath ilan because they do not have a regulatory process that has gone completely out of control); the closest thing in Earth terms would be a mandatory shall-license patent system.\n> \n> If you did the equivalent of what BioNTech did and invented a Covid-19 vaccine, and Moderna had no mRNA vaccine of their own (meaning this was something that *only* BioNTech could do), anybody would be allowed to produce it, but *EVERYBODY* would be honor-bound to donate rather a *LOT* of money to BioNTech and to the inventors and discoverers of RNA vaccines; who donated how much money would be a public fact and your friends might look at you funny if you donated nothing; with some of that money tending to be in the form of donations to let the inventors of mRNA vaccines set up their own scientific funding agencies, to make the way easier for the next generation of scientists. All of this would be the sort of thing that Very Serious People had heated debates about in newspapers; and all of the solutions inside their Overton Window would be about equally good in the larger scheme of things compared to Earth solutions, regardless of which exact details ended up being picked.\n> \n> Copyright doesn't face the same collision issue where multiple parties can extract all value, and by one of those weird little coincidences, dath ilan has the same 14-year copyright rule that was originally baked into the US Constitution (albeit with no renewals allowed, because it happens to not be an exact coincidental collision). 14 years is enough for people to make a profit on copyrighted creations, thereby incentivizing its existence; and then that work goes into the same pool of public domain that implicitly helped create that copyrighted work and whatever shoulders it stood upon.\n> \n> **Eliezer Yudkowsky:** Very Serious People are not rulers invested with formal authority; their work, by its nature, consists of public debates with other Very Serious People. Any \"malfeasance\" in the obvious sense would have to consist of saying the wrong thing in a public debate, but then if other VSPs didn't catch it and call them on it, couldn't they just claim it was an honest mistake?\n> \n> I guess you could have an instance where a Very Serious Person pretended to have important anecdotal evidence from their own life history about something, and then was caught out on having made it up. I can't actually remember hearing about a case like that, per se; the big deal tends to be groups of Very Serious People making Very Serious dire predictions and then turning out to be totally wrong, which is considered especially iffy because of how making a dire prediction about the conditional result of doing X can prevent anybody from actually testing X and thereby finding out the dire results. Albeit that dath ilan is *much* more likely to set aside a little hamlet where people try doing X anyways, which is how the Very Serious People get caught being too pessimistic.\n> \n> But it's still understood that when it comes to, say, messing around with breeding viruses, it is *reasonable* to say that this is a direly dangerous thing even to experiment with. In fact it is emphasized *oddly* strongly how *totally reasonable* this would be, if Civilization ever *did* run into something that could plausibly wipe it out in one shot; and that the fact that somebody *could* have an incentive to gain attention by warning direly against ever trying X, then escaping falsification through X not being tried, must *not* mean that we reject arguments of this type out of hand; because then Civilization is inevitably and undignifiedly doomed if it ever runs into anything that actually can wipe it out in one shot.\n> \n> The very strong emphasis here makes more sense to me now that I realize that AGI issues were probably secretly in the background informing some of the top people.\n> \n> **Eliezer Yudkowsky:** To a first approximation, everybody in dath ilan is an economist, in the same way that everybody on Earth is a scribe and a calculator from the perspective of Earth's medieval era. So when it comes to things like setting up courts, people understand that you get what you pay for and you pay for what you measure and that measuring things is dangerous.\n> \n> There's much more of an emphasis on courts producing judgments where they write out all of the reasoning used, in a way that superior courts and ultimately cities and delegates can check over, and less of an emphasis on \"His Honor said so, so shut up and respect him, peon\". But you're paying courts for a certain kind of reliable reproducible reasoning that's supposed to reflect a particular set of agreed-on standards and agreed-on rules, not just for having a very scary honorable person in robes hand down a dictum that everybody has to agree with; the emphasis on writing out all the reasoning is to try to make it easier to measure what you're supposed to be paying for. If it was a big enough issue, you'd pay for two courts so you could check if they agreed.\n> \n> \\[...\\]\n> \n> **Ben Pace:** This \\[method for incentivizing intellectual innovation\\] is a bit harder in art. Like, I can indeed track down the authors that I think influenced me the most (my puns from Scott, my titles from Hanson, my concepts from you, etc) and pay them money, but it’s harder to enforce than if I am using the vaccine built by BioNTech. It’s harder for others to see that happened.\n> \n> That said, I can also imagine publicly ”taking” people‘s ideas is just good and encouraged, such that it’s more natural. Right now I don’t say that I stole my writing style from people too much, in part because it can be seen as bad form to copy people’s intellectual/artistic work, but if it were more encouraged then accounting would be easier too.\n> \n> **Matthew Graves:** I assume this is also tied to a crowdfunding-like invention system, instead of a monopoly-profits-driven invention system? Or does the honor extend all the way up the stack?\n> \n> Motivating example: suppose there are 10k people with a disease, who in aggregate would be willing to pay $10k each to not have the disease, so there's $100M of value 'on the table' for curing it. Alice develops a treatment, hoping to sell it to each of those people and get $100M in revenue, and then Bob produces it at marginal cost (say, $100 each), makes $1M in revenue, and has no profits to donate to Alice.\n> \n> One could imagine all of the customers deciding that, well, they need to donate $1.5k to Alice to pay her back for the invention. But this is substantially less than Alice would have gotten in monopoly revenues, and (more importantly in my eyes) requires all 10k customers to track the question of whether or not their treatment supplier has honorably discharged their duty to reward Alice. But also how much that duty is depends on how much those patients actually value not having the disease.\n> \n> (And if I employ one of those patients, do I have a duty to reward Alice for the increased productivity? Or to track whether or not my employees have rewarded Alice enough?)\n> \n> **Eliezer Yudkowsky:** Ben: I wouldn't need to pay anybody for having written [HPMOR](http://www.hpmor.com/) \\- that kind of inspiration is just considered part of the common pool where everybody is inspired by everybody else. Paying an author for their quoted work is payment enough.\n> \n> Matthew: You're not supposed to capture all of the value you create. If the treatment is worth $10k each to the treated, $1.5K is a very reasonable amount for them to donate to Alice.\n\n* * *\n\n> **Jean-Baptiste Clemens:** Did successful coordination require an authoritarian government or dictatorship and omnipresent surveillance to ensure compliance?\n> \n> Because I don't see how else this could work with billions of people, the vast majority of them being strangers to each other, when even a small group of close friends can have trouble reaching a consensus on something as simple as where to have lunch.\n> \n> **Eliezer Yudkowsky:** Leviathan is what people here do *instead of* having everybody in the room know what a coordination problem is, work out a coordination solution (including coordinated enforcement of the solution if there's large defection incentives), pool their solutions to all work together, and then all do the thing. This requires *previously* having coordinated your Civilization well enough to make sure that everybody in the room knows the abstract theory and has practiced it across many trials, but the equilibrium is stable once you get there, especially if everybody knows what the equilibrium *is* and how important it is to keep it stable.\n> \n> **Jean-Baptiste Clemens:** @Eliezer Yudkowsky Right, that's a good point. The social contract is this Earth's sub-optimal arrangement, but I'm interested to know how everybody on parallel Earth initially came to understand coordination problems and reach a consensus on how to solve these problems in the first place, despite the statistical inevitability of so many people having competing and incompatible ideas, priorities and preferences. Does everybody on parallel Earth have a shared mind/consciousness?\n> \n> I would like to rephrase my original question but I feel it would be unfair to edit it after Eliezer has already responded, so I will rephrase my question here:\n> \n> Did the *initial* coordination necessary to reach a stable equilibrium of successfully solving coordination problems require an authoritarian government or dictatorship and omnipresent surveillance to ensure compliance during the initial coordination problem solving period?\n> \n> **Eliezer Yudkowsky:** I don't think we can get better coordination the way I suspect that dath ilan wandered into it, certainly not before AGI. Gregory Cochran (IIRC) has a theory about how the secret sauce of the Industrial Revolution was the children of shopkeeper classes starting with a larger inheritance and outreproducing others. As I previously mentioned a couple of comments up, a lot of dath ilan's earlier history is considered a Highly Unpleasant Thing It Is Sometimes Necessary To Know and a mild cognitohazard, but my suspicion is that a lot of the real work was done by a historical accident of this sort.\n\n* * *\n\n> **Luca Ross:** How did they solve competing access needs?\n> \n> **Eliezer Yudkowsky:** Be more specific?\n> \n> **Luca Ross:** @Eliezer Yudkowsky   I think of competing access needs as a sort of coordination problem, probably. In that it seems like coordination is the only feasible solution prior to the Glorious Transhumanist Future. Stuff like people with severe dog allergies or phobias and people with service dogs needing access to the same space at the same time. Or people who are triggered to self harm by seeing images with self harm scars in them, and people who need a place where they can post normal pictures of themselves without their scars being brought up needing access to the same support groups. People who have audible stims and people who are noise sensitive but need to be able to hear other things in the same environment (like class).\n> \n> Anything where people's needs are mutually exclusive but they need access to the same thing.\n> \n> **Eliezer Yudkowsky:** Diversity of places! On Earth you have dozens of countries with nearly cookie-cutter regulatory systems and equilibria, containing thousands of nearly identical cities. On dath ilan, the whole reason for having *different places* is either to mine natural resources, to run an experiment, or to pick a different public equilibrium of this kind.\n> \n> **Daniel Powell:** @Eliezer Yudkowsky oh, so you don’t have competing access needs because everyone with aphonia and experience distress from sounds lives in one dimension where they don’t have to interact with people who need a clicker to stim!\n\n> That’s a lot of parallel NYC subway systems. How long does it take to phase to the right one?\n\n* * *\n\n> **David Spearman:** Besides the whole “kink and macroecon” thing, are there any other ways they were consistently worse than Earth? In particular, how is the economy not a constant, negative-sum war between price fixing cartels, consumer cartels, etc. What stops otherwise-competitive producers from coordinating around “charge the monopolistic price and don’t overproduce”?\n> \n> **Eliezer Yudkowsky:** That's an excellent question! The first thing I'd say in reply is to point out that some *individual* at the head of one of these concerns, does not themselves capture the whole value of the cartel - maybe the cartel makes a billion dollars, but that person doesn't get to take home the whole value themselves. But they do get to take home a terrible reputation for having knowingly acted against the interests of Civilization at scale.\n> \n> On Earth, you have puddled reputational pools where somebody gets to go home with their fellow cigar-smoking villains and have a high reputation among them as a great successful villain. If something like that started to develop on dath ilan, it would be a Huge Problem and all the Very Serious People would see it as a Huge Problem and it would be on *all* the news programs once uncovered, and if they had to pay a billion dollars to set up a new competitor to drive that concern out of business among all the good people who would immediately boycott it once uncovered, the Kickstarter would be funded the next morning.\n> \n> Where the Huge Problem, to be clear, would not so much be the price-fixing aspect, as the fact that a separate reputational puddle had developed in which powerful people could hoard a reputation as villains; who knows, maybe next up they're going to develop a bioweapon and wipe out half the planet so they can brag about *that* to their friends.\n> \n> So long as this separate reputational puddle hasn't been allowed to develop, then a would-be price-fixing cartel would have to comprise a lot of people who didn't get to take home all the money captured themselves, whose spouses and kids were all raised to believe that your pride and honor rest in coordinating good equilibria for Civilization, not for coordinating with a few people to defect in a way that benefits you a little and damages Civilization a lot. And if that starts to go wrong, there can be actual boycotts, Kickstarters to develop a new competitor, and lots of people who will remember your sins and still boycott you five years later. It's not a perfect equilibrium, but it's not that unstable once you're in it. The key difference is that *everybody* is *seeing* the global equilibrium and thinking five moves ahead and that's the way all the newspaper stories are written.\n> \n> **Simon Sandoval-Moshenberg:** ^ lots of Marx and Engels here but this comment is the Marx-iest of all.\n> \n> **David Spearman:** @Simon Sandoval-Moshenberg   not really. His comment equally applies to a consumer consortium which unionized to gain unwarranted monopsony power. Though I will note that it assumes a no-cartel equilibrium where the VSP’s can just play whack-a-mole as cartels arise. I’d expect a many-cartel equilibrium, at least at first, with no obvious path from there to the no-cartel state where the moles can be whacked case-by-case.\n> \n> **Ben Pace:** This aspect of the civilization sounds like there's a lot of cultural homogeneity within the civilization. Ensuring that everyone has a shared set of values regardless of decisions about upbringing and morality and way of life reduces cultural variance quite a bit. It's harder to have the Ravenclaws who as a point of their culture sit in an ivory tower reading/writing books and never leave to act on the world (though will answer questions occasionally). It's harder to have the Quakers who live technologically backward because they think the tech damages their community. It's harder to have a group that leaves broader civilization for 30 years because they think it's failing and they need to think clearly on their own and bring up children elsewhere.\n> \n> But I guess it doesn't actually sound that hard, nor costly enough to make it not worth it. Just have a set of agreements with these people when they set up their cultures, about how to interface with the outside world, and check in occasionally to ensure agreements are being kept. Sounds deeply worth it.\n> \n> **Eliezer Yudkowsky:** Global homogeneity, local variations. 99% of the planet may be thinking the same way about something because that's what the textbooks say, but it's also *much* easier than on Earth to set up a town somewhere that does one *particular* thing very differently. It's understood that being Able To Run Experiments is an important feature of Civilization.\n> \n> **Ben Pace:** More global coordination and more local experimentation.\n\n* * *\n\n> **David Spearman:** What is their fiction like? What would the most-cited pages of their equivalent of TVTropes look like?\n> \n> (Alternative framing: if they had an equivalent to Warhammer 40k, what would the different factions look like?)\n> \n> **Eliezer Yudkowsky:** All my brilliantly original work over here is me just writing completely stereotypical dath ilan cliches and cackling to myself. [HPMOR](http://www.hpmor.com/) would be their equivalent of a Harlequin romance or a Kindle Unlimited dungeon-core cultivation monster-girl harem novel.\n\n* * *\n\n> **Alex Zavoluk:** What's the best food on your home planet?\n> \n> **Eliezer Yudkowsky:** Swedish meatballs!\n\n* * *\n\n> **Adam Priest:** Do people have jobs?\n> \n> Are \\[workplaces\\] unionised?\n> \n> What prevents the bosses exploiting the workers?\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** Yes, no, and the economy runs hot enough that a hair salon has to work hard to find labor if they want to be able to expand. As for why, if you asked dath ilan *why* they always have enough job openings to compete hard for labor, they'd give you a puzzled look and point out that since people could afford to pay for labor 1000 years ago when technological productivity was much lower, obviously today there should be *lots* of people who can pay at least survival food and shelter for some bit of labor they want done; and indeed, since there are many more payable jobs like this than laborers, employers have to pay much *more* than this for labor they want done; so employees can choose which job they want, and can tell abusive bosses to either fuck off or pay them a huge premium for putting up with it.\n> \n> If you showed them Earth's economy where people struggle to find jobs and employers treat employment as a huge favor they dole out that can come packaged with abuse, they would stare with huge eyes and then try to figure out what the hell had gone so wrong and how it was even *possible* to build a system like ours in a world with 100x medieval agricultural productivity.\n> \n> They would start to understand once you listed out all of the different obstacles to an employer employing somebody. But my guess is that if you come at this from the frame of \"what prevents the bosses exploiting the workers?\", it might be a long conversation before I could describe how a dath ilan eye parses up the obstacles to employment here, that prevent there from being much more competition for labor here.\n\n* * *\n\n> **Marcello Herreshoff:** How are the parallel Earth's societal coordination mechanisms protected from corruption and conflicts of interest? (On Earth classic, the intended vehicles for solving coordination problems that markets and independent actors cannot are our various governments, but we all know how that's turning out.)\n> \n> **Eliezer Yudkowsky:** I'm not quite sure how to answer this - everybody in dath ilan is an economist the same way everybody in the USA is a scribe from the perspective of medieval times, so everybody knows that enforcement is part of the problem of maintaining an equilibrium, that this enforcement is often a nonrival and/or non-excludable good you have to coordinate to pay for, that you can only enforce what you can see and measure, etc?\n> \n> In terms of things I can say about that in general... maybe one thing is that dath ilan understands that the first remedy to potential corruption or conflict of interest is *visibility* and *accountability*, rather than trying to make sure that no one person has power via committee, or writing big books of regulations to supposedly govern the committee decision. That is, the dath ilan approach is to appoint one person who has to hear things in public, write down their reasoning, publish their reasoning and decision, and get their results checked three years later.\n\n* * *\n\n> **Bill Doherty:** How does the dating market differ on your Earth?\n> \n> **Eliezer Yudkowsky:** It's much more the domain of paid professionals, something like a real estate broker where you tell them all about who you are and who you're looking for, they get together with other real estate brokers to look for matches, and you'd pay them based on results if you were still happy 6 months or 10 years later.\n\n* * *\n\n> **Daniel Sturtevant:** How did y'all compensate for \"my stake in this coordination problem is more important than yours,\" bias in everyday, not-super-deliberative cognition? Or did everyone get eaten by tigers?\n> \n> **Eliezer Yudkowsky:** I'm not quite sure which problem class you're envisioning. People here on Earth routinely run into situations, any time there's a coordination problem, where somebody could conceivably claim their stakes mattered more? And people here on this Earth have a reputation concept and an implicit social capital system, where somebody who claims exceptions too often, and who doesn't produce compensating value, will start to lose friends, so nobody wants to hang out with them after the third time that they claimed they couldn't afford to pay for their share of the pizza?\n> \n> People in dath ilan don't know they're supposed to be in a Utopian illustration of people talking about coordination problems all the time, or that they're supposed to be solving them perfectly. The main difference, if there even is one, is that if somebody repeatedly doesn't pay for the pizza, everybody has a shared verbal abstract concept of what's going on, and they can say, \"Look, if you don't believe us about what we think you're doing, namely defecting in a <dath ilan equivalent of the Prisoner's Dilemma parable>, we can ask a disinterested third party\" or \"You're spending social capital like water, pal\" instead of quietly and resentfully dumping the defector.\n\n* * *\n\n> **Ben Pace:** Broadly, what do leaders look like in your world? Are there people who are respected and are able to take action for the nations of your world without being punished for it and slapped down? Somewhat relatedly, do they interact with social media more like Elon Musk or more like Paul Graham?\n> \n> **Eliezer Yudkowsky:** Computing media less advanced, social media doesn't exist yet and probably wouldn't be allowed to exist. If they did exist, the Very Serious People would look like Paul Graham but with something closer to Elon Musk's sense of humor, albeit with the humor more restrained and carefully set apart in most cases.\n> \n> The Earth concept of \"nations\" is very much about barriers to immigration and barriers to trade, both of which would be considered harmful in dath ilan. There are huge factions, there are huge special interest groups, there are regions with their local public goods that you are required to pay for if you live there, but people belong to more than one of those; they don't belong to A Country. Somebody has to run those organizations but it is not assumed that they can Speak For Their People the same way *unless* somebody has actually set up a voting/delegation structure for that organization.\n> \n> It's understood that it can be dangerous to leave out the details and that governance is dangerous. Imagine if the newspapers consistently described Biden as \"the FPTP primary-general Electoral College indirect delegate with a 53% overall approval rating\" or some such instead of \"The President\", except that the phrase was much shorter because it contained abbreviation symbols that everyone already recognized, like using INTP to describe somebody's personality. Then as much ability to speak for the citizens as you want to recognize somebody like that as having, that's how much Biden has.\n> \n> Open combat where you actually destroy people and property is a huge undignified failure in dath ilan, like, that cannot *possibly* be on the Pareto equilibrium, what are you doing wrong. If somebody hasn't gone literally schizophrenic and needed to be restrained, it's seen as shameful for everybody that it got that far. If you violate regional rules you're told to leave the region; if you refuse to pay a fine you contracted for, the contract says that your bank can take it from your account.\n> \n> Since you can move about from one region to another rather than there being this huge Immigration deal, you're not supposed to be in Your Country that's the only country you can live and that has to punish you to make you obey its rules; what it tends to do is kick you out, and depending on reason, fewer other countries may accept you for a time. Less prisons, more Australia (actually the continent we'd call Japan, but a totally different place).\n> \n> So the Grand Authority that concentrates in countries, counties, cities, and police forces as arbiters of violence, is more distributed across factions and regional experiments and people heading up particular organizations. There is less authority that stems from being the person who commands the organization of people with lots of guns, and more authority that stems from enough people having actually explicitly said that they'll follow you if you say to boycott somebody. The judge who's contracted to follow a constitution and determines on that basis when to tell somebody \"You're not playing by the experimental rules we set out, get out of this regional experiment\" or \"You can't enter this city, based on the public reason you were expelled from your last home\" is not a spiritual leader, they are not the holy person entitled to command violence.\n> \n> It's not that there are no people trained to use advanced weapons and factories set to be quickly repurposable to making them - people do understand that you don't want to let the first defector conquer the whole world with just a bread knife - but it's understood that the actual use of this power to settle conflicts represents a grave danger to everybody; it's not used *routinely* the same way it is on Earth. It is possible to coordinate around deploying less pride-injurious solutions instead, where you don't have to walk around in public being visibly way off the Pareto bargaining frontier.\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** I don't want to make it sound like taxes are voluntary. Different regions will, to different degrees, be trying to produce non-excludable goods, and entry to that region will require an explicit commitment to pay for them. But this is less of an issue of being an Earth-style milkable tax cattle because no other nice place in the world will let you emigrate and be a citizen; local goods like this are more often produced on the city or megacity level than on an enormous national level, so there are a *lot* of choices in where to pay taxes to, and you can travel a short distance and pay taxes somewhere else. It is more like being a customer of a business establishment and less like being a cow that gets milked, and if that sounds utopian, it's because people explicitly sat down and thought about the problem and put some effort into optimizing the larger structures that got adopted.\n> \n> **David Moscovici:** What typically happens if a regional experiment is engaging in behavior X, that is utterly abhorrent to the people/leaders in a neighboring bigger/stronger administration?\n> \n> **Eliezer Yudkowsky:** Like... preventing people from leaving? Brainwashing kids? Leaving people's brains to rot instead of freezing them? I can't recall offhand reading about that happening - the Chroniclers try to write about things in proportion to how many people they actually affect, not how outrageous they are - but I think they'd expect an army to march on them, which is why it wouldn't happen often.\n> \n> **David Moscovici:** How long has it been since the last major bordered administration?\n> \n> **Eliezer Yudkowsky:** Don't know, wasn't a student of history. There wasn't an epochal moment so far as I know - just trade barriers going down, immigration barriers going down, taxation authority devolving to internal regions.\n\n* * *\n\n> **Ben Pace:** Has your world ever coordinated to slow economic progress down (e.g. due to concerns around nanotech/AGI/biorisk)? How did that work, what were the main pushbacks, and how were they appeased? And how did society continue to function given that massive numbers of people were told to e.g. not work?\n> \n> **Eliezer Yudkowsky:** I mean, given that I never heard anybody discussing Artificial General Intelligence and that computing progress was suspiciously slow compared to Earth, somebody clearly did something, but I don't know what, and in fact I had no clue whatsoever that anything unusual was missing until I got to this Earth. I mean, that is what an *actually effective* global conspiracy *should* look like. Some of the logic behind it ought to be clear from the point that, if I'd had any inkling in dath ilan, I would have found the highest-ranked shadarak I could easily get to and told them that I suspected I'd run across a bigass infohazard; and they would have told me what, if anything, to do from there.\n> \n> I don't understand what you mean about people being told to not work. Nobody knew they were being told not to work on Artificial Intelligence. They just worked on other things instead.\n\n* * *\n\n> **Ben Pace:** What's the simplest piece of software that over 1 billion people use on your Earth that is net positive and that we don't have?\n> \n> **Eliezer Yudkowsky:** Computing in general is less advanced, but we do have some of it. Most of the software that comes to mind is software supporting other political and organizational systems that you don't have here; the closest pure software I can think of is Kickstarter But In Full Generality, and that may actually exist by now but with not enough people using it or it not being legal to use it for the right things. There's software that supports a popular multilevel-delegation political system, what I might call a Dunbararchy, and I guess you could conceivably try to build that and let people use it to see what happened, even if you weren't allowed to make its results politically binding.\n\n* * *\n\n> **Jessop A Breth:** Does your planet have humans?\n> \n> **Eliezer Yudkowsky:** I have no particular reason to expect they'd have any trouble having kids with people from this planet, if you want to define \"the human species\" the way a species is usually defined.\n\n* * *\n\n> **David Spearman:** How does the society figure out what better alternative to the status quo are? Is there some system like [Archipelago](https://slatestarcodex.com/2014/06/07/archipelago-and-atomic-communitarianism/) where you can run small-scale experiments which the VSP’s can go on to signal boost to the rest of the population?\n> \n> **Eliezer Yudkowsky:** It's widely understood that the point of having regions apart from one giant homogenous optimal Megacity is so that you can do a thing differently in different places, yes. Either to house people with different priorities about how to set regional parameters that need a single setting - there is probably somewhere out there where everybody goes naked, though I didn't particularly look for it at the time - or to run experiments.\n\n* * *\n\n> **Maximilian Schlederer:** Are people on parallel Earth more intelligent than on our Earth?\n> \n> **Eliezer Yudkowsky:** Yes because they have actual education. And even apart from that, it's almost impossible that they wouldn't be, if only because there's a norm against chronically unhappy people having kids, and that probably reduces prevalence of a bunch of low-grade health issues.\n\n* * *\n\n> **Connor Heaton:** How much closer are they to general AI?\n> \n> **Eliezer Yudkowsky:** Impossible to even guess. I imagine that they're treating their ability to take it slowly as a huge resource, which implies that they're doing a bunch of capabilities research and restricting the results. I have no idea at all how far they've gotten with the lower computing resources they have.\n\n* * *\n\n> **Tom Pandolfo:** Does your world have a minimum standard of living guaranteed by law?\n> \n> (That is, every person has adequate housing, is adequately fed, and has reasonably adequate healthcare - and these are guaranteed by law, not as an \"inevitable consequence of The Market.\")\n> \n> **Eliezer Yudkowsky:** The mineable resources of the world have economic rents that's captured via Georgist principle and is available equally to all citizens as an income, but there's no attempt to protect that income from taxation or treat it as a Universal Basic Income that ought to be sufficient to live on; it's just the rent that people get paid for respecting the existence of a property system at all.\n> \n> People donate around 15% of their income to good causes in one form another, and while that's always been very competitive, it's also always been enough to support the existence of Quiet Cities where you can go to retire from Civilization if you haven't had any kids. If you *did* have kids, your kids would have other options for being supported, but *you* might be allowed to starve; and if you tried to steal to support yourself, you'd be deported until only Australia (actually the territory we'd call Japan) would accept you, and then you might die there. You're not supposed to be having kids if you aren't sure of your ability to support yourself in Civilization, and it's understood that there should be some incentive structure against that.\n> \n> If you randomly get injured in an avalanche, there's insurance for that kind of thing, and if somehow you ended up with Real Genuine Unforeseeable Unavoidable Bad Luck after having kids, your relatives or friends or some Very Serious Person might serve as a safety net. But the socially general safety net is deliberately and consciously restricted (for reasons of avoiding Malthusian equilibria and societies full of unhappy people) to people who have not had any kids.\n\n* * *\n\n> **David Bahry:** Is oligopolistic collusion considered a market failure, or an admirable instance of solving a coordination problem?\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** It'd be considered a failure of the consumers to coordinate on starting a non-oligopolistic competitor, I'd guess? And depending on what exactly had happened, I would expect a swarm of Very Serious People writing Very Serious Essays about some larger civilizational dysfunction that had allowed it to happen in the first place. See previous answers about individual oligopolists not being able to individually capture most of the monetary value seized by cartels, and on the understood civilizational importance of not having separate reputational puddles of important people who would pay them in prestige for having pulled off an elaborate locally coordinated defection.\n\n* * *\n\n> **David Bahry:** Are unions considered a market failure, or an admirable instance of solving a coordination problem?\n> \n> **Eliezer Yudkowsky:** Everybody in dath ilan is an economist the way everybody in this Earth is a scribe, so they would immediately reply that this is the sort of thing where supply-demand balancing prices seem like a perfectly fine solution; if you presented them with a situation where unions were necessary in order for employees to capture a fair amount of the value they created, they'd ask how the *hell* some employer had ended up with an effective monopsony on labor. This question sometimes has an answer on Earth; on dath ilan they'd spend their effort on avoiding the monopsony situation rather than on unionizing afterwards.\n> \n> I think there's a certain amount of *implicit* unionization in the sense that, sure, most of the employees in a company could get together and yell something, and it would not be considered very socially acceptable for the company to try to prevent that.\n> \n> On Earth, formal unions got started when companies were literally bringing in people with rifles to force miners to work. I think dath ilan would agree that unions are an appropriate and indeed *overly mild* response to this condition, but it is not a problem they are currently trying to solve.\n\n* * *\n\n> **Samy Gallienne:** How is the media system funded and distributed while ensuring quality? On our Earth, we haven't figured this one yet.\n> \n> Digital information wants to be free, but the labor to produce the information needs to be compensated. As a result, newspapers are struggling. \\[Government\\] doesn't seem to be the problem since that limits competition while risking propaganda.\n> \n> Meanwhile, distribution is \\[increasingly\\] reliant on social media which pushed dubious \\[ethical\\] practices like clickbait and rewards outrage-creation over quality.\n> \n> **Eliezer Yudkowsky:** Among the public goods that gets supported by the 15% of income that people publicly donate to charity, are the Chroniclers of Humankind - somewhere between what judges want to be, what journalists used to pretend to be, what Wikipedia aspires to be but with better writing; they are supposed to be very neutral, very fair, very kind to people who haven't deliberately massively screwed up, very well-paid, etcetera.\n> \n> Lots of other people write about things too, of course; but to the extent that there's such a thing as a Story there, it becomes part of the Story when the Chroniclers start to retell it. If Scott Alexander adopted a special voice that he used to speak when he wasn't taking sides in any partisan conflicts, he could in that voice be a Chronicler.\n> \n> \\[...\\]\n> \n> **David Moscovici:** Chron 1. How do Chroniclers avoid (or how is it avoided that Chroniclers) select new truths to 'declare' depending what ideology might grasp them? Do they never \"fringe out\" as most of our thinker/\\[analyst\\] categories do?\n> \n> Chron 2. Does their Chronicler status become threatened if they do gain a very uncommon understanding of the world?\n> \n> Chron 3. Who retires their status if their objectivity becomes heavily compromised?\n> \n> Chron 4. And how have 'new truths' become recognized?\n> \n> Chron 5. What happens when there are apparent and direct contradictions between pockets of Chroniclers?\n> \n> Chron 6. Does anything actively prevent capture in a certain space by a specific thinkgroup of Chroniclerhood?\n> \n> **Eliezer Yudkowsky:** The closest Earthly analogue is Wikipedia, I'd guess, or somewhere between Wikipedia and a high-functioning science journal; except that there's a common understanding of much more sophisticated discourse norms and reasoning norms, when it comes to saying that Chroniclers should be using standard reasoning to write about things. You need to remember that I am an *average* kid from that place; I did not *invent* the stuff I wrote about on Less Wrong but I did *know* all of it, albeit not with much sense of your Earth's appropriate citations.\n> \n> If a Chronicler was writing about something especially controversial using a non-public source, they'd probably call in a senior retired Chronicler to act as witness to the conversation, or something like that. Otherwise, why would there be any need to trust them in the first place? The Chronicler would just show their work.\n\n* * *\n\n> **David Schneider-Joseph:** In what way is the parallel Earth doing worse than ours?\n> \n> **Eliezer Yudkowsky:** At least some types of people in it are probably having less fun than those people would be having here, if those people were otherwise fairly wealthy in both places; though no especially striking non-socially-harmful examples are coming to mind except for people who want to take a lot of drugs and people into BDSM.\n> \n> **David Schneider-Joseph:** @Eliezer Yudkowsky Why would drugs and BDSM be adversely affected by society having a higher ability to solve coordination problems?\n\n* * *\n\n> **John Wentworth:** Sounds like a lot of previous answers involve making lots of information available to lots of people, and relying on reputation. How do people decide what to pay attention to? How do people notice when nobody else is paying attention to some important information?\n> \n> **Eliezer Yudkowsky:** I'm not sure there was any particularly magical solution other than having lots of people walking around knowing that this was an issue and a public good that they had to pay for in money and reputation. It's like asking why we had scientists actually running replications of experiments; we understood that this was important, and that you actually have to pay in money and honor for important things, not just pay lip service to them, so funding was available and newspapers would report the names of the first two replicators next to the people who'd found the preliminary hinting.\n\n* * *\n\n> **Brett C Allen:** Do informational asymmetries exist, or is everyone equally informed at the best resolution they have capacity to interpret?\n> \n> **Eliezer Yudkowsky:** Of course informational asymmetries exist, they're still bounded agents for heaven's sake!\n> \n> **Brett C Allen:** But then you cannot solve the class of problem such as the prisoners' dilemma, because geometries of action and incentive are possible that confer advantage based on a defect strategy?\n> \n> **Eliezer Yudkowsky:** Allow me to introduce the number one technical thingy I actually managed to partially remember from dath ilan: [https://arbital.com/p/logical_dt/?l=58f](https://arbital.com/p/logical_dt/?l=58f)\n\n* * *\n\n> **Thor Taylor:** If you could materialise a small city-state on Earth following Dath Ilani norms and customs, would it be robust to international politics? Or does the equilibrium on Dath Ilan require general consensus to be able to punish transgressors? E.g. is the militarisation required for a small state to fend off militant neighbours compatible with the freedoms you consider crucial? Are norms from a cooperative world robust against attack by selfish \\[foreign\\] actors playing zero sum or even negative sum games with trade and espionage?\n> \n> **Eliezer Yudkowsky:** I think that city state would effectively materialize far out of equilibrium, would immediately regard itself as being under siege, and would immediately start to try to build weapons of mass destruction in order to have a credible threat to prevent its conquest by the environment around it, which I'd expect to rapidly go *extremely* hostile if presented with a non-politely-Facebook-censored version of dath ilan culture. Our kids get explicitly trained on perspective-taking, and wouldn't have the expectation for an absolutely foreign culture to look very nice and pretty by the norms of suburban pontificators. Your culture makes no such allowances, has no such concept, and is full of \"low-decouplers\" who literally lack any internal grasp of the mental motion they would need to perform.\n\n* * *\n\n> **Ben Albert Pace:** What role does Robin Hanson have in your world? What job does he have?\n> \n> **David Spearman:** I imagine “Crazy Idea Guy” is a sub-genre of Very Serious Person.\n> \n> **Ben Pace:** I kinda want to know if he runs a university or manages a government department of prediction or (more likely) some third thing I haven't imagined.\n> \n> **Eliezer Yudkowsky:** There are no direct people-level analogues between worlds - even if dath ilan had only diverged 100 years earlier, that would be more than enough to butterfly out of existence almost everybody born more than a couple of years later, and dath ilan must have diverged much much before that.\n> \n> And there can also be no metaphorical analogue of Robin Hanson, because the whole concept of Robin Hanson is that he understands some particular things that *aren't* common knowledge here. You can't have that one person who goes around loudly observing that \"education isn't about human capital\" because dath ilan doesn't have an education system like that, and because it doesn't make you a contrarian iconoclast to suggest that you'll get what you measure and pay for.\n> \n> Or to put it another way, if there actually was a Robin Hanson back in that world, he was too much smarter than I was, and I was part of the mob of hoi polloi who couldn't tell the difference between that and any other crazy person.\n> \n> **Ben Pace:** What do the contrarian iconoclasts look like in your earth? I expect you'll say that they're off running experiments that nobody understands, and somedays they come back with incredible results and then they're absorbed into the way of being for the whole civilization.\n> \n> **Eliezer Yudkowsky:** Or not-so-incredible but still-cool results, but, uh, yeah? I mean, what do the contrarian iconoclasts look like inside the little crippled partial tiny fragment of my home culture that I managed to reproduce here? They look like a bunch of outright psychiatric nutcases, a bunch of people being loudly wrong, and a few people who made fortunes outbetting the markets during the Covid-19 crisis.\n\n* * *\n\n> **Patrick Lozada:** Which one (or ones) do you use and who decided this.\n> \n> ![](https://lh6.googleusercontent.com/GCC5z7VHZo8oF35ZrACMdioEAGmy8Om1ppbAyez_aIa_9tNwGq6ifcKAHJEX1DEN-KBTS-ROsMcukqorquDWLjCKdJMAsHkP96RJD6HYMsfbWG2aVfD_ggyNAT5H0fkZfOtpgtUT)\n> \n> **Eliezer Yudkowsky:** All cables had already evolved to their final universal form of USB-C by the time I was born.\n\n* * *\n\n> **David Bahry:** Does marriage exist (monogamous or otherwise), does divorce exist, and how common are they?\n> \n> **Eliezer Yudkowsky:** Monogamous heterosexual marriage is The Rule to an even greater extent than in the modern USA, and this is one of the places where I suspect our Earth is doing slightly better. Either social pressure is actually and effectively producing sexual conformity in dath ilan, or the general sense of \"If you're an unhappy misfit, don't have kids\" or \"Don't lie to yourself and others about who you are\" caused homosexuals to actually not reproduce or not be in fake heterosexual marriages with kids, over several generations, and they actually became a smaller population segment. 🙁\n\n* * *\n\n> **Daniel Powell:** What method do you use to determine how to distribute scarce resources?\n> \n> **Eliezer Yudkowsky:** If they're raw resources, conduct a Georgist auction to capture the economic rents from them, modulo a premium-paid to establish ownership-momentum if you have to build permanent installations near them, but with continuing rents due. If they're resources produced primarily by value added by scarce labor, those resources are owned by the producer and you trade with them. If that doesn't answer your question, what do you have in mind?\n> \n> **Daniel Powell:** @Eliezer Yudkowsky so there’s even more poor people, and the lines for Hamilton tickets are even longer?\n> \n> **Eliezer Yudkowsky:** I don't get it. :confused pikachu face:\n> \n> **Daniel Powell:** Well, the people who overestimated the mineral rights value of Gaul couldn’t participate in the bidding on Brittania or anywhere since, so they and their heirs have been locked out ever since.\n> \n> **Eliezer Yudkowsky:** That's the \"continuing rents due\" part? You don't just bid a little on the mines and own them forever.\n> \n> **Daniel Powell:** Yeah, they *over*estimated the first thing to auction off, and the person who bought the lease from them overestimated it less. Forever after they can’t afford to purchase any rights unless everyone with an average amount of currency underestimated the value.\n\n* * *\n\n> **Michael Blume:** Do private cars exist and if so how many people own them and how much explicit legal structure is there for handling their costs?\n> \n> **Eliezer Yudkowsky:** I mean, I'd expect there to *maybe* be regions for ore-mining where they just pave a road and have humans drive over the road, instead of paying the additional expense to set up automated car lines, because it's not worth the added expense considering the very small amount of irregular traffic - something like that? But if you let humans drive cars, they crash into each other and kill people, and much worse they occasionally *crush the brain and destroy the soul*. If there were regions where people go to crush their souls, I didn't hear about them. The length dath ilan goes to in order to avoid brain-destruction scenarios is finite, but *large*, and humans driving cars at high speeds sure do cause that.\n\n* * *\n\n> **Jessica Evans:** A world with more coordination sounds like a world with less liberty for the same basic reasons that democracy consistently produces tyrannies of the majority. Explain how you get \"more coordination\" for the same price at any level of complexity or be ridiculed.\n> \n> Putting it another way, I wish to a genie for \"more coordination\". Why doesn't the genie instantly solve this by making the world homogeneous in opinion, or making fringe disagreement dramatically more costly, or some other horrifying thing.\n> \n> **Eliezer Yudkowsky:** Coordinatees who understand coordination better? That world wasn't the product of the genie wish you just described?\n\n* * *\n\n> **Richard Wilde:** How did climate change work out for you?\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** Better-coordinated people can do the same amount of Science and Engineering with a smaller global population, meaning that we reached a roughly equivalent technological level with around a tenth of the population, so we didn't put a significant amount of CO2 into the atmosphere before transitioning to liquid-phase fission reactors as the primary energy source.\n> \n> \\[...\\]\n> \n> **Ben Pace:** I hadn't notice how path dependent the issue of climate change was!\n> \n> **Jim Syler:** @Eliezer Yudkowsky Waaait, but doesn't innovation scale with population, because you have a larger number of smart/lucky/innovative/etc. people?\n> \n> **Eliezer Yudkowsky:** Innovation scales *poorly* with population, and even more so here than there. On my home planet you are *much* more likely to see a big company producing an amount of innovation that is, like, proportional to the square root of the company's employment, which is to say that it is increasing *at all* with population size; as opposed to here on Earth where tiny startups are often around *equally* innovative with entire established companies.\n> \n> **Richard Wilde:** And how did you manage to keep your population so low?\n> \n> **Eliezer Yudkowsky:** It wasn't a coordination thing, it was just a question of igniting earlier along the population-growth curve. We didn't know our population was \"low\" because we weren't comparing it to Earth.\n\n* * *\n\n> **Erica Edelman:** Given that nobody is happy all the time, how do you separate out unhappy people (who shouldn't have kids) from not unhappy people? Is there an objective test that tells you your current happiness rating? Is there an absolute number on the happiness scale you should be, or is it like... the lowest 10% of the population. If it's an absolute number did a very large percentage of people not meet standards when this system first rolled out?\n> \n> **Eliezer Yudkowsky:** You could literally spend the rest of your life reading all the Very Serious People arguing about that. I would personally yell \"Bottom 20%!\" and then run away before they got me.\n> \n> **Erica Edelman:** Don't you worry that doing bottom 20% over lots and lots of generations is going to eventually lead to a world where everyone is... psychotically happy / mentally ill levels of happy?\n> \n> **Eliezer Yudkowsky:** THANK YOU MISS VERY SERIOUS PERSON FOR POINTING OUT THIS IMPORTANT FUTURE PROBLEM\n> \n> **Eliezer Yudkowsky:** (runs faster)\n> \n> **Eliezer Yudkowsky:** (summons David Pearce to distract her)\n\n* * *\n\n> **Ymir Vigfusson:** How does the police on your Earth bargain with pairs of prisoners who have been arrested for an alleged crime?\n> \n> **Eliezer Yudkowsky:** For one thing, the people who do the arresting are not the people who run the interrogation who are not the people who do the prosecuting who are not the people who run the prisons! See [https://yudkowsky.medium.com/a-comprehensive-reboot-of-law-enforcement-b76bfab850a3](https://yudkowsky.medium.com/a-comprehensive-reboot-of-law-enforcement-b76bfab850a3) for an elementary concept of how an economic literate might look at this kind of thing. Offering prisoners clemency in exchange for them purporting to inform on each other runs into all kinds of obvious horrible incentive problems *within the court system* that any Very Serious Person would point out before five seconds had elapsed.\n\n* * *\n\n> **Steve Jackson:** What types of tokens do they use to signal where the problems are, and how do they make sure the tokens keep moving?\n> \n> **Eliezer Yudkowsky:** Uh, are you referring to money? They use money.\n> \n> **Steve Jackson:** How do they make sure their money signals problems that need solving well? Or do they do that?\n> \n> **Kelley Meck:** Put another way:\n> \n> Do you have loans at interest? If yes, what did/do you do with the bad kind of lenders? If no, how does the market clear between now-problems and later-problems?\n> \n> **Eliezer Yudkowsky:** I'm confused. Surely the punishment for being a bad lender is that the bad lenders lose money? The public policy intervention here is not to bail them out, there, you're done; private insurance on deposits will now price-signal the riskiness of those deposits.\n\n* * *\n\n> **Andrew McKnight:** How do para-people notice when their problems are caused by coordination failure?\n> \n> **Eliezer Yudkowsky:** In virtue of literally everybody knowing what a Nash equilibrium is, what a Pareto optimum is, classroom situations that show them blowing up in practice, newspaper stories that analyze things in those terms, Very Serious People debating edge cases in Very Serious Debates? This is like asking how economists notice when something is balancing supply and demand, or when an arithmetician knows that it's time to count or add or multiply something; it's a form of literacy that is understood to underpin Civilization in much the same way as counting or reading, so the Very Serious People are constantly being Very Concerned any time it shows a 5% drop in one state region.\n> \n> **Andrew McKnight:** Eliezer Yudkowsky ahh, basic coordinacy to go with their literacy and numeracy. I see.\n> \n> If I may add a follow-up, are there major problems on para-Earth with false coordination where para-folks overcoordinate when they should just do the standard Earthly thing?\n> \n> **Eliezer Yudkowsky:** I get the impression they could be overthinking a few things - nothing specific comes to mind, just the general level of overhead and how often Very Serious People have Very Serious Takes on things, sort of like looking at Earth and saying that it spends too much on \"Left vs Right\" takes on things. But whoooa nelly does it look like it's better to spend too much thought on coordination problems than too little. If there's a Golden Mean Earth, it's probably 80% of the way to dath ilan.\n> \n> **Ben Pace:** I'm a bit confused by this. One of the advantages of noticing that there's a supply problem, is that the supplier can unilaterally change their price. Prices are indeed set by buyers unilaterally outbidding each other, and sellers unilaterally underbidding each other.\n> \n> Yet with coordination problems, even if everyone recognizes that something is a coordination problem (and has the concepts of Nash equilibria), you still often have to do much more surprising/novel things to switch equilibrium. I mean, you can just use politics as usual (\"Let's all jump at the same time because we're in a bad Nash equilibrium!\"), and that will go more easily if everyone understands the concept.\n\n> But I was expecting you might say something more like \"In my world is a much better reward mechanism for people who successfully solve such problems. If you manage to move your school/business/community into a better equilibrium, you are massively rewarded even if you did not monetize it, because at each level there is an equilibrium team whose job is to pour money into the bank accounts of people who do this.\" Or something. That probably doesn't work, but it sounds to me slightly more like it could.\n> \n> **Eliezer Yudkowsky:** Obviously you'd win an insane amount of social marbles if you somehow coordinated a really big jump toward a much better equilibrium, but that presumes that a much worse equilibrium was somehow allowed to develop in the first place and that you were the first person to spot it and see a solution. This is *actually* hard to pull off in dath ilan, *because* when a good solution is visible the activation energy is actually available to jump there. Part of the reason I was so quickly able to spot adequacy fantasies in this world as psychologically dysfunctional, is because I've *been* in the world where it's *actually* hard to spot big problems and big fixes, and *that world looks very different from this one*.\n> \n> **Ben Pace:** Okay. I think I have a picture of how it works on your Earth.\n> \n> I'm imagining the situation with the QWERTY keyboards, and Bob realizing that what was good for the old typewriters (not hitting keys next to each other) is not needed any more for our new keyboards.\n> \n> At this point in time there's lots of companies (i.e. 10-50) making these newfangled 'personal computers' in lots of countries and lots of different languages.\n> \n> I'm imagining Bob talking to each company, and saying (a) \"this is sort of maximally inefficient for our hands\" and (b) \"we're gonna be pretty soon in a Nash equilibrium where none of us can unilaterally improve it and everyone's learned the old one\".\n> \n> Then the companies probably give Bob command over when they all jump to the new one, because there's standard social protocols around doing this (and, though it's not something Bob actually thinks about, Bob knows in the back of his mind that if he were to use it corruptly they would follow through on punishing him, letting his community/employer know the sort of person he is, etc). He also knows he will get financially compensated by all of the companies, to the tune of like $5k (peanuts for them, adding up to a year's salary for him).\n\n> Once Bob has got 50% of them, the others quickly follow into line, because you know that when a Coordinator has got 50% of parties ready to jump, they're to be trusted and jumped with. Then after maybe 4-6 months of work (traveling and persuading initial companies), he hits jump, they all commit to changing keyboards on their upcoming computer, and the customers will just be forced to learn, with a slight dip in the economy for a bit followed by slightly faster growth after.\n> \n> **Eliezer Yudkowsky:** If you built a better school than an Earth-school baseline, people would be like, \"Okay, 5% of the story is about this awesome person here, to whom all due congratulations are due, and 95% of this story is about WHAT THE HELL FUCK WERE WE DOING?\"\n> \n> **Eliezer Yudkowsky:** Bob in your story is, like... three hundred Very Serious People arguing with each other in newspapers for several months.\n\n* * *\n\n> **J. Caitlin Elizondo:** Do you guys run on the same meat hardware, with the same type and degree of inclinations e.g. towards sex, love, status? If so, how do you manage for those whims not to sidetrack everything into oblivion?\n> \n> **Eliezer Yudkowsky:** It seems to me like pretty much the same meat hardware to the same extent that, say, Ashkenazic meat hardware is the same as Eskimo meat hardware? And I'm not really sure how to answer your question, maybe something like, \"Greater economic literacy and conscious awareness of short-term incentives means that people have put a lot more deliberation into being aware of where short-term incentives point and trying not to misalign them.\" Nobody has ever invented Twitter, and if anyone did, there would be immediate unanimous coordination around jumping to something else with better incentives and no 280-character limit, and if Twitter somehow still existed despite that, it would be taken for granted that you didn't want it inside the same web browser as your work web browser.\n> \n> One of the aspects of dath ilan civilization that I'd expect an Earth-person to find much more relatively offputting is the degree to which anybody in the top 75% (not a typo, I mean the top three-quarters) of personal attractiveness would be expected to wear a veil, or makeup to look uglier, applying to both men and women but with a lower threshold for women. If there were any such thing as a beach with people wearing scanty swimwear, it would have all kinds of Cognitohazard signs slapped all over it and nobody would ever publish any photographs of it.\n> \n> Why? So that if you go into a bedroom with somebody and get naked, you're not comparing them to the attractiveness of the top 0.1% of the population. If you want to shoot yourself in the foot like that, you'd have to go out of your way to do it and tromp past a lot of warning signs, because the rest of Civilization has comprehended \"ability to be attracted to the average naked person\" as a public good and is coordinating around preserving that public good. It lends all of Civilization a very deliberate and abstract quality that I'd expect to put off a lot of Earthers, and not without reason. But if Earth civilization doesn't *immediately* blow up in a vast orgy of sex and cookie-eating, it definitely shouldn't be surprising that dath ilan manages to walk on.\n\n> **Jay Schweikert:** @Eliezer Yudkowsky But what does that mean for the dating landscape? It sounds like dath ilan probably doesn’t have the equivalent of Tinder, or at least it’s not widely used for the same cognito-hazard reasons. But is relative attractiveness a factor at *all* when people are trying to figure out who they want to date?\n> \n> That is, I can understand “ability to be attracted to the average naked person” as a public good, and that a well-coordinated society could have social enforcement mechanisms like you describe. But assuming people still are actually more attracted to more attractive people, it’s harder to understand that people would just give up on trying to signal this information, or give up on trying to read those signals.\n> \n> So, for example, if you’re in a smaller groups of friends, can you take off the veil, or is that basically always required for anyone but a romantic partner? Are people expected to wear sweatpants and sweatshirts when they work out? Is there a whisper network for figuring out how attractive people actually are? How scandalous would a “no makeup singles bar” be? Is pornography a thing at all, or is it locked behind major warning gates?\n> \n> **Eliezer Yudkowsky:** If pornography is a thing, it was locked behind warning gates big enough that I was literally not exposed to the concept before Earth! There's sex manuals illustrated with carefully 20th-percentile unattractive people. There's a kind of loose robe-like clothing you'd wear to work out that would also serve the purpose of absorbing sweat and preventing it from getting all over the equipment, which honestly still seems like a pretty good idea to me, if I'm not just being homesick.\n\n> Computing in general is less advanced, I assume because somebody knows about AGI, and dath ilan never started having social media. If they started getting results remotely similar to Earth's social media it would all be shut down.\n> \n> So no, no Tinder. But a lot of deliberate understanding of the matching problem in dating. Some Very Serious People who are Very Concerned about Where It's All Headed would say that it's *too* deliberate, and people should just, like, fall in love at first sight properly. But roughly speaking, what they have instead of Tinder is real-estate brokers; you tell somebody *all* about yourself, *they* get to see you naked and take pictures; and if they can find you a mate by talking to their fellow brokers, then they get a bounty that's supposed to reflect 5% of the value-added of being in the better-than-you-could-have-found-on-your-own relationship they found you.\n> \n> When there's a big problem, expect dath ilan to have paid professional specialists to solve it; and everybody in dath ilan is an economist the same way that everybody on Earth is a scribe from a medieval perspective, so they will be very careful about what they measure and pay for. The problem of finding good people to date is a big one with lots of value dependent on it, so *obviously of course* there's going to be a well-paid professional class devoted to it, which people actually use, with payouts dependent on results because they know you get what you pay for.\n> \n> **J. Caitlin Elizondo:** This doesn't matter but how do people decide/figure out what attractiveness percentile specific people are in? Does everyone know their number, like an IQ score?\n\n> **Ben Pace:** I do anticipate that you can pay for sex and for dates and things on Dath Ilan, and that there is some price discrimination there, and that generally the people you pay are attractive and agreeable and extraverted and so on than average (though with lots more buying niches than are available on earth e.g. disagreeable quiet people). This would be valuable for all sorts of people e.g. people unable to get a mate, busy people, etc.\n> \n> **Eliezer Yudkowsky:** If you're more attractive than people you see on the street presenting as, don't let yourself look more attractive than that.\n> \n> **Raymond Arnold:** Huh, does this mean you don't have anime catgirls?\n> \n> **Eliezer Yudkowsky:** We don't.\n> \n> That is like *super* a thing that the Very Serious People would be *super* against.\n> \n> **JJ Treadway:** Is there a reason you don't instead *raise* the attractiveness of *less* attractive people (using e.g. plastic surgery or genetic engineering or mind-uploading into attractive artificial bodies) in order to reduce attractiveness-inequality? Do you just lack the technology to do this cheaply/safely, or is there some more subtle reason why this would be a bad idea regardless of its technological feasibility?\n> \n> **Raymond Arnold:** A thing that sticks out in my mind is that it's a runaway arms race, that doesn't really have a point (assuming a model that this is a domain where you just hedonically adapt, which you might or might not buy)\n> \n> **Eliezer Yudkowsky:** We don't have the tech? I don't think anybody would object to raising everybody's attractiveness and refiguring where the 25% cutoff was.\n\n> **Raymond Arnold:** Oh, to clarify – when I said anime catgirls, I meant, like, \"anime shows, that feature catgirls\" as opposed to actual catgirls. I'm assuming the answer is the same based on the porn one but just doublechecking we communicated successfully\n> \n> **Eliezer Yudkowsky:** Right. They try not to present people with fantasy worlds more attractive than reality. Respectable fantasy novels will generally start the protagonist off with a disadvantage and force them to reform some awful place, for the same reason.\n> \n> **Rodrigo Moreno Nuñez:** \"Everybody in dath ilan is an economist the same way that everybody on Earth is a scribe from a medieval perspective\" makes dath ilan so much more believable in \\[one\\] sentence. props!\n> \n> **J. Caitlin Elizondo:** Woa, do other people (here) walk down the street and have a clear sense how people compare in attractiveness to them?\n> \n> \\[...\\]\n> \n> **J. Caitlin Elizondo:** \"If pornography is a thing, it was locked behind warning gates big enough that I was literally not exposed to the concept before Earth.\" Why wouldn't you suspect the same might be true of kink?\n\n> **Eliezer Yudkowsky:** Maybe there were special regions where it was different, but I think the standard background of departure was very much in a headspace of, \"You want somebody to hurt you? Injure you? Cause you pain? That's not being Light-aligned. That's not how a biological organism is supposed to work. What's wrong with you? Do you have psychological damage? And you who want to cause pain, that's just called being Evil. Force it down, and if that makes you sufficiently unhappy, don't have kids so they won't be unhappy too.\" Even if there were places that were experimenting with having things be different, it wouldn't have occurred to me to look for them, and I expect a lot of other potentially kinky people wouldn't have looked for them either, or ever be exposed to the stimuli that could have made them realize they were kinky.\n> \n> **Eliezer Yudkowsky:** To be clear, I think that if dath ilan got a good look at the Earth equilibrium, they wouldn't just go into denial about it, there would be a Huge Very Serious Blowup about what had happened.\n> \n> **J. Caitlin Elizondo:** What had happened to Earth? Or them?\n> \n> **David Moscovici:** Would a category-fetish/preference, with criteria like hair color, or race, or profession be seen as Light-deviant?\n> \n> **Eliezer Yudkowsky:** What had happened to them - it *is* understood that having more fun is in some sense the ultimate purpose of existence.\n> \n> **Eliezer Yudkowsky:** David - no, that's just individual taste (or so they would agree). The nonobvious step for them would be distinguishing the desire to hurt somebody and cause them pain in a sexual way, versus, say, the desire to kill them in a sexual way. Among other things, you need to realize there are people who want in a sexual way to be hurt, which is not something you can necessarily figure out from inside of your head; there's a multi-step cognitive inference problem here.\n\n> **David Moscovici:** Would pursuit of subservience to the point of non-physical ill-treatment be seen as Light-deviant?\n> \n> And/or would offering such treatment be Light-deviant?\n> \n> (Just testing the bounds of such cultural freedoms beyond violent kink)\n> \n> **Eliezer Yudkowsky:** Yes and yes.\n\n* * *\n\n> **Kelley Meck:** What is music like on parallel earth? Is it still used for marketing of products? Is it still used for dancing?\n> \n> **Eliezer Yudkowsky:** More melodic, fewer words, the popular stuff is less repetitive and less based around very loud beats. I'm enough of a barbarian that I actually like Earth music better. It doesn't try to be respectable.\n> \n> Advertising is understood to be mostly a negative-sum game where people try to steal customers from each other or from other business sectors, and slightly a positive-sum game that could theoretically produce more informed consumers if for some reason the Chroniclers and Very Serious People and lesser reporters were all asleep on the job.\n> \n> It's not illegal, but it's understood that if you saw a Pepsi advertisement and switched some of your consumption from Coke to Pepsi, or from orange juice to Pepsi, or from pretty LED jewelry to Pepsi, you'd be contributing to a negative-sum game, which would generally go against your self-concept.\n> \n> But if Pepsi had an actual superior product they would, like, obviously go pay one hundred Very Serious People a *small* token fee to spend ten minutes trying their product. If they were trying to play great music along with having their product shown on TV, I think everybody would watch that and go, \"How dumb do they think we are?! What does this music have to do with the product?!\"\n> \n> Rhythmic sound and dancing is older than writing. That hasn't changed.\n\n* * *\n\n> **Alex Gunning:** “dath ilan” is an anagram of “thailand”, was this an intentional decision?\n> \n> **Eliezer Yudkowsky:** How could that possibly be true? Oh, you mean if it was a joke? If counterfactually this was all a joke, I don't see why I'd have anagrammed Thailand or what I could have meant by that.\n\n* * *\n\n> **Jared Collins:** How did you incorporate the bottom 10% distributions of the population on measures of intelligence, conscientiousness, industriousness, and agreeableness? I would think people in these demos, especially overlapping more than one, would be most inclined to defect or, worse, be mentally unable to think abstractly enough to govern their own behavior based on coordination values.\n> \n> There is a not-insignificant portion of the population, for example, who could not pass your second grade even as an adult based on cognitive ability or conformity deficit grounds. If you exiled all of them, the Australia equivalent would start looking worryingly crowded and fractious to the point of a human rights crisis.\n> \n> **Eliezer Yudkowsky:** Australia (actually Japan) is where people go when they've violated the regional rules to the point that no region wants them. If you haven't stolen or committed violence, and haven't had kids, then there are plenty of charity-supported Quiet Places that will accept you. We have 100X medieval productivity just like Earth ought to, and we're not setting all our resources on fire, so it's not hard to support any number of Quiet Places where people can go if they can't handle Civilization.\n> \n> **Jared Collins:** @Eliezer, Quiet Places are the part I missed. What goes on in them? Do they have jobs, factories? It seems like a very obvious sort of disutility to have significant portions (back - of- my- hand math clocks in north of 30%) of the pop base doing nothing because they can't be made to do so at the efficiency frontier. And many aren't going to sit around; the willingness to work is ingrained pretty deep. If they aren't rewarded for working by your civilization, they'll make their own (suboptimal) options.\n> \n> And if they can't handle civilization, they're not going to respect the self- imposed strictures on reproduction. It can't have escaped the attention of your VSPs that there's a real potential for an Eloi/ Morlock situation being set up (ref. H.G. Wells' 'The Time Machine', a culturally-well-known piece of speculative sci-fi from our side).\n\n* * *\n\n> **Karl Nordenstorm:** How is Esperanto or other optimized languages doing?\n> \n> **Eliezer Yudkowsky:** Finished over a hundred years ago. The benefits of having one shared planetary language were just too obvious, and that's leaving aside how much prettier our equivalent of Quenya is compared to the past's equivalent of Russian.\n\n* * *\n\n> **Patrick Hunter:** Do you know the earliest point of departure from Earth's history and any significant turning point that put you on the trajectory towards everyone knowing pretty modern economics? In particular were do things fall on the spectrum of economic ideas being invented much earlier vs institutions adapting faster to their existence. Like understanding Nash equilibrium seems pretty important to your society but the Earth concept dates to somewhere between 1838-1951 depending on if you want to count Cournot and even the early end of that postdates the existence of modern states.\n> \n> **Eliezer Yudkowsky:** I don't know about any overlap between histories at all, so we're probably looking at a divergence well before Sumeria.\n> \n> **Patrick Hunter:** That explains the size of the divergence. Have you talked to any linguists in/about your language, and do you have a rough idea of how old the basics of economic theory are in your world?\n> \n> **Eliezer Yudkowsky:** How the heck would I talk to a linguist about that?\n> \n> I don't actually have much of a sense of how old things are because the phenomenon of genius is less pronounced - people actually do make improvements as they become available, in some sense, rather than leaving them bundled up for one huge genius to take in one huge leap - which means that I don't have any Big Name like Adam Smith whose century I've memorized. I could wave my hands and say \"eh maybe three hundred years\"?\n> \n> **Patrick Hunter:** Ask on social media for linguist followers, and they'll be able to guide you through what they need to know.\n> \n> **Eliezer Yudkowsky:** On reflection, I think this problem is unsolvable? I can't think of a large-enough batch of place-names or preserved words that are all from the same region, and that could serve as something to pattern-match our pre-Universal languages against ancient Earth languages. Like, let's say that all you knew is the equivalent of Quenya, a synthetic language, plus a few names of cities that are older than a hundred years. Could somebody on Earth trace back the roots to Sumeria?\n> \n> **Patrick Hunter:** I don't know enough about linguistics to know I've just been primed by Glowfic to treat this as a default question for people from other worlds.\n\n* * *\n\n> **Jim Syler:** How do you maintain liberalism in a society?\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** Question too broad, maybe read answer comments above and then narrow it?\n> \n> **Jim Syler:** Hopefully you're asking me to be more specific rather than to ask a narrower question.\n> \n> Liberalism is probably the best social system—or at least the best social system for *this* Earth—honestly \\[virtuous\\], non-self-serving, and far-sighted philosopher-kings telling everyone what to think might work better, but that's not a feasible option here. So we're left with liberalism, in which no ideas are forbidden to be thought or expressed (which is not to say that every utterance is appropriate in every context, but there's *somewhere* you can go to discuss *anything* without risking being ostracized from society), so that error is tolerated and the best arguments are allowed to rise to the top in the marketplace of ideas. Basically what Jonathan Rauch can't shut up about, plus the notion of a balance of competing interests that the Constitution was based on.\n> \n> The problem with this is that it's in everyone's interest to have *their* ingroup rise to the top and suppress all the other groups, so that they've got all the power and no one is allowed to question their edicts, so that on *this* Earth, liberalism seems to be a fragile and unstable equilibrium.\n> \n> So how does one shape a society so that liberalism is robust and antifragile?\n> \n> (Note that although this question has both cultural and political aspects, I'm focusing on the social ones, as (I believe) politics is downstream of culture .)\n> \n> **Eliezer Yudkowsky:** I know of no solution to this problem that uses neither better-educated voters nor philosopher-kings. The dath ilan solution is that the voters explicitly understand the thing about the incentive for some faction, even a majority faction, to burn the free-speech commons, and they'd see where that would go, especially with all the Very Serious People who would yell about it.\n> \n> **Jim Syler:** @Eliezer Yudkowsky My question, then, is how you move toward having a populace that is generally aware of that.\n> \n> **Eliezer Yudkowsky:** Well, I tried.\n\n* * *\n\n> **Justin John Holt:** How is babby formed?\n> \n> **Eliezer Yudkowsky:** When a mommy and a daddy coordinate with each other very well.\n\n* * *\n\n> **David Bahry:** Does sex work exist and if so how is it viewed?\n> \n> \\[...\\]\n> \n> **Eliezer Yudkowsky:** Visiting an Experienced Professional Sex Worker is viewed as Concerning or Potentially Irreversible because if you have sex with somebody extremely experienced and good at it, who's focusing entirely on your pleasure, that's the sort of experience that could potentially ruin you for regular sex. But there isn't any concept of it being wrong to, I dunno, trade around regular money with regular people in order to remedy some imbalance of regular sex, like, \"I'd like to have sex where I don't have to worry about your orgasm, can I pay you fifty dollars for that\" isn't remarkable any more than asking somebody to do dishes that week in exchange for money. \"Okay, I realize this flirtation attempt failed, can I just pay you three hundred bucks up front\" might be a little weird and funny but it certainly wouldn't be *illegal*.\n> \n> Being an experienced professional sex worker isn't illegal either, it's just one of those things where you'd be honor-bound to warn potential customers what they're getting into and that reduces the number of customers because people actually pay attention to warnings like that.\n> \n> (I'd assume that somebody actually did run experiments somewhere about the effect on people of visiting highly experienced sex workers, which would be *extremely* legal. It would be legal even if counterfactually the rest was illegal for some reason. All *kinds* of things are legal if you do it on a small scale in the name of Science.)\n> \n> \\[...\\]\n> \n> **Kayla O'Brien:** I find the first part a bit strange because we don't assume that having an Extremely Experienced tutor or guide during our first experiences in other arenas to be likely to \"ruin\" us. Hearing my flute teacher play a particularly difficult piece inspires me it doesn't \"ruin\" me. And while my therapist focuses entirely on me and my problems and my thoughts and feelings, it doesn't make me any worse or \"ruined\" at talking with my other loved ones (in fact, it makes me better!)\n> \n> Why couldn't a Very Experienced Sex Worker help a less experienced partner be a better and more communicative lover just as easily?\n> \n> **Daniel Speyer:** Seems like Sex Teacher would be its own role, distinct from normal Sex Worker and probably a bit more prestigious.\n> \n> **Eliezer Yudkowsky:** Yup! Sex Teacher is a very different concept from High-Grade Expensive Professional Sex Worker. Your sex teacher is definitely *not* more attractive than your average partner will be, for example.\n\n* * *\n\n> **Roman Ponomaryov:** Who's cleaning the toilets there? (Meaning, who's doing all the jobs that no one would be interested in doing provided there is no financial or other kind of pressure).\n> \n> **Eliezer Yudkowsky:** People who get paid enough to do it anyways.\n\n* * *\n\n> **Cameron Taylor:** How do people coordinate around drug safety, quality and applicability? (What do you have instead of the FDA and a Doctor gatekeeper class.)\n> \n> **Eliezer Yudkowsky:** Hire several different reputable scientific-investigation companies to run trials and publish the likelihood functions? It's not that hard? Just strip out the violence from the system and leave the science.\n> \n> **Tomáš Kafka:** @Eliezer Yudkowsky Why isn't 40+ % of a drug market run by charlatans peddling homeopathy and garlic tinctures then?\n> \n> **Eliezer Yudkowsky:** Same reason that 40% of the furniture on Amazon isn't made from cardboard, mostly, with a side order of people who can read likelihood functions and old reputable institutions that produce them.\n\n* * *\n\n> **Karl Katz:** Are there recreational or social drugs? Same deal as over in the Culture, or not there yet, or something else?\n> \n> **Eliezer Yudkowsky:** I can't really remember hearing about them. I think that, like pornography, it would probably be restricted to special regions or factions where you had to look hard for them and cross some warning signs. The knowledge that there exists a long-term-costly substance you can consume to give you short-term pleasure or relief from psychic pain, is the sort of thing they'd treat as a minor infohazard in its own right, a Highly Unpleasant Thing It Is Sometimes Necessary To Know. The shadarak act as a repository for all the things like that.\n> \n> **Mateusz Drewienkowski:** @Eliezer Yudkowsky   I'd assume the same goes for alcohol? How about coffee? Tea? Sugar?\n> \n> **Marcello Herreshoff:** @Eliezer Yudkowsky\n> \n> \\> I think that, like pornography, it would probably be restricted to special regions or factions where you had to look hard for them and cross some warning signs.\n> \n> I can believe that answer for drugs, but this line about pornography shook my suspension of disbelief.\n> \n> It feels like it would require things roughly in the vicinity of:\n> \n> A. Technology like our modern cellphones with their video recording capabilities not being put into the hands of the citizenry at all (or with pretty draconian DRM style restrictions) because serious people think it's dangerous (which is frankly a kinda reasonable option, given the destructive nature of social media on earth, though not ideal given how much extra economic coordination power their unfettered use gives people.)\n> \n> or\n> \n> B. Some fairly intense norms telling people not to send each other explicit pictures using such devices (this is the sort of thing the horniest 10% of the population is going to do if you give them communication devices with cameras).\n> \n> or\n> \n> C. Some other strong social conventions drawing a boundary on the otherwise slippery slope between the existence of sexting and the existence of mass-distributed pornography.\n> \n> Which if any of these options did Dath Ilan society pick?\n> \n> **Eliezer Yudkowsky:** Primarily A from your list; computing technology was less generally advanced and there were bulky cellphones, not smartphones. But people otherwise on boinking terms, trading nude photos with each other, wouldn't have been seen as problematic in the first place; because it doesn't introduce the problem of real average people having to compete aesthetically with airbrushed photos of the top 0.01%, which is the part that would be seen as burning a commons of the population's hedonic treadmill, for somebody's short-term gain, by way of placing short-term temptations in front of other people.\n> \n> In other words, had the tech been that easy, it would have been answer C: people would see a very clear and obvious line between tempting people with what they can't realistically get except at great cost, and tempting people with what they can gain through an ordinary effort.\n\n* * *\n\n> **Ben Pace:** What are some of your favorite works of fiction from your Earth?\n> \n> **Eliezer Yudkowsky:** Considering that I can never read them again, this question is too painful for me to want to answer or think about.\n> \n> **David Moscovic:** Why are rereads forbidden?\n> \n> **Eliezer Yudkowsky:** Because I didn't get my book collection or any libraries with me when I suddenly found my mind here. Things would be very very different otherwise.\n> \n> **David Moscovici:** In our world, re-reads are a fairly niche behavior, even among readers. Is re-reading more common back home than we have it here, and is that for a cultural reason?\n> \n> **Eliezer Yudkowsky:** Have you read HPMOR? Most novels in dath ilan are meant to be read at least twice; many more times than that if you want to catch \\*all\\* the hints and foreshadowing.",
      "plaintextDescription": "Related: My April Fools Day Confession; Inadequate Equilibria\n\nOn April 1, Eliezer Yudkowsky ran a dath ilan AMA on Facebook:\n\n> I came from a parallel Earth that successfully coordinated around maintaining a higher level of ability to solve coordination problems. Ask me anything.\n\nWith Eliezer’s blessing, I’ve quoted the resultant discussion below, leaving out threads that were repeats or didn’t go anywhere.\n\n----------------------------------------\n\n> Guy Srinivasan: Did parallel Earth coordinate around a specific day each year for everyone to play with falsity?\n> \n> Eliezer Yudkowsky: Not a specific day as such. There's very much a tradition of leading somebody down a garden path, and also of pretending to be led down the garden path — similar to the \"MIRI pomodoro: 25 minutes of work followed by 5 minutes of trolling\" — but there's a verbal handshake you're supposed to give at the end to prevent that from going out of control and any tragic errors.\n\n----------------------------------------\n\n> Emielle Potgieter: What is parallel earth's biggest problem, then?\n> \n> [...]\n> \n> Eliezer Yudkowsky: I'd assume that Artificial General Intelligence is being seen by the Senior Very Serious People as a big problem, given the degree to which nobody ever talked about it, how relatively slow computing progress was compared to here, and how my general education just happened to prepare me to make a ton of correct inferences about it as soon as anybody mentioned the possibility to me. They claim to you it's about hypothetical aliens and economic dysfunction scenarios, but boy howdy do you get a lot of Orthogonality and Goodhart's Curse in the water supply.\n\n----------------------------------------\n\n> Stācia Gāel: Why did you come here?\n> \n> Jean-Baptiste Clemens: @Stācia Gāel   Everyone on parallel Earth was attempting to meet for lunch in the absence of communication and Eliezer was wrong about the Schelling point.\n> \n> [...]\n> \n> Eliezer Yudkowsky: No clue, then or ever.\n\n---",
      "wordCount": 18225
    },
    "tags": [
      {
        "_id": "EmaCLRKb4baBFq4ra",
        "name": "Dath Ilan",
        "slug": "dath-ilan"
      },
      {
        "_id": "YgizoZqa7LEb3LEJn",
        "name": "AMA",
        "slug": "ama"
      },
      {
        "_id": "chuP2QqQycjD8qakL",
        "name": "Coordination / Cooperation",
        "slug": "coordination-cooperation"
      },
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "EL4HNa92Z95FKL9R2",
    "title": "A Semitechnical Introductory Dialogue on Solomonoff Induction",
    "slug": "a-semitechnical-introductory-dialogue-on-solomonoff-1",
    "url": null,
    "baseScore": 144,
    "voteCount": 59,
    "viewCount": null,
    "commentCount": 32,
    "createdAt": null,
    "postedAt": "2021-03-04T17:27:35.591Z",
    "contents": {
      "markdown": "(*Originally posted in December 2015: A dialogue between Ashley, a computer scientist who's never heard of* [*Solomonoff's theory of inductive inference*](https://arbital.com/p/solomonoff_induction/)*, and Blaine, who thinks it is the best thing since sliced bread.*)\n\n* * *\n\n### i.  Unbounded analysis\n\n**ASHLEY:**  Good evening, Msr. Blaine.\n\n**BLAINE:**  Good evening, Msr. Ashley.\n\n**ASHLEY:**  I've heard there's this thing called \"Solomonoff's theory of inductive inference\".\n\n**BLAINE:**  The rumors have spread, then.\n\n**ASHLEY:**  Yeah, so, what the heck is that about?\n\n**BLAINE:**  Invented in the 1960s by the mathematician Ray Solomonoff, the key idea in Solomonoff induction is to do sequence prediction by using Bayesian updating on a prior composed of a mixture of all computable probability distributions—\n\n**ASHLEY:**  Wait. Back up a lot. Before you try to explain what Solomonoff induction *is*, I'd like you to try to tell me what it *does*, or why people study it in the first place. I find that helps me organize my listening. Right now I don't even know why I should be interested in this.\n\n**BLAINE:**  Um, okay. Let me think for a second...\n\n**ASHLEY:**  Also, while I can imagine things that \"sequence prediction\" might mean, I haven't yet encountered it in a technical context, so you'd better go a bit further back and start more at the beginning. I do know what \"computable\" means and what a \"probability distribution\" is, and I remember the formula for [Bayes's Rule](https://arbital.com/p/bayes_rule/) although it's been a while.\n\n**BLAINE:**  Okay. So... one way of framing the usual reason why people study this general field in the first place, is that sometimes, by studying certain idealized mathematical questions, we can gain valuable intuitions about epistemology. That's, uh, the field that studies how to reason about factual questions, how to build a map of reality that reflects the territory—\n\n**ASHLEY:**  I have some idea what 'epistemology' is, yes. But I think you might need to start even further back, maybe with some sort of concrete example or something.\n\n**BLAINE:**  Okay. Um. So one anecdote that I sometimes use to frame the value of computer science to the study of epistemology is Edgar Allen Poe's argument in 1833 that chess was uncomputable.\n\n**ASHLEY:**  That doesn't sound like a thing that actually happened.\n\n**BLAINE:**  I know, but it totally *did* happen and not in a metaphorical sense either! Edgar Allen Poe wrote an [essay](http://www.eapoe.org/works/essays/maelzel.htm) explaining why no automaton would ever be able to play chess, and he specifically mentioned \"Mr. Babbage's computing engine\" as an example.\n\nYou see, in the nineteenth century, there was for a time this sensation known as the Mechanical Turk—supposedly a machine, an automaton, that could play chess. At the grandmaster level, no less.\n\nNow today, when we're accustomed to the idea that it takes a reasonably powerful computer to do that, we can know *immediately* that the Mechanical Turk must have been a fraud and that there must have been a concealed operator inside—a person with dwarfism, as it turned out. *Today* we know that this sort of thing is *hard* to build into a machine. But in the 19th century, even that much wasn't known.\n\nSo when Edgar Allen Poe, who besides being an author was also an accomplished magician, set out to write an essay about the Mechanical Turk, he spent the *second* half of the essay dissecting what was known about the Turk's appearance to (correctly) figure out where the human operator was hiding. But Poe spent the first half of the essay arguing that no automaton—nothing like Mr. Babbage's computing engine—could possibly play chess, which was how he knew *a priori* that the Turk had a concealed human operator.\n\n**ASHLEY:**  And what was Poe's argument?\n\n**BLAINE:**  Poe observed that in an algebraical problem, each step followed from the previous step of necessity, which was why the steps in solving an algebraical problem could be represented by the deterministic motions of gears in something like Mr. Babbage's computing engine. But in a chess problem, Poe said, there are many possible chess moves, and no move follows with necessity from the position of the board; and even if you did select one move, the opponent's move would not follow with necessity, so you couldn't represent it with the determined motion of automatic gears. Therefore, Poe said, whatever was operating the Mechanical Turk must have the nature of Cartesian mind, rather than the nature of deterministic matter, and this was knowable *a priori*. And then he started figuring out where the required operator was hiding.\n\n**ASHLEY:**  That's some amazingly impressive reasoning for being completely wrong.\n\n**BLAINE:**  I know! Isn't it great?\n\n**ASHLEY:**  I mean, that sounds like Poe correctly identified the *hard* part of playing computer chess, the branching factor of moves and countermoves, which is the reason why no *simple* machine could do it. And he just didn't realize that a deterministic machine could deterministically check many possible moves in order to figure out the game tree. So close, and yet so far.\n\n**BLAINE:**  More than a century later, in 1950, Claude Shannon published the first paper ever written on computer chess. And in passing, Shannon gave the formula for playing perfect chess if you had unlimited computing power, the algorithm you'd use to extrapolate the entire game tree. We could say that Shannon gave a short program that would solve chess if you ran it on a hypercomputer, where a hypercomputer is an ideal computer that can run any finite computation immediately. And then Shannon passed on to talking about the problem of locally guessing how good a board position was, so that you could play chess using only a *small* search.\n\nI say all this to make a point about the value of knowing how to solve problems using hypercomputers, even though hypercomputers don't exist. Yes, there's often a *huge* gap between the unbounded solution and the practical solution. It wasn't until 1997, forty-seven years after Shannon's paper giving the unbounded solution, that Deep Blue actually won the world chess championship—\n\n**ASHLEY:**  And that wasn't just a question of faster computing hardware running Shannon's ideal search algorithm. There were a lot of new insights along the way, most notably the alpha-beta pruning algorithm and a lot of improvements in positional evaluation.\n\n**BLAINE:**  Right!\n\nBut I think some people overreact to that forty-seven year gap, and act like it's *worthless* to have an unbounded understanding of a computer program, just because you might still be forty-seven years away from a practical solution. But if you don't even have a solution that would run on a hypercomputer, you're Poe in 1833, not Shannon in 1950.\n\nThe reason I tell the anecdote about Poe is to illustrate that Poe was *confused* about computer chess in a way that Shannon was not. When we don't know how to solve a problem even given infinite computing power, the very work we are trying to do is in some sense murky to us. When we can state code that would solve the problem given a hypercomputer, we have become *less* confused. Once we have the unbounded solution we understand, in some basic sense, *the kind of work we are trying to perform,* and then we can try to figure out how to do it efficiently.\n\n**ASHLEY:**  Which may well require new insights into the structure of the problem, or even a conceptual revolution in how we imagine the work we're trying to do.\n\n**BLAINE:**  Yes, but the point is that you can't even get started on that if you're arguing about how playing chess has the nature of Cartesian mind rather than matter. At that point you're not 50 years away from winning the chess championship, you're 150 years away, because it took an extra 100 years to move humanity's understanding to the point where Claude Shannon could trivially see how to play perfect chess using a large-enough computer. I'm not trying to exalt the unbounded solution by denigrating the work required to get a bounded solution. I'm not saying that when we have an unbounded solution we're practically there and the rest is a matter of mere lowly efficiency. I'm trying to compare having the unbounded solution to the horrific confusion of *not understanding what we're trying to do.*\n\n**ASHLEY:**  Okay. I think I understand why, on your view, it's important to know how to solve problems using infinitely fast computers, or hypercomputers as you call them. When we can say how to answer a question using infinite computing power, that means we crisply understand the question itself, in some sense; while if we can't figure out how to solve a problem using unbounded computing power, that means we're *confused* about the problem, in some sense. I mean, anyone who's ever tried to teach the more doomed sort of undergraduate to write code knows what it means to be confused about what it takes to compute something.\n\n**BLAINE:**  Right.\n\n**ASHLEY:**  So what does this have to do with \"Solomonoff induction\"?\n\n**BLAINE:**  Ah! Well, suppose I asked you how to do epistemology using infinite computing power?\n\n**ASHLEY:**  My good fellow, I would at once reply, \"Beep. Whirr. Problem 'do epistemology' not crisply specified.\" At this stage of affairs, I do not think this reply indicates any fundamental confusion on my part; rather I think it is you who must be clearer.\n\n**BLAINE:**  Given unbounded computing power, how would you reason in order to construct an accurate map of reality?\n\n**ASHLEY:**  That still strikes me as rather underspecified.\n\n**BLAINE:**  Perhaps. But even there I would suggest that it's a mark of intellectual progress to be able to take vague and underspecified ideas like 'do good epistemology' and turn them *into* crisply specified problems. Imagine that I went up to my friend Cecil, and said, \"How would you do good epistemology given unlimited computing power and a short Python program?\" and Cecil at once came back with an answer—a good and reasonable answer, once it was explained. Cecil would probably know something quite interesting that you do not presently know.\n\n**ASHLEY:**  I confess to being rather skeptical of this hypothetical. But if that actually happened—if I agreed, to my own satisfaction, that someone had stated a short Python program that would 'do good epistemology' if run on an unboundedly fast computer—then I agree that I'd probably have learned something *quite interesting* about epistemology.\n\n**BLAINE:**  What Cecil knows about, in this hypothetical, is Solomonoff induction. In the same way that Claude Shannon answered \"Given infinite computing power, how would you play perfect chess?\", Ray Solomonoff answered \"Given infinite computing power, how would you perfectly find the best hypothesis that fits the facts?\"\n\n**ASHLEY:**  Suddenly, I find myself strongly suspicious of whatever you are about to say to me.\n\n**BLAINE:**  That's understandable.\n\n**ASHLEY:**  In particular, I'll ask at once whether \"Solomonoff induction\" assumes that our hypotheses are being given to us on a silver platter along with the exact data we're supposed to explain, or whether the algorithm is organizing its own data from a big messy situation and inventing good hypotheses from scratch.\n\n**BLAINE:**  Great question! It's the second one.\n\n**ASHLEY:**  Really? Okay, now I have to ask whether Solomonoff induction is a recognized concept in good standing in the field of academic computer science, because that does not sound like something modern-day computer science knows how to do.\n\n**BLAINE:**  I wouldn't say it's a widely known concept, but it's one that's in good academic standing. The method isn't used in modern machine learning because it requires an infinitely fast computer and isn't easily approximated the way that chess is.\n\n**ASHLEY:**  This really sounds very suspicious. Last time I checked, we hadn't *begun* to formalize the creation of good new hypotheses from scratch. I've heard about claims to have 'automated' the work that, say, Newton did in inventing classical mechanics, and I've found them all to be incredibly dubious. Which is to say, they were rigged demos and lies.\n\n**BLAINE:**  I know, but—\n\n**ASHLEY:**  And then I'm even more suspicious of a claim that someone's algorithm would solve this problem if only they had infinite computing power. Having some researcher claim that their Good-Old-Fashioned AI semantic network *would* be intelligent if run on a computer so large that, conveniently, nobody can ever test their theory, is not going to persuade me.\n\n**BLAINE:**  Do I really strike you as that much of a charlatan? What have I ever done to you, that you would expect me to try pulling a scam like that?\n\n**ASHLEY:**  That's fair. I shouldn't accuse you of planning that scam when I haven't seen you say it. But I'm pretty sure the problem of \"coming up with good new hypotheses in a world full of messy data\" is [AI-complete](https://en.wikipedia.org/wiki/AI-complete). And even Mentif-\n\n**BLAINE:**  Do not say the name, or he will appear!\n\n**ASHLEY:**  Sorry. Even the legendary first and greatest of all AI crackpots, He-Who-Googles-His-Name, could assert that his algorithms would be all-powerful on a computer large enough to make his claim unfalsifiable. So what?\n\n**BLAINE:**  That's a very sensible reply and this, again, is exactly the kind of mental state that reflects a problem that is *confusing* rather than just hard to implement. It's the sort of confusion Poe might feel in 1833, or close to it. In other words, it's just the sort of conceptual issue we *would* have solved at the point where we could state a short program that could run on a hypercomputer. Which Ray Solomonoff did in 1964.\n\n**ASHLEY:**  Okay, let's hear about this supposed general solution to epistemology.\n\n### ii.  Sequences\n\n**BLAINE:**  First, try to solve the following puzzle. 1, 3, 4, 7, 11, 18, 29...?\n\n**ASHLEY:**  Let me look at those for a moment... 47.\n\n**BLAINE:**  Congratulations on engaging in, as we snooty types would call it, 'sequence prediction'.\n\n**ASHLEY:**  I'm following you so far.\n\n**BLAINE:**  The smarter you are, the more easily you can find the hidden patterns in sequences and predict them successfully. You had to notice the resemblance to the Fibonacci rule to guess the next number. Someone who didn't already know about Fibonacci, or who was worse at mathematical thinking, would have taken longer to understand the sequence or maybe never learned to predict it at all.\n\n**ASHLEY:**  Still with you.\n\n**BLAINE:**  It's not a sequence of *numbers* per se... but can you see how the question, \"The sun has risen on the last million days. What is the probability that it rises tomorrow?\" could be viewed as a kind of sequence prediction problem?\n\n**ASHLEY:**  Only if some programmer neatly parses up the world into a series of \"Did the Sun rise on day X starting in 4.5 billion BCE, 0 means no and 1 means yes? 1, 1, 1, 1, 1...\" and so on. Which is exactly the sort of shenanigan that I see as cheating. In the real world, you go outside and see a brilliant ball of gold touching the horizon, not a giant \"1\".\n\n**BLAINE:**  Suppose I have a robot running around with a webcam showing it a \\\\(1920\\times1080\\\\) pixel field that refreshes 60 times a second with 32-bit colors. I could view that as a giant sequence and ask the robot to predict what it will see happen when it rolls out to watch a sunrise the next day.\n\n**ASHLEY:**  I can't help but notice that the 'sequence' of webcam frames is absolutely enormous, like, the sequence is made up of 66-megabit 'numbers' appearing 3600 times per minute... oh, right, computers much bigger than the universe. And now you're smiling evilly, so I guess that's the point. I also notice that the sequence is no longer deterministically predictable, that it is no longer a purely mathematical object, and that the sequence of webcam frames observed will depend on the robot's choices. This makes me feel a bit shaky about the analogy to predicting the mathematical sequence 1, 1, 2, 3, 5.\n\n**BLAINE:**  I'll try to address those points in order. First, Solomonoff induction is about assigning *probabilities* to the next item in the sequence. I mean, if I showed you a box that said 1, 1, 2, 3, 5, 8 you would not be absolutely certain that the next item would be 13. There could be some more complicated rule that just looked Fibonacci-ish but then diverged. You might guess with 90% probability but not 100% probability, or something like that.\n\n**ASHLEY:**  This has stopped feeling to me like math.\n\n**BLAINE:**  There is a *large* branch of math, to say nothing of computer science, that deals in probabilities and statistical prediction. We are going to be describing absolutely lawful and deterministic ways of assigning probabilities after seeing 1, 3, 4, 7, 11, 18.\n\n**ASHLEY:**  Okay, but if you're later going to tell me that this lawful probabilistic prediction rule underlies a generally intelligent reasoner, I'm already skeptical.\n\nNo matter how large a computer it's run on, I find it hard to imagine that some simple set of rules for assigning probabilities is going to encompass truly and generally intelligent answers about sequence prediction, like [Terence Tao](https://en.wikipedia.org/wiki/Terence_Tao) would give after looking at the sequence for a while. We just have no idea how Terence Tao works, so we can't duplicate his abilities in a formal rule, no matter how much computing power that rule gets... you're smiling evilly again. I'll be *quite* interested if that evil smile turns out to be justified.\n\n**BLAINE:**  Indeed.\n\n**ASHLEY:**  I also find it hard to imagine that this deterministic mathematical rule for assigning probabilities would notice if a box was outputting an encoded version of \"To be or not to be\" from Shakespeare by mapping A to Z onto 1 to 26, which I would notice eventually though not immediately upon seeing 20, 15, 2, 5, 15, 18... And you're *still* smiling evilly.\n\n**BLAINE:**  Indeed. That is *exactly* what Solomonoff induction does. Furthermore, we have theorems establishing that Solomonoff induction can do it way better than you or Terence Tao.\n\n**ASHLEY:**  A *theorem* proves this. As in a necessary mathematical truth. Even though we have no idea how Terence Tao works empirically... and there's evil smile number four. Okay. I am very skeptical, but willing to be convinced.\n\n**BLAINE:**  So if you actually did have a hypercomputer, you could cheat, right? And Solomonoff induction is the most ridiculously cheating cheat in the history of cheating.\n\n**ASHLEY:**  Go on.\n\n**BLAINE:**  We just run all possible computer programs to see which are the simplest computer programs that best predict the data seen so far, and use those programs to predict what comes next. This mixture contains, among other things, an exact copy of Terence Tao, thereby allowing us to prove theorems about their relative performance.\n\n**ASHLEY:**  Is this an actual reputable math thing? I mean really?\n\n**BLAINE:**  I'll deliver the formalization later, but you did ask me to first state the point of it all. The point of Solomonoff induction is that it gives us a gold-standard ideal for sequence prediction, and this gold-standard prediction only errs by a bounded amount, over infinite time, relative to the best computable sequence predictor. We can also see it as formalizing the intuitive idea that was expressed by William Ockham a few centuries earlier that simpler theories are more likely to be correct, and as telling us that 'simplicity' should be measured in algorithmic complexity, which is the size of a computer program required to output a hypothesis's predictions.\n\n**ASHLEY:**  I think I would have to read more on this subject to actually follow that. What I'm hearing is that Solomonoff induction is a reputable idea that is important because it gives us a kind of ideal for sequence prediction. This ideal also has something to do with Occam's Razor, and stakes a claim that the simplest theory is the one that can be represented by the shortest computer program. You identify this with \"doing good epistemology\".\n\n**BLAINE:**  Yes, those are legitimate takeaways. Another way of looking at it is that Solomonoff induction is an ideal but uncomputable answer to the question \"What should our priors be?\", which is left open by understanding [Bayesian updating](https://arbital.com/p/bayes_rule/).\n\n**ASHLEY:**  Can you say how Solomonoff induction answers the question of, say, the prior probability that Canada is planning to invade the United States? I once saw a crackpot website that tried to invoke Bayesian probability about it, but only after setting the prior at 10% or something like that, I don't recall exactly. Does Solomonoff induction let me tell him that he's making a math error, instead of just calling him silly in an informal fashion?\n\n**BLAINE:**  If you're expecting to sit down with Leibniz and say, \"Gentlemen, let us calculate\" then you're setting your expectations too high. Solomonoff gives us an idea of how we *should* compute that quantity given unlimited computing power. It doesn't give us a firm recipe for how we can best approximate that ideal in real life using bounded computing power, or human brains. That's like expecting to play perfect chess after you read Shannon's 1950 paper. But knowing the ideal, we can extract some intuitive advice that might help our online crackpot if only he'd listen.\n\n**ASHLEY:**  But according to you, Solomonoff induction does say in principle what is the prior probability that Canada will invade the United States.\n\n**BLAINE:**  Yes, up to a choice of universal Turing machine.\n\n**ASHLEY: ** *(looking highly skeptical) * So I plug a universal Turing machine into the formalism, and in principle, I get out a uniquely determined probability that Canada invades the USA.\n\n**BLAINE:**  Exactly!\n\n**ASHLEY:**  Uh huh. Well, go on.\n\n**BLAINE:**  So, first, we have to transform this into a sequence prediction problem.\n\n**ASHLEY:**  Like a sequence of years in which Canada has and hasn't invaded the US, mostly zero except around 1812—\n\n**BLAINE:**  *No!* To get a good prediction about Canada we need much more data than that, and I don't mean a graph of Canadian GDP either. Imagine a sequence that contains all the sensory data you have ever received over your lifetime. Not just the hospital room that you saw when you opened your eyes right after your birth, but the darkness your brain received as input while you were still in your mother's womb. Every word you've ever heard. Every letter you've ever seen on a computer screen, not as ASCII letters but as the raw pattern of neural impulses that gets sent down from your retina.\n\n**ASHLEY:**  That seems like a lot of data and some of it is redundant, like there'll be lots of similar pixels for blue sky—\n\n**BLAINE:**  That data is what *you* got as an agent. If we want to translate the question of the prediction problem Ashley faces into theoretical terms, we should give the sequence predictor *all* the data that you had available, including all those repeating blue pixels of the sky. Who knows? Maybe there was a Canadian warplane somewhere in there, and you didn't notice.\n\n**ASHLEY:**  But it's impossible for my brain to remember all that data. If we neglect for the moment how the retina actually works and suppose that I'm seeing the same \\\\(1920 \\times1080\\\\) @60Hz feed the robot would, that's far more data than my brain can realistically learn per second.\n\n**BLAINE:**  So then Solomonoff induction can do better than you can, using its unlimited computing power and memory. That's fine.\n\n**ASHLEY:**  But what if you can do better by forgetting more?\n\n**BLAINE:**  If you have limited computing power, that makes sense. With unlimited computing power, that really shouldn't happen and that indeed is one of the lessons of Solomonoff induction. An unbounded Bayesian never expects to do worse by updating on another item of evidence—for one thing, you can always just do the same policy you would have used if you hadn't seen that evidence. That kind of lesson *is* one of the lessons that might not be intuitively obvious, but which you can feel more deeply by walking through the math of probability theory. With unlimited computing power, nothing goes wrong as a result of trying to process 4 gigabits per second; every extra bit just produces a better expected future prediction.\n\n**ASHLEY:**  Okay, so we start with literally all the data I have available. That's 4 gigabits per second if we imagine \\\\(1920\\times1080\\\\) frames of 32-bit pixels repeating 60 times per second. Though I remember hearing 100 megabits per second would be a better estimate of what the retina sends out, and that it's pared down to 1 megabit per second very quickly by further processing.\n\n**BLAINE:**  Right. We start with all of that data, going back to when you were born. Or maybe when your brain formed in the womb, though it shouldn't make much difference.\n\n**ASHLEY:**  I note that there are some things I know that don't come from my sensory inputs at all. Chimpanzees learn to be afraid of skulls and snakes much faster than they learn to be afraid of other arbitrary shapes. I was probably better at learning to walk in Earth gravity than I would have been at navigating in zero G. Those are heuristics I'm born with, based on how my brain was wired, which ultimately stems from my DNA specifying the way that proteins should fold to form neurons—not from any photons that entered my eyes later.\n\n**BLAINE:**  So, for purposes of following along with the argument, let's say that your DNA is analogous to the code of a computer program that makes predictions. What you're observing here is that humans have 750 megabytes of DNA, and even if most of that is junk and not all of what's left is specifying brain behavior, it still leaves a pretty large computer program that could have a lot of prior information programmed into it.\n\nLet's say that your brain, or rather, your infant pre-brain wiring algorithm, was effectively a 7.5 megabyte program—if it's actually 75 megabytes, that makes little difference to the argument. By exposing that 7.5 megabyte program to all the information coming in from your eyes, ears, nose, proprioceptive sensors telling you where your limbs were, and so on, your brain updated itself into forming the modern Ashley, whose hundred trillion synapses might be encoded by, say, one petabyte of information.\n\n**ASHLEY:**  The thought does occur to me that some environmental phenomena have effects on me that can't be interpreted as \"sensory information\" in any simple way, like the direct effect that alcohol has on my neurons, and how that feels to me from the inside. But it would be perverse to claim that this prevents you from trying to summarize all the information that the Ashley-agent receives into a single sequence, so I won't press the point.\n\n(**ELIEZER:  **(*whispering*)  *More on this topic later.*)\n\n**ASHLEY:**  Oh, and for completeness's sake, wouldn't there also be further information embedded in the laws of physics themselves? Like, the way my brain executes implicitly says something about the laws of physics in the universe I'm in.\n\n**BLAINE:**  Metaphorically speaking, our laws of physics would play the role of a particular choice of Universal Turing Machine, which has some effect on which computations count as \"simple\" inside the Solomonoff formula. But normally, the UTM should be very simple compared to the amount of data in the sequence we're trying to predict, just like the laws of physics are very simple compared to a human brain. In terms of [algorithmic complexity](https://arbital.com/p/Kolmogorov_complexity/), the laws of physics are very simple compared to watching a \\\\(1920\\times1080\\\\) @60Hz visual field for a day.\n\n**ASHLEY:**  Part of my mind feels like the laws of physics are quite complicated compared to going outside and watching a sunset. Like, I realize that's false, but I'm not sure how to say out loud exactly why it's false...\n\n**BLAINE:**  Because the algorithmic complexity of a system isn't measured by how long a human has to go to college to understand it, it's measured by the size of the computer program required to generate it. The language of physics is differential equations, and it turns out that this is something difficult to beat into some human brains, but differential equations are simple to program into a simple Turing Machine.\n\n**ASHLEY:**  Right, like, the laws of physics actually have much fewer details to them than, say, human nature. At least on the Standard Model of Physics. I mean, in principle there could be another decillion undiscovered particle families out there.\n\n**BLAINE:**  The concept of \"algorithmic complexity\" isn't about seeing something with lots of gears and details, it's about the size of computer program required to compress all those details. The [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set) looks very complicated visually, you can keep zooming in using more and more detail, but there's a very simple rule that generates it, so we say the algorithmic complexity is very low.\n\n**ASHLEY:**  All the visual information I've seen is something that happens *within* the physical universe, so how can it be more complicated than the universe? I mean, I have a sense on some level that this shouldn't be a problem, but I don't know why it's not a problem.\n\n**BLAINE:**  That's because particular parts of the universe can have much higher algorithmic complexity than the entire universe!\n\nConsider a library that contains all possible books. It's very easy to write a computer program that generates all possible books. So any *particular* book in the library contains much more algorithmic information than the *entire* library; it contains the information required to say 'look at this particular book here'.\n\nIf pi is normal, then somewhere in its digits is a copy of Shakespeare's *Hamlet*—but the number saying which particular digit of pi to start looking at, will be just about exactly as large as *Hamlet* itself. The copy of Shakespeare's *Hamlet* that exists in the decimal expansion of pi is more complex than pi itself.\n\nIf you zoomed way in and restricted your vision to a particular part of the Mandelbrot set, what you saw might be much more *algorithmically* complex than the entire Mandelbrot set, because the specification has to say where in the Mandelbrot set you are.\n\nSimilarly, the world Earth is much more algorithmically complex than the laws of physics. Likewise, the visual field you see over the course of a second can easily be far more algorithmically complex than the laws of physics.\n\n**ASHLEY:**  Okay, I think I get that. And similarly, even though the ways that proteins fold up are very complicated, in principle we could get all that info using just the simple fundamental laws of physics plus the relatively simple DNA code for the protein. There are all sorts of obvious caveats about epigenetics and so on, but those caveats aren't likely to change the numbers by a whole order of magnitude.\n\n**BLAINE:**  Right!\n\n**ASHLEY:**  So the laws of physics are, like, a few kilobytes, and my brain has say 75 megabytes of innate wiring instructions. And then I get to see a lot more information than that over my lifetime, like a megabit per second after my initial visual system finishes preprocessing it, and then most of that is forgotten. Uh... what does that have to do with Solomonoff induction again?\n\n**BLAINE:**  Solomonoff induction quickly catches up to any single computer program at sequence prediction, even if the original program is very large and contains a lot of prior information about the environment. If a program is 75 megabytes long, it can only predict 75 megabytes worth of data better than the Solomonoff inductor before the Solomonoff inductor catches up to it.\n\nThat doesn't mean that a Solomonoff inductor knows everything a baby does after the first second of exposure to a webcam feed, but it does mean that after the first second, the Solomonoff inductor is already no more surprised than a baby by the vast majority of pixels in the next frame.\n\nEvery time the Solomonoff inductor assigns half as much probability as the baby to the next pixel it sees, that's one bit spent permanently out of the 75 megabytes of error that can happen before the Solomonoff inductor catches up to the baby.\n\nThat your brain is written in the laws of physics also has some implicit correlation with the environment, but that's like saying that a program is written in the same programming language as the environment. The language can contribute something to the power of the program, and the environment being written in the same programming language can be a kind of prior knowledge. But if Solomonoff induction starts from a standard Universal Turing Machine as its language, that doesn't contribute any more bits of lifetime error than the complexity of that programming language in the UTM.\n\n**ASHLEY:**  Let me jump back a couple of steps and return to the notion of my brain wiring itself up in response to environmental information. I'd expect an important part of that process was my brain learning to *control* the environment, not just passively observing it. Like, it mattered to my brain's wiring algorithm that my brain saw the room shift in a certain way when it sent out signals telling my eyes to move.\n\n**BLAINE:**  Indeed. But talking about the sequential *control* problem is more complicated math. [AIXI](https://arbital.com/p/AIXI/) is the ideal agent that uses Solomonoff induction as its epistemology and expected reward as its decision theory. That introduces extra complexity, so it makes sense to talk about just Solomonoff induction first. We can talk about AIXI later. So imagine for the moment that we were *just* looking at your sensory data, and trying to predict what would come next in that.\n\n**ASHLEY:**  Wouldn't it make more sense to look at the brain's inputs *and* outputs, if we wanted to predict the next input? Not just look at the series of previous inputs?\n\n**BLAINE:**  It'd make the problem easier for a Solomonoff inductor to solve, sure; but it also makes the problem more complicated. Let's talk instead about what would happen if you took the complete sensory record of your life, gave it to an ideally smart agent, and asked the agent to predict what you would see next. Maybe the agent could do an even better job of prediction if we also told it about your brain's outputs, but I don't think that subtracting the outputs would leave it helpless to see patterns in the inputs.\n\n**ASHLEY:**  It sounds like a pretty hard problem to me, maybe even an unsolvable one. I'm thinking of the distinction in computer science between needing to learn from non-chosen data, versus learning when you can choose particular queries. Learning can be much faster in the second case.\n\n**BLAINE:**  In terms of what can be predicted *in principle* given the data, what facts are *actually reflected in it* that Solomonoff induction might uncover, we shouldn't imagine a human trying to analyze the data. We should imagine [an entire advanced civilization pondering it for years](https://www.lesswrong.com/lw/qk/that_alien_message/). If you look at it from that angle, then the alien civilization isn't going to balk at the fact that it's looking at the answers to the queries that Ashley's brain chose, instead of the answers to the queries it chose itself.\n\nLike, if the Ashley had already read Shakespeare's *Hamlet*—if the image of those pages had already crossed the sensory stream—and then the Ashley saw a mysterious box outputting 20, 15, 2, 5, 15, 18, I think somebody eavesdropping on that sensory data would be equally able to guess that this was encoding 'tobeor' and guess that the next thing the Ashley saw might be the box outputting 14. You wouldn't even need an entire alien civilization of superintelligent cryptographers to guess that. And it definitely wouldn't be a killer problem that Ashley was controlling the eyeball's saccades, even if you could learn even faster by controlling the eyeball yourself.\n\nSo far as the computer-science distinction goes, Ashley's eyeball *is* being controlled to make intelligent queries and seek out useful information; it's just Ashley controlling the eyeball instead of you—that eyeball is not a query-oracle answering *random* questions.\n\n**ASHLEY:**  Okay, I think this example is helping my understanding of what we're doing here. In the case above, the next item in the Ashley-sequence wouldn't actually be 14. It would be this huge \\\\(1920 \\times 1080\\\\) visual field that showed the box flashing a little picture of '14'.\n\n**BLAINE:**  Sure. Otherwise it would be a rigged demo, as you say.\n\n**ASHLEY:**  I think I'm confused about the idea of *predicting* the visual field. It seems to me that what with all the dust specks in my visual field, and maybe my deciding to tilt my head using motor instructions that won't appear in the sequence, there's no way to *exactly* predict the 66-megabit integer representing the next visual frame. So it must be doing something other than the equivalent of guessing \"14\" in a simpler sequence, but I'm not sure what.\n\n**BLAINE:**  Indeed, there'd be some element of thermodynamic and quantum randomness preventing that exact prediction even in principle. So instead of predicting one particular next frame, we put a probability distribution on it.\n\n**ASHLEY:**  A probability distribution over possible 66-megabit frames? Like, a table with \\\\(2^{66,000,000}\\\\) entries, summing to 1?\n\n**BLAINE:**  Sure. \\\\(2^{32 \\times 1920 \\times 1080}\\\\) isn't a large number when you have unlimited computing power. As Martin Gardner once observed, \"Most finite numbers are very much larger.\" Like I said, Solomonoff induction is an epistemic ideal that requires an unreasonably large amount of computing power.\n\n**ASHLEY:**  I don't deny that big computations can sometimes help us understand little ones. But at the point when we're talking about probability distributions that large, I have some trouble holding onto what the probability distribution is supposed to *mean*.\n\n**BLAINE:**  Really? Just imagine a probability distribution over \\\\(N\\\\) possibilities, then let \\\\(N\\\\) go to \\\\(2^{66,000,000}\\\\). If we were talking about a letter ranging from A to Z, then putting 100 times as much probability mass on (X, Y, Z) as on the rest of the alphabet, would say that although you didn't know *exactly* what letter would happen, you expected it would be toward the end of the alphabet. You would have used 26 probabilities, summing to 1, to precisely state that prediction.\n\nIn Solomonoff induction, since we have unlimited computing power, we express our uncertainty about a \\\\(1920 \\times 1080\\\\) video frame the same way. All the various pixel fields you could see if your eye jumped to a plausible place, saw a plausible number of dust specks, and saw the box flash something that visually encoded '14', would have high probability. Pixel fields where the box vanished and was replaced with a glow-in-the-dark unicorn would have very low, though not zero, probability.\n\n**ASHLEY:**  Can we really get away with viewing things that way?\n\n**BLAINE:**  If we could not make identifications like these *in principle*, there would be no principled way in which we could say that you had ever *expected to see something happen*—no way to say that one visual field your eyes saw had higher probability than any other sensory experience. We couldn't justify science; we couldn't say that, having performed Galileo's experiment by rolling an inclined cylinder down a plane, Galileo's theory was thereby to some degree supported by having assigned *a high relative probability* to the only actual observations our eyes ever report.\n\n**ASHLEY:**  I feel a little unsure of that jump, but I suppose I can go along with that for now. Then the question of \"What probability does Solomonoff induction assign to Canada invading?\" is to be identified, in principle, with the question \"Given my past life experiences and all the visual information that's entered my eyes, what is the relative probability of seeing visual information that encodes Google News with the headline 'CANADA INVADES USA' at some point during the next 300 million seconds?\"\n\n**BLAINE:**  Right!\n\n**ASHLEY:**  And Solomonoff induction has an in-principle way of assigning this a relatively low probability, which that online crackpot could do well to learn from as a matter of principle, even if he couldn't *begin* to carry out the exact calculations that involve assigning probabilities to exponentially vast tables.\n\n**BLAINE:**  Precisely!\n\n**ASHLEY:**  Fairness requires that I congratulate you on having come further in formalizing 'do good epistemology' as a sequence prediction problem than I previously thought you might.\n\nI mean, you haven't satisfied me yet, but I wasn't expecting you to get even this far.\n\n### iii.  Hypotheses\n\n**BLAINE:**  Next, we consider how to represent a *hypothesis* inside this formalism.\n\n**ASHLEY:**  Hmm. You said something earlier about updating on a probabilistic mixture of computer programs, which leads me to suspect that in this formalism, a hypothesis or *way the world can be* is a computer program that outputs a sequence of integers.\n\n**BLAINE:**  There's indeed a version of Solomonoff induction that works like that. But I prefer the version where a hypothesis assigns *probabilities* to sequences. Like, if the hypothesis is that the world is a fair coin, then we shouldn't try to make that hypothesis predict \"heads—tails—tails—tails—heads\" but should let it just assign a \\\\(1/32\\\\) prior probability to the sequence **HTTTH**.\n\n**ASHLEY:**  I can see that for coins, but I feel a bit iffier on what this means as a statement *about the real world*.\n\n**BLAINE:**  A single hypothesis inside the Solomonoff mixture would be a computer program that took in a series of video frames, and assigned a probability to each possible next video frame. Or for greater simplicity and elegance, imagine a program that took in a sequence of bits, ones and zeroes, and output a rational number for the probability of the next bit being '1'. We can readily go back and forth between a program like that, and a probability distribution over sequences.\n\nLike, if you can answer all of the questions, \"What's the probability that the coin comes up heads on the first flip?\", \"What's the probability of the coin coming up heads on the second flip, if it came up heads on the first flip?\", and \"What's the probability that the coin comes up heads on the second flip, if it came up tails on the first flip?\" then we can turn that into a probability distribution over sequences of two coinflips. Analogously, if we have a program that outputs the probability of the next bit, conditioned on a finite number of previous bits taken as input, that program corresponds to a probability distribution over infinite sequences of bits.\n\n\\\\\\[\\mathbb{P}_{prog}(bits_{1 \\dots N}) = \\prod_{i=1}^{N} InterpretProb(prog(bits_{1 \\dots i-1}), bits_i)\\\\\\]\\\\\\[InterpretProb(prog(x), y) = \\left\\{ \\begin{array}{ll} InterpretFrac(prog(x)) & \\text{if } y = 1 \\ 1-InterpretFrac(prog(x)) & \\text{if } y = 0 \\ 0 & \\text{if $prog(x)$ does not halt} \\end{array} \\right\\}\\\\\\]\n\n**ASHLEY:**  I think I followed along with that in theory, though it's not a type of math I'm used to (yet). So then in what sense is a program that assigns probabilities to sequences, a way the world could be—a hypothesis about the world?\n\n**BLAINE:**  Well, I mean, for one thing, we can see the infant Ashley as a program with 75 megabytes of information about how to wire up its brain in response to sense data, that sees a bunch of sense data, and then experiences some degree of relative surprise. Like in the baby-looking-paradigm experiments where you show a baby an object disappearing behind a screen, and the baby looks longer at those cases, and so we suspect that babies have a concept of object permanence.\n\n**ASHLEY:**  That sounds like a program that's a way Ashley could be, not a program that's a way the world could be.\n\n**BLAINE:**  Those indeed are dual perspectives on the meaning of Solomonoff induction. Maybe we can shed some light on this by considering a simpler induction rule, Laplace's Rule of Succession, invented by the Reverend Thomas Bayes in the 1750s, and named after Pierre-Simon Laplace, the inventor of Bayesian reasoning.\n\n**ASHLEY:**  Pardon me?\n\n**BLAINE:**  Suppose you have a biased coin with an unknown bias, and every possible bias between \\\\(0\\\\) and \\\\(1\\\\) is equally probable.\n\n**ASHLEY:**  Okay. Though in the real world, it's quite likely that an unknown frequency is exactly \\\\(0\\\\), \\\\(1\\\\), or \\\\(1/2\\\\). If you assign equal probability density to every part of the real number field between \\\\(0\\\\) and \\\\(1\\\\), the probability of \\\\(1\\\\) is \\\\(0\\\\). Indeed, the probability of all rational numbers put together is zero.\n\n**BLAINE:**  The original problem considered by Thomas Bayes was about an ideal billiard ball bouncing back and forth on an ideal billiard table many times and eventually slowing to a halt; and then bouncing other billiards to see if they halted to the left or the right of the first billiard. You can see why, in first considering the simplest form of this problem without any complications, we might consider every position of the first billiard to be equally probable.\n\n**ASHLEY:**  Sure. Though I note with pointless pedantry that if the billiard was really an ideal rolling sphere and the walls were perfectly reflective, it'd never halt in the first place.\n\n**BLAINE:**  Suppose we're told that, after rolling the original billiard ball and then 5 more billiard balls, one billiard ball was to the right of the original, an **R**. The other four were to the left of the original, or **L**s. Again, that's 1 **R** and 4 **L**s. Given only this data, what is the probability that the next billiard ball rolled will be on the left of the original, another **L**?\n\n**ASHLEY:**  Five sevenths.\n\n**BLAINE:**  Ah, you've heard this problem before?\n\n**ASHLEY:**  No, but it's obvious.\n\n**BLAINE:**  Uh... really?\n\n**ASHLEY:**  Combinatorics. Consider just the orderings of the balls, instead of their exact positions. Designate the original ball with the symbol **❚**, the next five balls as **LLLLR**, and the next ball to be rolled as **✚**. Given that the current ordering of these six balls is **LLLL❚R** and that all positions and spacings of the underlying balls are equally likely, after rolling the **✚**, there will be seven equally likely orderings **✚LLLL❚R**, **L✚LLL❚R**, **LL✚LL❚R**, and so on up to **LLLL❚✚R** and **LLLL❚R✚**. In five of those seven orderings, the **✚** is on the left of the **❚**. In general, if we see \\\\(M\\\\) of **L** and \\\\(N\\\\) of **R**, the probability of the next item being an **L** is \\\\((M + 1) / (M + N + 2)\\\\).\n\n**BLAINE:**  Gosh... Well, the much more complicated proof originally devised by Thomas Bayes starts by considering every position of the original ball to be equally likely *a priori*, the additional balls as providing evidence about that position, and then integrating over the posterior probabilities of the original ball's possible positions to arrive at the probability that the next ball lands on the left or right.\n\n**ASHLEY:**  Heh. And is all that extra work useful if you also happen to know a little combinatorics?\n\n**BLAINE:**  Well, it tells me exactly how my beliefs about the original ball change with each new piece of evidence—the new posterior probability function on the ball's position. Suppose I instead asked you something along the lines of, \"Given 4 **L** and 1 **R**, where do you think the original ball **✚** is most likely to be on the number line? How likely is it to be within 0.1 distance of there?\"\n\n**ASHLEY:**  That's fair; I don't see a combinatoric answer for the later part. You'd have to actually integrate over the density function \\\\(f^M(1-f)^N  \\mathrm{d}f\\\\).\n\n**BLAINE:**  Anyway, let's just take at face value that Laplace's Rule of Succession says that, after observing \\\\(M\\\\) 1s and \\\\(N\\\\) 0s, the probability of getting a 1 next is \\\\((M + 1) / (M + N + 2)\\\\).\n\n**ASHLEY:**  But of course.\n\n**BLAINE:**  We can consider Laplace's Rule as a short Python program that takes in a sequence of 1s and 0s, and spits out the probability that the next bit in the sequence will be 1. We can also consider it as a probability distribution over infinite sequences, like this:\n\n*   **0** : \\\\(1/2\\\\)\n*   **1** : \\\\(1/2\\\\)\n*   **00** : \\\\(1/2 * 2/3 = 1/3\\\\)\n*   **01** : \\\\(1/2 * 1/3 = 1/6\\\\)\n*   **000** : \\\\(1/2 * 2/3 * 3/4 = 1/4\\\\)\n*   **001** : \\\\(1/2 * 2/3 * 1/4 = 1/12\\\\)\n*   **010** : \\\\(1/2 * 1/3 * 1/2 = 1/12\\\\)\n\n... and so on.\n\nNow, we can view this as a rule someone might espouse for *predicting* coinflips, but also view it as corresponding to a particular class of possible worlds containing randomness.\n\nI mean, Laplace's Rule isn't the only rule you could use. Suppose I had a barrel containing ten white balls and ten green balls. If you already knew this about the barrel, then after seeing \\\\(M\\\\) white balls and \\\\(N\\\\) green balls, you'd predict the next ball being white with probability \\\\((10 - M) / (20 - M - N)\\\\).\n\nIf you use Laplace's Rule, that's like believing the world was like a billiards table with an original ball rolling to a stop at a random point and new balls ending up on the left or right. If you use \\\\((10 - M) / (20 - M - N)\\\\), that's like the hypothesis that there are ten green balls and ten white balls in a barrel. There isn't really a sharp border between rules we can use to predict the world, and rules for how the world behaves—\n\n**ASHLEY:**  Well, that sounds just plain wrong. The map is not the territory, don'cha know? If Solomonoff induction can't tell the difference between maps and territories, maybe it doesn't contain all epistemological goodness after all.\n\n**BLAINE:**  Maybe it'd be better to say that there's a dualism between good ways of computing predictions and being in actual worlds where that kind of predicting works well? Like, you could also see Laplace's Rule as implementing the rules for a world with randomness where the original billiard ball ends up in a random place, so that the first thing you see is equally likely to be 1 or 0. Then to ask what probably happens on round 2, we tell the world what happened on round 1 so that it can update what the background random events were.\n\n**ASHLEY:**  Mmmaybe.\n\n**BLAINE:**  If you go with the version where Solomonoff induction is over programs that just spit out a determined string of ones and zeroes, we could see those programs as corresponding to particular environments—ways the world *could be* that would produce our sensory input, the sequence.\n\nWe could jump ahead and consider the more sophisticated decision-problem that appears in [AIXI](https://arbital.com/p/AIXI/): an environment is a program that takes your motor outputs as its input, and then returns your sensory inputs as its output. Then we can see a program that produces Bayesian-updated predictions as corresponding to a hypothetical probabilistic environment that implies those updates, although they'll be conjugate systems rather than mirror images.\n\n**ASHLEY:**  Did you say something earlier about the deterministic and probabilistic versions of Solomonoff induction giving the same answers? Like, is it a distinction without a difference whether we ask about simple programs that reproduce the observed data versus simple programs that assign high probability to the data? I can't see why that should be true, especially since Turing machines don't include a randomness source.\n\n**BLAINE:**  I'm *told* the answers are the same but I confess I can't quite see why, unless there's some added assumption I'm missing. So let's talk about programs that assign probabilities for now, because I think that case is clearer.\n\n### iv.  Simplicity\n\n**BLAINE:**  The next key idea is to prefer *simple* programs that assign high probability to our observations so far.\n\n**ASHLEY:**  It seems like an obvious step, especially considering that you were already talking about \"simple programs\" and Occam's Razor a while back. Solomonoff induction is part of the Bayesian program of inference, right?\n\n**BLAINE:**  Indeed. Very much so.\n\n**ASHLEY:**  Okay, so let's talk about the program, or hypothesis, for \"This barrel has an unknown frequency of white and green balls\", versus the hypothesis \"This barrel has 10 white and 10 green balls\", versus the hypothesis, \"This barrel always puts out a green ball after a white ball and vice versa.\"\n\nLet's say we see a green ball, then a white ball, the sequence **GW**. The first hypothesis assigns this probability \\\\(1/2 * 1/3 = 1/6\\\\), the second hypothesis assigns this probability \\\\(10/20 * 9/19\\\\) or roughly \\\\(1/4\\\\), and the third hypothesis assigns probability \\\\(1/2 * 1\\\\).\n\nNow it seems to me that there's some important sense in which, even though Laplace's Rule assigned a lower probability to the data, it's significantly simpler than the second and third hypotheses and is the wiser answer. Does Solomonoff induction agree?\n\n**BLAINE:**  I think you might be taking into account some prior knowledge that isn't in the sequence itself, there. Like, things that alternate either **101010...** or **010101...** are *objectively* simple in the sense that a short computer program simulates them or assigns probabilities to them. It's just unlikely to be true about an actual barrel of white and green balls.\n\nIf **10** is literally the first sense data that you ever see, when you are a fresh new intelligence with only two bits to rub together, then \"The universe consists of alternating bits\" is no less reasonable than \"The universe produces bits with an unknown random frequency anywhere between \\\\(0\\\\) and \\\\(1\\\\).\"\n\n**ASHLEY:**  Conceded. But as I was going to say, we have three hypotheses that assigned \\\\(1/6\\\\), \\\\(\\sim1/4\\\\), and \\\\(1/2\\\\) to the observed data; but to know the posterior probabilities of these hypotheses we need to actually say how relatively likely they were *a priori*, so we can multiply by the odds ratio. Like, if the prior odds were \\\\(3:2:1\\\\), the posterior odds would be \\\\(3:2:1 * (2/12 : 3/12 : 6/12) = 3:2:1 * 2:3:6 = 6:6:6 = 1:1:1\\\\). Now, how would Solomonoff induction assign prior probabilities to those computer programs? Because I remember you saying, way back when, that you thought Solomonoff was the answer to \"How should Bayesians assign priors?\"\n\n**BLAINE:**  Well, how would you do it?\n\n**ASHLEY:**  I mean... yes, the simpler rules should be favored, but it seems to me that there's some deep questions as to the exact relative 'simplicity' of the rules \\\\((M + 1) / (M + N + 2)\\\\), or the rule \\\\((10 - M) / (20 - M - N)\\\\), or the rule \"alternate the bits\"...\n\n**BLAINE:**  Suppose I ask you to just make up some simple rule.\n\n**ASHLEY:**  Okay, if I just say the rule I think you're looking for, the rule would be, \"The complexity of a computer program is the number of bits needed to specify it to some arbitrary but reasonable choice of compiler or Universal Turing Machine, and the prior probability is \\\\(1/2\\\\) to the power of the number of bits. Since, e.g., there's 32 possible 5-bit programs, so each such program has probability \\\\(1/32\\\\). So if it takes 16 bits to specify Laplace's Rule of Succession, which seems a tad optimistic, then the prior probability would be \\\\(1/65536\\\\), which seems a tad pessimistic.\n\n**BLAINE:**  Now just apply that rule to the infinity of possible computer programs that assign probabilities to the observed data, update their posterior probabilities based on the probability they've assigned to the evidence so far, sum over all of them to get your next prediction, and we're done. And yes, that requires a [hypercomputer](https://arbital.com/p/hypercomputer/) that can solve the [halting problem](https://en.wikipedia.org/wiki/Halting_problem), but we're talking ideals here. Let \\\\(\\mathcal{P}\\\\) be the set of all programs and \\\\(s_1s_2\\ldots s_n\\\\) also written \\\\(s_{\\preceq n}\\\\) be the sense data so far, then\n\n\\\\\\[\\mathbb{Sol}(s_{\\preceq n}) := \\sum_{\\mathrm{prog} \\in \\mathcal{P}} 2^{-\\mathrm{length}(\\mathrm{prog})} \\cdot {\\prod_{j=1}^n \\mathop{InterpretProb}(\\mathrm{prog}(s_{\\preceq j-1}), s_j)}\\\\\\]\\\\\\[\\mathbb{P}(s_{n+1}=1\\mid s_{\\preceq n}) = \\frac{\\mathbb{Sol}(s_1s_2\\ldots s_n 1)}{\\mathbb{Sol}(s_1s_2\\ldots s_n 1) + \\mathbb{Sol}(s_1s_2\\ldots s_n 0)}.\\\\\\]\n\n**ASHLEY:**  Uh.\n\n**BLAINE:**  Yes?\n\n**ASHLEY:**  Um...\n\n**BLAINE:**  What is it?\n\n**ASHLEY:**  You invoked a countably infinite set, so I'm trying to figure out if my predicted probability for the next bit must necessarily converge to a limit as I consider increasingly large finite subsets in any order.\n\n**BLAINE:**  *(sighs)*  Of course you are.\n\n**ASHLEY:**  I think you might have left out some important caveats. Like, if I take the rule literally, then the program \"**0**\" has probability \\\\(1/2\\\\), the program \"**1**\" has probability \\\\(1/2\\\\), the program \"**01**\" has probability \\\\(1/4\\\\) and now the total probability is \\\\(1.25\\\\) which is *too much.* So I can't actually normalize it because the series sums to infinity. Now, this just means we need to, say, decide that the probability of a program having length 1 is \\\\(1/2\\\\), the probability of it having length 2 is \\\\(1/4\\\\), and so on out to infinity, but it's an added postulate.\n\n**BLAINE:**  The conventional method is to require a [prefix-free code](https://en.wikipedia.org/wiki/Prefix_code). If \"**0111**\" is a valid program then \"**01110**\" cannot be a valid program. With that constraint, assigning \"\\\\(1/2\\\\) to the power of the length of the code\", to all valid codes, will sum to less than \\\\(1\\\\); and we can normalize their relative probabilities to get the actual prior.\n\n**ASHLEY:**  Okay. And you're sure that it doesn't matter in what order we consider more and more programs as we approach the limit, because... no, I see it. Every program has positive probability mass, with the total set summing to \\\\(1\\\\), and Bayesian updating doesn't change that. So as I consider more and more programs, in any order, there are only so many large contributions that can be made from the mix—there's only so often that the final probability can change.\n\nLike, let's say there are at most 99 programs with probability 1% that assign probability \\\\(0\\\\) to the next bit being a 1; that's only 99 times the final answer can go down by as much as \\\\(0.01\\\\), as the limit is approached.\n\n**BLAINE:**  This idea generalizes, and is important. List all possible computer programs, in any order you like. Use any definition of *simplicity* that you like, so long as for any given amount of simplicity, there are only a finite number of computer programs that simple. As you go on carving off chunks of prior probability mass and assigning them to programs, it *must* be the case that as programs get more and complicated, their prior probability approaches zero!—though it's still positive for every finite program, because of [Cromwell's Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule).\n\nYou can't have more than 99 programs assigned 1% prior probability and still obey Cromwell's Rule, which means there must be some *most complex* program that is assigned 1% probability, which means every more complicated program must have less than 1% probability out to the end of the infinite list.\n\n**ASHLEY:**  Huh. I don't think I've ever heard that justification for Occam's Razor before. I think I like it. I mean, I've heard a lot of appeals to the empirical simplicity of the world, and so on, but this is the first time I've seen a *logical* proof that, in the limit, more complicated hypotheses *must* be less likely than simple ones.\n\n**BLAINE:**  Behold the awesomeness that is Solomonoff Induction!\n\n**ASHLEY:**  Uh, but you didn't actually use the notion of *computational* simplicity to get that conclusion; you just required that the supply of probability mass is finite and the supply of potential complications is infinite. Any way of counting discrete complications would imply that conclusion, even if it went by surface wheels and gears.\n\n**BLAINE:**  Well, maybe. But it so happens that Yudkowsky did invent or reinvent that argument after pondering Solomonoff induction, and if it predates him (or Solomonoff) then Yudkowsky doesn't know the source. Concrete inspiration for simplified arguments is also a credit to a theory, especially if the simplified argument didn't exist before that.\n\n**ASHLEY:**  Fair enough.\n\n### v.  Choice of Universal Turing Machine\n\n**ASHLEY:**  My next question is about the choice of Universal Turing Machine—the choice of compiler for our program codes. There's an infinite number of possibilities there, and in principle, the right choice of compiler can make our probability for the next thing we'll see be anything we like. At least I'd expect this to be the case, based on how the \"[problem of induction](https://en.wikipedia.org/wiki/Problem_of_induction)\" usually goes. So with the right choice of Universal Turing Machine, our online crackpot can still make it be the case that Solomonoff induction predicts Canada invading the USA.\n\n**BLAINE:**  One way of looking at the problem of good epistemology, I'd say, is that the job of a good epistemology is not to make it *impossible* to err. You can still blow off your foot if you really insist on pointing the shotgun at your foot and pulling the trigger.\n\nThe job of good epistemology is to make it *more obvious* when you're about to blow your own foot off with a shotgun. On this dimension, Solomonoff induction excels. If you claim that we ought to pick an enormously complicated compiler to encode our hypotheses, in order to make the 'simplest hypothesis that fits the evidence' be one that predicts Canada invading the USA, then it should be obvious to everyone except you that you are in the process of screwing up.\n\n**ASHLEY:**  Ah, but of course they'll say that their code is just the simple and natural choice of Universal Turing Machine, because they'll exhibit a meta-UTM which outputs that UTM given only a short code. And if you say the meta-UTM is complicated—\n\n**BLAINE:**  Flon's Law says, \"There is not now, nor has there ever been, nor will there ever be, any programming language in which it is the least bit difficult to write bad code.\" You can't make it impossible for people to screw up, but you can make it *more obvious.* And Solomonoff induction would make it even more obvious than might at first be obvious, because—\n\n**ASHLEY:**  Your Honor, I move to have the previous sentence taken out and shot.\n\n**BLAINE:**  Let's say that the whole of your sensory information is the string **10101010...** Consider the stupid hypothesis, \"This program has a 99% probability of producing a **1** on every turn\", which you jumped to after seeing the first bit. What would you need to claim your priors were like—what Universal Turing Machine would you need to endorse—in order to maintain blind faith in that hypothesis in the face of ever-mounting evidence?\n\n**ASHLEY:**  You'd need a Universal Turing Machine **blind-utm** that assigned a very high probability to the **blind** program \"def ProbNextElementIsOne(previous_sequence): return 0.99\". Like, if **blind-utm** sees the code **0**, it executes the **blind** program \"return 0.99\".\n\nAnd to defend yourself against charges that your UTM **blind-utm** was not itself simple, you'd need a meta-UTM, **blind-meta**, which, when it sees the code **10**, executes **blind-utm**.\n\nAnd to really wrap it up, you'd need to take a fixed point through all towers of meta and use diagonalization to create the UTM **blind-diag** that, when it sees the program code **0**, executes \"return 0.99\", and when it sees the program code **10**, executes **blind-diag**.\n\nI guess I can see some sense in which, even if that doesn't resolve Hume's problem of induction, anyone *actually advocating that* would be committing blatant shenanigans on a commonsense level, arguably more blatant than it would have been if we hadn't made them present the UTM.\n\n**BLAINE:**  Actually, the shenanigans have to be much worse than that in order to fool Solomonoff induction. Like, Solomonoff induction using your **blind-diag** isn't fooled for a minute, even taking **blind-diag** entirely on its own terms.\n\n**ASHLEY:**  Really?\n\n**BLAINE:**  Assuming 60 sequence items per second? Yes, absolutely, Solomonoff induction shrugs off the delusion in the first minute, unless there are further and even more blatant shenanigans.\n\nWe did require that your **blind-diag** be a *Universal* Turing Machine, meaning that it can reproduce every computable probability distribution over sequences, given some particular code to compile. Let's say there's a 200-bit code **laplace** for Laplace's Rule of Succession, \"lambda sequence: return (sequence.count('1') + 1) / (len(sequence) + 2)\", so that its prior probability relative to the 1-bit code for **blind** is \\\\(2^{-200}\\\\). Let's say that the sense data is around 50/50 1s and 0s. Every time we see a 1, **blind** gains a factor of 2 over **laplace** (99% vs. 50% probability), and every time we see a 0, **blind** loses a factor of 50 over **laplace** (1% vs. 50% probability).\n\nOn average, every 2 bits of the sequence, **blind** is losing a factor of 25 or, say, a bit more than 4 bits, i.e., on average **blind** is losing two bits of probability per element of the sequence observed.\n\nSo it's only going to take 100 bits, or a little less than two seconds, for **laplace** to win out over **blind**.\n\n**ASHLEY:**  I see. I was focusing on a UTM that assigned lots of prior probability to **blind**, but what I really needed was a compiler that, *while still being universal* and encoding every possibility somewhere, still assigned a really tiny probability to **laplace**, **faircoin** that encodes \"return 0.5\", and every other hypothesis that does better, round by round, than **blind**. So what I really need to carry off the delusion is **obstinate-diag** that is universal, assigns high probability to **blind**, requires billions of bits to specify **laplace**, and also requires billions of bits to specify any UTM that can execute **laplace** as a shorter code than billions of bits. Because otherwise we will say, \"Ah, but given the evidence, this other UTM would have done better.\" I agree that those are even more blatant shenanigans than I thought.\n\n**BLAINE:**  Yes. And even *then*, even if your UTM takes two billion bits to specify **faircoin**, Solomonoff induction will lose its faith in **blind** after seeing a billion bits.\n\nWhich will happen before the first year is out, if we're getting 60 bits per second.\n\nAnd if you turn around and say, \"Oh, well, I didn't mean *that* was my UTM, I really meant *this* was my UTM, this thing over here where it takes a *trillion* bits to encode **faircoin**\", then that's probability-theory-violating shenanigans where you're changing your priors as you go.\n\n**ASHLEY:**  That's actually a very interesting point—that what's needed for a Bayesian to maintain a delusion in the face of mounting evidence is not so much a blindly high prior for the delusory hypothesis, as a blind skepticism of all its alternatives.\n\nBut what if their UTM requires a googol bits to specify **faircoin**? What if **blind** and **blind-diag**, or programs pretty much isomorphic to them, are the only programs that can be specified in less than a googol bits?\n\n**BLAINE:**  Then your desire to shoot your own foot off has been made very, very visible to anyone who understands Solomonoff induction. We're not going to get absolutely objective prior probabilities as a matter of logical deduction, not without principles that are unknown to me and beyond the scope of Solomonoff induction. But we can make the stupidity really *blatant* and force you to construct a downright embarrassing Universal Turing Machine.\n\n**ASHLEY:**  I guess I can see that. I mean, I guess that if you're presenting a ludicrously complicated Universal Turing Machine that just refuses to encode the program that would predict Canada not invading, that's more *visibly* silly than a verbal appeal that says, \"But you must just have faith that Canada will invade.\" I guess part of me is still hoping for a more objective sense of \"complicated\".\n\n**BLAINE:**  We could say that reasonable UTMs should contain a small number of wheels and gears in a material instantiation under our universe's laws of physics, which might in some ultimate sense provide a prior over priors. Like, the human brain evolved from DNA-based specifications, and the things you can construct out of relatively small numbers of physical objects are 'simple' under the 'prior' implicitly searched by natural selection.\n\n**ASHLEY:**  Ah, but what if I think it's likely that our physical universe or the search space of DNA won't give us a good idea of what's complicated?\n\n**BLAINE:**  For your alternative notion of what's complicated to go on being believed even as other hypotheses are racking up better experimental predictions, you need to assign a *ludicrously low probability* that our universe's space of physical systems buildable using a small number of objects, could *possibly* provide better predictions of that universe than your complicated alternative notion of prior probability.\n\nWe don't need to appeal that it's *a priori* more likely than not that \"a universe can be predicted well by low-object-number machines built using that universe's physics.\" Instead, we appeal that it would violate [Cromwell's Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule), and would constitute exceedingly special pleading, to assign the possibility of a physically learnable universe a probability of *less* than \\\\(2^{−1,000,000}\\\\). It then takes only a megabit of exposure to notice that the universe seems to be regular.\n\n**ASHLEY:**  In other words, so long as you don't start with an absolute and blind prejudice against the universe being predictable by simple machines encoded in our universe's physics—so long as, on this planet of seven billion people, you don't assign probabilities less than \\\\(2^{−1,000,000}\\\\) to the other person being right about what is a good Universal Turing Machine—then the pure logic of Bayesian updating will rapidly force you to the conclusion that induction works.\n\n### vi.  Why algorithmic complexity?\n\n**ASHLEY:**  Hm. I don't know that good *pragmatic* answers to the problem of induction were ever in short supply. Still, on the margins, it's a more forceful pragmatic answer than the last one I remember hearing.\n\n**BLAINE:**  Yay! *Now* isn't Solomonoff induction wonderful?\n\n**ASHLEY:**  Maybe?\n\nYou didn't really use the principle of *computational* simplicity to derive that lesson. You just used that *some inductive principle* ought to have a prior probability of more than \\\\(2^{−1,000,000}\\\\).\n\n**BLAINE:**  ...\n\n**ASHLEY:**  Can you give me an example of a problem where the *computational* definition of simplicity matters and can't be factored back out of an argument?\n\n**BLAINE:**  As it happens, yes I can. I can give you *three* examples of how it matters.\n\n**ASHLEY:**  Vun... two... three! Three examples! Ah-ah-ah!\n\n**BLAINE:**  Must you do that every—oh, never mind. Example one is that galaxies are not so improbable that no one could ever believe in them, example two is that the limits of possibility include Terrence Tao, and example three is that diffraction is a simpler explanation of rainbows than divine intervention.\n\n**ASHLEY:**  These statements are all so obvious that no further explanation of any of them is required.\n\n**BLAINE:**  On the contrary! And I'll start with example one. Back when the Andromeda Galaxy was a hazy mist seen through a telescope, and someone first suggested that maybe that hazy mist was an incredibly large number of distant stars—that many \"nebulae\" were actually *distant galaxies*, and our own Milky Way was only one of them—there was a time when Occam's Razor was invoked against that hypothesis.\n\n**ASHLEY:**  What? Why?\n\n**BLAINE:**  They invoked Occam's Razor against the galactic hypothesis, because if that were the case, then there would be *a much huger number of stars* in the universe, and the stars would be entities, and Occam's Razor said \"Entities are not to be multiplied beyond necessity.\"\n\n**ASHLEY:**  That's not how Occam's Razor works. The \"entities\" of a theory are its types, not its objects. If you say that the hazy mists are distant galaxies of stars, then you've reduced the number of laws because you're just postulating a previously seen type, namely stars organized into galaxies, instead of a new type of hazy astronomical mist.\n\n**BLAINE:**  Okay, but imagine that it's the nineteenth century and somebody replies to you, \"Well, I disagree! William of Ockham said not to multiply entities, this galactic hypothesis obviously creates a huge number of entities, and that's the way I see it!\"\n\n**ASHLEY:**  I think I'd give them your spiel about there being no human epistemology that can stop you from shooting off your own foot.\n\n**BLAINE:**  I *don't* think you'd be justified in giving them that lecture.\n\nI'll parenthesize at this point that you ought to be very careful when you say \"I can't stop you from shooting off your own foot\", lest it become a Fully General Scornful Rejoinder. Like, if you say that to someone, you'd better be able to explain exactly why Occam's Razor counts types as entities but not objects. In fact, you'd better explain that to someone *before* you go advising them not to shoot off their own foot. And once you've told them what you think is foolish and why, you might as well stop there. Except in really weird cases of people presenting us with enormously complicated and jury-rigged Universal Turing Machines, and then we say the shotgun thing.\n\n**ASHLEY:**  That's fair. So, I'm not sure what I'd have answered before starting this conversation, which is much to your credit, friend Blaine. But now that I've had this conversation, it's obvious that it's new types and not new objects that use up the probability mass we need to distribute over all hypotheses. Like, I need to distribute my probability mass over \"Hypothesis 1: there are stars\" and \"Hypothesis 2: there are stars plus huge distant hazy mists\". I don't need to distribute my probability mass over all the actual stars in the galaxy!\n\n**BLAINE:**  In terms of Solomonoff induction, we penalize a program's *lines of code* rather than its *runtime* or *RAM used*, because we need to distribute our probability mass over possible alternatives each time we add a line of code. There's no corresponding *choice between mutually exclusive alternatives* when a program uses more runtime or RAM.\n\n(**ELIEZER:**  (*whispering*)  *Unless we need a* [*leverage prior*](https://www.lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/) *to consider the hypothesis of being a particular agent inside all that RAM or runtime.*)\n\n**ASHLEY:**  Or to put it another way: any fully detailed model of the universe would require some particular arrangement of stars, and the more stars there are, the more possible arrangements there are. But when we look through the telescope and see a hazy mist, we get to sum over all arrangements of stars that would produce that hazy mist. If some galactic hypothesis required a hundred billion stars to *all* be in *particular exact places* without further explanation or cause, then that would indeed be a grave improbability.\n\n**BLAINE:**  Precisely. And if you needed all the hundred billion stars to be in particular exact places, that's just the kind of hypothesis that would take a huge computer program to specify.\n\n**ASHLEY:**  But does it really require learning Solomonoff induction to understand that point? Maybe the bad argument against galaxies was just a motivated error somebody made in the nineteenth century, because they didn't want to live in a big universe for emotional reasons.\n\n**BLAINE:**  The same debate is playing out today over no-collapse versions of quantum mechanics, also somewhat unfortunately known as \"many-worlds interpretations\". Now, regardless of what anyone thinks of all the other parts of that debate, there's a *particular* sub-argument where somebody says, \"It's simpler to have a collapse interpretation because all those extra quantum 'worlds' are extra entities that are unnecessary under Occam's Razor since we can't see them.\" And Solomonoff induction tells us that this invocation of Occam's Razor is flatly misguided because Occam's Razor does not work like that.\n\nBasically, they're trying to cut down the RAM and runtime of the universe, at the expensive of adding an extra line of code, namely the code for the collapse postulate that prunes off parts of the wavefunction that are in undetectably weak causal contact with us.\n\n**ASHLEY:**  Hmm. Now that you put it that way, it's not so obvious to me that it makes sense to have *no* prejudice against *sufficiently* enormous universes. I mean, the universe we see around us is exponentially vast but not superexponentially vast—the visible atoms are \\\\(10^{80}\\\\) in number or so, not \\\\(10^{10^{80}}\\\\) or \"bigger than Graham's Number\". Maybe there's some fundamental limit on how much gets computed.\n\n**BLAINE:**  You, um, know that on the Standard Model, the universe doesn't just cut out and stop existing at the point where our telescopes stop seeing it? There isn't a giant void surrounding a little bubble of matter centered perfectly on Earth? It calls for a literally infinite amount of matter? I mean, I guess if you don't like living in a universe with more than \\\\(10^{80}\\\\) entities, a universe where *too much gets computed,* you could try to specify *extra laws of physics* that create an abrupt spatial boundary with no further matter beyond them, somewhere out past where our telescopes can see—\n\n**ASHLEY:**  All right, point taken.\n\n(**ELIEZER:  **(*whispering*)  *Though I personally suspect that the spatial multiverse and the quantum multiverse are the same multiverse, and that what lies beyond the reach of our telescopes is not entangled with us—meaning that the universe is as finitely large as the superposition of all possible quantum branches, rather than being literally infinite in space.*)\n\n**BLAINE:**  I mean, there is in fact an alternative formalism to Solomonoff induction, namely [Levin search](http://www.scholarpedia.org/article/Universal_search#Levin_complexity), which says that program complexities are further penalized by the logarithm of their runtime. In other words, it would say that 'explanations' or 'universes' that require a long time to run are inherently less probable.\n\nSome people like Levin search more than Solomonoff induction because it's more computable. I dislike Levin search because (a) it has no fundamental epistemic justification and (b) it assigns probability zero to quantum mechanics.\n\n**ASHLEY:**  Can you unpack that last part?\n\n**BLAINE:**  If, as is currently suspected, there's no way to simulate quantum computers using classical computers without an exponential slowdown, then even in principle, this universe requires exponentially vast amounts of classical computing power to simulate.\n\nLet's say that with sufficiently advanced technology, you can build a quantum computer with a million qubits. On Levin's definition of complexity, for the universe to be like that is as improbable *a priori* as *any particular* set of laws of physics that must specify on the order of one million equations.\n\nCan you imagine how improbable it would be to see a list of one hundred thousand differential equations, without any justification or evidence attached, and be told that they were the laws of physics? That's the kind of penalty that Levin search or Schmidhuber's Speed Prior would attach to any laws of physics that could run a quantum computation of a million qubits, or, heck, any physics that claimed that a protein was being folded in a way that ultimately went through considering millions of quarks interacting.\n\nIf you're *not* absolutely certain *a priori* that the universe *isn't* like that, you don't believe in Schmidhuber's Speed Prior. Even with a collapse postulate, the amount of computation that goes on before a collapse would be prohibited by the Speed Prior.\n\n**ASHLEY:**  Okay, yeah. If you're phrasing it that way—that the Speed Prior assigns probability nearly zero to quantum mechanics, so we shouldn't believe in the Speed Prior—then I can't easily see a way to extract out the same point without making reference to ideas like penalizing algorithmic complexity but not penalizing runtime. I mean, maybe I could extract the lesson back out but it's easier to say, or more obvious, by pointing to the idea that Occam's Razor should penalize algorithmic complexity but not runtime.\n\n**BLAINE:**  And that isn't just *implied by* Solomonoff induction, it's pretty much the whole idea of Solomonoff induction, right?\n\n**ASHLEY:**  Maaaybe.\n\n**BLAINE:**  For example two, that Solomonoff induction outperforms even Terence Tao, we want to have a theorem that says Solomonoff induction catches up to every computable way of reasoning in the limit. Since we iterated through all possible computer programs, we know that somewhere in there is a simulated copy of Terence Tao in a simulated room, and if this requires a petabyte to specify, then we shouldn't have to make more than a quadrillion bits of error relative to Terence Tao before zeroing in on the Terence Tao hypothesis.\n\nI mean, in practice, I'd expect far less than a quadrillion bits of error before the system was behaving like it was vastly smarter than Terence Tao. It'd take a lot less than a quadrillion bits to give you some specification of a universe with simple physics that gave rise to a civilization of vastly greater than intergalactic extent. Like, [Graham's Number](http://googology.wikia.com/wiki/Graham's_number) is a very simple number, so it's easy to specify a universe that runs for that long before it returns an answer. It's not obvious how you'd extract Solomonoff predictions from that civilization and incentivize them to make good ones, but I'd be surprised if there were no Turing machine of fewer than one thousand states which did that somehow.\n\n**ASHLEY:**  ...\n\n**BLAINE:**  And for all I know there might be even better ways than that of getting exceptionally good predictions, somewhere in the list of the first decillion computer programs. That is, somewhere in the first 100 bits.\n\n**ASHLEY:**  So your basic argument is, \"Never mind Terence Tao, Solomonoff induction dominates *God*.\"\n\n**BLAINE:**  Solomonoff induction isn't the epistemic prediction capability of a superintelligence. It's the epistemic prediction capability of something that eats superintelligences like potato chips.\n\n**ASHLEY:**  Is there any point to contemplating an epistemology so powerful that it will never begin to fit inside the universe?\n\n**BLAINE:**  Maybe? I mean, a lot of times, you just find people *failing to respect* the notion of ordinary superintelligence, doing the equivalent of supposing that a superintelligence behaves like a bad Hollywood genius and misses obvious-seeming moves. And a lot of times you find them insisting that \"there's a limit to how much information you can get from the data\" or something along those lines. \"[That Alien Message](https://www.lesswrong.com/lw/qk/that_alien_message/)\" is intended to convey the counterpoint, that smarter entities can extract more info than is immediately apparent on the surface of things.\n\nSimilarly, thinking about Solomonoff induction might also cause someone to realize that if, say, you simulated zillions of possible simple universes, you could look at which agents were seeing exact data like the data you got, and figure out where you were inside that range of possibilities, so long as there was literally *any* correlation to use.\n\nAnd if you say that an agent *can't* extract that data, you're making a claim about which shortcuts to Solomonoff induction are and aren't computable. In fact, you're probably pointing at some *particular* shortcut and claiming nobody can ever figure that out using a reasonable amount of computing power *even though the info is there in principle.* Contemplating Solomonoff induction might help people realize that, yes, the data *is* there in principle. Like, until I ask you to imagine a civilization running for Graham's Number of years inside a Graham-sized memory space, you might not imagine them trying all the methods of analysis that *you personally* can imagine being possible.\n\n**ASHLEY:**  If somebody is making that mistake in the first place, I'm not sure you can beat it out of them by telling them the definition of Solomonoff induction.\n\n**BLAINE:**  Maybe not. But to brute-force somebody into imagining that [sufficiently advanced agents](https://arbital.com/p/advanced_agent/) have [Level 1 protagonist intelligence](http://yudkowsky.tumblr.com/writing/level1intelligent), that they are [epistemically efficient](https://arbital.com/p/efficiency/) rather than missing factual questions that are visible even to us, you might need to ask them to imagine an agent that can see *literally anything seeable in the computational limit* just so that their mental simulation of the ideal answer isn't running up against stupidity assertions.\n\nLike, I think there are a lot of people who could benefit from looking over the evidence they already personally have, and asking what a Solomonoff inductor could deduce from it, so that they wouldn't be running up against stupidity assertions *about themselves.* It's the same trick as asking yourself what God, Richard Feynman, or a \"perfect rationalist\" would believe in your shoes. You just have to pick a real or imaginary person that you respect enough for your model of that person to lack the same stupidity assertions that you believe about yourself.\n\n**ASHLEY:**  Well, let's once again try to factor out the part about Solomonoff induction in particular. If we're trying to imagine something epistemically smarter than ourselves, is there anything we get from imagining a complexity-weighted prior over programs in particular? That we don't get from, say, trying to imagine the reasoning of one particular Graham-Number-sized civilization?\n\n**BLAINE:**  We get the surety that even anything we imagine *Terence Tao himself* as being able to figure out, is something that is allowed to be known after some bounded number of errors versus Terence Tao, because Terence Tao is inside the list of all computer programs and gets promoted further each time the dominant paradigm makes a prediction error relative to him.\n\nWe can't get that dominance property without invoking \"all possible ways of computing\" or something like it—we can't incorporate the power of all reasonable processes, unless we have a set such that all the reasonable processes are in it. The enumeration of all possible computer programs is one such set.\n\n**ASHLEY:**  Hm.\n\n**BLAINE:**  Example three, diffraction is a simpler explanation of rainbows than divine intervention.\n\nI don't think I need to belabor this point very much, even though in one way it might be the most central one. It sounds like \"Jehovah placed rainbows in the sky as a sign that the Great Flood would never come again\" is a 'simple' explanation; you can explain it to a child in nothing flat. Just the diagram of diffraction through a raindrop, to say nothing of the Principle of Least Action underlying diffraction, is something that humans don't usually learn until undergraduate physics, and it *sounds* more alien and less intuitive than Jehovah. In what sense is this intuitive sense of simplicity wrong? What gold standard are we comparing it to, that could be a better sense of simplicity than just 'how hard is it for me to understand'?\n\nThe answer is Solomonoff induction and the rule which says that simplicity is measured by the size of the computer program, not by how hard things are for human beings to understand. Diffraction is a small computer program; any programmer who understands diffraction can simulate it without too much trouble. Jehovah would be a much huger program—a complete mind that implements anger, vengeance, belief, memory, consequentialism, etcetera. Solomonoff induction is what tells us to retrain our intuitions so that differential equations feel like less [burdensome](https://www.lesswrong.com/posts/Yq6aA4M3JKWaQepPJ/burdensome-details) explanations than heroic mythology.\n\n**ASHLEY:**  Now hold on just a second, if that's actually how Solomonoff induction works then it's not working very well. I mean, Abraham Lincoln was a great big complicated mechanism from an algorithmic standpoint—he had a hundred trillion synapses in his brain—but that doesn't mean I should look at the historical role supposedly filled by Abraham Lincoln, and look for simple mechanical rules that would account for the things Lincoln is said to have done. If you've already seen humans and you've already learned to model human minds, it shouldn't cost a vast amount to say there's one *more* human, like Lincoln, or one more entity that is *cognitively humanoid*, like the Old Testament jealous-god version of Jehovah. It may be *wrong* but it shouldn't be vastly improbable *a priori*.\n\nIf you've already been forced to acknowledge the existence of some humanlike minds, why not others? Shouldn't you get to reuse the complexity that you postulated to explain humans, in postulating Jehovah?\n\nIn fact, shouldn't that be what Solomonoff induction *does?* If you have a computer program that can model and predict humans, it should only be a slight modification of that program—only slightly longer in length and added code—to predict the modified-human entity that is Jehovah.\n\n**BLAINE:**  Hm. That's fair. I may have to retreat from that example somewhat.\n\nIn fact, that's yet another point to the credit of Solomonoff induction! The ability of programs to reuse code, incorporates our intuitive sense that if you've already postulated one kind of thing, it shouldn't cost as much to postulate a similar kind of thing elsewhere!\n\n**ASHLEY:**  Uh huh.\n\n**BLAINE:**  Well, but even if I was wrong that Solomonoff induction should make Jehovah seem very improbable, it's still Solomonoff induction that says that the alternative hypothesis of 'diffraction' shouldn't itself be seen as burdensome—even though diffraction might require a longer time to explain to a human, it's still at heart a simple program.\n\n**ASHLEY:**  Hmm.\n\nI'm trying to think if there's some notion of 'simplicity' that I can abstract away from 'simple program' as the nice property that diffraction has as an explanation for rainbows, but I guess anything I try to say is going to come down to some way of counting the wheels and gears inside the explanation, and justify the complexity penalty on probability by the increased space of possible configurations each time we add a new gear. And I can't make it be about surface details because that will make whole humans seem way too improbable.\n\nIf I have to use simply specified systems and I can't use surface details or runtime, that's probably going to end up basically equivalent to Solomonoff induction. So in that case we might as well use Solomonoff induction, which is probably simpler than whatever I'll think up and will give us the same advice. Okay, you've mostly convinced me.\n\n**BLAINE:**  *Mostly?* What's left?\n\n### vii.  Limitations\n\n**ASHLEY:**  Well, several things. Most of all, I think of how the 'language of thought' or 'language of epistemology' seems to be different in some sense from the 'language of computer programs'.\n\nLike, when I think about the laws of Newtonian gravity, or when I think about my Mom, it's not just one more line of code tacked onto a big black-box computer program. It's more like I'm crafting an explanation with modular parts—if it contains a part that looks like Newtonian mechanics, I step back and reason that it might contain other parts with differential equations. If it has a line of code for a Mom, it might have a line of code for a Dad.\n\nI'm worried that if I understood how humans think like that, maybe I'd look at Solomonoff induction and see how it doesn't incorporate some further key insight that's needed to do good epistemology.\n\n**BLAINE:**  Solomonoff induction literally incorporates a copy of you thinking about whatever you're thinking right now.\n\n**ASHLEY:**  Okay, great, but that's *inside* the system. If Solomonoff learns to promote computer programs containing good epistemology, but is not itself good epistemology, then it's not the best possible answer to \"How do you compute epistemology?\"\n\nLike, natural selection produced humans but population genetics is not an answer to \"How does intelligence work?\" because the intelligence is in the inner content rather than the outer system. In that sense, it seems like a reasonable worry that Solomonoff induction might incorporate only *some* principles of good epistemology rather than *all* the principles, even if the *internal content* rather than the *outer system* might bootstrap the rest of the way.\n\n**BLAINE:**  Hm. If you put it *that* way...\n\n(*long pause*)\n\n... then I guess I have to agree. I mean, Solomonoff induction doesn't explicitly say anything about, say, the distinction between analytic propositions and empirical propositions, and knowing that is part of good epistemology on my view. So if you want to say that Solomonoff induction is something that bootstraps to good epistemology rather than being all of good epistemology by itself, I guess I have no choice but to agree.\n\nI do think the outer system already contains a *lot* of good epistemology and inspires a lot of good advice all on its own. Especially if you give it credit for formally reproducing principles that are \"common sense\", because correctly formalizing common sense is no small feat.\n\n**ASHLEY:**  Got a list of the good advice you think is derivable?\n\n**BLAINE:**  Um. Not really, but off the top of my head:\n\n1.  The best explanation is the one with the best mixture of simplicity and matching the evidence.\n2.  \"Simplicity\" and \"matching the evidence\" can both be measured in bits, so they're commensurable.\n3.  The simplicity of a hypothesis is the number of bits required to formally specify it—for example, as a computer program.\n4.  When a hypothesis assigns twice as much probability to the exact observations seen so far as some other hypothesis, that's one bit's worth of relatively better matching the evidence.\n5.  You should actually be making your predictions using all the explanations, not just the single best one, but explanations that poorly match the evidence will drop down to tiny contributions very quickly.\n6.  Good explanations let you compress lots of data into compact reasons which strongly predict seeing just that data and no other data.\n7.  Logic can't dictate prior probabilities absolutely, but if you assign probability less than \\\\(2^{−1,000,000}\\\\) to the prior that mechanisms constructed using a small number of objects from your universe might be able to well predict that universe, you're being unreasonable.\n8.  So long as you don't assign infinitesimal prior probability to hypotheses that let you do induction, they will very rapidly overtake hypotheses that don't.\n9.  It is a logical truth, not a contingent one, that more complex hypotheses must in the limit be less probable than simple ones.\n10.  Epistemic rationality is a precise art with no user-controlled degrees of freedom in how much probability you ideally ought to assign to a belief. If you think you can tweak the probability depending on what you want the answer to be, you're doing something wrong.\n11.  Things that you've seen in one place might reappear somewhere else.\n12.  Once you've learned a new language for your explanations, like differential equations, you can use it to describe other things, because your best hypotheses will now already encode that language.\n13.  We can learn meta-reasoning procedures as well as object-level facts by looking at which meta-reasoning rules are simple and have done well on the evidence so far.\n14.  So far, we seem to have no *a priori* reason to believe that universes which are more expensive to compute are less probable.\n15.  People were wrong about galaxies being *a priori* improbable because that's not how Occam's Razor works. Today, other people are equally wrong about other parts of a continuous wavefunction counting as extra entities for the purpose of evaluating hypotheses' complexity.\n16.  If something seems \"weird\" to you but would be a consequence of simple rules that fit the evidence so far, well, there's nothing in these explicit laws of epistemology that adds an extra penalty term for weirdness.\n17.  Your epistemology shouldn't have extra rules in it that aren't needed to do Solomonoff induction or something like it, including rules like \"science is not allowed to examine this particular part of reality\"—\n\n**ASHLEY:**  This list isn't finite, is it.\n\n**BLAINE:**  Well, there's a *lot* of outstanding debate about epistemology where you can view that debate through the lens of Solomonoff induction and see what Solomonoff suggests.\n\n**ASHLEY:**  But if you don't mind my stopping to look at your last item, #17 above—again, it's attempts to add *completeness* clauses to Solomonoff induction that make me the most nervous.\n\nI guess you could say that a good rule of epistemology ought to be one that's promoted by Solomonoff induction—that it should arise, in some sense, from the simple ways of reasoning that are good at predicting observations. But that doesn't mean a good rule of epistemology ought to explicitly be in Solomonoff induction or it's out.\n\n**BLAINE:**  Can you think of good epistemology that doesn't seem to be contained in Solomonoff induction? Besides the example I already gave of distinguishing logical propositions from empirical ones.\n\n**ASHLEY:**  I've been trying to. First, it seems to me that when I reason about laws of physics and how those laws of physics might give rise to higher levels of organization like molecules, cells, human beings, the Earth, and so on, I'm not constructing in my mind a great big chunk of code that reproduces my observations. I feel like this difference might be important and it might have something to do with 'good epistemology'.\n\n**BLAINE:**  I guess it could be? I think if you're saying that there might be this unknown other thing and therefore Solomonoff induction is terrible, then that would be the [nirvana fallacy](https://en.wikipedia.org/wiki/Nirvana_fallacy). Solomonoff induction is the best formalized epistemology we have *right now*—\n\n**ASHLEY:**  I'm not saying that Solomonoff induction is terrible. I'm trying to look in the direction of things that might point to some future formalism that's better than Solomonoff induction. Here's another thing: I feel like I didn't have to learn how to model the human beings around me from scratch based on environmental observations. I got a jump-start on modeling other humans by observing *myself*, and by recruiting my brain areas to run in a sandbox mode that models other people's brain areas—empathy, in a word.\n\nI guess I feel like Solomonoff induction doesn't incorporate that idea. Like, maybe *inside* the mixture there are programs which do that, but there's no explicit support in the outer formalism.\n\n**BLAINE:**  This doesn't feel to me like much of a disadvantage of Solomonoff induction—\n\n**ASHLEY:**  I'm not *saying* it would be a disadvantage if we actually had a hypercomputer to run Solomonoff induction. I'm saying it might point in the direction of \"good epistemology\" that isn't explicitly included in Solomonoff induction.\n\nI mean, now that I think about it, a generalization of what I just said is that Solomonoff induction assumes I'm separated from the environment by a hard, Cartesian wall that occasionally hands me observations. Shouldn't a more realistic view of the universe be about a simple program that *contains me somewhere inside it,* rather than a simple program that hands observations to some other program?\n\n**BLAINE:**  Hm. Maybe. How would you formalize *that?* It seems to open up a big can of worms—\n\n**ASHLEY:**  But that's what my actual epistemology actually says. My world-model is not about a big computer program that provides inputs to my soul, it's about an enormous mathematically simple physical universe that instantiates Ashley as one piece of it. And I think it's good and important to have epistemology that works that way. It wasn't *obvious* that we needed to think about a simple universe that embeds us. Descartes *did* think in terms of an impervious soul that had the universe projecting sensory information onto its screen, and we had to get *away* from that kind of epistemology.\n\n**BLAINE:**  You understand that Solomonoff induction makes only a bounded number of errors relative to any computer program which does reason the way you prefer, right? If thinking of yourself as a contiguous piece of the universe lets you make better experimental predictions, programs which reason that way will rapidly be promoted.\n\n**ASHLEY:**  It's still unnerving to see a formalism that seems, in its own structure, to harken back to the Cartesian days of a separate soul watching a separate universe projecting sensory information on a screen. Who knows, maybe that would somehow come back to bite you?\n\n**BLAINE:**  Well, it wouldn't bite you in the form of repeatedly making wrong experimental predictions.\n\n**ASHLEY:**  But it might bite you in the form of having no way to represent the observation of, \"I drank this 'wine' liquid and then my emotions changed; could my emotions themselves be instantiated in stuff that can interact with some component of this liquid? Can alcohol touch neurons and influence them, meaning that I'm not a separate soul?\" If we interrogated the Solomonoff inductor, would it be able to understand that reasoning?\n\nWhich brings up that dangling question from before about modeling the effect that my actions and choices have on the environment, and whether, say, an agent that used Solomonoff induction would be able to correctly predict \"If I drop an anvil on my head, my sequence of sensory observations will *end*.\"\n\n**ELIEZER:  **And that's my cue to step in!\n\nThe natural next place for this dialogue to go, if I ever write a continuation, is the question of actions and choices, and the agent that uses Solomonoff induction for beliefs and expected reward maximization for selecting actions—the perfect rolling sphere of advanced agent theory, [AIXI](https://arbital.com/p/AIXI/).\n\nMeanwhile: For more about the issues Ashley raised with agents being a contiguous part of the universe, see \"[Embedded Agency](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh).\"",
      "plaintextDescription": "(Originally posted in December 2015: A dialogue between Ashley, a computer scientist who's never heard of Solomonoff's theory of inductive inference, and Blaine, who thinks it is the best thing since sliced bread.)\n\n----------------------------------------\n\n\ni.  Unbounded analysis\nASHLEY:  Good evening, Msr. Blaine.\n\nBLAINE:  Good evening, Msr. Ashley.\n\nASHLEY:  I've heard there's this thing called \"Solomonoff's theory of inductive inference\".\n\nBLAINE:  The rumors have spread, then.\n\nASHLEY:  Yeah, so, what the heck is that about?\n\nBLAINE:  Invented in the 1960s by the mathematician Ray Solomonoff, the key idea in Solomonoff induction is to do sequence prediction by using Bayesian updating on a prior composed of a mixture of all computable probability distributions—\n\nASHLEY:  Wait. Back up a lot. Before you try to explain what Solomonoff induction is, I'd like you to try to tell me what it does, or why people study it in the first place. I find that helps me organize my listening. Right now I don't even know why I should be interested in this.\n\nBLAINE:  Um, okay. Let me think for a second...\n\nASHLEY:  Also, while I can imagine things that \"sequence prediction\" might mean, I haven't yet encountered it in a technical context, so you'd better go a bit further back and start more at the beginning. I do know what \"computable\" means and what a \"probability distribution\" is, and I remember the formula for Bayes's Rule although it's been a while.\n\nBLAINE:  Okay. So... one way of framing the usual reason why people study this general field in the first place, is that sometimes, by studying certain idealized mathematical questions, we can gain valuable intuitions about epistemology. That's, uh, the field that studies how to reason about factual questions, how to build a map of reality that reflects the territory—\n\nASHLEY:  I have some idea what 'epistemology' is, yes. But I think you might need to start even further back, maybe with some sort of concrete example or something.",
      "wordCount": 16230
    },
    "tags": [
      {
        "_id": "G5zEXFxtMSAnNfBhX",
        "name": "Solomonoff induction",
        "slug": "solomonoff-induction"
      },
      {
        "_id": "hQiuNkBhn6xxcedTD",
        "name": "Occam's Razor",
        "slug": "occam-s-razor"
      },
      {
        "_id": "yXNtYNHJB54T3bGm3",
        "name": "Dialogue (format)",
        "slug": "dialogue-format"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MzKKi7niyEqkBPnyu",
    "title": "Your Cheerful Price",
    "slug": "your-cheerful-price",
    "url": null,
    "baseScore": 275,
    "voteCount": 145,
    "viewCount": null,
    "commentCount": 83,
    "createdAt": null,
    "postedAt": "2021-02-13T05:41:53.511Z",
    "contents": {
      "markdown": "There's a concept I draw on often in social interactions.  I've been calling it the \"happy price\", but that is [originally terminology by Michael Ellsberg](https://www.forbes.com/sites/michaelellsberg/2014/09/19/happy-price-vs-sad-price/) with subtly different implications.  So I now fork off the term \"cheerful price\", and specialize it anew.  [Earlier Facebook discussion here](https://www.facebook.com/yudkowsky/posts/10159395351309228).\n\nTl;dr:\n\n*   When I ask you for your Cheerful Price for doing something, I'm asking you for the price that:\n    *   Gives you a cheerful feeling about the transaction;\n    *   Makes you feel energized about doing the thing;\n    *   Doesn't generate an ouch feeling to be associated with me;\n    *   Means I'm not expending social capital or friendship capital to make the thing happen;\n    *   Doesn't require the executive part of you, that knows you need money in the long-term, to shout down and override other parts of you.\n*   The Cheerful Price is not:\n    *   A \"fair\" price;\n    *   The price *you* would pay somebody else to do similar work;\n    *   The lowest price such that you'd feel sad about learning the transaction was canceled;\n    *   The price that you'd charge a non-friend, though this is a good thing to check (see below);\n    *   A price you're willing to repeat for future transactions, though this is a good thing to check (see below);\n    *   The *bare minimum* amount of money such that you feel cheerful.  It should include some safety margin to account for fluctuating feelings.\n*   The point of a Cheerful Price, from my perspective as somebody who's usually the one trying to emit money, is that:\n    *   It lets me avoid the nightmare of accidentally inflicting small ouches on people;\n    *   It lets me avoid the nightmare of spending social capital while having no idea how much I'm spending;\n    *   It lets me feel good instead of bad about asking other people to do things.\n*   Warnings:\n    *   Not everybody was raised with an attitude of \"money is the unit of caring and the medium of cooperation\" towards exchanges with an overt financial element.  Some people may just *not* have a monetary price for some things, such that the exchange would boost rather than hurt their friendship, and their feelings too are valid.  Not as valid as mine, of course, but still valid.\n    *   \"I don't have a cheerful price for that, would you like a non-cheerful price\" is a valid response.\n    *   Any time you ask somebody for a Cheerful Price, you are implicitly promising not to hold any price they name against them, even if it's a billion dollars.\n    *   If [Tell Culture](https://www.lesswrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture) doesn't work for someone, Cheerful Prices may not work for them either.\n    *   If a friend didn't already ask for your cheerful price, it may be good to explicitly tell them when you're naming your cheerful price rather than your minimum price.\n    *   Life does not promise us that we will always get our Cheerful Prices, even from our friends.\n\n*Q:  Why is my Cheerful Price not the same as the minimum price that would make me prefer doing the transaction to not doing it?  If, on net, I'd rather do something than not do it, and I get to do it, shouldn't I feel cheerful about that?*\n\nAs an oversimplified model, imagine that your mind consists of a bunch of oft-conflicting desires, plus a magic executive whose job it is to decide what to do in the end.  This magic executive also better understands concepts like \"hyperbolic discounting\" that the wordless voices don't understand as well.\n\nNow suppose that I don't want to hurt you even a little; and that I live in terror of accidentally overdrawing on other people's senses of friendship or obligation towards me; and that I worry about generating small ouches that your mind will thereafter associate with me.\n\nIn this case I do *not* want to offer you the minimum price such that your executive part, which knows you need money in the long-term, would forcibly override your inner voices that don't understand hyperbolic discounting, and force them to accept the offer.  Those parts of you may then feel bad while you're actually performing the task.  I want to offer you an amount of money large enough to produce an inner \"Yay!\" loud enough that your executive does not *have* to shout down the other inner voices.\n\n*Q:  Say more about why you'd pay extra to make my balance of inner voices be very yayful?*\n\nSome possible reasons:\n\n*   Because I feel worried about the trade not getting around to taking place at all, if you are reluctant to name a price high enough to make you feel cheerful.\n*   Because I worry about you feeling a sense of ouchness about trading with me, and I want to be sure you don't end up unconsciously avoiding trading with me in the future.\n*   Because I think you're otherwise likely to name too low a price compared to my willingness to pay, and I'd feel bad about an unfair division of gains from trade.\n*   Because my willingness to pay marginally more, for a marginally better thingy, is high; so I'm eager to pay more in order to have you feeling better about doing the thing, in hopes that I get a slightly better result.\n*   Or I just don't want to hurt you even a little; either because I care about you, or because I selfishly don't want to worry about the guilt of worrying I hurt somebody.\n\n*Q:  Before we get too much further, is there anybody for whom this whole document is diametrically the wrong advice?*\n\nFor one, anybody who tends to already set their prices so high that they end up not getting enough business to keep them busy, should not read things that they might interpret as exhorting them to go set even huger prices.  For more on this, see Michael Ellsberg's [original cautions around the \"happy price\"](https://www.forbes.com/sites/michaelellsberg/2014/09/19/happy-price-vs-sad-price/), and how it differs from people being exhorted to set ever higher sky-high prices on their seminars.\n\nFor two, anybody who already experiences a lot of negative emotion from ruminating on how little they receive; or who feels sad and depressed about how little they have to give; and who doesn't have any power to get more, or give more.  They should maybe not be reading this at all?  They should maybe just stop reading this whole essay immediately, because it may just make them feel sadder.  You need to have [Slack](https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack) to benefit from an essay about how to use Slack.\n\n*Q:  Thanks, back to other questions.  Why is a Cheerful Price not the same quantity as \"the least price that would make me feel sad and disappointed about the transaction being cancelled\"?*\n\nBecause of loss aversion and inner multiplicity.  Once you're expecting to get some money in exchange for doing something, even if you weren't *cheerful* about that price, the part of you that did want the money will feel a sting of loss about losing the money.  If you're setting the standard to \"minimum price that leads to a feeling of loss about losing the money\" then, especially if you strongly need money, you may be setting your price *way* too low and might not be capturing any of the gains from trade.\n\nAlso, since your strength of feelings may fluctuate over time, you should not be trying to cut a Cheerful Price very finely; you should name a price that includes some safety margin.  If I was uncomfortable with you taking some safety margin, I wouldn't be asking you to name your Cheerful Price in the first place.  I'm fine if I end up with a little more social capital than when I started.  Nothing goes wrong if you end up feeling slightly grateful about making the trade.\n\n*Q:  For several of your desiderata above, why not ask me instead for a price such that I don't feel any ouchness about it?*\n\nBecause I expect people to have a much harder time correctly detecting whether they are feeling any tiny lingering ouches, compared to noticing a positive feeling of cheerfulness.\n\n*Q:  Why is it better to ask somebody to name \"a price that makes you cheerful\" rather than \"a price that seems fair to you\"?*\n\nWell, because those two things aren't interchangeable, and the thing that I actually want is the Cheerful Price?\n\nBut in particular I'd worry that the notion of a \"fair price\" might lead people to name prices-that-make-them-feel-bad.  \n\nSuppose I want to pay somebody else to do my laundry this week.  If I ask them for a *fair price,* instead of a *cheerful price,* to do my laundry, they may substitute some other \"fairness\"-related question like:\n\n\"How much would *I* be willing to pay somebody else to do my laundry?\"\n\nAnd this presumes several symmetries that I think should not be symmetries.  My willingness to pay is not the same as your willingness to pay.  The price you'd pay to not have to do your own laundry this week, isn't the same as the price you'd accept to do an additional load of laundry this week.\n\nThis may not seem *fair,* because it doesn't seem universal and symmetrical.  But to me, in context, those seem like *false* symmetries and *mistakenly* substituted questions, that might lead somebody into naming a price they didn't actually want to take, and then feeling trapped into taking it.\n\nSo I'd see this as a case where the recipe \"raise the price until the thought of accepting it makes you feel a noticeable feeling of cheerfulness\" may beat the recipe \"try to figure out what price would be 'fair'\".\n\n*Q:  Hold on, technical objection to the above:  Isn't it suspicious from a coherent-decisions perspective if the price you'd accept to do somebody else's laundry is much higher or lower than the price you'd pay to not do your own laundry?  If you have a lot of opportunities like this, it's a* [*theorem*](https://arbital.com/p/expected_utility_formalism/?l=7hh) *that either you can be seen as placing some consistent monetary price on your time and stamina, or you can rearrange your choices to end up with strictly more money, time, and stamina.  I mean, suppose somebody offers you $40 to personally do their laundry that week, but you can pay $20 to get a roommate to do your own roughly-equivalent amount of laundry -*\n\nWhen I ask you for your Cheerful Price, I'm asking what I need to pay to make your *current* chorus of inner voices cheerful about taking the money, instead of them feeling slightly resentful at me afterwards.  It's fine and noble if you want to cultivate your inner voices to make your life decisions more coherent; but please do that on your own time rather than by expending *my* social capital.\n\n*Q:  Why is it better to name a \"cheerful price\" rather than \"a price where both parties benefit\"?  Don't we want trades to execute whenever both parties benefit from them?*\n\nFirst of all, it's *not* always better.  It's better in those special cases when one or both parties to the trade have particular desiderata, as listed above, that matter to them equally or more than the variation in price.  This will not always hold true.\n\nMore generally, though:  Part of the great economic problem is finding trades that benefit both parties.  But even after we find a trade like that, we then encounter a further problem on top: *the division of gains from trade.*\n\nLet's say I want to pay you to bake me a cake.  Suppose that $40 is the absolute most I'd be willing to pay, if I had no other options, and that I wouldn't feel good about it (the inner voices are then discordant and require an executive to shout them down and accept the transaction).  Conversely, you'd accept a bare minimum of $10, and wouldn't feel good about that price either.\n\nThen the price should fall somewhere between $10 and $40, for the transaction to occur at all.  But within that range, there's a zero-sum problem of *dividing the gains from trade,* on top of the positive-sum interaction of *having the trade occur at all.*  At a price point of $15 we're both better off, and at a price point of $35 we're both better off; but I am better off, and you are worse off, at a price point of $15 compared to $35.\n\nIf *many* transactions like this are taking place, we have a third, positive-sum game on top of the zero-sum second game: the game of supply and demand, the invisible hand.  You set a price on your cakes that will cause you to sell as much of your cake-baking time as you want to sell; and the more people want your cakes, the higher your price goes; and if your price is high enough, that signals others to enter the market and supply more cakes.  If we have a *market price* that balances a public supply function and public demand function for interchangeable cakes, all is good.  But not everything in life is exchangeable that way; and if not, there's a gains-division problem between a unique buyer and a unique seller of a unique good.\n\nThe domain of the Cheerful Price is the pricing-of-the-trade issue.  Though indeed, one of my potential reasons above for requesting a Cheerful Price was \"Because I'm nervous about the trade happening at all, so I want you to name a price that makes you feel energized about getting around to it.\"\n\n*Q:  But if you ask somebody to name a Cheerful Price, doesn't that mean they might name a price too high for the trade to take place, even if at a lower price the trade would benefit both sides?  Or what if they just name an astronomical price?*\n\nWhen I ask you to name a Cheerful Price, this often - not quite always - happens when I have what I suspect is a relatively high willingness to pay.  But if your price goes so high that *I* no longer feel cheerful, the transaction won't actually take place at that price, and you won't get to feel cheerful about it.  So you still have some incentive to keep your quoted price to \"makes me feel cheerful at all, plus some safety margin in case my feelings fluctuate, but not *too* much higher than that\".\n\n*Q:  What if my cheerful price feels very high and I'm too embarrassed to say it?*\n\nIf I'm willing to pay it, you probably shouldn't feel embarrassed about accepting it?  I wouldn't ordinarily advise other people to always directly confront their embarrassment about everything.  But \"accepting money that other people are happy to pay you\" is in fact an *unusually good and important place* to start overcoming your embarrassed feelings.\n\n*Q:  But what if you're not willing to pay the price I name?  Wouldn't that be socially awful?*\n\nImplicit in my asking you to name a Cheerful Price is a social promise that I *will not hold any price you name against you.*\n\nYour Cheerful Price is a fact about you and your feelings.  It's not a statement that you think you're deserved something, or owed something, or that you expect to get that price from me.  From one possible perspective, I'm asking you to do me the favor of telling me that useful fact about your own state of mind, and you are doing me the favor of telling me.  If [Tell Culture](https://www.lesswrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture) doesn't work for one or both parties to a transaction, the idiom of talking about Cheerful Prices may not serve them either.\n\nIf  I ask you \"What price would make you feel cheerful about baking me a cake?\" and you are feeling generally horrible and it would take a life-changing amount of money to make you feel good about kitchen work, you could say:  \"Cheerful?  Probably a hundred thousand dollars.  But I'd rather do it for fifty dollars than not do it.\"  And that would be fine.\n\nIf your Cheerful Price makes me feel unhappy with the trade, I can tell you so.  And then we could just not do it; or I, or you, could try to negotiate the price downward to a non-cheerful but mutually beneficial price.\n\n*Q:  If you're not promising to pay my Cheerful Price and we might end up negotiating anyway, what's the point of asking me to name one?*\n\nBecause there's no point in negotiating below your cheerful region, if your cheerful price is already inside my comfortable-willingness to pay?\n\nIn some contexts you could think of it as me asking you to start off with an unusually high opening bid, such that you'd feel quite cheerful if I just accepted that bid.  Which I'd do because, e.g., I expect that, compared to my trying to save a fraction of the price I'm guessing you'll name, your non-sadness and/or eagerness to deal with me again in the future, will end up more important to me.\n\n*Q:  Wait, does that mean that if I give you a Cheerful Price, I'm obligated to accept the same price again in the future?*\n\nNo, because there may be aversive qualities of a task, or fun qualities of a task, that scale upward or downward with repeating that task.  So the price that makes your inner voices feel cheerful about doing something once, is not necessarily the same price that makes you feel cheerful about doing it twenty times.\n\nAlso in general, any time you imagine feeling *obligated* to do something, you have probably missed the point of the Cheerful Price methodology.\n\nThat said, you should probably *check* to see how how you would feel about repeating the transaction - it might turn up a hidden sense of \"I'll do this for you once because I'm your friend\", where your friend was hoping to pay you enough that they weren't expending social capital at all.  Similarly, you might want to check \"How much would I charge this person if they weren't my friend?\", not because your Cheerful Price for one person *has* to be the same as your Cheerful Price for somebody else, but in case your brain's first answer was mostly the friendship voice glossing over real costs that the other person is actively requesting to compensate you for.\n\n*Q:  I question the whole concept of a Cheerful Price between friends.  I don't think that's how friendship works in the first place.  If I'm willing to bake you a cake because I'm your friend, bringing money into it would just make me feel icky.  If it was more money I'd just feel ickier.*\n\nYou have mentally arranged your friendships differently from how I arrange them!  But your feelings are also valid, and you should clearly signal them to anybody who starts talking about \"cheerful prices\" at you.  Tell them explicitly that's not how friendship works for you!  They offered you a Cheerful Price in the first place because they wanted you to be happy.  They *don't* want you to feel icky.\n\n*Q:  Just reading all this already made me feel icky.  When I bake you a cake, you're not* expending social capital, *we're being* friends *-*\n\nIn that case, you should not read the rest of this post.  It's a cognitive hazard to you.  Leave immediately.\n\n...\n\nUh, are they gone now?\n\n*Q:  Yeah, they're gone.*\n\nReally?\n\n*Q:  No, they're still reading, but now with an additional sense of offense that you think they're too low-status to withstand the weight of your words.  Obviously, only low-status people could possibly be damaged by reading something.*\n\nSigh.  There are many things I wish I could unread, cough-Gray-Boy-cough.  \"Just stop reading things that are damaging you\" is an important life skill which has commonalities with \"Don't leave your hand on a hot stove-plate if that hurts you\" and \"Speak up when people are touching you in ways you don't like.\"  \n\n*Q:  Fine, but there's nothing more you can do at this point to warn them off.  So, what would* *you actually say to somebody who claims that, when they bake you a cake, you're not \"expending social capital\" to get the cake, because them doing you a favor can actually strengthen your friendship?*\n\nI'd try to explain that economics is about \"limited resources\" rather than, I don't know, things that are easy to quantify, or things that are standard and interchangeable, or whether people feel like they're losing something in the process of a trade.  The fact that somebody won't willingly bake me an *infinite amount of cake* is enough to call that a limited resource, even if they didn't feel bad or lossy about baking one cake.\n\nAnd that finite cake limitation is enough to make *me* worry about what I'm losing when I ask a friend to bake me just one cake, even if they don't feel bad or lossy about it the first time.  Because I'm the kind of person who ends a computer RPG with 99 unused healing potions, that's why.  And because I grew up relatively isolated, and I don't have a good sense of how much I'm losing when I ask somebody to bake me a cake.  I don't trust my ability to read someone's reactions if I ask them to bake me a cake.  I don't trust my ability to judge whether that will strengthen the friendship or weaken it.\n\nSo in reality, I'm not very likely to end up friends in the first place with somebody who's made sad by my asking to quantify the cost to them of baking a cake.  I can't tweak my state of mind far enough to encompass their state of mind, or vice versa.\n\n*Q:  Okay, but as you admit, some people, maybe even most people, would rather not put financial prices on things at all, in their friendships.  They'd rather just do favors for each other without a felt sense of trying to keep track of everything.  Why did you claim back up top that those feelings were valid, but less valid than yours?*\n\nI was speaking mostly tongue-in-cheek.  But in fact there are [coherence theorems](https://arbital.com/p/expected_utility_formalism/?l=7hh) saying that either you have consistent quantitative tradeoffs between the things you want, or your strategies can be rearranged to get you strictly more of everything you want.  I think that truly understanding these theorems is not compatible with being horrified at the prospect of pricing one thing in terms of another thing.  I think that there is a true bit of mathematical enlightenment that you get, and see into the structure of choicemaking, and then you are less horrified by the thought of pricing things in money.\n\n*Q:  Fine, but why is it not valid to let people go on feeling whatever they feel without demanding that they be enlightened by coherence theorems right now at the cost of doing violence to their emotions?  Who's to say they're not happier than you by living more the life of a human and less the life of a paperclip maximizer, while both of you are still mortals in the end?*\n\nWell, sure?  Hence it being \"mostly tongue-in-cheek\" rather than \"slightly tongue-in-cheek\".\n\n*Q:  Despite your pretended demurral, I get the sense that you actually hold it against them a bit more than that.*\n\nFine, to be wholly frank, I do tend to see the indignant reaction \"How dare you price that in money!\" as a sign that somebody was brought up generally economically illiterate.  I mean, if somebody says, \"Sorry, I haven't attained the stage of enlightenment where explicitly exchanging money stops making me feel bad\", I'm like, \"Your feelings are valid!  I'm still human in many ways too!\"  But if they say, \"There are some things you can't put a price on!  How dare you!\", I'm like, \"This low-decoupling indignation engine will probably have a happier childhood if I rudely walk away instead of trying to explain how the notion of a 'price' generalizes beyond things that are explicitly denominated in money.\"\n\n*Q:  On a completely different note, I worry that the notion of a Cheerful Price is unfair to people who start out poorer, because it will take less money to make them feel cheerful.*\n\nGenerally speaking, when I ask somebody to name a \"cheerful price\", I'm trying to prompt them into naming a *higher* price so that I can avoid the fear of ouching them and/or do more transactions with them in the future.  Giving people more money is rarely less fair to them?  But if I try to probe at the implicit sense and worry of \"unfair\" that you're raising as a concern, I might try to rephrase it as something like:\n\n\"If you tell somebody they have to accept as fair the least price that makes them cheerful, they might accept a lower price than they could have gotten - a price that would be an uneven division of gains from trade, or a price below the going market rate - and this is more likely to happen to people who start out poorer.\"\n\nAnd I agree *that* would be unfair.  If you can get more for your skills or your goods by going above the lowest price that makes you comfortably cheerful, go for it.\n\nThat said, if I'm asking everybody in the room their Cheerful Price to do my laundry, and the poorest person present names the lowest Cheerful Price, I think that's... actually everything working as intended?  The effect is that the person with the lowest prior endowment is the one who actually gets the money and feels cheerful about that; and the Cheerful part means they get *more* money (I hope) than if I asked everybody present to name their price without specifying the Cheerful part.\n\nMy current cheerful price for \"Please write me a short story about the following\" might be above $10,000 today.  In 2001 it might have been $100, back when $100 was 1/20th of the cost of the car I was driving.  The end result of this 10,000% increase in how much money it takes to make me happy, as I've accumulated more money... is that now people who'll write you a story for $100 get your money, instead of me.  That seems a good phenomenon from the standpoint of financial equality; it causes money to flow from people who have more money towards people who have less money.\n\nBut on a more personal level, if I ask someone to name an amount of money that makes them feel cheerful and energized, I expect and hope that this causes *more* money to flow from me to them.  If the technically defined \"cheerful price\" is less than the person otherwise thinks they can get from a payor, then by all means, they should tell me:  \"Don't worry!  I set my standard fee of $X high enough that I already feel cheerful.\"  And then I won't feel worried about paying too little and everything will be fine.\n\n*Q:  Technical objection:  Surely if you're asking everybody in the room to name their Cheerful Price for something, you should pay the lowest bidder the second-lowest bid, not pay the lowest bidder their actual bid?*\n\nUhhh... possibly?  I'm not actually sure that this logic works the same way when you're asking people for Cheerful Prices - I think you're already asking them to nudge the price upwards from \"the lowest they'd accept\", which means you don't have to give them the second-price of the auction in order to ensure they get any gains from trade.  It's a more friendly idiom in general - you're asking them and trusting them to tell you the truth about what won't make them say \"ow\".  And despite that example of the laundry, the whole thing seems more useful for individual interactions than auctions?  But you may still have a point.  I'll have to think about it.\n\n*Q:  Now that I think about it, this whole document seems to be written like the cheerful price is always something that the payor requests the payee to name.  Why would it always be like that?*\n\nI wrote from that perspective because it's the perspective I usually occupy - at this stage of my life, I'm usually looking for more ways to trade away money for what I want, not looking for more ways to trade away other things for money.\n\nAnd since not everybody can afford to offer us our Cheerful Price, in interactions between friends, there's nonzero reason to not just rush ahead and name a cheerful price before being asked.  Or to avoid misunderstandings and possible resentment about you expecting too much, you should say \"This is my cheerful price\" rather than \"This is my price\", if the other person didn't already explicitly say \"Please name your cheerful price.\"\n\nBut sure:  Some people who are in the habit of unilaterally underpricing themselves, to the point where they're undertrading, even though they have pricing power to ask for more, might do well to take the initiative on their side to think \"What would I need to be getting to make me cheerful about more interactions here?\"  And this is true whether the trade is for money, or not.\n\nOr some people who are in the habit of underpaying, and not getting all the things they want, even though they have more to offer, might do well to think, \"Would I still feel cheerful about paying much more for X, if I got more or faster or better X?  Is there some way to give more and get more?\"  And this is true whether the trade is for money, or not.\n\nSome people may benefit from switching perspectives to ask about cheerful prices with respect to some *internal* bargains between the voices in their head; to apply the same perspective to one-person transactions, not just two-person transactions.\n\nBut to repeat the warning from above:   If you have nothing more to give, or no pricing power to ask for more; then thinking about what it would take to make the bargain cheerful, may only make you sadder.  And this is true whether for outer bargains, or inner bargains.\n\nLife does not promise us that we will always get our Cheerful Prices, even from our friends, and not even from ourselves.  Not every trade produces so much gain to divide, even among many good trades worth making.",
      "plaintextDescription": "There's a concept I draw on often in social interactions.  I've been calling it the \"happy price\", but that is originally terminology by Michael Ellsberg with subtly different implications.  So I now fork off the term \"cheerful price\", and specialize it anew.  Earlier Facebook discussion here.\n\nTl;dr:\n\n * When I ask you for your Cheerful Price for doing something, I'm asking you for the price that:\n   * Gives you a cheerful feeling about the transaction;\n   * Makes you feel energized about doing the thing;\n   * Doesn't generate an ouch feeling to be associated with me;\n   * Means I'm not expending social capital or friendship capital to make the thing happen;\n   * Doesn't require the executive part of you, that knows you need money in the long-term, to shout down and override other parts of you.\n * The Cheerful Price is not:\n   * A \"fair\" price;\n   * The price you would pay somebody else to do similar work;\n   * The lowest price such that you'd feel sad about learning the transaction was canceled;\n   * The price that you'd charge a non-friend, though this is a good thing to check (see below);\n   * A price you're willing to repeat for future transactions, though this is a good thing to check (see below);\n   * The bare minimum amount of money such that you feel cheerful.  It should include some safety margin to account for fluctuating feelings.\n * The point of a Cheerful Price, from my perspective as somebody who's usually the one trying to emit money, is that:\n   * It lets me avoid the nightmare of accidentally inflicting small ouches on people;\n   * It lets me avoid the nightmare of spending social capital while having no idea how much I'm spending;\n   * It lets me feel good instead of bad about asking other people to do things.\n * Warnings:\n   * Not everybody was raised with an attitude of \"money is the unit of caring and the medium of cooperation\" towards exchanges with an overt financial element.  Some people may just not have a monetary price for some things, su",
      "wordCount": 5084
    },
    "tags": [
      {
        "_id": "chuP2QqQycjD8qakL",
        "name": "Coordination / Cooperation",
        "slug": "coordination-cooperation"
      },
      {
        "_id": "AADZcNS24mmSfPp2w",
        "name": "Communication Cultures",
        "slug": "communication-cultures"
      },
      {
        "_id": "PDJ6KqJBRzvKPfuS3",
        "name": "Economics",
        "slug": "economics"
      },
      {
        "_id": "5A5ZGTQovxbay6fpr",
        "name": "Fairness",
        "slug": "fairness"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cmiRk9XtT9Psnd3Yr",
    "title": "Movable Housing for Scalable Cities",
    "slug": "movable-housing-for-scalable-cities",
    "url": null,
    "baseScore": 124,
    "voteCount": 52,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2020-05-15T21:21:05.395Z",
    "contents": {
      "markdown": "*First posted* [*on Steemit*](https://steemit.com/startup/@eliezeryudkowsky/movable-housing-for-scalable-cities) *on July 30, 2016.*\n\n* * *\n\n0: Summary.\n-----------\n\nThe non-scalability of current major cities poses a challenge to economic growth; it is a burden that falls disproportionately on those least able to bear it.\n\nUnfortunately, any would-be startup city must confront the network effects that pose a tremendous potential energy barrier to the attractiveness of a city that isn't yet mature. From day one, you need to compete with the hedonic attractiveness of San Francisco, in the eyes of the person who decides where to locate the company.\n\nMovable houses mated to modular foundations – nice large customized houses, not small ugly trailers – can make cities more scalable *and* more hedonically attractive:\n\n*   By making it easier for groups to relocate and untangle themselves in a coordinated way, movable houses can increase the natural dispersion of the city as it grows larger.\n*   Since people could buy and retain customized houses manufactured with economies of scale, movable houses could have better technology and amenities not available today. (Contrast your current house to a modern car.)\n*   Movable houses could allow unprecedented opportunities to live next to your friends, or to groups of similar-minded people. You and your friends just need to find a set of available modular foundations close together.\n*   Movable housing might help on key points of governance and realpolitik, for example by making land value taxes more attractive, and decreasing exit costs.\n\n*(This was written in response to Y Combinator's* [*request for ideas*](https://blog.ycombinator.com/new-cities) *about the design of new cities.)*\n\n1: Introduction: The challenge of scalable cities.\n==================================================\n\nManhattan doesn't scale well. San Francisco doesn't scale well. Tokyo and London don't scale well. On the present system, each added resident comes with some tiny added degree in inconvenience for every other resident already living in a city, most notably in the form of traffic jams and housing prices. Yes, every added programmer also provides some tax revenue; but the city infrastructure, qua infrastructure, is not scaling well.\n\nThis is not *all* due to anti-housing housing policy, though we've certainly gone to every possible length to shoot ourselves in that foot. Limited road bandwidth is a real issue. The air in Manhattan is genuinely dirtier than in Berkeley – I acquire a sore throat every time I visit New York City.\n\nBuilding cities that can scale better is a problem of overwhelming human importance. Some analyses have suggested that increased housing costs have eaten huge chunks out of middle-class income; that housing costs may be responsible all on their own for the stagnating middle class. The costs and inconveniences of our most active and growing cities, prevent people in less active economic areas from seeking better fortunes. Cities that don't scale are problems measured in trillions of dollars of lost economic growth and a crushing amount of real human despair.\n\n1.1: All-robotic car fleets are an obvious first step.\n------------------------------------------------------\n\nThe technology for an all-robotic car fleet is probably already available... if we take all the non-robotic cars off the road.\n\nA robotic car fleet makes cities more scalable in many, many ways. The most obvious boost is widening space, compressing time, and removing the cost in stamina to travel. Robotic cars that don't need stoplights or speed limits might let you get anywhere within 6 miles in 10 minutes. (Say, 2 minutes to get to a thoroughfare, 6 minutes to go 6 miles at 60mph, and 2 minutes to go the last mile.) A 6-mile radius is a *lot* of territory (113 square miles). You can be 10 minutes away from a *lot* of friends.\n\nRobotic cars lend themselves to fleets that show up when needed, rather than individually owned cars. If every individual car is being used much more regularly, that changes the cost/benefit calculation for batteries versus gasoline. Which implies less pollution per added resident in concentrated populations. Fewer parking lots implies more space for people, and so on.\n\nBut robotic cars, by themselves, may not be enough. Designing a new city is the *last* place you want to stop and congratulate yourself on the improvements you already have planned.\n\n1.2: The first business challenge is being hedonically attractive to corporate decisionmakers.\n----------------------------------------------------------------------------------------------\n\nCities run on network effects. Moving to a location with 1000 companies that might want to hire you is far less risky to moving to a location with 3 companies that might want to hire you.\n\nAny new city faces a *huge* potential energy gradient during its youngest and most vulnerable stages. The gravity of Manhattan and Tokyo and London is absolutely enormous, drawing away your potential residents. You need to offer something *extremely* attractive about your new city if you want companies to move there instead of San Francisco. And your city's first critical business challenge *is* about attracting companies, not humans; humans go where there are jobs.\n\n\"But we'll have lower housing prices!\" you cry. Alas, it's a sad fact of life that the people who decide where to locate their companies face different incentives than the average employee in those countries. (As in the Dilbert cartoon where the CEO is moving the company to what happens to be his old hometown, because that way he can get free babysitting from his grandparents.) I once asked a Google employee why Google didn't start an offshoot campus somewhere with lower housing costs – surely, I ventured, Google had the scale to do that, and enough employees who dispreferred city living. My friend replied that no project manager at Google would want to be so far from the center of corporate politics.\n\nThe cofounder who decides to locate their hot startup in San Francisco may have plenty of reasonable and selfless motives to do so – better access to venture capital, better access to programmers. But it seems worth noticing regardless that this decision is being made by someone who can probably afford more Ubers and higher rents than other company employees. If the company did locate itself somewhere cheaper, it could perhaps pay lower salaries and so capture *part* of the gains from lower housing costs. The decisionmaker who decides where to locate the company may capture a *part* of the company's gains from those lower salaries. But quantitatively speaking, those fractions do not multiply out to 100%. The incentives are not aligned between the decisionmaker and the rank-and-file.\n\nThe overwhelming proportion of the *real human good* accomplished by a more scalable city comes in the form of schoolteachers who can afford the rent. But if you want that city to boot up successfully, you need to think about how to make it *personally, hedonically* attractive to the decisionmakers who choose where to put their companies.\n\nRobotic cars help here too! Even compared to taking a private car through traffic, taking a robotic car through an absence of traffic might be more attractive to a CEO.\n\nSlap some flying drones riding the cars, to take off and make deliveries as the destination approaches, and you can have amenities that non-robotized cities just don't have! If delivering hot meals across a 3-mile radius became truly cheap and ubiqitous, so that even non-CEOs could afford it, then we would see much greater specialization on selling meals. Meal-creation businesses would compete on cost and healthiness, instead of restaurants competing on location and dining experiences. It could become typical to order a not-so-expensive, non-high-calorie, home-cooked-style hot meal that would have taken you an hour to make from scratch, and have it delivered in 10 minutes. Maybe the CEO of a *big* company has a home chef who can compete with that – but it's an attractive amenity for the CEO of any middle-sized company or hot startup.\n\nAgain, the vast majority of real good comes from giving the thousands of *other* residents access to healthy food without a huge cost in personal labor. But you have to appeal to the decisionmakers before anyone else gets that chance.\n\nHaving remarked on this cool and futuristic amenity, we are still in no position to relax and declare ourselves done. We need every possible attractive feature for our shiny new city to exert a draw that is remotely competitive with Manhattan.\n\n1.3: Inside or outside the US?\n------------------------------\n\nI will digress at this point – though it will turn out to not really be a digression – to consider the possibility of locating the New City outside the United States (and outside the UK and the EU and Australia).\n\nBy far the hugest attraction of this possibility for cofounders would be EASIER IMMIGRATION. If your city is otherwise competitive in terms of first world amenities and safety, has lower housing prices, and cofounders can bring people there without Eternal Visa Hell, startups will flock to you that *couldn't* form anywhere else.\n\nAvoiding regulatory molasses will be another huge attraction for some companies and potential companies. Cough cough PATENT MONSTERS cough cough.\n\nIf the new city were located in, say, a special district negotiated with the government of Uruguay, this might permit some basic utopian legal features. For example, not living in a country where marijuana is theoretically illegal if you're white and rich, and potentially life-destroyingly illegal if you're black or poor. For many cofounders or company-owners, this is a point of principle rather than practice; but to some people it is a matter of practice. Or some people, with decision-making power even, may dislike at a deep emotional level the prospect of living in a country where a SWAT team can kick down their door at any time.\n\nThe United States is unique among nations in insisting that it can tax the income of its citizens even if they live and earn outside the US. But the tax policy surrounding your city still matters to anyone who isn't a US citizen, or to any US citizen earning not much more than $92,000 per year.\n\nAct 20/22 in Puerto Rico enables people moving to Puerto Rico for the first time to negotiate a two-decade contractual exemption from US *federal* income taxes. Puerto Rico is just about the only place on Earth, inside or outside the US, that a US citizen can go to not pay US federal taxes. For this reason I'm fond of saying that the three obvious places to start a new city are 2 hours from the Bay Area, Puerto Rico if you're going to be located in the US but not near any particular existing metropolitan area, and Uruguay (hurricane and earthquake free, extremely sensible and stable government). Not paying federal taxes in Puerto Rico would be a hugely favorable attractive gradient – for the decisionmakers that locate companies, in particular, but heck, even for programmers making a dinky $150K per year.\n\nInternational development has its own negatives. Most obviously, the extra barrier posed by being a *lot* further away from existing centers of gravity; compared to starting up 2 hours from the Bay Area, or in coastal Oregon, or Nevada. It's one thing to live a 2-hour drive away from venture capitalists, and quite another to be separated from them by a 6-hour plane flight.\n\nThere may also be language issues – especially if your company needs to interact with government agencies whose employees don't speak your language. Google Translate will only continue to improve in the future, lowering language barriers further; but the technology isn't there *right now* to render international communication frictionless.\n\nThese barriers will be particularly fatal during the early stages, when the potential energy barriers surrounding your city are the highest.\n\nWhat we *really* want – one might think – is to *initially* start up our city 2 hours from the Bay Area. But to have some mysterious feature that made it *unusually easy* for many of the city's inhabitants and whole companies to relocate to coastal Oregon, to Puerto Rico, or to Uruguay, once the system had been proven.\n\nEspecially if this mysterious new feature also improved the hedonic attractiveness, the cost profile, and the fundamental scalability of our startup city...\n\n2: Movable housing.\n===================\n\nLet's assume at the beginning that we're talking about an early town small enough to be composed mainly of houses instead of apartment buildings.\n\nImagine that in this town, houses are portable objects with standard connections that match up to standard modular house-foundations. We can disconnect a house from its water and electricity and Internet cables, have a standard vehicle pick the house up from its foundations, and gently drive the house over to somewhere else inside the city.\n\nWe could also imagine apartments that slot into towers on rails. We can imagine office buildings built up from modules that could be taken apart at need. But as we'll soon see, this new city might be able to scale a lot further before it *needed* apartment buildings and office towers.\n\nMovable housing would require some amount of new technology, and more importantly, a green-field city – the roads need to take the weight (unless we can replace or supplement carriers with skyhooks or other lifters); the robotic-vehicle idiom would also help, because it means we can clear any road of cars as a house-carrier is passing. I'm pretty sure all of this is within reach of human technology; the question is whether it's too expensive.\n\nAnd whether anything is 'too expensive', of course, depends on what benefits you're buying.\n\nBenefit #1: You can live closer to where you most want to live.\n---------------------------------------------------------------\n\nIn the world today, when you want to move to a new city or district, you hunt around until you find a house that is shaped *sorta* like you want a house to be shaped, which isn't located *too* far from where you wish you actually lived. And all of your friends and coworkers – network effects will shortly become very important here – are implementing that same process.\n\nMovable housing decouples the problem of finding a good *location* from the problem of finding a good *house*. It disaggregates the two businesses, [as my father would say](http://amzn.to/2azlmAB).\n\nTo find a nice place to live, you just need to find any available plot whose modular foundations match your current house. (There'd better not be more than three varieties of foundation, though, or we're back to the same old matching hell.)\n\nBenefit #2: You can own a better house.\n---------------------------------------\n\nA modern car contains vastly more interesting technology than a modern house. That's because there are big-company car manufacturers with centralized factories and R&D departments competing to offer the car that you'll like the best.\n\nImagine if instead, a boutique car-constructor company sauntered over to a parking space and built a car there over a year or so. And then you looked around for an available parking space that wasn't too far from your house and had an associated car that was acceptable. If no car was good enough, you could buy a parking space and its car, trash that car, and have a different car handcrafted in that parking space over the next year by the small boutique car company of your choice.\n\nCars would be a lot less advanced, to put it mildly.\n\nYou wouldn't realize what you were missing. You'd never have *seen* cars with electronic stability control and automatic parallel parking and collision alarms and radio keyfobs. Maybe some rich people would have hand-constructed cars with primitive versions of those features, to go with the gold-plated ashtrays.\n\nSo if you can buy a house from a company that has a real R&D department and is actually trying to please the customer and has real competition… well, I'm not an entire R&D department myself. But my personal starting list of feature requests for an Apple iHouse might include:\n\n*   *Completely* blackout shutters over my windows, which open gradually and automatically at the time of morning when I actually wake up.\n*   Noiseproofing. Extremely serious noiseproofing, with thicker walls, better-sealed doors and windows, active sound cancellation, and maybe white noise generation if that still wasn't enough. When I close my door I want *silence*.\n*   Centralized, extremely powerful LED lighting modules with the light carried by optical fibers to whichever room I was actually using at the time. Dimming to red as my bedtime approached, of course.\n*   Centralized humidification with per-room control plus per-room temperature control, so I'm not running a loud hot humidifier in my bedroom at night. (I need a lot of humidity, personally.)\n*   A Jacuzzi on the roof, open to the stars. (You think this is some kind of huge luxury and that I'm spoiled for even wanting it, but that's because you live in a civilization where everything house-related costs too much.)\n*   Anti-insect laser bug-zappers on the roof which fry any mosquitos, wasps, hornets, other stingy things, or even houseflies that get too close. (Yes, I know we're all supposed to be too tough to be bothered by little things like insects; just like there was a time when British people made fun of people who carried umbrellas on drizzly days. I observe that you do, in fact, work inside an office instead of outdoors. I bet those little annoyances you're supposed to be too tough to care about have an awful lot to do with that cultural decision in practice.)\n\nFinally, bigger rooms! More storage space! Relatively empty square meters in a house just should not cost that much on the margin!\n\nExtra marginal people impose costs on cities. Electricity costs money. The central LED lighting modules and Powerwall batteries will cost money. An extra square meter of floor or ceiling, even an extra story on a house, should not cost that much more on the margins.\n\nIt does feel to me like the *small, crowded* apartments are an arcane penance that we are imposing on ourselves. Even people who don't like expanding cities, ought to see more people as being the issue, not more square meters of living space; having more square meters per person seems like something our civilization should be able to manage. Our ancestors lived in bigger houses. We really ought to be able to get that back.\n\n(This does mean tolerating higher roofs, or larger apartment-tower frames for modular apartments on rails, or less grass on the hill. Or living further away with lower local population density; see Benefit #4.)\n\nSome of us have chosen to live in large house-complexes with friends (though it's not obvious how much of this is just due to modern housing costs). Imagine a central 'house' that has the kitchen and the game room and the Jacuzzi, and six modules radiating out from it, each module with a bedroom, bathroom, storage space, and small reading room. Want to try out a different group house for a month? Pick up the bedroom module, connect it to a different house, put it back down again.\n\nYour own wishlist would look different, I expect. And in another 5 years the Apple iHouse 4e would offer us features that neither of us imagined. But one point is sure: under the present system of the world, we'll never get a chance to find out.\n\nBenefit #3: Friends living near each other.\n-------------------------------------------\n\nAll sufficiently tight clusters of friends can find a set of empty foundations, rent them, and live literally next door to each other.\n\nWe could have neighbors again.\n\nWe could have *tribes* again.\n\nYes yes, I know, you're thinking of that one person who's friends with a bunch of your friends but who you don't want on the same block as you. But really, human beings have been living in tribes for a *while.* You can probably work it out. Many people could benefit a *lot* from living in close proximity to fifty people whose company you can tolerate – not necessarily your coworkers, even; you might be choosing to live next to your fellow D&D 3.5e roleplayers who also like dubstep.\n\nIt could give us back something human that we lost, and start to make headway against lives that are *sad,* never mind the poverty.\n\nThis notion will admittedly create some coordination problems: \"Does *everybody* in the block need to be in our group for us to have the kind of... parties... that we want?\" and \"How do we try to make sure that a lot of foundations go empty at the same time so a new group can move in?\" and \"Uh, what if 60% of the group prefers somebody *not* move in to their cluster, is that going to be enforceable at all and doesn't that imply a whole new level of local government?\"\n\nI can think of some obvious policies to try, but I have a strong suspicion that policy v.1 won't work all that well and it will take until policy v.3 before work tolerably well. Sometimes you just gotta try things.\n\nBenefit #4: Small towns can scale better.\n-----------------------------------------\n\nAnd not just to larger sizes, but to higher levels of economic activity.\n\nThis is the prediction that I'm least sure about. But one of the reasons why I'm talking about movable *houses*, instead of modules on rails in a huge apartment-tower frame… is that maybe, a lot of us won't even need the huge apartment-frames?\n\nMaybe you, dear reader, positively prefer to live in a huge apartment building. Maybe you don't *care* where a robotic car can take you in 5 minutes. Maybe what you really want, more than any other style of life, is to be able to walk out onto your sidewalk and see lots and lots of human faces (even unfamiliar ones), then turn right and walk into that lovely coffeeshop that is downstairs and half a block to the right of your eighth-floor apartment. Maybe you like the hustle and the bustle. There is no arguing with terminal components of the utility function, as David Hume observed.\n\nSome of us – and I'm not saying we have better preferences, but we have *different* preferences – some of us would rather live in Rivendell.\n\nOr failing that, we'd like to live in a quiet little house on a green hill, where the technologically advanced soundproofing doesn't need to work that hard.\n\nIf we had robotic cars *and* movable houses, it would be a lot easier for all of us who work at my nonprofit to *not* need to live downtown – not even in downtown Berkeley. Part of the reason we located in downtown Berkeley is that it has sufficient density of housing that we originally could, and new employees still can, find available non-horrible apartments within walking distance of the office.\n\nBut in a city with robotic cars *and* movable houses, we could perhaps all live in the *same* section of green hills, 10 miles or 20 miles from the skyscrapers, and move our office modules there too. Robotic cars could teleport us to the large, centralized supermarket that served a big area and therefore had just as much selection as a supermarket in a big city. Or some of us might live closer to the skyscrapers, or with the Burning Man tribe; and for those who made that choice, a robotic car would take them to the green hilly workplace in 24 minutes while they browsed their cellphones.\n\n24 minutes isn't far from the time it takes to cross San Francisco right now! Anyone who considers that good enough could live anywhere within a 20-mile circle, served by robotic cars with no traffic lights cruising at 60mph down the central throughways.\n\nEven so, some offices and some people would need to be closer to the center of gravity, or would just yield in preference to the siren call of network effects. Sometimes there are just too many graph links that all need to be located within 5 minutes of one another. Those companies might still need to live in office modules slotted into towering office-complex-frames at the New City's center.\n\nBut if you can coordinate locations more cheaply, move more cheaply, and travel more cheaply because of the robotic cars, then it becomes a whole lot more feasible to have *more* software companies located in the quiet hills. It's not a panacea and it won't work for every organization, but *more* people will be able to live in real houses like our parents owned.\n\nEven more of us will be able to live in Rivendell when virtual reality tech matures, which will loosen (though not cut) the bonds of spatial distance that much further. Or VR might not really make any pragmatic difference, but it's worth trying to think 5 years ahead at least; the headsets *will* improve. It will be one more marginal force exerting a bit more quantitative push towards the attractiveness of living in a bigger house in a quieter place, if your new city can offer that with fewer than usual disadvantages.\n\nBenefit #5: Land value taxes.\n-----------------------------\n\nEconomists since Adam Smith have observed that land is the ideal thing to tax. Literally, tax the square meters of planetary surface. We can't make more land, so it's not like a land tax discourages the production of land, the way that income taxes discourage work. *Somebody* is going to collect the implicit rent on land; so long as the relevant collector doesn't tax more than the price point at which the supply of land balances the demand for land, the tax doesn't change that price. It's a frictionless tax on a HUGE flow of land rent, and the *alternative* to taxing that huge flow of land rent, is frictionful taxes. Taxing almost anything other than land (except carbon dioxide emissions), *before* taxing land, is one of those insanely stupid aspects of modern civilization that make economists want to stab themselves to death with a butter knife.\n\nWith movable housing, the houses are moving around and the foundations are staying in one place. This makes it even more obvious that you might as well have the rent on the foundations supporting government services.\n\nI pay $2500/month on my little 2-bedroom apartment in Berkeley, the supermajority of which is certainly land rent. That land rent is probably more than I pay in state and federal income taxes. Which makes me want to gouge out my eyes with a spoon. Because – no offense to my innocent landlord who paid a corresponding price for that land and is not earning an excess profit in on it after mortgage costs – I am paying *two* huge taxes where an even slightly saner system could be charging me *one* tax.\n\nYes, you can see how it *might* go wrong if one government tries to own all the modular foundations and therefore all the urban land. (The more so if there are no competing cities: see benefit #6.) But the economic factors here are huge enough that it seems worth trying to do things the sane way just once.\n\nSince most taxes are federal, making real headway on eliminating the 'double tax' (income tax plus land-rent) might require locating in Puerto Rico or Uruguay. But even without that, to whatever extent the modular foundations have a natural equilibrium rent, that rent can provide for fire trucks, maybe even health care if the land rents equilibrate high enough – without *additional* taxes. Where, again, the current model is to have the residents pay one stream of highly frictional tax-rent to the government, and an entirely separate stream of land-rent.\n\nThis isn't really a technology problem. But having houses moving around, so that the rent collected on the stolid immobile foundations is entirely separate from any handcrafted structure nailed to them, makes the solution that much more obvious and maybe more politically feasible.\n\nBenefit #6: Exit threats and political relocations.\n---------------------------------------------------\n\nWhen Patri Friedman proposed 'seasteading', a lot of the hoped-for good systemic properties came from the fact that sea-based platforms would be easier to move around. Movable housing can be seen as trying to get several similar systemic properties, without taking on the engineering or political challenges of building at sea.\n\nThis analogy extends over to one of the primary political ideas motivating seasteading, the notion of \"voice and exit\".\n\nWhen your [BATNA](https://en.wikipedia.org/wiki/Best_alternative_to_a_negotiated_agreement) to staying in a place is to pick up your houses and/or your office modules and move them somewhere else... then that creates a different negotiating position than when you need to destroy your painstakingly handcrafted structures, create new handcrafted structures somewhere else, rearrange all of your personal possessions, etcetera.\n\nYes, there's still friction – you have friends who may not follow you, maybe your kids end up going to a different school. But there's *less* friction with movable houses, the coordination problems are that much easier to solve in groups; it could make a quantitative difference.\n\nThis improved BATNA could apply at the level of a whole city that negotiated a special economic zone with some state or country. Maybe it's wacky to think that \"Kay thanks bye\" would ever be a plausible reply on that scale if the host country tries to \"alter the bargain, pray I do not alter it any further\". There might just be too many non-movable objects creating too much inertia. But even having a large *fraction* of the potential victims, having the option of putting their movable modules on a cargo ship and heading elsewhere, could make a difference. It matters quantitatively to a victimizer whether making conditions worse for their victims means that 20% of their victims leave or 2% of their victims leave.\n\nI'm trying not to go on too much of a rant here. But one of the enormous overlooked questions of the modern age is how poverty still manages to exist, when agricultural and economic productivity have risen by a factor of literally 100 since the time when 98% of the population was farming. We have *fewer* poor people, to be sure, the life of the lowest income quartile is a *lot less* horrible than it was in the 13th century. But there's still some sense in which it seems a little *embarrassing* to imagine going back to a world where people managed to survive despite being 100 times less productive, telling them that we are now 100 times wealthier, and then having to explain why there are any horribly poor and desperate people in our country *at all*.\n\nWhen a condition is that sticky, we should suspect it to be an equilibrium with strong restoring forces. There must be some powerful factor that makes some people be poor, no matter how much wealth is flowing around – a factor that gets stronger as more wealth flows, even by a factor of 100.\n\nOne of the obvious forces that could be stabilizing a Poverty Equilibrium is if the standard state of affairs, for human civilizations in general, is for there to always be a few groups here or there that can extract a little more value. The Ferguson Police Department, issuing 3 warrants per household per year, is one obvious example of this idiom. But you should also be thinking of taxi medallions, licensed haircutters, NIMBY house-owners, and health insurance companies without much statewide competition. I don't mean to single out one group as a target for the Two Minutes Hate. There can just be these endless small sets of local factors with the power to drain one more dollar; and these factors will collectively go on draining one more dollar until they can't drain any *more* dollars without some victims dying. Actually, the equilibrium for multiple extractive forces is a [commons problem](https://en.wikipedia.org/wiki/Tragedy_of_the_commons) – Alice knows that if she doesn't steal a dollar from your pocket, Bob will steal it instead, so Alice might as well steal that dollar even if the result is disastrous. Which means that in many cases the little extractions *do* continue past the point where people riot.\n\nThis is one reason I'm skeptical of the ability of a Guaranteed Basic Income to solve poverty in general, leaving aside various other technical problems. We increased economic productivity by a factor of 100 and there are still poor people. Is a GBI really going to be the last marginal improvement that solves it all? A GBI might still *help –* just like increasing economic productivity by a factor of 100 *helped* the people who are still living lives of awful suffering and desperation. But after you introduce a GBI, I'm guessing, there will be a number of factors that start to extract one more dollar here, one more dollar there. The Ferguson police department issues another arrest warrant per household, the state increases its court costs, hey, people can afford it now, they've got a GBI right? And what do you know, almost everyone will still have to get awful jobs just to survive.\n\nSo it's not at all a side issue, or a mere bugaboo of the independent-minded, to think about the political power of a cheaper exit. To consider whether mature VR, and to a lesser extent, movable housing, might make it a little bit harder to extract value from victims anchored too solidly to run away. The mobility of labor might affect how fast the poverty equilibrium restores itself.\n\nI'm not saying that corporate taxes are the correct level of organization on which to have any tax at all... but it does happen to be the case that taxing corporate profits located in your country is very hard to do, at least to large corporations, because they just locate their profits somewhere else. Making individual human beings and small companies more mobile would grant them some of the same power of resistance.\n\nNo, let me be more blunt. If your shiny new city would otherwise be generating a huge amount of excess value for the people inside the new city, and the people inside the city have no credible threat of exit, the people inside your city will not be allowed to keep that value. There are things in the ecology that like to eat free energy, and your city will not be allowed to keep that energy indefinitely if it is so temptingly available for a little more taking every year. It could be eaten by any level of regional government, or any organization empowered by any level of regional government. If you're dumb enough to let somebody patent the connection scheme of the modular foundations, they can let you build out the city, watch to see how much excess wealth is being generated, and then jack up fees to try to capture nearly all of that value. It could be an invasion of patent monsters under a national jurisdiction that permits them. It only takes one factor that can threaten to shut down your whole process, to extract nearly all of the free energy from your city.\n\nSo if your well-meaning goal is to make your new city generate lots of excess wealth that the people inside the city get to keep, you'd better make it as cheap as possible for coordinated groups to leave.\n\nBenefit #7: Lower-frictional flow of people through economies.\n--------------------------------------------------------------\n\nI'll finish by remarking that, in a very generic and boring sense, friction is usually bad for economies and reduced friction is usually good. It is a terrifying sign of stagnation that people in the United States, especially the less wealthy states, are moving house less often. Housing prices are probably a bigger part of that problem than any one-time cost of shuffling possessions. But every little bit helps, and if you reduce the physical cost and emotional wear of moving from Point A to Point B, it will matter *some*. Alleged benefits #3 to #6 mostly reduce to, \"Some nice things might happen if we reduce the cost and friction of being somewhere else\". So benefit #7 is just the generic observation that movable houses would reduce economic friction in a very generic way and therefore other nice things might happen as well.\n\nWe could also see movable housing as a kind of *modularity* that has the same kind of benefits as modularity in code; you can think about fewer things at a time. Time to extend the city? No need to plan Housing Projects and Development. Your job is just to dig a bunch of new foundations, and hook them up to water and electricity and Internet and roads. There, now your city is bigger. Everything else will sort itself out.\n\n3: Conclusion.\n==============\n\nI would put high odds against any of this happening in real life before further events are derailed by a near-lightspeed expanding front of von Neumann machines eating the galaxy (as happens in both good and bad AI scenarios). But if advanced AI does happen to take that long, or if Elon Musk gets bitten by the movable housing bug and makes it happen in two years, I'd enjoy living in Rivendell for some of the remaining time.\n\nImagine living in a higher-tech bigger nicer house, with robotic cars to teleport you wherever, and drones to deliver hot meals. Imagine that the rents not being so damned high – or even that 18th-century utopia where you *just* pay rent instead of rent plus tax – has produced a decrease of economic friction and a corresponding economic boom, so that it's less hard for people to make a living and get by. Is that a little more like that legendary future our parents were promised, of which it is said that instead we got 140 characters? Better steel factories didn't just produce shinier steel, back in the day; people made more tools that required steel, and used those steel tools to make other things. Scalable cities are something in that class. Movable houses, I think, might be a significant incremental piece of something in that class.\n\nTo me, it seems nearly certain that none of that will actually happen. This essay is not a prediction of future glories ahead of us, more of a wistful sigh. If we lived in a civilization where we could have nice things at that complexity level, we'd already have nicer versions of much simpler things.\n\nBut so long as Y Combinator is asking for essays on new cities anyway, this seemed worth writing up. I hope you enjoyed it!\n\nPS: Please pave the sidewalks with the bouncy kind of pavement so that people can run on sidewalks without destroying their knees.",
      "plaintextDescription": "First posted on Steemit on July 30, 2016.\n\n----------------------------------------\n\n\n0: Summary.\nThe non-scalability of current major cities poses a challenge to economic growth; it is a burden that falls disproportionately on those least able to bear it.\n\nUnfortunately, any would-be startup city must confront the network effects that pose a tremendous potential energy barrier to the attractiveness of a city that isn't yet mature. From day one, you need to compete with the hedonic attractiveness of San Francisco, in the eyes of the person who decides where to locate the company.\n\nMovable houses mated to modular foundations – nice large customized houses, not small ugly trailers – can make cities more scalable and more hedonically attractive:\n\n * By making it easier for groups to relocate and untangle themselves in a coordinated way, movable houses can increase the natural dispersion of the city as it grows larger.\n * Since people could buy and retain customized houses manufactured with economies of scale, movable houses could have better technology and amenities not available today. (Contrast your current house to a modern car.)\n * Movable houses could allow unprecedented opportunities to live next to your friends, or to groups of similar-minded people. You and your friends just need to find a set of available modular foundations close together.\n * Movable housing might help on key points of governance and realpolitik, for example by making land value taxes more attractive, and decreasing exit costs.\n\n(This was written in response to Y Combinator's request for ideas about the design of new cities.)\n\n\n1: Introduction: The challenge of scalable cities.\nManhattan doesn't scale well. San Francisco doesn't scale well. Tokyo and London don't scale well. On the present system, each added resident comes with some tiny added degree in inconvenience for every other resident already living in a city, most notably in the form of traffic jams and housing prices. Yes, every adde",
      "wordCount": 6293
    },
    "tags": [
      {
        "_id": "b9FzogZE4pAGfo5bY",
        "name": "Urban Planning / Design",
        "slug": "urban-planning-design"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RQpNHSiWaXTvDxt6R",
    "title": "Coherent decisions imply consistent utilities",
    "slug": "coherent-decisions-imply-consistent-utilities",
    "url": null,
    "baseScore": 156,
    "voteCount": 69,
    "viewCount": null,
    "commentCount": 83,
    "createdAt": null,
    "postedAt": "2019-05-12T21:33:57.982Z",
    "contents": {
      "markdown": "(_Written for Arbital in 2017._)\n\n* * *\n\nIntroduction to the introduction: Why expected utility?\n=======================================================\n\nSo we're talking about how to make good decisions, or the idea of 'bounded rationality', or what sufficiently advanced Artificial Intelligences might be like; and somebody starts dragging up the concepts of 'expected utility' or 'utility functions'.\n\nAnd before we even ask what those are, we might first ask, _Why?_\n\nThere's a mathematical formalism, 'expected utility', that some people invented to talk about making decisions. This formalism is very academically popular, and appears in all the textbooks.\n\nBut so what? Why is that _necessarily_ the best way of making decisions under every kind of circumstance? Why would an Artificial Intelligence care what's academically popular? Maybe there's some better way of thinking about rational agency? Heck, why is this formalism popular in the first place?\n\nWe can ask the same kinds of questions about [probability theory](https://arbital.com/p/probability_theory/):\n\nOkay, we have this mathematical formalism in which the chance that X happens, aka  P(X), plus the chance that X doesn't happen, aka P(¬X), must be represented in a way that makes the two quantities sum to unity: P(X)+P(¬X)=1.\n\nThat formalism for probability has some neat mathematical properties. But so what? Why should the best way of reasoning about a messy, uncertain world have neat properties? Why shouldn't an agent reason about 'how likely is that' using something completely unlike probabilities? How do you _know_ a sufficiently advanced Artificial Intelligence would reason in probabilities? You haven't seen an AI, so what do you think you know and how do you think you know it?\n\nThat entirely reasonable question is what this introduction tries to answer. There are, indeed, excellent reasons beyond academic habit and mathematical convenience for why we would by default invoke 'expected utility' and 'probability theory' to think about good human decisions, talk about rational agency, or reason about sufficiently advanced AIs.\n\nThe broad form of the answer seems easier to show than to tell, so we'll just plunge straight in.\n\n  \n\nWhy not circular preferences?\n=============================\n\n_De gustibus non est disputandum,_ goes the proverb; matters of taste cannot be disputed. If I like onions on my pizza and you like pineapple, it's not that one of us is right and one of us is wrong. We just prefer different pizza toppings.\n\nWell, but suppose I declare to you that I _simultaneously_:\n\n*   Prefer onions to pineapple on my pizza.\n*   Prefer pineapple to mushrooms on my pizza.\n*   Prefer mushrooms to onions on my pizza.\n\nIf we use >P to denote my pizza preferences, with X>PY denoting that I prefer X to Y, then I am declaring:\n\nonions>Ppineapple>Pmushrooms>Ponions\n\nThat sounds strange, to be sure. But is there anything _wrong_ with that? Can we disputandum it?\n\nWe used the math symbol > which denotes an ordering. If we ask whether >P can be an ordering, it naughtily violates the standard transitivity axiom x>y,y>z⟹x>z.\n\nOkay, so then maybe we shouldn't have used the symbol >P or called it an ordering. Why is that necessarily bad?\n\nWe can try to imagine each pizza as having a numerical score denoting how much I like it. In that case, there's no way we could assign consistent numbers x,y,z to those three pizza toppings such that x>y>z>x.\n\nSo maybe I don't assign numbers to my pizza. Why is that so awful?\n\nAre there any grounds besides \"we like a certain mathematical formalism and your choices don't fit into our math,\" on which to criticize my three simultaneous preferences?\n\n(Feel free to try to answer this yourself before continuing...)\n\n* * *\n\nClick here to reveal and continue:\n\nSuppose I tell you that I prefer pineapple to mushrooms on my pizza. Suppose you're about to give me a slice of mushroom pizza; but by paying one penny ($0.01) I can instead get a slice of pineapple pizza (which is just as fresh from the oven). It seems realistic to say that most people with a pineapple pizza preference would probably pay the penny, if they happened to have a penny in their pocket.**¹**\n\nAfter I pay the penny, though, and just before I'm about to get the pineapple pizza, you offer me a slice of onion pizza instead—no charge for the change! If I was telling the truth about preferring onion pizza to pineapple, I should certainly accept the substitution if it's free.\n\nAnd then to round out the day, you offer me a mushroom pizza instead of the onion pizza, and again, since I prefer mushrooms to onions, I accept the swap.\n\nI end up with exactly the same slice of mushroom pizza I started with... and one penny poorer, because I previously paid $0.01 to swap mushrooms for pineapple.\n\n* * *\n\nThis seems like a _qualitatively_ bad behavior on my part. By virtue of my incoherent preferences which cannot be given a consistent ordering, I have shot myself in the foot, done something self-defeating. We haven't said _how_ I ought to sort out my inconsistent preferences. But no matter how it shakes out, it seems like there must be _some_ better alternative—some better way I could reason that wouldn't spend a penny to go in circles. That is, I could at least have kept my original pizza slice and not spent the penny.\n\nIn a phrase you're going to keep hearing, I have executed a 'dominated strategy': there exists some other strategy that does strictly better.**²**\n\nOr as Steve Omohundro put it: If you prefer being in Berkeley to being in San Francisco; prefer being in San Jose to being in Berkeley; and prefer being in San Francisco to being in San Jose; then you're going to waste a lot of time on taxi rides.\n\nNone of this reasoning has told us that a non-self-defeating agent must prefer Berkeley to San Francisco or vice versa. There are at least six possible consistent orderings over pizza toppings, like mushroom>Ppineapple>Ponion etcetera, and _any_ consistent ordering would avoid paying to go in circles.**³** We have not, in this argument, used pure logic to derive that pineapple pizza must taste better than mushroom pizza to an ideal rational agent. But we've seen that eliminating a certain kind of shoot-yourself-in-the-foot behavior, corresponds to imposing a certain _coherence_ or _consistency_ requirement on whatever preferences are there.\n\nIt turns out that this is just one instance of a large family of _coherence theorems_ which all end up pointing at the same set of core properties. All roads lead to Rome, and all the roads say, \"If you are not shooting yourself in the foot in sense X, we can view you as having coherence property Y.\"\n\nThere are some caveats to this general idea.\n\nFor example: In complicated problems, perfect coherence is usually impossible to compute—it's just too expensive to consider _all_ the possibilities.\n\nBut there are also caveats to the caveats! For example, it may be that if there's a powerful machine intelligence that is not _visibly to us humans_ shooting itself in the foot in way X, then _from our perspective_ it must look like the AI has coherence property Y. If there's some sense in which the machine intelligence is going in circles, because _not_ going in circles is too hard to compute, well, _we_ won't see that either with our tiny human brains. In which case it may make sense, from our perspective, to think about the machine intelligence _as if_ it has some coherent preference ordering.\n\nWe are not going to go through all the coherence theorems in this introduction. They form a very large family; some of them are a _lot_ more mathematically intimidating; and honestly I don't know even 5% of the variants.\n\nBut we can hopefully walk through enough coherence theorems to at least start to see the reasoning behind, \"Why expected utility?\" And, because the two are a package deal, \"Why probability?\"\n\n  \n\nHuman lives, mere dollars, and coherent trades\n==============================================\n\nAn experiment in 2000—from a paper titled \"[The Psychology of the Unthinkable: Taboo Trade-Offs, Forbidden Base Rates, and Heretical Counterfactuals](http://scholar.harvard.edu/files/jenniferlerner/files/2000_the_psychology_of_the_unthinkable.pdf?m=145089665)\"—asked subjects to consider the dilemma of a hospital administrator named Robert:\n\n> Robert can save the life of Johnny, a five year old who needs a liver transplant, but the transplant procedure will cost the hospital $1,000,000 that could be spent in other ways, such as purchasing better equipment and enhancing salaries to recruit talented doctors to the hospital. Johnny is very ill and has been on the waiting list for a transplant but because of the shortage of local organ donors, obtaining a liver will be expensive. Robert could save Johnny's life, or he could use the $1,000,000 for other hospital needs.\n\nThe main experimental result was that most subjects got angry at Robert for even considering the question.\n\nAfter all, you can't put a dollar value on a human life, right?\n\nBut better hospital equipment also saves lives, or at least one hopes so.**⁴** It's not like the other potential use of the money saves zero lives.\n\nLet's say that Robert has a total budget of $100,000,000 and is faced with a long list of options such as these:\n\n*   $100,000 for a new dialysis machine, which will save 3 lives\n*   $1,000,000 for a liver for Johnny, which will save 1 life\n*   $10,000 to train the nurses on proper hygiene when inserting central lines, which will save an expected 100 lives\n*   ...\n\nNow suppose—this is a supposition we'll need for our theorem—that Robert _does not care at all about money,_ not even a tiny bit. Robert _only_ cares about maximizing the total number of lives saved. Furthermore, we suppose for now that Robert cares about every human life equally.\n\nIf Robert does save as many lives as possible, given his bounded money, then Robert must _behave like_ somebody assigning some consistent dollar value to saving a human life.\n\nWe should be able to look down the long list of options that Robert took and didn't take, and say, e.g., \"Oh, Robert took all the options that saved more than 1 life per $500,000 and rejected all options that saved less than 1 life per $500,000; so Robert's behavior is _consistent_ with his spending $500,000 per life.\"\n\nAlternatively, if we can't view Robert's behavior as being coherent in this sense—if we cannot make up _any_ dollar value of a human life, such that Robert's choices are consistent with that dollar value—then it must be possible to move around the same amount of money, in a way that saves more lives.\n\nWe start from the qualitative criterion, \"Robert must save as many lives as possible; it shouldn't be possible to move around the same money to save more lives.\" We end up with the quantitative coherence theorem, \"It must be possible to view Robert as trading dollars for lives at a consistent price.\"\n\nWe haven't proven that dollars have some intrinsic worth that trades off against the intrinsic worth of a human life. By hypothesis, Robert doesn't care about money at all. It's just that every dollar has an _opportunity cost_ in lives it could have saved if deployed differently; and this opportunity cost is the same for every dollar because money is fungible.\n\nAn important caveat to this theorem is that there may be, e.g., an option that saves a hundred thousand lives for $200,000,000. But Robert only has $100,000,000 to spend. In this case, Robert may fail to take that option even though it saves 1 life per $2,000. It was a good option, but Robert didn't have enough money in the bank to afford it. This does mess up the elegance of being able to say, \"Robert must have taken _all_ the options saving at least 1 life per $500,000\", and instead we can only say this with respect to options that are in some sense small enough or granular enough.\n\nSimilarly, if an option costs $5,000,000 to save 15 lives, but Robert only has $4,000,000 left over after taking all his other best opportunities, Robert's last selected option might be to save 8 lives for $4,000,000 instead. This again messes up the elegance of the reasoning, but Robert is still doing exactly what an agent _would_ do if it consistently valued lives at 1 life per $500,000—it would buy all the best options _it could afford_ that purchased at least that many lives per dollar. So that part of the theorem's conclusion still holds.\n\nAnother caveat is that we haven't proven that there's some specific dollar value in Robert's head, as a matter of psychology. We've only proven that Robert's outward behavior can be _viewed as if_ it prices lives at _some_ consistent value, assuming Robert saves as many lives as possible.\n\nIt could be that Robert accepts every option that spends less than $500,000/life and rejects every option that spends over $600,000, and there aren't any available options in the middle. Then Robert's behavior can equally be _viewed as_ consistent with a price of $510,000 or a price of $590,000. This helps show that we haven't proven anything about Robert explicitly _thinking_ of some number. Maybe Robert never lets himself think of a specific threshold value, because it would be taboo to assign a dollar value to human life; and instead Robert just fiddles the choices until he can't see how to save any more lives.\n\nWe naturally have not proved by pure logic that Robert must want, in the first place, to save as many lives as possible. Even if Robert is a good person, this doesn't follow. Maybe Robert values a 10-year-old's life at 5 times the value of a 70-year-old's life, so that Robert will sacrifice five grandparents to save one 10-year-old. A lot of people would see that as entirely consistent with valuing human life in general.\n\nLet's consider that last idea more thoroughly. If Robert considers a preteen equally valuable with 5 grandparents, so that Robert will shift $100,000 from saving 8 old people to saving 2 children, then we can no longer say that Robert wants to save as many 'lives' as possible. That last decision would decrease by 6 the total number of 'lives' saved. So we can no longer say that there's a qualitative criterion, 'Save as many lives as possible', that produces the quantitative coherence requirement, 'trade dollars for lives at a consistent rate'.\n\nDoes this mean that coherence might as well go out the window, so far as Robert's behavior is concerned? Anything goes, now? Just spend money wherever?\n\n\"Hm,\" you might think. \"But... if Robert trades 8 old people for 2 children _here_... and then trades 1 child for 2 old people _there_...\"\n\nTo reduce distraction, let's make this problem be about apples and oranges instead. Suppose:\n\n*   Alice starts with 8 apples and 1 orange.\n*   Then Alice trades 8 apples for 2 oranges.\n*   Then Alice trades away 1 orange for 2 apples.\n*   Finally, Alice trades another orange for 3 apples.\n\nThen in this example, Alice is using a strategy that's _strictly dominated_ across all categories of fruit. Alice ends up with 5 apples and one orange, but could've ended with 8 apples and one orange (by not making any trades at all). Regardless of the _relative_ value of apples and oranges, Alice's strategy is doing _qualitatively_ worse than another possible strategy, if apples have any positive value to her at all.\n\nSo the fact that Alice can't be viewed as having any coherent relative value for apples and oranges, corresponds to her ending up with qualitatively less of some category of fruit (without any corresponding gains elsewhere).\n\nThis remains true if we introduce more kinds of fruit into the problem. Let's say the set of fruits Alice can trade includes {apples, oranges, strawberries, plums}. If we can't look at Alice's trades and make up some relative quantitative values of fruit, such that Alice could be trading consistently with respect to those values, then Alice's trading strategy must have been dominated by some other strategy that would have ended up with strictly more fruit across all categories.\n\nIn other words, we need to be able to look at Alice's trades, and say something like:\n\n\"Maybe Alice values an orange at 2 apples, a strawberry at 0.1 apples, and a plum at 0.5 apples. That would explain why Alice was willing to trade 4 strawberries for a plum, but not willing to trade 40 strawberries for an orange and an apple.\"\n\nAnd if we _can't_ say this, then there must be some way to rearrange Alice's trades and get _strictly more fruit across all categories_ in the sense that, e.g., we end with the same number of plums and apples, but one more orange and two more strawberries. This is a bad thing if Alice _qualitatively_ values fruit from each category—prefers having more fruit to less fruit, ceteris paribus, for each category of fruit.\n\nNow let's shift our attention back to Robert the hospital administrator. _Either_ we can view Robert as consistently assigning some _relative_ value of life for 10-year-olds vs. 70-year-olds, _or_ there must be a way to rearrange Robert's expenditures to save either strictly more 10-year-olds or strictly more 70-year-olds. The same logic applies if we add 50-year-olds to the mix. We must be able to say something like, \"Robert is consistently behaving as if a 50-year-old is worth a third of a ten-year-old\". If we _can't_ say that, Robert must be behaving in a way that pointlessly discards some saveable lives in some category.\n\nOr perhaps Robert is behaving in a way which implies that 10-year-old girls are worth more than 10-year-old boys. But then the relative values of those subclasses of 10-year-olds need to be viewable as consistent; or else Robert must be qualitatively failing to save one more 10-year-old boy than could've been saved otherwise.\n\nIf you can denominate apples in oranges, and price oranges in plums, and trade off plums for strawberries, all at consistent rates... then you might as well take it one step further, and factor out an abstract unit for ease of notation.\n\nLet's call this unit _1 utilon,_ and denote it €1. (As we'll see later, the letters 'EU' are appropriate here.)\n\nIf we say that apples are worth €1, oranges are worth €2, and plums are worth €0.5, then this tells us the relative value of apples, oranges, and plums. Conversely, if we _can_ assign consistent relative values to apples, oranges, and plums, then we can factor out an abstract unit at will—for example, by arbitrarily declaring apples to be worth €100 and then calculating everything else's price in apples.\n\nHave we proven by pure logic that all apples have the same utility? Of course not; you can prefer some particular apples to other particular apples. But when you're done saying which things you qualitatively prefer to which other things, if you go around making tradeoffs in a way that can be _viewed as_ not qualitatively leaving behind some things you said you wanted, we can _view you_ as assigning coherent quantitative utilities to everything you want.\n\nAnd that's one coherence theorem—among others—that can be seen as motivating the concept of _utility_ in decision theory.\n\nUtility isn't a solid thing, a separate thing. We could multiply all the utilities by two, and that would correspond to the same outward behaviors. It's meaningless to ask how much utility you scored at the end of your life, because we could subtract a million or add a million to that quantity while leaving everything else conceptually the same.\n\nYou could pick anything you valued—say, the joy of watching a cat chase a laser pointer for 10 seconds—and denominate everything relative to that, without needing any concept of an extra abstract 'utility'. So (just to be extremely clear about this point) we have not proven that there is a separate thing 'utility' that you should be pursuing instead of everything else you wanted in life.\n\nThe coherence theorem says nothing about which things to value more than others, or how much to value them relative to other things. It doesn't say whether you should value your happiness more than someone else's happiness, any more than the notion of a consistent preference ordering >P tells us whether onions>Ppineapple.\n\n(The notion that we should assign equal value to all human lives, or equal value to all sentient lives, or equal value to all Quality-Adjusted Life Years, is _utilitarianism._ Which is, sorry about the confusion, a whole 'nother separate different philosophy.)\n\nThe conceptual gizmo that maps thingies to utilities—the whatchamacallit that takes in a fruit and spits out a utility—is called a 'utility function'. Again, this isn't a separate thing that's written on a stone tablet. If we multiply a utility function by 9.2, that's conceptually the same utility function because it's consistent with the same set of behaviors.\n\nBut in general: If we can sensibly view any agent as doing as well as qualitatively possible at _anything_, we must be able to view the agent's behavior as consistent with there being some coherent relative quantities of wantedness for all the thingies it's trying to optimize.\n\n  \n\nProbabilities and expected utility\n==================================\n\nWe've so far made no mention of _probability._ But the way that probabilities and utilities interact, is where we start to see the full structure of _expected utility_ spotlighted by all the coherence theorems.\n\nThe basic notion in expected utility is that some choices present us with uncertain outcomes.\n\nFor example, I come to you and say: \"Give me 1 apple, and I'll flip a coin; if the coin lands heads, I'll give you 1 orange; if the coin comes up tails, I'll give you 3 plums.\" Suppose you relatively value fruits as described earlier: 2 apples / orange and 0.5 apples / plum. Then _either_ possible outcome gives you something that's worth more to you than 1 apple. Turning down a so-called 'gamble' like that... why, it'd be a dominated strategy.\n\nIn general, the notion of 'expected utility' says that we assign certain quantities called _probabilities_ to each possible outcome. In the example above, we might assign a 'probability' of 0.5 to the coin landing heads (1 orange), and a 'probability' of 0.5 to the coin landing tails (3 plums). Then the total value of the 'gamble' we get by trading away 1 apple is:\n\nP(heads)⋅U(1 orange)+P(tails)⋅U(3 plums)=0.50⋅€2+0.50⋅€1.5=€1.75\n\nConversely, if we just keep our 1 apple instead of making the trade, this has an expected utility of 1⋅U(1 apple)=€1. So indeed we ought to trade (as the previous reasoning suggested).\n\n\"But wait!\" you cry. \"Where did these probabilities come from? Why is the 'probability' of a fair coin landing heads 0.5 and not, say, −0.2 or 3? Who says we ought to multiply utilities by probabilities in the first place?\"\n\nIf you're used to approaching this problem from a [Bayesian](https://arbital.com/p/bayes_rule_guide/) standpoint, then you may now be thinking of notions like [prior probability](https://arbital.com/p/prior_probability/) and Occam's Razor and [universal priors](https://arbital.com/p/universal_prior/)...\n\nBut from the standpoint of coherence theorems, that's putting the cart before the horse.\n\nFrom the standpoint of coherence theorems, we don't _start with_ a notion of 'probability'.\n\nInstead we ought to prove something along the lines of: if you're not using qualitatively dominated strategies, then you must _behave as if_ you are multiplying utilities by certain quantitative thingies.\n\nWe might then furthermore show that, for non-dominated strategies, these utility-multiplying thingies must be between 0 and 1 rather than say −0.3 or 27.\n\nHaving determined what coherence properties these utility-multiplying thingies need to have, we decide to call them 'probabilities'. And _then_—once we know in the first place that we need 'probabilities' in order to not be using dominated strategies—we can start to worry about exactly what the numbers ought to be.\n\n  \n\nProbabilities summing to 1\n--------------------------\n\nHere's a taste of the kind of reasoning we might do:\n\nSuppose that—having already accepted some previous proof that non-dominated strategies dealing with uncertain outcomes, must multiply utilities by quantitative thingies—you then say that you are going to assign a probability of 0.6 to the coin coming up heads, and a probability of 0.7 to the coin coming up tails.\n\nIf you're already used to the standard notion of probability, you might object, \"But those probabilities sum to 1.3 when they ought to sum to 1!\"**⁵** But now we are in coherence-land; we don't ask \"Did we violate the standard axioms that all the textbooks use?\" but \"What rules must non-dominated strategies obey?\" _De gustibus non est disputandum;_ can we _disputandum_ somebody saying that a coin has a 60% probability of coming up heads and a 70% probability of coming up tails? (Where these are the only 2 possible outcomes of an uncertain coinflip.)\n\nWell—assuming you've already accepted that we need utility-multiplying thingies—I might then offer you a gamble. How about you give me one apple, and if the coin lands heads, I'll give you 0.8 apples; while if the coin lands tails, I'll give you 0.8 apples.\n\nAccording to you, the expected utility of this gamble is:\n\nP(heads)⋅U(0.8 apples)+P(tails)⋅U(0.8 apples)=0.6⋅€0.8+0.7⋅€0.8=€1.04.\n\nYou've just decided to trade your apple for 0.8 apples, which sure sounds like one of 'em dominated strategies.\n\nAnd that's why _the thingies you multiply probabilities by_—the thingies that you use to weight uncertain outcomes in your imagination, when you're trying to decide how much you want one branch of an uncertain choice—must sum to 1, whether you call them 'probabilities' or not.\n\nWell... actually we just argued**⁶** that probabilities for [mutually exclusive](https://arbital.com/p/exclusive_exhaustive/) outcomes should sum to _no more than 1._ What would be an example showing that, for non-dominated strategies, the probabilities for [exhaustive](https://arbital.com/p/exclusive_exhaustive/) outcomes should sum to no less than 1?\n\n* * *\n\nWhy exhaustive outcomes should sum to at least 1:\n\nSuppose that, in exchange for 1 apple, I credibly offer:\n\n\\* To pay you 1.1 apples if a coin comes up heads.  \n\\* To pay you 1.1 apples if a coin comes up tails.  \n\\* To pay you 1.1 apples if anything else happens.\n\nIf the probabilities you assign to these three outcomes sum to say 0.9, you will refuse to trade 1 apple for 1.1 apples.\n\n(This is strictly dominated by the strategy of agreeing to trade 1 apple for 1.1 apples.)\n\n* * *\n\n  \n\nDutch book arguments\n--------------------\n\nAnother way we could have presented essentially the same argument as above, is as follows:\n\nSuppose you are a market-maker in a prediction market for some event X. When you say that your price for event X is x, you mean that you will sell for $x a ticket which pays $1 if X happens (and pays out nothing otherwise). In fact, you will sell any number of such tickets!\n\nSince you are a market-maker (that is, you are trying to encourage trading in X for whatever reason), you are also willing to _buy_ any number of tickets at the price $x. That is, I can say to you (the market-maker) \"I'd like to sign a contract where you give me N⋅$x now, and in return I must pay you $N iff X happens;\" and you'll agree. (We can view this as you selling me a negative number of the original kind of ticket.)\n\nLet X and Y denote two events such that _exactly one_ of them must happen; say, X is a coin landing heads and Y is the coin not landing heads.\n\nNow suppose that you, as a market-maker, are motivated to avoid combinations of bets that lead into _certain_ losses for you—not just losses that are merely probable, but combinations of bets such that _every_ possibility leads to a loss.\n\nThen if exactly one of X and Y must happen, your prices x and y must sum to exactly $1. Because:\n\n*   If x+y<$1, I buy both an X-ticket and a Y-ticket and get a guaranteed payout of $1 minus costs of x+y. Since this is a guaranteed profit for me, it is a guaranteed loss for you.\n*   If x+y>$1, I sell you both tickets and will at the end pay you $1 after you have already paid me x+y. Again, this is a guaranteed profit for me of x+y−$1>$0.\n\nThis is more or less exactly the same argument as in the previous section, with trading apples. Except that: (a) the scenario is more crisp, so it is easier to generalize and scale up much more complicated similar arguments; and (b) it introduces a whole lot of assumptions that people new to expected utility would probably find rather questionable.\n\n\"What?\" one might cry. \"What sort of crazy bookie would buy and sell bets at exactly the same price? Why ought _anyone_ to buy and sell bets at exactly the same price? Who says that I must value a gain of $1 exactly the opposite of a loss of $1? Why should the price that I put on a bet represent my degree of uncertainty about the environment? What does all of this argument about gambling have to do with real life?\"\n\nSo again, the key idea is not that we are assuming anything about people valuing every real-world dollar the same; nor is it in real life a good idea to offer to buy or sell bets at the same prices.**⁷** Rather, Dutch book arguments can stand in as shorthand for some longer story in which we only assume that you prefer more apples to less apples.\n\nThe Dutch book argument above has to be seen as one more added piece in the company of all the _other_ coherence theorems—for example, the coherence theorems suggesting that you ought to be quantitatively weighing events in your mind in the first place.\n\n  \n\nConditional probability\n-----------------------\n\nWith more complicated Dutch book arguments, we can derive more complicated ideas such as 'conditional probability'.\n\nLet's say that we're pricing three kinds of gambles over two events Q and R:\n\n*   A ticket that costs $x, and pays $1 if Q happens.\n*   A ticket that doesn't cost anything or pay anything if Q doesn't happen (the ticket price is refunded); and if Q does happen, this ticket costs $y, then pays $1 if R happens.\n*   A ticket that costs $z, and pays $1 if Q and R both happen.\n\nIntuitively, the idea of [conditional probability](https://arbital.com/p/conditional_probability/) is that the probability of Q and R both happening, should be equal to the probability of Q happening, times the probability that R happens assuming that Q happens:\n\nP(Q∧R)=P(Q)⋅P(R∣Q)\n\nTo exhibit a Dutch book argument for this rule, we want to start from the assumption of a qualitatively non-dominated strategy, and derive the quantitative rule z=x⋅y.\n\nSo let's give an example that violates this equation and see if there's a way to make a guaranteed profit. Let's say somebody:\n\n*   Prices at x=$0.60 the first ticket, aka P(Q).\n*   Prices at y=$0.70 the second ticket, aka P(R∣Q).\n*   Prices at z=$0.20 the third ticket, aka P(Q∧R), which ought to be $0.42 assuming the first two prices.\n\nThe first two tickets are priced relatively high, compared to the third ticket which is priced relatively low, suggesting that we ought to sell the first two tickets and buy the third.\n\nOkay, let's ask what happens if we sell 10 of the first ticket, sell 10 of the second ticket, and buy 10 of the third ticket.\n\n*   If Q doesn't happen, we get $6, and pay $2. Net +$4.\n*   If Q happens and R doesn't happen, we get $6, pay $10, get $7, and pay $2. Net +$1.\n*   If Q happens and R happens, we get $6, pay $10, get $7, pay $10, pay $2, and get $10. Net: +$1.\n\nThat is: we can get a guaranteed positive profit over all three possible outcomes.\n\nMore generally, let A,B,C be the (potentially negative) amount of each ticket X,Y,Z that is being bought (buying a negative amount is selling). Then the prices x,y,z can be combined into a 'Dutch book' whenever the following three inequalities can be simultaneously true, with at least one inequality strict:\n\n−Ax+0−Cz≧0A(1−x)−By−Cz≧0A(1−x)+B(1−y)+C(1−z)≧0\n\nFor x,y,z∈(0..1) this is impossible exactly iff z=x⋅y. The proof via a bunch of algebra is left as an exercise to the reader.**⁸**\n\n  \n\nThe Allais Paradox\n------------------\n\nBy now, you'd probably like to see a glimpse of the sort of argument that shows in the first place that we need expected utility—that a non-dominated strategy for uncertain choice must behave as if multiplying utilities by some kinda utility-multiplying thingies ('probabilities').\n\nAs far as I understand it, the real argument you're looking for is [Abraham Wald's complete class theorem](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177730345), which I must confess I don't know how to reduce to a simple demonstration.\n\nBut we can catch a glimpse of the general idea from a famous psychology experiment that became known as the Allais Paradox (in slightly adapted form).\n\nSuppose you ask some experimental subjects which of these gambles they would rather play:\n\n*   1A: A certainty of $1,000,000.\n*   1B: 90% chance of winning $5,000,000, 10% chance of winning nothing.\n\nMost subjects say they'd prefer 1A to 1B.\n\nNow ask a separate group of subjects which of these gambles they'd prefer:\n\n*   2A: 50% chance of winning $1,000,000; 50% chance of winning $0.\n*   2B: 45% chance of winning $5,000,000; 55% chance of winning $0.\n\nIn this case, most subjects say they'd prefer gamble 2B.\n\nNote that the $ sign here denotes real dollars, not utilities! A gain of five million dollars isn't, and shouldn't be, worth exactly five times as much to you as a gain of one million dollars. We can use the € symbol to denote the expected utilities that are abstracted from how much you relatively value different outcomes; $ is just money.\n\nSo we certainly aren't claiming that the first preference is paradoxical because 1B has an expected dollar value of $4.5 million and 1A has an expected dollar value of $1 million. That would be silly. We care about expected utilities, not expected dollar values, and those two concepts aren't the same at all!\n\nNonetheless, the combined preferences 1A > 1B and 2A < 2B are not compatible with any coherent utility function. We cannot simultaneously have:\n\nU(gain $1M)>0.9⋅U(gain $5M)+0.1⋅U(gain $0)0.5⋅U(gain $0)+0.5⋅U(gain $1M)<0.45⋅U(gain $5M)+0.55⋅U(gain $0)\n\nThis was one of the earliest experiments seeming to demonstrate that actual human beings were not expected utility maximizers—a very tame idea nowadays, to be sure, but the _first definite_ demonstration of that was a big deal at the time. Hence the term, \"Allais Paradox\".\n\nNow, by the general idea behind coherence theorems, since we can't _view this behavior_ as corresponding to expected utilities, we ought to be able to show that it corresponds to a dominated strategy somehow—derive some way in which this behavior corresponds to shooting off your own foot.\n\nIn this case, the relevant idea seems non-obvious enough that it doesn't seem reasonable to demand that you think of it on your own; but if you like, you can pause and try to think of it anyway. Otherwise, just continue reading.\n\n* * *\n\nAgain, the gambles are as follows:\n\n*   1A: A certainty of $1,000,000.\n*   1B: 90% chance of winning $5,000,000, 10% chance of winning nothing.\n*   2A: 50% chance of winning $1,000,000; 50% chance of winning $0.\n*   2B: 45% chance of winning $5,000,000; 55% chance of winning $0.\n\nNow observe that Scenario 2 corresponds to a 50% chance of playing Scenario 1, and otherwise getting $0.\n\nThis, in fact, is why the combination 1A > 1B; 2A < 2B is incompatible with expected utility. In terms of [one set of axioms](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#The_axioms) frequently used to describe expected utility, it violates the Independence Axiom: if a gamble L is preferred to M (that is, L>M), then we ought to be able to take a constant probability p>0 and another gamble N and have p⋅L+(1−p)⋅N>p⋅M+(1−p)⋅N.\n\nTo put it another way, if I flip a coin to decide whether or not to play some entirely different game N, but otherwise let you choose L or M, you ought to make the same choice as if I just ask you whether you prefer L or M. Your preference between L and M should be 'independent' of the possibility that, instead of doing anything whatsoever with L or M, we will do something else instead.\n\nAnd since this is an axiom of expected utility, any violation of that axiom ought to correspond to a dominated strategy somehow.\n\nIn the case of the Allais Paradox, we do the following:\n\nFirst, I show you a switch that can be set to A or B, currently set to A.\n\nIn one minute, I tell you, I will flip a coin. If the coin comes up heads, you will get nothing. If the coin comes up tails, you will play the gamble from Scenario 1.\n\nFrom your current perspective, that is, we are playing Scenario 2: since the switch is set to A, you have a 50% chance of getting nothing and a 50% chance of getting $1 million.\n\nI ask you if you'd like to pay a penny to throw the switch from A to B. Since you prefer gamble 2B to 2A, and some quite large amounts of money are at stake, you agree to pay the penny. From your perspective, you now have a 55% chance of ending up with nothing and a 45% chance of getting $5M.\n\nI then flip the coin, and luckily for you, it comes up tails.\n\nFrom your perspective, you are now in Scenario 1B. Having observed the coin and updated on its state, you now think you have a 90% chance of getting $5 million and a 10% chance of getting nothing. By hypothesis, you would prefer a certainty of $1 million.\n\nSo I offer you a chance to pay another penny to flip the switch back from B to A. And with so much money at stake, you agree.\n\nI have taken your two cents on the subject.\n\nThat is: You paid a penny to flip a switch and then paid another penny to switch it back, and this is dominated by the strategy of just leaving the switch set to A.\n\nAnd that's at least a glimpse of why, if you're not using dominated strategies, the thing you do with relative utilities is multiply them by probabilities in a consistent way, and prefer the choice that leads to a greater expectation of the variable representing utility.\n\n  \n\n**From the Allais Paradox to real life**\n\nThe real-life lesson about what to do when faced with Allais's dilemma might be something like this:\n\nThere's _some_ amount that $1 million would improve your life compared to $0.\n\nThere's some amount that an additional $4 million would further improve your life after the first $1 million.\n\nYou ought to visualize these two improvements as best you can, and decide whether another $4 million can produce at least _one-ninth_ as much improvement, as much true value to you, as the first $1 million.\n\nIf it can, you should consistently prefer 1B > 1A; 2B > 2A. And if not, you should consistently prefer 1A > 1B; 2A > 2B.\n\nThe standard 'paradoxical' preferences in Allais's experiment are standardly attributed to a certainty effect: people value the _certainty_ of having $1 million, while the difference between a 50% probability and a 55% probability looms less large. (And this ties in to a number of other results about certainty, need for closure, prospect theory, and so on.)\n\nIt may sound intuitive, in an Allais-like scenario, to say that you ought to derive some value from being _certain_ about the outcome. In fact this is just the reasoning the experiment shows people to be using, so of course it might sound intuitive. But that does, inescapably, correspond to a kind of thinking that produces dominated strategies.\n\nOne possible excuse might be that certainty is valuable if you need to make plans about the future; knowing the exact future lets you make better plans. This is admittedly true and a phenomenon within expected utility, though it applies in a smooth way as confidence increases rather than jumping suddenly around 100%. But in the particular dilemma as described here, you only have 1 minute before the game is played, and no time to make other major life choices dependent on the outcome.\n\nAnother possible excuse for certainty bias might be to say: \"Well, I value the emotional feeling of certainty.\"\n\nIn real life, we do have emotions that are directly about probabilities, and those little flashes of happiness or sadness are worth something if you care about people being happy or sad. If you say that you value the emotional feeling of being _certain_ of getting $1 million, the freedom from the fear of getting $0, for the minute that the dilemma lasts and you are experiencing the emotion—well, that may just be a fact about what you value, even if it exists outside the expected utility formalism.\n\nAnd this genuinely does not fit into the expected utility formalism. In an expected utility agent, probabilities are just thingies-you-multiply-utilities-by. If those thingies start generating their own utilities once represented inside the mind of the person who is an object of ethical value, you really are going to get results that are incompatible with the formal decision theory.\n\nHowever, _not_ being viewable as an expected utility agent does always correspond to employing dominated strategies. You are giving up _something_ in exchange, if you pursue that feeling of certainty. You are potentially losing all the real value you could have gained from another $4 million, if that realized future actually would have gained you more than one-ninth the value of the first $1 million. Is a fleeting emotional sense of certainty over 1 minute, worth _automatically_ discarding the potential $5-million outcome? Even if the correct answer given your values is that you properly ought to take the $1 million, treasuring 1 minute of emotional gratification doesn't seem like the wise reason to do that. The wise reason would be if the first $1 million really was worth that much more than the next $4 million.\n\nThe danger of saying, \"Oh, well, I attach a lot of utility to that comfortable feeling of certainty, so my choices are coherent after all\" is not that it's mathematically improper to value the emotions we feel while we're deciding. Rather, by saying that the _most valuable_ stakes are the emotions you feel during the minute you make the decision, what you're saying is, \"I get a huge amount of value by making decisions however humans instinctively make their decisions, and that's much more important than the thing I'm making a decision _about._\" This could well be true for something like buying a stuffed animal. If millions of dollars or human lives are at stake, maybe not so much.\n\n  \n\nConclusion\n==========\n\nThe demonstrations we've walked through here aren't the professional-grade coherence theorems as they appear in real math. Those have names like \"[Cox's Theorem](https://en.wikipedia.org/wiki/Cox's_theorem)\" or \"the complete class theorem\"; their proofs are difficult; and they say things like \"If seeing piece of information A followed by piece of information B leads you into the same epistemic state as seeing piece of information B followed by piece of information A, plus some other assumptions, I can show an isomorphism between those epistemic states and classical probabilities\" or \"Any decision rule for taking different actions depending on your observations either corresponds to Bayesian updating given some prior, or else is strictly dominated by some Bayesian strategy\".\n\nBut hopefully you've seen enough concrete demonstrations to get a general idea of what's going on with the actual coherence theorems. We have multiple spotlights all shining on the same core mathematical structure, saying dozens of different variants on, \"If you aren't running around in circles or stepping on your own feet or wantonly giving up things you say you want, we can see your behavior as corresponding to this shape. Conversely, if we can't see your behavior as corresponding to this shape, you must be visibly shooting yourself in the foot.\" Expected utility is the only structure that has this great big family of discovered theorems all saying that. It has a scattering of academic competitors, because academia is academia, but the competitors don't have anything like that mass of spotlights all pointing in the same direction.\n\nSo if we need to pick an interim answer for \"What kind of quantitative framework should I try to put around my own decision-making, when I'm trying to check if my thoughts make sense?\" or \"By default and barring special cases, what properties might a sufficiently advanced machine intelligence _look to us_ like it possessed, at least approximately, if we couldn't see it _visibly_ running around in circles?\", then there's pretty much one obvious candidate: Probabilities, utility functions, and expected utility.\n\n  \n\nFurther reading\n===============\n\n*   To learn more about agents and AI: [Consequentialist cognition](https://arbital.com/p/consequentialist/); [the orthogonality of agents' utility functions and capabilities](https://arbital.com/p/1y); [epistemic and instrumental efficiency](https://arbital.com/p/10g); [instrumental strategies sufficiently capable agents tend to converge on](https://arbital.com/p/instrumental_convergence/); [properties of sufficiently advanced agents](https://arbital.com/p/advanced_agent/).\n*   To learn more about decision theory: [The controversial counterfactual at the heart of the expected utility formula](https://intelligence.org/2018/10/31/embedded-decisions/).\n\n  \n\n* * *\n\n  \n  \n\n**¹** It could be that somebody's pizza preference is real, but so weak that they wouldn't pay one penny to get the pizza they prefer. In this case, imagine we're talking about some stronger preference instead. Like your willingness to pay at least one penny not to have your house burned down, or something.\n\n² This does assume that the agent prefers to have more money rather than less money. \"Ah, but why is it bad if one person has a penny instead of another?\" you ask. If we insist on pinning down every point of this sort, then you can also imagine the $0.01 as standing in for the _time_ I burned in order to move the pizza slices around in circles. That time was burned, and nobody else has it now. If I'm an effective agent that goes around pursuing my preferences, I should in general be able to sometimes convert time into other things that I want. In other words, my circular preference can lead me to incur an opportunity cost denominated in the sacrifice of other things I want, and not in a way that benefits anyone else.\n\n**³** There are more than six possibilities if you think it's possible to be absolutely indifferent between two kinds of pizza.\n\n**⁴** We can omit the 'better doctors' item from consideration: The supply of doctors is mostly constrained by regulatory burdens and medical schools rather than the number of people who want to become doctors; so bidding up salaries for doctors doesn't much increase the total number of doctors; so bidding on a talented doctor at one hospital just means some other hospital doesn't get that talented doctor. It's also illegal to pay for livers, but let's ignore that particular issue with the problem setup or pretend that it all takes place in a more sensible country than the United States or Europe.\n\n**⁵** Or maybe a [tiny bit less](https://arbital.com/p/cromwells_rule/) than 1, in case the coin lands on its edge or something.\n\n**⁶** Nothing we're walking through here is really a coherence theorem _per se_, more like intuitive arguments that a coherence theorem ought to exist. Theorems require proofs, and nothing here is what real mathematicians would consider to be a 'proof'.\n\n**⁷** In real life this leads to a problem of 'adversarial selection', where somebody who knows more about the environment than you can decide whether to buy or sell from you. To put it another way, from a [Bayesian](https://arbital.com/p/bayes_rule_guide/) standpoint, if an _intelligent_ counterparty is deciding whether to buy or sell from you a bet on X, the fact that they choose to buy (or sell) should cause you to [update](https://arbital.com/p/bayes_update/) in favor (or against) X actually happening. After all, they wouldn't be taking the bet unless they thought they knew something you didn't!\n\n**⁸** The quick but advanced argument would be to say that the left-hand-side must look like a singular matrix, whose determinant must therefore be zero.",
      "plaintextDescription": "(Written for Arbital in 2017.)\n\n----------------------------------------\n\n\nIntroduction to the introduction: Why expected utility?\nSo we're talking about how to make good decisions, or the idea of 'bounded rationality', or what sufficiently advanced Artificial Intelligences might be like; and somebody starts dragging up the concepts of 'expected utility' or 'utility functions'.\n\nAnd before we even ask what those are, we might first ask, Why?\n\nThere's a mathematical formalism, 'expected utility', that some people invented to talk about making decisions. This formalism is very academically popular, and appears in all the textbooks.\n\nBut so what? Why is that necessarily the best way of making decisions under every kind of circumstance? Why would an Artificial Intelligence care what's academically popular? Maybe there's some better way of thinking about rational agency? Heck, why is this formalism popular in the first place?\n\nWe can ask the same kinds of questions about probability theory:\n\nOkay, we have this mathematical formalism in which the chance that X happens, aka P(X), plus the chance that X doesn't happen, aka P(¬X), must be represented in a way that makes the two quantities sum to unity: P(X)+P(¬X)=1.\n\nThat formalism for probability has some neat mathematical properties. But so what? Why should the best way of reasoning about a messy, uncertain world have neat properties? Why shouldn't an agent reason about 'how likely is that' using something completely unlike probabilities? How do you know a sufficiently advanced Artificial Intelligence would reason in probabilities? You haven't seen an AI, so what do you think you know and how do you think you know it?\n\nThat entirely reasonable question is what this introduction tries to answer. There are, indeed, excellent reasons beyond academic habit and mathematical convenience for why we would by default invoke 'expected utility' and 'probability theory' to think about good human decisions, talk about rational agency, ",
      "wordCount": 7881
    },
    "tags": [
      {
        "_id": "HAFdXkW4YW4KRe2Gx",
        "name": "Utility Functions",
        "slug": "utility-functions"
      },
      {
        "_id": "6DDtyKtotNehTjmRn",
        "name": "Coherence Arguments",
        "slug": "coherence-arguments"
      },
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "LRKXuxLrnxx3nSESv",
    "title": "Should ethicists be inside or outside a profession?",
    "slug": "should-ethicists-be-inside-or-outside-a-profession",
    "url": null,
    "baseScore": 97,
    "voteCount": 33,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2018-12-12T01:40:13.298Z",
    "contents": {
      "markdown": "_Originally written in 2007._\n\n* * *\n\nMarvin Minsky in an interview with Danielle Egan for _New Scientist:_\n\n> **Minsky:** The reason we have politicians is to prevent bad things from happening. It doesn’t make sense to ask a scientist to worry about the bad effects of their discoveries, because they’re no better at that than anyone else. Scientists are not particularly good at social policy.\n\n> **Egan:** But shouldn’t they have an ethical responsibility for their inventions\n\n> **Minsky:** No they shouldn’t have an ethical responsibility for their inventions. They should be able to do what they want. You shouldn’t have to ask them to have the same values as other people. Because then you won’t get them. They’ll make stupid decisions and not work on important things, because they see possible dangers. What you need is a separation of powers. It doesn’t make any sense to have the same person do both.\n\nThe Singularity Institute was recently asked to comment on this interview - which by the time it made it through the editors at _New Scientist_, contained just the unvarnished [quote](http://www.newscientist.com/channel/opinion/death/mg19626251.800-death-special-the-plan-for-eternal-life.html) “Scientists shouldn’t have an ethical responsibility for their inventions. They should be able to do what they want. You shouldn’t have to ask them to have the same values as other people.” Nice one, _New Scientist._ Thanks to Egan for providing the original interview text.\n\nThis makes an interesting contrast with what I said in my “[Cognitive biases](https://intelligence.org/files/CognitiveBiases.pdf)” chapter for Bostrom’s _Global Catastrophic Risks:_\n\n> Someone on the physics-disaster committee should know what the term “[existential risk](http://www.nickbostrom.com/existential/risks.html)” means; should possess whatever skills the field of existential risk management has accumulated or borrowed. For maximum safety, that person should also be a physicist. The domain-specific expertise and the expertise pertaining to existential risks should combine in one person. I am skeptical that a scholar of heuristics and biases, unable to read physics equations, could check the work of physicists who knew nothing of heuristics and biases.\n\nShould ethicists be inside or outside a profession?\n\nIt seems to me that trying to separate ethics and engineering is like trying to separate the crafting of paintings into two independent specialties: a profession that’s in charge of pushing a paintbrush over a canvas, and a profession that’s in charge of artistic beauty but knows nothing about paint or optics.\n\nThe view of ethics as a separate profession is part of the _problem._ It arises, I think, from the same deeply flawed worldview that sees technology as something foreign and distant, something _opposed to_ life and beauty. Technology is an expression of human intelligence, which is to say, an expression of human nature. Hunter-gatherers who crafted their own bows and arrows didn’t have cultural nightmares about bows and arrows being a mechanical death force, a blank-faced System. When you craft something with your own hands, it seems like a part of you. It’s the Industrial Revolution that enabled people to buy artifacts which they could not make or did not even understand.\n\nEthics, like engineering and art and mathematics, is a natural expression of human minds.\n\nAnyone who gives a part of themselves to a profession discovers a sense of beauty in it. Writers discover that sentences can be beautiful. Programmers discover that code can be beautiful. Architects discover that house layouts can be beautiful. We all start out with a native sense of beauty, which already responds to rivers and flowers. But as we begin to _create_ \\- sentences or code or house layouts or flint knives - our sense of beauty develops with use.\n\nLike a sense of beauty, one’s native ethical sense must be continually used in order to develop further. If you’re just working at a job to make money, so that your real goal is to make the rent on your apartment, then neither your aesthetics nor your morals are likely to get much of a workout.\n\nThe way to develop a highly specialized sense of professional ethics is to do something, ethically, a whole bunch, until you get good at both the thing itself and the ethics part.\n\nWhen you look at the “bioethics” fiasco, you discover bioethicists writing mainly for an audience of other bioethicists. Bioethicists aren’t writing to doctors or bioengineers, they’re writing to tenure committees and journalists and foundation directors. Worse, bioethicists are not _using_ their ethical sense in bio-work, the way a doctor whose patient might have incurable cancer must choose how and what to tell the patient.\n\nA doctor treating a patient should not try to be _academically original,_ to come up with a brilliant new theory of bioethics. As I’ve written before, ethics is not _supposed_ to be [counterintuitive](https://www.lesswrong.com/posts/Aud7CL7uhz55KL8jG/transhumanism-as-simplified-humanism), and yet academic ethicists are biased to be just exactly counterintuitive enough that people won’t say, “Hey, I could have thought of that.” The purpose of ethics is to shape a well-lived life, not to be impressively complicated. Professional ethicists, to get paid, must transform ethics into something difficult enough to require professional ethicists.\n\nIt’s, like, a good idea to save lives? “Duh,” the foundation directors and the review boards and the tenure committee would say.\n\nBut there’s nothing _duh_ about saving lives if you’re a doctor.\n\nA book I once read about writing - I forget which one, alas - observed that there is a level of depth beneath which repetition ceases to be boring. Standardized phrases are called “cliches” (said the author of writing), but murder and love and revenge can be woven into a thousand plots without ever becoming old. “You should save people's lives, mmkay?” won’t get you tenure - but as a theme of real life, it’s as old as thinking, and no more obsolete.\n\nBoringly obvious ethics are just fine if you’re _using_ them in your work rather than talking about them. [The goal is to do it right, not to do it originally.](http://www.overcomingbias.com/2007/10/outside-the-box.html) Do your best whether or not it is “original”, and originality comes in its own time; not every change is an improvement, but every improvement is necessarily a change.\n\nAt the Singularity Summit 2007, several speakers alleged we should “reach out” to artists and poets to encourage their participation in the Singularity dialogue. And then a woman went to a microphone and said: “I am an artist. I want to participate. What should I do?”\n\nAnd there was a [long, delicious silence](http://www.overcomingbias.com/2007/09/we-dont-really-.html).\n\nWhat I would have said to a question like that, if someone had asked it of me in the conference lobby, was: “You are not an ‘artist’, you are a human being; art is only one facet in which you express your humanity. Your reactions to the Singularity should arise from your entire self, and it’s okay if you have a standard human reaction like ‘I’m afraid’ or ‘Where do I send the check?’, rather than some special ‘artist’ reaction. If your artistry has something to say, it will express itself naturally in your response as a human being, without needing a conscious effort to say something artist-like. I would feel patronized, like a dog commanded to perform a trick, if someone presented me with a painting and said ‘Say something mathematical!’”\n\nAnyone who calls on “artists” to participate in the Singularity clearly thinks of artistry as a special function that is only performed in Art departments, an icing dumped onto cake from outside. But you can always pick up some [cheap applause](http://www.overcomingbias.com/2007/09/applause-lights.html) by calling for more icing on the cake.\n\nEthicists should be inside a profession, rather than outside, because ethics itself should be inside rather than outside. It should be a natural expression of yourself, like math or art or engineering. If you don’t like trudging up and down stairs you’ll build an escalator. If you don’t want people to get hurt, you’ll try to make sure the escalator doesn’t suddenly speed up and throw its riders into the ceiling. Both just natural expressions of desire.\n\nThere are opportunities for market distortions here, where people get paid more for installing an escalator than installing a safe escalator. If you don’t _use_ your ethics, if you don’t wield them as part of your profession, they will grow no stronger. But if you want a safe escalator, by far the best way to get one - if you can manage it - is to find an engineer who naturally doesn’t want to hurt people. Then you’ve just got to keep the managers from demanding that the escalator ship immediately and without all those expensive safety gadgets.\n\nThe first iron-clad steamships were actually _much safer_ than the _Titanic_; the first ironclads were built by engineers without much management supervision, who could design in safety features to their heart’s content.  The _Titanic_ was built in an era of cutthroat price competition between ocean liners.  The grand fanfare about it being unsinkable was a marketing slogan like “World’s Greatest Laundry Detergent”, not a failure of engineering prediction.\n\nYes, safety inspectors, yes, design reviews; but these just _verify _that the engineer put forth an effort of ethical design intelligence. Safety-inspecting doesn’t build an elevator. Ethics, to be effective, must be part of the intelligence that expresses those ethics - you can’t add it in like icing on a cake.\n\nWhich leads into the question of the ethics of AI. “Ethics, to be effective, must be part of the intelligence that expresses those ethics - you can’t add it in like icing on a cake.” My goodness, I wonder how I could have learned such [Deep Wisdom](http://www.overcomingbias.com/2007/10/how-to-seem-and.html)?\n\nBecause I studied AI, and the art spoke to me.  Then I translated it back into English.\n\nThe truth is that I can’t inveigh properly on bioethics, because I am not myself a doctor or a bioengineer. If there is a special ethic of medicine, beyond the obvious, I do not know it. I have not worked enough healing for that art to speak to me.\n\nWhat I do know a thing or two about, is AI. There I can testify definitely and from direct knowledge, that anyone who sets out to study “AI ethics” without a technical grasp of cognitive science, is [absolutely doomed](http://www.overcomingbias.com/2007/10/inferential-dis.html).\n\nIt’s the technical knowledge of AI that forces you to [deal with the world in its own strange terms,](http://www.overcomingbias.com/2007/05/think_like_real.html) rather than the surface-level concepts of everyday life. In everyday life, you can take for granted that “people” are easy to identify; if you look at the modern world, the humans are easy to pick out, to categorize. An unusual boundary case, like Terri Schiavo, can throw a whole nation into a panic: Is she “alive” or “dead”? AI explodes the language that people are described of, unbundles the properties that are always together in human beings. Losing the standard view, throwing away the human conceptual language, forces you to _think for yourself_ about ethics, rather than parroting back things that sound Deeply Wise.\n\nAll of this comes of studying the math, nor may it be divorced from the math. That’s not as comfortably egalitarian as my earlier statement that ethics isn’t meant to be complicated. But if you mate ethics to a highly technical profession, you’re going to get ethics expressed in a _conceptual language_ that is highly technical.\n\nThe technical knowledge provides the conceptual language in which to express ethical problems, ethical options, ethical decisions. If politicians don’t understand the distinction between terminal value and instrumental value, or the difference between a utility function and a probability distribution, then some fundamental _problems_ in Friendly AI are going to be complete gibberish to them - never mind the solutions. I’m sorry to be the one to say this, and I don’t like it either, but Lady Reality does not have the goal of making things easy for political idealists.\n\nIf it helps, the technical ethical thoughts I’ve had so far require only comparatively basic math like Bayesian decision theory, not high-falutin’ complicated damn math like real mathematicians do all day. Hopefully this condition does not hold merely because I am stupid.\n\nSeveral of the responses to Minsky’s statement that politicians should be the ones to “prevent bad things from happening” were along the lines of “Politicans are not particularly good at this, but neither necessarily are most scientists.” I think it’s sad but true that modern industrial civilization, or even modern academia, imposes many shouting external demands within which the quieter internal voice of ethics is lost. It may even be that a majority of people are not particularly ethical to begin with; the thought seems to me uncomfortably elitist, but that doesn’t make it comfortably untrue.\n\nIt may even be true that most scientists, say in AI, haven’t really had a lot of opportunity to express their ethics and so the art hasn’t said anything in particular to them.\n\nIf you talk to some AI scientists about the Singularity / Intelligence Explosion they may say something cached like, “Well, who’s to say that humanity really ought to survive?” This doesn’t sound to me like someone whose art is speaking to them. But then artificial intelligence is not the same as artificial _general _intelligence; and, well, to be brutally honest, I think a lot of people who claim to be working in AGI haven’t really gotten all that far in their pursuit of the art.\n\nSo, if I listen to the voice of experience, rather to the voice of comfort, I find that most people are not very good at ethical thinking. Even most doctors - who ought properly to be confronting ethical questions in every day of their work - don’t go on to write famous memoirs about their ethical insights. The terrifying truth may be that Sturgeon’s Law applies to ethics as it applies to so many other human endeavors: “Ninety percent of everything is crap.”\n\nSo asking an engineer an ethical question is not a sure-fire way to get an especially ethical answer. I wish it were true, but it isn’t.\n\nBut what experience tells me, is that there is no way to obtain the ethics of a _technical _profession _except_ by being ethical inside that profession. I’m skeptical enough of nondoctors who propose to tell doctors how to be ethical, but I _know _it’s not possible in AI. There are all sorts of AI-ethical questions that anyone should be able to answer, like “Is it good for a robot to kill people? No.” But if a dilemma requires more than this, the specialist ethical expertise will only come from someone who has practiced expressing their ethics from inside their profession.\n\nThis doesn’t mean that all AI people are on their own. It means that if you want to have specialists telling AI people how to be ethical, the “specialists” have to be AI people who express their ethics within their AI work, and _then _they can talk to other AI people about what the art said to them.\n\nIt may be that most AI people will not be above-average at AI ethics, but without technical knowledge of AI you don’t even get an _opportunity_ to develop ethical expertise because you’re not thinking in the right language. That’s the way it is in my profession. Your mileage may vary.\n\nIn other words:  To get good AI ethics you need someone technically good at AI, but not all people technically good at AI are automatically good at AI ethics. The technical knowledge is _necessary_ but not _sufficient_ to ethics.\n\nWhat if you think there are specialized ethical concepts, typically taught in philosophy classes, which AI ethicists will need? Then you need to make sure that at least some AI people take those philosophy classes. If there is such a thing as special ethical knowledge, it has to _combine in the same person_ who has the technical knowledge.\n\nHeuristics and biases are critically important knowledge relevant to ethics, in my humble opinion. But if you want that knowledge expressed in a profession, you’ll have to find a professional expressing their ethics and teach them about heuristics and biases - not pick a random cognitive psychologist off the street to add supervision, like so much icing slathered over a cake.\n\nMy nightmare here is people saying, “Aha! A randomly selected AI researcher is not guaranteed to be ethical!” So they turn the task over to professional “ethicists” who are _guaranteed_ to fail: who will simultaneously try to sound counterintuitive enough to be worth paying for as specialists, while also making sure to not think up anything _really_ technical that would scare off the foundation directors who approve their grants.\n\nBut even if professional “AI ethicists” fill the popular air with nonsense, all is not lost. AIfolk who express their ethics as a continuous, non-separate, non-special function of the same life-existence that expresses their AI work, will yet learn a thing or two about the special ethics pertaining to AI. They will not be able to avoid it. Thinking that ethics is a separate profession which judges engineers from above, is like thinking that math is a separate profession which judges engineers from above. If you’re doing ethics _right,_ you can’t _separate _it from your profession.",
      "plaintextDescription": "Originally written in 2007.\n\n----------------------------------------\n\nMarvin Minsky in an interview with Danielle Egan for New Scientist:\n\n> Minsky: The reason we have politicians is to prevent bad things from happening. It doesn’t make sense to ask a scientist to worry about the bad effects of their discoveries, because they’re no better at that than anyone else. Scientists are not particularly good at social policy.\n\n> Egan: But shouldn’t they have an ethical responsibility for their inventions\n\n> Minsky: No they shouldn’t have an ethical responsibility for their inventions. They should be able to do what they want. You shouldn’t have to ask them to have the same values as other people. Because then you won’t get them. They’ll make stupid decisions and not work on important things, because they see possible dangers. What you need is a separation of powers. It doesn’t make any sense to have the same person do both.\n\nThe Singularity Institute was recently asked to comment on this interview - which by the time it made it through the editors at New Scientist, contained just the unvarnished quote “Scientists shouldn’t have an ethical responsibility for their inventions. They should be able to do what they want. You shouldn’t have to ask them to have the same values as other people.” Nice one, New Scientist. Thanks to Egan for providing the original interview text.\n\nThis makes an interesting contrast with what I said in my “Cognitive biases” chapter for Bostrom’s Global Catastrophic Risks:\n\n> Someone on the physics-disaster committee should know what the term “existential risk” means; should possess whatever skills the field of existential risk management has accumulated or borrowed. For maximum safety, that person should also be a physicist. The domain-specific expertise and the expertise pertaining to existential risks should combine in one person. I am skeptical that a scholar of heuristics and biases, unable to read physics equations, could check the work of physic",
      "wordCount": 2726
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "oNcqyaWPXNGTTRPHm",
        "name": "Existential risk",
        "slug": "existential-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cq4DsXzGRXJBmYuyB",
    "title": "Transhumanists Don't Need Special Dispositions",
    "slug": "transhumanists-don-t-need-special-dispositions",
    "url": null,
    "baseScore": 98,
    "voteCount": 43,
    "viewCount": null,
    "commentCount": 18,
    "createdAt": null,
    "postedAt": "2018-12-07T22:24:17.072Z",
    "contents": {
      "markdown": "_This essay was originally posted in 2007._\n\n* * *\n\nI have [claimed](https://www.lesswrong.com/posts/Aud7CL7uhz55KL8jG/transhumanism-as-simplified-humanism) that transhumanism arises strictly from love of life.  A bioconservative humanist says that it is good to save someone's life or cure them of debilitating syndromes if they are young, but once they are \"too old\" (the exact threshold is rarely specified) we should stop trying to keep them alive and healthy.  A transhumanist says unconditionally:  \"Life is good, death is bad; health is good, death is bad.\"  Whether you're 5, 50, or 500, life is good, why die?  Nothing more is required.\n\nThen why is there a widespread misunderstanding that transhumanism involves a special fetish for technology, or an unusually strong fear of death, or some other abnormal personal disposition?\n\nI offer an analogy:  Rationality is often thought to be about cynicism.  The one comes to us and says, \"Fairies make the rainbow; I believe this because it makes me feel warm and fuzzy inside.\"  And you say, \"No.\"  And the one reasons, \"I believe in fairies because I enjoy feeling warm and fuzzy.  If I imagine that there are no fairies, I feel a sensation of deadly existential emptiness.  Rationalists say there are no fairies.  So they must enjoy sensations of deadly existential emptiness.\"  Actually, rationality follows a completely different rule - examine the rainbow very closely and see how it actually works.  If we find fairies, we accept that, and if we don't find fairies, we accept that too.  The look-and-see rule makes no mention of our personal feelings about fairies, and it fully determines the rational answer.  So you cannot infer that a competent rationalist hates fairies, or likes feelings of deadly existential emptiness, by looking at what they believe about rainbows.\n\nBut this rule - the notion of actually _looking_ at things - is not widely understood.  The more common belief is that rationalists make up stories about boring old math equations, instead of pretty little fairies, because rationalists have a math fetish instead of a fairy fetish.  A personal taste, and an odd one at that, but how else would you explain rationalists' strange and unusual beliefs?\n\nSimilarly, love of life is not commonly understood as a motive for saying that, if someone is sick, and we can cure them using medical nanotech, we really ought to do that.  Instead people suppose that transhumanists have a taste for technology, a futurism fetish, that we just love those pictures of little roving nanobots.  A personal taste, and an odd one at that, but how else would you explain transhumanists' strange and unusual moral judgments?\n\nOf course I'm not claiming that transhumanists take no joy in technology.  That would be like saying a rationalist should take no joy in math.  _Homo sapiens_ is the tool-making species; a complete human being should take joy in a contrivance of special cleverness, just as we take joy in music or storytelling.  It is likewise incorrect to say that the aesthetic beauty of a technology is a distinct good from its beneficial use - their sum is not merely additive, there is a harmonious combination.  The equations underlying a rainbow are all the more beautiful for being true, rather than just made up.  But the esthetic of transhumanism is very strict about positive outcomes taking precedence over how cool the technology looks.  If the choice is between using an elegant technology to save a million lives and using an ugly technology to save a million and one lives, you choose the latter.  Otherwise the harmonious combination vanishes like a soap bubble popping.  It would be like preferring a more elegant theory of rainbows that was not actually true.\n\nIn social psychology, the \"[correspondence bias](http://en.wikipedia.org/wiki/Fundamental_attribution_error)\" is that we see far too direct a correspondence between others' actions and their personalities.  As [Gilbert and Malone](http://www.danielgilbert.com/Gilbert%20&%20Malone%20%28CORRESPONDENCE%20BIAS%29.pdf) put it, we \"draw inferences about a person's unique and enduring dispositions from behaviors that can be entirely explained by the situations in which they occur.\"  For example, subjects listen to speakers giving speeches for and against abortion.  The subjects are explicitly told that the speakers are reading prepared speeches assigned by coin toss - and yet the subjects still believe the pro-abortion speakers are personally in favor of abortion.\n\nWhen we see someone else kick a vending machine for no visible reason, we assume he is \"an angry person\".  But if you yourself kick the vending machine, you will tend to see your actions as caused by your situation, not your disposition.  The bus was late, the train was early, your report is overdue, and now the damned vending machine has eaten your money twice in a row.  But others will not see this; they cannot see your situation trailing behind you in the air, and so they will attribute your behavior to your disposition.\n\nBut, really, most of the people in the world are not mutants - are probably not exceptional in any given facet of their emotional makeup.  A key to understanding human nature is to realize that the vast majority of people see themselves as behaving normally, given their situations.  If you wish to understand people's behaviors, then don't ask after mutant dispositions; rather, ask what situation they might believe themselves to be in.\n\nSuppose I gave you a control with two buttons, a red button and a green button.  The red button destroys the world, and the green button stops the red button from being pressed.  Which button would you press?  The green one.  This response is _perfectly normal._ No special world-saving disposition is required, let alone a special preference for the color green.  Most people would choose to press the green button and save the world, _if they saw their situation in those terms._\n\nAnd yet people sometimes ask me why I want to [save the world](https://intelligence.org/files/AIPosNegFactor.pdf). _Why?_ They want to know _why_ someone would want to save the world?  Like you have to be traumatized in childhood or something?  _Give_ me a _break_.\n\nWe all seem normal to ourselves.  One must understand this to understand all those strange other people.\n\nCorrespondence bias can also be seen as essentialist reasoning, like explaining rain by water spirits, or explaining fire by phlogiston.  If you kick a vending machine, why, it must be because you have a vending-machine-kicking disposition.\n\nSo the transhumanist says, \"Let us use this technology to cure aging.\"  And the reporter thinks, _How strange!  He must have been born with an unusual technology-loving disposition_.  Or, _How strange!  He must have an unusual horror of aging!_\n\nTechnology means many things to many people.  So too, death, aging, sickness have different implications to different personal philosophies.  Thus, different people incorrectly attribute transhumanism to different mutant dispositions.\n\nIf someone prides themselves on being cynical of all Madison Avenue marketing, and the meaning of technology unto them is Madison Avenue marketing, they will see transhumanists as shills for The Man, trying to get us to spend money on expensive but ultimately meaningless toys.\n\nIf someone has been fed Deep Wisdom about how death is part of the Natural Cycle of Life ordained by heaven as a transition to beyond the flesh, etc., then they will see transhumanists as Minions of Industry, Agents of the Anti-Life Death Force that is Science.\n\nIf someone has a postmodern ironic attitude toward technology, then they'll see transhumanists as being on a mission to make the world even stranger, more impersonal, than it already is - with the word \"Singularity\" obviously referring to complete disconnection and incomprehensibility.\n\nIf someone sees computers and virtual reality as an escape from the real world, opposed to sweat under the warm sun and the scent of real flowers, they will think that transhumanists must surely hate the body; that they want to escape the scent of flowers into a grayscale virtual world.\n\nIf someone associates technology with Gosh-Wow-Gee-Whiz-So-Cool flying cars and jetpacks, they'll think transhumanists have gone overboard on youthful enthusiasm for toys.\n\nIf someone associates the future with scary hyperbole from Wired magazine - _humans will merge with their machines and become indistinguishable from them_ \\- they'll think that transhumanists yearn for the cold embrace of metal tentacles, that we want to _lose our identity and be eaten by the Machine_ or some other dystopian nightmare of the month.\n\nIn all cases they make the same mistake - drawing a one-to-one correspondence between the way in which the behavior strikes them as strange, and a mutant mental essence that exactly fits the behavior.  This is an unnecessarily burdensome explanation for why someone would advocate healing the sick by any means available, including advanced technology.",
      "plaintextDescription": "This essay was originally posted in 2007.\n\n----------------------------------------\n\nI have claimed that transhumanism arises strictly from love of life.  A bioconservative humanist says that it is good to save someone's life or cure them of debilitating syndromes if they are young, but once they are \"too old\" (the exact threshold is rarely specified) we should stop trying to keep them alive and healthy.  A transhumanist says unconditionally:  \"Life is good, death is bad; health is good, death is bad.\"  Whether you're 5, 50, or 500, life is good, why die?  Nothing more is required.\n\nThen why is there a widespread misunderstanding that transhumanism involves a special fetish for technology, or an unusually strong fear of death, or some other abnormal personal disposition?\n\nI offer an analogy:  Rationality is often thought to be about cynicism.  The one comes to us and says, \"Fairies make the rainbow; I believe this because it makes me feel warm and fuzzy inside.\"  And you say, \"No.\"  And the one reasons, \"I believe in fairies because I enjoy feeling warm and fuzzy.  If I imagine that there are no fairies, I feel a sensation of deadly existential emptiness.  Rationalists say there are no fairies.  So they must enjoy sensations of deadly existential emptiness.\"  Actually, rationality follows a completely different rule - examine the rainbow very closely and see how it actually works.  If we find fairies, we accept that, and if we don't find fairies, we accept that too.  The look-and-see rule makes no mention of our personal feelings about fairies, and it fully determines the rational answer.  So you cannot infer that a competent rationalist hates fairies, or likes feelings of deadly existential emptiness, by looking at what they believe about rainbows.\n\nBut this rule - the notion of actually looking at things - is not widely understood.  The more common belief is that rationalists make up stories about boring old math equations, instead of pretty little fairies, becaus",
      "wordCount": 1411
    },
    "tags": [
      {
        "_id": "jiuackr7B5JAetbF6",
        "name": "Transhumanism",
        "slug": "transhumanism"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Aud7CL7uhz55KL8jG",
    "title": "Transhumanism as Simplified Humanism",
    "slug": "transhumanism-as-simplified-humanism",
    "url": null,
    "baseScore": 181,
    "voteCount": 89,
    "viewCount": null,
    "commentCount": 34,
    "createdAt": null,
    "postedAt": "2018-12-05T20:12:13.114Z",
    "contents": {
      "markdown": "_This essay was originally posted in 2007._\n\n* * *\n\n[Frank Sulloway](http://www.robertboynton.com/?art_id=119) once said: “Ninety-nine per cent of what Darwinian theory says about human behavior is so obviously true that we don’t give Darwin credit for it. Ironically, psychoanalysis has it over Darwinism precisely because its predictions are so outlandish and its explanations are so counterintuitive that we think, _Is that really true? How radical!_ Freud’s ideas are so intriguing that people are willing to pay for them, while one of the great disadvantages of Darwinism is that we feel we know it already, because, in a sense, we do.”\n\nSuppose you find an unconscious six-year-old girl lying on the train tracks of an active railroad. What, morally speaking, ought you to do in this situation? Would it be better to leave her there to get run over, or to try to save her? How about if a 45-year-old man has a debilitating but nonfatal illness that will severely reduce his quality of life – is it better to cure him, or not cure him?\n\nOh, and by the way: This is not a trick question.\n\nI answer that I would save them if I had the power to do so – both the six-year-old on the train tracks, and the sick 45-year-old. The obvious answer isn’t _always_ the best choice, but sometimes it _is._\n\nI won’t be lauded as a brilliant ethicist for my judgments in these two ethical dilemmas. My answers are not surprising enough that people would pay me for them. If you go around proclaiming “What does two plus two equal? Four!” you will not gain a reputation as a deep thinker. But it is still the correct answer.\n\nIf a young child falls on the train tracks, it is good to save them, and if a 45-year-old suffers from a debilitating disease, it is good to cure them. If you have a logical turn of mind, you are bound to ask whether this is a special case of a general ethical principle which says “Life is good, death is bad; health is good, sickness is bad.” If so – and here we enter into controversial territory – we can follow this general principle to a surprising new conclusion: If a 95-year-old is threatened by death from old age, it would be good to drag them from those train tracks, if possible. And if a 120-year-old is starting to feel slightly sickly, it would be good to restore them to full vigor, if possible. With current technology it is _not_ possible. But if the technology became available in some future year – given sufficiently advanced medical nanotechnology, or such other contrivances as future minds may devise – would you judge it a good thing, to save that life, and stay that debility?\n\nThe important thing to remember, which I think all too many people forget, is that _it is not a trick question._\n\nTranshumanism is simpler – requires fewer bits to specify – because it has no special cases. If you believe professional bioethicists (people who get paid to explain ethical judgments) then the rule “Life is good, death is bad; health is good, sickness is bad” holds only until some critical age, and then flips polarity. Why should it flip? Why not just keep on with life-is-good? It would seem that it is good to save a six-year-old girl, but bad to extend the life and health of a 150-year-old. Then at what _exact_ age does the term in the utility function go from positive to negative? Why?\n\nAs far as a transhumanist is concerned, if you see someone in danger of dying, you should save them; if you can improve someone’s health, you should. There, you’re done. No special cases. You don’t have to ask anyone’s age.\n\nYou also don’t ask whether the remedy will involve only “primitive” technologies (like a stretcher to lift the six-year-old off the railroad tracks); or technologies invented less than a hundred years ago (like penicillin) which nonetheless seem ordinary because they were around when you were a kid; or technologies that seem scary and sexy and futuristic (like gene therapy) because they were invented after you turned 18; or technologies that seem absurd and implausible and sacrilegious (like nanotech) because they haven’t been invented yet. Your ethical dilemma report form doesn’t have a line where you write down the invention year of the technology. Can you save lives? Yes? Okay, go ahead. There, you’re done.\n\nSuppose a boy of 9 years, who has tested at IQ 120 on the Wechsler-Bellvue, is threatened by a lead-heavy environment or a brain disease which will, if unchecked, gradually reduce his IQ to 110. I reply that it is a good thing to save him from this threat. If you have a logical turn of mind, you are bound to ask whether this is a special case of a general ethical principle saying that intelligence is precious. Now the boy’s sister, as it happens, currently has an IQ of 110. If the technology were available to gradually raise her IQ to 120, without negative side effects, would you judge it good to do so?\n\nWell, of course. Why not? It’s not a trick question. Either it’s better to have an IQ of 110 than 120, in which case we should strive to decrease IQs of 120 to 110. Or it’s better to have an IQ of 120 than 110, in which case we should raise the sister’s IQ if possible. As far as I can see, the obvious answer is the correct one.\n\nBut – you ask – _where does it end?_ It may seem well and good to talk about extending life and health out to 150 years – but what about 200 years, or 300 years, or 500 years, or more? What about when – in the course of properly integrating all these new life experiences and expanding one’s mind accordingly over time – the equivalent of IQ must go to 140, or 180, or beyond human ranges?\n\nWhere does it end? It doesn’t. Why should it? Life is good, health is good, beauty and happiness and fun and laughter and challenge and learning are good. This does not change for arbitrarily large amounts of life and beauty. If there were an upper bound, it would be a special case, and that would be inelegant.\n\nUltimate physical limits may or may not permit a lifespan of at least length X for some X – just as the medical technology of a particular century may or may not permit it. But physical limitations are questions of simple fact, to be settled strictly by experiment. Transhumanism, as a moral philosophy, deals only with the question of whether a healthy lifespan of length X is desirable _if_ it is physically possible. Transhumanism answers yes for all X. Because, you see, it’s not a trick question.\n\nSo that is “transhumanism” – loving life without special exceptions and without upper bound.\n\nCan transhumanism really be that simple? Doesn’t that make the philosophy trivial, if it has no extra ingredients, just common sense? Yes, in the same way that the scientific method is nothing but common sense.\n\nThen why have a complicated special name like “transhumanism” ? For the same reason that “scientific method” or “secular humanism” have complicated special names. If you take common sense and rigorously apply it, through multiple inferential steps, to areas outside everyday experience, successfully avoiding many possible distractions and tempting mistakes along the way, then it often ends up as a minority position and people give it a special name.\n\nBut a moral philosophy should not _have_ special ingredients. The purpose of a moral philosophy is not to look delightfully strange and counterintuitive, or to provide employment to bioethicists. The purpose is to guide our choices toward life, health, beauty, happiness, fun, laughter, challenge, and learning. If the judgments are simple, that is no black mark against them – morality doesn’t always have to be complicated.\n\nThere is nothing in transhumanism but the same common sense that underlies standard humanism, rigorously applied to cases outside our modern-day experience. A million-year lifespan? If it’s possible, why not? The prospect may seem very foreign and strange, relative to our current everyday experience. It may create a sensation of future shock. And yet – is life a _bad_ thing?\n\nCould the moral question really be just that simple?\n\nYes.",
      "plaintextDescription": "This essay was originally posted in 2007.\n\n----------------------------------------\n\nFrank Sulloway once said: “Ninety-nine per cent of what Darwinian theory says about human behavior is so obviously true that we don’t give Darwin credit for it. Ironically, psychoanalysis has it over Darwinism precisely because its predictions are so outlandish and its explanations are so counterintuitive that we think, Is that really true? How radical! Freud’s ideas are so intriguing that people are willing to pay for them, while one of the great disadvantages of Darwinism is that we feel we know it already, because, in a sense, we do.”\n\nSuppose you find an unconscious six-year-old girl lying on the train tracks of an active railroad. What, morally speaking, ought you to do in this situation? Would it be better to leave her there to get run over, or to try to save her? How about if a 45-year-old man has a debilitating but nonfatal illness that will severely reduce his quality of life – is it better to cure him, or not cure him?\n\nOh, and by the way: This is not a trick question.\n\nI answer that I would save them if I had the power to do so – both the six-year-old on the train tracks, and the sick 45-year-old. The obvious answer isn’t always the best choice, but sometimes it is.\n\nI won’t be lauded as a brilliant ethicist for my judgments in these two ethical dilemmas. My answers are not surprising enough that people would pay me for them. If you go around proclaiming “What does two plus two equal? Four!” you will not gain a reputation as a deep thinker. But it is still the correct answer.\n\nIf a young child falls on the train tracks, it is good to save them, and if a 45-year-old suffers from a debilitating disease, it is good to cure them. If you have a logical turn of mind, you are bound to ask whether this is a special case of a general ethical principle which says “Life is good, death is bad; health is good, sickness is bad.” If so – and here we enter into controversial territory – ",
      "wordCount": 1383
    },
    "tags": [
      {
        "_id": "ZTRNmvQGgoYiymYnq",
        "name": "Consequentialism",
        "slug": "consequentialism"
      },
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "vmvTYnmaKA73fYDe5",
        "name": "Life Extension",
        "slug": "life-extension"
      },
      {
        "_id": "jiuackr7B5JAetbF6",
        "name": "Transhumanism",
        "slug": "transhumanism"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "YicoiQurNBxSp7a65",
    "title": "Is Clickbait Destroying Our General Intelligence?",
    "slug": "is-clickbait-destroying-our-general-intelligence",
    "url": null,
    "baseScore": 210,
    "voteCount": 132,
    "viewCount": null,
    "commentCount": 68,
    "createdAt": null,
    "postedAt": "2018-11-16T23:06:29.506Z",
    "contents": {
      "markdown": "(Cross-posted from Facebook.)\n\n* * *\n\nNow and then people have asked me if I think that other people should also avoid high school or college if they want to develop new ideas. This always felt to me like a wrong way to look at the question, but I didn't know a right one.\n\nRecently I thought of a scary new viewpoint on that subject.\n\nThis started with a conversation with Arthur where he mentioned an idea by Yoshua Bengio about the software for [general intelligence](https://arbital.com/p/general_intelligence/) having been developed memetically. I remarked that I didn't think duplicating this culturally transmitted software would be a significant part of the problem for AGI development. (Roughly: low-fidelity software tends to be algorithmically shallow. Further discussion moved to comment below.)\n\nBut this conversation did get me thinking about the topic of culturally transmitted software that contributes to human general intelligence. That software can be an _important_ gear even if it's an algorithmically shallow part of the overall machinery. Removing a few simple gears that are 2% of a machine's mass can reduce the machine's performance by way more than 2%. Feral children would be the case in point.\n\nA scary question is whether it's possible to do _subtler_ damage to the culturally transmitted software of general intelligence.\n\nI've had the sense before that the Internet is turning our society stupider and meaner. My primary hypothesis is \"The Internet is selecting harder on a larger population of ideas, and sanity falls off the selective frontier once you select hard enough.\"\n\nTo review, there's a general idea that strong (social) selection on a characteristic imperfectly correlated with some other metric of goodness can be bad for that metric, where weak (social) selection on that characteristic was good. If you press scientists a _little_ for publishable work, they might do science that's of greater interest to others. If you select very harshly on publication records, the academics spend all their time worrying about publishing and real science falls by the wayside.\n\nOn my feed yesterday was an essay complaining about how the intense competition to get into Harvard is producing a monoculture of students who've lined up every single standard accomplishment and how these students don't know anything else they want to do with their lives. Gentle, soft competition on a few accomplishments might select genuinely stronger students; hypercompetition for the appearance of strength produces weakness, or just emptiness.\n\nA hypothesis I find plausible is that the Internet, and maybe television before it, selected much more harshly from a much wider field of memes; and also allowed tailoring content more narrowly to narrower audiences. The Internet is making it possible for ideas that are optimized to appeal hedonically-virally within a filter bubble to outcompete ideas that have been even slightly optimized for anything else. We're looking at a collapse of reference to expertise because deferring to expertise costs a couple of hedons compared to being told that all your intuitions are perfectly right, and at the harsh selective frontier there's no room for that. We're looking at a collapse of interaction between bubbles because there used to be just a few newspapers serving all the bubbles; and now that the bubbles have separated there's little incentive to show people how to be fair in their judgment of ideas for other bubbles, it's not _the most_ appealing Tumblr content. Print magazines in the 1950s were hardly perfect, but they could get away with sometimes presenting complicated issues as complicated, because there weren't a hundred blogs saying otherwise and stealing their clicks. Or at least, that's the hypothesis.\n\nIt seems plausible to me that _basic_ software for intelligent functioning is being damaged by this hypercompetition. Especially in a social context, but maybe even outside it; that kind of thing tends to slop over. When someone politely presents themselves with a careful argument, does your cultural software tell you that you're supposed to listen and make a careful response, or make fun of the other person and then laugh about how they're upset? What about when your own brain tries to generate a careful argument? Does your cultural milieu give you any examples of people showing how to really care deeply about something (i.e. debate consequences of paths and hew hard to the best one), or is everything you see just people competing to be loud in their identification? The Occupy movement not having any demands or agenda could represent mild damage to a gear of human general intelligence that was culturally transmitted and that enabled processing of a certain kind of goal-directed behavior. And I'm not sure to what extent that is merely a metaphor, versus it being simple fact if we could look at the true software laid out. If you look at how some bubbles are talking and thinking now, \"intellectually feral children\" doesn't seem like entirely inappropriate language.\n\nShortly after that conversation with Arthur, it occurred to me that I was pretty much raised and socialized by my parents' collection of science fiction.\n\nMy parents' collection of _old_ science fiction.\n\nIsaac Asimov. H. Beam Piper. A. E. van Vogt. Early Heinlein, because my parents didn't want me reading the later books.\n\nAnd when I did try reading science fiction from later days, a lot of it struck me as... icky. _Neuromancer_, bleah, what is _wrong_ with this book, it feels _damaged_, why do people like this, it feels like there's way too much flash and it ate the substance, it's showing off way too hard.\n\nAnd now that I think about it, I feel like a lot of my writing on rationality would be a lot more popular if I could go back in time to the 1960s and present it there. \"Twelve Virtues of Rationality\" is what people could've been reading instead of Heinlein's _Stranger in a Strange Land_, to take a different path from the branching point that found _Stranger in a Strange Land_ appealing.\n\nI didn't stick to merely the culture I was raised in, because that wasn't what that culture said to do. The characters I read didn't keep to the way _they_ were raised. They were constantly being challenged with new ideas and often modified or partially rejected those ideas in the course of absorbing them. If you were immersed in an alien civilization that had some good ideas, you were supposed to consider it open-mindedly and then steal only the good parts. Which... kind of sounds axiomatic to me? You could make a case that this is an obvious guideline for how to do _generic optimization_. It's just what you do to process an input. And yet \"when you encounter a different way of thinking, judge it open-mindedly and then steal only the good parts\" is directly contradicted by some modern software that seems to be memetically hypercompetitive. It probably sounds a bit alien or weird to some people reading this, at least as something that you'd say out loud. Software contributing to generic optimization has been damaged.\n\nLater the Internet came along and exposed me to some modern developments, some of which are indeed improvements. But only after I had a cognitive and ethical foundation that could judge which changes were progress versus damage. More importantly, a cognitive foundation that had the idea of even _trying_ to do that. Tversky and Kahneman didn't exist in the 1950s, but when I was [exposed](https://www.lesswrong.com/posts/Ti3Z7eZtud32LhGZT/my-bayesian-enlightenment) to this new cognitive biases literature, I reacted like an Isaac Asimov character trying to integrate it into their existing ideas about psychohistory, instead of a William Gibson character wondering how it would look on a black and chrome T-Shirt. If that reference still means anything to anyone.\n\nI suspect some culturally transmitted parts of the general intelligence software got damaged by radio, television, and the Internet, with a key causal step being an increased hypercompetition of ideas compared to earlier years. I suspect this independently of any other hypotheses about my origin story. It feels to me like the historical case for this thesis ought to be visible by mere observation to anyone who watched the quality of online discussion degrade from 2002 to 2017.\n\nBut if you consider me to be more than usually intellectually productive for an average Ashkenazic genius in the modern generation, then in this connection it's an interesting and scary further observation that I was initially socialized by books written before the Great Stagnation. Or by books written by authors from only a single generation later, who read a lot of old books themselves and didn't watch much television.\n\nThat hypothesis doesn't feel wrong to me the way that \"oh you just need to not go to college\" feels wrong to me.",
      "plaintextDescription": "(Cross-posted from Facebook.)\n\n----------------------------------------\n\nNow and then people have asked me if I think that other people should also avoid high school or college if they want to develop new ideas. This always felt to me like a wrong way to look at the question, but I didn't know a right one.\n\nRecently I thought of a scary new viewpoint on that subject.\n\nThis started with a conversation with Arthur where he mentioned an idea by Yoshua Bengio about the software for general intelligence having been developed memetically. I remarked that I didn't think duplicating this culturally transmitted software would be a significant part of the problem for AGI development. (Roughly: low-fidelity software tends to be algorithmically shallow. Further discussion moved to comment below.)\n\nBut this conversation did get me thinking about the topic of culturally transmitted software that contributes to human general intelligence. That software can be an important gear even if it's an algorithmically shallow part of the overall machinery. Removing a few simple gears that are 2% of a machine's mass can reduce the machine's performance by way more than 2%. Feral children would be the case in point.\n\nA scary question is whether it's possible to do subtler damage to the culturally transmitted software of general intelligence.\n\nI've had the sense before that the Internet is turning our society stupider and meaner. My primary hypothesis is \"The Internet is selecting harder on a larger population of ideas, and sanity falls off the selective frontier once you select hard enough.\"\n\nTo review, there's a general idea that strong (social) selection on a characteristic imperfectly correlated with some other metric of goodness can be bad for that metric, where weak (social) selection on that characteristic was good. If you press scientists a little for publishable work, they might do science that's of greater interest to others. If you select very harshly on publication records, the aca",
      "wordCount": 1430
    },
    "tags": [
      {
        "_id": "PvridmTCj2qsugQCH",
        "name": "Goodhart's Law",
        "slug": "goodhart-s-law"
      },
      {
        "_id": "kEX5CzbfiAzGn4q8B",
        "name": "Superstimuli",
        "slug": "superstimuli"
      },
      {
        "_id": "bxhzaWtdNoEMMkE8r",
        "name": "General intelligence",
        "slug": "general-intelligence"
      },
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "T4GgauaEfp6dHsR5P",
        "name": "Memetics",
        "slug": "memetics"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "st7DiQP23YQSxumCt",
    "title": "On Doing the Improbable",
    "slug": "on-doing-the-improbable",
    "url": null,
    "baseScore": 131,
    "voteCount": 61,
    "viewCount": null,
    "commentCount": 36,
    "createdAt": null,
    "postedAt": "2018-10-28T20:09:32.056Z",
    "contents": {
      "markdown": "(Cross-posted from Facebook.)\n\n* * *\n\nI've noticed that, by my standards and on an Eliezeromorphic metric, most people seem to require catastrophically high levels of faith in what they're doing in order to stick to it. By this I mean that they would not have stuck to writing the Sequences or HPMOR or working on AGI alignment past the first few months of real [difficulty](https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing), without assigning odds in the vicinity of 10x what I started out assigning that the project would work. And this is not a kind of estimate you can get via good epistemology.\n\nI mean, you can legit estimate 100x higher odds of success than the [Modest](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d) and the Outside Viewers think you can possibly assign to \"writing the most popular HP fanfiction on the planet out of a million contenders on your first try at published long-form fiction or Harry Potter, using a theme of Harry being a rationalist despite there being no evidence of demand for this\" blah blah et Modest cetera. Because in fact Modesty flat-out doesn't work as metacognition. You might as well be reading sheep entrails in whatever way supports your sense of social licensing to accomplish things.\n\nBut you can't get numbers in the range of what I estimate to be something like 70% as the required threshold before people will carry on through bad times. \"It might not work\" is enough to force them to make a great effort to continue past that 30% failure probability. It's not good decision theory but it seems to be how people actually work on group projects where they are not personally madly driven to accomplish the thing.\n\nI don't want to have to artificially cheerlead people every time I want to cooperate in a serious, real, extended shot at accomplishing something. Has anyone ever solved this organizational problem by other means than (a) bad epistemology (b) amazing primate charisma?\n\n* * *\n\nEDIT: Guy Srinivasan reminds us that paying people a lot of money to work on an interesting problem is also standardly known to help in producing real perseverance.",
      "plaintextDescription": "(Cross-posted from Facebook.)\n\n----------------------------------------\n\nI've noticed that, by my standards and on an Eliezeromorphic metric, most people seem to require catastrophically high levels of faith in what they're doing in order to stick to it. By this I mean that they would not have stuck to writing the Sequences or HPMOR or working on AGI alignment past the first few months of real difficulty, without assigning odds in the vicinity of 10x what I started out assigning that the project would work. And this is not a kind of estimate you can get via good epistemology.\n\nI mean, you can legit estimate 100x higher odds of success than the Modest and the Outside Viewers think you can possibly assign to \"writing the most popular HP fanfiction on the planet out of a million contenders on your first try at published long-form fiction or Harry Potter, using a theme of Harry being a rationalist despite there being no evidence of demand for this\" blah blah et Modest cetera. Because in fact Modesty flat-out doesn't work as metacognition. You might as well be reading sheep entrails in whatever way supports your sense of social licensing to accomplish things.\n\nBut you can't get numbers in the range of what I estimate to be something like 70% as the required threshold before people will carry on through bad times. \"It might not work\" is enough to force them to make a great effort to continue past that 30% failure probability. It's not good decision theory but it seems to be how people actually work on group projects where they are not personally madly driven to accomplish the thing.\n\nI don't want to have to artificially cheerlead people every time I want to cooperate in a serious, real, extended shot at accomplishing something. Has anyone ever solved this organizational problem by other means than (a) bad epistemology (b) amazing primate charisma?\n\n----------------------------------------\n\nEDIT: Guy Srinivasan reminds us that paying people a lot of money to work on an int",
      "wordCount": 342
    },
    "tags": [
      {
        "_id": "NzSTgAtKwgivkfeYm",
        "name": "Heroic Responsibility",
        "slug": "heroic-responsibility"
      },
      {
        "_id": "EeSkeTcT4wtW2fWsL",
        "name": "Cause Prioritization",
        "slug": "cause-prioritization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Gg9a4y8reWKtLe3Tn",
    "title": "The Rocket Alignment Problem",
    "slug": "the-rocket-alignment-problem",
    "url": null,
    "baseScore": 232,
    "voteCount": 126,
    "viewCount": null,
    "commentCount": 44,
    "createdAt": null,
    "postedAt": "2018-10-04T00:38:58.795Z",
    "contents": {
      "markdown": "  \n\nThe following is a fictional dialogue building off of [AI Alignment: Why It’s Hard, and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/).\n\n* * *\n\n(_Somewhere in a not-very-near neighboring world, where science took a very different course…_)\n\n* * *\n\n**ALFONSO:**  Hello, Beth. I’ve noticed a lot of speculations lately about “spaceplanes” being used to attack cities, or possibly becoming infused with malevolent spirits that inhabit the celestial realms so that they turn on their own engineers.\n\nI’m rather skeptical of these speculations. Indeed, I’m a bit skeptical that airplanes will be able to even rise as high as stratospheric weather balloons anytime in the next century. But I understand that your institute wants to address the potential problem of malevolent or dangerous spaceplanes, and that you think this is an important present-day cause.\n\n**BETH:**  That’s… really not how we at the Mathematics of Intentional Rocketry Institute would phrase things.\n\nThe problem of malevolent celestial spirits is what all the news articles are focusing on, but we think the real problem is something entirely different. We’re worried that there’s a difficult, theoretically challenging problem which modern-day rocket punditry is mostly overlooking. We’re worried that if you aim a rocket at where the Moon is in the sky, and press the launch button, the rocket may not actually end up at the Moon.\n\n**ALFONSO:**  I understand that it’s very important to design fins that can stabilize a spaceplane’s flight in heavy winds. That’s important spaceplane safety research and someone needs to do it.\n\nBut if you were working on that sort of safety research, I’d expect you to be collaborating tightly with modern airplane engineers to test out your fin designs, to demonstrate that they are actually useful.\n\n**BETH:**  Aerodynamic designs are important features of any safe rocket, and we’re quite glad that rocket scientists are working on these problems and taking safety seriously. That’s not the sort of problem that we at MIRI focus on, though.\n\n**ALFONSO:**  What’s the concern, then? Do you fear that spaceplanes may be developed by ill-intentioned people?\n\n**BETH:**  That’s not the failure mode we’re worried about right now. We’re more worried that right now, _nobody_ can tell you how to point your rocket’s nose such that it goes to the moon, nor indeed _any_ prespecified celestial destination. Whether Google or the US Government or North Korea is the one to launch the rocket won’t make a pragmatic difference to the probability of a successful Moon landing from our perspective, because right now _nobody knows how to aim any kind of rocket anywhere_.\n\n**ALFONSO:**  I’m not sure I understand.\n\n**BETH:**  We’re worried that even if you aim a rocket at the Moon, such that the nose of the rocket is clearly lined up with the Moon in the sky, the rocket won’t go to the Moon. We’re not sure what a realistic path from the Earth to the moon looks like, but we suspect it [might not be a very straight path](https://airandspace.si.edu/webimages/highres/5317h.jpg), and it may not involve pointing the nose of the rocket at the moon at all. We think the most important thing to do next is to advance our understanding of rocket trajectories until we have a better, deeper understanding of what we’ve started calling the “rocket alignment problem”. There are other safety problems, but this rocket alignment problem will probably take the most total time to work on, so it’s the most urgent.\n\n**ALFONSO:**  Hmm, that sounds like a bold claim to me. Do you have a reason to think that there are invisible barriers between here and the moon that the spaceplane might hit? Are you saying that it might get very very windy between here and the moon, more so than on Earth? Both eventualities could be worth preparing for, I suppose, but neither seem likely.\n\n**BETH:**  We don’t think it’s particularly likely that there are invisible barriers, no. And we don’t think it’s going to be especially windy in the celestial reaches — quite the opposite, in fact. The problem is just that we don’t yet know how to plot _any_ trajectory that a vehicle could realistically take to get from Earth to the moon.\n\n**ALFONSO:**  Of course we can’t plot an actual trajectory; wind and weather are too unpredictable. But your claim still seems too strong to me. Just aim the spaceplane at the moon, go up, and have the pilot adjust as necessary. Why wouldn’t that work? Can you prove that a spaceplane aimed at the moon won’t go there?\n\n**BETH:**  We don’t think we can _prove_ anything of that sort, no. Part of the problem is that realistic calculations are extremely hard to do in this area, after you take into account all the atmospheric friction and the movements of other celestial bodies and such. We’ve been trying to solve some drastically simplified problems in this area, on the order of assuming that there is no atmosphere and that all rockets move in perfectly straight lines. Even those unrealistic calculations strongly suggest that, in the much more complicated real world, just pointing your rocket’s nose at the Moon also won’t make your rocket end up at the Moon. I mean, the fact that the real world is more complicated doesn’t exactly make it any _easier_ to get to the Moon.\n\n**ALFONSO:**  Okay, let me take a look at this “understanding” work you say you’re doing…\n\nHuh. Based on what I’ve read about the math you’re trying to do, I can’t say I understand what it has to do with the Moon. Shouldn’t helping spaceplane pilots exactly target the Moon involve looking through lunar telescopes and studying exactly what the Moon looks like, so that the spaceplane pilots can identify particular features of the landscape to land on?\n\n**BETH:**  We think our present stage of understanding is much too crude for a detailed Moon map to be our next research target. We haven’t yet advanced to the point of targeting one crater or another for our landing. We can’t target _anything_ at this point. It’s more along the lines of “figure out how to talk mathematically about curved rocket trajectories, instead of rockets that move in straight lines”. Not even realistically curved trajectories, right now, we’re just trying to get past straight lines at all –\n\n**ALFONSO:**  But planes on Earth move in curved lines all the time, because the Earth itself is curved. It seems reasonable to expect that future spaceplanes will also have the capability to move in curved lines. If your worry is that spaceplanes will only move in straight lines and miss the Moon, and you want to advise rocket engineers to build rockets that move in curved lines, well, that doesn’t seem to me like a great use of anyone’s time.\n\n**BETH:**  You’re trying to draw much too direct of a line between the math we’re working on right now, and actual rocket designs that might exist in the future. It’s _not_ that current rocket ideas are almost right, and we just need to solve one or two more problems to make them work. The conceptual distance that separates anyone from solving the rocket alignment problem is _much greater_ than that.\n\nRight now everyone is _confused_ about rocket trajectories, and we’re trying to become _less confused_. That’s what we need to do next, not run out and advise rocket engineers to build their rockets the way that our current math papers are talking about. Not until we stop being _confused_ about extremely basic questions like why the Earth doesn’t fall into the Sun.\n\n**ALFONSO:**  I don’t think the Earth is going to collide with the Sun anytime soon. The Sun has been steadily circling the Earth for a long time now.\n\n**BETH:**  I’m not saying that our goal is to address the risk of the Earth falling into the Sun. What I’m trying to say is that if humanity’s present knowledge can’t answer questions like “Why doesn’t the Earth fall into the Sun?” then we don’t know very much about celestial mechanics and we won’t be able to aim a rocket through the celestial reaches in a way that lands softly on the Moon.\n\nAs an example of work we’re presently doing that’s aimed at improving our understanding, there’s what we call the “[tiling positions](https://intelligence.org/files/TilingAgentsDraft.pdf)” problem. The tiling positions problem is [how to fire a cannonball from a cannon](https://en.wikipedia.org/wiki/Newton%27s_cannonball) in such a way that the cannonball circumnavigates the earth over and over again, “tiling” its initial coordinates like repeating tiles on a tessellated floor –\n\n**ALFONSO:**  I read a little bit about your work on that topic. I have to say, it’s hard for me to see what firing things from cannons has to do with getting to the Moon. Frankly, it sounds an awful lot like Good Old-Fashioned Space Travel, which everyone knows doesn’t work. Maybe Jules Verne thought it was possible to travel around the earth by firing capsules out of cannons, but the modern study of high-altitude planes has completely abandoned the notion of firing things out of cannons. The fact that you go around talking about firing things out of cannons suggests to me that you haven’t kept up with all the innovations in airplane design over the last century, and that your spaceplane designs will be completely unrealistic.\n\n**BETH:**  We know that rockets will not actually be fired out of cannons. We really, really know that. We’re intimately familiar with the reasons why nothing fired out of a modern cannon is ever going to reach escape velocity. I’ve previously written several sequences of articles in which I describe why cannon-based space travel doesn’t work.\n\n**ALFONSO:**  But your current work is all about firing something out a cannon in such a way that it circles the earth over and over. What could that have to do with any realistic advice that you could give to a spaceplane pilot about how to travel to the Moon?\n\n**BETH:**  Again, you’re trying to draw much too straight a line between the math we’re doing right now, and direct advice to future rocket engineers.\n\nWe think that if we could find an angle and firing speed such that an ideal cannon, firing an ideal cannonball at that speed, on a perfectly spherical Earth with no atmosphere, would lead to that cannonball entering what we would call a “stable orbit” without hitting the ground, then… we might have understood something really fundamental and important about celestial mechanics.\n\nOr maybe not! It’s hard to know in advance which questions are important and which research avenues will pan out. All you can do is figure out the next tractable-looking problem that confuses you, and try to come up with a solution, and hope that you’ll be less confused after that.\n\n**ALFONSO:**  You’re talking about the cannonball hitting the ground as a problem, and how you want to avoid that and just have the cannonball keep going forever, right? But real spaceplanes aren’t going to be aimed at the ground in the first place, and lots of regular airplanes manage to not hit the ground. It seems to me that this “being fired out of a cannon and hitting the ground” scenario that you’re trying to avoid in this “tiling positions problem” of yours just isn’t a failure mode that real spaceplane designers would need to worry about.\n\n**BETH:**  We are not worried about real rockets being fired out of cannons and hitting the ground. That is not why we’re working on the tiling positions problem. In a way, you’re being far too optimistic about how much of rocket alignment theory is already solved! We’re not so close to understanding how to aim rockets that the kind of designs people are talking about now _would_ work if only we solved a particular set of remaining difficulties like not firing the rocket into the ground. You need to go more meta on understanding the kind of progress we’re trying to make.\n\nWe’re working on the tiling positions problem because we think that being able to fire a cannonball at a certain instantaneous velocity such that it enters a stable orbit… is the sort of problem that somebody who could really actually launch a rocket through space and have it move in a particular curve that really actually ended with softly landing on the Moon would be able to solve _easily_. So the fact that we can’t solve it is alarming. If we can figure out how to solve this much simpler, much more crisply stated “tiling positions problem” with imaginary cannonballs on a perfectly spherical earth with no atmosphere, which is a lot easier to analyze than a Moon launch, we might thereby take one more incremental step towards eventually becoming the sort of people who could plot out a Moon launch.\n\n**ALFONSO:**  If you don’t think that Jules-Verne-style space cannons are the wave of the future, I don’t understand why you keep talking about cannons in particular.\n\n**BETH:**  Because there’s a lot of sophisticated mathematical machinery already developed for aiming cannons. People have been aiming cannons and plotting cannonball trajectories since the sixteenth century. We can take advantage of that existing mathematics to say exactly how, if we fired an ideal cannonball in a certain direction, it would plow into the ground. If we tried talking about rockets with realistically varying acceleration, we can’t even manage to prove that a rocket like that _won’t_ travel around the Earth in a perfect square, because with all that realistically varying acceleration and realistic air friction it’s impossible to make any sort of definite statement one way or another. Our present understanding isn’t up to it.\n\n**ALFONSO:**  Okay, another question in the same vein. Why is MIRI sponsoring work on adding up lots of tiny vectors? I don’t even see what that has to do with rockets in the first place; it seems like this weird side problem in abstract math.\n\n**BETH:**  It’s more like… at several points in our investigation so far, we’ve run into the problem of going from a function about time-varying accelerations to a function about time-varying positions. We kept running into this problem as a blocking point in our math, in several places, so we branched off and started trying to analyze it explicitly. Since it’s about the pure mathematics of points that don’t move in discrete intervals, we call it the “[logical undiscreteness](https://intelligence.org/files/QuestionsLogicalUncertainty.pdf)” problem. Some of the ways of investigating this problem involve trying to add up lots of tiny, varying vectors to get a big vector. Then we talk about how that sum seems to change more and more slowly, approaching a limit, as the vectors get tinier and tinier and we add up more and more of them… or at least that’s one avenue of approach.\n\n**ALFONSO:**  I just find it hard to imagine people in future spaceplane rockets staring out their viewports and going, “Oh, no, we don’t have tiny enough vectors with which to correct our course! If only there was some way of adding up even more vectors that are even smaller!” I’d expect future calculating machines to do a pretty good job of that already.\n\n**BETH:**  Again, you’re trying to draw much too straight a line between the work we’re doing now, and the implications for future rocket designs. It’s not like we think a rocket design will almost work, but the pilot won’t be able to add up lots of tiny vectors fast enough, so we just need a faster algorithm and then the rocket will get to the Moon. This is foundational mathematical work that we think might play a role in multiple basic concepts for understanding celestial trajectories. When we try to plot out a trajectory that goes all the way to a soft landing on a moving Moon, we feel confused and blocked. We think part of the confusion comes from not being able to go from acceleration functions to position functions, so we’re trying to resolve our confusion.\n\n**ALFONSO:**  This sounds suspiciously like a philosophy-of-mathematics problem, and I don’t think that it’s possible to progress on spaceplane design by doing philosophical research. The field of philosophy is a stagnant quagmire. Some philosophers still believe that going to the moon is impossible; they say that the celestial plane is fundamentally separate from the earthly plane and therefore inaccessible, which is clearly silly. Spaceplane design is an engineering problem, and progress will be made by engineers.\n\n**BETH:**  I agree that rocket design will be carried out by engineers rather than philosophers. I also share some of your frustration with philosophy in general. For that reason, we stick to well-defined mathematical questions that are likely to have actual answers, such as questions about how to fire a cannonball on a perfectly spherical planet with no atmosphere such that it winds up in a stable orbit.\n\nThis often requires developing new mathematical frameworks. For example, in the case of the logical undiscreteness problem, we’re developing methods for translating between time-varying accelerations and time-varying positions. You can call the development of new mathematical frameworks “philosophical” if you’d like — but if you do, remember that it’s a very different kind of philosophy than the “speculate about the heavenly and earthly planes” sort, and that we’re always pushing to develop new mathematical frameworks or tools.\n\n**ALFONSO:**  So from the perspective of the public good, what’s a good thing that might happen if you solved this logical undiscreteness problem?\n\n**BETH:**  Mainly, we’d be less confused and our research wouldn’t be blocked and humanity could actually land on the Moon someday. To try and make it more concrete – though it’s hard to do that without actually knowing the concrete solution – we might be able to talk about incrementally more realistic rocket trajectories, because our mathematics would no longer break down as soon as we stopped assuming that rockets moved in straight lines. Our math would be able to talk about exact curves, instead of a series of straight lines that approximate the curve.\n\n**ALFONSO:**  An exact curve that a rocket follows? This gets me into the main problem I have with your project in general. I just don’t believe that any future rocket design will be the sort of thing that can be analyzed with absolute, perfect precision so that you can get the rocket to the Moon based on an absolutely plotted trajectory with no need to steer. That seems to me like a bunch of mathematicians who have no clue how things work in the real world, wanting everything to be perfectly calculated. Look at the way Venus moves in the sky; usually it travels in one direction, but sometimes it goes retrograde in the other direction. We’ll just have to steer as we go.\n\n**BETH:**  That’s not what I meant by talking about exact curves… Look, even if we can invent logical undiscreteness, I agree that it’s futile to try to predict, in advance, the precise trajectories of all of the winds that will strike a rocket on its way off the ground. Though I’ll mention parenthetically that things might actually become calmer and easier to predict, once a rocket gets sufficiently high up –\n\n**ALFONSO:**  Why?\n\n**BETH:**  Let’s just leave that aside for now, since we both agree that rocket positions are hard to predict exactly during the atmospheric part of the trajectory, due to winds and such. And yes, if you can’t exactly predict the initial trajectory, you can’t exactly predict the later trajectory. So, indeed, the proposal is definitely not to have a rocket design so perfect that you can fire it at exactly the right angle and then walk away without the pilot doing any further steering. The point of doing rocket math isn’t that you want to predict the rocket’s exact position at every microsecond, in advance.\n\n**ALFONSO:**  Then why obsess over pure math that’s too simple to describe the rich, complicated real universe where sometimes it rains?\n\n**BETH:**  It’s true that a real rocket isn’t a simple equation on a board. It’s true that there are all sorts of aspects of a real rocket’s shape and internal plumbing that aren’t going to have a mathematically compact characterization. What MIRI is doing isn’t the right degree of mathematization for all rocket engineers for all time; it’s the mathematics for us to be using right now (or so we hope).\n\nTo build up the field’s understanding incrementally, we need to talk about ideas whose consequences can be pinpointed precisely enough that people can analyze scenarios in a shared framework. We need enough precision that someone can say, “I think in scenario X, design Y does Z”, and someone else can say, “No, in scenario X, Y actually does W”, and the first person responds, “Darn, you’re right. Well, is there some way to change Y so that it would do Z?”\n\nIf you try to make things realistically complicated at this stage of research, all you’re left with is verbal fantasies. When we try to talk to someone with an enormous flowchart of all the gears and steering rudders they think should go into a rocket design, and we try to explain why a rocket pointed at the Moon doesn’t necessarily end up at the Moon, they just reply, “Oh, my rocket won’t do _that_.” Their ideas have enough vagueness and flex and underspecification that they’ve achieved the safety of nobody being able to prove to them that they’re wrong. It’s impossible to incrementally build up a body of collective knowledge that way.\n\nThe goal is to start building up a library of tools and ideas we can use to discuss trajectories formally. Some of the key tools for formalizing and analyzing _intuitively_ plausible-seeming trajectories haven’t yet been expressed using math, and we can live with that for now. We still try to find ways to represent the key ideas in mathematically crisp ways whenever we can. That’s not because math is so neat or so prestigious; it’s part of an ongoing project to have arguments about rocketry that go beyond “Does not!” vs. “Does so!”\n\n**ALFONSO:**  I still get the impression that you’re reaching for the warm, comforting blanket of mathematical reassurance in a realm where mathematical reassurance doesn’t apply. We can’t obtain a mathematical certainty of our spaceplanes being absolutely sure to reach the Moon with nothing going wrong. That being the case, there’s no point in trying to pretend that we can use mathematics to get absolute guarantees about spaceplanes.\n\n**BETH:**  Trust me, I am not going to feel “reassured” about rocketry no matter what math MIRI comes up with. But, yes, of course you can’t obtain a mathematical assurance of any physical proposition, nor assign probability 1 to any empirical statement.\n\n**ALFONSO:**  Yet you talk about proving theorems – proving that a cannonball will go in circles around the earth indefinitely, for example.\n\n**BETH:**  Proving a theorem about a rocket’s trajectory won’t ever let us feel comfortingly certain about where the rocket is actually going to end up. But if you can prove a theorem which says that your rocket would go to the Moon if it launched in a perfect vacuum, maybe you can attach some steering jets to the rocket and then have it actually go to the Moon in real life. Not with 100% probability, but with probability greater than zero.\n\nThe point of our work isn’t to take current ideas about rocket aiming from a 99% probability of success to a 100% chance of success. It’s to get past an approximately 0% chance of success, which is where we are now.\n\n**ALFONSO:**  Zero percent?!\n\n**BETH:**  Modulo [Cromwell’s Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule), yes, zero percent. If you point a rocket’s nose at the Moon and launch it, it does not go to the Moon.\n\n**ALFONSO:**  I don’t think future spaceplane engineers will actually be that silly, if direct Moon-aiming isn’t a method that works. They’ll lead the Moon’s current motion in the sky, and aim at the part of the sky where Moon will appear on the day the spaceplane is a Moon’s distance away. I’m a bit worried that you’ve been talking about this problem so long without considering such an obvious idea.\n\n**BETH:**  We considered that idea very early on, and we’re pretty sure that it still doesn’t get us to the Moon.\n\n**ALFONSO:**  What if I add steering fins so that the rocket moves in a more curved trajectory? Can you prove that no version of that class of rocket designs will go to the Moon, no matter what I try?\n\n**BETH:**  Can you sketch out the trajectory that you think your rocket will follow?\n\n**ALFONSO:**  It goes from the Earth to the Moon.\n\n**BETH:**  In a bit more detail, maybe?\n\n**ALFONSO:**  No, because in the real world there are always variable wind speeds, we don’t have infinite fuel, and our spaceplanes don’t move in perfectly straight lines.\n\n**BETH:**  Can you sketch out a trajectory that you think a simplified version of your rocket will follow, so we can examine the [assumptions](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) your idea requires?\n\n**ALFONSO:**  I just don’t believe in the general methodology you’re proposing for spaceplane designs. We’ll put on some steering fins, turn the wheel as we go, and keep the Moon in our viewports. If we’re off course, we’ll steer back.\n\n**BETH:**  … We’re actually a bit concerned that [standard steering fins may stop working once the rocket gets high enough](https://intelligence.org/files/Corrigibility.pdf), so you won’t actually find yourself able to correct course by much once you’re in the celestial reaches – like, if you’re already on a good course, you can correct it, but if you screwed up, you won’t just be able to turn around like you could turn around an airplane –\n\n**ALFONSO:**  Why not?\n\n**BETH:**  We can go into that topic too; but even given a simplified model of a rocket that you _could_ steer, a walkthrough of the steps along the path that simplified rocket would take to the Moon would be an important step in moving this discussion forward. Celestial rocketry is a domain that we expect to be unusually difficult – even compared to building rockets on Earth, which is already a famously hard problem because they usually just explode. It’s not that everything has to be neat and mathematical. But the overall difficulty is such that, in a proposal like “lead the moon in the sky,” if the core ideas don’t have a certain amount of solidity about them, it would be equivalent to firing your rocket randomly into the void.\n\nIf it feels like you don’t know for sure whether your idea works, but that it might work; if your idea has many plausible-sounding elements, and to you it feels like nobody has been able to _convincingly_ explain to you how it would fail; then, in real life, that proposal has a roughly 0% chance of steering a rocket to the Moon.\n\nIf it seems like an idea is extremely solid and clearly well-understood, if it feels like this proposal should definitely take a rocket to the Moon without fail in good conditions, then maybe under the best-case conditions we should assign an 85% subjective credence in success, or something in that vicinity.\n\n**ALFONSO:**  So uncertainty automatically means failure? This is starting to sound a bit paranoid, honestly.\n\n**BETH:**  The idea I’m trying to communicate is something along the lines of, “If you can reason rigorously about why a rocket should definitely work in principle, it might work in real life, but if you have anything less than that, then it definitely won’t work in real life.”\n\nI’m not asking you to give me an absolute mathematical proof of empirical success. I’m asking you to give me something more like a sketch for how a simplified version of your rocket could move, that’s sufficiently determined in its meaning that you can’t just come back and say “Oh, I didn’t mean _that_” every time someone tries to figure out what it actually does or pinpoint a failure mode.\n\nThis isn’t an unreasonable demand that I’m imposing to make it impossible for any ideas to pass my filters. It’s the primary bar all of us have to pass to contribute to collective progress in this field. And a rocket design which can’t even pass that conceptual bar has roughly a 0% chance of landing softly on the Moon.",
      "plaintextDescription": "The following is a fictional dialogue building off of AI Alignment: Why It’s Hard, and Where to Start.\n\n----------------------------------------\n\n(Somewhere in a not-very-near neighboring world, where science took a very different course…)\n\n----------------------------------------\n\nALFONSO:  Hello, Beth. I’ve noticed a lot of speculations lately about “spaceplanes” being used to attack cities, or possibly becoming infused with malevolent spirits that inhabit the celestial realms so that they turn on their own engineers.\n\nI’m rather skeptical of these speculations. Indeed, I’m a bit skeptical that airplanes will be able to even rise as high as stratospheric weather balloons anytime in the next century. But I understand that your institute wants to address the potential problem of malevolent or dangerous spaceplanes, and that you think this is an important present-day cause.\n\nBETH:  That’s… really not how we at the Mathematics of Intentional Rocketry Institute would phrase things.\n\nThe problem of malevolent celestial spirits is what all the news articles are focusing on, but we think the real problem is something entirely different. We’re worried that there’s a difficult, theoretically challenging problem which modern-day rocket punditry is mostly overlooking. We’re worried that if you aim a rocket at where the Moon is in the sky, and press the launch button, the rocket may not actually end up at the Moon.\n\nALFONSO:  I understand that it’s very important to design fins that can stabilize a spaceplane’s flight in heavy winds. That’s important spaceplane safety research and someone needs to do it.\n\nBut if you were working on that sort of safety research, I’d expect you to be collaborating tightly with modern airplane engineers to test out your fin designs, to demonstrate that they are actually useful.\n\nBETH:  Aerodynamic designs are important features of any safe rocket, and we’re quite glad that rocket scientists are working on these problems and taking safety seriousl",
      "wordCount": 4588
    },
    "tags": [
      {
        "_id": "RyNWXFjKNcafRKvPh",
        "name": "Agent Foundations",
        "slug": "agent-foundations"
      },
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "yXNtYNHJB54T3bGm3",
        "name": "Dialogue (format)",
        "slug": "dialogue-format"
      },
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "CPP2uLcaywEokFKQG",
    "title": "Toolbox-thinking and Law-thinking",
    "slug": "toolbox-thinking-and-law-thinking",
    "url": null,
    "baseScore": 181,
    "voteCount": 122,
    "viewCount": null,
    "commentCount": 49,
    "createdAt": null,
    "postedAt": "2018-05-31T21:28:19.354Z",
    "contents": {
      "markdown": "Tl;dr:\n\nI've noticed a dichotomy between \"thinking in toolboxes\" and \"thinking in laws\".\n\nThe toolbox style of thinking says it's important to have a big bag of tools that you can adapt to context and circumstance; people who think very toolboxly tend to suspect that anyone who goes talking of a single optimal way is just ignorant of the uses of the other tools.\n\nThe lawful style of thinking, done correctly, distinguishes between descriptive truths, normative ideals, and prescriptive ideals. It may talk about certain paths being optimal, even if there's no executable-in-practice algorithm that yields the optimal path. It considers truths that are not tools.\n\nWithin nearly-Euclidean mazes, the triangle inequality - that the path AC is never spatially longer than the path ABC - is always true but only sometimes useful. The triangle inequality has the prescriptive implication that _if_ you _know_ that one path choice will travel ABC and one path will travel AC, and _if_ the _only_ pragmatic path-merit you care about is going the minimum spatial distance (rather than say avoiding stairs because somebody in the party is in a wheelchair), then you should pick the route AC. But the triangle inequality goes on governing Euclidean mazes whether or not you know which path is which, and whether or not you need to avoid stairs.\n\nToolbox thinkers may be extremely suspicious of this claim of universal lawfulness if it is explained less than perfectly, because it sounds to them like \"Throw away all the other tools in your toolbox! All you need to know is Euclidean geometry, and you can always find the shortest path through any maze, which in turn is always the best path.\"\n\nIf you think that's an unrealistic depiction of a misunderstanding that would never happen in reality, keep reading.\n\n* * *\n\nHere's a recent conversation from Twitter which I'd consider a nearly perfect illustration of the toolbox-vs.-laws dichotomy:\n\n> [David Chapman](https://twitter.com/Meaningness/status/993602725316186112): \"By _rationalism,_ I mean any claim that there is an ultimate criterion according to which thinking and acting could be judged to be correct or optimal... Under this definition, 'rationalism' must go beyond 'systematic methods are often useful, hooray!'... A rationalism claims there is _one weird trick_ to correct thinking, which guarantees an optimal result. (Some rationalisms specify the trick; others insist there must be one, but that it is not currently knowable.) A rationalism makes strongly normative judgments: everyone _ought_ to think that way.\"\n\n> [Graham Rowe](https://twitter.com/grahamsrowe/status/993781462263574528): \"Is it fair to say that rationalists see the world entirely through rationality while meta-rationalists look at rationality as one of many tools (that they can use fluently and appropriately) to be used in service of a broader purpose?\"\n\n> [David Chapman](https://twitter.com/Meaningness/status/993879794180747264): \"More-or-less, I think! Although I don’t think rationalists _do_ see the world entirely through rationality, they just say they think they ought to.\"\n\n> [Julia Galef](https://twitter.com/juliagalef/status/993621055221608449): \"I don't think the 'one weird trick' description is accurate. It's more like: there's one correct normative model in theory, which cannot possibly be approximated by a single rule in practice, but we can look for collections of 'tricks' that seem like they bring us closer to the normative model. e.g., 'On the margin, taking more small risks is likely to increase your EV' is one example.\"\n\n> [David Chapman](https://twitter.com/Meaningness/status/993626127842213889): \"The element that I’d call clearly meta-rational is understanding that rationality is not one well-defined thing but a bag of tricks that are more-or-less applicable in different situations.\"\n\nJulia then [quoted](https://twitter.com/juliagalef/status/993715985768177665) a paper mentioning \"The best prescription for human reasoning is not necessarily to always use the normative model to govern one's thinking.\" To which Chapman [replied](https://twitter.com/Meaningness/status/993718942827921408):\n\n> \"Baron’s distinction between 'normative' and 'prescriptive' is one I haven’t seen before. That seems useful and maybe key. OTOH, if we’re looking for a disagreement crux, it might be whether a normative theory that can’t be achieved, even in principle, is a good thing.\"\n\nI'm now going to badly stereotype this conversation in the form I feel like I've seen it many times previously, including e.g. in the discussion of p-values and frequentist statistics. On this stereotypical depiction, there is a dichotomy between the thinking of Msr. Toolbox and Msr. Lawful that goes like this:\n\nMsr. Toolbox: \"It's important to know how to use a broad variety of statistical tools and adapt them to context. The many ways of calculating p-values form one broad family of tools; any particular tool in the set has good uses and bad uses, depending on context and what exactly you do. [Using likelihood ratios](https://arbital.com/p/likelihoods_not_pvalues/?l=4xx) is an interesting statistical technique, and I'm sure it has its good uses in the right contexts. But it would be very surprising if that one weird trick was the best calculation to do in every paper and every circumstance. If you claim it is the universal best way, then I suspect you of blind idealism, insensitivity to context and nuance, ignorance of all the other tools in the toolbox, the sheer folly of callow youth. You only have a hammer and no real-world experience using screwdrivers, so you claim everything is a nail.\"\n\nMsr. Lawful: \"On complex problems we may not be able to compute exact [Bayesian updates](https://arbital.com/p/bayes_rule_guide/), but the math still describes the _optimal_ update, in the same way that a Carnot cycle describes a thermodynamically ideal engine even if you can't build one. You are unlikely to find a superior viewpoint that makes some other update even more optimal than the Bayesian update, not without doing a great deal of fundamental math research and maybe not at all. We didn't choose that formalism arbitrarily! We have a very broad variety of [coherence theorems](https://arbital.com/p/expected_utility_formalism/?l=7hh) all spotlighting the same central structure of probability theory, saying variations of 'If your behavior cannot be viewed as coherent with probability theory in sense X, you must be executing a dominated strategy and shooting off your foot in sense Y'.\"\n\nI currently suspect that when Msr. Law talks like this, Msr. Toolbox hears \"I prescribe to you the following recipe for your behavior, the Bayesian Update, which you ought to execute in every kind of circumstance.\"\n\nThis also appears to me to frequently turn into one of those awful durable forms of misunderstanding: Msr. Toolbox doesn't see what you could possibly be telling somebody to do with a \"good\" or \"ideal\" algorithm besides executing that algorithm.\n\nIt would not surprise me if there's a symmetrical form of durable misunderstanding where a Lawist has trouble processing a Toolboxer's disclaimer: \"No, you don't understand, I am not trying to describe the one true perfect optimal algorithm here, I'm trying to describe a context-sensitive tool that is sometimes useful in real life.\" Msr. Law may not see what you could possibly be doing with a supposedly \"prudent\" or \"actionable\" recipe besides saying that it's the correct answer, and may feel very suspicious of somebody trying to say everyone should use an answer while disclaiming that they don't really think it's true. Surely this is just the setup for some absurd motte-and-bailey where we claim something is the normative answer, and then as soon as we're challenged we walk back and claim it was 'just one tool in the toolbox'.\n\nAnd it's not like those callow youths the Toolboxer is trying to lecture don't actually exist. The world is full of people who think they have the One True Recipe (_without_ having a normative ideal by which to prove that this is indeed the optimal recipe given their preferences, knowledge, and available computing power).\n\nThe only way I see to resolve this confusion is by grasping a certain particular abstraction and distinction - as a more Lawfully inclined person might put it. Or by being able to deploy both kinds of thinking, depending on context - as a more Toolbox-inclined person might put it.\n\nIt may be that none of my readers need the lecture at this point, but I've learned to be cautious about that sort of thing, so I'll walk through the difference anyways.\n\n* * *\n\nEvery traversable maze has a spatially shortest path; or if we are to be precise in our claims but not our measurements, a set of spatially shortest-ish paths that are all nearly the same distance.\n\nWe may perhaps call this spatially shortest path the \"best\" or \"ideal\" or \"optimal\" path through the maze, _if_ we think our preference for walking shorter distances is the _only_ pragmatically important merit of a path.\n\nThat there exists some shortest path, which may even be optimal according to our preferences, doesn't mean that you can come to an intersection at the maze and \"just choose whichever branch is on the shortest path\".\n\nAnd the fact that you cannot, at an intersection, just choose the shorter path, doesn't mean that the concepts of distance and greater or lesser distance aren't useful.\n\nIt might even be that the maze-owner could truthfully tell you, \"By the way, this right-hand turn here keeps you on the shortest path,\" and yet you'd still be wiser to take the left-hand turn... because you're following the left-hand rule. Where the left-hand rule is to keep your left hand on the wall and go on walking, which works for not getting lost inside a maze whose exit is connected to the start by walls. It's a good rule for agents with sharply bounded memories who can't always remember their paths exactly.\n\nAnd if you're using the left-hand rule it is a terrible, terrible idea to jump walls and make a different turn just once, even if that looks like a great idea at the time, because that is an excellent way to get stuck traversing a disconnected island of connected walls inside the labyrinth.\n\nSo making the left-hand turn leads you to walk the shortest expected distance, relative to the other rules you're using. Making the right-hand turn instead, even if it seemed locally smart, might have you traversing an infinite distance instead.\n\nBut then you may not be on the shortest path, even though you are following the recommendations of the wisest and most prudent rule given your current resources. By contemplating the difference, you know that there is in principle room for improvement. Maybe that inspires you to write a maze-mapping, step-counting cellphone app that lets you get to the exit faster than the left-hand rule.\n\nAnd the reason that there's a better recipe isn't that \"no recipe is perfect\", it isn't that there exists an infinite sequence of ever-better roads. If the maze-owner gave you a map with the shortest path drawn in a line, you could walk the true shortest path and there wouldn't be any shorter path than that.\n\nShortness is a property of paths; a tendency to produce shorter paths is a property of recipes. What makes a phone app an improvement is not that the app is adhering more neatly to some ideal sequence of left and right turns, it's that the path is shorter in a way that can be [defined independently of the app's algorithms](https://arbital.com/p/fair_problem_class/).\n\nOnce you can admit a path can be \"shorter\" in a way that abstracts away from the walker - not _better,_ which does depend on the walker, but _shorter_ \\- it's hard not to admit the notion of there being a shortest path.\n\nI mean, I suppose you could try very hard to never talk about a shortest path and only talk about alternative recipes that yield _shorter_ paths. You could diligently make sure to _never_ imagine this shorterness as a kind of decreased distance-in-performance-space from any 'shortest path'. You could make very sure that in your consideration of new recipes, you maintain your ideological purity as a toolboxer by only ever asking about laws that govern which of two paths are shorter, and never getting any inspiration from any kind of law that governs which path is shortest.\n\nIn which case you would have diligently eliminated a valuable conceptual tool from your toolbox. You would have carefully made sure that you always had to take longer roads to those mental destinations that can be reached the fastest by contemplating properties of ideal solutions, or distance from ideal solutions.\n\nBut why? Why would you?\n\n* * *\n\nI think at this point the Toolbox reply - though I'm not sure I could pass its Ideological Turing Test - might be that idealistic thinking has a great trap and rottenness at its heart.\n\nIt might say:\n\nSomebody who doesn't wisely shut down all this thinking about \"shortest paths\" instead of the left-hand rule as a good tool for some mazes - someone who begins to imagine some unreachable ideal of perfection, instead of a series of apps that find shorter paths most of the time - will surely, in practice, begin to confuse the notion of the left-hand rule, or their other current recipe, with _the shortest path._\n\nAfter all, nobody can see this \"shortest path\", and it's supposedly a virtuous thing. So isn't it an inevitable consequence of human nature that people will start to use that idea as praise for their current recipes?\n\nAnd also in the real world, surely Msr. Law will inevitably forget the extra premise involved with the step from \"spatially shortest path\" to \"best path\"- the contextual requirement that our only important preference was shorter spatial distances so defined. Msr. Law will insist that somebody in a wheelchair go down the \"best path\" of the maze, even though that path involves going up and down a flight of stairs.\n\nAnd Msr. Law will be unable to mentally deal with a helicopter overflying the maze that violates their ontology relative to which \"the shortest path\" was defined.\n\nAnd it will also never occur to Msr. Law to pedal around the maze in a bicycle, which is a much easier trip even if it's not the shortest spatial distance.\n\nAnd Msr. Law will assume that the behavior of mortgage-backed securities is independently Gaussian-random because the math is neater that way, and then derive a definite theorem showing a top-level tranche of MBSs will almost never default, thus bringing down their trading firm -\n\nTo all of which I can only reply: \"Well, yes, that happens some of the time, and there are contextual occasions where it is a useful tool to lecture Msr. Law on the importance of having a diverse toolbox. But it is not a _universal_ truth that _everyone_ works like that and needs to be prescribed the same lecture! You need to be sensitive to context here!\"\n\nThere are definitely versions of Msr. Law who think the universal generalization they've been told about is a One Weird Trick That Is All You Need To Know; people who could in fact benefit from a lecture on the importance of diverse toolboxes.\n\nThere are also extreme toolbox thinkers could benefit from a lecture on the importance of thinking that considers unreachable ideals, and how to get closer to them, and the obstacles that are moving us away from them.\n\nNot to commit the [fallacy of the golden mean](https://en.wikipedia.org/wiki/Argument_to_moderation) or anything, but the two viewpoints are both metatools in the metatoolbox, as it were. You're better off if you can use both in ways that depend on context and circumstance, rather than insisting that _only_ toolbox reasoning is the _universally best context-insensitive_ metaway to think.\n\nIf that's not putting the point too sharply.\n\nThinking in terms of Law is often useful. You just have to be careful to understand the context and the caveats: when is the right time to think in Law, how to think in Law, and what type of problems call for Lawful thinking.\n\nWhich is _not_ the same as saying that every Law has exceptions. Thermodynamics still holds even at times, like playing tennis, when it's not a good time to be thinking about thermodynamics. If you thought that every Law had exceptions because it wasn't always useful to think about that Law, you'd be rejecting the metatool of Law entirely, and thinking in toolbox terms at a time when it wasn't useful to do so.\n\nAre there Laws of optimal thought governing the optimal way to contextualize and caveat, which might be helpful for finding good executable recipes? The naturally Lawful thinker will immediately suspect so, even if they don't know what those Laws are. Not knowing these Laws won't panic a healthy Lawful thinker. Instead they'll proceed to look around for useful yet chaotic-seeming prescriptions to use now instead of later - _without_ mistaking those chaotic prescriptions for Laws, _or_ treating the chaos of their current recipes as proof that there's no good normative ideals to be had.\n\nIndeed, it can sometimes be useful to contemplate, in detail, that there are probably Laws you don't know. But that's a more advanced metatool in the metatoolbox, useful in narrower ways and in fewer contexts having to do with the invention of new Laws as well as new recipes, and I'd rather not strain Msr. Toolbox's credulity any further.\n\n* * *\n\nTo close out, one recipe I'd prescribe to reduce confusion in the toolbox-inclined is to try to see the Laws as descriptive statements, rather than being any kind of normative ideal at all.\n\nThe idea that there's a shortest path through the maze isn't a \"normative ideal\" instead of a \"prescriptive ideal\", it's just true. Once you define distance there is in fact a shortest path through the maze.\n\nThe triangle inequality might sound very close to a prescriptive rule that you ought to walk along AC instead of ABC. But actually the prescriptive rule is only if you _want_ to walk shorter distances ceteris paribus, only if you know which turn is which, only if you're not trying to avoid stairs, and only if you're not taking an even faster route by getting on a bicycle and riding outside the whole maze to the exit. The prescriptive rule \"try walking along AC\" isn't the same as the triangle inequality itself, which goes on being _true_ of spatial distances in Euclidean or nearly-Euclidean geometries - whether or not you know, whether or not you care, whether or not it's useful to think about at any given moment, even if you own a bicycle.\n\nThe statement that you can't have a heat-pressure engine more efficient than a Carnot cycle isn't about gathering in a cultish circle to sing praises of the Carnot cycle as being the ideally best possible kind of engine. It's just a true fact of thermodynamics. This true fact might helpfully suggest that you think about obstacles to Carnot-ness as possible places to improve your engine - say, that you should try to prevent heat loss from the combustion chamber, since heat loss prevents an adiabatic cycle. But even at times when it's not in fact useful to think about Carnot cycles, it doesn't mean your heat engine is allowed on those occasions to perform better than a Carnot engine.\n\nYou can't extract any more evidence from an observation than is given by its likelihood ratio. You could see this as being true because Bayesian updating is an often-unreachable normative ideal of reasoning, so therefore nobody can do better than it. But I'd call it a deeper level of understanding to see it as a law saying that you can't get a higher expected score by making any different update. This is a generalization that holds over both Bayes-inspired recipes and non-Bayes-inspired recipes. If you _want_ to assign higher probability to the correct hypothesis, it's a short step from that preference to regarding Bayesian updates as a normative ideal; but the idea begins life as a descriptive assertion, not as a normative assertion.\n\nIt's a relatively shallow understanding of the [coherence theorems](https://arbital.com/p/expected_utility_formalism/?l=7hh) to say \"Well, they show that if you don't use probabilities and expected utilities you'll be incoherent, which is bad, so you shouldn't do that.\" It's a deeper understanding to state, \"If you do something that is incoherent in way X, it will correspond to a dominated strategy in fashion Y. This is a universal generalization that is true about every tool in the statistical toolbox, whether or not they are in fact coherent, whether or not you personally prefer to avoid dominated strategies, whether or not you have the computing power to do any better, even if you own a bicycle.\"\n\nI suppose that when it comes to the likes of [Fun Theory](http://lesswrong.com/lw/xy/the_fun_theory_sequence/), there isn't any deeper fact of nature underlying the \"normative ideal\" of a eudaimonic universe. But in simpler matters of math and science, a \"normative ideal\" like the Carnot cycle or Bayesian decision theory is almost always the manifestation of some simpler fact that is _so closely related to something we want_ that we are tempted to take one step to the right and view it as a \"normative ideal\". If you're allergic to normative ideals, maybe a helpful course would be to discard the view of whatever-it-is as a normative ideal and try to understand it as a fact.\n\nBut that is a more advanced state of understanding than trying to understand what is better or best. If you're not allergic to ideals, then it's okay to try to understand why Bayesian updates are often-unreachable normative ideals, before you try to understand how they're just there.",
      "plaintextDescription": "Tl;dr:\n\nI've noticed a dichotomy between \"thinking in toolboxes\" and \"thinking in laws\".\n\nThe toolbox style of thinking says it's important to have a big bag of tools that you can adapt to context and circumstance; people who think very toolboxly tend to suspect that anyone who goes talking of a single optimal way is just ignorant of the uses of the other tools.\n\nThe lawful style of thinking, done correctly, distinguishes between descriptive truths, normative ideals, and prescriptive ideals. It may talk about certain paths being optimal, even if there's no executable-in-practice algorithm that yields the optimal path. It considers truths that are not tools.\n\nWithin nearly-Euclidean mazes, the triangle inequality - that the path AC is never spatially longer than the path ABC - is always true but only sometimes useful. The triangle inequality has the prescriptive implication that if you know that one path choice will travel ABC and one path will travel AC, and if the only pragmatic path-merit you care about is going the minimum spatial distance (rather than say avoiding stairs because somebody in the party is in a wheelchair), then you should pick the route AC. But the triangle inequality goes on governing Euclidean mazes whether or not you know which path is which, and whether or not you need to avoid stairs.\n\nToolbox thinkers may be extremely suspicious of this claim of universal lawfulness if it is explained less than perfectly, because it sounds to them like \"Throw away all the other tools in your toolbox! All you need to know is Euclidean geometry, and you can always find the shortest path through any maze, which in turn is always the best path.\"\n\nIf you think that's an unrealistic depiction of a misunderstanding that would never happen in reality, keep reading.\n\n----------------------------------------\n\nHere's a recent conversation from Twitter which I'd consider a nearly perfect illustration of the toolbox-vs.-laws dichotomy:\n\n> David Chapman: \"By rationalism, ",
      "wordCount": 3488
    },
    "tags": [
      {
        "_id": "EewHHv3ewvQ3mqbyb",
        "name": "Law-Thinking",
        "slug": "law-thinking"
      },
      {
        "_id": "AHK82ypfxF45rqh9D",
        "name": "Distinctions",
        "slug": "distinctions"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xdwbX9pFEr7Pomaxv",
    "title": "Meta-Honesty: Firming Up Honesty Around Its Edge-Cases",
    "slug": "meta-honesty-firming-up-honesty-around-its-edge-cases-1",
    "url": null,
    "baseScore": 143,
    "voteCount": 92,
    "viewCount": null,
    "commentCount": 155,
    "createdAt": null,
    "postedAt": "2018-05-29T00:59:22.084Z",
    "contents": {
      "markdown": "(Cross-posted [from Facebook](https://www.facebook.com/yudkowsky/posts/10156161113669228).)\n\n0: Tl;dr.\n---------\n\n*   A problem with the obvious-seeming \"wizard's code of honesty\" aka \"never say things that are false\" is that it draws on high verbal intelligence and unusually permissive social embeddings. I.e., you can't always say \"Fine\" to \"How are you?\" This has always made me feel very uncomfortable about the privilege implicit in recommending that anyone else be more honest.\n*   Genuinely consistent Glomarization (i.e., consistently saying \"I cannot confirm or deny\" whether or not there's anything to conceal) does not work in principle because there are too many counterfactual selves who might want to conceal something.\n*   Glomarization also doesn't work in practice if the Nazis show up at your door asking if you have fugitive Jews in your attic.\n*   If you would lie to Nazis about fugitive Jews, then absolute truthsaying can't be the whole story, which makes \"never say things that are false\" feel to me like a shaky foundation in that it is literally false, and something less shaky would be nice.\n*   Robin Hanson's \"[automatic norms](http://www.overcomingbias.com/2017/12/automatic-norms.html)\" problem suggests different people might have very different ideas about what constitutes a good person's normal honesty, without realizing that they have very different ideas. Perceived violations of an honesty norm can blow up and cause interpersonal conflict. It seems to me that this is something that doesn't always work well when people leave it alone.\n\nA rule which seems to me more \"normal\" than the wizard's literal-truth rule, more like a version of standard human honesty reinforced around the edges, would be as follows:\n\n\"Don't lie when a normal highly honest person wouldn't, and furthermore, be honest when somebody asks you which hypothetical circumstances would cause you to lie or mislead—absolutely honest, if they ask under this code. However, questions about meta-honesty should be careful not to probe object-level information.\"\n\nI've been tentatively calling this \"meta-honesty\", but better terminology is solicited.\n\n1: Glomarization can't practically cover many cases.\n----------------------------------------------------\n\nSuppose that last night I helped hide a fugitive marijuana seller from the Feds. You ask me what I was doing last night, and I, preferring not to emit false statements, reply, \"I can't confirm or deny what I was doing last night.\"\n\nWe now have two major problems here:\n\n*   Even on an ordinary day, if you casually ask me what I was doing last night, I theoretically ought to answer \"I can't confirm or deny what I was doing last night\" because some of my counterfactual selves were hiding fugitive marijuana sellers from the Feds. If I don't do this consistently, and I actually was hiding fugitives last night, I can't Glomarize without revealing information. But then the number of counterfactuals I have to worry about is too large for me to ever answer anything.\n*   If the Feds actually ask you this question, they will not be familiar with your previous practice of Glomarization and will probably not be very impressed with your answer.\n\nThis doesn't mean that Glomarization is never helpful. If you ask me whether my submarine is carrying nuclear weapons, or whether I'm secretly the author of \"The Waves Arisen\", I think most listeners would understand if I replied, \"I have a consistent policy of not saying which submarines are carrying nuclear weapons, nor whether I wrote or helped write a document that doesn't have my name on it.\" An ordinary honest person does not need to lie on these occasions because Glomarization is both theoretically possible and pragmatically practical, so one should adopt a consistent Glomarization rather than lie.\n\nBut that doesn't work for hiding fugitives. Or any other occasion where an ordinary high-honesty person would consider it obligatory to lie, in answer to a question where the asker is not expecting evasion or Glomarization.\n\n(I'm sure some people reading this think it's all very cute for me to be worried about the fact that I wouldn't tell the truth all the time. Feel free to state this in the comments so that we aren't confused about who's using which norms. Smirking about it, or laughing, especially conveys important info about you.)\n\n2: The law of no literal falsehood.\n-----------------------------------\n\nOne formulation of my automatic norm for honesty, the one that feels like the obvious default from which any departure requires a crushingly heavy justification, was given by Ursula K. LeGuin in _A Wizard of Earthsea_:\n\n> He told his tale, and one man said, \"But who saw this wonder of dragons slain and dragons baffled? What if he—\"  \n>   \n> \"Be still!\" the Head Isle-Man said roughly, for he knew, as did most of them, that a wizard may have subtle ways of telling the truth, and may keep the truth to himself, but that if he says a thing the thing is as he says. For that is his mastery.\n\nOr in simpler summary, this policy says:\n\n_Don't say things that are literally false._\n\nOr with some of the unspoken finicky details added back in: \"Don't say things that you believe to be literally false in a context where people will (with reasonably high probability) persistently believe that you believe them to be true.\" Jokes are still allowed, even jokes that only get revealed as jokes ten seconds later. Or quotations, etcetera ad obviousum.\n\nThe no-literal-falsehood code of honesty has three huge advantages:\n\n*   To the extent people observe you to consistently practice it, it is easier for you to communicate believably when you _want_ to say a thing. They may still not be able to trust you perfectly, but the hypothetical is \"Did this person break their big-deal code of honesty?\" rather than \"Did this person tell an ordinary lie?\" One would hope this would be good for coordination and other interpersonal issues, though this might only be a fond wish on my part.\n*   Most people, even most unusually honest people, wander about their lives in a fog of internal distortions of reality. Repeatedly asking yourself of every sentence you say aloud to another person, \"Is this statement actually and literally true?\", helps you build a skill for navigating out of your internal smog of not-quite-truths. For that is our mastery.\n*   It's good for your soul. At least, it's good for my soul for reasons I'd expect to generalize if I'm not just committing the typical-mind fallacy.\n\nFrom Frank Hebert's _Dune Messiah_, writing about Truthsayers, people who had trained to extreme heights the ability to tell when others were lying and who also never lied themselves:\n\n> \"It requires that you have an inner agreement with truth which allows ready recognition.\"\n\nThis is probably not true in normal human practice for detecting other people's lies. I'd expect a lot of con artists are better than a lot of honest people at that.\n\nBut the phrase \"It requires you have an inner agreement with truth which allows ready recognition\" is something that resonates strongly with me. It feels like it points to the part that's good for your soul. Saying only true things is a kind of respect for the truth, a pact that you forge with it.\n\n3: The privilege of truthtelling.\n---------------------------------\n\nI've never suggested to anyone else that they adopt the wizard's code of honesty.\n\nThe code of literal truth only lets people navigate anything like ordinary social reality to the extent that they are very fast on their verbal feet, and can respond to the question \"How are you?\" by saying \"Getting along\" instead of \"Horribly\" or with an awkward silence while they try to think of something technically true. (Because often \"I'm fine\" is false, you see. If this has never bothered you then you are perhaps not in the target audience for this essay.)\n\nSo I haven't advocated any particular code of honesty before now. I was aware of the fact that I had an unusually high verbal SAT score, and also, that I spend little time interfacing with mundanes and am not dependent on them for my daily bread. I thought it wasn't my place for me to suggest to anyone else that they try their hand at saying only true things all the time, or for me to act like this conveys moral virtue. I'm only even describing the wizard's code publicly now that I can think of at least one alternative.\n\nI once heard somebody claim that rationalists ought to practice lying, so that they could separate their internal honesty from any fears of needing to _say_ what they believed. That is, if they became good at lying, they'd feel freer to consider geocentrism without worrying what the Church would think about it. I do not in fact think this would be good for the soul, or for a cooperative spirit between people. This is the sort of proposed solution of which I say, \"That is a terrible solution and there has to be a better way.\"\n\nBut I do see the problem that person was trying to solve. One can also be privileged in stubbornness when it comes to overriding the fear of other people finding out what you believe. I can see how telling fewer routine lies than usual would make that fear even worse, exacerbating the pressure it can place on what you believe you believe; especially if you didn't have a lot of confidence in your verbal agility. It's one more reason not to pressure people (even a little) into adopting the wizard's code, but then it would be nice to have some other code instead.\n\n4: Literal-truth as my automatic norm, maybe not shared.\n--------------------------------------------------------\n\nThis set of thoughts started, as so many things do, with a post by Robin Hanson.\n\nIn particular Robin [tweeted](https://twitter.com/robinhanson/status/929006858911801345) the paper: \"The surprising costs of silence: Asymmetric preferences for prosocial lies of commission and omission.\"\n\n> Abstract: Across 7 experiments (N = 3883), we demonstrate that communicators and targets make egocentric moral judgments of deception. Specifically, communicators focus more on the costs of deception to them—for example, the guilt they feel when they break a moral rule—whereas targets focus more on whether deception helps or harms them. As a result, communicators and targets make asymmetric judgments of prosocial lies of commission and omission: Communicators often believe that omitting information is more ethical than telling a prosocial lie, whereas targets often believe the opposite.\n\nThis got me wondering whether my default norm of the wizard's code is something other people will even perceive as prosocial. Yes, indeed, I feel like not saying things is much more law-abiding than telling literal falsehoods. But if people feel just as wounded, or _more_ wounded, then that policy isn't really benefiting anyone else. It's just letting me feel ethical and maybe being good for my own personal soul.\n\nRobin commented, \"Mention all relevant issues, even if you have to lie about them.\"\n\nI don't think this is a bullet I can bite in daily practice. I think I still want to emit literal truths for most dilemmas short of hiding fugitives. But it's one more argument worth mentioning against trying to make an absolute wizard's code into a bedrock solution for interpersonal reliability.\n\nRobin also published a blog post about \"[automatic norms](https://www.overcomingbias.com/2017/12/10-implications-of-automatic-norms.html)\" in general:\n\n> We are to just know easily and surely which actions violate norms, without needing to reflect on or discuss the matter. We are to presume that framing effects are unimportant, and that everyone agrees on the relevant norms and how they are to be applied.\n\n> In a relatively simple world with limited sets of actions and norms, and a small set of people who grew up together and later often enough observe and gossip about possible norm violations of others, such people might in fact learn from enough examples to mostly apply the same norms the same way. This was plausibly the case for most of our distant ancestors. They could in fact mostly be sure that, if they judged themselves as innocent, most everyone else would agree. And if they judged someone else as guilty, others should agree with that as well. Norm application could in fact usually be obvious and automatic.\n\n> Today however, there are far more people, and more intermixed, who grow up in widely varying contexts and now face far larger spaces of possible actions and action contexts. Relative to this huge space, gossip about particular norm violations is small and fragmented...\n\n> We must see ourselves as tolerating a _lot_ of norm violation. We actually tell others about and attempt to punish socially only a tiny fraction of the violations that we could know of. When we look most anywhere at behavior details, it must seem to us like we are living in a Sodom and Gomorrah of sin. Compared to the ancient world, it must seem a lot easier to get away for a long time with a lot of norm violations...\n\n> We must also see ourselves as tolerating a _lot_ of overeager busybodies applying what they see as norms to what we see as our own private business where their social norms shouldn’t apply.\n\nThis made me realize that the wizard's code of honesty I grew up with is, indeed, an automatic norm for me. Which meant I was probably overestimating and eliezeromorphizing the degree to which other people even cared at all, or would think I was keeping any promises by doing it. Again, I don't see this as a good reason to give up on emitting literally true sentences almost all of the time, but it's one more reason I feel more open to alternatives than I would've ten years ago. That said, I do expect a lot of people reading this also have something like that same automatic norm, and I still feel like that makes us more like part of the same tribe.\n\n5: Counterargument: The problem of non-absolute rules.\n------------------------------------------------------\n\nA proposal like this one ought to come with a lot of warning signs attached. Here's one of them:\n\nThere's a passage in John M. Ford's _Web of Angels_, when the protagonist has finally killed someone even after all the times his mentor taught him to never ever kill. His mentor says:\n\n> \"No words can prevent all killing. Words are not iron bands. But I taught you to hesitate, to stay your hands until the weight of duty crushed them down.\"\n\nSurprise! Really the mentor just meant to try to get him to _wait_ before killing people instead of jumping to that _right away_.\n\nHumans are kind of insane, and there are all sorts of insane institutions that have evolved among us. A fairly large number of those institutions are twisted up in such a way that something explodes if people try to talk openly about how they work.\n\nIt's a human kind of thinking to verbally insist that \"Don't kill\" is an [absolute rule](https://wiki.lesswrong.com/wiki/Ethical_injunction), why, it's right up there in the Ten Commandments. Except that what soldiers do doesn't count, at least if they're on the right side of the war. And sure, it's also okay to kill a crazy person with a gun who's in the middle of shooting up a school, because that's just not what the absolute law \"Don't kill\" _means_, you know!\n\nWhy? Because any rule that's not labeled \"absolute, no exceptions\" lacks weight in people's minds. So you have to perform that the \"Don't kill\" commandment is absolute and exceptionless (even though it totally isn't), because that's what it takes to get people to even _hesitate_. To stay their hands _at least_ until the weight of duty is crushing them down. A rule that isn't even absolute? People just disregard that whenever.\n\n(I speculate this may have to do with how the human mind reuses physical ontology for moral ontology. I speculate that brains started with an ontology for material possibility and impossibility, and reused that ontology for morality; and it internally feels like only the moral reuse of \"impossible\" is a rigid moral law, while anything short of \"moral-impossible\" is more like a guideline. Kind of like how, if something isn't [absolutely certain](https://www.readthesequences.com/The-Fallacy-Of-Gray), people think that means it's okay to make up their own opinion about it, because if it's not absolutely certain it must not be the domain of Authority. But I digress, and it's just a hypothesis. We don't need to know exactly what is the buried cause of the surface craziness to observe that the craziness is in fact there.)\n\nSo you have to perform that the Law is absolute in order to make [the actual flexible Law](https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization) exist. That doesn't mean people lie about how the Law applies to the edge cases—that's not what I mean to convey by the notion of \"performing\" a statement. More like, proclaim the Law is absolute and then _just not talk about_ anything that contradicts the absoluteness.\n\nAnd when that happens, it's one more little chunk of insanity that nobody can talk about on the meta-level without it exploding.\n\nNow, you will note that I am going ahead and writing this all down explicitly, because... well, because I expect that in the long run we have to find a way that doesn't require a little knot of madness that nobody is allowed to describe faithfully on the meta-level. So we might as well start today.\n\nI trust that you, the reader, will be able to understand that \"Don't kill\" is the kind of rule where you give it enough force-as-though-of-absoluteness that it actually takes a deontology-breaking weight of duty to crush down your hands, as opposed to you cheerfully going \"oh well I guess there's a crushing weight now! let's go!\" at the first sign of inconvenience.\n\nActually, I don't trust that everyone reading this can do that. That's not even close to literally true. But most you won't ever be called on to kill, and society frowns upon that strongly enough to discourage you anyway. So I did feel it was worth the risk to write that example explicitly.\n\n\"Don't lie\" is more dangerous to mess with. That's something that most people don't take as an exceptionless absolute to begin with, even in the sense of performing its absoluteless so that it will exist at all. Even extremely honest people will agree that you can lie to the Gestapo about whether you are hiding any Jews in the attic, and not bother to Glomarize your response either; and I think they will mostly agree that this is in fact a \"lie\" rather than trying to dance around the subject. People who are less than extremely honest think that \"I'm fine\" is an okay way to answer \"How are you?\" even if you're not fine.\n\nSo there's still a very obvious thing that could go wrong in people's heads, a very obvious way that the notion of \"meta-honesty\" could blow up, or _any other code_besides \"don't say false things\" could blow up. It's why the very first description in the opening paragraphs says \"Don't lie when a normal highly honest person wouldn't, and furthermore…\" and you should never omit that preamble if you post any discussion of this on your own blog. THIS IS NOT THE IDEA THAT IT'S OKAY TO LIE SO LONG AS YOU ARE HONEST ABOUT WHEN YOU WOULD LIE IF ANYONE ASKS. It's not an escape hatch.\n\nIf anything, meta-honesty is the idea that you should be careful enough about when you break the rule \"Don't lie\" that, if somebody else asked the hypothetical question, you would be willing to PUBLICLY DEFEND EVERY ONE OF THOSE EXTRAORDINARY EXCEPTIONS as times when even an unusually honest person should lie.\n\n(Unless you were never claiming to be unusually honest, and your pattern of meta-honest responses to hypotheticals openly shows that you lie about as much as an average person. But even here, I'd worry that anyone who lets themselves be as wicked as they imagine the 'average' person to be, would be an unusually wicked person indeed. After all, if Robin Hanson speaks true, we are constantly surrounded by people violating what seem to us like automatic norms.)\n\n6: Meta-honesty, the basics.\n----------------------------\n\nOkay, enough preamble, let's speak of the details of meta-honesty, which may or may not be a terrible idea to even talk about, we don't know at this point.\n\nThe basic formulation of meta-honesty would be:\n\n\"Be at least as honest as an unusually honest person. Furthermore, when somebody asks for it and especially when you believe they're asking for it under this code, try to convey to them a frank and accurate picture of the sort of circumstances under which you would lie. Literally never swear by your meta-honesty that you wouldn't lie about a hypothetical situation that you would in fact lie about.\"\n\nMy first horrible terminology for this was the \"Bayesian code of honesty\", on the theory that this code meant your sentences never provided Bayesian evidence in the wrong direction. Suppose you say \"Hey, Eliezer, what were you doing last night?\" and I reply \"Staying at home doing the usual things I do before going to bed, why?\" If you have a good mental picture of what I would lie about, you have now definitely learned that I was _not_ out watching a movie, because that is not something I would lie about. A very large number of possibilities have been ruled out, and most of your remaining probability mass should now be on me having stayed home last night. You know that I wasn't on a secret date with somebody who doesn't want it known we're dating, because you can ask me that hypothetical and I'll say, \"Sure, I'd happily hide that fact, but that isn't enough to force me to _lie_. I would just say 'Sorry, I can't tell you where I was last night,' instead of lying.\"\n\nYou have _not_ however gained any Bayesian evidence against my hiding a fugitive marijuana seller from the Feds, where somebody's life or freedom is at stake and it's vital to conceal that a secret even exists in the first place. Ideally we'd have common knowledge of that, and hopefully we'd agree that it was fine to lie in that case to a friend who asks a casual-seeming question.\n\nLet's be clear, although this is a kind of softening of deception, it's still deception. Even if somebody has extensively discussed your code of honesty with you, they aren't logically omniscient and won't explicitly have the possibility in mind every time. That's why we should go on holding ourselves to the standard of, \"Would I defend this lie even if the person I was defending it to had never heard of meta-honesty?\"\n\n\"Eliezer,\" you say, \"if you had a temporary schizophrenic breakdown and robbed a bank and this news hadn't become public, would you lie to keep it from becoming public?\"\n\nAnd this would cause me to stop and think and agonize for a bit (which itself tells you something about me, that my answer is not instantly No or Yes). I do have important work to do which should not be trashed without strong reason, and this hypothetical situation would not have involved a great deliberate betrayal on my part; but it is also the sort of thing that you could reasonably argue an unusually honest person ought _not_ to lie about, where lies do not in general serve the social good.\n\nI think in the end I might reply something like \"I wouldn't lie freely and would probably try to use at least technical truth or Glomarize, but in the end I might conceal that event rather than letting my work be trashed for no reason. I think I'd understand if somebody else had done likewise, if I thought they were doing good work in the first place. Except that obviously I'd need to tell various people who are engaged in positive-sum trades with me, where it's a directly important issue to them whether I can be trusted never to have mental breakdowns, and remove myself from certain positions of trust. And if it happened twice I'd be more likely to give up. If it got to the point where people were openly asking questions I don't imagine myself as trying to continue a lie. I also want to caveat that I'm describing my ethical views, what I think is right in this situation, and obviously enough pressure can make people violate their own ethics and it's not always predictable how much pressure it takes, though I generally consider myself fairly strong in that regard. But if this had actually happened I would have spent a lot more time thinking about it than the two minutes I spent writing this paragraph.\" And this would help give you an accurate picture of the sort of person that I am in general, and what I take into account in considering exceptions.\n\nInsofar as you are practicing a mental discipline in being meta-honest, the discipline is to be explicitly aware of every time you say something false, and to ask yourself, \"Would I be okay publicly saying, if somebody asked me the hypothetical, that this is a situation where a person ought to lie?\"\n\nI still worry that this is _not_ the thing that people need to do to establish their inner pact with truth. Maybe you could pick some friends to whom you just never tell any kind of literal falsehood, in the process of becoming initially aware of how many false things you were just saying all the time… but I don't actually know if that works either. Maybe that's like trying to stop smoking cigarettes on odd-numbered days. It'd be something to notice if the experimental answer is \"In reality, meta-honesty turns out not to work for practicing the respect of truth.\"\n\nMeta-honesty should be for people who are comfortable, not with absolute honesty, but with not trying to appear any more honest than they are. This itself is not the ordinary equilibrium, and if you want to do things the standard human way and not forsake a well-tested and somewhat enforced social equilibrium in pursuit of a bright-eyed novel idealistic agenda, then you should not declare yourself meta-honest, or should let somebody else try it first.\n\n7: Consistent object-level glomarization in meta-level honest responses.\n------------------------------------------------------------------------\n\nGlomarization can be workable when restricted to special cases, such as only questions about nuclear weapons and submarines. Meta-honesty is such a special case and, if we're doing this, we should all Glomarize it accordingly. In particular meta-questions are not to be used to extract object-level data, and we should all respect that in our questions, and consistently Glomarize about it in our answers, including some random times when Glomarization seems silly.\n\nSome key responses that need to be standard:\n\n*   \"That question sounds too object-level.\"\n*   \"I think you're doing meta-honesty wrong.\"\n*   \"I think I'm supposed to Glomarize that sort of answer in general.\"\n*   \"I should answer a more abstract version of that.\"\n*   \"I worry that some of my counterfactual selves are not in a mutually beneficial situation in this discussion.\"\n\nAnd if you clearly say that you \"irrevocably worry\" about any of these things, it means the meta-honest conversation has crashed; the other person is not supposed to keep pressing you, and if they do, you can lie. Ideally, this is something you should consistently do in any case where a substantial measure of your counterfactual selves as the other person might imagine them would be feeling pressured to the point of maybe meta-lying. That is, you should not only say \"irrevocably worry\" in cases where you actually have something to conceal, you should say it in cases where the discussion would be pressuring somebody who did have something to conceal and this seems high-enough-probability to you or to your model of the person talking to you.\n\nFor example: \"Eliezer, would you lie about having robbed a bank?\"\n\nI consider whether this sounds like an attempt to extract object-level information from some of my counterfactual selves, and conclude that you probably place very little probability on my having actually robbed a bank. I reply, \"Either it is the case that I did rob a bank and I think it is okay to lie about that, or alternatively, my reply is as follows: I wouldn't ordinarily rob a bank. It seems to me that you are postulating some extraordinary circumstance which has driven me to rob a bank, and you need to tell me more about this extraordinary circumstance before I tell you whether I'd lie about it. Or you're postulating a counterfactual version of me that's fallen far enough off the ethical rails that he'd probably stop being honest too.\"\n\nSome additional statements that ought to be taken as praiseworthy:\n\n*   \"I only feel free to have a frank discussion about that if everyone in the room has agreed to abide by the meta-honesty code.\"\n*   \"I notice that I'm feeling interrogated, and should not try to give a code-abiding answer to that right now.\"\n*   \"It is either the case that this actually happened and I think it is okay to lie about it, or that my current quick guess is that I wouldn't lie in that case.\"\n*   \"Hold on, let me either generate a random number or pretend to generate a random number, such that if I'm actually generating a random number and it comes up as 0, I will try to seem more evasive than usual in this conversation even if I have nothing to actually hide.\"\n\nThis is not _supposed_ to be a clever way to extract information from people and you should shut down any attempt to use it that way.\n\n\"Harry,\" says HPMOR!Dumbledore, \"I ask you under the code of meta-honesty (which we have just anachronistically acquired): Would you lie about having robbed the Gringotts Bank?\"\n\nHarry thinks, _Maybe this is about the Azkaban breakout_, and says, \"Do you in fact suspect me of having robbed a bank?\"\n\n\"I think that if I suspected you of having robbed a bank,\" says Dumbledore, \"and I did not wish you to know that, I would not ask you if you had robbed a bank. Why do you ask?\"\n\n\"Because the circumstances under which you're invoking meta-honesty have something to do with how I answer,\" says Harry (who has suddenly acquired a view on this subject that some might consider implausibly detailed). \"In particular, I think I react differently depending on whether this is basically about you trying to construct a new mutually beneficial arrangement with the person you think I am, or if you're in an adversarial situation with respect to some of my counterfactual selves (where the term 'counterfactual' is standardly taken to include the actual world as one that is counterfactually conditioned on being like itself). Also I think it might be a good idea generally that the first time you try to have an important meta-honest conversation with someone, you first spend some time having a meta-meta-honest conversation to make sure you're on the same page about meta-honesty.\"\n\n\"I am not sure I understood all that,\" said Dumbledore. \"Do you mean that if you think we have become enemies, you might meta-lie to me about when you would lie?\"\n\nHarry shook his head. \"No,\" said Harry, \"because then if we weren't enemies, you would still never really be able to trust what I say even assuming me to abide by my code of honesty. You would have to worry that maybe I secretly thought you were an enemy and didn't tell you. But the fact that I'm meta-honest shouldn't be something that you can use against me to figure out whether I… sneaked into the girl's dorm and wrote in somebody's diary, say. So if I'm in that situation I've got to protect my counterfactual selves and Glomarize harder. Whereas if this is more of a situation where you want to know if we can go to Mordor together, then I'd feel more open and try to give you a fuller picture of me with more detail and not worry as much about Glomarizing the specific questions you ask.\"\n\n\"I suspect,\" Dumbledore said gravely, \"that those who try to be honest at all will always be at something of a disadvantage relative to the most ready liars, at least if they've robbed Gringotts. But yes, Harry, I am afraid that this is more of a situation where I am… concerned… about some of your counterfactual selves. But then why would you answer at all, in such a case?\"\n\n\"Because sometimes people are honest and have good intentions,\" answered Harry, \"and I think that if in general they can have an accurate picture of the other person's honesty, everybody is on net a bit better off. Even if I _had_ robbed a bank, for example, you and I would both still not want anything bad to happen to Britain. And some of my counterfactual selves are innocent, and they're not better off if you think I'm more dishonest than I am.\"\n\n\"Then I ask again,\" said Dumbledore, \"under the code of meta-honesty, whether you would lie about having robbed a bank.\"\n\n\"Then my answer is that I wouldn't ordinarily rob a bank,\" Harry said, \"and I'd feel even worse about lying about having robbed a bank, than having robbed a bank. And I'd know that if I robbed a bank I'd also have to lie about it. So whatever weird reason made me rob the bank, it'd have to be weird enough that I was willing to rob the bank _and_ willing to lie about it, which would take a pretty extreme situation. Where it should be clear that I'm not trying to answer about having specifically robbed a bank, I'm trying to give you a general picture of what sort of person I am.\"\n\n\"What if you had been blackmailed into robbing the bank?\" inquired Dumbledore. \"Or what if things crept up on you bit by bit, so that in the end you found yourself in an absurd situation you'd never intended to enter?\"\n\nHarry shrugged helplessly. \"Either it's the case that I did end up in a weird situation and I don't want to let you know about that, or alternatively, I feel like you're describing a very broad range of possibilities that I'd have to think about more, because I haven't yet ended up in that kind of situation and I'm not quite sure how I'd behave… I think I'd have in mind that just telling the Headmaster the truth can prevent big problems from blowing up any further, but there'd be cases extreme enough that I wouldn't do that either… I mean, the basic answer is, yes, there's things that would make me lie right to your face, but, I wouldn't do that just for having stolen candy from the kitchen, I don't think. I'd just be like 'I consistently Glomarize when people ask me if I've stolen candy from the kitchen.'\"\n\n\"Would you lie to protect a fugitive from the law who you believed was being unjustly pursued?\" said Dumbledore. \"Say that Sirius Black was innocent after all, as the Quibbler would have it; would you hide him and then lie to Minerva about what you were doing on the night you hid him?\"\n\n\"Yes,\" Harry said unhesitatingly. \"That's something I think is clearly right and the sort of thing I would ordinarily do.\"\n\n\"What if you thought that somebody was being unjustly pursued by the law?\" said Dumbledore. \"Is that worth lying for?\"\n\nHarry frowned. \"I think I maybe shouldn't answer questions about what in particular I think ought not to be illegal,\" Harry said, \"but if the basic question is whether I'd lie to my friends to keep somebody who'd only used marijuana out of Azkaban, then I would. I don't think my friends should expect otherwise; they should want me to do that.\"\n\n\"We are not being meta-honest any more, only honest,\" said Dumbledore. \"Were you involved in the breakout from Azkaban?\"\n\nHarry stared at Dumbledore a moment, and said, \"I think I have to consistently not confirm or deny that, but my counterfactual and possibly actual self who wasn't involved would really like to know at this point what the _heck_ you think might've happened.\"\n\nIf somehow the Gestapo has heard of meta-honesty and they ask you if you would lie to hide Jews in your attic, they _should_ be expecting a law-abiding meta-honest Nazi citizen to Glomarize, abstract the question in such a way as to not reveal the object-level information that might be sought, or say \"I irrevocably worry that some of my counterfactual selves are not in a positive-sum situation.\" That, I now proclaim, ought to be done by Nazis and resistance fighters alike, when you are faced with a question that _might_ be meant to reveal object-level information about what happened.\n\n\"Eliezer,\" says the hypothetical Gestapo officer who has somehow heard about my meta-honesty code, \"it happens that I'm a person who's heard of meta-honesty. Now, are you the sort of person who would lie about having Jews hidden in your attic?\"\n\nThis hypothetical Gestapo officer has a gun. Most people asking you meta-honest questions won't have a gun. In fact I bet this will literally never happen until the end of the world. Let's suppose he has a gun anyway.\n\n\"I am the following sort of person,\" I reply. \"If I was hiding the Führer in my attic to protect him from Jewish assassins, I'd lie about that to the assassins. It's clear you know about my code of meta-honesty, so you should understand that is a very innocent thing to say. But these circumstances and the exact counterfactual you are asking make me nervous, so I'm afraid to utter the words I think you may be looking for, namely the admission that if I were the kind of person who'd hide Jews in his attic then I'd be the kind of person who would lie to protect them. Can I say that I believe that in respect to your question as you mean it, I think that is no more and no less true of me than it is true of you?\"\n\n\"My, you are fast on your verbal feet,\" says the Gestapo officer. \"If somebody were less fast on their verbal feet, would you tell them that it was acceptable for a meta-honest person to just meta-lie to the Jewish assassins in order to hide the Führer?\"\n\n\"If they didn't feel that their counterfactual loyal Nazi self would think that their counterfactual disloyal self was being pressured and clearly state that fact irrevocably,\" I say, \"I'd say that, just like their counterfactual loyal self, they should make some effort to reveal the general limits of their honesty without betraying any of their counterfactual selves, but say they irrevocably couldn't handle the conversation as soon as they thought their alternate loyal self would think their alternate's counterfactual disloyal self couldn't handle the conversation. It's not as if the Jewish assassins would be fooled if they said otherwise. If the Jewish assassins do continue past that point, which is blatantly forbidden and everyone should know that, they may lie.\"\n\n\"I see,\" says the Gestapo officer. \"If you are telling me the truth, I think I have grasped the extent of what you claim to be honest about.\" He turns to his subordinates. \"Go search his attic.\"\n\n\"Now I'm curious,\" I say. \"What would you have done if I'd sworn to you that I was an absolutely loyal German citizen, and that my character was such that I would certainly never lie about having Jews in my attic even if I were the sort of disloyal citizen who had Jews in his attic in the first place?\"\n\n\"I would have detailed twice as many men to search your house,\" says the Gestapo officer, \"and had you detained, for that is not the response I would expect from an honest Nazi who knew how meta-honesty was supposed to work. Now I ask you meta-meta-honestly, why haven't you said that you are irrevocably worried that I am abusing the code? Obviously I put substantial probability on you being a traitor, meaning I am deliberately pressuring you into a meta-conversation and trying to use your code of honesty against those counterfactual selves. Why didn't you just shut me down?\"\n\n\"Because you do have a gun, sir,\" I say. \"I agree that it's what the rules called for me to say, but I thought over the situation and decided that I was comfortable with saying that in general this was a sort of situation where that rule could be bent so as for me to not end up being shot—and I tell you meta-meta-honestly that I _do_ believe the situation has to be that extreme in order for that rule to even be bent.\"\n\nReally the principle is that it is not okay to meta-ask what the Gestapo officer is meta-asking here. This kind of detailed-edge-case-checking conversation might be appropriate for shoring up the edges of an interaction intended to be mutually beneficial, but absolutely not for storming in looking for Jews in the attic of a person who in your mind has a lot of measure on having something to hide.\n\nBut I do want to have trustworthy foundations somewhere.\n\nAnd I think it's reasonable to expect that over the course of a human lifetime you will _literally never_ end up in a situation where a Gestapo officer who has read this essay is pointing a gun at you and asking overly-object-level-probing meta-honesty questions, and will shoot you if you try to glomarize but will believe you if you lie outright, given that we _all know_ that everyone, innocent or guilty, is supposed to glomarize in situations like that. Up until today I don't think I've ever seen any questions like this being asked in real life at all, even hanging out with a number of people who are heavily into recursion.\n\nSo if one is declaring the meta-honesty code at all, then one shouldn't meta-lie, period; I think the rules have been set up to allow that to be absolute. I don't want you to have to worry that maybe I think I'm being pressured, or maybe I thought you meta-asked the wrong thing, so now I think it's okay to meta-lie even though I haven't given any outward sign of that. To that end, I am willing to sacrifice the very tiny fraction of the measure of my future selves who will end up facing an extremely weird Gestapo officer. To me, for now, there doesn't seem to be _any_ real-life circumstance where you should lie in response to a meta-honesty question—rather than consistently glomarize that kind of question, consistently abstract that kind of question, consistently answer in an analogy rather than the original question, or consistently say \"I believe some counterfactual versions of me would say that cuts too close to the object level.\" (It being a standard convention that counterfactuals may include the actual.)\n\nI also think we can reasonably expect that from now until the end of the world, honest people should literally absolutely never need to _evade or mislead at all_ on the meta-meta-level, like if somebody asks if you feel like the meta-level conversation has abided by the rules. (And just like meta-honesty doesn't excuse object-level dishonesty, by saying that meta-meta-honesty seems like it could be everywhere open and total, I don't mean to excuse meta-level lies. We should all still regard meta-lies as extremely bad and a Code Violation and You Cannot Be Trusted Anymore.)\n\nIf there's a meta-honest discussion about someone's code of honesty, _and_ a discussion of what they think about the current meta-meta conditions of how the meta-honesty code is being used, and it sounds to you like they think things are fine… then things should be fine, period. If you ask, do they think that any pressure strong enough to potentially shake their meta-honesty is potentially around, do they think that the overall situation here would have treated any of their plausible counterfactual selves in a negative-sum way, and they say no it's all fine—then that is supposed to be absolute under the code. That ought to establish a foundation that's as reliable as the person's claim to be meta-honest at all.\n\nIf you go through all that and lie and meta-lie and meta-meta-lie after saying you wouldn't, you've lied under some of the kindest environments that were ever set up on this Earth to let people not lie, among people who were trying to build trust in that code so we could all use it together. You are being a genuinely awful person as I'd judge that, and I may advocate for severe social sanctions to apply.\n\nAssuming this ends up being a thing, that is. I haven't run it past many people yet and this is the first public discussion. Maybe there's some giant hole in it I haven't spotted.\n\nIf anybody ever runs into an actual real circumstance where it seems to them that meta-honesty as they tried to use it was giving the essay-reading Gestapo too much power or too much information, maybe because they weren't fast enough on their verbal feet, please email me about it so I can consider whether to modify or backtrack on this whole idea. I will try to protect your anonymity under all circumstances up to and including the end of the world unless you say otherwise. The previous sentence is not the sort of thing I would lie about.\n\n8: Counterargument: Maybe meta-honesty is too subtle.\n-----------------------------------------------------\n\nI worry that the notion of meta-honesty is too complicated and subtle. In that it has subtleties in it, at all.\n\nThis concept is certainly too subtle for Twitter. Maybe it's too subtle for us too.\n\nMaybe \"meta-honesty\" is just too complicated a concept to be able to make it be part of a culture's Law, compared to the standard-twistiness-compliant performance of saying \"Always be honest!\" and waiting for the weight of duty to crush down people's hands, or saying \"Never say anything false!\" and just-not-discussing all the exceptions that people think obviously don't count.\n\n(But of course that system also has disadvantages, like people having different automatic norms about what they think are obvious exceptions.)\n\nI've started to worry more, recently, about which cognitive skills have other cognitive skills as prerequisites. One of the reasons I hesitated to publish _[Inadequate Equilibria](https://www.lesswrong.com/sequences/oLGCcbnvabyibnG9d)_ (before certain persons _yanked it out of my drafts folder and published it anyway_) was that I worried that maybe the book's ideas were useless or harmful without mastery of other skills. Like, maybe you need to have developed a skill for demotivating cognition, and until then you can't reason about charged political issues or your startup idea well enough for complicated thoughts about Nash equilibria to do more good than harm. Or maybe unless you already know a bunch of microeconomics, you just stare at society and see a diffuse mass of phenomena that might or might not be bad equilibria, and you can't even guess non-wildly in a way that lets you get started on learning.\n\nMaybe meta-honesty contains enough meta, in that it has meta at all, that it just blows up in most people's heads. Sure, people in our little subcommunity tend to max out the Cognitive Reflection Test and everything that correlates with it. But compared to scoring 3 out of 3 on the CRT, the concept of meta-honesty is probably harder to live in real life—stopping and asking yourself \"Would I be willing to publicly defend this as a situation in which unusually honest people should lie, if somebody posed it as a hypothetical?\" Maybe that just gets turned into \"It's permissible to lie so long as you'd be honest about whether you'd tell that lie if anyone asks you that exact question and remembers to say they're invoking the meta-honesty code,\" because people can't process the meta-part correctly. Or maybe there's some subtle nonobvious skill that a few people have practiced extensively and can do very easily, and that most people haven't practiced extensively and can't do that easily, and this subskill is required to think about meta-honesty without blowing up. Or maybe I just get an email saying \"I tried to be meta-honest and it didn't work because my verbal SAT score was not high enough, you need to retract this.\"\n\nIf so, I'm not sure there's much that could be done about it, besides me declaring that Meta-Honesty had turned out to be a terrible idea as a social innovation and nobody should try that anymore. And then that might not undo the damage to the law-as-absolute performance that makes something be part of the Law.\n\nBut I'd outright lie to the Gestapo about Jews in my attic. And even to friends, I can't consistently Glomarize about every point in my life where one of my counterfactual selves could possibly have been doing that. So I can't actually promise to be a wizard, and I want there to exist firm foundations somewhere.\n\nQuestions? Comments?",
      "plaintextDescription": "(Cross-posted from Facebook.)\n\n\n0: Tl;dr.\n * A problem with the obvious-seeming \"wizard's code of honesty\" aka \"never say things that are false\" is that it draws on high verbal intelligence and unusually permissive social embeddings. I.e., you can't always say \"Fine\" to \"How are you?\" This has always made me feel very uncomfortable about the privilege implicit in recommending that anyone else be more honest.\n * Genuinely consistent Glomarization (i.e., consistently saying \"I cannot confirm or deny\" whether or not there's anything to conceal) does not work in principle because there are too many counterfactual selves who might want to conceal something.\n * Glomarization also doesn't work in practice if the Nazis show up at your door asking if you have fugitive Jews in your attic.\n * If you would lie to Nazis about fugitive Jews, then absolute truthsaying can't be the whole story, which makes \"never say things that are false\" feel to me like a shaky foundation in that it is literally false, and something less shaky would be nice.\n * Robin Hanson's \"automatic norms\" problem suggests different people might have very different ideas about what constitutes a good person's normal honesty, without realizing that they have very different ideas. Perceived violations of an honesty norm can blow up and cause interpersonal conflict. It seems to me that this is something that doesn't always work well when people leave it alone.\n\nA rule which seems to me more \"normal\" than the wizard's literal-truth rule, more like a version of standard human honesty reinforced around the edges, would be as follows:\n\n\"Don't lie when a normal highly honest person wouldn't, and furthermore, be honest when somebody asks you which hypothetical circumstances would cause you to lie or mislead—absolutely honest, if they ask under this code. However, questions about meta-honesty should be careful not to probe object-level information.\"\n\nI've been tentatively calling this \"meta-honesty\", but better termino",
      "wordCount": 8214
    },
    "tags": [
      {
        "_id": "cRaweRcZcXnb9Qryt",
        "name": "Meta-Honesty",
        "slug": "meta-honesty"
      },
      {
        "_id": "nANxo5C4sPG9HQHzr",
        "name": "Honesty",
        "slug": "honesty"
      },
      {
        "_id": "KqDvFFzqX9vqooji9",
        "name": "Consistent Glomarization",
        "slug": "consistent-glomarization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]