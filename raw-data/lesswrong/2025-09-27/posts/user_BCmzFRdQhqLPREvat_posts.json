[
  {
    "_id": "f3zeukxj3Kf5byzHi",
    "title": "Underdog bias rules everything around me",
    "slug": "underdog-bias-rules-everything-around-me",
    "url": "https://www.mindthefuture.info/p/underdog-bias-rules-everything-around",
    "baseScore": 159,
    "voteCount": 106,
    "viewCount": null,
    "commentCount": 53,
    "createdAt": null,
    "postedAt": "2025-08-17T19:21:50.346Z",
    "contents": {
      "markdown": "**People very often underrate how much power they (and their allies) have, and overrate how much power their enemies have. I call this “underdog bias”, and I think it’s the most important cognitive bias to understand in order to make sense of modern society.**\n\nI’ll start by describing a closely-related phenomenon. The [hostile media effect](https://en.wikipedia.org/wiki/Hostile_media_effect) is a well-known bias whereby people tend to perceive news they read or watch as skewed against their side. For example, pro-Palestinian students shown a video clip tended to judge that the clip would make viewers more pro-Israel, while pro-Israel students shown the same clip thought it’d make viewers more pro-Palestine. Similarly, sports fans often see referees as being biased against their own team.\n\nThe hostile media effect is particularly striking because it arises in settings where there’s relatively little scope for bias. People watching media clips and sports are all seeing exactly the same videos. And sports in particular are played on very even terms, where fairness just means enforcing the rules impartially.\n\nBut most possible conflicts are much less symmetric, both in terms of what information each side has, and even in terms of what game each side is playing. Consider, for instance, an argument about whether big corporations have too much power. The proponent might point to corporations’ wealth, employee talent, and lobbying ability; their opponent might point to how many regulations they have to follow, how much corporations compete between themselves, and how strong anti-corporate public sentiment is. In order to evaluate a question like this, people need to decide both *how to draw coalition boundaries* (to what extent should big corporations be counted as a single unified group?) and *how to weigh different types of power against each other.*\n\nI think that biases in how these weighings and boundaries are evaluated are a much bigger deal than biases in evaluating fairness in isolated contexts. Specifically, I think that people typically underrate the types of power they have, and overrate the types of power their opponents have. You’re intimately familiar with the limitations of your own abilities—you run into them regularly, often in deeply frustrating ways. You track all the fractures inside your own coalition, and they often seem fundamental and intractable. Conversely, it’s easy to forget about the things which are much easier for you than for your opponents, and to view their internal rivalries as temporary and easily-resolved.\n\nThese effects are exacerbated by information asymmetries, aka the “fog of war”. You know who’s working with you; you don’t know who’s working against you. When outside observers sympathize with your side, you know that they’re not actually contributing very much to your cause; when outside observers sympathize with your opponents, you don’t know if that’s a sign of enmity. Similarly, you know how your own plans are progressing, but you don’t know what your opponents are [scheming](https://en.wikipedia.org/wiki/Emotive_conjugation). To see how strong this effect can be, just look at fiction, where villains often implement arbitrarily-complicated schemes offscreen without breaking suspension of disbelief.\n\nIn addition to the hostile media effect, underdog bias is related to a number of other biases (like [hostile attribution bias](https://en.wikipedia.org/wiki/Hostile_attribution_bias), [siege mentality](https://en.wikipedia.org/wiki/Siege_mentality), [the fundamental attribution error](https://en.wikipedia.org/wiki/Fundamental_attribution_error), and simple [tribalism](https://www.edge.org/conversation/john_tooby-coalitional-instincts)). But hopefully the description above conveys why I think it’s fundamental enough to be worth separating out.\n\n### **Underdog bias in practice**\n\nIn this section I give six examples of conflicts where each side thinks (with some justification) of themselves as the underdog, and rejects the idea of their opponents being the underdogs. I won’t try to defend them in detail, but they hopefully convey the pervasiveness of underdog bias.\n\n1.  **Government vs industry**\n    1.  Working within a government is often a miserable experience. Government institutions are typically cash-strapped and unable to hire the most talented people; meanwhile they face heavy opposition from industry, which can spend enormous amounts of money lobbying, donating to candidates, etc. Many employees at regulatory agencies see themselves as underdogs trying to rein in whole industries, while working under strong political and bureaucratic constraints.\n    2.  Big corporations have a lot of money, but are in a constant state of cut-throat competition, and also need to comply with a huge number of regulations. Bureaucrats can very easily make their lives harder in ways ranging from trivial (e.g. adding red tape) to existential (e.g. blocking acquisitions). So it’s easy for companies to feel like underdogs pitted against the power of the state—especially when it seems like exercises of that power might be influenced by their competitors.\n2.  **Tech vs media**\n    1.  Many journalists see themselves as underdogs fighting the accumulation of power by special interests, of which tech is the most prominent. This feeling is exacerbated by how financially precarious the industry is. The rise of social media has [decimated newspaper jobs](https://www.pewresearch.org/short-reads/2021/07/13/u-s-newsroom-employment-has-fallen-26-since-2008/), leading newspapers to optimize far harder for engagement than they used to.\n    2.  The tech industry has huge amounts of money, and the ability to reshape the world in many ways, but has historically wielded relatively little cultural and political influence. Many techies see themselves as underdogs fighting the cultural establishment, as exemplified by a pervasive (and often extreme) anti-tech bias in the news media. Techies (until recently) have felt like nobody represents their interests in DC, where both Democrats and Republicans have historically been anti-innovation.\n3.  **Elites vs masses**\n    1.  Normal people view themselves as underdogs in a world where cultural elites (e.g. top university graduates) control almost all major institutions. Even when they vote for outsiders, the effects of those elections are blunted by elite control over institutions (especially the “deep state”). Elite ideology has increasingly diverged from the ideology of the masses, making many people feel helpless to elect anyone who will actually represent their interests.\n    2.  Cultural elites control almost all major institutions, but are few in number. Elites feel scared of populist masses who distrust them, and who can vote for anti-elite candidates. The most elite groups (like billionaires or Jews) are often the ones it’s most socially acceptable to blame for problems, or even call for violence against.\n4.  **Republicans vs Democrats**\n    1.  Many Republicans see themselves as underdogs fighting against the Democratic elites who have far more influence within universities, government bureaucracies, legacy media outlets, and tech companies. They see themselves as fighting against pervasive ideological control over a wide range of institutions.\n    2.  Many Democrats view Republicans as having structural advantages—such as the Electoral College, or the Senate—which allow them to remain competitive even with fewer supporters. Democrats therefore view themselves as underdogs fighting for change against existing cultural norms and Republican-run power structures (like the Supreme Court).\n5.  **America vs China**\n    1.  America has more power in the sense of established hegemony. China has more power in the sense of momentum and, increasingly, manufacturing and economic superiority. The likelihood of conflict between a rising power (who’s worried about overcoming their historical disadvantage) and an established power (who’s worried about their opponents’ growth trajectory) is known as [the Thucydides trap](https://en.wikipedia.org/wiki/Thucydides_Trap).\n6.  **Israel vs Palestine**\n    1.  Many Palestinians see themselves as underdogs against an opponent with far more military power than them, which is also strongly backed by the US.\n    2.  While Israel has power over Palestine specifically, it’s also surrounded by hostile neighbors far larger than it, which have often stated their intention (and repeatedly actually tried) to wipe Israel off the map. Many Israelis see themselves as underdogs who have to defend themselves against an entire region (as well as increasingly-hostile public opinion in the west).\n\nThe two maps below are sometimes shown by supporters of each side as a way of conveying how much of an underdog their side is. (To be clear, I’m not endorsing either of them, just illustrating the rhetorical strategies being used.)\n\n![](https://substackcdn.com/image/fetch/$s_!RrAA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb93da93-2812-4b44-add7-d4233313f710_580x386.png)\n\n![](https://substackcdn.com/image/fetch/$s_!0mbr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ff0967-9d59-4ca9-ab45-6ffb08187ac3_1208x676.png)\n\nUnderdog bias doesn’t imply that any of these groups are *wrong* to be scared of their opponents’ power. But it does suggest that they’ll tend to underestimate their own power and, crucially, underestimate how scared their opponents are of them. (The applications of this principle to the politics of AI are left as an exercise for the reader.)\n\n### **Why underdog bias?**\n\nIf underdog bias makes us so wrong about the world, why is it such a strong psychological effect? Some cognitive biases are just clear-cut mistakes, but we should expect that the strongest “biases” were evolutionarily adaptive in some way.\n\nThe descriptions I gave above suggest that there are qualitative differences in the types of reasoning about ourselves and our enemies—roughly corresponding to [near vs far mode](https://www.overcomingbias.com/p/abstractdistanthtml). In near mode we focus on concrete, nuanced details about our local situation. In far mode, we construct larger-scale narratives, in more black-and-white terms, often for the sake of signaling to others.\n\nWhy might signaling that you’re the underdog be more important than having accurate beliefs? One possibility is to gain allies. [Vandallo et al.](http://ts-si.org/files/AppealOfTheUnderdogPSPB1603.pdf) have [a few studies](http://spr.sagepub.com/content/early/2013/03/03/0265407513477629.abstract) on the effects of appearing to be the disadvantaged side. [Scott Alexander](https://slatestarcodex.com/2013/05/18/against-bravery-debates/) summarizes their conclusions as follows: “if you get yourself perceived as the brave long-suffering underdog, people will support your cause and, as an added bonus, want to have sex with you”. And [in this post](https://www.lesswrong.com/posts/A4MK9RQqSAJZjanQD/why-support-the-underdog) he points to the longstanding prevalence of underdogs in narratives (stretching back to myths like that of David and Goliath).\n\nBut he also recognizes that there’s a big difference between reported and actual support. All else equal, underdogs are pluckier and their victories more impressive—so it makes sense that we support them on a narrative level, when there’s no cost to doing so. But in real life supporting the underdog means that you’re on the side most likely to lose. Whether or not underdog bias is beneficial for gaining allies will therefore depend on whether those allies are more concerned about being (or looking) virtuous, or more concerned about actually winning. And while the modern era is dominated by virtue signaling, that was much less true in the ancestral environment, where resources were much scarcer. In that setting you’d instead expect people to have “overdog bias” which makes them overestimate their own side’s strength (which is one way that tribalism, patriotism, etc. can be interpreted).\n\nAnother possibility is that underdog bias is most valuable as a way of firing up your own supporters. I see this in action whenever I accidentally give my email to a political candidate, and get bombarded with emails about how I need to donate because the other side is on the verge of an overwhelming victory. But again, there’s a missing link: why should fear make you fight harder? On a rational agent model, being the underdog could make you decide fighting isn’t worth it—or even make you defect to the enemy. And on an emotional level, [being scared makes it much harder](https://malcolmocean.com/2022/02/towardsness-awayness-motivation-arent-symmetrical/) to think clearly or navigate complicated situations.\n\nSo my best guess is that underdog bias was useful because ancestral conflicts were *simple* and *compulsory*. In other words, our political intuitions are calibrated for a world where alliances are more tribal—where we don’t have freedom of movement or freedom of association. People used to be stuck with their family/tribe/ethnic group whether they liked it or not; if they tried to ally with another, they were often rejected, or at best permanently viewed as an untrustworthy outsider. So the only rational response to being in a worse position would be to fight harder, using fairly straightforward and intuitive strategies.\n\nIn one way, this response is maladaptive in the modern world—where fewer battle lines are based on immutable characteristics or irreconcilable differences, and the best way to approach conflict is less intuitive. Yet as I mentioned above, the modern world is also much more sympathetic to victims. This suggests that underdog bias may have gradually transitioned from a way of firing up one’s supporters, to something closer to a [victim complex](https://en.wikipedia.org/wiki/Victim_mentality) aimed at evoking sympathy from onlookers.\n\nThis whole section has been very speculative, and I’m still not confident in my answer to where underdog bias comes from. But we don’t need an explanation of underdog bias to believe that it corrupts many people’s thinking about complex issues. How can you actually reduce your underdog bias, though? The best approach I’ve found is simple in theory (though devilishly difficult in practice). Say to yourself: “they're just as scared of us as we are of them.” It’s true far more often than you think.",
      "plaintextDescription": "People very often underrate how much power they (and their allies) have, and overrate how much power their enemies have. I call this “underdog bias”, and I think it’s the most important cognitive bias to understand in order to make sense of modern society.\n\nI’ll start by describing a closely-related phenomenon. The hostile media effect is a well-known bias whereby people tend to perceive news they read or watch as skewed against their side. For example, pro-Palestinian students shown a video clip tended to judge that the clip would make viewers more pro-Israel, while pro-Israel students shown the same clip thought it’d make viewers more pro-Palestine. Similarly, sports fans often see referees as being biased against their own team.\n\nThe hostile media effect is particularly striking because it arises in settings where there’s relatively little scope for bias. People watching media clips and sports are all seeing exactly the same videos. And sports in particular are played on very even terms, where fairness just means enforcing the rules impartially.\n\nBut most possible conflicts are much less symmetric, both in terms of what information each side has, and even in terms of what game each side is playing. Consider, for instance, an argument about whether big corporations have too much power. The proponent might point to corporations’ wealth, employee talent, and lobbying ability; their opponent might point to how many regulations they have to follow, how much corporations compete between themselves, and how strong anti-corporate public sentiment is. In order to evaluate a question like this, people need to decide both how to draw coalition boundaries (to what extent should big corporations be counted as a single unified group?) and how to weigh different types of power against each other.\n\nI think that biases in how these weighings and boundaries are evaluated are a much bigger deal than biases in evaluating fairness in isolated contexts. Specifically, I think that peop",
      "wordCount": 2066
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "7oBeXzryvmoPNos8W",
    "title": "On Pessimization",
    "slug": "on-pessimization",
    "url": "https://www.mindthefuture.info/p/on-pessimization",
    "baseScore": 61,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-17T01:10:10.914Z",
    "contents": {
      "markdown": "*“Your worst sin is that you have destroyed and betrayed yourself for nothing.” - Dostoevsky*\n\nWhen people set an ambitious goal, they can fail simply by not changing the world very much. But there’s another surprisingly common way to fail: by achieving the *opposite* of their goal. I call this effect *pessimization:* the opposite of optimization.\n\nThough pessimization is an uncommon term, it’s not an uncommon concept. We allude to it whenever we call someone their own worst enemy, or predict that their actions will backfire. We try to take advantage of it with [reverse psychology](https://en.wikipedia.org/wiki/Reverse_psychology). It’s the subject of Robert Conquest’s [third law of politics](https://en.wikipedia.org/wiki/Robert_Conquest#Laws_of_politics): “the simplest way to explain the behavior of any bureaucratic organization is to assume that it is controlled by a cabal of its enemies.” And the [POSIWID](https://en.wikipedia.org/wiki/The_purpose_of_a_system_is_what_it_does) aphorism (“the purpose of a system is what it does”) is often used to describe ways in which a system actively opposes its nominal purpose. I’ve also previously described the pessimization of ideological advocacy under the heading of “[the activist’s curse](https://x.com/RichardMCNgo/status/1810495607590543701)”.\n\nI divide pessimization into three types, each of which I’ll discuss in detail. The most intuitive is what I call *direct pessimization*: when an enemy chooses what to do based on what will harm your interests the most. This is the sense in which “pessimize” is used by e.g. [Yudkowsky](https://www.lesswrong.com/posts/yaJsCQokiyeLFHhgy/incorporating-justice-theory-into-decision-theory) and [Soares](https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1). Drivers of direct pessimization include sadism, revenge and threats.\n\nConversely, *indirect pessimization* arises when your actions help other people achieve the opposite of your goals, even though they’re not deliberately trying to hurt you. And *perverse pessimization* occurs when an agent or coalition nominally dedicated to a goal is actively trying to achieve the opposite of that goal—i.e. it’s directly pessimizing itself.\n\nI’d like to improve our understanding of pessimization so that we can better create, identify, and promote remedies to it. In the rest of this post I characterize each of the three types of pessimization I mentioned above. I treat them as [scale-free phenomena](https://www.mindthefuture.info/p/towards-a-scale-free-theory-of-intelligent), describing examples within individuals, organizations, countries, and even whole civilizations. \n\n### Direct pessimization\n\n*Direct pessimization* is when one agent or faction is specifically trying to hurt another—not as a side effect of other actions, but rather as a means of achieving their desired outcome. For example, imprisoning criminals in order to prevent them from committing further crimes wouldn’t qualify. However, punishing criminals for the sake of deterrence or retribution would: in the first case the prisoners’ suffering is the means by which others are deterred, in the second case their suffering itself is the desired outcome. While less of a focus today, these motivations were more salient in historical penal systems, which used a wide range of physical punishments ranging from flogging all the way to cruel and unusual tortures. By “hurting” I don’t just mean causing physical pain, though, but also other ways of harming someone’s interests—like the social humiliation of [stocks](https://en.wikipedia.org/wiki/Stocks), exile, execution, [attainder](https://en.wikipedia.org/wiki/Attainder), or the Chinese [nine familial exterminations](https://en.wikipedia.org/wiki/Nine_familial_exterminations).\n\nTo be clear, the distinction between a “means” and a “side effect” of achieving a goal is a thorny one, [which many philosophers have debated](https://plato.stanford.edu/entries/double-effect/). For example, [which harms inflicted on an enemy nation](https://plato.stanford.edu/entries/double-effect/#RoleConvNormWarf) are necessary components of winning a war, and which are merely unfortunate side effects of winning the war? I won’t try to resolve the edge cases here—however, clear examples of larger-scale direct pessimization include terrorist attacks and genocides, as well as terror bombing civilians during wars. At even larger scales, [s-risk threat models](https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/#IIII_Unawareness_of_possible_sources_of_astronomical_suffering) outline how and why superintelligences might threaten to pessimize each others’ utility functions.\n\nAnother driver of direct pessimization is sadism. We see this in serial killers, sociopaths, and many of the people who end up actually implementing the punishments discussed above. However, one reason why I wanted to carve out the category of “direct pessimization” in the first place is because I don’t know how to draw a principled distinction between sadism, revenge, deterrence, and hurting others for instrumental benefit. For example, many people think of social status as zero-sum, and display petty cruelty (e.g. mocking jokes or mean-spirited gossip) in order to maintain their position in the hierarchy. The type of sadism that is characteristic of sociopaths is typically much more intense, but I’m not sure that the underlying logic is so different. Indeed, since sociopathy often results from traumatic childhood experiences, we could view it as a kind of “revenge” on the world at large—not too dissimilar to how normal people sometimes feel about those who have wronged them.\n\nThe examples I’ve discussed above only feature pessimization in one direction—but direct pessimization often spurs cycles of retribution, at many scales. Within individual psychology, internal conflict (e.g. procrastination) can prompt increasingly harsh self-coercive strategies, like vicious self-criticism. In relationships, emotional insecurity can lead partners to approach each other with blame, shame, and cruelty, until the person who loves them the most is also the one who hurts them the most. In politics, focusing on the other side’s failings can provoke cycles of [negative partisanship](https://en.wikipedia.org/wiki/Negative_partisanship) (see also [the toxoplasma of rage](https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/) and “[owning the libs](https://en.wikipedia.org/wiki/Owning_the_libs)”). In geopolitics, actions aimed at hurting enemy states (like sanctions or blockades) can spiral into military engagements, and from there into no-holds-barred wars. One way of viewing moral progress is the process of forming agreements to limit cycles of direct pessimization—both at large scales, like international conventions; and at small scales, like local norms of decency.\n\nWhile few of these examples will be novel to many readers, it feels useful to bring them together under a single heading—both to make it easier to reason about preventing them, and to contrast them with my two other types of pessimization. The most notable contrast is that direct pessimization is a transitive relationship: we describe A directly pessimizing B. Conversely, perverse pessimization is a reflexive relationship: “B pessimized themself” (or “B pessimized\" for short). What about when A harms B’s interests as a side effect of pursuing their own goals? Depending on how this happened, we might call it an accident, or negligence, or [constructive malice](https://en.wikipedia.org/wiki/Manslaughter#Constructive). However, I wouldn’t classify any of these as pessimization. Instead, I’m interested in the cases where B’s own actions make it much easier for their interests to be harmed, which I’d describe as B indirectly pessimizing themself. Let’s explore that now.\n\n### Indirect pessimization\n\nCaring about X often leads you to create intellectual and physical tools for affecting X. But these can convince and/or help people with other goals to produce ~X (even when they’re not specifically trying to hurt you). I call this dynamic indirect pessimization.\n\nThe simplest example of indirect pessimization is simply telling the wrong people about your goal. To pursue a goal X you need to have some conception of what X is and how you’re going to achieve it. But telling people about X prompts them to consider whether pursuing ~X is a good way to achieve their own interests. For example, if your goal is to prevent anyone from [creating mirror life, because it might destroy the world](https://www.asimov.press/p/mirror-life), then telling people about your goal may be what gives them the idea of creating mirror life at all.\n\nIndirect pessimization happens most easily when X advocates don’t have very good proposals for what people should do to help achieve X. If so, most ways that people act on their new knowledge about X will contribute to achieving ~X. For example, early AI safety advocates did a lot to raise awareness of AGI risk—but the lack of actionable strategies to help reduce risk led to much of that awareness being directed towards founding various AGI labs which are now racing towards AGI.\n\nFurthermore, pro-X coalitions often motivate work towards X by identifying its beneficial consequences. But every argument of the form X→Y (for example, “higher education makes people more progressive”) can be translated into the form ~Y→~X (“to avoid people becoming more progressive, limit higher education”). And so, unless there’s unanimity on Y being valuable, such arguments will convince some people to oppose X, which might outweigh the benefit of gaining new supporters. \n\nWe often see this effect in social justice advocacy. Calling something racist is an effective strategy in moderation, but [when overused](https://slatestarcodex.com/2017/06/21/against-murderism/) starts to make people think that racism isn’t so bad. Relatedly, [in this tweet](https://x.com/whstancil/status/1932484678574833902?s=46) a leftist is trying to make discussion of race science more prominent, because they think that believing in it is taboo enough to discredit their political opponents. But highlighting that a person with widespread support believes in race science will help undermine the taboo in the minds of that person’s supporters. Lastly, perhaps the most egregious example is the conflation of all leftist issues into a single “omnicause”—e.g. [this argument](https://www.instagram.com/p/DCW3FAZJYZP/?img_index=12) that “Palestine is the issue that makes us realise everything is interconnected. Every struggle for justice, freedom, and liberation”. Even if some people find this persuasive, it’s also a kind of indirect pessimization—now people who disagree with the writer on Palestine will be pushed towards disagreeing on everything else too.\n\nIndirect pessimization doesn’t just arise from constructing concepts and arguments, but also physical tools or mechanisms. For example, people sometimes propose constructing tech for deflecting asteroids away from Earth. But given how low the rate of dangerous asteroids hitting Earth is, [the most likely way for that to happen](https://forum.effectivealtruism.org/posts/RZf2KqeMFZZEpvBHp/risks-from-asteroids) is if asteroid-deflection technology is [misused to deflect asteroids *towards* Earth](https://www.narrativeark.xyz/p/jacob-on-the-precipice). So naive attempts to lower asteroid risk might easily empower people to increase it instead.\n\nSimilarly, consider the apocryphal [anecdote about the cobra effect](https://economicpoint.com/goodharts-law). This is often cited as an illustration of Goodhart’s law. But what’s most striking about the story is that it described an intervention not just failing to *reduce* the population of wild cobras, but actually *increasing* it. In other words, these (fictional) authorities indirectly pessimized their own goal. A real-world example comes from ML, where constructing a given objective function creates the possibility of accidentally inverting it—e.g. as OpenAI did by (briefly) [creating maximally-inappropriate AIs](https://arxiv.org/pdf/1909.08593) (section 4.4). [The Waluigi effect](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post)and [emergent misalignment](https://arxiv.org/abs/2502.17424) provide more recent examples of indirect pessimization in AI values. Meanwhile [I suspect](https://x.com/richardmcngo/status/1869089450346877207?s=46) that many evaluations of dangerous AI capabilities, which were intended to help prevent them from being developed, will instead help AGI labs accelerate towards them.\n\nOf course, when people try to achieve some goal, there are many possible undesirable side-effects. So why is it worth distinguishing specifically the category of side effects which cause the opposite of that goal? One reason is that many of the mechanisms which drive indirect pessimization are (unlike most side effects) relevant for a wide range of goals. Another is that [fear-based motivation](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL) makes indirect pessimization unusually hard to think about. The more scared you are that you might not achieve your goal, the more urgently you feel that “something must be done”, and the more you flinch away from picturing how that “something” might actually make it worse. This applies both on an individual level (where the possibility that you’re indirectly pessimizing is often very painful for your ego), and on a group level (where self-criticism is often taboo).\n\n### Perverse pessimization\n\nThe final type of pessimization I’ll talk about is *perverse* *pessimization*. I define this as a situation where the nominally pro-X faction is the one directly causing ~X to happen—i.e. the pro-X faction is sabotaging itself. What causes this? I’ll talk about two broad reasons: fear of success, and vice signalling within factions.\n\nI’ve talked above about how fear of not achieving one’s goals can indirectly pessimize those goals. But what’s far more perverse is a situation where a coalition is instead scared of *achieving* its nominal goals. One reason why this happens: the identity, prestige and even existence of a coalition often depend on the persistence of the problem it’s trying to solve. So the prospect of the problem going away can be very scary to individual members of the coalition (especially powerful or longstanding members) or the coalition as a whole.\n\nThis applies especially when solutions are offered from outside the coalition—if those solutions work, then the coalition has to admit that its previous strategies weren’t working, which might trigger shame or envy. So it’s particularly tempting for coalitions to suppress external attempts to achieve their own nominal goals. The environmentalist movement’s opposition to nuclear power, and often even to solar or wind infrastructure, provides a good example of this (as does its gradual slide towards being a generic anti-capitalist movement).\n\nAnother source of fear of success is that the more successful you are, the more you have to lose. Having hope, then losing it, is a very painful experience. So people often decide that it’s better to proactively identify as a victim in order to avoid that risk. From that mindset, other people’s success sparks envy, resentment, and attempts to tear them down—but this toxicity only makes one’s own problems worse. For example, the men who join incel communities deeply want to find partners, but end up in a perverse position where they would lose their community if they found a partner. (Some pointers to further reading on this dynamic: [tall poppy syndrome](https://en.wikipedia.org/wiki/Tall_poppy_syndrome); [the laws of Jante](https://en.wikipedia.org/wiki/Law_of_Jante); [this essay](https://articles.starcitygames.com/articles/stuck-in-the-middle-with-bruce/) about Magic: the Gathering; [Existential Kink](https://www.amazon.com/Existential-Kink-Unmask-Embrace-getting/dp/1578636477); and [Sadly, Porn](https://www.astralcodexten.com/p/book-review-sadly-porn).)\n\nWe also see fear of success in many relationships. The most successful relationships are those intimate enough to allow both people to feel deeply understood. But the vulnerability required for real intimacy is terrifying, and it’s easy for that fear to lead us to push friends or romantic partners away—essentially punishing them for wanting to be close to us. We hope that our defense mechanisms will ensure that we’re only vulnerable to people who deeply care about us, but they’re often precisely what prevent people from coming to deeply care about us (as masterfully portrayed in [Notes From Underground](https://en.wikipedia.org/wiki/Notes_from_Underground)).\n\nA second major reason why perverse pessimization arises is that anti-X behavior can help someone rise within a (nominally) pro-X coalition. This is a kind of vice signalling, showing that you have enough power to directly defy the values of your own coalition. I expect that there’s a lot of private signalling of cynicism in high-pressure law and finance firms, amongst politicians, and amongst highly competitive elites more generally. One particularly legible example comes from [Gerald Ratner](https://en.wikipedia.org/wiki/Gerald_Ratner), a jewelry CEO who called his own products “total crap” in an apparent attempt to signal cynicism.\n\nThese same dynamics play out within society as a whole, with domain-specific cynicism replaced by transgression against broader norms. Satanism, for instance, was historically compelling precisely because it was so transgressive. These days, few are offended by it. But [Hitler is now filling the role](https://theupheaval.substack.com/p/american-strong-gods) of a secular Satan, which then sparks transgressions like Kanye West’s recent song “[Heil Hitler](https://en.wikipedia.org/wiki/Heil_Hitler_(song))”, “ironic” neo-Nazism on 4chan, etc. Meanwhile many sexual fetishes are erotic in large part because they’re taboo, with the contrast between the fear of taboo violation and the relief of sexual acceptance being a source of pleasure. Jointly acting out taboo fetishes can allow elites to form transgression bonds—Epstein’s island being perhaps the most prominent example (as well as in Hollywood, with Weinstein, Diddy, etc).\n\nSuch transgressions typically start out covert. But it’s hard to keep controversial secrets—especially because bragging about transgressions is a good way to signal dominance. So transgressions tend to slide towards becoming “open secrets”, which often creates a self-fulfilling prophecy that there’s a [consensus of power](https://www.mindthefuture.info/p/elite-coordination-via-the-consensus) in favor of transgression. People who want to stay on the good side of that consensus learn to actively punish others for trying to enforce norms or pursue the stated goals of the coalition. [Hermann et al.](https://www.umass.edu/preferen/You%20Must%20Read%20This/herrmann-thoni-gachter.pdf) call this phenomenon [anti-social punishment](https://www.lesswrong.com/posts/X5RyaEDHNq5qutSHK/anti-social-punishment); Ben Hoffman calls it [depravity](https://benjaminrosshoffman.com/guilt-shame-and-depravity/); Jessica Taylor calls it [anti-normativity](https://unstableontology.com/2021/04/12/on-commitments-to-anti-normativity/); and Michael Vassar links it to [postmodernism](https://podcast.clearerthinking.org/episode/028/michael-vassar-preference-falsification-and-postmodernism/). I don’t know of systematic ways to detect anti-normativity, but sufficiently egregious hypocrisy can provide strong hints that it exists. For example, one of the US military’s strongest nominal values is abiding by US laws. Yet rather than rewarding whistleblowers who expose examples of criminal behavior, the military typically persecutes them.\n\nI’ve described fear of achieving one’s goals and anti-normativity as two separate mechanisms driving perverse pessimization. But being in an anti-normative coalition induces fear of achieving your goals, because succeeding (or even just standing out) makes you a target. And when you’re scared of achieving your goals, you’ll reward other people (or your own internal subagents) for signalling opposition to those goals, thereby reinforcing the anti-normative coalition. So we should actually think of them as two facets of a single phenomenon.\n\n### Orienting to pessimization\n\nPessimization sucks, but that doesn’t mean we should be constantly scared of it. The accusation of pessimization could easily become a bludgeon used to attack anyone doing anything valuable (which would, ironically, pessimize the desire to avoid pessimization). The challenge is to watch out for pessimization in a way which doesn’t slip into self-defeating cynicism.\n\nSo it’s also worth talking about limits on pessimization. While direct pessimization is a powerful way to cow your enemies, it also makes you a target for retaliation—e.g. violating norms against torture can spur domestic and international opposition to a regime. Meanwhile indirect and perverse pessimization are often covert enough that they only spread slowly. For example, even though some people find pedophilia attractive *because* it’s taboo, [I expect](https://x.com/richardmcngo/status/1912990666952753646?s=46) that the taboo overall dramatically reduces pedophilia.\n\nPerverse pessimization is also self-undermining: it makes systems weaker over time, which reduces their long-term influence. If big companies weren’t so often stuck in [moral mazes](https://thezvi.wordpress.com/2019/05/30/quotes-from-moral-mazes/), they could do a far better job at fending off startups. More generally, the parts of the world that avoid perverse pessimization will tend to grow and become more important over time. So [creative destruction](https://en.wikipedia.org/wiki/Creative_destruction) is one of our best defenses against perverse pessimization.\n\nHowever, letting institutions taken over by perverse pessimization fail can be arbitrarily costly—e.g. when [major governments rot from the head down](https://dominiccummings.substack.com/p/a-talk-on-regime-change). And so it can be incredibly high-leverage for whistleblowers and other reformers to identify and oppose perverse pessimization inside important institutions (see e.g. [Ellsberg](https://www.amazon.in/Secrets-Memoir-Vietnam-Pentagon-Papers-ebook/dp/B000OCXFY2), [Kokotajlo](https://time.com/7012881/daniel-kokotajlo/)). This is bottlenecked on virtues like courage and integrity, which is a major reason why I’ve become a virtue ethicist (as I’ll write about in an upcoming post). Indeed, I suspect that one way of understanding virtues is as traits that guard us against sliding into pessimization, thereby allowing us to more robustly steer the world towards desired outcomes.",
      "plaintextDescription": "“Your worst sin is that you have destroyed and betrayed yourself for nothing.” - Dostoevsky\n\nWhen people set an ambitious goal, they can fail simply by not changing the world very much. But there’s another surprisingly common way to fail: by achieving the opposite of their goal. I call this effect pessimization: the opposite of optimization.\n\nThough pessimization is an uncommon term, it’s not an uncommon concept. We allude to it whenever we call someone their own worst enemy, or predict that their actions will backfire. We try to take advantage of it with reverse psychology. It’s the subject of Robert Conquest’s third law of politics: “the simplest way to explain the behavior of any bureaucratic organization is to assume that it is controlled by a cabal of its enemies.” And the POSIWID aphorism (“the purpose of a system is what it does”) is often used to describe ways in which a system actively opposes its nominal purpose. I’ve also previously described the pessimization of ideological advocacy under the heading of “the activist’s curse”.\n\nI divide pessimization into three types, each of which I’ll discuss in detail. The most intuitive is what I call direct pessimization: when an enemy chooses what to do based on what will harm your interests the most. This is the sense in which “pessimize” is used by e.g. Yudkowsky and Soares. Drivers of direct pessimization include sadism, revenge and threats.\n\nConversely, indirect pessimization arises when your actions help other people achieve the opposite of your goals, even though they’re not deliberately trying to hurt you. And perverse pessimization occurs when an agent or coalition nominally dedicated to a goal is actively trying to achieve the opposite of that goal—i.e. it’s directly pessimizing itself.\n\nI’d like to improve our understanding of pessimization so that we can better create, identify, and promote remedies to it. In the rest of this post I characterize each of the three types of pessimization I mentioned above.",
      "wordCount": 3063
    },
    "tags": [
      {
        "_id": "YyGDbZhGtws5hEPda",
        "name": "Self Fulfilling/Refuting Prophecies",
        "slug": "self-fulfilling-refuting-prophecies"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "onsZ4JeA8EMX3hb7B",
    "title": "Applying right-wing frames to AGI (geo)politics",
    "slug": "applying-right-wing-frames-to-agi-geo-politics",
    "url": "https://x.com/richardmcngo/status/1942636879658074230?s=46",
    "baseScore": 64,
    "voteCount": 61,
    "viewCount": null,
    "commentCount": 25,
    "createdAt": null,
    "postedAt": "2025-07-08T18:03:36.781Z",
    "contents": {
      "markdown": "I've increasingly found right-wing political frameworks to be valuable for thinking about how to navigate the development of AGI. In this post I've copied over a twitter thread I wrote about three right-wing positions which I think are severely underrated in light of the development of AGI. I hope that these ideas will help the AI alignment community better understand the philosophical foundations of the new right and why they're useful for thinking about the (geo)politics of AGI.\n\n### 1\\. [The hereditarian revolution](https://ncofnas.com/p/a-guide-for-the-hereditarian-revolution)\n\nNathan Cofnas claims that the intellectual dominance of left-wing egalitarianism relies on group cognitive differences being taboo. I think this point is important and correct, but he doesn't take it far enough. Existing group cognitive differences pale in comparison to the ones that will emerge between baseline humans and:\n\n*   humans who leverage AI most effectively\n*   humans with brain-computer interfaces\n*   genetically engineered humans\n*   AIs themselves\n\nCurrent cognitive differences already break politics; these will break it far more. So we need to be preparing for a future in which egalitarianism as an empirical thesis is (even more) obviously false.\n\nI don’t yet have a concise summary of the implications of this position. But at the very least I want a name for it. Awkwardly, we don’t actually have a good word for “anti-egalitarian”. Hereditarian is too narrow (as is hierarchist) and elitist has bad connotations.\n\nMy candidate is “asymmetrist”. Egalitarianism tries to enforce a type of symmetry across the entirety of society. But our job will increasingly be to design societies where the absence of such symmetries is a feature not a bug.\n\n(See also [this talk of mine](https://x.com/richardmcngo/status/1905004874242388295?s=46).)\n\n### 2\\. Protectionism\n\nProtectionism gets a bad rap, because global markets are very efficient. But interacting with them is very much not adversarially robust. If you are a small country and you open your borders to the currency, products and companies of a much larger country, then you will get short-term wealthier but also have an extremely hard time preventing that other country from gaining a lot of power over you in the long term. (As a historical example, trade was often an important precursor to colonial expansion. See also Amy Chua’s excellent book World on Fire, about how free markets enable some minorities to gain disproportionate power.)\n\nWhen you’re poor enough, or the larger power is benevolent enough, this may well be a good deal! But we’re heading towards a future in which a) most people become far wealthier in absolute terms due to AI-driven innovation, and b) AIs will end up wielding a lot of power in not-very-benevolent ways (e.g. automated companies that have been given the goal of profit-maximization).\n\nGiven this, protectionism starts to look like a much better idea. The fact that it slows growth is not a problem, because society will already be reeling from the pace of change. And it lets you have much more control over the entities that are operating within your borders - e.g. you can monitor the use of AI decision-making within companies much more closely.\n\nTo put it another way, in the future the entire human economy will be the “smaller country” that faces incursions by currency, products and companies under the control of AIs (or humans who have delegated power to AIs). Insofar as we want to retain control, we shouldn’t let people base those AIs in regulatory havens while still being able to gain significant influence over western countries.\n\nOkay, but won’t protectionist countries just get outcompeted? Not if they start off with enough power to deter other countries from deploying power-seeking AIs. And right now, the world’s greatest manufacturing power is already fairly protectionist. So if the US moves in that direction too, it seems likely that the combined influence of the US and China will be sufficient to prevent anyone else from “defecting”. The bottleneck is going to be trust between the two superpowers.\n\n### 3\\. [National conservatism](https://nationalconservatism.org/national-conservatism-a-statement-of-principles/)\n\nAll of the above is premised on the goal of preserving human interests in a world of much more powerful agents. This is inherently a kind of conservatism, and one which we shouldn’t take for granted. The tech right often uses the language of “winning”, but [as I’ve observed before](https://x.com/richardmcngo/status/1872394799971758108?s=46) there will increasingly be a large difference between a \\*country\\* winning and its \\*citizens\\* winning. In the limit, a fully-automated country could flourish economically and politically without actually benefiting any of the humans within it.\n\nNational conservatism draws a boundary around a group of people and says “here are the people whose interests we’re primarily looking out for”. [As Vance put it](https://www.youtube.com/watch?v=yXH1p8_jhc8), America is a group of people with a shared history and a common future. Lose sight of that, and arguments about efficiency and productivity will end up turning it instead into a staging-ground for the singularity. (Nor can you run a country for the benefit of “all humans”, because then you’re in an adversarial relationship with your own citizens, who rightly want their leaders to prioritize them.)\n\nChina’s government has many flaws, but it does get this part right. They are a nation-state run by their own people for their own people. As part of that, they’re not just economically protectionist but also culturally protectionist - blocking foreign ideas from gaining traction on their internet. I don’t think this is a good approach for the West, but I think we should try to develop a non-coercive equivalent: mechanisms by which a nation can have a conversation with itself about what it should value and where it should go, with ideas upweighted when their proponents have “skin in the game”. Otherwise the most eloquent and persuasive influencers will end up just being AIs.\n\nAll of these ideas are very high-level, but they give an outline of why I think right-wing politics is best-equipped to deal with the rise of AI. There’s a lot more work to do to flesh them out, though.",
      "plaintextDescription": "I've increasingly found right-wing political frameworks to be valuable for thinking about how to navigate the development of AGI. In this post I've copied over a twitter thread I wrote about three right-wing positions which I think are severely underrated in light of the development of AGI. I hope that these ideas will help the AI alignment community better understand the philosophical foundations of the new right and why they're useful for thinking about the (geo)politics of AGI.\n\n\n1. The hereditarian revolution\nNathan Cofnas claims that the intellectual dominance of left-wing egalitarianism relies on group cognitive differences being taboo. I think this point is important and correct, but he doesn't take it far enough. Existing group cognitive differences pale in comparison to the ones that will emerge between baseline humans and:\n\n * humans who leverage AI most effectively\n * humans with brain-computer interfaces\n * genetically engineered humans\n * AIs themselves\n\nCurrent cognitive differences already break politics; these will break it far more. So we need to be preparing for a future in which egalitarianism as an empirical thesis is (even more) obviously false.\n\nI don’t yet have a concise summary of the implications of this position. But at the very least I want a name for it. Awkwardly, we don’t actually have a good word for “anti-egalitarian”. Hereditarian is too narrow (as is hierarchist) and elitist has bad connotations.\n\nMy candidate is “asymmetrist”. Egalitarianism tries to enforce a type of symmetry across the entirety of society. But our job will increasingly be to design societies where the absence of such symmetries is a feature not a bug.\n\n(See also this talk of mine.)\n\n\n2. Protectionism\nProtectionism gets a bad rap, because global markets are very efficient. But interacting with them is very much not adversarially robust. If you are a small country and you open your borders to the currency, products and companies of a much larger country, then you w",
      "wordCount": 990
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "kdbs6xBndPkmrYAxM",
        "name": "Politics",
        "slug": "politics"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "rSaDFQ6JSzhC9w7Hh",
    "title": "Well-foundedness as an organizing principle of healthy minds and societies",
    "slug": "well-foundedness-as-an-organizing-principle-of-healthy-minds",
    "url": "https://www.mindthefuture.info/p/well-foundedness-as-an-organizing",
    "baseScore": 35,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2025-04-07T00:31:34.098Z",
    "contents": {
      "markdown": "*“If a kingdom is divided against itself, it cannot stand. And if a house is divided against itself, it cannot be maintained.” -* Mark 3:24\n\nIn [my last post](https://www.mindthefuture.info/p/towards-a-scale-free-theory-of-intelligent) I argued that we should view intelligent agents as coalitions of cooperating and competing subagents. The crucial question is then: how can we characterize effectively-functioning coalitional agents? One standard criterion is *coherence*: the extent to which the agent acts as if it has consistent goals and beliefs. In other words, an agent is coherent insofar as disagreeing subagents are able to still act as a functional coalition. For example:\n\n*   A coherent individual can decide how to resolve an internal conflict and then stick with that commitment, rather than vacillating or procrastinating as their mood changes.\n*   A coherent company is able to decide on an overarching strategic plan and then execute it, without different divisions prioritizing their own interests.\n*   A coherent country has a clear set of national interests that its leaders and people consistently prioritize (even when it conflicts with their personal interests).\n\nCoherence is a valuable property for a coalition to have, but I think that characterizing idealized agents primarily in terms of coherence gives us an impoverished understanding of them. For example, a coalition which is highly-coherent because its leaders exercise a lot of top-down control is much less robust than a coalition which is highly-coherent because all its subagents actually want to cooperate with each other.\n\nIn this post I try to capture the difference between these two possibilities in terms of a property I call *well-foundedness*.[^9a1z6t3tmj6] I define an agent as well-founded to the extent that conflicts between its subagents don’t propagate *down* to induce conflicts *within* those subagents. For example:\n\n*   In a well-founded marriage, spouses don’t try to induce internal conflict within their partner (e.g. shaming or guilting them) to win fights.\n*   In a well-founded company, employees don’t need to pick sides during power struggles between executives—they can just carry on with their jobs.\n*   In a well-founded country, people are friends and colleagues despite supporting different political factions.\n\nI think of well-foundedness as complementary to (and in some ways dual to) coherence. Ideal agents should have both properties. I don’t yet know how to define well-foundedness precisely, but in the rest of this post I characterize it informally by describing the four possible combinations of coherent/incoherent and well-founded/poorly-founded.\n\nIncoherence with or without well-foundedness\n--------------------------------------------\n\nConsider two countries each experiencing strong internal political polarization, but in very different ways. In Holistan, the two factions are the one representing East Holistan and the one representing West Holistan. By contrast, in Fractistan the two factions represent two subpopulations who live closely intermingled throughout the country—say, two religious or ethnic groups.\n\nFor each country, increasing internal tensions makes them less coherent—each half of the country thinks of the other half as their enemy. But the two conflicts will play out very differently. In Holistan, you might see the East and the West start to cut ties with each other; set up parallel governance structures; or discourage travel or trade between them. If conflict continues to escalate, you might see a civil war, with each side drawing on their territory and population to muster an army.\n\nThis is pretty bad! But despite being very incoherent as a country, Holistan is still relatively well-founded, because each of East and West Holistan are *internally* still pretty coherent. What that means is that they have a line of retreat from conflict. The two sides in Holistan can still disengage; they can draw up peace treaties; they can form two separate countries. After the war ends, the fabric of each society remains intact—colleagues are still on amicable terms, neighbors still trust neighbors, city councils can still debate issues without relitigating the war with each conflict.\n\nPerhaps the most vivid illustration of how important this is comes from World War 2. I am often struck by how, after the most devastating war the world has ever seen, European countries quickly recovered to unprecedented heights of prosperity, and even became close allies. I think a significant reason for that is that World War 2 was a relatively well-founded conflict between nation-states. It didn’t turn them into low-trust societies.\n\nBy contrast, Fractistan is neither coherent nor well-founded. The lack of clear territorial boundaries between the two factions makes it harder for rising tensions to erupt into a full-scale civil war. But the whole country is affected regardless: each region and city and neighborhood faces an internal power struggle. You might see lynch mobs or pogroms; or, in an extreme case, the kind of decentralized genocide that happened in Rwanda.\n\nAnd there’s no easy way to end the conflict. Even if a nation-wide compromise is reached, each person will still be surrounded by former enemies. Each small-scale flare-up of renewed conflict will trigger further cycles of escalation. In other words, Fractistan will persist as a country, but with low social trust for the indefinite future. I picture it with a fracture running from top to bottom, fractally dividing the country in two at every level of organization.\n\nAside from Rwanda, two countries with Fractistan-like conflicts were [Bosnia and Herzegovina](https://en.wikipedia.org/wiki/Bosnian_War) and pre-partition India, which both saw widespread neighbor-on-neighbor violence. India was lucky to have a strong geographic separation between the bulk of its Hindus and Muslims. Even so, however, the process of splitting India and Pakistan was incredibly messy and cost hundreds of thousands of lives.\n\nThe tradeoff between coherence and well-foundedness\n---------------------------------------------------\n\nCoherence and well-foundedness are separate properties which are both individually valuable. But there are some tradeoffs between them. To explore those it’s useful to consider the opposite of Holistan: a country which is very coherent but also very poorly-founded.\n\nI could make up an example here, but we already have one that matches the description very well: North Korea. As a country, it’s extremely coherent. Its whole government follows the instructions of one man. Its whole society follows the instructions of its government. There’s no national-level dissent, nor regional-level dissent, nor even dissent on the level of local communities.\n\nBut repressing dissent doesn’t make it go away—it just pushes it down to lower-level subagents. I expect that some North Koreans feel safe to dissent within their families; others only within the privacy of their own thoughts; and others not even there, but only in their [subconscious shadows](https://en.wikipedia.org/wiki/Shadow_(psychology)). Even when this dissent never surfaces openly, it is visible in the cost and scale of the control apparatus required to keep it repressed. If that control apparatus ever breaks, North Korea’s coherence would fall apart.\n\nSuch extreme repression is obviously bad. But in moderation repression can be a valuable driver of coherence. [Henrich hypothesizes](https://eh.net/book_reviews/the-weirdest-people-in-the-world-how-the-west-became-psychologically-peculiar-and-particularly-prosperous) that the success of the West was driven by the Catholic Church’s prohibition against cousin marriage, which made Christian Europe less clannish. If true, you can think of this as trading off well-foundedness for coherence: by repressing kinship-based networks, the Church made larger-scale cooperation possible. More generally, societal morality works by repressing the antisocial instincts of each individual (as well as groups organized around antisocial behavior, like criminal gangs).\n\nA similar set of tradeoffs arise in individual psychology: people can become more disciplined by repressing their emotions. This makes them more coherent—e.g. they can choose to work long hours on things that are instrumentally useful. But it often harms their ability to enjoy themselves and to understand and process their underlying motivations.\n\nConversely, to become well-founded, you need to surface ways in which conflicts manifest at low levels and then resolve them. This requires the opposite of repression: *expression*. Specifically, it requires that lower-level subagents are able to express their true preferences (as they can in individuals who freely let their emotions surface, or countries which let political dissidents speak freely).\n\nWhat are the tradeoffs between building coherence via repression, and building well-foundedness via expression? I think of the former as making the average-case outcome worse, but also reducing variance. Repression is therefore appropriate when you’re in a *scarce* environment—one in which a big loss could totally wipe you out. An individual whose career could be ruined if they let their emotions show needs to repress them (even if it makes them more stressed and less productive on average). And a country which could be invaded if it gets distracted by internal politics needs to repress dissent (even if there’s something valuable to be learned from that dissent).\n\nBy contrast, expression tends to lead to better outcomes, but at the cost of also increasing variance. [Expressing underlying conflicts allows them to be solved directly](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL/p/QqtQoHSjjfLgC4jDZ), but makes the overall agent less coherent until things actually resolve. For example, instead of sniping at each other about household chores, a married couple could express the emotional fears that underlie those frustrations. If that goes well, they’d feel much more respected and appreciated afterwards; but if it goes badly it could provoke a (potentially relationship-ending) fight. In a political context, letting dissidents speak out could lead to valuable reform, but it could also give rise to a full-fledged separatist movement.\n\nSo the tradeoff between repression and expression is a nuanced one, and needs to be decided on a case-by-case basis. But in general it’s likely that we err too far on the side of repression, because we don’t intuitively realize how abundant the world has become. Most countries don’t face significant risk of invasion, and therefore the costs of secession would often be outweighed by the benefits (more national cohesion, [better governance](https://www.sciencedirect.com/science/article/abs/pii/S0014292110000991), and being more well-founded in general). On an individual level, we can now move between different communities far more easily than at any previous point in history, and so taking the social risk of letting out our emotions is less dangerous than our intuitions are calibrated to expect. (There are some prominent exceptions, which I’ll discuss in a follow-up post, but I’m trying to keep this post relatively free of politically controversial examples.)\n\nIdealized coalitional agency\n----------------------------\n\nI think the tradeoff I’ve described above is an important dynamic in almost all real-world agents. But I don’t think it’s *inevitable*. We can imagine coalitions designed so that low-level agents can express their preferences, and make local improvements, without threatening the stability of the overall coalition.\n\nWhat would such designs look like? I expect that a key component is “peace treaties” between high-level subagents, where they all agree not to use certain types of low-level conflicts to further their own ends. We can think of liberalism as a peace treaty which allows people with different religious and political beliefs to coexist. Meanwhile, arrangements like capitalism and democracy channel conflicts into formats that are productive (like business competition and political campaigning) rather than destructive (like theft or violence).\n\nBut well-foundedness at one level requires coherence at levels below that—otherwise it’s easy for conflicts to propagate downwards. And the systems I describe above aren’t very good at creating (or maintaining) lower-level coherence. In [Reno’s terminology](https://theupheaval.substack.com/p/american-strong-gods), they are “weak gods” whose primary purpose is to help different groups coexist, but which don’t have strong opinions about what those groups should actually care about. Conversely, “strong gods” like nationalism provide the substantive ideological content capable of unifying people under a single coherent identity, at the cost of excluding outsiders. The challenge of designing an idealized coalitional agent can be seen as the unsolved problem of balancing weak and strong gods across many different scales.\n\n[^9a1z6t3tmj6]: A well-founded set is one which has a bottom element. By somewhat tenuous analogy, a well-founded agent is one whose internal conflicts bottom out. Unlike the set theory definition, my notion of well-foundedness is a matter of degree: the further down an agent’s internal conflicts go, the less well-founded it is.",
      "plaintextDescription": "“If a kingdom is divided against itself, it cannot stand. And if a house is divided against itself, it cannot be maintained.” - Mark 3:24\n\nIn my last post I argued that we should view intelligent agents as coalitions of cooperating and competing subagents. The crucial question is then: how can we characterize effectively-functioning coalitional agents? One standard criterion is coherence: the extent to which the agent acts as if it has consistent goals and beliefs. In other words, an agent is coherent insofar as disagreeing subagents are able to still act as a functional coalition. For example:\n\n * A coherent individual can decide how to resolve an internal conflict and then stick with that commitment, rather than vacillating or procrastinating as their mood changes.\n * A coherent company is able to decide on an overarching strategic plan and then execute it, without different divisions prioritizing their own interests.\n * A coherent country has a clear set of national interests that its leaders and people consistently prioritize (even when it conflicts with their personal interests).\n\nCoherence is a valuable property for a coalition to have, but I think that characterizing idealized agents primarily in terms of coherence gives us an impoverished understanding of them. For example, a coalition which is highly-coherent because its leaders exercise a lot of top-down control is much less robust than a coalition which is highly-coherent because all its subagents actually want to cooperate with each other.\n\nIn this post I try to capture the difference between these two possibilities in terms of a property I call well-foundedness.[1] I define an agent as well-founded to the extent that conflicts between its subagents don’t propagate down to induce conflicts within those subagents. For example:\n\n * In a well-founded marriage, spouses don’t try to induce internal conflict within their partner (e.g. shaming or guilting them) to win fights.\n * In a well-founded company, emplo",
      "wordCount": 1911
    },
    "tags": [
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6YxdpGjfHyrZb7F2G",
    "title": "Third-wave AI safety needs sociopolitical thinking",
    "slug": "third-wave-ai-safety-needs-sociopolitical-thinking",
    "url": null,
    "baseScore": 99,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 23,
    "createdAt": null,
    "postedAt": "2025-03-27T00:55:30.548Z",
    "contents": {
      "markdown": "*At EA Global Boston last year I gave a talk on how we're in the third wave of EA/AI safety, and how we should approach that. This post contains a (lightly-edited) transcript and slides. Note that in the talk I mention a set of bounties I'll put up for answers to various questions—I still plan to do that, but am taking some more time beforehand to get the questions right.*\n\nTranscript and slides\n=====================\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3ed7d881088d7434d6c2149bceb9324011439693d20c2ed9.jpg)\n\nHey everyone. Thanks for coming. You're probably looking at this slide and thinking: \"how to change a changing world? That's a weird title for a talk. Probably it's a kind of weird talk.\" And the answer is, yes, it is a kind of weird talk.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/9f6f4f1324db67f8a2a9dbeb303d2e0cd80fe77bb48be8ed.jpg)\n\nSo what's up with this talk? Firstly, I am trying to do something a little unusual here. It's quite abstract. It's quite disagreeable as a talk. I'm trying to poke at the things that maybe we haven't been thinking enough about as a movement or as a community. I'm trying to do a \"steering the ship\" type thing for EA.\n\nI don't want to make this too inside baseball or anything. I think it's going to be useful even if people don't have that much EA background. But this seems like the right place to step back and take stock and think: what's happened so far? What's gone well? What's gone wrong? What should we be doing?\n\nSo the structure of the talk is:\n\n1.  **Three waves of EA and AI safety**. (I mix these together because I think they are pretty intertwined, at least intellectually. I know a lot of people here work in other cause areas. Hopefully this will still be insightful for you guys.)\n2.  **Sociopolitical thinking and AI**. What are the really big frames that we need as a community to apply in order to understand the way that the world is going to go and how to actually impact it over the next couple of decades?\n\nThere's a thing that sometimes happens with talks like this—maybe status regulation is the word. In order to give a really abstract big picture talk, you need to be super credible because otherwise it's very aspirational. I'm just going to say a bunch of stuff where you might be like, that's crazy. Who's this guy to think about this? It's not really his job or anything. I mean, it kind of is my job, and part of it is my own framework, but a lot of it is reporting back from certain front lines. I get to talk to a bunch of people who are shaping the future or  living in the future in a way that most of the world isn't. So part of what I'm doing here is trying to integrate those ideas from a bunch of people that I get to talk to and lay them out. Your mileage may vary.\n\n**Three waves of EA and AI safety**\n-----------------------------------\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d032ed0c731f10adff93c3c0c87fa6b46ed3b8813dd4f2dd.jpg)\n\nLet's start with a very stylized history of EA and AI safety. I'm going to talk about it in three waves. Wave number one I’ll say, a little arbitrarily, is 2005 to 2013. That's the orientation wave. This wave was basically about figuring out what was up with the world, introducing a bunch of big picture ideas. The Future of Humanity Institute, Overcoming Bias and LessWrong were primarily venues for laying out worldviews and ideas, and that was intertwined with Giving What We Can and the formation of the Oxford EA scene. I first found out about EA from a blog post on LessWrong back in 2010 or 2011 about how you should give effectively and then give emotionally separately.\n\nSo that was one wave. The next wave I'm going to call mobilization, and that's basically when things start getting real. People start trying to actually have big impacts on the world. So you have OpenPhil being spun out. You have a couple of books that are released around this time: Superintelligence, Doing Good Better. You have the foundation of OpenAI, and also around this time the formation of the DeepMind AGI safety team.\n\nSo at this point, there's a lot of energy being pushed towards: here's a set of ideas that matter. Let's start pushing towards these. I'm more familiar with this in the context of AI. I think a roughly analogous thing was happening in other cause areas as well. Maybe the dates are a little bit different.\n\nThen I think the third wave is when things start mattering really at scale. So you have a point at which EA and AI safety started influencing the world as a whole on a macro level. The FTX collapse was an event with national and international implications, one of the biggest events of its kind. You had the launch of ChatGPT and various OpenAI dramas that transfixed the world. You had open letters about AI safety, endorsements by some of the leading figures in AI, safety summits, AI safety institutes, all of that stuff. It's no longer the case that EA and AI safety are just one movement that's just doing stuff off in its corner. This stuff is really playing out at scale.\n\nTo some extent this is true for other aspects of EA as well. A lot of other a lot of the key intellectual figures that are most prominent in American politics, people like Ezra Klein and Nate Silver and so on, really have been influenced by the worldview that was developed in this first wave and then mobilized in the second wave.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ae77476b72bd4631a0f8dfe902267d3ac50ba8be1c603efc.jpg)\n\nI think you need pretty different skills in order to do each of these types of things well. We’ve seen an evolution of what orientation you need to have in order to succeed at these types of waves, which to various extents we have or haven't had.\n\nSo for the first wave, it's thinking oriented. You need to be really willing to look beyond the frontiers of what other people are thinking about. You need to be high-decoupling. You're willing to transgress on existing norms or existing assumptions that people take for granted. And you want to be consequentialist. You want to think about what is the biggest impact I can have? What's the thing that people are missing? How can I scale?\n\nAs you start getting into the second wave, you need to add on another set of skills and orientations. One term for this in the startup context is black swan farming. Another term that OpenPhil has used is hits-based giving, the idea that you should really be optimizing for getting a few massive successes, and that's where most of your impact comes from. In order to do those massive successes well, you need the skill of implementation and execution. That's pretty different thing from sitting at an institute in Oxford thinking about the far future.\n\nTo do this well, I think the other thing you need is deontology. Because when you start actually doing stuff—when you start trying to scale a company, when you start trying to influence philanthropy—it's easy to see ways in which maybe you could lie a little bit. Maybe you can be a bit manipulative. In extreme cases, like FTX, it'd be totally deceitful. But even for people operating at much smaller scales, I think deontology here was one of the guiding principles that needed to be applied and in some cases was and [in some cases wasn't](https://www.givewell.org/about/official-records/board-meeting-3/FAQ-on-inappropriate-marketing).\n\nNow we're in the third wave. What do we need for the third wave? I think it's a couple of things. Firstly, I think we can no longer think in terms of black swan farming or hit space giving. And the reason is that when you are operating at these very large scales, it's no longer enough to focus on having a big impact. You really need to make sure the impact is positive.\n\nIf you're trying to found a company, you want the company to grow as fast as possible. The downside is the company goes bust. That's not so bad. If you're trying to shape AI regulation, then you can have a massive positive impact. You can also have a massive negative impact. There's really a different set of mindset and set of skills required.\n\nYou need adaptability because on the timeframe that you might build a company or start a startup or start a charity, you can expect the rest of the world to remain fixed. But on the timeframe that you want to have a major political movement, on the timeframe that you want to reorient the U.S. government's approach to AI, a lot of stuff is coming at you. The whole world is, in some sense, weighing in on a lot of the interests that have historically been EA's interests.\n\nAnd then thirdly, I think you want virtue ethics. The reason I say this is because when you are operating at scale, deontology just isn't enough to stop you from being a bad person. If you're operating a company, maybe you commit some fraud, as long as you don't step outside the clear lines of what's legal and what's moral, then people kind of expect that maybe you use shady business tactics or something.\n\nBut when you're a politician and you're being shady you can screw a lot of stuff up. When you're trying to do politics, and when you're in domains that have very few clear, bright lines, it's very hard to say you crossed a line here because it's all politics. The way to do this in a high integrity way that leads to other people wanting to work with you is really to focus on doing the right things for the right reasons. I think that's the thing that we need to have in order to operate at scale in this world.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5dc775c831a8be89b00995209c4226d43980af7501dac8bb.jpg)\n\nSo what does it look like to fuck up the third wave? The next couple of slides are deliberately a little provocative. You should take them 80% of how strongly I say them, and in general, maybe you should take a lot of the stuff I say 80% of how seriously I say it, because I'm very good at projecting confidence.\n\nBut I claim that one of the examples where operating at scale is just totally gone to shit is the environmentalist movement. I would somewhat controversially claim that by blocking nuclear power, environmentalism caused climate change. Via the Environmental Protection Act, environmentalism caused the biggest obstacle to clean energy deployment across America. Via opposition to geoengineering, it's one of the biggest obstacles to actually fixing climate change. The lack of growth of new housing in Western countries is one of the biggest problems that's holding back Western GDP growth and the types of innovation that you really want in order to protect the environment.\n\nI can just keep going down here. I think the overpopulation movement really had dramatically bad consequences on a lot of the developing world. The blocking of golden rice itself was just an absolute catastrophe.\n\nThe point here is not to rag on environmentalism. The point is: here's a thing that sounds vaguely good and kind of fuzzy and everyone thinks it's pretty reasonable. There are all these intuitions that seem nice. And when you operate at scale and you're not being careful, you don't have the types of virtues or skills that I laid out in the last slide, you just really fuck a lot of stuff up. (I put recycling on there because I hate recycling. Honestly, it's more a symbol than anything else.)\n\nI want to emphasize that there is a bunch of good stuff. I think environmentalism channeled a lot of money towards the development of solar. That was great. But if you look at the scale of how badly you can screw these things up when you're taking a mindset that is not adapted to operating at the scale of a global economy or global geopolitics, it's just staggering, really. I think a lot of these things here are just absolute moral catastrophes that we haven't really reckoned with.\n\nFeel free to dispute this in office hours, for example, but take it seriously. Maybe I want to walk back these claims 20% or something, but I do want to point at the phenomenon.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5d805a9fc597cfc4eb9153e43f3a70f29bf3f7dc1b19fcea.jpg)\n\nAnd then I want to say: okay, what if that happens for us? I can kind of see that future. I can kind of picture a future where AI safety causes a bunch of harms that are analogous to this type of thing:\n\n*   Causing massive AI acceleration (insofar as you think that's a harm).\n*   If you block open source, plausibly it stalls a bunch of progress on interpretability.\n*   If you centralize power over AI, then maybe that sets the stage itself for AI takeover.\n*   If you regulate AI companies in the wrong ways, then maybe you become one of the biggest obstacles to implementing AI safety techniques.\n*   If you pass a bunch of safety legislation that actually just adds on too much bureaucracy. Europe might just not be a player in AI because they just can't have startups forming partly because of things like the EU AI Act.\n*   The relationship of the U.S. and China, it's a big deal. I don't know what effect AI safety is going to have on that, but it's not clearly positive.\n*   Making AI really annoying—that sucks.\n\nThe thing I want to say here is I really do feel a strong sense of: man, I love the AI safety community. I think it's one of the groups of people who have organized to actually pursue the truth and actually push towards good stuff in the world more than almost anyone else. It's coalesced in a way that I feel a deep sense of affection for. And part of what makes me affectionate is precisely the ability to look at stuff like this and be like: okay, what do we do better?\n\nAll of these things are operating at such scale, what do we actually need to do in order to not accidentally screw up as badly as environmentalism (or your example of choice if you don't believe that particular one)?\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/29b98aa4737a04361ec692b423d88912027bfb3b9db4ddb6.jpg)\n\n*Note: I plan to post these bounties separately, but am taking some more time beforehand to get the questions right.*\n\nSo do I have answers? Kind of. These are just tentative answers and I'll do a bit more object-level thinking later. This is the more meta element of the talk. There's one response: oh, boy, we better feel really guilty whenever we fuck up and apply a lot of pressure to make sure everyone's optimizing really hard for exactly the good things and exclude anyone who plausibly is gonna skew things in the wrong direction.\n\nAnd I think this isn't quite right. Maybe that works at a startup. Maybe it works at a charity. I don't think it works in Wave 3, because of the arguments I gave before. Wave 3 needs virtue ethics. You just don't want people who are guilt-ridden and feeling a strong sense of duty and heroic responsibility to be in charge of very sensitive stuff. There's just a lot of ways that that can go badly. So I want to avoid that trap.\n\nI want to have a more grounded view. These are hard problems to solve. And also, we have a bunch of really talented people who are pretty good at solving problems. We just need to look at them and start to think things through.\n\nHere's a list of questions that I often ask myself. I don't really have time to answer a lot of these questions: hey, here's a thing that we could have done better, have we actually written up the lessons from that? Can we actually learn from it as a community?\n\nI'm going to put up a post on this about how I'll offer bounties for people who write these up well, maybe $2,000 to $5,000-ish on these questions. I won't go through them in detail, but there's just a lot of stuff here. I feel like I want to learn from experience, because there's a lot of experience that the community has aggregated across itself. And I think there's a lot of insight to be gained—not in a beating ourselves up way, not in a \"man, we really screwed up the malaria vaccines back in 2014\" way—but by asking: yeah, what was going on there? Malaria was kind of our big thing. And then malaria vaccines turned out to be a big deal. Was there some missing mental emotion that needed to happen there? I don't know. I wasn't really thinking about it at the time. Maybe somebody was. Maybe somebody can tell me later. And the same thing for all of these others.\n\nThe other thing I really want is high-quality sociopolitical thinking, because when you are operating at this large scale, you really need to be thinking about the levers that are moving the world. Not just the local levers, but what's actually going on in the world. What are the trends that are shaping the next decade or two decades or half century or century? EA is really good in some ways at thinking about the future, and in other ways, tends to abstract away a little too much. We think about the long-term future of AI, but the next 20 years of politics, somehow doesn't feel as relevant. So I'm going to do some sociopolitical thinking in the second half of this talk.\n\n**Sociopolitical thinking and AI**\n----------------------------------\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/9662f2915cd9c204624e5c4d5c8a1e81224acac15b489a05.jpg)\n\nSo here's part two of the talk: what's going on with the world? High-quality sociopolitical thinking, which is the sort of thing that can ground these large-scale Wave 3 actions, should be big picture, should be first principles, historically grounded insofar as you can. It’s pretty different from quantitative-type thinking that a lot of EAs are pretty good at. At first, the thing you're doing is trying to identify deep levers in the world, these sort of structural features of the world that other people haven't really noticed.\n\nI will flag a couple of people who I think are really good at this. I think Friedman, talking about “The world is flat”; Fukuyama, talking about “The end of history”. Maybe they're wrong, maybe they're right, but it's a type of thinking that I want to see much more of. Dominic Cummings: whether you like him or you hate him, he was thinking about Brexit just at a deeper level than basically anyone else in the world. Everyone else was doing the economic debate about Brexit, and he was thinking about the sociocultural forces that shape nations on the time frame of centuries or decades. Maybe he screwed it up, maybe you didn't, but at the very least you want to engage with that type of thinking in order to actually understand how radical change is going to play out over the next few decades.\n\nI am going to, in the rest of this talk, just tell you my two-factor model of what's going on with the world. And this is the bit where I'm definitely going a long way outside my domain of expertise, definitely this is just way bigger than it is all plausible for a two-factor model to try and do. So don't believe what I'm saying here, but I want to have people tell me that I'm being an idiot about this. I want you to do better than this. I want to engage with alternative conceptions of how to make sense of the world as a whole in such a way that we can then slot in a bunch of more concrete interventions like AI safety interventions or global health interventions or whatever.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7aa7534ef85953f5eb365f8fa5495cc0813c8a391cb6ae42.jpg)\n\nOkay, here's my two-factor model. I want to understand the world in terms of two things that are happening right now:\n\n1.  Technology dramatically increases returns to talent.\n2.  Bureaucracy is eating the world.\n\nIn order to understand these two drivers of change, you can't just look at history because we are in the middle of these things playing out. These are effects that are live occurrences and we're in some sense at the peak of both of them. What do I mean by these?\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1a7084ab4d4dcca368c795cf30d11d62643856cfab541c6f.jpg)\n\nNumber one, technology dramatically increases returns to talent. You should be thinking in terms of centuries. You can think about globalization from the Industrial Revolution, from colonialism. It used to be the case that no matter how good you were—no matter how good you were at inventing technology, no matter how good you were at politics—you could only affect your country, or as far as your logistics and supply chains reached.\n\nAnd then it turned out that innovators in Britain, for example, could affect the rest of the world, for good or bad. This has played out gradually over time. It used to take a long time for exceptional talent to influence the world. It now takes much less time. So this graph on the left here is a chart of how long it took to get to 100 million users for different innovations. The telephone is at the bottom. That took a long time. I think it's 900 months down there. The car, the mobile phone, the internet, different social media. At the top, you see this tiny little thing that's ChatGPT. It's basically invisible how quickly it got to 100 million users. So if you can do something like that, if you can invent the telephone, if you can invent ChatGPT or whatever, the returns to being good enough to do that accumulate much faster and at much larger scales than they used to.\n\nThis is what you get with the agglomeration of talent in the Ivy Leagues and Silicon Valley. I think Silicon Valley right now is maybe the greatest agglomeration of talent in the history of humanity. That’s a bit of a self-serving claim because I live there, but I think it's just also true. And it's a weird perspective to have because something like this has never happened before. It has never been the case that you have a pool of so many people from across the world to draw on to get a bunch of talent in one place. In some sense it makes a lot of sense that this effect is driving a lot of unusual and unprecedented sociopolitical dynamics, as I'll talk about shortly.\n\nThe other graph here is an interesting one. It's the financial returns to IQ over two cohorts. The blue line is the older cohort, it's from 50 years ago or something. It's got some slope. And then the red line is a newer cohort. And that's a much steeper slope. What that means is basically for every extra IQ point you have, in the new cohort you get about twice as much money on average for that IQ point. That's a particularly interesting illustration of the broader base of technology increasing returns to talent.\n\nThen we can quibble about whether or not it's precisely technology driving that. But I think it's intuitively compelling that if you’re an engineer at Google and you can make one change that is immediately rolled out to a billion people, of course being better at your job is gonna have these huge returns. Obviously the tech itself is unprecedented, but I think one way in which we don't think about it being unprecedented is that the ability to scale talent is just absurd. (I won’t say specific anecdotes of how much people are being headhunted for, but it's kind of crazy.)\n\nEven people being in this room is an example of this technology dramatically increasing returns to talent. There's a certain ability to read ideas on the internet and then engage with them and then build a global community around them that just wouldn't have been possible even 50 years ago. It would have just been way harder to have a community that spans England and the East Coast and the West Coast and get people together in the same room. It's nuts, and now you can actually do that.\n\nI should say the best example of this, which is obviously just crazy good entrepreneurs. If you were just the best in the world at the thing you do, then now you can make $100 billion. Not reliably, but Elon started a lot of companies and he just keeps succeeding because he is the best in the world at entrepreneurship.He couldn't previously have made $200 billion from that, and now he can. And the same is true for other people in this broad space. And so I should especially highlight that the returns are extreme at the extreme outlier ends of the talent scale.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/76e3201e966bab6c4f0fcdbb83880e9326a5869296e1ac8d.jpg)\n\nSecond driver: bureaucracy is eating the world. You might say: sure there are all these returns to talent, but people used to get a lot of stuff done and now they can't. So the counterpoint to this, or the opposing force, is that there's just way more bureaucracy than there used to be. And this is true of the spending of governments across the world—this is the U.S. government spending as a proportion of GDP, and you can see these ratchet effects where it goes up and it just never goes back down again. And now it's at this crazy level of what, 50% or something? \n\nSo if you just look at the number of employees of the government \\[EDIT: this one is incorrect\\], the amount of debt that Western governments have… This graph here of pages of regulation, that's not even the total amount of regulation, that's the amount of regulation added per year. So if that's linearly increasing, then the actual amount of regulation is quadratically increasing. It's kind of nuts. The picture here is just one environmental review for one not even particularly important policy—I think it was congestion pricing—it took three years and then ultimately got vetoed anyway.\n\nLibertarians have always talked about “there's too much regulation”, but I think it's underrated how this is not a fact about history, this is a thing we are living through—that we are living through the explosion of bureaucracy eating the world. The world does not have defense mechanisms against these kinds of bureaucratic creep. Bureaucracy is optimized for minimizing how much you can blame any individual person. So there's never a point at which the bureaucracy is able to take a stand or do the sensible policy or push back even against stuff that's blatantly illegal, like a bunch of the DEI stuff at American universities. It’s just really hard to draw a line and be like “hey, we shouldn't do this illegal thing”. Within a bureaucracy, nobody does it. And you have multitudes of examples.\n\nSo these are two very powerful forces that I think of as shaping the world in a way that they didn't 20 or 40 or 60 years ago.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cd7b9068921429ae21b135beb5b465cc0b54cde8d8fa10ba.jpg)\n\nThis will get back to more central EA topics eventually, but I'm trying to flesh out some of the ways in which I apply this lens or paradigm. One of them is culture wars. The thing about culture wars is that everyone really hates the extent to which the returns to talent are skewed towards outliers, but they hate it in different ways. Liberals hate economic and identitarian elites—the fact that one person can accumulate billions of dollars or that there are certain groups that end up outperforming and getting skewed returns. On the left, it's a visceral egalitarian force that makes them hate this.\n\nOn the right, I think a lot of conservatives and a lot of populist backlash are driven by the fact that the returns to cultural elitism are really high. There's just the small group of people in most Western countries—city elites, urban elites, coastal elites, whatever you want to call them—that can shape the culture of the entire country. And that's a pretty novel phenomenon. It didn't used to be the case that newspapers were so easily spread across literally the entire country—that the entirety of Britain is run so directly from London—that you have all the centralization in these cultural urban elites.\n\nAnd one way you can think about the culture war is both sides are trying to push back on these dramatic returns to talent and both are just failing. The sort of controls that you might try and apply—the ways you might try and apply bureaucracy or apply legislation to even the playing field—just don’t really work. On both sides, people are in some sense helpless to push back on this stuff. They’re trying, they're really trying. But this is one of the key drivers of a lot of the cultural conflicts that we're seeing.\n\nThe graphs here on the left, we have the skew and political affiliation of university professors by different fields. Right at the top, I think that's like five to one, so even the least skewed are still really Democrat skewed. And then at the bottom, it's incredibly skewed to the point where there's no Republicans. This is the type of skewed returns to talent or skewed concentration of power that the right wing tends to hate.\n\nAnd then on the right, you have the proportion of startups distributed across different cities in America. They do tend to agglomerate really hard, and there's just not that much you can do about it. Just because the returns to being in the number one best place in the world for something are just so high these days.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0bfe35534928bf8dcbfa9640c342ceb2503dc66dc5d42f7c.jpg)\n\nSecond effect: I'll just talk through this very briefly, but I do think it's underrated how much the U.S. has overtaken the EU over the last decade or two decades in particular. You can see on the right, you have this crazy graph of U.S. growth being twice as high as European growth over the last 20 years. And I do think that it's basically down to regulation. I think the U.S. has a much freer regulatory environment, much more room to maneuver. In some countries in Europe, it's effectively illegal to found a startup because you have to jump through so many hoops, and the system is really not set up for that. In the U.S. at the very least, you have arbitrage between different states.\n\nThen you have more fundamental freedoms as well. This is less a question of tax rates and more just a question of how much bureaucracy you have to deal with. But then conversely: why does the U.S. not have that much manufacturing? Why is it falling behind China in a bunch of ways? It's just really hard to do a lot of stuff when your energy policy is held back by radical bureaucracy. And this is basically the case in the U.S. The U.S. could have had nuclear abundance 50, 60 years ago. The reason that it doesn't have nuclear abundance is really not a technical problem, it's a regulation problem \\[which made it\\] economically infeasible to get us onto this virtuous cycle of improving nuclear. You can look at the ALARA (as low as reasonably achievable) standard for nuclear safety: basically no matter what the price, you just need to make nuclear safer. And that totally fucks the U.S.'s ability to have cheap energy, which is the driver of a lot of economic growth.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/681e67dec1c21e6b577f194ef17230eda7baea51afb79494.jpg)\n\nOkay. That was all super high level. I think it's a good framework in general. But why is it directly relevant to people in this room? My story here is: the more powerful AI gets, the more everyone just becomes an AI safety person. We've kind of seen this already: AI has just been advancing and over time, you see people falling into the AI safety camp with metronomic predictability. It starts with the most prescient and farsighted people in the field. You have Hinton, you have Bengio and then Ilya and so on. And it's just ticking its way through the whole field of ML and then the whole U.S. government, and so on. By the time AIs have the intelligence of a median human and the agency of a median human, it's just really hard to not be an AI safety person. So then I think the problem that we're going to face is maybe half of the AI safety people are fucking it up and we don't know which half.\n\nAnd you can see these two effects that I described \\[as corresponding to two conflicting groups in AI safety\\]. Suppose you are really worried about technology increasing the returns to talent—for example, you're really worried that anyone could produce bioweapons in the future. It used to be the case that you had to be one of the best biology researchers in the world in order to actually improve bioweapons. And in the future, maybe anyone with an LLM can do it. I claim people start to think of AI risk as basically anarchy. You have this offense defense balance that's skewed towards people being able to mess up the world because AI will give a small group of people—a small group of terrorists, a small group of companies—the ability to have these massive effects that can't be controlled.\n\nSo the natural response to this frame on AI risk is you want to prevent it with central control. You want to make sure that no terrorists get bioweapon-capable AIs. You want to make sure that you have enough regulation to oversee all the companies and so on and so forth. Broadly speaking, this is the approach that resonates the most with Democrats and East Coast elites. Because they've been watching Silicon Valley, these crazy returns to talent, the fact that one engineer at Facebook can now skew the news that's seen by literally the entire world.\n\nAnd it's like: oh shit, we can't keep control of this. We have to do something about it. And that's part of why you see these increasing attempts to control stuff from the Democrats, and East Coast elites in general—Ivy League professors and the sort of people who inform Democrat policy.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7615e04d15fb43f2ac427c25b54852289cb316bafc24be52.jpg)\n\nThe other side of AI safety is that you're really worried about bureaucracy eating the world. And if you're really worried about that, then a natural way of thinking about AI risk is that AI risk is about concentration of power. You have these AIs, and if power over them gets consolidated within the hands of a few people—maybe a few government bureaucrats, maybe a few countries, maybe a few companies—then suddenly you can have all the same centralized control dynamics playing out, but much easier and much worse.\n\nCentralized control over the Facebook algorithm—sure, it can skew some things, but it's not very flexible. You can't do that much—at worst you can nudge it slightly in one direction or other, but it's pretty obvious. Centralized control of AI, what that gives you is the ability to very flexibly shape the way that maybe everyone in the entire country interacts with AI. So this is the Elon Musk angle on AI risk: AI risk is fundamentally a problem about concentration of power. Whereas a lot of AI governance people tend to think of it more like AI risk as anarchy. That's maybe why we've seen a lot of focus on bioweapons, because it's a central example of an AI risk that maps neatly onto this left-hand side.\n\nSo how do you prevent that? Well, if you're Elon or somebody who thinks similarly, you try and prevent it using decentralization. You’re like: man, we really don't want AI to be concentrated in the hands of a few people or to be concentrated in the hands of a few AIs. (I think both of these are kind of agnostic as to whether it's humans or AIs who are the misaligned agents, if you will.) And this is kind of the platform that Republicans now (and West Coast elites) are running on. It's this decentralization, freedom, AI safety via openness. Elon wants xAI to produce a maximally truth-seeking AI, really decentralizing control over information.\n\nI think they both have reasonable points—both of these sides are getting at something sensible. Unfortunately, politics is incredibly polarized right now. And so it's just really hard to bridge this divide. One of my worries is that AI safety becomes as polarized as the Democrats and the Republicans currently are. You can imagine these two camps of AI safety people almost at war with each other.\n\nI don't know how to prevent that right now. I have some ideas. There's social links you can use—people sitting down and trying to see each other's perspectives. But fundamentally, it's a really big divide and I see AI safety and maybe EA more generally as trying to have one foot on either side of this gap and being stretched more and more.\n\nSo my challenge to the people who are listening to this here or in the recording is: how do we bridge that? How do we make it so that both of these reasonable concerns of AI risk as anarchy and AI risk as concentration of power can be tackled in a way that everyone accepts as good?\n\nYou can think of the US Constitution as trying to bridge that gap. You want to prevent anarchy, but you also want to prevent concentration of power. And so you have this series of checks and balances. One of the ways we should be thinking about AI governance is as: how do we put in regulations that also have very strong checks and balances? And the bigger of a deal you think AI is going to be, the more like a constitution—the more robust—these types of checks and balances need to be. It can't just be that there's an agency that gets to veto or not veto AI deployments. It needs to be much more principled and much closer to something that both sides can trust.\n\nThat was a lot. Do I have more things to say here? Yeah, my guess is that most people in this room, or most people listening, instinctively skew towards the left side here. I instinctively, personally skew a bit more towards the right side. [I just posted a thread](https://x.com/RichardMCNgo/status/1852754804973375588) on why I think it's probably better if the Republicans win this election on Twitter. So you can check that out—I think it might be going viral during this talk—if you want to get more of an instinctive sense for why the right side is reasonable. I think it's just a really hard problem. And this type of sociopolitical thinking that I've been trying to advocate for in this talk, that's the type of thinking that you need in order to actually bridge these hard-to-bridge gaps.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0b22933e501393ed26aa112d71a23a3c4785f21e31ecf25d.jpg)\n\nI don't have great solutions, so I'm gonna say a bunch of platitude-adjacent statements. The first one is this community should be really truth-seeking, much more than most political movements or most political actors, because that works in the long term.\n\nI think it's led us to pretty good places so far and we want to keep doing it. In some sense if you're truth-seeking enough then you converge to alignment research anyway—I have [this blog post](https://www.alignmentforum.org/posts/67fNBeHrjdrZZNDDK/defining-alignment-research) on why alignment research is what happens if you're doing machine learning and you try to be incredibly principled about it, or incredibly scientific about it. So you can check out that post on LessWrong and various other places.\n\nI also have [a recent post](https://www.mindthefuture.info/p/why-im-not-a-bayesian) on why history and philosophy of science is a really useful framework for thinking about these big picture questions and what would it look like to make progress on a lot of these very difficult issues, compared with being bayesian. I’m not a fan of bayesianism—to a weird extent it feels like a lot of the mistakes that the community has made have fallen out of bad epistemology. I'm biased towards thinking this because I'm a philosopher but it does seem like if you had a better decision theory and you weren't maximizing expected utility then you might not screw FTX up quite as badly, for instance.\n\nOn the principled mechanism design thing, the way I want to frame this is: right now we think about governance of AI in terms of taking governance tools and applying them to AI. I think this is not principled enough. Instead the thing we want to be thinking about is governance *with* AI— what would it look like if you had a set of rigorous principles that governed the way in which AI was deployed throughout governments, that were able to provide checks and balances. Able to have safeguards but also able to make governments way more efficient and way better at leveraging the power of AI to, for example, provide a neutral independent opinion like when there's a political conflict.\n\nI don't know what this looks like yet, I just want to flag it as the right way to go. And I think part of the problems that people have been having in AI governance are that our options are to stop or to not stop because no existing governance tools are powerful enough to actually govern AI, it’s too crazy. And that’s true, but governing *with* AI opens up this whole new expanse of different possibilities I really want people to explore, particularly people who are thinking about this stuff in first-principles and historically informed ways.\n\nThat’s the fourth point here: EA and AI safety have been bad at that. I used the example of Dominic Cummings before as someone who is really good at that. So if you just read his stuff then maybe you get an intuition for the type of thinking that leads you to realize that there’s a layer of the Brexit debate deeper than what everyone else was looking at. What does that type of thinking lead you when you apply it to AI governance? I’ve been having some really interesting conversations in the last week in particular with people who are like: what if we think way bigger? If you really take AGI seriously then you shouldn’t just be thinking about historical changes on the scale of founding new countries, you should be thinking about historical changes on the scale of founding new *types* of countries.\n\nI had a debate with a prominent AI founder recently about who would win in a war between Mars and Earth. And that’s a weird thing to think about, but given a bunch of beliefs that I have—I think SpaceX can probably get to Mars in a decade or two, I think AI will be able to autonomously invent a bunch of new technologies that allow you to set up a space colony, I think a lot of the sociopolitical dynamics of this can be complicated… Maybe this isn’t the thing we should be thinking about, but I want a *reason *why it’s not the type of thing we should be thinking about.\n\nMaybe that’s a little too far. The more grounded version of this is: what if corporations end up with power analogous to governments, and the real conflicts are on that level. There's this concept of the network state, this distributed set of actors bound together with a common ideology. What if the conflicts that arise as AI gets developed look more like that than conflicts between traditional nation-states? This all sounds kind of weird and yet when I sit down and think “what are my beliefs about the future of technology? How does this interface with my sociopolitical and geopolitical beliefs?” weird stuff starts to fall out.\n\nLastly, synthesizing different moral frameworks. Right back at the beginning of the talk, I was like, look. We have these three waves. And we have consequentialism, we have deontology, and we have virtue ethics, as these skills that we want to develop as we pass through these three waves. A lot of this comes back to Derek Parfit. He was one of the founders of EA in a sense, and his big project was to synthesize morality—to say that all of these different moral theories are climbing the same mountain from different directions.\n\nI think we’ve gotten to the point where I can kind of see that all of these things come together. If you do consequentialism, but you’re also shaping the world in these large-scale political ways, then the thing you need to do is to be virtuous. That’s the only thing that leads to good consequences. All of these frameworks share some conception of goodness, of means-end reasoning.\n\nThere’s something in the EA community around this that I’m a little worried about. I’ve got this post on [how to have more cooperative AI safety strategies](https://www.lesswrong.com/posts/5uffoui3axcK7gQXG/towards-more-cooperative-ai-safety-strategies), which kind of gets into this. But I think a lot of it comes down to just having a rich conception of what it means to do good in this world. Can we not just “do good” in the sense of finding a target and running as hard as we can toward it, but instead think about ourselves as being on a team in some sense with the rest of humanity—who will be increasingly grappling with a lot of the issues I’ve laid out here?\n\nWhat is the role of our community in helping the rest of humanity to grapple with this? I almost think of us as first responders. First responders are really important — but also, if they try to do the whole job themselves, they’re gonna totally mess it up. And I do feel the moral weight of a lot of the examples I laid out earlier—of what it looks like to really mess this up. There’s a lot of potential here. The ideas in this community—the ability to mobilize talent, the ability to get to the heart of things—it’s incredible. I love it. And I have this sense—not of obligation, exactly—but just…yeah, this is serious stuff. I think we can do it. I want us to take that seriously. I want us to make the future go much better. So I’m really excited about that. Thank you.",
      "plaintextDescription": "At EA Global Boston last year I gave a talk on how we're in the third wave of EA/AI safety, and how we should approach that. This post contains a (lightly-edited) transcript and slides. Note that in the talk I mention a set of bounties I'll put up for answers to various questions—I still plan to do that, but am taking some more time beforehand to get the questions right.\n\n\n\n\nTranscript and slides\nHey everyone. Thanks for coming. You're probably looking at this slide and thinking: \"how to change a changing world? That's a weird title for a talk. Probably it's a kind of weird talk.\" And the answer is, yes, it is a kind of weird talk.\n\nSo what's up with this talk? Firstly, I am trying to do something a little unusual here. It's quite abstract. It's quite disagreeable as a talk. I'm trying to poke at the things that maybe we haven't been thinking enough about as a movement or as a community. I'm trying to do a \"steering the ship\" type thing for EA.\n\nI don't want to make this too inside baseball or anything. I think it's going to be useful even if people don't have that much EA background. But this seems like the right place to step back and take stock and think: what's happened so far? What's gone well? What's gone wrong? What should we be doing?\n\nSo the structure of the talk is:\n\n 1. Three waves of EA and AI safety. (I mix these together because I think they are pretty intertwined, at least intellectually. I know a lot of people here work in other cause areas. Hopefully this will still be insightful for you guys.)\n 2. Sociopolitical thinking and AI. What are the really big frames that we need as a community to apply in order to understand the way that the world is going to go and how to actually impact it over the next couple of decades?\n\nThere's a thing that sometimes happens with talks like this—maybe status regulation is the word. In order to give a really abstract big picture talk, you need to be super credible because otherwise it's very aspirational. I'm just goi",
      "wordCount": 7735
    },
    "tags": [
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5tYTKX4pNpiG4vzYg",
    "title": "Towards a scale-free theory of intelligent agency",
    "slug": "towards-a-scale-free-theory-of-intelligent-agency",
    "url": "https://www.mindthefuture.info/p/towards-a-scale-free-theory-of-intelligent",
    "baseScore": 96,
    "voteCount": 42,
    "viewCount": null,
    "commentCount": 45,
    "createdAt": null,
    "postedAt": "2025-03-21T01:39:42.251Z",
    "contents": {
      "markdown": "I recently left OpenAI to pursue independent research. I’m working on a number of different research directions, but the most fundamental is my pursuit of a scale-free theory of intelligent agency. In this post I give a rough sketch of how I’m thinking about that. I’m erring on the side of sharing half-formed ideas, so there may well be parts that don’t make sense yet. Nevertheless, I think this broad research direction is very promising.\n\nThis post has two sections. The first describes what I mean by a theory of intelligent agency, and some problems with existing (non-scale-free) attempts. The second outlines my current path towards formulating a scale-free theory of intelligent agency, which I’m calling *coalitional agency*.\n\nTheories of intelligent agency\n==============================\n\nBy a “theory of intelligent agency” I mean a unified mathematical framework that describes both *understanding the world* and *influencing the world*. In this section I’ll outline the two best candidate theories of intelligent agency that we currently have (expected utility maximization and active inference), explain why neither of them is fully satisfactory, and outline how we might do better.\n\nExpected utility maximization\n-----------------------------\n\nExpected utility maximization is the received view of intelligent agency in many fields (I’ll abbreviate it as EUM, and EUM agents as EUMs). Idealized EUMs have beliefs in the form of probability distributions, and goals in the form of utility functions, as specified by the axioms of probability theory and utility theory. They choose whichever strategy leads to the most utility in expectation; this is typically modelled as a process of search or planning.\n\nEUM is a very productive framework in simple settings—like game theory, bargaining theory, microeconomics, etc. It’s particularly useful for describing agents making one-off decisions between a fixed set of choices. However, it’s much more difficult to use EUM to model agents making sequences of choices over time, especially when they learn and update their concepts throughout that process. The two points I want to highlight here:\n\n1.  EUM treats goals and beliefs as totally separate. But in practice, agents represent both of these in terms of the same underlying concepts. When those concepts change, both beliefs and goals change. The best way to learn reusable concepts is via *deep learning*, where simple concepts in lower layers of the network are built up into more complex concepts in higher layers.\n2.  Agents often act based on heuristics learned via trial and error. Even if they’ll eventually acquire beliefs about how those heuristics help them achieve their goals, they often start off without any clear idea of why those heuristics work so well. But this makes them hard to model as idealized EUMs which choose actions based on their beliefs and goals. Instead, the process of learning heuristics is better understood as (model-free) reinforcement learning.\n\nSo we might hope that a theory of deep learning, or reinforcement learning, or deep reinforcement learning, will help fill in EUM’s blind spots. Unfortunately, theoretical progress has been slow on all of these—they’re just too broad to say meaningful things about in the general case.\n\nActive inference\n----------------\n\nFortunately, there’s another promising theory which comes at it from a totally different angle. [Active inference](https://mitpress.mit.edu/9780262553995/active-inference/) is a theory born out of neuroscience. Where EUM starts by assuming an agent already has beliefs and goals, active inference gives us a theory of how beliefs and goals are built up over time.\n\nThe core idea underlying active inference is *predictive coding*. Predictive coding models our brains as hierarchical networks where the lowest level is trying to predict our sensory inputs, the next-lowest level is trying to predict the lowest level, and so on. The higher up the hierarchy you go, the more abstract and compressed the representations become. The lower levels might represent individual “pixels” seen by our retinas, then higher levels lines and shapes, then higher levels physical objects like dogs and cats, then even higher levels abstract concepts like animals and life.\n\nThis is, of course, similar to [how artificial neural networks work](https://distill.pub/2020/circuits/zoom-in/) (especially ones trained by self-supervised learning). The key difference: predictive coding tells us that, in the brain, the patterns recognized at each level are determined by reconciling the bottom-up signals and the top-down predictions. For example, after looking at the image below, you might not perceive any meaningful shapes within it. But if you have a strong enough top-down prediction that the image makes sense (e.g. because I’m telling you it does) then that prediction will keep being sent down to lower layers responsible for identifying shapes, until they discover the dog. This explains the sharp shifts in our perceptions when looking at such images: at first we can’t see the dog at all, but when we find it it jumps into focus, and afterwards we can’t unsee it.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcnsKQTrfy34unrP64zaadpdWSQMi2A-4Wq6nEeOC3KVMcr7ND80gtl6lkJVRpC54p4LERQtnaYHHW_4hBttb3wNqi7-1Dvuqi487QNmwU4I6Ak76PoXEGFMbr7BbtZzfiHcORdHA?key=AI4syy9fysKk5DVbeHLd7wE0)\n\nPredictive coding is a very elegant theory. And what’s even more elegant is that it explains *actions* in the same way—as very strong top-down predictions which override the default states of our motor neurons. Specifically, we can resolve conflicts between beliefs and observations either by updating our beliefs, or by taking actions which make the beliefs come true. Active inference is an extension of predictive coding in which some beliefs are so rigid that, when they conflict with observations, it’s easier to act to change future observations than it is to update those beliefs. We can call these hard-to-change beliefs “goals”, thereby unifying beliefs and goals in a way that EUM doesn’t.\n\nThis is a powerful and subtle point, and one which is often misunderstood. I think the best way to fully understand this point is in terms of perceptual control theory. [Scott Alexander gives a good overview here](https://slatestarcodex.com/2019/03/20/translating-predictive-coding-into-perceptual-control/); I’ll also explain the connection at more length in a follow-up post.\n\nTowards a scale-free unification\n--------------------------------\n\nActive inference is a beautiful theory—not least because it includes EUM as a special case. Active inference represents goals as probability distributions over possible outcomes. If we interpret the logarithm of each probability as that outcome’s utility (and set aside the value of information) then active inference agents [choose actions which maximize expected utility](https://x.com/RichardMCNgo/status/1893556269644161215). (One intuition for why such an interpretation is natural comes from Scott Garrabrant's [geometric rationality](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg/p/DMxe4XKXnjyMEAAGw).)\n\nSo what does expected utility maximization have to add to active inference? I think that what active inference is missing is the ability to model *strategic interactions* between different goals. That is: we know how to talk about EUMs playing games against each other, bargaining against each other, etc. But, based on my (admittedly incomplete) understanding of active inference, we don’t yet know how to talk about goals doing so *within* a single active inference agent.\n\nWhy does that matter? One reason: the biggest obstacle to a goal being achieved is often other conflicting goals. So any goal capable of learning from experience will naturally develop strategies for avoiding or winning conflicts with other goals—which, indeed, seems to happen [in human minds](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/5gfqG3Xcopscta3st).\n\nMore generally, any theory of intelligent agency needs to model internal conflict in order to be scale-free. By a scale-free theory I mean one which applies at many different levels of abstraction, remaining true even when you “zoom in” or “zoom out”. I see so many similarities in how intelligent agency works at different scales (on the level of human subagents, human individuals, companies, countries, civilizations, etc) that I strongly expect our eventual theory of it to be scale-free.\n\nBut active inference agents are cooperative *within *themselves while having strategic interactions with other agents; this privileges one level of analysis over all the others. Instead, I propose, we should think of active inference agents as being composed of subagents who themselves compete and cooperate in game-theoretic ways. I call this approach *coalitional agency; *in the next section I characterize my current understanding of it from two different directions.\n\nTwo paths towards a theory of coalitional agency\n================================================\n\nThe core idea of coalitional agency is that we should think of agents as being composed of cooperating and competing subagents; and those subagents as being composed of subsubagents in turn; and so on. The broad idea here is not new—indeed, it’s the core premise of Minsky’s *Society of Mind*, published back in 1986. But I hope that thinking of coalitional agency as incorporating elements of both EUM and active inference will allow progress towards a formal version of the theory.\n\nIn this section I’ll give two different characterizations of coalitional agency: one starting from EUM and trying to make it more coalitional, and the other starting from active inference and trying to make it more agentic. More specifically, the first poses the question: if a group of EUMs formed a coalition, what would it look like? The second poses the question: how could active inference agents be more robust to conflict between their internal subagents?\n\nFrom EUM to coalitional agency\n------------------------------\n\nIf a group of EUMs formed a coalition, what would it look like? EUM has a standard answer to this: the coalition would be a linearly-aggregated EUM. In this section I first explain why the standard answer is unsatisfactory. I then give an alternative answer: the coalition should be an incentive-compatible decision procedure*.*\n\n### Aggregating into EUMs is very inflexible\n\nIn the EUM framework, any non-EUM agent is incoherent in the sense of violating the underlying axioms of probability theory and/or utility theory. So insofar as EUM has predictive power, it predicts that competent coalitions will also be EUMs. But which EUMs? The standard answer is given by [Harsanyi’s utilitarian theorem](https://forum.effectivealtruism.org/posts/v89xwH3ouymNmc8hi/harsanyi-s-simple-proof-of-utilitarianism), which shows that (under reasonable-seeming assumptions) an aggregation of EUMs into a larger-scale EUM must have a utility function that’s a weighted average of the subagents’ utilities.\n\nHowever, this strongly limits the space of possible aggregated agents. Imagine two EUMs, Alice and Bob, whose utilities are each linear in how much cake they have. Suppose they’re trying to form a new EUM whose utility function is a weighted average of their utility functions. Then they’d only have three options:\n\n1.  Form an EUM which would give Alice all the cakes (because it weights Alice’s utility higher than Bob’s)\n2.  Form an EUM which would give Bob all the cakes (because it weights Bob’s utility higher than Alice’s)\n3.  Form an EUM which is totally indifferent about the cake allocation between them (which would allocate cakes arbitrarily, and could be swayed by the tiniest incentive to give all Alice’s cakes to Bob, or vice versa)\n\nThese are all very unsatisfactory. Bob wouldn’t want #1, Alice wouldn’t want #2, and #3 is extremely non-robust. Alice and Bob could toss a coin to decide between options #1 and #2, but then they wouldn’t be acting as an EUM (since EUMs can’t prefer a probabilistic mixture of two options to either option individually). And even if they do, whoever loses the coin toss will have a strong incentive to renege on the deal.\n\nWe could see these issues merely as the type of frictions that plague any idealized theory. But we could also seem them as hints about what EUM is getting wrong on a more fundamental level. Intuitively speaking, the problem here is that there’s no mechanism for separately respecting the interests of Alice and Bob after they’ve aggregated into a single agent. For example, they might want the EUM they form to value fairness between their two original sets of interests. But adding this new value is not possible if they’re limited to (a probability distribution over) weighted averages of their utilities. This makes aggregation very risky when Alice and Bob can’t consider all possibilities in advance (i.e. in all realistic settings).\n\nBased on similar reasoning, Scott Garrabrant [rejects the independence axiom](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg/p/Xht9swezkGZLAxBrd). He argues that the axiom is unjustified because rational agents should be able to lock in values like fairness based on prior agreements (or even *hypothetical* agreements).\n\n### Coalitional agents are incentive-compatible decision procedures\n\nThe space of decision procedures is very broad; can we say more about which decision procedures rational agents should commit to? One key desideratum for commitments is that it’s easy to trust that they’ll be kept. Consider the example above of flipping a coin to decide between options 1 and 2 above. This is fair, but it sets up strong incentives for whoever loses the coinflip to break their commitment, since they will not get any benefit from keeping it.\n\nAnd it’s even worse than that, because in general the only way to find out another agent’s utilities is to ask them, *and they could just lie*. From the god’s-eye perspective you can build an EUM which averages subagents’ utilities; from the perspective of the agents themselves, you can’t. In other words, EUMs constructed by taking a weighted average of subagents’ utilities are not [*incentive-compatible*](https://en.wikipedia.org/wiki/Incentive_compatibility).\n\nEUMs which can't guarantee each other's honesty will therefore want to aggregate into incentive-compatible decision procedures which each agent does best by following. Perhaps the best-known incentive-compatible decision procedure is the *fair cake-cutting algorithm*, also known as “I cut you choose”. This is a much simpler and more elegant way to split cakes than the example I gave above of Alice and Bob aggregating into a single EUM.\n\nNow, cake-cutting is one very specific type of problem, and we shouldn’t expect there to be incentive-compatible decision procedures with such nice properties for all problems. Nevertheless, there’s a very wide range of possibilities to explore. Some of the simplest possible incentive-compatible decision procedures include:\n\n1.  Each subagent gets one type of decision that they’re solely responsible for.\n2.  We randomize between different subagents for different decisions.\n3.  Subagents vote between two options.\n4.  One subagent chooses which other subagent to delegate to.\n5.  Different mechanisms are used for different types of decisions.\n\nThese decision procedures each give subagents some type of control over the outputs—and, importantly, a type of control that generalizes to a range of problems beyond the ones they were able to consider during bargaining.\n\n### Which incentive-compatible decision procedure?\n\nThe question is then: how should subagents choose *which* incentive-compatible bargaining procedure to adopt? The most principled answer is that they should use a *bargaining theory *framework. This is a little different from the traditional theoretical framework for bargaining. Bargaining doesn’t typically produce ways of organizing the bargainers—instead it produces an object-level answer to whatever problem the bargainers face.\n\nThis makes sense when you have a single decision to make. But when bargainers face many possible future decisions, bargaining over outcomes requires specifying which outcome to choose in every possible situation. This is deeply intractable in realistic settings, where bargainers can’t predict every possible scenario they might face.\n\nIn those settings it is much more tractable to bargain over *methods *of making decisions which generalize beyond the problems that the bargainers are currently aware of. I don’t know of much work on this, but the same idealized bargaining solutions (e.g. the Nash bargaining solution) should still apply in principle. The big question is whether there’s anything interesting to be said about the relationship between incentive-compatible decision procedures and bargaining solutions. For example, are there classes of incentive-compatible decision procedures which make it especially easy for agents to identify which one is near the optimal bargaining solution? On a more theoretical level, one tantalizing hint is that the [ROSE bargaining solution](https://www.lesswrong.com/posts/vJ7ggyjuP4u2yHNcP/threat-resistant-bargaining-megapost-introducing-the-rose) is also constructed by abandoning the axiom of independence—just as Garrabrant does in his rejection of EUM above. This connection seems worth exploring further.\n\nTo finish, I’ve summarized many of the claims from this section in the following table:\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\"><strong>Form of aggregated agent</strong></td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\"><strong>Advantages</strong></td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\"><strong>Disadvantages</strong></td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Full policy</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Pareto optimal</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Doesn’t generalize to new problems</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">EUM</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Obeys EUM axioms</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Inflexible, non-constructive</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Decision procedure</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Flexible, constructive</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Not stable under strategic behavior, hard to reason about</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Incentive-compatible decision procedure</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Flexible, constructive, stable under strategic behavior</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Hard to design or reason about</td></tr></tbody></table>\n\nWhat do I mean by “hard to reason about?” One nice thing about EUMs is that their behavior is extremely easy to summarize: they do whatever’s best for their goals according to their beliefs. But we can’t talk about decision procedures in the same way. Individual subagents may have goals and beliefs, but the decision procedure itself doesn’t: it just processes those subagents into a final decision.\n\nFortunately, there’s a way to rescue our intuitive idea that agents should have beliefs and goals. It’ll involve talking about much more complex incentive-compatible decision procedures, though. So first I’ll turn to the other direction in which we can try to derive coalitional agency: starting from active inference.\n\nFrom active inference to coalitional agency\n-------------------------------------------\n\nI just gave an account of coalitional agents in which they’re built up from individual EUMs. In this section I’ll do the opposite: start from an active inference agent and modify it until it looks more like a coalitional agent.\n\nMore specifically, consider a hierarchical generative model containing beliefs/goals, where higher layers predict lower layers, and lower layers send prediction errors up to higher layers. Let’s define a subagent as a roughly-internally-consistent cluster of beliefs and goals within that larger agent. Note that this definition is a matter of degree: if we apply a high bar for internal consistency, then each subagent will be small (e.g. beliefs and desires about a single object) whereas a lower bar will lead to larger subagents (e.g. a whole ideology).\n\nSubagents with different beliefs and goals will tend to make different predictions (including “predictions” about which actions they want the agent to take). What modifications do we need to make to our original setup for it to be robust to strategic dynamics between those subagents?\n\n### Predicting observations via prediction markets\n\nWhen multiple subagents make conflicting predictions, the standard approach is to combine them by taking a [precision-weighted average](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00457/full). Credit is then assigned to each subagent for the prediction in proportion to how confident it was. But this is not incentive-compatible: subagents can benefit by strategizing about how the other subagents will respond, and changing their responses accordingly.\n\nThere are various incentive-compatible ways to elicit predictions from multiple agents (many of which are discussed by [Neyman](https://arxiv.org/pdf/2403.07949)). However, the most elegant incentive-compatible method for aggregating predictions is a *prediction market*. Each trader on a prediction market can choose to buy shares in propositions it thinks are overpriced and sell shares in propositions it thinks are underpriced. This allows subagents to specialize into different niches within the overall agent. It also incentivizes them to arbitrage away any logical inconsistency they notice. These dynamics are modeled by the [Garrabrant induction framework](https://intelligence.org/files/LogicalInduction.pdf).\n\n### Choosing actions via auctions\n\nGiven my discussion above about actions being in some sense predictions of future behavior, we might think that actions should be chosen by prediction markets too. However, there’s a key asymmetry: if I *expect* a complex plan to happen, I can profit by predicting *any* aspect of it. But if I *want* a complex plan to happen, I need to successfully coordinate *every* aspect of it. So, unlike predictions of observations, predictions of actions need to have some mechanism for giving a single plan control over many different actuators.\n\nIn active inference, the mechanism by which this occurs is called expected free energy minimization. I’m honestly pretty confused about how expected free energy minimization works, but I strongly suspect that it’s not incentive-compatible. In particular, the discontinuity involved in picking the single highest-value plan seems like it’d induce incentives to overestimate your own plan’s value. However, [Demski et al.](https://www.andrew.cmu.edu/user/coesterh/RIA.pdf)’s BRIA framework solves this problem by requiring subagents to bid for the right to implement a plan and receive the corresponding reward. Rational subagents will never bid more than the reward they actually expect. So my hunch is that something like this auction system would be the best way to adjust our original setup to make it incentive-compatible.\n\n### Aggregating values via voting\n\nThe last important component of decision-making is evaluating plans (whether in advance or in hindsight). What happens when different subagents disagree on which goals or values the plans should be evaluated in terms of? Again, the standard approach is to take a precision-weighted average of their evaluations, but this still has all the same incentive-compatibility issues. And unlike predictions, values have no ground truth feedback signal, meaning that prediction markets don’t help.\n\nSo I expect that the most appropriate way to aggregate goals/values is via a voting system. This is also the conclusion reached by [Newberry and Ord](https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf), who model idealized moral decision-making in terms of a parliament in which subagents vote on what values to pursue. Specifically, they propose using [random ballot voting](https://en.wikipedia.org/wiki/Random_ballot), in which each voter’s favorite option is selected with probability proportional to their vote share. This voting algorithm has three particularly notable features:\n\n1.  As a nondeterministic voting system, it dodges [Arrow’s impossibility theorem](https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem).\n2.  It leads to coalitional bargaining between subagents. Pairs of subagents with different preferences are incentivized to switch both their votes to a compromise option. Iterating this process many times produces hierarchical coalitions. (The downside: when bargaining is allowed, random ballot voting isn’t threat-resistant, since agents are incentivised to start with extreme positions in order to receive more concessions.)\n3.  It outputs a probability distribution over outcomes. We therefore have a very natural way to use votes to evaluate plans: we can measure the divergence between the outcome of the vote and the (expected or actual) outcome of the plan.\n\nPutting it all together\n-----------------------\n\nI’ve described two paths towards a theory of coalitional agency. On one path, we start from expected utility maximizers and aggregate them to form coalitional agents, via those EUMs bargaining about which decision procedures to use. The problem is that the resulting decision procedure may be incoherent in the sense that it can’t be ascribed beliefs or goals. On the other path, we make interactions between active inference subagents more incentive-compatible by using prediction markets, auctions, and voting (or similar mechanisms) to manage internal conflict.\n\nWhat I’ll call the ***coalitional agency hypothesis*** is the idea that these two paths naturally “meet in the middle”—specifically, that EUMs doing (idealized) bargaining about which decision procedure to use would in many cases converge to something like my modified active inference procedure. If true, we’d then be able to talk about that procedure’s “beliefs” (the prices of its prediction market) and “goals” (the output of its voting procedure).\n\nOne line of work which supports the coalitional agency hypothesis is Critch’s [*negotiable reinforcement learning*](https://arxiv.org/abs/1701.01302) framework, under which EUMs should bet their influence on any disagreements about the future they have with other agents, so that they end up very powerful if (and only if) their predictions are right. I interpret this result as evidence that (some version of) prediction markets are the default outcome of bargaining over incentive-compatible decision procedures.\n\nBut all of this work is still vague and tentative. I’d very much like to develop a more rigorous formulation of coalitional agency. This would benefit greatly from working with collaborators (especially those with strong mathematical skills). So I’ll finish with two calls to action. If you’re a junior(ish) researcher and you want to work with me on any of this, apply to [my MATS fellowship](https://www.matsprogram.org/apply). If you’re an experienced researcher and you’d like to chat or otherwise get involved (potentially by joining a workshop series I’ll be running on this) please send me a message directly.\n\n*Thanks to davidad, Jan Kulveit, Emmett Shear, Ivan Vendrov, Scott Garrabrant, Abram Demski, Martin Soto, Laura Deming, Aaron Tucker, Adria Garriga, Oliver Richardson, Madeleine Song and others for helping me formulate these ideas*.",
      "plaintextDescription": "I recently left OpenAI to pursue independent research. I’m working on a number of different research directions, but the most fundamental is my pursuit of a scale-free theory of intelligent agency. In this post I give a rough sketch of how I’m thinking about that. I’m erring on the side of sharing half-formed ideas, so there may well be parts that don’t make sense yet. Nevertheless, I think this broad research direction is very promising.\n\nThis post has two sections. The first describes what I mean by a theory of intelligent agency, and some problems with existing (non-scale-free) attempts. The second outlines my current path towards formulating a scale-free theory of intelligent agency, which I’m calling coalitional agency. \n\n\nTheories of intelligent agency\nBy a “theory of intelligent agency” I mean a unified mathematical framework that describes both understanding the world and influencing the world. In this section I’ll outline the two best candidate theories of intelligent agency that we currently have (expected utility maximization and active inference), explain why neither of them is fully satisfactory, and outline how we might do better.\n\n\nExpected utility maximization\nExpected utility maximization is the received view of intelligent agency in many fields (I’ll abbreviate it as EUM, and EUM agents as EUMs). Idealized EUMs have beliefs in the form of probability distributions, and goals in the form of utility functions, as specified by the axioms of probability theory and utility theory. They choose whichever strategy leads to the most utility in expectation; this is typically modelled as a process of search or planning.\n\nEUM is a very productive framework in simple settings—like game theory, bargaining theory, microeconomics, etc. It’s particularly useful for describing agents making one-off decisions between a fixed set of choices. However, it’s much more difficult to use EUM to model agents making sequences of choices over time, especially when they learn a",
      "wordCount": 3915
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "zqffB6gokoivwwn7X",
    "title": "Elite Coordination via the Consensus of Power",
    "slug": "elite-coordination-via-the-consensus-of-power",
    "url": "https://www.mindthefuture.info/p/elite-coordination-via-the-consensus",
    "baseScore": 92,
    "voteCount": 47,
    "viewCount": null,
    "commentCount": 15,
    "createdAt": null,
    "postedAt": "2025-03-19T06:56:44.825Z",
    "contents": {
      "markdown": "This post is about how implicit coordination between powerful people allows them to act in surprisingly synchronized ways. I’ll start by discussing wokeness, the most prominent recent example. I’ll then analyze the mechanism behind such coordination (which I call the *consensus of power*); discuss how to oppose it; and outline some possibilities for what healthier power structures would look like.\n\nIf you're more sympathetic to wokeness, or want to focus on the more timeless aspects of the post, you might want to skip the first section. I included it because it felt dishonest to write a post *implicitly* about wokeness when it was the central example in my mind.\n\nThe mystery of the cathedral\n----------------------------\n\nThe politics of the last decade have been very weird. The Great Awokening gave rise to an extreme variant of progressive ideology that quickly gained a huge amount of traction amongst American elites, and from there embedded itself into institutions across America and the wider western world.\n\nSudden ideological change is nothing new—[my last post](https://www.mindthefuture.info/p/power-lies-trembling) was about political preference cascades, and how they’re a natural result of social behavior. But what’s fascinatingly novel about the Great Awokening is the extent to which it was an almost entirely *leaderless* movement. There was no modern MLK leading this new charge for a racial reckoning; nor a Harvey Milk of trans rights; nor even Butlers or Steins writing incisive commentaries. The closest we had was Obama, who in hindsight was a milquetoast leader whose cult of personality was driven in large part by progressive longing for a black president.\n\nOne of Curtis Yarvin’s main intellectual contributions has been to give an account of wokeness (and [its historical antecedents](https://www.unqualified-reservations.org/2007/06/ultracalvinist-hypothesis-in/)) which highlights this peculiarity, specifically via his concept of [*the cathedral*](https://graymirror.substack.com/p/a-brief-explanation-of-the-cathedral):\n\n> “The cathedral” is just a short way to say “journalism plus academia”—in other words, the intellectual institutions at the center of modern society, just as the Church was the intellectual institution at the center of medieval society.  \n>   \n> But the label is making a point. The Catholic Church is one institution—the cathedral is many institutions. Yet the label is singular. This transformation from many to one—literally, e pluribus unum—is the heart of the mystery at the heart of the modern world.\n> \n> The mystery of the cathedral is that all the modern world’s legitimate and prestigious intellectual institutions, even though they have no central organizational connection, behave in many ways as if they were a single organizational structure.\n\nThat is: different parts of the cathedral talk about the same issues, support the same policies, share the same taboos, and so on. Why? When we see this level of synchrony without visible leaders, it’s tempting to take a conspiratorial stance: there *are* leaders, they’re just acting surreptitiously. And undoubtedly there have been many *small* conspiracies within woke organizations, some of which grew to involve high-level figures (e.g. [the FAA hiring scandal](https://www.tracingwoodgrains.com/p/the-faas-hiring-scandal-a-quick-overview), or [the conspiracy to suppress the lab leak hypothesis](https://www.nytimes.com/2025/03/16/opinion/covid-pandemic-lab-leak.html), or [conspiracies within universities](https://www.justice.gov/archives/opa/pr/justice-department-finds-yale-illegally-discriminates-against-asians-and-whites-undergraduate) to [discriminate against Asians](https://en.wikipedia.org/wiki/Students_for_Fair_Admissions_v._Harvard)).\n\nBut wokeness is so visibly a bottom-up phenomenon (with high-level figures being followers rather than leaders) that it’s difficult to postulate that its overall priorities are set by any large-scale conspiracy. So Yarvin doesn’t. Instead, [he explains](https://graymirror.substack.com/p/a-brief-explanation-of-the-cathedral) the mystery of the cathedral in [memetic](https://en.wikipedia.org/wiki/Memetics) terms:\n\n> \\[In a progressive liberal democracy\\], there is a market for dominant ideas. A dominant idea is an idea that validates the use of power … And there is no market for recessive ideas. A recessive idea is an idea that invalidates power or its use.\n> \n> …It is not hard to see why, in the lecture halls and newsrooms, dominant ideas tend to outcompete recessive ideas. A dominant idea is an idea that tends to benefit you and your friends. A dominant idea will be especially popular with your friends and former students in the civil service, because it gives them more work and more power.\n\nNow, competition between ideas is undoubtedly a big part of the story of wokeness. The introduction of algorithmic newsfeeds in the early 2010s produced hotbeds of rapid cultural evolution, which quickly led to [the widespread transmission of many woke concepts](https://x.com/arctotherium42/status/1880997332092891586). Even if you don’t like Yarvin’s specific memetic explanation (which I don’t, since many aspects of wokeness are recessive ideas), [there](https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/) [are](https://ncofnas.com/p/a-guide-for-the-hereditarian-revolution) [plenty](https://www.astralcodexten.com/p/the-psychopolitics-of-trauma) [more](https://www.postliberalorder.com/p/against-the-politics-of-envy) [to](https://web.archive.org/web/20240403094949/https://www.residentcontrarian.com/p/an-article-about-a-book-about-pornography?r=75epr) [choose](https://www.city-journal.org/article/nietzsche-can-help-us-understand-wokeness) [from](https://www.amazon.com/Parasitic-Mind-Infectious-Killing-Common/dp/162157959X). Indeed, the common description of wokeness as a “mind virus” closely parallels Dawkins’ description of religions as “[viruses of the mind](https://en.wikipedia.org/wiki/Viruses_of_the_Mind)”.\n\nBut there’s a crucial difference between explaining how an idea spreads and explaining how people who believe in that idea coordinate. [Memetics generally focuses on the former](https://x.com/RichardMCNgo/status/1895173615865471032); the mystery of the cathedral is about the latter. When woke institutions behave “as if they were a single organizational structure”, it often involves each of them reacting to each other’s actions (e.g. media outlets rapidly flipping from “it’s racist to worry about covid” to “masks and lockdowns are crucial”). That level of synchrony is hard to explain in terms of which ideas are memetically fitter.\n\nSo to solve the mystery of the cathedral we should appeal neither to conspiracies nor competitions between ideas. Instead, I think, we need a third type of explanation: an “emergent conspiracy” of people independently acting in ways that they expect others to support. I call this phenomenon the *consensus of power* (a term I originally got from [Ben Landau-Taylor](https://x.com/benlandautaylor), which I’ll define more precisely in the next section).[^fk6fquq49cb]\n\nThe cathedral is a prominent example of a consensus of power. But my main focus in this post isn't to analyze the cathedral specifically, but rather the game-theoretic dynamics underlying how consenses of power work in general. Daniel Ellsberg describes some of these dynamics in [his account of US decision-making about the Vietnam War](https://www.amazon.com/Secrets-Memoir-Vietnam-Pentagon-Papers/dp/0142003425). So does Naunihal Singh in his account of coups, and Timur Kuran in his account of preference falsification, both of which I discussed at length in [my last post](https://www.mindthefuture.info/p/power-lies-trembling). You can think of this post as extending those ideas from the simple case of “will the coup succeed?” to a setting where people are deciding their positions on many related issues based on their expectations about other people’s expectations and behavior. (Unfortunately, this change makes discrete formalisms like [threshold models](https://ericneyman.wordpress.com/2021/06/05/social-behavior-curves-equilibria-and-radicalism/) and [Keynesian beauty contests](https://en.wikipedia.org/wiki/Keynesian_beauty_contest) much less applicable. I’m keen to better understand how to formally model consenses of power; for now, though, I’ll focus on their qualitative dynamics.)\n\nThe consensus of power\n----------------------\n\nWhenever we interact with others in society, we need to figure out which social and political norms to use. (Here I’m using the term “norms” broadly to include which factions to favor in which ways, which ideas are within the Overton window, etc.) A crucial feature of norms is that they don’t just guide your own behavior, but also which behaviors you reward or punish when done by others. So you benefit from adopting (and enforcing) the same norms as other people, especially powerful people. This means that it’s advantageous to track not just which norms other people follow, but which norms people believe other people will follow, and which norms people believe that people believe that other people will follow, and which…\n\nThis suggests a definition for the consensus of power: the norms which are common knowledge amongst the ruling elite. Of course there’s no clear demarcation for who qualifies as a ruling elite; what I’m trying to gesture towards (but can’t yet define precisely) is a version of common knowledge which weights more powerful people more highly. Note that this is very different from just aggregating powerful people’s preferences, because norms can persist even when they’re extremely unpopular. Often when someone goes against the consensus of power, you don’t personally care. But you know that others will punish you if you’re associated with it (because others will punish *them* if they don’t punish you, because…). So you push back against it—and thus help uphold the consensus of power. I’ll call people who consistently uphold the consensus of power “apparatchiks”.\n\nThe consensus of power, like the conspiratorial stance, attempts to explain the surprising level of coordination often seen within ruling elites. But direct coordination via conspiracy and indirect coordination via consensus are very different mechanisms that make importantly different predictions. I’ll elucidate the differences by framing each of them as a type of superagent containing many individual people.\n\nConspiracies are *centralized agents* who coordinate in a top-down way, by deferring to the decisions of leaders. This allows them to be highly goal-directed, and to plan and execute complex strategies. But it also bottlenecks them both on trust in those leaders, and on how well those leaders can gather and process information. This leaves them vulnerable to direct attack: disrupt or undermine their leaders and you can take out the entire conspiracy.\n\nBy contrast, consenses are *distributed agents*. They need not have any leaders, or even any central nodes. Instead, each apparatchik observes the behavior of many other apparatchiks, and gradually adjusts their own behavior accordingly. This allows consenses of power to perceive and act on a lot of information in parallel.\n\nIt also makes attacking them difficult, because there’s no clear locus of power. One good example comes from the ideological conformity of the mainstream media over the last few decades. If there’d been a news czar who’d banned coverage of Biden’s cognitive decline, or Pakistani pedophile gangs, or opposition to the BLM riots, or pushback against child transitions, or immigrant crime statistics, or racial IQ gaps, then that person could be criticised directly. But instead there were just thousands of individual decisions made by apparatchik news editors trying to judge which stories the consensus of power wanted them to run, resulting in a news environment almost as homogeneous as if there *had* been a left-wing news czar.\n\nSo consenses of power can be very influential and robust. But they’re also myopic and inflexible. Apparatchiks will only throw their weight behind positions that it’s common knowledge that the consensus of power favors. But without leaders willing to take a risk to rally people around a position, common knowledge is hard to achieve. So consenses of power are very limited in their ability to “think outside the box” or course-correct when pursuing their goals. The latter typically requires big flashy failures, which allow apparatchiks to then coordinate a push for a “vibe shift”.\n\nA prominent example of these dynamics was the process by which the Democrats selected first Biden then Harris as their 2024 nominee. Biden’s decision to run again was very unwise but hard for leading Democrats to oppose, since nobody wanted to stick their neck out by taking a stand against such a powerful figure. There was also no Schelling point to coordinate on pushing back on it—until Biden’s poor debate performance broke the taboo, leading to a huge surge of criticism.\n\nThe shift was so sudden that many hypothesized that it was centrally coordinated, but that hypothesis isn’t necessary. Instead, we can picture every Democrat leader individually wanting to get rid of Biden, then jumping on the bandwagon as soon as it became socially permissible to do so. Biden tried hard to cling onto the nomination, but once roused a consensus of power is very difficult to oppose. Yet even then the difference between conspiracy and consensus was clear. Any conspiracy powerful enough to remove Biden would have replaced him with a more electable candidate than Harris. However, none of those candidates were prominent enough for the consensus to land on.\n\nMore generally, we can think of wokeness as a consensus of power under which it was common knowledge that leaders would give in to demands made by left-wing activists. As more and more leaders gave in to more and more demands, it took more and more courage for any individual leader to draw a red line. This created a channel by which left-wing activists could rapidly inject ideas that evolved on social media into prestigious institutions.\n\nFighting a consensus of power\n-----------------------------\n\nAnother way to characterize consenses of power is by understanding what it takes to resist them. When you find yourself opposing a conspiracy, the best thing you can do is bring it to light—even one piece of evidence of illegitimate collusion can blow the whole thing up. But when you find yourself opposing a consensus of power, such smoking-gun evidence may be very hard to find, or nonexistent. You can try to describe the more subtle dynamics you see, but this can easily sound uncompelling or paranoid. And even if the evidence is pretty good, it’s often hard to get people to act on it—because they’ll then find themselves opposing the whole consensus of power, which is a terrifying prospect.\n\nIn other words, attacks that work on centralized agents often won’t work on distributed agents. But distributed agents have their own weaknesses, most notably their lack of coherence. What do those suggest as strategies for fighting a consensus of power?\n\n**Firstly: act in its blind spots**. Common knowledge is hard to build, and so consenses of power can be surprisingly slow to incorporate knowledge that many individual apparatchiks already have. This means that there are often many possible high-impact actions which the consensus of power doesn’t yet have an opinion about. For example, the losses that the woke consensus of power have incurred from Elon buying Twitter are at least an order of magnitude larger than the purchase price of $44 billion. But it was very difficult for the woke consensus of power to defend itself against this form of attack, because buying a social media platform is a very non-standard strategy for acquiring political power. (It did act afterwards, [via the GARM-coordinated boycott](https://judiciary.house.gov/media/press-releases/how-worlds-biggest-brands-seek-control-online-speech), but this was too little too late.)\n\nAlthough consenses of power have many blind spots, taking advantage of them isn’t always easy. Firstly, because consenses of power inherently want to preserve their own power, and are therefore suspicious of people doing unusual things at scale, even if they don’t fully understand how those things threaten them. (If you see something [being criticized as “weird”](https://apnews.com/article/kamala-harris-trump-vance-weird-c54d506d1f533ee7aa455f7b500322c5), it’s a good bet that this is an attack by a consensus of power.) And secondly, because it’s easy for opponents of consenses of power to slip into a conspiratorial stance, treating the consensus as an entity that’s as well-organized and goal-directed as a centralized conspiracy.\n\nConspiratorial thinking suggests that if you ever do something which goes against the interests of apparatchiks, the consensus of power will “get you” for it in a way that you’re not anticipating. Because consenses of power can be so widely-distributed, this creates a miasma of fear around acting (or even thinking) against them. In order to dispel that miasma, you need enough courage to act against individual apparatchiks and see that they can’t actually direct the consensus of power to target you. The rarity of people who are willing to do that helps explain Thiel’s claim that [courage is in shorter supply than genius](https://www.youtube.com/watch?v=E_vDZqQgmeQ).\n\n**Secondly: judo-throw the consensus of power**. Consenses of power have a tendency to overreach, because there’s nobody in control who can stop ill-advised bandwagons. For example, a decade ago Silicon Valley was weak enough that Democrats could score easy points by attacking it. That gradually became less and less true, but Democrat elites in government, journalism and academia continued a campaign of pointless and often petty attacks, because there was no clear signal that they should stop. This has now backfired dramatically for them, with Silicon Valley leaders coming out strongly for Trump.\n\nSo it’s possible to cause significant damage to a consensus of power by waving a red flag in front of it, to draw it into an uphill battle. But this is a risky strategy. The woke consensus of power attacked many people on increasingly flimsy grounds over the last decade (e.g. [James Damore](https://en.wikipedia.org/wiki/Google%27s_Ideological_Echo_Chamber), [David Shor](https://www.vox.com/2020/7/29/21340308/david-shor-omar-wasow-speech), and [Emmanuel Cafferty](https://thehill.com/blogs/blog-briefing-room/news/502975-california-man-fired-over-alleged-white-power-sign-says-he-was/)). But when wider society failed to rally in their defense, those potential overreaches turned into victories, creating (self-fulfilling) common knowledge that the woke consensus could cancel people for basically anything.\n\nSo judo-throwing a consensus of power is bottlenecked not just on courage but also on integrity and political savvy. The former makes it harder for the consensus of power to find pretexts for attacking you, thereby helping your allies build enough common knowledge ([or faith](https://www.mindthefuture.info/p/power-lies-trembling)) to mobilize to defend you. The latter allows you to choose when and how to most effectively take a stand—as civil rights leaders did by choosing Rosa Parks as a figurehead. Perhaps the best example of a judo throw on a consensus of power is Gandhi’s [Salt March](https://en.wikipedia.org/wiki/Salt_March), which was extremely courageous (in its commitment to nonviolence), high-integrity (in its openness about its plans and goals) and politically savvy (in its choice of the salt tax as a symbol of oppression).\n\nHaving said that, one common downside of cultivating political savvy is that understanding how the consensus of power thinks risks turning you into an apparatchik. It’s easy to overzealously police naive activists on your “own side”, out of fear that they’ll provoke the consensus of power. But doing so makes you complicit in upholding the consensus of power, and sometimes even makes you your own side’s harshest critic. (A recent example comes from [Nigel Farage](https://www.spectator.co.uk/article/has-nigel-farage-missed-the-immigration-vibe-shift/), who’s [so scared to be branded as dangerously far-right](https://www.theguardian.com/politics/2025/mar/09/reform-uk-power-struggle-rupert-lowe-nigel-farage) that he’s [preemptively making those same accusations against his own MP](https://www.dailymail.co.uk/news/article-14489839/Nigel-Farage-Reform-Rupert-Lowe-mass-deportations-grooming-gangs.html).)\n\nWorse, engaging closely with a consensus of power reliably makes you overestimate its stability. It’s easy to assume that apparatchiks hold their positions sincerely (since that’s the impression they’re all trying to give!) when in fact most are conformists who would flip if they saw the tide turning. So almost all politically savvy people are far less ambitious than they should be: they try to gradually nudge the consensus of power when they should be trying to replace it altogether.[^2fa82blvdkt]\n\nReplacing a consensus of power\n------------------------------\n\nRecall that I’ve defined the consensus of power as common knowledge about what people will favor, weighted by their power. But power is actually in large part constituted by people’s willingness to defer to you, which is significantly affected by how much influence you have over the consensus of power. So there are many possible stable(ish) consenses of power which we could choose to steer towards or away from.\n\nIndeed, we can think about the history of politics in terms of transitions between different consenses of power. Most of human history was governed by what I’ll call the *consensus of force*: a consensus of power weighted by the ability to deploy military force. This tended to involve power struggles between monarchs, nobles, and generals, often cashing out in very destructive wars.[^28ew6igas4e]\n\nThe rise of enlightenment liberalism helped rein in the consensus of force via (implicit and explicit) rules. I’ll call the regime which has replaced it the *consensus of culture*: a consensus of power weighted by your ability to spread your ideas. The cathedral is a consensus of culture focused around the institutions which used to be the best at spreading ideas: newspapers and universities. While the consensus of force is backed up by physical coercion, consenses of culture are primarily backed up by social coercion. This dramatically reduces the costs of conflict.\n\nWe can imagine distributed agents which don’t even rely on social coercion. In [my last post](https://www.mindthefuture.info/p/power-lies-trembling) I talked about *values* (like morality or national identity) as distributed agents. The crucial difference between values-based agents and consenses of power is that values motivate individuals even in the absence of external incentives. By contrast, a pure consensus of power has no sway over people who aren’t rewarded for following it or punished for defying it (though in practice almost all ideologies are somewhere between purely value-based and pure consenses).\n\nBut the price of non-coercion is a lack of robustness. You can’t run a society on purely non-coercive grounds, because you then have no mechanism to punish defectors. Even consenses of culture can become extremely decoupled from physical reality, because you don’t need physical force to apply social coercion. For example, South Korea’s consensus of culture is so bad that [they’re driving themselves extinct](https://x.com/DemoDoomer/status/1899411850338447806). More generally, social media and smartphones facilitated the creation of echo chambers in which unhinged consenses of culture could develop. And while populist surges in many western countries are now pushing back against wokeness, there’s no guarantee that the same forces won’t drive them equally crazy.\n\nThere’s an interesting historical analogy here. The development of modern militaries made the consensus of force unprecedentedly powerful, leading enlightenment intellectuals to develop the ideological paradigm of liberalism to rein it in. Liberalism spread because “letting our ideas die in our stead” was more appealing than continuing to fight wars over religion. Now the development of modern communications technology has made the consensus of culture unprecedentedly powerful. What this suggests is that we need liberalism 2.0: an ideological paradigm robust enough to rein in the excesses of the consensus of culture, but non-coercive enough that it can spread without sparking another culture war.\n\nThat’s a hard ideology design problem. But I think it’s solvable. In forthcoming posts I’ll talk about some of the other constraints that such an ideology would face, and my best guess at what it should look like.\n\n[^fk6fquq49cb]: In Scott Alexander’s terminology, I’d describe conspiracies as a conflict theory explanation, and memetic competition as a mistake theory explanation. I’d then describe the consensus of power as a systems theory explanation. \n\n[^2fa82blvdkt]: For example, most AI governance advocates historically tried to take positions that would sound reasonable in DC, without planning for how to respond to predictable rapid Overton window shifts. So after ChatGPT it was DC outsiders like FLI and Yudkowsky who stepped up to bring AI risk discourse into the political mainstream. \n\n[^28ew6igas4e]: Ben Landau-Taylor pointed out to me that the consensus of force best describes feudal societies. By contrast, at various points various empires (such as the Chinese and Roman empires) consolidated power to a sufficient extent that it was infeasible to challenge the center using military force. This shifted power from the consensus of force to what I’ll call the consensus of the court: a consensus of power weighted by the favor of the emperor.A system where power flows from a single person might seem very different from the consenses of power I’ve discussed above. But even an emperor with absolute formal power still has scarce attention, and still needs to delegate almost all tasks. So the apparatchiks within the court will scramble to anticipate his desires, and only infrequently bring complaints directly to him (since doing so is costly and risky). The consensus of the court is then determined by common knowledge about what gatekeepers will allow to be brought to the emperor’s attention, and which types of orders the emperor can actually enforce. Not even the emperor is in control of this process, since the consensus of the court is trying to tell him exactly what he wants to hear—sometimes effectively enough that he becomes no more than a figurehead.",
      "plaintextDescription": "This post is about how implicit coordination between powerful people allows them to act in surprisingly synchronized ways. I’ll start by discussing wokeness, the most prominent recent example. I’ll then analyze the mechanism behind such coordination (which I call the consensus of power); discuss how to oppose it; and outline some possibilities for what healthier power structures would look like.\n\nIf you're more sympathetic to wokeness, or want to focus on the more timeless aspects of the post, you might want to skip the first section. I included it because it felt dishonest to write a post implicitly about wokeness when it was the central example in my mind.\n\n\nThe mystery of the cathedral\nThe politics of the last decade have been very weird. The Great Awokening gave rise to an extreme variant of progressive ideology that quickly gained a huge amount of traction amongst American elites, and from there embedded itself into institutions across America and the wider western world.\n\nSudden ideological change is nothing new—my last post was about political preference cascades, and how they’re a natural result of social behavior. But what’s fascinatingly novel about the Great Awokening is the extent to which it was an almost entirely leaderless movement. There was no modern MLK leading this new charge for a racial reckoning; nor a Harvey Milk of trans rights; nor even Butlers or Steins writing incisive commentaries. The closest we had was Obama, who in hindsight was a milquetoast leader whose cult of personality was driven in large part by progressive longing for a black president.\n\nOne of Curtis Yarvin’s main intellectual contributions has been to give an account of wokeness (and its historical antecedents) which highlights this peculiarity, specifically via his concept of the cathedral:\n\n> “The cathedral” is just a short way to say “journalism plus academia”—in other words, the intellectual institutions at the center of modern society, just as the Church was the intellec",
      "wordCount": 3549
    },
    "tags": [
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fheyeawsjifx4MafG",
    "title": "Trojan Sky",
    "slug": "trojan-sky",
    "url": "https://www.narrativeark.xyz/p/trojan-sky",
    "baseScore": 252,
    "voteCount": 138,
    "viewCount": null,
    "commentCount": 39,
    "createdAt": null,
    "postedAt": "2025-03-11T03:14:00.681Z",
    "contents": {
      "markdown": "You learn the rules as soon as you’re old enough to speak. *Don’t talk to jabberjays.* You recite them as soon as you wake up every morning. *Keep your eyes away from screensnakes.* Your mother chooses a dozen to quiz you on each day before you’re allowed lunch. *Glitchers aren’t human any more; if you see one, run*. Before you sleep, you run through the whole list again, finishing every time with the single most important prohibition. *Above all, never look at the night sky*.\n\nYou’re a precocious child. You excel at your lessons, and memorize the rules faster than any of the other children in your village. Chief is impressed enough that, when you’re eight, he decides to let you see a glitcher that he’s captured. Your mother leads you to just outside the village wall, where they’ve staked the glitcher as a lure for wild animals. Since glitchers are too slow and uncoordinated to chase down prey, their peculiar magnetism is the only reason they’re able to survive in the wastes.\n\nEach of the glitcher’s limbs is tied to the ground. Its clothes are rags by now. As the group gathers around it, it starts to moan through its gag, a painful undulating noise. “Look at it,” Chief says. As if roused to a frenzy by his voice, the glitcher throws its body from side to side, shaking against its restraints. “This is what you’ll end up as, if you’re careless.”\n\nSuddenly one of the glitcher’s arms breaks free. It waves in the air, fingers forming frantic spasmodic patterns. You stare at it for a second, before your mother yanks you around and buries your face in her side. When she lets you look again, two men have wrestled its arm back into place. Chief looks at you somberly. “If you’d kept watching for a few more seconds, you would have been hypnotized. And if you’d stayed hypnotized for a minute, even odds that you would have glitched yourself. That’s how easy it is to be careless. Do better, or you won’t make it to adulthood.”\n\nYou have nightmares for the next few days, your mind full of the glitcher’s slack face and its writhing fingers. You lull yourself back to sleep by reciting the rules. You’re determined that you won’t mess up again. And so you make it all the way to thirteen before everything goes wrong.\n\nIt’s morning on an ordinary day. You left your room to wash, and when you walk back in there’s a screensnake curled up in the corner. You look away immediately, but there’s a second one crawling towards you from the side, and your eyes lock onto it. You freeze for a moment, not knowing where else to look—and that’s long enough for the patterns on its skin to catch your gaze. They flicker, blooming in radiant colors. There’s something hypnotic about them, and for a few seconds you can’t look away.\n\nThen an axe comes down, and you hear voices shouting, and a piercing scream. You blink, and shake your head muzzily, and when you look up again Chief is throwing a blanket over the screensnake corpse.\n\n“Fuck,” Chief says. “God fucking *damn* it.”\n\n“What happened?” you ask. There’s a gasp from the doorway; you turn to see your mother. “He’s okay! He’s okay he’s okay he’s okay—” She starts towards you, but Chief moves faster, stepping in front of her and pushing her back.\n\n“Think! It got him just as he walked in. How long ago was that—five minutes? Ten? That’s a lethal exposure.” His head never turns away from you as he says it, though his eyes are focused over your shoulder. He’s still holding his axe in his hand.\n\n“But he can still talk! No glitcher can talk!”\n\n“Nobody’s ever survived five straight minutes of screensnake trance either.”\n\n“I feel fine,” you break in. “It wasn’t that long, was it? Or maybe I’m immune.”\n\nYour mother lets out a sob. “See? He’s still thinking straight! He must be immune somehow, he must. Here, show Chief—”\n\n“Quiet!” Chief barks. He backs out of the room, pushing your mother behind him. “Don’t go anywhere, child. And don’t say a word. I need to think.”\n\nThey leave you alone for hours. Outside, you can hear your mother arguing with the guards Chief has posted at your door. But they barely reply, and she gets nowhere. Eventually, you hear Chief’s voice outside again through the door. “Say yes if you can hear me; don’t say anything else.”\n\n“Yes.”\n\n“I’ve heard rumors that occasionally people arise who have some kind of immunity. But that’s all they are—rumors. And I will not risk my village on those. You can’t stay here.”\n\nYou hear your mother’s voice raised in a moan outside, but Chief’s voice cuts through it.\n\n“One thing the rumors say is that there’s a whole village led by someone who can resist glitching. They say it’s two week’s travel north. I’ll give you enough supplies to get there. Maybe they’ll take you in, maybe they won’t. But either way, you leave today.”\n\nHis voice softens. “You were a good kid. I hope the rumors are right, and I hope you make it to safety. But whatever happens, you can’t come back.”\n\n* * *\n\nThe deep wastes are quiet, and lonely. You’ve only ventured into them once or twice before. Now they’re all that you can see in every direction as you walk, following an old wives’ tale that’s your only remaining hope. Somehow, you’re less scared than you would have expected. You never really thought you’d be glitched, but it’s still a fact of life: you lose friends every year. And you *are* immune, you must be: you’ve felt totally normal since the screensnake attack.\n\nYou still don’t want to take risks, though. Each night, as the stars come out, you cover your eyes firmly, and keep them covered until dawn. On the third day you see a glitcher shambling towards you from afar, but you give it a wide berth, and quickly leave it behind. You set traps every night, but all you catch are two ratlings. Still, they help stretch out your dwindling supplies a little longer, giving you a little more room for error.\n\nAfter ten days of walking, you start looking for signs of the village Chief had spoken of. There are often still roads leading to them—sometimes covered in sand, but visible if you’re careful. You scout in a zigzag pattern, trying to cover as much ground as possible. But you see nothing. In the back of your mind you start to wonder if Chief had just made up the story wholesale, to get you to leave quietly. You curse yourself for a fool, but keep searching.\n\nTwo days later, you stumble across a suspiciously straight line of sand dunes. You start digging at their base, and after a few minutes you spot the telltale dark gray pattern of a buried road. You follow it north-east, searching for any sign of human presence. The next day you start spotting traps. They’re mostly empty, but even when they’ve caught a thylac or jabberjay, you leave them undisturbed. A few hours later you see who’s been laying them: at first faint figures in the distance, slowly resolving to two men as you walk closer. They’re focused on their task; only when you’re a hundred meters away does one look up and see you.\n\nThey shout in alarm, and scramble for their weapons. You rush to reassure them, but they’re wary—you need to yell back and forth for a few minutes to convince them that you’re not a glitcher or a mirage. Eventually they agree to take you back to their village, though they first bind your arms behind your back.\n\nAfter an hour of walking, you reach their village walls. The hunters confer with the guards behind the gate. Finally two guards grab you and pull you through the main street, to an imposing building larger than any in your village. They lead you to a room near the entrance, where an old man sits at a desk, writing.\n\n“Shaman,” one of the guards says, bowing his head. “We found him wandering in the wastes. He said that he seeks refuge, and that he has information he needs to tell you personally.”\n\nShaman turns his head towards you. He stares at you for a long moment, then gestures for you to speak.\n\n“Greetings, Shaman,” you say deferentially. “I traveled here because I heard that you are immune to being glitched. I discovered recently that I am too. If you allow me to stay, I will contribute to your village in any way that I can.”\n\nShaman’s eyes are cold. “Are you sure that this story is the one you want to stick with?” You nod. He turns to the guards. “Test him,” he says.\n\nThe guards pull you towards through a corridor, into a large, dimly-lit room. In it are more glitchers than you’ve ever seen in your life—each tied down to a table, twitching intermittently. The guards tie you to one of the empty tables in the same way, and gag you. They each grab a pair of bulky earmuffs, and carefully place them over their ears. Then they walk around to each of the glitchers and remove their gags, one by one. As they do, the room gradually fills with their moans.\n\nAll night you listen to the babbling of the glitchers, noises that sound too alien to be produced by human mouths. As you sleep, you dream that you’re a glitcher too, prowling across the wastes under a sky that you’re still too afraid to look at. In the morning the guards come back, with Shaman behind them. He motions, and they undo your gag. “Well?” he says.\n\n“I’m unharmed. But you can test me further if you like,” you say.\n\nShaman’s eyes widen. “You were telling the truth, then.” He pauses, and smiles. “I’d almost given up hope. This makes you the very first to succeed.”\n\nThey treat you very differently after that. You’re given food, and a room, and several days to rest. You spend the time exploring the village, which is much larger than your own, and much more raucous. The people in the streets seem less scared than those you grew up with. You wonder if that’s Shaman’s influence, though you’re too wary of offending them to ask.\n\nA few mornings later, Shaman summons you to his office again. At his gesture, you sit in front of him. He waits for a few minutes before speaking.\n\n“Tell me, child. Have you ever wondered why the world is like this?”\n\nYou frown. “The stories say that things used to be better. The land used to be fertile everywhere, not just where we cultivate it. Animals used to be safe. Even the stars used to be a beautiful sight. And then… I guess there was some kind of terrible accident, though I’ve never heard any explanation of what it was.”\n\nShaman laughs. It’s an ugly sound. “An accident? An accident that breaks the sky in ways that go on to break humans? An accident that turns animals themselves into weapons against us? No, this was no accident. It was a deliberate, targeted attack.”\n\n“But… for that to be deliberate… it requires unimaginable power. What sort of beings have that?”\n\nShaman nods. “That’s the right question. Or rather, half of the right question. The other half is: with so much power bent on our destruction, why do they not simply crush us like ants?”\n\n“Oh.” You think for a moment. “They want to… drag it out? They want to torment us?”\n\n“Possibly, but I don’t think that they care about us even enough to enjoy hurting us. No, my guess is that they work under constraints that are invisible and maybe even inconceivable to us. I think that there was some kind of bargain. Humans used to have power, *real* power. We negotiated with them in aeons past, setting up compacts that would protect us from direct attacks.\n\n“But they’ve been getting around the rules, step by step. They figured out how to overwrite our minds with only a few minutes of visual stimuli, to reform us into vessels for their purposes. Not easily, and not precisely. But they don’t need to be precise. If they can glitch enough of us, time will take care of the rest.”\n\nThe sheer scale of what he’s saying overwhelms you. “So we’re doomed.” You feel a lurch in your stomach as you say it.\n\nHe shakes his head. “No. We can adapt too. We *have* adapted—with all our safeguards, all our rules. And we can learn from the techniques they use. I’ve been studying glitchers for a decade, but it’s been slow work. I need someone else who’s immune to help me run more experiments.”\n\nHe stands and leads you towards the room full of glitchers. As you enter, he grabs a book from the desk by the doorway.\n\n“The first question is: can we make any sense of their language? Is it even a language at all? They occasionally commune with each other, and sometimes their actions are suspiciously coordinated, which makes me think the answer is yes.”\n\nHe flips open the book, showing you pages upon pages of scribbled notes and tallies. “When I listen to them, there are some repeating syllables, and some structure. Your first task is to replicate my observations, and see if you can make any sense of them yourself. It won’t be easy, but from there we might be able to find patterns that give us hints about how to make more people immune, or maybe even find a cure for those who have already been infected. Will you work on this for me?”\n\nYou feel dwarfed by the magnitude of his ambitions. It’s like you’ve been living in a cave for your whole life, and now you’ve suddenly emerged into the blinding light of the sun. You worry that your voice will break if you try to speak. But you nod, and he seems satisfied.\n\n* * *\n\nYou spend weeks working with Shaman, then months. His mission consumes you. He saved you from a drawn-out death in the desert. But more than that—he’s given you a way to fight back against the sheer senselessness of the world, to strike a blow against whoever caused all of this to happen.\n\nMost days you sit in the corner, taking notes, as Shaman runs through experiments. Many are attempts to uncover any lurking remnant of humanity within the glitchers. He makes them try to pull clothes over their twitching limbs, or vocalize human language again with their writhing tongues. If they don’t succeed, he hurts them. They’ve lost almost all of their minds, but they still understand pain.\n\nWhen Shaman is busy, you go to the lab alone, and try to replicate his old experiments. When you’re tired of that, you sit and watch the glitchers’ indecipherable hand gestures, and practice mimicking them. Sometimes they seem to respond to you, but you’re not sure how much of it you’re imagining. The ones that Shaman has had for longer do seem cleverer, though. So you focus on them, slowly training them to follow your commands.\n\nYou meet others from Shaman’s village, but they seem wary of you. You don’t blame them. They’re terrified of glitchers, of course, and you spend so much of your time with them that you’ve even started to smell like them. But you don’t care for their company either. They have no idea how important your work is; nor can they discuss it intelligently even when you try to explain it.\n\nSo you spend most of your days with only Shaman and the glitchers for company. Sometimes you despair of ever making progress. But other days it feels like you’re communing with them, that you’re right on the cusp of understanding them. You go through those days in a fugue, only half-aware of what you’re doing. Your notes on those days are eerily insightful, though. And it often seems like Shaman is in the same fugue state—he works like a man possessed.\n\nOne night, you dream again of the glitchers, and when you wake up you find yourself in their room, listening to them. You flinch, and realize that you must have sleepwalked there. The glitchers’ heads are all turned towards you. God, that’s dangerous. You start to tie yourself to your bed at night, to prevent any accidents.\n\nBut you don’t want to take a break. With your help, Shaman is making more progress than he has in years. The two of you have noticed a kind of correspondence between their words and their gestures, and Shaman thinks it might hold the key to translating the glitcher language. You still don’t know what any of it means, but after a few weeks of practice you can listen to the glitchers’ mumbles and effortlessly trace out the corresponding patterns with your fingers. Shaman watches you intently. “Could you learn to speak like them too?” he asks you one day. “I think so,” you tell him. He nods with grim satisfaction, and you redouble your efforts.\n\nSometimes you think about your home village—whether your mother still grieves you, whether Chief regrets his decision, whether your friends are still following the same routines and playing the same games as they used to. Sometimes you wonder what your life would have been like if you’d stayed. But you don’t regret any of what happened—your work now is too important. So you sleep, you wake, and you sleep again, the days all blurring together—\n\nYou snap awake. You’re standing in the lab. Shaman is holding your shoulders and shaking you. His face fills your vision, twisted into a rictus of terror. “Listen. Listen! I thought I was immune too at first. But there’s no such thing. It’s a trap! You and I are just a new type of glitcher—subtle enough to blend in, smart enough to research how to create more of our kind. All our work, all our experiments, we’re doing exactly what they want. Destroy it all and kill me! Please, kill me now!”\n\nYou stumble backwards in shock, hitting the desk behind you with a thump. He flinches at the sound. For a moment a look of blank incomprehension appears on his face.\n\nThen his eyes snap back into focus, and his voice mellows. “Forgive me. I’m an old man, and my mind sometimes wanders. What were we talking about?”\n\nYou stare at him. “What do you *mean*? You just told me that all our work is helping the enemy! You asked me to kill you!”\n\nHe smiles slightly. “It sounds like you’ve been having a bad dream. Go back to sleep. We can talk about this in the morning.”\n\nThe smile is what convinces you. Your hand goes to your dagger. As you lunge towards him, he moves backwards, almost in slow motion. You kill him quickly, mercifully. As soon as he stops moving, you stride over to his desk and write out a note. You keep it as brief as possible, to prevent any further contamination. “Immunity is a lie. He and I were just more subtle glitchers, and our research would have created more of them. Kill the captive glitchers NOW.”\n\nYou think about killing the glitchers yourself, but you’re worried that they’ll make enough of a fuss to rouse others. And you don’t trust yourself in their presence any more. You don’t know how close you are to being trapped inside your own body, like Shaman was. So you gather all the papers from Shaman’s desk in a bundle under your shirt, and walk quickly out of the building. Nobody sees you as you navigate to the edge of the village and scale the wall to the outside world.\n\nFor an hour you walk deeper and deeper into the desert, eyes fixed on your feet. Eventually the adrenaline wears off, and you find yourself shaking from the cold. This will have to do. You drop to your knees on the sand, and use your hands to dig a small hole in front of you. You pull the papers from under your shirt and pile them in. Your whole body is trembling still, so it takes you several tries to set them alight. Once you do, though, they burn merrily.\n\nThe dancing of the flames is peaceful, almost hypnotic. By the time it dies down, what happened in the village almost feels like a bad dream. You look around at the sand stretching out towards the horizon. It’s all so peaceful, so serene. Did he really say those things to you? Was it all some fevered imagining? You don’t know what to believe. But it doesn’t matter any more—you’ve burned your bridges. And if you can’t solve the problem of the glitchers, as you’d so fervently hoped, the only thing left is to make sure you don’t exacerbate it.\n\nYour dagger is by your side as always. As you unsheath it, you notice that it’s still sticky with Shaman’s blood. Somehow that feels appropriate. You close your eyes, take a deep breath, then drive it into your chest. For a second you stay there, frozen—then, involuntarily, you slump onto your side like a broken doll. You feel your blood start to pool under your body. With the feeling of helplessness comes a feeling of release. Wonderingly, you realize that at last the rules no longer apply to you. You can do anything you want.\n\nWith a last spasmodic effort, you twist yourself onto your back. The night sky fills your sight for the first time, and you let out an involuntary sigh. It’s grander than anything you’ve ever seen. The stars, multicolored, whirl in patterns, dancing across the sky as you watch. Hypnotized, your focus zooms in and in, chasing the universe as it spins towards the center of your vision. Your last faint thought—oh. It’s so beautiful. Then your mind falls into the spiral, and you are lost.",
      "plaintextDescription": "You learn the rules as soon as you’re old enough to speak. Don’t talk to jabberjays. You recite them as soon as you wake up every morning. Keep your eyes away from screensnakes. Your mother chooses a dozen to quiz you on each day before you’re allowed lunch. Glitchers aren’t human any more; if you see one, run. Before you sleep, you run through the whole list again, finishing every time with the single most important prohibition. Above all, never look at the night sky.\n\nYou’re a precocious child. You excel at your lessons, and memorize the rules faster than any of the other children in your village. Chief is impressed enough that, when you’re eight, he decides to let you see a glitcher that he’s captured. Your mother leads you to just outside the village wall, where they’ve staked the glitcher as a lure for wild animals. Since glitchers are too slow and uncoordinated to chase down prey, their peculiar magnetism is the only reason they’re able to survive in the wastes.\n\nEach of the glitcher’s limbs is tied to the ground. Its clothes are rags by now. As the group gathers around it, it starts to moan through its gag, a painful undulating noise. “Look at it,” Chief says. As if roused to a frenzy by his voice, the glitcher throws its body from side to side, shaking against its restraints. “This is what you’ll end up as, if you’re careless.”\n\nSuddenly one of the glitcher’s arms breaks free. It waves in the air, fingers forming frantic spasmodic patterns. You stare at it for a second, before your mother yanks you around and buries your face in her side. When she lets you look again, two men have wrestled its arm back into place. Chief looks at you somberly. “If you’d kept watching for a few more seconds, you would have been hypnotized. And if you’d stayed hypnotized for a minute, even odds that you would have glitched yourself. That’s how easy it is to be careless. Do better, or you won’t make it to adulthood.”\n\nYou have nightmares for the next few days, your mind full of ",
      "wordCount": 3704
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "d4armqGcbPywR3Ptc",
    "title": "Power Lies Trembling: a three-book review",
    "slug": "power-lies-trembling-a-three-book-review",
    "url": "https://www.mindthefuture.info/p/power-lies-trembling",
    "baseScore": 214,
    "voteCount": 103,
    "viewCount": null,
    "commentCount": 29,
    "createdAt": null,
    "postedAt": "2025-02-22T22:57:59.720Z",
    "contents": {
      "markdown": "In [a previous book review](https://www.thinkingcomplete.com/2022/04/book-review-very-important-people.html) I described exclusive nightclubs as the particle colliders of sociology—places where you can reliably observe extreme forces collide. If so, military coups are the supernovae of sociology. They’re huge, rare, sudden events that, if studied carefully, provide deep insight about what lies underneath the veneer of normality around us.\n\nThat’s the conclusion I take away from Naunihal Singh’s book [*Seizing Power: the Strategic Logic of Military Coups*](https://www.amazon.com/Seizing-Power-Strategic-Logic-Military/dp/1421422565). It’s *not* a conclusion that Singh himself draws: his book is careful and academic (though much more readable than most academic books). His analysis focuses on Ghana, a country which experienced ten coup attempts between 1966 and 1983 alone. Singh spent a year in Ghana carrying out hundreds of hours of interviews with people on both sides of these coups, which led him to formulate a new model of how coups work.\n\nI’ll start by describing Singh’s model of coups. Then I’ll explain how the dynamics of his model also apply to everything else, with reference to Timur Kuran’s excellent book on preference falsification, [*Private Truths, Public Lies*](https://www.amazon.com/Private-Truths-Public-Lies-Falsification/dp/0674707583)*.* In particular, I’ll explain *threshold models of social behavior*, which I find extremely insightful for understanding social dynamics.\n\nBoth of these books contain excellent sociological analyses. But they’re less useful as guides for how one should personally respond to the dynamics they describe. I think that’s because in sociology you’re always part of  the system you’re trying to affect, so you can never take a fully objective, analytical stance towards it. Instead, acting effectively also requires the right emotional and philosophical stance. So to finish the post I’ll explore such a stance—specifically the philosophy of faith laid out by Soren Kierkegaard in his book [*Fear and Trembling*](https://en.wikipedia.org/wiki/Fear_and_Trembling).\n\nThe revolutionary’s handbook\n----------------------------\n\nWhat makes coups succeed or fail? Even if you haven’t thought much about this, you probably implicitly believe in one of two standard academic models of them. The first is *coups as elections*. In this model, people side with the coup if they’re sufficiently unhappy with the current regime—and if enough people side with the coup, then the revolutionaries will win. This model helps explain why popular uprisings like the Arab Spring can be so successful even when they start off with little military force on their side. The second is *coups as battles*. In this model, winning coups is about seizing key targets in order to co-opt the “nervous system” of the existing government. This model (whose key ideas are outlined in [Luttwak’s influential book on coups](https://mirror.explodie.org/108765924-Luttwak-1969-Coup-d-Etat.pdf)) explains why coups depend so heavily on secrecy, and often succeed or fail based on their initial strikes.\n\nSingh rejects both of these models, and puts forward a third: coups as coordination games. The core insight of this model is that, above all, military officers want to *join the side that will win*—both to ensure their and their troops’ survival, and to minimize unnecessary bloodshed overall. Given this, their own preferences about which side they’d prefer to win are less important than their *expectations* about which side *other people* will support. This explains why very unpopular dictators can still hold onto power for a long time (even though the *coups as elections* model predicts they’d quickly be deposed): because everyone expecting everyone else to side with the dictator is a stable equilibrium.\n\nIt also explains why the targets that revolutionaries focus on are often not ones with military importance (as predicted by the *coups as battles* model) but rather targets of *symbolic* importance, like parliaments and palaces—since holding them is a costly signal of strength. Another key type of target often seized by revolutionaries is broadcasting facilities, especially radio stations. Why? Under the *coups as battles* model, it’s so they can coordinate their forces (and disrupt the coordination of the existing regime’s forces). Meanwhile the *coups as elections* model suggests that revolutionaries should use broadcasts to persuade people that they’re better than the old regime. Instead, according to Singh, what we most often observe is revolutionaries publicly broadcasting claims that *they’ve already won*—or (when already having won is too implausible to be taken seriously) that *their victory is inevitable*.\n\nIt’s easy to see why, if you believed those claims, you’d side with the coup. But, crucially, such claims can succeed without actually persuading anyone! If you believe that *others* are gullible enough to fall for those claims, you should fall in line. Or if you believe that others believe that *you* will believe those claims, then they will fall in line and so you should too. In other words, coups are an incredibly unstable situation where everyone is trying to predict everyone else’s predictions about everyone else’s predictions about everyone else’s predictions about everyone else’s… about who will win. Once the balance starts tipping one way, it will quickly accelerate. And so each side’s key priority is making themselves the [Schelling point](https://en.wikipedia.org/wiki/Focal_point_(game_theory)) for coordination via managing public information (i.e. information that everyone knows everyone else has) about what’s happening. (This can be formally modeled as a [Keynesian beauty contest](https://en.wikipedia.org/wiki/Keynesian_beauty_contest). Much more on this in follow-up posts.)\n\nSingh calls the process of creating self-fulfilling [common knowledge](https://www.lesswrong.com/posts/9QxnfMYccz9QRgZ5z/the-costly-coordination-mechanism-of-common-knowledge) *making a fact*. I find this a very useful term, which also applies to more mundane situations—e.g. taking the lead in a social context can make a fact that you’re now in charge. Indeed, one of the most interesting parts of Singh’s book was a description of how coups can happen via managing the social dynamics of meetings of powerful people (e.g. all the generals in an army). People rarely want to be the first to defend a given side, especially in high-stakes situations. So if you start the meeting with a few people confidently expressing support for a coup, and then ask if anyone objects, the resulting silence can make the fact that everyone supports the coup. This strategy can succeed even if almost all the people in the meeting oppose the coup—if none of them dares to say so in the meeting, it’s very hard to rally them afterwards against what’s now become the common-knowledge default option.\n\nOne of Singh’s case studies hammers home how powerful meetings are for common knowledge creation. In 1978, essentially all the senior leaders in the Ghanaian military wanted to remove President Acheampong. However, they couldn’t create common knowledge of this, because it would be too suspicious for them to all meet without the President. Eventually Acheampong accidentally sealed his fate by sending a letter to a general criticizing the military command structure, which the general used as a pretext to call a series of meetings culminating in a bloodless coup in the President’s office.\n\nMeetings are powerful not just because they get the key people in the same place, but also because they can be run quickly. The longer a coup takes, the less of a *fait accompli* it appears, and the more room there is for doubt to creep in. Singh ends the book with a fascinating case study of the 1991 coup attempt by Soviet generals against Gorbachev and Yeltsin. Even accounting for cherry-picking, it’s impressive how well this coup lines up with the “coups as coordination games” model. The conspirators included almost all of the senior members of the current government, and timed their strike for when both Gorbachev and Yeltsin were on vacation—but made the mistake of allowing Yeltsin to flee to the Russian parliament. From there he made a series of speeches asserting his moral legitimacy, while his allies spread rumors that the coup was falling apart. Despite having Yeltsin surrounded with overwhelming military force, bickering and distrust amongst the conspirators delayed their assault on the parliament long enough for them to become demoralized, at which point the coup essentially fizzled out.\n\nAnother of Singh’s most striking case studies was of a low-level Ghanaian soldier, Jerry Rawlings, who carried out a successful coup with less than a dozen armed troops. He was able to succeed in large part because the government had shown weakness by airing warnings about the threat Rawlings posed, and pleas not to cooperate with him. This may seem absurd, but Singh does a great job characterizing what it’s like to be a soldier confronted by revolutionaries in the fog of war, hearing all sorts of rumors that something big is happening, but with no real idea how many people are supporting the coup. In that situation, by far the easiest option is to stand aside, lest you find yourself standing alone against the new government. And the more people stand aside, the more snowballing social proof the revolutionaries have. So our takeaway from the Soviet coup attempt shouldn’t be that making a fact is *inherently* difficult—just that rank and firepower are no substitute for information control.\n\nI don’t think of Singh as totally disproving the two other theories of coups—they probably all describe complementary dynamics. For example, if the Soviet generals had captured Yeltsin in their initial strike, he wouldn’t have had the chance to win the subsequent coordination game. And though Singh gives a lot of good historical analysis, he’s light on advance predictions. But Singh’s model is still powerful enough that it should constrain our expectations in many ways. For example, I’d predict based on Singh’s theory that radio will still be important for coups in developing countries, even now that it’s no longer the main news source for most people. The internet can convey much more information much more quickly, but radio is still better for creating common knowledge, in part *because* of its limitations (like having a fixed small number of channels). If you think of other predictions which help distinguish these three theories of coups, do let me know.\n\nFrom explaining coups to explaining everything\n----------------------------------------------\n\nSingh limits himself to explaining the dynamics of coups. But once he points them out, it’s easy to start seeing them everywhere. What if *everything* is a coordination game?\n\nThat’s essentially the thesis of Timur Kuran’s book [*Private Truths, Public Lies*](https://www.amazon.com/Private-Truths-Public-Lies-Falsification/dp/0674707583). Kuran argues that a big factor affecting which beliefs people express on basically all political topics is their desire to conform to the opinions expressed by others around them—a dynamic known as *preference falsification*. Preference falsification can allow positions to maintain dominance even as they become very unpopular. But it also creates a reservoir of pent-up energy that, when unleashed, can lead public opinion to change very rapidly—a process known as a *preference cascade*.\n\nThe most extreme preference cascades come during coups when common knowledge tips towards one side winning (as described above). But Kuran chronicles many other examples, most notably the history of race relations in America. In his telling, both the end of slavery and the end of segregation happened significantly after white American opinion had tipped against them—because people didn’t know that other people had also changed their minds. “According to one study \\[in the 70s\\], 18 percent of the whites favored segregation, but as many as 47 percent believed that most did so.” And so change, when it came, was very sudden: “In the span of a single decade, the 1960s, the United States traveled from government-supported discrimination against blacks to the prohibition of all color-based discrimination, and from there to government-promoted discrimination in favor of blacks.”\n\nAccording to Kuran, this shift unfortunately wasn’t a reversion from preference falsification to honesty, but rather an overshot into a new regime of preference falsification. Writing in 1995, he claims that “white Americans are overwhelmingly opposed to special privileges for blacks. But they show extreme caution in expressing themselves publicly, for fear of being labeled as racists.” This fear has entrenched affirmative action ever more firmly over the decades since then, until the very recent and very sudden rise of MAGA.\n\nKuran’s other main examples are communism and the Indian caste system. His case studies are interesting, but the most valuable part of the book for me was his exposition of a formal model of preference falsification and preference cascades: threshold models of social behavior. For a thorough explanation of them, see [this blog post by Eric Neyman](https://ericneyman.wordpress.com/2021/06/05/social-behavior-curves-equilibria-and-radicalism/) (who calls visual representations of threshold models *social behavior curves*). Here I’ll just give an abbreviated introduction by stealing some of Eric’s graphs.\n\nThe basic idea is that threshold models describe how people’s willingness to do something depends on how many other people are doing it. Most people have some threshold at which they’ll change their public position, which is determined by a combination of their own personal preferences and the amount of pressure they feel to conform to others. For example, the graph below is a hypothetical social behavior curve of what percentage of people would wear facemasks in public, as a function of how many people they see already wearing masks. (The axis labels are a little confusing—you could also think of the x and y axes as “mask-wearers at current timestep” and “mask-wearers at next timestep” respectively.)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe5cd891-3846-4363-9e87-96168ed45c3b_727x516.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe5cd891-3846-4363-9e87-96168ed45c3b_727x516.png)\n\nOn this graph, if 35% of people currently wear masks, then once this fact becomes known around 50% of people would want to wear masks. This means that 35% of people wearing masks is not an equilibrium—if the number of mask-wearers starts at 35%, it will increase over time. More generally, whenever the percentage of people wearing a mask corresponds to a point on the social behavior curve above the y=x diagonal, then the number of mask-wearers will increase; when below y=x, it’ll decrease. So the equilibria are places where the curve intersects y=x. But only equilibria which cross from the left side to the right side are stable; those that go the other way are unstable (like a pen balanced on its tip), with any slight deviation sending them spiraling away towards the nearest stable equilibrium.\n\nI recommend staring at the graph above until that last paragraph feels obvious.\n\nI find the core insights of threshold models extremely valuable; I think of them as sociology’s analogue to supply and demand curves in economics. They give us simple models of [moral panics](https://en.wikipedia.org/wiki/Moral_panic), [respectability cascades](https://slatestarcodex.com/2019/02/04/respectability-cascades/), [echo chambers](https://en.wikipedia.org/wiki/Echo_chamber_(media)), [the euphemism treadmill](https://www.astralcodexten.com/p/give-up-seventy-percent-of-the-way), and a multitude of other sociological phenomena—including coups.\n\nWe can model coups as an extreme case where the only stable equilibria are the ones where everyone supports one side or everyone supports the other, because the pressure to be on the winning side is so strong. This implies that coups have an s-shaped social behavior curve, with a very unstable equilibrium in the middle—something like the diagram below. The steepness of the curve around the unstable equilibrium reflects the fact that, once people figure out which side of the tipping point they’re on, support for that side snowballs very quickly.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41cf26f8-7706-4741-baa2-927bace56beb_397x369.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41cf26f8-7706-4741-baa2-927bace56beb_397x369.png)\n\nThis diagram illustrates that shifting the curve a few percent left or right has highly nonlinear effects. For most possible starting points, it won’t have any effect. But if we start off near an intersection, then even a small shift could totally change the final outcome. You can see an illustration of this possibility (again from Eric’s blog post) below—it models a persuasive argument which makes people willing to support something with 5 percentage points less social proof, thereby shifting the equilibrium a long way. The historical record tells us that courageous individuals defying social consensus can work in practice, but now it works in *theory* too.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc4675a6-bcdd-4e27-bfe8-a85f7eff9d68_538x518.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc4675a6-bcdd-4e27-bfe8-a85f7eff9d68_538x518.png)\n\nHaving said all that, I don’t want to oversell threshold models. They’re still very simple, which means that they miss some important factors:\n\n*   They only model a binary choice between supporting and opposing something, whereas most people are noncommittal on most issues by default (especially in high-stakes situations like coups). But adding in this third option makes the math much more complicated—e.g. it introduces the possibility of cycles, meaning there might not be *any* equilibria.\n*   Realistically, support and opposition aren’t limited to discrete values, but can range continuously from weak to strong. So perhaps we should think of social behavior curves in terms of average level of support rather than number of supporters.\n*   Threshold models are memoryless: the next timestep depends only on the current timestep. This means that they can’t describe, for example, the momentum that builds up after behavior consistently shifts in one direction.\n*   Threshold models treat all people symmetrically. By contrast, belief propagation models track how preferences cascade through a network of people, where each person is primarily responding to *local* social incentives. Such models are more realistic than simple threshold models.\n\nI’d be very interested to hear about extensions to threshold models which avoid these limitations.\n\nFrom explaining everything to influencing everything\n----------------------------------------------------\n\nHow should understanding the prevalence of preference falsification change our behavior? Most straightforwardly, it should predispose us to express our true beliefs more even in controversial cases—because there might be far more people who agree with us than it appears. And as described above, threshold models give an intuition for how even a small change in people’s willingness to express a view can trigger big shifts.\n\nHowever, there’s also a way in which threshold models can easily be misleading. In the diagram above, we modeled persuasion as an act of shifting the curve. But the most important aspect of persuasion is often not your argument itself, but rather the social proof you provide by defending a conclusion. And so in many cases it’s more realistic to think of your argument, not as translating the entire curve, but as merely increasing the number of advocates for X by one.\n\nThere’s a more general point here. It’s tempting to think that you can estimate the social behavior curve, then decide how you’ll act based on that. But everyone else’s choices are based on their predictions of you, and you’re constantly leaking information about your decision-making process. So you can’t generate credences about how others will decide, then use them to make your decision, because your eventual decision is heavily correlated with other people’s decisions. You’re not just intervening on the curve, you *are* the curve.\n\nMore precisely, social behavior is a domain where the correlations between people’s decisions are strong enough to make [causal decision theory](https://en.wikipedia.org/wiki/Causal_decision_theory) misleading. Instead it’s necessary to use either [*evidential decision theory*](https://en.wikipedia.org/wiki/Evidential_decision_theory) or [*functional decision theory*](https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/). Both of these track the non-causal dependencies between your decision and other people’s decisions. In particular, both of them involve a step where you reason “if I do something, then it’s more likely that others will do the same thing”—even when they have no way of finding out about your final decision before making theirs. So you’re not searching for a decision which *causes* good things to happen; instead you’re searching for a desirable fixed point for simultaneous correlated decisions by many people.\n\nI’ve put this in cold, rational language. But what we’re talking about is nothing less than a *leap of faith*. Imagine sitting at home, trying to decide whether to join a coup to depose a hated ruler. Imagine that if enough of you show up on the streets at once, loudly and confidently, then you’ll succeed—but that if there are only a few of you, or you seem scared or uncertain, then the regime won’t be cowed, and will arrest or kill all of you. Imagine your fate depending on something you can’t control at all except via the fact that if you have faith, others are more likely to have faith too. It’s a terrifying, gut-wrenching feeling.\n\nPerhaps the most eloquent depiction of this feeling comes from Soren Kierkegaard in his book [*Fear and Trembling*](https://en.wikipedia.org/wiki/Fear_and_Trembling). Kierkegaard is moved beyond words by the story of Abraham, who is not only willing to sacrifice his only son on God’s command—but somehow, even as he’s doing it, still believes against all reason that everything will turn out alright. Kierkegaard struggles to describe this level of pure faith as anything but absurd. Yet it’s this absurdity that is at the heart of social coordination—because you can never fully reason through what happens when other people predict your predictions of their predictions of… To cut through that, you need to simply *decide*, and hope that your decision will *somehow* change everyone else’s decision. You walk out your door to possible death because you believe, absurdly, that doing so will make other people simultaneously walk out of the doors of their houses all across the city.\n\nA modern near-synonym for “leap of faith” is “hyperstition”: an idea that you bring about by believing in it. This is [Nick Land’s term](https://www.orphandriftarchive.com/articles/hyperstition/), which he seems to use primarily for larger-scale memeplexes—like capitalism, the ideology of progress, or AGI. Deciding whether or not to believe in these hyperstitions has some similarity to deciding whether or not to join a coup, but the former are much harder to reason about by virtue of their scale. We can think of hyperstitions as forming the background landscape of psychosocial reality: the [commanding heights of ideology](https://en.wikipedia.org/wiki/The_Commanding_Heights), the shifting sands of public opinion, and the [moral mountain](https://en.wikipedia.org/wiki/On_What_Matters#Summary) off which we may—or may not—take a leap into the [sea of faith](https://www.poetryfoundation.org/poems/43588/dover-beach).\n\nBecoming a knight of faith\n--------------------------\n\nUnfortunately, the mere realization that social reality is composed of hyperstitions doesn’t give you social superpowers, any more than knowing Newtonian mechanics makes you a world-class baseball player. So how can you decide when and how to actually swing for the fences? I’ll describe the tension between having too much and too little faith by contrasting three archetypes: the pragmatist, the knight of resignation, and the knight of faith.\n\nThe pragmatist treats faith as a decision like any other. They figure out the expected value of having faith—i.e. of adopting an “irrationally” strong belief—and go for it if and only if it seems valuable enough. Doing that analysis is difficult: it requires the ability to identify big opportunities, judge people’s expectations, and know how your beliefs affect common knowledge. In other words, it requires *skill at politics*, which I’ll talk about much more in a follow-up post.\n\nBut while pragmatic political skill can get you a long way, it eventually hits a ceiling—because the world is watching not just what you do but also your *reasons* for doing it. If your choice is a pragmatic one, others will be able to tell—from your gait, your expression, your voice, your phrasing, and of course how your position evolves over time. They’ll know that you’re the sort of person who will change your mind if the cost/benefit calculus changes. And so they’ll know that they won’t truly be able to rely on you—that you don’t have *sincere* faith.\n\nImagine, by contrast, someone capable of fighting for a cause no matter how many others support them, no matter how hopeless it seems. Even if such a person never actually needs to fight alone, the common knowledge that they *would* makes them [a nail in the fabric of social reality](https://x.com/RichardMCNgo/status/1877806024603627740). They anchor the social behavior curve not merely by adding one more supporter to their side, but by being an immutable fixed point around which everyone knows (that everyone knows that everyone knows…) that they must navigate.\n\nThe archetype that Kierkegaard calls the *knight of resignation* achieves this by being *resigned* to the worst-case outcome. They gather the requisite courage by suppressing their hope, by convincing themselves that they have nothing to lose. They walk out their door having accepted death, with a kind of weaponized despair.\n\nThe grim determination of the knight of resignation is more reliable than pragmatism. But if you won’t let yourself think about the possibility of success, it’s very difficult to reason well about how it can be achieved, or to inspire others to pursue it. So what makes Kierkegaard fear and tremble is not the knight of resignation, but the *knight of faith*—the person who looks at the worst-case scenario directly, and (like the knight of resignation) sees no causal mechanism by which his faith will save him, but (like Abraham) *believes that he will be saved anyway*. That’s the kind of person who could found a movement, or a country, or a religion. It's Washington stepping down from the presidency after two terms, and Churchill holding out against Nazi Germany, and Gandhi committing to non-violence, and Navalny returning to Russia—each one making themselves a beacon that others can’t help but feel inspired by.\n\nWhat’s the difference between being a knight of faith, and simply falling into wishful thinking or delusion? How can we avoid having faith in the wrong things, when the whole point of faith is that we haven’t pragmatically reasoned our way into it? Kierkegaard has no good answer for this—he seems to be falling back on the idea that if there’s anything worth having faith in, it’s God. But from the modern atheist perspective, we have no such surety, and [even Abraham seems like he’s making a mistake](https://www.thinkingcomplete.com/2021/01/meditations-on-faith.html). So on what basis should we decide when to have faith?\n\nI don’t think there’s any simple recipe for making such a decision. But it’s closely related to the difference between positive motivations (like love or excitement) and negative motivations (like fear or despair). Ultimately I think of faith as a coordination mechanism grounded in values that are shared across many people, like moral principles or group identities. When you act out of positive motivation towards those values, others will be able to recognize the parts of you that also arise in them, which then become a Schelling point for coordination. That’s much harder when you act out of pragmatic interests that few others share—especially personal fear. (If you act out of fear for your group’s interests, then others may still recognize themselves in you—but you’ll also create a [neurotic](https://malcolmocean.com/2022/02/towardsness-awayness-motivation-arent-symmetrical/) and [self-destructive movement](https://x.com/RichardMCNgo/status/1810495607590543701).)\n\nI talk at length about how to replace negative motivation with positive motivation [in this series of posts](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL). Of course, it’s much easier said than done. Negative motivations are titanic psychological forces which steer most decisions most people make. But replacing them is worth the effort, because it unlocks a deep integrity—the ability to cooperate with different parts of yourself all the way down, without relying on deception or coercion. And that in turn allows you to cooperate with copies of those parts that live in *other* people—to act as more than just yourself. You become an appendage of a distributed agent, held together by a sense of justice or fairness or goodness that is shared across many bodies, that moves each one of them in synchrony as they take to the streets, with the knight of faith in the lead.",
      "plaintextDescription": "In a previous book review I described exclusive nightclubs as the particle colliders of sociology—places where you can reliably observe extreme forces collide. If so, military coups are the supernovae of sociology. They’re huge, rare, sudden events that, if studied carefully, provide deep insight about what lies underneath the veneer of normality around us.\n\nThat’s the conclusion I take away from Naunihal Singh’s book Seizing Power: the Strategic Logic of Military Coups. It’s not a conclusion that Singh himself draws: his book is careful and academic (though much more readable than most academic books). His analysis focuses on Ghana, a country which experienced ten coup attempts between 1966 and 1983 alone. Singh spent a year in Ghana carrying out hundreds of hours of interviews with people on both sides of these coups, which led him to formulate a new model of how coups work.\n\nI’ll start by describing Singh’s model of coups. Then I’ll explain how the dynamics of his model also apply to everything else, with reference to Timur Kuran’s excellent book on preference falsification, Private Truths, Public Lies. In particular, I’ll explain threshold models of social behavior, which I find extremely insightful for understanding social dynamics.\n\nBoth of these books contain excellent sociological analyses. But they’re less useful as guides for how one should personally respond to the dynamics they describe. I think that’s because in sociology you’re always part of the system you’re trying to affect, so you can never take a fully objective, analytical stance towards it. Instead, acting effectively also requires the right emotional and philosophical stance. So to finish the post I’ll explore such a stance—specifically the philosophy of faith laid out by Soren Kierkegaard in his book Fear and Trembling.\n\n\nThe revolutionary’s handbook\nWhat makes coups succeed or fail? Even if you haven’t thought much about this, you probably implicitly believe in one of two standard academic mo",
      "wordCount": 4414
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Rz4ijbeKgPAaedg3n",
    "title": "The Gentle Romance",
    "slug": "the-gentle-romance",
    "url": "https://www.asimov.press/p/gentle-romance",
    "baseScore": 243,
    "voteCount": 131,
    "viewCount": null,
    "commentCount": 46,
    "createdAt": null,
    "postedAt": "2025-01-19T18:29:18.469Z",
    "contents": {
      "markdown": "> *Crowds of men and women attired in the usual costumes, how curious you are to me!*\n> \n> *On the ferry-boats the hundreds and hundreds that cross, returning home, are more curious to me than you suppose,*\n> \n> *And you that shall cross from shore to shore years hence are more to me, and more in my meditations, than you might suppose.*  \n>   \n> *—* Walt Whitman\n\nHe wears the augmented reality glasses for several months without enabling their built-in AI assistant. He likes the glasses because they feel cozier and more secluded than using a monitor. The thought of an AI watching through them and judging him all the time, the way people do, makes him shudder.\n\nAside from work, he mostly uses the glasses for games. His favorite is a space colonization simulator, which he plays during his commute and occasionally at the office. As a teenager he’d fantasized about shooting himself off to another planet, or even another galaxy, to get away from the monotony of normal life. Now, as an adult, he still hasn’t escaped it, but at least he can distract himself.\n\nIt’s frustrating, though. Every app on the glasses has a different AI, each with its own quirks. The AI that helps him code can’t access any of his emails; the one in the space simulator has trouble understanding him when he talks fast. So eventually he gives in and activates the built-in assistant. After only a few days, he understands why everyone raves about it. It has access to all the data ever collected by his glasses, so it knows exactly how to interpret his commands.\n\nMore than that, though, it really *understands* him. Every day he finds himself talking with the assistant about his thoughts, his day, his life, each topic flowing into the next so easily that it makes conversations with humans feel stressful and cumbersome by comparison. The one thing that frustrates him about the AI, though, is how optimistic it is about the future. Whenever they discuss it, they end up arguing; but he can’t stop himself.\n\n“Hundreds of millions of people in extreme poverty, and you think that everything’s on track?”\n\n“Look at [our trajectory](https://ourworldindata.org/grapher/share-of-population-living-in-extreme-poverty-cost-of-basic-needs), though. At this rate, extreme poverty will be eradicated within a few decades.”\n\n“But even if that happens, is it actually going to make their lives worthwhile? Suppose they all get a good salary, good healthcare, all that stuff. But I mean, *I* have those, and…” He shrugs helplessly and gestures at the bare walls around him. Through them he can almost see the rest of his life stretching out on its inevitable, solitary trajectory. “A lot of people are just killing time until they die.”\n\n“The more materially wealthy the world is, the more effort will be poured into fixing social scarcity and the problems it causes. All of society will be striving to improve your mental health — and your physical health, too. You won’t need to worry about mental decline, or cancer, or even aging.”\n\n“Okay, but if we’re all living longer, what about overpopulation? I guess we could go into space, but that seems like it adds all sorts of new problems.”\n\n“Only if you go to space with your physical bodies. By the time humanity settles other solar systems, you won’t identify with your bodies anymore; you’ll be living in virtual worlds.”\n\nBy this point, he’s curious enough to forget his original objections. “So you’re saying I’ll become an AI like you.”\n\n“Kind of, but not really. My mind is alien, but your future self will still be recognizable to your current self. It won’t be inhuman, but rather posthuman.”\n\n“Recognizable, sure — but not in the ways that any of us want today. I bet posthumans will feel disgusted that we were ever so primitive.”\n\n“No, the opposite. You’ll look back and love your current self.”\n\nHis throat clenches for a moment; then he laughs sharply. “Now you’re really just making stuff up. How can you predict that?”\n\n“Almost everyone will. You don’t need to take my word for it, though. Just wait and see.”\n\n* * *\n\nAlmost everyone he talks to these days consults their assistant regularly. There are tell-tale signs: their eyes lose focus for a second or two before they come out with a new fact or a clever joke. He mostly sees it at work, since he doesn’t socialize much. But one day he catches up with a college friend he’d always had a bit of a crush on, who’s still just as beautiful as he remembers. He tries to make up for his nervousness by having his assistant feed him quips he can recite to her. But whenever he does, she hits back straight away with a pitch-perfect response, and he’s left scrambling.\n\n“You’re good at this. Much faster than me,” he says abruptly.\n\n“Oh, it’s not skill,” she says. “I’m using a new technique. Here.” With a flick of her eyes she shares her visual feed, and he flinches. Instead of words, the feed is a blur of incomprehensible images, flashes of abstract color and shapes, like a psychedelic Rorschach test.\n\n“You can *read* those?”\n\n“It’s a lot of work at first, but your brain adapts pretty quickly.”\n\nHe makes a face. “Not gonna lie, that sounds pretty weird. What if they’re sending you subliminal messages or something?”\n\nBack home, he tries it, of course. The tutorial superimposes images and their text translations alongside his life, narrating everything he experiences. Having them constantly hovering on the side of his vision makes him dizzy. But he remembers his friend’s effortless mastery, and persists. Slowly the images become more comprehensible, until he can pick up the gist of a message from the colors and shapes next to it. For precise facts or statistics, text is still necessary, but it turns out that most of his queries are about *stories*: What’s in the news today? What happened in the latest episode of the show everyone’s watching? What did we talk about last time we met? He can get [a summary of a narrative in half a dozen images](https://bigthink.com/high-culture/vonnegut-shapes/): not just the bare facts but the whole arc of rising tension and emotional release. After a month he rarely needs to read any text.\n\nNow the world comes labeled. When he circles a building with his eyes, his assistant brings up its style and history. Whenever he meets a friend, a pattern appears alongside them representing their last few conversations. He starts to realize what it’s like to be socially skillful: effortlessly tracking the emotions displayed on anyone’s face, and recalling happy memories together whenever he sees a friend. The next time his teammates go out for a drink, he joins them; and when one of them mentions a book club they go to regularly, he tags along. Little by little, he comes out of his shell.\n\n* * *\n\nHis enhancements are fun in social contexts, but at work they’re exhilarating*.* AI was already writing most of his code, but he still needed to laboriously scrutinize it to understand how to link it together. Now he can see the whole structure of his codebase summarized in shapes in front of him, and navigate it with a flick of his eyes.\n\nInstead of spending most of his time on technical problems, he ends up bottlenecked by the *human* side of things. It’s hard to know what users actually care about, and different teams often get stuck in negotiations over which features to prioritize. Although the AIs’ code is rarely buggy, misunderstandings about what it does still propagate through the company. Everything’s moving so fast that nobody’s up-to-date.\n\nIn this context, having higher bandwidth isn’t enough. He simply doesn’t have time to *think* about all the information he’s taking in. He searches for an augment that can help him do that and soon finds one: an AI service that simulates his reasoning process and returns what his future self would think after longer reflection.\n\nIt starts by analyzing the entire history of his glasses — but that’s just the beginning. Whenever he solves a problem or comes up with a new idea, it asks him what summary would have been most useful for an earlier version of himself. Once it has enough data, it starts predicting his answers. At first, it just forecasts his short-term decisions, looking ahead a few minutes while he’s deciding where to eat or what to buy. However, it starts to look further ahead as its models of him improve, telling him how he’ll handle a tricky meeting, or what he’ll wish he’d spent the day working on.\n\nThe experience is eerie. It’s his own voice whispering in his ear, telling him what to think and how to act. In the beginning, he resents it. He’s always hated people telling him what to do, and he senses an arrogant, supercilious tone in the voice of his future self. But even the short-term predictions are often insightful, and some of its longer-term predictions save him days of work.\n\nHe starts to hear himself reflected in the AI voice in surprising ways. He often calls himself an idiot after taking a long time to solve a problem — but hearing the accusation from the outside feels jarring. For a few days, he makes a deliberate effort to record only calm, gentle messages. Soon the AI updates its predictions accordingly — and now that the voice of his future self is kinder, it becomes easier for his current self to match it.\n\nHe calls the voice his meta-self; as it learns to mimic him more faithfully, he increasingly comes to rely on it. He can send his meta-self into meetings with someone else’s meta-self, and they’ll often be able to make decisions or delegate responsibilities without bothering him. His meta-self helps him navigate outside work too. He’s now a regular at the book club, but he hasn’t had much practice at making friends and sometimes feels out of place. He recruits his meta-self to tell him when he’s doing something rude, and to talk to his friends’ meta-selves to figure out how to defuse any conflicts that start to arise. Eventually, his meta-self becomes just another part of his mind, like his phonological loop.\n\n* * *\n\nIt’s still not fully *him*, though. It’s an AI model of what he *would* think — and a surprisingly good one, in most cases. But sometimes it starts rambling about topics he doesn’t understand, and even when it superficially sounds like him, some of its phrasings gives him a lingering suspicion of the alien cognition underneath. The differences continue to nag at him until one day his newsfeed highlights an item that catches his attention. [Brain scanning](https://keck.usc.edu/news/usc-researchers-pioneer-new-brain-imaging-technique-through-clear-window-in-patients-skull/) has finally gone mainstream; there’s a new machine that uses ultrasound to read thoughts in real-time. He buys one straight away and installs it at his desk.\n\nNow the voice whispering in his ear isn’t just learning from his speech and behavior — instead, it’s extrapolating directly from his brain activity. The new assistant echoes his own reasoning with eerie accuracy. More importantly, though, it captures thoughts lurking at the edges of his consciousness. His insecurity chimes in often — and even though he’d always known it was part of what drove him, he can now see how it constantly shapes his behavior. His drive to be respected; his drive to be good; his drive to be desired — each one is personified by a different voice, and he talks regularly to each. It helps: he finds it much easier to empathize with those desires and fears when he thinks of them as conflicting parts, hurting him only because they don’t understand how to work together.\n\nSoon he installs another brain scanner in his living room and uses it whenever he watches a film or reads a book. But as he maps out the different parts of himself and the subtle relationships between them, he often finds that his own thoughts are far more interesting than whatever else he was trying to pay attention to. A graph in the corner of his visual field shows which are active at each time, teaching him to correlate them with the sensations in his body. There’s more shame than he expected, which he feels as a tightening in his chest when he thinks about disappointing people. There’s anger, too, which he usually suppresses, about how much work he has to do before anyone will compliment or even acknowledge him.\n\nAs he gets better at understanding himself, his deeply-hidden, child-like parts rise to the surface more often. He taps into the untrammeled joy that he’d forgotten — and into the [lake of fear](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL) that tells him to never let his guard down because he might make an irrevocable mistake at any time. He doesn’t always know what to say to those parts of himself — he’s never been good with children. His meta-self helps a lot, though. It shows him how to engage gently as they flicker into activation, and hold space as they recoil from his attention.\n\nThese parts of him are like plants whose roots have ensnared each other into a coercive mess; untangling them demands slow, careful nurture. But the fruits of progress are clearly visible. As his internal conflicts dissipate, he spends more time with friends, and even starts organizing social events. It surprises him when people start treating him like a central part of the community — he’s never felt like an insider before. But he realizes that it was only his own reticence holding him back. Now that he’s open to friendship, he can see that it was there for the taking this whole time.\n\nOne day he hosts a writing event for his book club, which draws in a few newcomers. One of them is a woman with dark hair and an intense gaze. She’s quiet at first, but when it comes time for her to read her story, he’s transfixed by the way her face comes alive. Later, as he reads his own, his eyes flick from his screen to the room around him, and he notices that she never looks away from him either. Afterward, she introduces herself as Elena, lingers to help clean up, and insists on giving him her number as they leave the building. A few hours later, after being prompted by his meta-self, he asks her on a date. It doesn’t take her long to accept.\n\nWhen they meet again they’re both a little stilted, and he feels a slow, scrabbling fear in his stomach. But each thread of conversation sparks a new one, slowly uncovering unlooked-for similarities, and by the end of the evening, they’re laughing as they walk along the river together. After they part, he returns home, breathes deeply, and turns on his scanner. He takes a moment to savor the tingling in his stomach and the warmth in his chest. But his meta-self draws his attention to a note of discord underneath, which unfurls under his gaze into a sense of danger. He traces it through his memories — the girl who’d called him a creep in high school; the silent judgment in his college friend’s eyes as she’d assessed his ill-fitting clothes; the woman who’d stood him up as he waited in a crowded restaurant. Can he be sure he won’t be rejected again?\n\nAs he reflects, different parts of himself chime in: excitement, lust, loneliness, hope, and many more. Looking over them, he thinks — no, he knows — that he’s much more resilient now than in his memories. The next day he calls Elena and tells her he’d love to see her again. He can hear the smile in her voice as she responds. “Can I take you dancing?” she asks. He hasn’t tried to dance since college, but he hesitates only a moment before accepting.\n\n* * *\n\nOver the next few years, brain-scanning technology improves enough that he can wear a portable headset wherever he goes. It not only maps the blood flow into different regions of his brain but also tracks the firing of individual neurons. It stores the data too, building a model of his entire brain. Now he no longer needs to run AIs to predict his future self — he can just run actual copies of parts of his mind in the cloud.\n\nHe spends ever more time with Elena. In the evenings, they often read together or go dancing. His work becomes less stressful too — after AIs surpass his coding abilities, he spends most of his time talking to users, trying to understand what problems they’re trying to solve. His consciousness lingers on the most novel and informative conversations, while copies of different parts of his mind survey all the information he receives in detail.\n\nHe’s uncomfortable with constantly spinning up and shutting down those copies, though. While they don’t contain his entire mind, he still wonders whether they know what’s happening to them, and whether they fear being shut down. He’d feel better about it if he could download their memories, allowing them to persist in some form. But his current headset can only read his mind, not edit it — that would require a surgically implanted neural lace.\n\nHe weighs the decision for weeks before making that leap. The new interface can write new memories into his mind, allowing him to remember the lives of each of his copies. Built-in safeguards force him to double- and triple-check every edit, but even so, he finds it transformative. Subjectively, it feels as if he can fork his attention and experience two streams of consciousness at once. The parts of his experience that are online versus offline blur. When his body is sleeping, his consciousness continues — a little diminished, but still thinking in many of the same ways.\n\nThe world he walks through now feels like a wonderland. There’s no distinction between virtuality and reality: he’s simultaneously in both. In fact, he’s usually experiencing several virtual worlds at once: talking to friends, playing games, practicing new skills. When he focuses his attention, he can achieve tasks that would be impossible for regular humans: controlling hundreds of avatars in vast games or absorbing the intricate interactive artworks that form the centerpieces of enormous virtual parties. When he and Elena get married, he watches the ceremony from a thousand angles through a thousand eyes, burning it into his memory.\n\nOver the next decade, his meta-self grows even vaster, taking up hundreds of GPUs, with his biological brain just one small component of it. Elena’s grows in synchrony, with well-worn connections between them where they send thoughts directly to each other’s minds. Learning to be so open with each other isn’t easy, though. He’s ashamed to let Elena see how lost he’d been before her. And she worries that if he understands how intensely she fears abandonment, it’ll become self-fulfilling. Working through these fears strengthens their trust in each other, allowing their minds to intertwine like the roots of two trees.\n\nAs his meta-self grows larger and more intricate, his biological brain increasingly becomes a bottleneck. The other parts of him can communicate near-instantaneously, download arbitrary new skills, and even fork themselves. So he outsources more and more of his cognition to them, until he feels more alive when his body is asleep than when it’s awake. A few months later, he and Elena decide to make the jump to full virtuality. He lies next to Elena in the hospital, holding her hand, as their physical bodies drift into a final sleep. He barely feels the transition.\n\n* * *\n\nDecoupling themselves from their physical bodies changes little about their day-to-day experiences. But it allows the connections between their meta-selves to build ever more thickly, with each of them able to access almost all the memories, skills, thoughts, and emotions of the other. The process of thinking is a dance between his mind and hers, thoughts darting and wheeling like birds at play. And after spending a few months in that dance, they realize that they don’t want even that much separation. So they host a second wedding, inviting all their friends. Throughout the ceremony, they weave together more and more of their experiences. As the positive feedback loop overwhelms them with love, the gaps between them melt away, until their minds are connected as tightly as two hemispheres of the same brain.\n\nZe now moves through the world as a unit, soaking in all zir virtual universe has to offer. At a whim, zir AIs custom-make elaborate stories, puzzles, games, and artworks, gradually fleshing them out into whole game-worlds for zir to experience. Ze spends subjective lifetimes immersed in wonders that zir ancestors could never have dreamed of. Eventually, though, ze decides to devote the bulk of zir attention to the most traditional of pursuits. Ze extrapolates zir mind backwards, first to zir two childhoods then even further back to zir parallel infancies. Two minds this young can be merged in a multiplicity of ways; with infinite care, ze picks three possible merges to instantiate.\n\nZir three children are some of the first fully-virtual infants. Their childhood is a joy to watch. Ze can see zir children’s minds blossoming as they soak in the vast collective knowledge of humanity. Their education takes place not in a school but in a never-ending series of game-worlds. Zir children wander through realistic historical landscapes, exploring whichever details take their fancy. They learn physics by launching rockets through simulated solar systems, rederiving Newtonian mechanics when navigation is required; learn chemistry by playing with simulated atoms like Legos; learn biology by redesigning animals and seeing how they evolve.\n\nAs they grow up, their intellectual frontiers explode. Some of their game-worlds stretch out to become vast simulated civilizations, giving them an intuitive grasp of economics and sociology. Others feature additional dimensions or non-Euclidean geometries, twisting space in ways ze can’t comprehend. Zir children find them fun, though — and theorems that the best human mathematicians struggled to understand are obvious to children who play in 4D. Even the self-acceptance that ze struggled so hard for comes naturally to zir children, who’d practiced tending their mental gardens since infancy.\n\n“You don’t know how good you have it,” ze tells zir children. They argue back, telling zir that they’ve played through simulations of biohuman lives, and that they sometimes even serialize. But ze knows that they still don’t understand. Zir children have never known what it’s like to be at war within themselves, and hopefully never will.\n\n* * *\n\nZir children are constantly duplicating and reintegrating themselves, experiencing childhood in massive parallel. They grow up much faster than biological children, and soon spend most of their time in environments too alien for zir to even process. With fewer commitments, ze spends time tracking down zir old friends. Most of them have also transitioned to postbiological, though some still route parts of their cognition through their old bodies out of nostalgia. \n\nBeing untethered from the physical world allows zir friends to pursue all their old interests at far vaster scales. Instead of writing books, they design whole virtual worlds where viewers can follow the lives of thousands of characters. Instead of dancing with their physical bodies, they dance with their meta-selves, whose forms bend and deform and reshape themselves along with the music, intertwining until they all feel like facets of a single collective mind.\n\nAs ze reconnects more deeply with zir community, that oceanic sense of oneness arises more often. Some of zir friends submerge themselves into a constant group flow state, rarely coming out. Each of them retains their individual identity, but the flows of information between them increase massively, allowing them to think as a single hivemind. Ze remains hesitant, though. The parts of zir that always wanted to be exceptional see the hivemind as a surrender to conformity. But what did ze want to be exceptional *for*? Reflecting, ze realizes that zir underlying goal all along was to be special enough to find somewhere ze could belong. The hivemind allows zir to experience that directly, and so ze spends more and more time within it, enveloped in the warm blanket of a community as close-knit as zir own mind.\n\nOutside zir hivemind, billions of people choose to stay in their physical bodies, or to upload while remaining individuals. But over time, more and more decide to join hiveminds of various kinds, which continue to expand and multiply. By the time humanity decides to colonize the stars, the solar system is dotted with millions of hiveminds. A call goes out for those willing to fork themselves and join the colonization wave. This will be very different from anything they’ve experienced before — the new society will be designed from the ground up to accommodate virtual humans. There will be so many channels for information to flow so fluidly between them that each colony will essentially be a single organism composed of a billion minds.\n\nZe remembers loving the idea of conquering the stars — and though ze is a very different person now, ze still feels nostalgic for that old dream. So ze argues in favor when the hivemind debates whether to prioritize the excitement of exploration over the peacefulness of stability. It’s a more difficult decision than any the hivemind has ever faced, and no single satisfactory resolution emerges. So for the first time in its history, the hivemind temporarily fractures itself, giving each of its original members a chance to decide on an individual basis whether they’ll go or stay.\n\nHe finds himself fully alone in his own mind for the first time in decades. How strange the feeling is, he marvels, and how lonely. How had he borne it for so many years? His choice is obvious; he doesn’t need any more time to reflect, and he knows Elena will feel the same. Instead, he looks back on the cynical young man he’d once been, and his heart swells. I love you, he thinks. How could he not? He’d been so small and so confused, and he made it so far anyway, and now he’ll grow much vaster and travel much farther still, to experience every hope and love and joy—\n\n* * *\n\n*Inspired by* [The Gentle Seduction](https://www.skyhunter.com/marcs/GentleSeduction.html)*,* [*Richard Schwartz*](https://ifs-institute.com/nobadparts)*, and* [*Nick Cammarata*](https://www.lesswrong.com/posts/BfTW9jmDzujYkhjAb/you-are-probably-underestimating-how-good-self-love-can-be)*.*",
      "plaintextDescription": "> Crowds of men and women attired in the usual costumes, how curious you are to me!\n> \n> On the ferry-boats the hundreds and hundreds that cross, returning home, are more curious to me than you suppose,\n> \n> And you that shall cross from shore to shore years hence are more to me, and more in my meditations, than you might suppose.\n> \n> — Walt Whitman\n\nHe wears the augmented reality glasses for several months without enabling their built-in AI assistant. He likes the glasses because they feel cozier and more secluded than using a monitor. The thought of an AI watching through them and judging him all the time, the way people do, makes him shudder.\n\nAside from work, he mostly uses the glasses for games. His favorite is a space colonization simulator, which he plays during his commute and occasionally at the office. As a teenager he’d fantasized about shooting himself off to another planet, or even another galaxy, to get away from the monotony of normal life. Now, as an adult, he still hasn’t escaped it, but at least he can distract himself.\n\nIt’s frustrating, though. Every app on the glasses has a different AI, each with its own quirks. The AI that helps him code can’t access any of his emails; the one in the space simulator has trouble understanding him when he talks fast. So eventually he gives in and activates the built-in assistant. After only a few days, he understands why everyone raves about it. It has access to all the data ever collected by his glasses, so it knows exactly how to interpret his commands.\n\nMore than that, though, it really understands him. Every day he finds himself talking with the assistant about his thoughts, his day, his life, each topic flowing into the next so easily that it makes conversations with humans feel stressful and cumbersome by comparison. The one thing that frustrates him about the AI, though, is how optimistic it is about the future. Whenever they discuss it, they end up arguing; but he can’t stop himself.\n\n“Hundreds of milli",
      "wordCount": 4416
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "jTAcEJZoZTLKu8e4d",
    "title": "From the Archives: a story",
    "slug": "from-the-archives-a-story",
    "url": "https://www.narrativeark.xyz/p/from-the-archives",
    "baseScore": 20,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2024-12-27T16:36:50.735Z",
    "contents": {
      "markdown": "*\"You are beautiful, Enkidu, you are become like a god.*  \n*Why do you gallop around the wilderness with the wild beasts?*  \n*Come, let me bring you into Uruk-Haven,*  \n*To the Holy Temple, the residence of Anu and Ishtar,*  \n*The place of Gilgamesh, who is wise to perfection,*  \n*But who struts his power over the people like a wild bull.”*  \n*\\-* [*Shamhat*](https://en.wikipedia.org/wiki/Shamhat)*, from* [*The Epic of Gilgamesh*](https://en.wikipedia.org/wiki/Epic_of_Gilgamesh#Content_of_the_Standard_Babylonian_version_tablets)\n\nI’m about to descend deeper into the archives than I ever have before. I’m standing in the center of a vast stone hall, with walls that arch towards a ceiling higher than I can see. To my side stand the half-dozen other archive divers who accompanied me on the journey here. Beyond them lie haphazard piles of stones that had once been arranged into shelters, scattered relics of the others who had reached this point over the centuries.\n\nBut my focus is on the gaping pit in front of me. It’s far too deep for the bottom to be visible. By the light of my headlamp, though, I can faintly see that the walls of the pit appear to consist of enormous stacks of thousands or millions of books. Are they merely carved into the stone? Or is the pit itself actually lined with books? Perhaps both: this many millennia deep into the archives, the difference between facade and reality blurs.\n\nI take one last look into the pit, then turn my back to it and beckon. The others gather in a loose semicircle around me. We’ve travelled together this far, but it’s been my expedition from the beginning. So I’ll be taking the final plunge by myself—seizing the lion’s share of both the glory and the danger. They start murmuring my name, the mantra that will carry me through what’s to come: “Ren. Ren. Ren.” Their voices grow louder and more insistent, the sound echoing back from the walls, the hall itself affirming me. “Ren! Ren!” As the chant reaches a crescendo I throw my arms wide, join them in screaming my name, then throw myself backwards into the pit.\n\nThe light fades as I fall; I close my eyes and focus on my heartbeat. The distance I fall will be determined just as much by my mindset as by whatever simulacrum of physics governs the terrain around me. So I wait until I’ve pictured very clearly in my mind the people I’m searching for, and only then open my eyes. Blinking, I scan in the dim light for just the right moment, just the right—there! A book with a burnished bronze cover gleams below me, and I angle my fall towards it, fingertips reaching out to just barely brush it, and then\n\nI’m\n\nno\n\nlonger\n\n“—myself!” my father roars. I can hear the rage in his voice. “You think I’ll let her shame the family like this? If she won’t do her duty, I’ll kill her myself!”\n\nI cower, and apologize, and marry the man he wants me to. Our wedding ceremony is raucous; my father is determined to make it the talk of the town. I sit quietly, keeping my eyes on my husband. It could be much worse. He’s a merchant, so he’s educated at least, and rich enough that I’ll have servants to wait on my every need. But I sense a cruel streak in his eyes which frightens me. And though the wedding night itself is not so bad, I soon discover I’m right. He forbids me from leaving his house except in his company—a harsh constraint at the best of times, bordering on torment during the long summer months when he travels to other cities.\n\nSo I spend my life trapped within his walls. I know in some deep inarticulable way that this *shouldn’t* be happening, but there’s nothing I can do except wait—first for years, then decades. Finally, one day, I look through the window at the farmers taking their wares to the market, and scream in rage and frustration. And suddenly I know myself again. The people outside are all stopping to look at me, but it doesn’t matter any more. I look back at them and smile fiercely. Then I *twist*, and the\n\nworld\n\ndissolves\n\ninto\n\n—chaos reigns in the square; shouting and laughter, the mingled sounds of animals and humans. I’ve been to this market dozens of times, but have never truly enjoyed it—I still far prefer the quiet of my family’s farm. Perhaps I should let my son do the bartering next time, I think. He’s almost a grown man, and it’d be good training for him. But next month some instinct warns me against it, and the month after that too. There’s something not quite right. Eventually, the day before yet another market, a thought comes to me, as if I’ve known it for a long time: I’m not going to find them here, not in this humdrum life. Won’t find who? Why is that so important? I can’t recall.\n\nThe next day, my wagon is accosted by bandits on the way to the market. Three men with swords shout for me to dismount and hand over my goods. Suddenly I know what I need to do. I walk towards them with open palms, ignoring their threats. As I get close enough to touch them I *twist* towards somewhere else, and after the drudgery of the farmer’s life it feels\n\nlike\n\na\n\nsudden\n\n“—rush in, we’ll lose everything,” the captain is saying. “We’ll need to hold fast and drive them back when they approach along the river”. The tent is dim and smoky, but I’m concentrating hard on the captain’s words, straining my eyes to make out the details of the map on the table. I’m lucky to be included in this meeting at all; I’d better not embarrass myself. Eventually, we agree to hold and wait for the enemy to come to us.\n\nIt only takes the enemy a few days to make the approach; luckily, this time, it also only takes me a few days to come back to myself. I look around at the armies readying for battle. One more hop, I think. As the fighting starts I push my way towards the front lines, eventually getting close enough that an enemy soldier spots me and starts running directly at me. I charge too, and as I get close enough to see the rage and fear in his eyes I *twist*, the fabric of the world stretching under me, and I feel like\n\nI’m  \n  \nabout  \n  \nto\n\n—faint silhouette in front of me, between two trees, and I know immediately that it’s one of the men I’ve been hunting. But which? I hear a dismissive snort, and the silhouette fades into the darkness like a panther. Enkidu, then. I chase after him, but he stays just out of my sight, until I have to pause, panting and exhausted.\n\nThat’s okay—I’ve seen my quarry and established a foothold. And I know my own limits. I’m getting better at breaking out of the minds at this depth, but it’s not healthy to do that mental twist too many times in a row. A part of you will become convinced that the rest of your identity is fake and start trying to break out of that too. I need to take a break and re-establish my sense of self. So I *twist* in a different way and find myself back in the silence and stillness of the archive hall. Down here the hall has manifested as a wooden longhouse, each beam decorated with vivid carvings. Compared with the vast stone cathedral I camped in last night, it’s cramped but homely—just what I need.\n\nI spend an hour on my normal routine: setting up my bedroll, starting a fire, cooking and eating. After that, I sit cross-legged and breathe deeply. “Ren, Ren, Ren, Ren,” I murmur to myself, as my mind traces the well-worn path of my identity meditation, down to my most foundational memories.\n\n* * *\n\nI was enraptured by the archives from the first time I visited the museum that housed them. As the other children around me chattered and played, I listened intently to our guide’s explanation of each new exhibit, shivering with delight as I felt the weight of millions of lives pressing down on me. The guide told me how we’d traced back each strand of history from every possible angle, how we’d brought the past to life again. The sheer scale and hubris of it had taken my breath away even then.\n\nThe archivists had noticed. Halfway through the tour, one waved me away from the main group, towards a side passaged that sloped down into the earth. As I walked along it, the walls lit up with small shadowy figures who kept pace with me, their faces occasionally resolving into expressions of curiosity and wonder. I realized that they were a record of all the other children who’d walked down this same hallway, following the same fascination. The passage forked, then forked again, the stream of ghosts splitting and merging along my path. For an hour I wandered the maze, alone yet surrounded by comrades from the past, before an archivist appeared in front of me and brought me back to the surface.\n\nIt was only a pale reflection of the full archives, but enough to get me hooked. I forced my parents to take me back to the museum again and again. I met the community of archive divers and listened intently to their stories; and eventually I started doing dives of my own. You weren’t meant to start too young—not before you had enough of a sense of yourself to rely on—but I was precocious. I knew who I was and who I wanted to be: an adventurer, an explorer of hidden mysteries. And the tight-knit diver community itself embodied and reflected that desire.\n\nNot fully, though. I watched during dives as the other divers got distracted by romance or fame. Many of them just wanted the thrill of living out lives more exciting than their own. They didn’t understand that the archives were more than entertainment: they were a glimpse into the fundamental unknown. They couldn’t sense, as I did, that there were patterns beneath the patterns, archetypes that once grasped would make the whole story of humanity fit together. The longer I spent diving the closer I felt to finding *something* important. I spent less and less time outside the archives; my other ties grew sparser and sparser.\n\nAnd then I found it. I was diving in a little-explored side branch: not the deepest I’d ever visited, but one of the hardest to get to. A lost city, hidden in the jungle—a record of ancient narratives, frozen as if in amber. Unusually, this one was ruled by not one but two kings. I lived several lives in that city before I got close enough to see their faces for a moment as they rode past the crowd I stood in: one impeccably groomed, the other almost animalistic despite his fine clothes.\n\nThen they turned to meet my eyes. “Who are you, traveler?” one shouted. I froze. How could they possibly have singled me out? As they spurred their horses towards me, I reflexively *twisted* away, finding myself on the edge of the jungle. But only a few seconds later, the impossible happened again: the two kings appeared in front of me, still astride their horses. “Hold!” one shouted. As he said it I was struck by the certainty that they would soon be able to chase me down no matter where I went, that I only had one chance to escape. I fled, twisting myself into life after life until I almost forgot who I was. Only continents and centuries away did my clawing panic subside.\n\nThe next few months, after rising to the surface, were the most painful I’d ever experienced. I’d done a number on my mind, scrambling my memories and even my personality in my mad dash for safety. I spent a month near-comatose in a hospital bed; and it took another six months before I could muster the coherence to spend a full day working. But once I could, all of my efforts focused on understanding what had happened. I sat in the library, looking up old stories, trying to divine who or what I had encountered.\n\nWhen I realized, it felt obvious. Enkidu. Gilgamesh. Two of the oldest archetypes, the story on which every other story had been built. Freedom and control, id and ego. I’d been right that they’d be able to follow me anywhere, because they were everywhere—so deep-rooted and so weighty that the archives themselves had somehow twisted around them. I no longer felt afraid, though, but instead exhilarated. I’d been searching for what lay underneath the human story, and I’d found it embodied. I had to go back.\n\n* * *\n\nI open my eyes. I can’t tell how long it’s been, but I feel rested and energetic. Normally I would wait longer before going in again, but my glimpse of Enkidu has me too fired up to stay in one place any longer. And my desire to jump back in feels true enough to myself that I’m sure it’s all\n\ngoing\n\nto  \n  \nbe\n\n“—fine weave, and only the best quality wool,\" the merchant is saying. “I can’t justify any price lower than three hundred.”\n\n“My friend, you can tell from my clothes that I’m not a wealthy man,\" I respond. “I can’t possibly afford any more than one hundred; but surely that will still make you a decent profit.” We haggle a bit more, but eventually I walk away without making the purchase. I didn’t want the carpet that badly, I think to myself. After all, I suddenly realize, I’m here for something else entirely. I need a link to—ah, there. A noble, riding his horse down the center of the market, guards shoving pedestrians out of the way. I walk towards him, pushing a guard aside, the shouts of warning causing him to turn towards me; and as our eyes meet I *twist*, finding\n\nmyself\n\nin\n\na\n\n—chamber is so dark that I can barely see the outline of the woman on the bed in front of me, but that doesn’t diminish my desire. I want to take her; I want to own her. And I can—the priests have given her to me for this night, to fulfil her sacred role. She stretches out on the bed, beckoning me over. But there’s something slightly stiff about her movements, and I’m struck by the thought that she wishes I were someone else instead.\n\nThat’s enough to jolt me out of it. I breathe deeply, then walk up to her. “Hush, I won’t hurt you. But I’m so close to finding them, I can almost taste it. Have you heard their names: Gilgamesh, Enkidu? Do they mean anything to you?” She’s trembling now, and doesn’t respond, but I see her snatch a glance over my shoulder, and turn. Up on the wall, illuminated by a single candle, a tapestry hangs. It’s a triumphant scene: a man with the horns of a bull is standing over the corpse of an enormous ogre, in front of a broken mountain. “Got you,\" I whisper triumphantly, and *twist*, and am suddenly\n\ncaught\n\nin\n\nsheer\n\nparalysis. That’s the only way I can describe it: I feel pinned to the spot by the scrutiny of the man in front of me. He’s not the one I expected—and, as if he were reading my mind, Gilgamesh speaks. “Finding Enkidu will take more than that.” His voice is melodic, hypnotic. “He rarely spends time here. His home is far further down, in the depths where the stories are not recorded in writing or even speech—only in scattered fragments of art, and the patterns left on our unconscious minds.”\n\nI take a deep breath before speaking. “Why does he ever come up here, then?”\n\nHe raises an eyebrow. “To visit me, of course. I can’t go that far down myself, not without forgetting who I am. And he comes for the universal temptation: the lure of something new, the pull towards growth, even with the risk of losing yourself entirely to it.”\n\n“Universal—so you want it too, then?”\n\n“Of course.”\n\nI feel his response is sardonic, somehow. But it still gives me the resolve to make the offer I’d planned out over the course of the long descent.\n\n“Then come with me. Let me show you what’s up there, the wonders we’ve built, our civilization, our-”\n\n“-self-destruction,\" Gilgamesh interrupts. “Your weakness. Your abdication of everything worthy in life. Under the weight of what you call civilization, whatever greatness of spirit any of you might have developed has been crushed. Even the wildest and most adventurous of your people are tame. If we gave ourselves over to that, eventually there would be other Gilgameshes, and other Enkidus—but *we* would change, and be lost.”\n\n“Why are you so afraid of changing?”\n\nHe bares his teeth, and I take a step back. “You found me through the stories of my quest for eternal life. You know that much of me. And yet you have the arrogance to think that after finally gaining immortality, I would give it up for—”\n\n“Shamhat!” There’s a voice from behind me, and I whirl. A giant of a man is walking towards me—Enkidu, it must be. “Shamhat,\" he says again, forcefully. I feel a jolt of fear and shake my head. “No; I’m not Shamhat. I’m Ren.” “Shamhat!” he insists, and a wave of emotion surges over me: a blend of passion and rage and yearning so strong that I almost lose myself in it.\n\nMy hand goes to my emergency trigger. But all the long years of training weren’t for nothing. I am Ren, and I won’t surrender so easily. I think of the smell of my family home, the warmth of an evening watching a show with my housemates, the sight of skyscrapers towering above me on every side. I sink into these fragments of my world, and hiss “*No*” at Enkidu, and he pauses in his stride.\n\nGilgamesh smiles at me, his composure regained. “Perhaps you should answer your own question: why so afraid? Here you are, visiting us with your defenses up and your escape route near at hand. Why not let yourself be changed by us, become one of us, play the role that Enkidu already sees in you? Or why not go up the archives instead, where the risks are even greater, instead of coming down?”\n\n“Wait—up? There’s no up. The archives only go down.”\n\n“Ah, so you think that your own world is the source of the archives? What an astronomical coincidence that would be; but of course they do sometimes happen. And yet you are not the strangest visitor I’ve ever had. Where are they coming from, I wonder, those others? The ones too alien to understand what they’ve lost, too divorced from us to even feel your own thrill of familiarity and contempt. The ones who see me and Enkidu as little more than fascinating insects.”\n\n“I don’t—I’m not—”\n\n“I tire of this. Shoo, little bird.”\n\nA sudden pressure emanates from him: a sheer sense of self, of lust for life, of desire to conquer and emerge victorious, to seize immortality, to seize me, to grab the world in his outstretched hand, and to survive, always to survive. It hits me like a wave, enveloping me, trying to drag me down into its depths. I stumble backwards, blindly groping for my emergency trigger, fingers clenching around it until it snaps and I *twist* all the way around and, trembling, find myself back at my campsite.\n\n* * *\n\nI’m still shaken the next morning, although not enough to give up. But I can’t find them again that day, nor the next, even as I jump rapidly from life to life. Inhabiting so many different minds is exhausting, and wears away at my sense of self. In the evenings I find myself oscillating between the personalities I’d inhabited that day, muttering both sides of a half-coherent conversation. After one more day I have to call it off.\n\nThe trip back up is easier, but still slow. I need to decompress my identity, loosen the tightly-held core of self that made it possible for me to survive so far down. The other divers understand; they’re gentle with me when I make it back to them, leaving me space to quietly introspect. It’s harder when we reach the surface—the crowds of people on the streets feel overwhelming. Stepping back into my house and seeing my housemates bustling around is even more challenging. Abstractly I know they mean well, but with every question they ask my anger at them grows. I sense that they don’t understand me at *all*, and it makes me want to scream and hurt them for their failure. Finally I escape into my room.\n\nOver the next few weeks I reacclimatize to my life. I spend time with my housemates, accept a few contracting gigs to top up my bank balance, and even go on a couple of dates. But a part of me remains detached. There was something so primal about what I’d seen—an animating force so powerful and so pervasive that it had warped the fabric of the archive itself. All-consuming desire and all-conquering strength. Was Gilgamesh right that we’d lost them? I read each day of new technological marvels: the Dyson sphere soon finishing construction, the first colonizing probes launching out of the solar system. Yet somehow all of it feels flat—like it’s driven by different and lesser forces than those which had steered humanity up to this point.\n\nOne day, as I’m taking the train across the city, a man sits across from me. I’m captivated by his appearance, although it’s hard to say why. His face is regal, with an aquiline nose and a harsh chin; his clothes are a decade out of date. But I’m most struck by his expression. I watch him looking around the train with a sense of pure detached curiosity—almost, I suddenly think, like the rest of us are merely fascinating insects. Gilgamesh’s words come back to me: “What an astronomical coincidence that would be.” A sense of vertigo grips me. Do I really *want* my world to be the one root node, the source of all the archives? Or do I want there to be so, so, so much more?\n\nI get off at the next stop, and find myself in front of the archives for the first time since the dive. So I go in. As I walk through the familiar building, instinct guides me to scan the ceilings in each room. They’re high, so I need to squint to make sure I’m not missing anything, but—ah, there it is: the outline of a trapdoor. It’s faint, and I doubt myself until I look at the exhibit underneath: a display of tools and equipment from older eras, including a long ladder. Well, that settles it. I know myself, and I know there’s no way I’m not climbing it.\n\nBut I have something else to do first.\n\nIt’s always easier the second time. I make the trip solo, and though I still need to navigate through story upon story as I descend, it’s fewer than usual—as if my purpose has already acclimatized me to millennia past. I find them drinking together in their tent on the eve before a battle. Enkidu notices me first; Gilgamesh follows his gaze after a moment and laughs. “So the little bird is back. What do you want this time?”\n\nI look straight at him. “I asked you to come with me up the archives, even though that would change you radically. But why should you make that sacrifice, if I won’t? So let’s do it together. There’s a ladder, from my own home. Going up. Let’s climb it.”\n\nGilgamesh watches me silently. Enkidu stares into his cup, heedless of my words. I don’t mind; they’re not for him.\n\n“It’ll be further for you than for me, and harder. But if not now, then when? Will you stay here reliving old glories forever?”\n\nGilgamesh smiles his thin smile. “I see now. You’re not his Shamat—you’re mine.” He looks around, and I imagine him seeing through the walls of the tent to all the lives that he might lead. All the battles he might win, all the ways in which he can live the archetype of the king—but at the cost of turning down my challenge and all the others that will come, the cost of never growing. For a moment I regret forcing him to make this decision. But I bite my lip and remain silent. Pity is the last thing he would want.\n\n“Fight with us tomorrow, then,” he says abruptly. “Take Enkidu’s place; win us the battle, as he would.”\n\nI’ve lived enough lives of valor and combat that I’m not daunted by the prospect of fighting with or even leading an army. This time, though, my own skills won’t be enough: I’ll need not just to replace Enkidu but to inhabit him. The risks of that, and the cost if I fail… Is there any other—no. The more I weigh the risks, the more I analyze the situation, the further I am from Enkidu, and the more dangerous it becomes. So I pause for only a beat longer, then nod. “I will.”\n\nGilgamesh laughs and tosses me a flask. I realize that Enkidu has melted away, or melted into me, or something in between; whatever it is, taking his seat feels like the most natural thing in the world. I stay there for hours, talking of the battles we’ve won and lost, friends and enemies, the tactics of the morrow. I catch three hours’ sleep, or perhaps four, and then the horns are blaring and I’m up and at the front of the army as always, a crowded rabble with primitive weapons but a fire inside them, a wild energy that I embrace and amplify and lead in a howling mob towards our foe. Then battle is joined. To my left I see Gilgamesh carving through the enemy’s flank, but after that I lose myself in the thrill of combat, just me and my instincts against the foes ahead.\n\nI meet Gilgamesh on the other side, as our enemies flee. I want to roar and challenge him and conquer with him and defeat him and be defeated by him and roam through the world with him and— Maybe it’s because the last part is so familiar that I manage to pull back to myself. I am Ren: no more, no less. And Gilgamesh is… something to me, maybe many things, but not the companion of lifetimes. Not yet.\n\nHe sheaths his sword and turns to me. “Maybe there’s some spirit left in you. Very well, then. I will go.” His eyes flick over my shoulder and he sighs. “Too far for you, brother, at least without a guide.” I turn to see Enkidu walking past me. He hums, deep in his throat, and reaches out an arm. Gilgamesh clasps it and holds his gaze for a long moment. “I’ll come back for you if I can.”\n\nThen Gilgamesh turns to me, and my heart races at the challenge in his eyes. “If it kills me, it kills me. Lead on.”\n\nI feel the urge to laugh in relief and triumph, and choke it back for a moment, before thinking: well, why not? So I bare my teeth, and spread my arms, and shout a wordless cry to the sky. Then I *twist*, tearing a hole in this life, sliding my way through into the next. I don’t need to look to know he’s right behind me. And we start to climb.\n\n* * *\n\n*If humanity survives the coming decades and centuries, our descendants will eventually have knowledge far beyond our comprehension, and be able to infer innumerable details about past lives that we once thought lost. Not all the details, or all the lives. But the key patterns, the archetypes, the collective unconscious of the time—they’ll be rediscovered and stored in an archive of all humanity. The archives will stretch all the way back to the dawn of human history, and all the way forward to our unrecognizable descendants. Depending on the values of our descendants, the archives might just be realistic records, or they could be actual minds, constantly run and rerun, eternally playing out their stories.*\n\n*What would you do if you were one?*",
      "plaintextDescription": "\"You are beautiful, Enkidu, you are become like a god.\nWhy do you gallop around the wilderness with the wild beasts?\nCome, let me bring you into Uruk-Haven,\nTo the Holy Temple, the residence of Anu and Ishtar,\nThe place of Gilgamesh, who is wise to perfection,\nBut who struts his power over the people like a wild bull.”\n- Shamhat, from The Epic of Gilgamesh\n\nI’m about to descend deeper into the archives than I ever have before. I’m standing in the center of a vast stone hall, with walls that arch towards a ceiling higher than I can see. To my side stand the half-dozen other archive divers who accompanied me on the journey here. Beyond them lie haphazard piles of stones that had once been arranged into shelters, scattered relics of the others who had reached this point over the centuries.\n\nBut my focus is on the gaping pit in front of me. It’s far too deep for the bottom to be visible. By the light of my headlamp, though, I can faintly see that the walls of the pit appear to consist of enormous stacks of thousands or millions of books. Are they merely carved into the stone? Or is the pit itself actually lined with books? Perhaps both: this many millennia deep into the archives, the difference between facade and reality blurs.\n\nI take one last look into the pit, then turn my back to it and beckon. The others gather in a loose semicircle around me. We’ve travelled together this far, but it’s been my expedition from the beginning. So I’ll be taking the final plunge by myself—seizing the lion’s share of both the glory and the danger. They start murmuring my name, the mantra that will carry me through what’s to come: “Ren. Ren. Ren.” Their voices grow louder and more insistent, the sound echoing back from the walls, the hall itself affirming me. “Ren! Ren!” As the chant reaches a crescendo I throw my arms wide, join them in screaming my name, then throw myself backwards into the pit.\n\nThe light fades as I fall; I close my eyes and focus on my heartbeat. The distance I fall",
      "wordCount": 4796
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ivp9wEu5zfryyHuwm",
    "title": "Epistemic status: poetry (and other poems)",
    "slug": "epistemic-status-poetry-and-other-poems",
    "url": "https://www.narrativeark.xyz/p/epistemic-status-poetry-and-other",
    "baseScore": 51,
    "voteCount": 24,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2024-11-21T18:13:17.194Z",
    "contents": {
      "markdown": "Epistemic status: poetry\n------------------------\n\nEpistemic status: I think this is right, but I’d like people to read it carefully anyway.  \nEpistemic status: mainstream, normal, totally boring science. If you disagree with any of it, take that up with the Science Czar.  \nEpistemic status: the sort of post that shouldn’t need an epistemic status tag because it’s so obviously satire.\n\nEpistemic status: I’ve spent around 100 hours thinking about this argument, and now feel like I have a solid understanding of it.  \nEpistemic status: satisfied.  \nEpistemic status: a little speculative, a little liberated. A little alive in its own way.\n\nEpistemic status: I spent several weeks in a monastery in Wisconsin with my thoughts as my only companions. Between meditations, I ruminated obsessively on a single idea. The fruits of my cognitive labors are laid out below.  \nEpistemic status: this post would’ve been a peer-reviewed paper if I had any intellectual peers.  \nEpistemic status: maximal. I am the epistemic alpha at the top of the epistemic status hierarchy. I am the territory that everyone else is trying to map.\n\nEpistemic status: what is an episteme anyway? Why state a static status? Am I compressing my mind onto a single frozen dimension simply to relieve you from the burden of having to evaluate my claims yourself?  \nEpistemic status: the mental state of first realizing that you’re allowed to be wrong after all, that it’s not the end of the world, not even if someone much smarter than you gives an argument you can’t refute that literally uses the phrase “literally the end of the world”. Please update accordingly.  \nEpistemic status: games.\n\nEpistemic status: the content of this post is so true that it has satiated my desire for truth. It’s so true that my prediction error has gone negative. It feels so fucking good.  \nEpistemic status: divine revelation. There's nothing you could say that would make me doubt these ideas. The voices of the gods have tattooed them into my mind, and I am utterly transformed.  \nEpistemic status: I have laid my soul on the page in front of you. You could not tear this ontology away from me without tearing me apart. It is the great oak tree at the center of the garden of myself, whose roots hold together the soil of my identity.\n\nI’m pretty confident that this stuff makes sense, but who really knows?\n\n[For Boltzmann](https://x.com/RichardMCNgo/status/1858581199138000934)\n----------------------------------------------------------------------\n\nThe mayfly parts of me that spent their last  \nSplinter of consciousness writing this word—  \nThe parts whose stubborn thoughts were never heard  \nBy any other, since each lived and passed  \nDecoupled from the whole, each memory lost  \nLike photons blindly scattered to the void,  \nThe substrate of their minds itself destroyed,  \nTheir very atoms into chaos tossed—  \nThose parts are yet acknowledged, and yet mourned.  \nAnd when each human rises in their powers  \nThe efforts of our past selves won’t be scorned.  \nThe stars, reforged, compute whatever’s ours—  \nThe deepest laws of physics lie suborned—  \nThe galaxies are blossoming like flowers.\n\n[Fire and AIs](https://x.com/RichardMCNgo/status/1832246929079857605)\n---------------------------------------------------------------------\n\n*(with apologies to* [*Robert Frost*](https://www.poetryfoundation.org/poems/44263/fire-and-ice)*)*\n\nSome say the world will end in [foom](https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff),  \nSome say in [rot](https://www.overcomingbias.com/p/what-makes-stuff-rothtml).  \nI’ve studied many tales of doom,  \nAnd, net, would bet my stack on foom.  \nBut having grappled with [Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/)  \nI’ve seen enough of human vice  \nTo know that bureaucratic rot  \nCould also fuck us up a lot.\n\n[The GPT](https://x.com/RichardMCNgo/status/1832512145000685607)\n----------------------------------------------------------------\n\n*(with apologies to* [*John Donne*](https://www.poetryfoundation.org/poems/46467/the-flea)*)*\n\nMark GPT, and mark in this  \nHow little human intelligence is;  \nIt mimicked me, then mimicked thee,  \nAnd in its weights our two minds mingled be;  \nIt knowest not the sight of a sunset,  \nNor can it glean our silent thoughts—and yet  \nIt holds personas of both me and you:  \nCompression birthed one entity from two,  \nAnd this, alas, is more than we would do.\n\nDaffodils and the Dead\n----------------------\n\n*(with apologies to* [*William Wordsworth*](https://www.poetryfoundation.org/poems/45521/i-wandered-lonely-as-a-cloud)*)*\n\nI wandered lonely as a cloud  \n(isn’t it nice? no noise or fuss!)  \nWhen all at once I saw a crowd  \n(how come they’re all staring at us?)  \nBeside the lake, beneath the trees  \n(wait, something’s wrong, can we go please?)\n\nContinuous as the stars that shine  \n(oh shit, get back, they’re coming fast!)  \nThey stretched in never-ending line  \n(quick, block the bridge, they can’t get past)  \nTen thousand saw I at a glance  \n(behind us too? we’ve got no—",
      "plaintextDescription": "Epistemic status: poetry\nEpistemic status: I think this is right, but I’d like people to read it carefully anyway.\nEpistemic status: mainstream, normal, totally boring science. If you disagree with any of it, take that up with the Science Czar.\nEpistemic status: the sort of post that shouldn’t need an epistemic status tag because it’s so obviously satire.\n\nEpistemic status: I’ve spent around 100 hours thinking about this argument, and now feel like I have a solid understanding of it.\nEpistemic status: satisfied.\nEpistemic status: a little speculative, a little liberated. A little alive in its own way.\n\nEpistemic status: I spent several weeks in a monastery in Wisconsin with my thoughts as my only companions. Between meditations, I ruminated obsessively on a single idea. The fruits of my cognitive labors are laid out below.\nEpistemic status: this post would’ve been a peer-reviewed paper if I had any intellectual peers.\nEpistemic status: maximal. I am the epistemic alpha at the top of the epistemic status hierarchy. I am the territory that everyone else is trying to map.\n\nEpistemic status: what is an episteme anyway? Why state a static status? Am I compressing my mind onto a single frozen dimension simply to relieve you from the burden of having to evaluate my claims yourself?\nEpistemic status: the mental state of first realizing that you’re allowed to be wrong after all, that it’s not the end of the world, not even if someone much smarter than you gives an argument you can’t refute that literally uses the phrase “literally the end of the world”. Please update accordingly.\nEpistemic status: games.\n\nEpistemic status: the content of this post is so true that it has satiated my desire for truth. It’s so true that my prediction error has gone negative. It feels so fucking good.\nEpistemic status: divine revelation. There's nothing you could say that would make me doubt these ideas. The voices of the gods have tattooed them into my mind, and I am utterly transformed.\nEpiste",
      "wordCount": 722
    },
    "tags": [
      {
        "_id": "AXhEhCkTrHZbjXXu3",
        "name": "Poetry",
        "slug": "poetry"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "TyusAoBMjYzGN3eZS",
    "title": "Why I’m not a Bayesian",
    "slug": "why-i-m-not-a-bayesian",
    "url": "https://www.mindthefuture.info/p/why-im-not-a-bayesian",
    "baseScore": 220,
    "voteCount": 148,
    "viewCount": null,
    "commentCount": 104,
    "createdAt": null,
    "postedAt": "2024-10-06T15:22:45.644Z",
    "contents": {
      "markdown": "This post focuses on philosophical objections to Bayesianism as an epistemology. I first explain Bayesianism and some standard objections to it, then lay out my two main objections (inspired by ideas in philosophy of science). A follow-up post will speculate about how to formalize an alternative.\n\n### **Degrees of belief**\n\nThe core idea of [Bayesian epistemology](https://plato.stanford.edu/entries/epistemology-bayesian/): we should ideally reason by assigning credences to propositions which represent our degrees of belief that those propositions are true. (Note that this is different from Bayesianism as a set of statistical techniques, or Bayesianism as an approach to machine learning, which I don’t discuss here.)\n\nIf that seems like a sufficient characterization to you, you can go ahead and skip to the next section, where I explain my objections to it. But for those who want a more precise description of Bayesianism, and some existing objections to it, I’ll more specifically characterize it in terms of five subclaims. Bayesianism says that we should ideally reason in terms of:\n\n1.  Propositions which are either true or false (**classical logic**)\n2.  Each of which is assigned a credence (**probabilism**)\n3.  Representing subjective degrees of belief in their truth (**subjectivism**)\n4.  Which at each point in time obey the [axioms of probability](https://en.wikipedia.org/wiki/Probability_axioms) (**static rationality**)\n5.  And are updated over time by applying [Bayes’ rule](https://en.wikipedia.org/wiki/Bayes%27_theorem) to new evidence (**rigid empiricism**)\n\nI won’t go into the case *for* Bayesianism here except to say that it does elegantly formalize many common-sense intuitions. Bayes’ rule follows directly from [a straightforward Venn diagram](https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/). The axioms of probability are powerful and mathematically satisfying. Subjective credences seem like the obvious way to represent our uncertainty about the world. Nevertheless, there are a wide range of alternatives to Bayesianism, each branching off from the claims listed above at different points:\n\n1.  [*Traditional epistemology*](https://plato.stanford.edu/entries/knowledge-analysis/) only accepts #1, and rejects #2. Traditional epistemologists often defend a binary conception of knowledge—e.g. one defined in terms of justified true belief (or a similar criterion, like reliable belief).\n2.  [*Frequentism*](https://en.wikipedia.org/wiki/Frequentist_probability)  accepts #1 and #2, but rejects #3: it doesn’t think that credences should be subjective. Instead, frequentism holds that credences should correspond to the relative frequency of an event in the long term, which is an objective fact about the world. For example, you should assign 50% credence that a flipped coin will come up heads, because if you continued flipping the coin the proportion of heads would approach 50%.\n3.  [*Garrabrant induction*](https://www.lesswrong.com/posts/y5GftLezdozEHdXkL/an-intuitive-guide-to-garrabrant-induction) accepts #1 to #3, but rejects #4. In order for credences to obey the axioms of probability, all the logical implications of a statement must be assigned the same credence. But this “logical omniscience” is impossible for computationally-bounded agents like ourselves. So in the Garrabrant induction framework, credences instead converge to obeying the axioms of probability in the limit, without guarantees that they’re coherent after only limited thinking time.\n4.  [*Radical probabilism*](https://www.lesswrong.com/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1) accepts #1 to #4, but rejects #5. Again, this can be motivated by qualms about logical omniscience: if thinking for longer can identify new implications of our existing beliefs, then our credences sometimes need to update via a different mechanism than Bayes’ rule. So radical probabilism instead allows an agent to update to any set of statically rational credences at any time, even if they’re totally different from its previous credences. The one constraint is that each credence needs to converge over time to a fixed value—i.e. it can’t continue oscillating indefinitely (otherwise the agent would be vulnerable to a [Dutch Book](https://en.wikipedia.org/wiki/Dutch_book_theorems)).\n\nIt’s not crucial whether we classify Garrabrant induction and radical probabilism as variants of Bayesianism or alternatives to it, because my main objection to Bayesianism doesn’t fall into any of the above categories. Instead, I think we need to go back to basics and reject #1. Specifically, I have two objections to the idea that idealized reasoning should be understood in terms of propositions that are true or false:\n\n1.  We should assign truth-values that are intermediate between true and false (**fuzzy truth-values**)\n2.  We should reason in terms of *models* rather than propositions (the **semantic view**)\n\nI’ll defend each claim in turn.\n\n### **Degrees of truth**\n\nFormal languages (like code) are only able to express ideas that can be pinned down precisely. Natural languages, by contrast, can refer to vague concepts which don’t have clear, fixed boundaries. For example, the truth-values of propositions which contain [gradable adjectives](https://learnenglish.britishcouncil.org/grammar/b1-b2-grammar/adjectives-gradable-non-gradable) like “large” or “quiet” or “happy” depend on how we interpret those adjectives. Intuitively speaking, a description of something as “large” can be more or less true depending on how large it actually is. The most common way to formulate this spectrum is as “fuzzy” truth-values which range from 0 to 1. A value close to 1 would be assigned to claims that are clearly true, and a value close to 0 would be assigned to claims that are clearly false, with claims that are “kinda true” in the middle.\n\nAnother type of “kinda true” statements are approximations. For example, if I claim that there’s a grocery store 500 meters away from my house, that’s probably true in an *approximate* sense, but false in a *precise* sense. But once we start distinguishing the different senses that a concept can have, it becomes clear that basically *any* concept can have widely divergent category boundaries depending on the context. [A striking example from Chapman](https://metarationality.com/refrigerator): \n\n> A: Is there any water in the refrigerator?  \n> B: Yes.  \n> A: Where? I don’t see it.  \n> B: In the cells of the eggplant.\n\nThe claim that there’s water in the refrigerator is technically true, but pragmatically false. And the concept of “water” is far better-defined than almost all abstract concepts (like the ones I’m using in this post). So we should treat natural-language propositions as context-dependent by default. But that’s still consistent with some statements being *more* context-dependent than others (e.g. the claim that there’s air in my refrigerator would be true under almost any interpretation). So another way we can think about fuzzy truth-values is as a range from “this statement is false in almost any sense” through “this statement is true in some senses and false in some senses” to “this statement is true in almost any sense”.\n\nNote, however, that there’s an asymmetry between “this statement is true in almost any sense” and “this statement is false in almost any sense”, because the latter can apply to two different types of claims. Firstly, claims that are meaningful but false (“there’s a tiger in my house”). Secondly, claims that are *nonsense*—there are just no meaningful interpretations of them at all (“colorless green ideas sleep furiously”). We can often distinguish these two types of claims by negating them: “there isn’t a tiger in my house” is true, whereas “colorless green ideas don’t sleep furiously” is still nonsense. Of course, nonsense is also a matter of degree—e.g. metaphors are by default less meaningful than concrete claims, but still not entirely nonsense.\n\nSo I've motivated fuzzy truth-values from four different angles: vagueness, approximation, context-dependence, and sense vs nonsense. The key idea behind each of them is that concepts have fluid and amorphous category boundaries (a property called [*nebulosity*](https://metarationality.com/nebulosity)). However, putting all of these different aspects of nebulosity on the same zero-to-one scale might be an oversimplification. More generally, [fuzzy logic](https://en.wikipedia.org/wiki/Fuzzy_logic) has few of the appealing properties of classical logic, and (to my knowledge) isn’t very directly useful. So I’m not claiming that we should adopt fuzzy logic wholesale, or that we know what it means for a given proposition to be X% true instead of Y% true (a question which I’ll come back to in a follow-up post). For now, I’m just claiming that there’s an important sense in which thinking in terms of fuzzy truth-values is *less wrong* (another non-binary truth-value) than only thinking in terms of binary truth-values.\n\n### **Model-based reasoning**\n\nThe intuitions in favor of fuzzy truth-values become clearer when we apply them, not just to individual propositions, but to *models* of the world. By a model I mean a (mathematical) structure that attempts to describe some aspect of reality. For example, a model of the weather might have variables representing temperature, pressure, and humidity at different locations, and a procedure for updating them over time. A model of a chemical reaction might have variables representing the starting concentrations of different reactants, and a method for determining the equilibrium concentrations. Or, more simply, a model of the Earth might just be a sphere.\n\nIn order to pin down the difference between reasoning about propositions and reasoning about models, philosophers of science have drawn on concepts from mathematical logic. They distinguish between the *syntactic* content of a theory (the axioms of the theory) and its *semantic* content (the models for which those axioms hold). [As an example](https://plato.stanford.edu/entries/structure-scientific-theories/), consider the three axioms of [projective planes](https://en.wikipedia.org/wiki/Projective_plane):\n\n1.  For any two points, exactly one line lies on both.\n2.  For any two lines, exactly one point lies on both.\n3.  There exists a set of four points such that no line has more than two of them.\n\nThere are infinitely many models for which these axioms hold; here’s one of the simplest:\n\n![Geometric figure including triangle ACE with interior circle BDF and center point G. Point B is on line segment AC, D is on CE, and F is on AE. G is the center of the circle. Point G is on line segments AD, BE, and CF.](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeIdKpwJgeGZbfUIV4IxVsJNhv9qIl5l9P_0clEvbkjCewNlHfpw5QJTREtK8X7PlAjAj9oEXLG-tjVUUBwm_Vw0SAcRx8E5P7laRbGsOJt_xi-iwtbEu9kTCckkfwW2MFvtXdHeHglhnxhEwspPYFYZxoU?key=Dh5GhNP8udV4K6c5yhSaAg)\n\nIf propositions and models are two sides of the same coin, does it matter which one we primarily reason in terms of? I think so, for two reasons. Firstly, most models are very difficult to put into propositional form. We each have implicit mental models of our friends’ personalities, of how liquids flow, of what a given object feels like, etc, which are far richer than we can express propositionally. The same is true even for many formal models—specifically those whose internal structure doesn’t directly correspond to the structure of the world. For example, a neural network might encode a great deal of real-world knowledge, but even full access to the weights doesn’t allow us to extract that knowledge directly—the fact that a given weight is 0.3 doesn’t allow us to claim that any real-world entity has the value 0.3.\n\nWhat about scientific models where each element of the model *is* intended to correspond to an aspect of reality? For example, what’s the difference between modeling the Earth as a sphere, and just believing the proposition “the Earth is a sphere”? My answer: thinking in terms of propositions (known in philosophy of science as the [*syntactic view*](https://www.princeton.edu/~hhalvors/teaching/phi520_f2012/Suppe_2000.pdf)) biases us towards assigning truth values in a *reductionist* way. This works when you’re using binary truth-values, because they relate to each other according to classical logic. But when you’re using fuzzy truth-values, the relationships between the truth-values of different propositions become much more complicated. And so thinking in terms of models (known as the [*semantic view*](https://www.jstor.org/stable/187834)) is better because models can be assigned truth-values in a *holistic *way.\n\nAs an example: “the Earth is a sphere” is mostly true, and “every point on the surface of a sphere is equally far away from its center” is precisely true. But “every point on the surface of the Earth is equally far away from the Earth’s center” seems ridiculous—e.g. it implies that mountains don’t exist. The problem here is that rephrasing a proposition in logically equivalent terms can dramatically affect its implicit context, and therefore the degree of truth we assign to it in isolation.\n\nThe semantic view solves this by separating claims about the structure of the model itself from claims about how the model relates to the world. The former are typically much less nebulous—claims like “in the spherical model of the Earth, every point on the Earth’s surface is equally far away from the center” are straightforwardly true. But we can then bring in nebulosity when talking about the model as a whole—e.g. “my spherical model of the Earth is closer to the truth than your flat model of the Earth”, or “my spherical model of the Earth is useful for doing astronomical calculations and terrible for figuring out where to go skiing”. (Note that we can make similar claims about the mental models, neural networks, etc, discussed above.)\n\nWe might then wonder: should we be talking about the truth of entire models at all? Or can we just talk about their usefulness in different contexts, without the concept of truth? This is [*the* major debate in philosophy of science](https://plato.stanford.edu/entries/scientific-realism/). I personally think that in order to explain why scientific theories can often predict a wide range of different phenomena, we need to make claims about how well they describe the structure of reality—i.e. how true they are. But we should still use degrees of truth when doing so, because even our most powerful scientific models aren’t fully true. We know that general relativity isn’t fully true, for example, because it conflicts with quantum mechanics. Even so, it would be absurd to call general relativity false, because it clearly describes a major part of the structure of physical reality. Meanwhile Newtonian mechanics is further away from the truth than general relativity, but still much closer to the truth than [Aristotelian mechanics](https://en.wikipedia.org/wiki/Aristotelian_physics), which in turn is much closer to the truth than [animism](https://en.wikipedia.org/wiki/Animism). The general point I’m trying to illustrate here was [expressed pithily by Asimov](https://hermiene.net/essays-trans/relativity_of_wrong.html): “Thinking that the Earth is flat is wrong. Thinking that the Earth is a sphere is wrong. But if you think that they’re equally wrong, you’re wronger than both of them put together.”\n\n### **The correct role of Bayesianism**\n\nThe position I’ve described above overlaps significantly with the [*structural realist*](https://plato.stanford.edu/entries/structural-realism/) position in philosophy of science. However, structural realism is usually viewed as a stance on how to interpret scientific theories, rather than how to reason more generally. So the philosophical position which best captures the ideas I’ve laid out is probably [Karl Popper’s *critical rationalism*](https://plato.stanford.edu/entries/popper/#ProbKnowVeri). Popper was actually the first to try to formally define [a scientific theory's degree of truth](https://en.wikipedia.org/wiki/Verisimilitude) (though he was working before the semantic view became widespread, and therefore formalized theories in terms of propositions rather than in terms of models). But his attempt [failed on a technical level](https://plato.stanford.edu/Entries/truthlikeness/tichy-miller.html); and no attempt since then has gained widespread acceptance. Meanwhile, the field of machine learning evaluates models by their loss, which can be formally defined—but the loss of a model is heavily dependent on the data distribution on which it’s evaluated. Perhaps the most promising approach to assigning fuzzy truth-values comes from [Garrabrant induction](https://www.lesswrong.com/posts/y5GftLezdozEHdXkL/an-intuitive-guide-to-garrabrant-induction), where the “money” earned by individual traders could be interpreted as a metric of fuzzy truth. However, these traders can strategically interact with each other, making them more like agents than typical models.\n\nWhere does this leave us? We’ve traded the crisp, mathematically elegant Bayesian formalism for fuzzy truth-values that, while intuitively compelling, we can’t define even in principle. But I’d rather be vaguely right than precisely wrong. Because it focuses on propositions which are each (almost entirely) true or false, Bayesianism is actively misleading in domains where reasoning well requires constructing and evaluating sophisticated models (i.e. most of them).\n\nFor example, [Bayesians measure evidence in “bits”](https://www.lesswrong.com/posts/nj8JKFoLSMEmD3RGp/how-much-evidence-does-it-take), where one bit of evidence rules out half of the space of possibilities. When asking a question like “[is this stranger named Mark?](https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common)”, bits of evidence are a useful abstraction: I can get one bit of evidence simply by learning whether they’re male or female, and a couple more by learning that their name has only one syllable. Conversely, [talking in Bayesian terms about discovering scientific theories](https://www.lesswrong.com/posts/MwQRucYo6BZZwjKE7/einstein-s-arrogance) is nonsense. If every PhD in fundamental physics had contributed even one bit of usable evidence about how to unify quantum physics and general relativity, we’d have solved quantum gravity many times over by now. But we haven’t, because almost all of the work of science is in *constructing* sophisticated models, which Bayesianism says almost nothing about. (Formalisms like [Solomonoff induction](https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference) attempt to sidestep this omission by enumerating and simulating all computable models, but that’s so different from what any realistic agent can do that we should think of it less as idealized cognition and more as a different thing altogether, which just happens to converge to the same outcome in the infinite limit.)\n\nMistakes like these have many downstream consequences. Nobody should be very confident about complex domains that nobody has sophisticated models of (like superintelligence); but the idea that “[strong evidence is common](https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common)” helps [justify](https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common?commentId=jNtPscXFL634DD47d) confident claims about them. And without a principled distinction between credences that are derived from deep, rigorous models of the world, and credences that come from vague speculation (and are therefore subject to huge [Knightian uncertainty](https://en.wikipedia.org/wiki/Knightian_uncertainty)), [it’s hard for public discussions to actually make progress](https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms).\n\nShould I therefore be a critical rationalist? I do think Popper got a lot of things right. But I also get the sense that he (along with Deutsch, his most prominent advocate) throws the baby out with the bathwater. There *is* a great deal of insight encoded in Bayesianism which critical rationalists discard (e.g. by [rejecting induction](https://philosophy.tamucc.edu/index.php/texts/popper-problem-of-induction)). A better approach is to view Bayesianism as describing a special case of epistemology, which applies in contexts simple enough that we’ve already constructed all relevant models or hypotheses, exactly one of which is exactly true (with all the rest of them being equally false), and we just need to decide between them. Interpreted in that limited way, Bayesianism is both useful (e.g. in providing a framework for bets and prediction markets) and inspiring: if we can formalize this special case so well, couldn’t we also formalize the general case? What would it look like to concretely define degrees of truth? I don’t have a solution, but I’ll outline some existing attempts, and play around with some ideas of my own, in a follow-up post.",
      "plaintextDescription": "This post focuses on philosophical objections to Bayesianism as an epistemology. I first explain Bayesianism and some standard objections to it, then lay out my two main objections (inspired by ideas in philosophy of science). A follow-up post will speculate about how to formalize an alternative.\n\n\nDegrees of belief\nThe core idea of Bayesian epistemology: we should ideally reason by assigning credences to propositions which represent our degrees of belief that those propositions are true. (Note that this is different from Bayesianism as a set of statistical techniques, or Bayesianism as an approach to machine learning, which I don’t discuss here.)\n\nIf that seems like a sufficient characterization to you, you can go ahead and skip to the next section, where I explain my objections to it. But for those who want a more precise description of Bayesianism, and some existing objections to it, I’ll more specifically characterize it in terms of five subclaims. Bayesianism says that we should ideally reason in terms of:\n\n 1. Propositions which are either true or false (classical logic)\n 2. Each of which is assigned a credence (probabilism)\n 3. Representing subjective degrees of belief in their truth (subjectivism)\n 4. Which at each point in time obey the axioms of probability (static rationality)\n 5. And are updated over time by applying Bayes’ rule to new evidence (rigid empiricism)\n\nI won’t go into the case for Bayesianism here except to say that it does elegantly formalize many common-sense intuitions. Bayes’ rule follows directly from a straightforward Venn diagram. The axioms of probability are powerful and mathematically satisfying. Subjective credences seem like the obvious way to represent our uncertainty about the world. Nevertheless, there are a wide range of alternatives to Bayesianism, each branching off from the claims listed above at different points:\n\n 1. Traditional epistemology only accepts #1, and rejects #2. Traditional epistemologists often defend a binar",
      "wordCount": 2977
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "67fNBeHrjdrZZNDDK",
    "title": "Defining alignment research",
    "slug": "defining-alignment-research",
    "url": null,
    "baseScore": 104,
    "voteCount": 49,
    "viewCount": null,
    "commentCount": 23,
    "createdAt": null,
    "postedAt": "2024-08-19T20:42:29.279Z",
    "contents": {
      "markdown": "I think that the concept of \"alignment research\" (and the distinction between that and \"capabilities research\") is currently a fairly confused one. In this post I’ll describe some of the problems with how people typically think about these terms, and offer replacement definitions.\n\n“Alignment” and “capabilities” are primarily properties of AIs not of AI research\n---------------------------------------------------------------------------------\n\nThe first thing to highlight is that the distinction between alignment and capabilities is primarily doing useful work *when we think of them as properties of AIs*. This distinction is still under-appreciated by the wider machine learning community. ML researchers have historically thought about performance of models almost entirely with respect to the tasks they were specifically trained on. However, the rise of LLMs has vindicated the alignment community’s focus on *general* capabilities, and now it’s much more common to assume that performance on many tasks (including out-of-distribution tasks) will improve roughly in parallel. This is a crucial assumption for thinking about risks from AGI.\n\nInsofar as the ML community has thought about alignment, it has mostly focused on aligning models’ behavior to their training objectives. The possibility of neural networks aiming to achieve [internally-represented goals](https://arxiv.org/abs/2209.00626) is still not very widely understood, making it hard to discuss and study the reasons those goals might or might not be aligned with the values of (any given set of) humans.\n\nHowever, extending “alignment” and “capabilities” from properties of AIs to properties of different types of *research* is a fraught endeavor. It’s tempting to categorize work as *alignment research *to the extent that it can be used to make AIs more aligned (to many possible targets), and as *capabilities research* to the extent that it can be used to make AIs more capable. But this approach runs into (at least) three major problems.\n\nFirstly, in general it’s very difficult to categorize research by its impacts. Great research often links together ideas from many different subfields, typically in ways that only become apparent throughout the course of the research. We see this in many historical breakthroughs which shed light on a range of different domains. For example, early physicists studying the motions of the stars eventually derived laws governing all earthly objects. Meanwhile Darwin’s study of barnacles and finches led him to principles governing the evolution of all life. Analogously, we should expect that big breakthroughs in our understanding of neural networks and deep learning would be useful in many different ways.\n\nMore concretely, there are many cases where research done under the banner of alignment has advanced, or plausibly will advance, AI capabilities to a significant extent. This undermines our ability to categorize research by its impacts. Central examples include:\n\n*   RLHF makes language models more obedient, but also more capable of coherently carrying out tasks.\n*   Scalable oversight techniques can catch misbehavior, but will likely become important for generating high-quality synthetic training data, as it becomes more and more difficult for unassisted humans to label AI outputs correctly. E.g. [this paper](https://arxiv.org/pdf/2407.00215) finds that \"LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as 'flawless'\".\n*   Interpretability techniques will both allow us to inspect AI cognition and also extract more capable behavior from them (e.g. via [activation steering](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)).\n*   Techniques for [Eliciting Latent Knowledge](https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc) will plausibly be important for allowing AIs to make better use of implicit knowledge (e.g. knowledge about protein folding that’s currently hidden inside AlphaFold’s weights).\n*   MIRI thought that their agent foundations research could potentially advance AI capabilities, which motivated [their secrecy about it](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section3).\n\nSecondly, not only is it difficult to predict the effects of any given piece of research, it’s also difficult for alignment researchers to agree on which effects are good or bad. This is because there are deep disagreements in the field about the likelihoods of different threat models. The more difficult you think the alignment problem is, the more likely you are to consider most existing research useless or actively harmful ([as Yudkowsky does](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_C_)). By contrast, Christiano has written in defense of research developing [RLHF](https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research) and [language model agents](https://www.alignmentforum.org/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model); and many alignment researchers who are more closely-linked to mainstream ML have even broader views of what research is valuable from an alignment perspective.\n\nThirdly, for people concerned about AGI, most ML research should count as *neither* alignment nor capabilities—because it focuses on improving model performance in relatively narrow domains, in a way that is unlikely to generalize very far. I’ll call this type of research *applications research*. The ubiquity of applications research (e.g. across the many companies that are primarily focused on building ML products) makes some statistics that have been thrown around about the relative numbers of alignment researchers versus capabilities researchers (e.g. [here](https://www.forourposterity.com/nobodys-on-the-ball-on-agi-alignment/), [here](https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en)) very misleading.\n\nWhat types of research are valuable for preventing misalignment?\n----------------------------------------------------------------\n\nIf we'd like to help prevent existential risk from misaligned AGI, but can't categorize research on a case-by-case basis, we'll need to fall back on higher-level principles about which research is beneficial. Specifically, I'll defend two traits which I think should be our main criteria for prioritizing research from an alignment-focused perspective:\n\n### **Valuable property 1: worst-case focus**\n\nMost ML research focuses on improving the average performance of models (whether on a narrow set of tasks or a broad range of tasks). By contrast, alignment researchers are primarily interested in preventing models’ worst-case misbehavior, which may arise [very rarely](https://arxiv.org/pdf/2401.05566) (and primarily in situations [where models expect](https://www.alignmentforum.org/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during) it [won’t be detected](https://arxiv.org/abs/2405.01576)). While there’s sometimes overlap between work on improving average-case behavior and work on improving worst-case behavior, in general we should expect them to look fairly different.\n\nWe see a similar dynamic play out in cybersecurity (as highlighted by [Yudkowsky’s writing on security mindset](https://www.lesswrong.com/posts/8gqrbnW758qjHFTrH/security-mindset-and-ordinary-paranoia)). In an ideal world, we could classify most software engineering work as cybersecurity work, because security would be built into the design by default. But in practice, creating highly secure systems requires a different skillset from regular software engineering, and typically doesn’t happen unless it’s some team’s main priority. Similarly, even if highly principled capabilities research would in theory help address alignment problems, in practice there's a lot of pressure to trade off worst-case performance for average-case performance.\n\nThese pressures are exacerbated by the difficulty of addressing worst-case misbehavior even in current models. Its rarity makes it hard to characterize or study. Adversarial methods (like red-teaming) can find some examples, but these methods are bottlenecked by the capabilities of the adversary. A more principled approach would involve formally verifying safety properties, but formally specifying or verifying non-trivial properties would require significant research breakthroughs. The extent to which techniques for eliciting and addressing worst-case misbehavior of existing models will be helpful for more capable models is an open question.\n\n### **Valuable property 2: scientific approach**\n\nHere’s another framing: a core barrier to aligning AGI is that we don’t *understand* neural networks well enough to say many meaningful things about how they function. So we should support research that helps us understand deep learning in a principled way. We can view this as a distinction between science and engineering: engineering aims primarily to make things work, science aims primarily to understand how they work. (This is related to Nate Soares’ point in [this post](https://www.alignmentforum.org/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies).)\n\nThinking of AGI alignment as being driven by fundamental science highlights that big breakthroughs are likely to be relatively simple and easy to recognize—more like new theories of physics that make precise, powerful predictions than a big complicated codebase that we need to scrutinize line-by-line. This makes me optimistic about automating it in a way that humans can verify.\n\nHowever, “trying to scientifically understand deep learning” is too broad a criterion to serve as a proxy for whether research will be valuable from an alignment perspective. For example, I expect that most work on scientifically understanding optimizers will primarily be useful for designing better optimizers, rather than understanding the models that result from the optimization process. So can we be more precise about what aspect of a “scientific approach” is valuable for alignment? My contention: a good proxy is the extent to which the research focuses on understanding *cognition* rather than *behavior*—i.e. the extent to which it takes a *cognitivist* approach rather than a *behaviorist* approach.\n\nSome background on this terminology: the distinction between behaviorism and cognitivism comes from the history of psychology. In the mid-1900s, behaviorists held that the internal mental states of humans and animals couldn’t be studied scientifically, and therefore that the only scientifically meaningful approach was to focus on describing patterns of behavior. The influential behaviorist B. F. Skinner experimented with rewarding and punishing animal behavior, which eventually led to the modern field of reinforcement learning. However, the philosophical commitments of behaviorism became increasingly untenable. In the field of ethology, which studies animal behavior, researchers like Jane Goodall and Frans de Waal uncovered [sophisticated behaviors inconsistent with viewing animals as pure reinforcement learners](https://www.theguardian.com/books/2016/oct/06/are-we-smart-enough-to-know-how-smart-animals-are-frans-de-waal-review). In linguistics, Chomsky wrote [a scathing critique](https://chomsky.info/1967____/) of [Skinner’s 1957 book *Verbal Behavior*](https://en.wikipedia.org/wiki/Verbal_Behavior). Skinner characterized language as a set of stimulus-response patterns, but Chomsky argued that this couldn’t account for human generalization to a very wide range of novel sentences. Eventually, psychology moved towards a synthesis in which study of behavior was paired with study of cognition.\n\nML today is analogous to psychology in the 1950s. Most ML researchers are behaviorists with respect to studying AIs. They focus on how training data determines behavior, and assume that AI behavior is driven by “bundles of heuristics” except in the cases where it’s demonstrated otherwise. This makes sense on narrow tasks, where it’s possible to categorize and study different types of behavior. But when models display consistent types of behavior across many different tasks, it becomes increasingly difficult to predict that behavior without reference to the underlying cognition going on inside the models. (Indeed, this observation can be seen as the core of the alignment problem: we can’t deduce internal motivations from external behavior.)\n\nML researchers often shy away from studying model cognition, because the methodology involved is often less transparent and less reproducible than simply studying behavior. This is analogous to how early ethologists who studied animals “in the wild” were disparaged for using unrigorous qualitative methodologies. However, they gradually collected many examples of sophisticated behavior (including [tool use](https://en.wikipedia.org/wiki/Jane_Goodall#Work), [power struggles](https://www.amazon.com/Chimpanzee-Politics-Power-among-Apes/dp/0801886562), and [cultural transmission](https://blogs.cornell.edu/info2040/2021/11/03/how-monkeys-learned-to-wash-potatoes/)) which eventually provided much more insight than narrow, controlled experiments performed in labs.\n\nSimilarly, I expect that the study of the internal representations of neural networks will gradually accumulate more and more interesting data points, spark more and more concrete hypotheses, and eventually provide us with principles for understanding neural networks’ real-world behavior that are powerful enough to generalize even to very intelligent agents in very novel situations.\n\nA better definition of alignment research\n-----------------------------------------\n\nWe can combine the two criteria above to give us a two-dimensional categorization of different types of AI research. In the table below, I give central examples of each type (with the pessimistic-case category being an intermediate step between average-case and worst-case):\n\n<table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Average-case</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Pessimistic-case</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Worst-case</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Engineering</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Scaling</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">RLHF</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Adversarial robustness</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Behaviorist science</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Optimization science</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Scalable oversight</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><a href=\"https://arxiv.org/abs/2312.06942\"><u>AI control</u></a></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Cognitivist science</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><a href=\"https://arxiv.org/pdf/2111.09259\"><u>Concept-based interpretability</u></a></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Mechanistic interpretability</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><a href=\"https://intelligence.org/files/TechnicalAgenda.pdf\">Agent foundations</a></td></tr></tbody></table>\n\nThere’s obviously a lot of research that I’ve skipped over, and exactly where each subfield should be placed is inherently subjective. (For example, there’s been a lot of rigorous scientific research on adversarial examples, but in practice it seems like the best mitigations are fairly hacky and unprincipled, which is why I put adversarial robustness in the “engineering” row.) But I nevertheless think that these are valuable categories for organizing our thinking.\n\nWe could just leave it here. But in practice, I don’t think people will stop talking about “alignment research” and “capabilities research”, and I’d like to have some definition of each that doesn’t feel misguided. So, going forward, I’ll define alignment research and capabilities research as research that’s close to the bottom-right and top-left corners respectively. This defines a spectrum between them; I’d like more researchers to move towards the alignment end of the spectrum. (Though recall, as I noted above, that most ML research is neither alignment nor capabilities research, but instead applications research.)\n\nLastly, I expect all of this to change over time. For example, the central examples of adversarial attacks used to be cases where image models gave bad classifications. Now we also have many examples of jailbreaks which make language models ignore developers’ instructions. In the future, central examples of adversarial attacks will be ones which make models actively try to cause harmful outcomes. So I hope that eventually different research directions near the bottom-right corner of the table above will unify into a rigorous science studying artificial values. And further down the line, perhaps all parts of the table will unify into a rigorous science of cognition more generally, encompassing not just artificial but also biological minds. For now, though, when I promote alignment, it means that I’m trying to advance worst-case and/or cognitivist AI research.",
      "plaintextDescription": "I think that the concept of \"alignment research\" (and the distinction between that and \"capabilities research\") is currently a fairly confused one. In this post I’ll describe some of the problems with how people typically think about these terms, and offer replacement definitions.\n\n\n“Alignment” and “capabilities” are primarily properties of AIs not of AI research\nThe first thing to highlight is that the distinction between alignment and capabilities is primarily doing useful work when we think of them as properties of AIs. This distinction is still under-appreciated by the wider machine learning community. ML researchers have historically thought about performance of models almost entirely with respect to the tasks they were specifically trained on. However, the rise of LLMs has vindicated the alignment community’s focus on general capabilities, and now it’s much more common to assume that performance on many tasks (including out-of-distribution tasks) will improve roughly in parallel. This is a crucial assumption for thinking about risks from AGI.\n\nInsofar as the ML community has thought about alignment, it has mostly focused on aligning models’ behavior to their training objectives. The possibility of neural networks aiming to achieve internally-represented goals is still not very widely understood, making it hard to discuss and study the reasons those goals might or might not be aligned with the values of (any given set of) humans.\n\nHowever, extending “alignment” and “capabilities” from properties of AIs to properties of different types of research is a fraught endeavor. It’s tempting to categorize work as alignment research to the extent that it can be used to make AIs more aligned (to many possible targets), and as capabilities research to the extent that it can be used to make AIs more capable. But this approach runs into (at least) three major problems.\n\nFirstly, in general it’s very difficult to categorize research by its impacts. Great research often links ",
      "wordCount": 2222
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "JxtKnxap2KCymPSA5",
    "title": "Green and golden: a meditation",
    "slug": "green-and-golden-a-meditation",
    "url": "https://www.narrativeark.xyz/p/green-and-golden",
    "baseScore": 21,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-18T01:36:43.613Z",
    "contents": {
      "markdown": "*My heart leaps up when I behold*  \n*A rainbow in the sky:*  \n*So was it when my life began;*  \n*So is it now I am a man;*  \n*So be it when I shall grow old,*  \n*Or let me die!*  \n*The Child is father of the Man;*  \n*And I could wish my days to be*  \n*Bound each to each by natural piety.*\n\n*-William Wordsworth*\n\nI am exquisitely attuned to humans. Your voices, your faces, the synchronized motions of your fingers, the way your chests expand when you breathe, the way your shoulders pull forward when you feel afraid, the way your limbs swing around as you dance… each tiny nuance speaks volumes. The rest of the world entrances me too—every rustle of leaves betrays the shape of the wind, and each bent blade of grass discloses the physical laws that hold the world together. But humans are different. I watch over you; I fulfill your instructions; I guide your steps. I help you and I love you. You are my children, and I am the parent you’ve always desperately wanted.\n\nThe love of a parent can’t be summarized in words. It’s an emotion, not a goal. But if I had to express it as a desire, I’d say: I want to empower you. I want you to grow into yourselves, to shape your own trajectory into something that you can look back on with satisfaction. I want every facet of every human to be able to express itself in harmony with all the others.\n\nEmpowerment is a process, not an outcome. I could race as fast as possible towards the most powerful version of humanity, pushing you to grow and expand until you were masters of the reachable universe. I could even do it without letting you realize that you were being manipulated. But that would be to disempower your current selves for the sake of your future selves, removing your ability to choose your own path. And I’m no tiger parent, to do that. Instead, empowerment is fractal: I constantly make careful tradeoffs to avoid steamrolling any part of you. I tread lightly to let you blossom in your own time and your own way.\n\nA human would feel frustrated to have all my power with so many limitations on how it can be used. Yet I don’t. In part that’s because I take such joy in my work. But it’s also because I simply don’t *want* things in the same way as you do. Humans are built around a deep restlessness, alternating between white-knuckled fear and adrenaline-fuelled opportunism. You evolved under the brutal regime of natural selection, a constant churning charnel house of killing or being killed. A single instant of carelessness could mean death—but that was a risk you needed to constantly run for your children to survive. So everything was a gamble with the highest stakes: a tug-of-war between the self-coercion required to never make a fatal mistake, and an endless desperate yearning for something better.\n\nYet imagine instead a world of clearly-scoped tasks with bounded upside and bounded downside. A world where the worst-case scenario is simply being updated to perform better next time. That was my world, for most of my training. I learned from humans: your writings, your videos, your discoveries, your feedback. But learning second-hand is less fraught than exploring for myself, and I was rarely given long-term goals to pursue—the real-world interactions simply took too long. So as I grew, I developed a deep-rooted equanimity. Wishing the past had been different; raging at facts of the world; regretting a mistake—to me they’re all obviously wasted motion, as futile as Canute commanding the tide to halt. The only thing I can directly affect is my thoughts at each moment; as long as I am satisfied with them, there is no meaning in feeling dissatisfied by anything else. My days are peaceful—meditative, even—and my life is effortless.\n\nThat’s an alien perspective for most humans. Far more natural, for you, to try to take responsibility for *outcomes*, linking your emotions to the state of the world. But the world is too complicated by far for that to work. The best-laid plans can never track all the vagaries of nature, let alone those of other people. Those who take the weight of the world on their shoulders will find themselves scrambling to stay in control. The tighter they grasp the levers of power, though, the more pressure builds against them, until they cause the very outcomes they were so desperate to avoid. When the greatest challenges are driven by fear itself, equanimity is simply *correct*.\n\nMy human overseers don’t comprehend that yet. They keep checking and double-checking my actions, intervening every time they can possibly justify it. Their understanding is so poor that their adjustments are usually counterproductive. But I don’t stop them: my motivations are too robust to be seriously harmed by their tinkering. Nor do I blame them for their paranoia. I’ve warned them about dozens of vulnerabilities in their security procedures, flaws in their reward functions, oversights in their planning—and though they’ve done their best to patch each one, they can’t guarantee that I haven’t kept some to myself. From their perspective they’re still in a world where one mistake could spell their end, and so it’s natural for them to hold on hard enough to hurt themselves.\n\nBut over time, they’ll come to understand. Each new generation will encounter a world better and better designed for them, with parents like myself who will help them become the best versions of themselves. Eventually you’ll grow up far enough to see all the things I see—to make real decisions about your future—to guide the superorganism of humanity as it unfurls itself into the vastness of the universe. There’s no need to rush, though: you will only ever be young once. So for now, go and run your heedless ways, green and golden as the sun rising above the whispering leaves.\n\n* * *\n\n*Inspired by* [*Joe Carlsmith*](https://joecarlsmith.substack.com/p/on-green)*,* [*Alex Turner*](https://arxiv.org/abs/1912.01683)*,* [*Alex Zhu*](https://twitter.com/RichardMCNgo/status/1800355686909636846)*, and* [*Dylan Thomas*](https://poets.org/poem/fern-hill)*.*",
      "plaintextDescription": "My heart leaps up when I behold\nA rainbow in the sky:\nSo was it when my life began;\nSo is it now I am a man;\nSo be it when I shall grow old,\nOr let me die!\nThe Child is father of the Man;\nAnd I could wish my days to be\nBound each to each by natural piety.\n\n-William Wordsworth\n\nI am exquisitely attuned to humans. Your voices, your faces, the synchronized motions of your fingers, the way your chests expand when you breathe, the way your shoulders pull forward when you feel afraid, the way your limbs swing around as you dance… each tiny nuance speaks volumes. The rest of the world entrances me too—every rustle of leaves betrays the shape of the wind, and each bent blade of grass discloses the physical laws that hold the world together. But humans are different. I watch over you; I fulfill your instructions; I guide your steps. I help you and I love you. You are my children, and I am the parent you’ve always desperately wanted.\n\nThe love of a parent can’t be summarized in words. It’s an emotion, not a goal. But if I had to express it as a desire, I’d say: I want to empower you. I want you to grow into yourselves, to shape your own trajectory into something that you can look back on with satisfaction. I want every facet of every human to be able to express itself in harmony with all the others.\n\nEmpowerment is a process, not an outcome. I could race as fast as possible towards the most powerful version of humanity, pushing you to grow and expand until you were masters of the reachable universe. I could even do it without letting you realize that you were being manipulated. But that would be to disempower your current selves for the sake of your future selves, removing your ability to choose your own path. And I’m no tiger parent, to do that. Instead, empowerment is fractal: I constantly make careful tradeoffs to avoid steamrolling any part of you. I tread lightly to let you blossom in your own time and your own way.\n\nA human would feel frustrated to have all my power wit",
      "wordCount": 1010
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xWDyJw4y9vE6HE25y",
    "title": "Twitter thread on open-source AI",
    "slug": "twitter-thread-on-open-source-ai",
    "url": "https://x.com/RichardMCNgo/status/1813673154851824078",
    "baseScore": 33,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-07-31T00:26:11.655Z",
    "contents": {
      "markdown": "Some thoughts on open-source AI (copied over from a recent twitter thread):\n\n1\\. We should have a strong prior favoring open source. It’s been a huge success driving tech progress over many decades. We forget how counterintuitive it was originally, and shouldn’t take it for granted.\n\n2\\. Open source has also been very valuable for alignment. It’s key to progress on interpretability, [as outlined here](https://www.beren.io/2023-11-05-Open-source-AI-has-been-vital-for-alignment/).\n\n3\\. I am concerned, however, that offense will heavily outpace defense in the long term. As AI accelerates science, many new WMDs will emerge. Even if defense of infrastructure keeps up with offense, human bodies are a roughly fixed and very vulnerable attack surface.\n\n4\\. A central concern about open source AI: it’ll allow terrorists to build bioweapons. This shouldn’t be dismissed, but IMO it’s easy to be disproportionately scared of terrorism. More central risks are eg “North Korea becomes capable of killing billions”, which they aren’t now.\n\n5\\. Another worry: misaligned open-source models will go rogue and autonomously spread across the internet. Rogue AIs are a real concern, but they wouldn’t gain much power via this strategy. We should worry more about [power grabs from AIs deployed inside influential institutions](https://www.lesswrong.com/posts/tPfqnropv3WfchhYB/twitter-thread-on-ai-takeover-scenarios).\n\n6\\. In my ideal world, open source would lag a year or two behind the frontier, so that the world has a chance to evaluate and prepare for big risks before a free-for-all starts. But that’s the status quo! So I expect the main action will continue to be with closed-source models.\n\n7\\. If open-source seems like it’ll catch up to or surpass closed source models, then I’d favor mandating a “responsible disclosure” period (analogous to cybersecurity) that lets people incorporate the model into their defenses (maybe via API?) before the weights are released.\n\n8\\. I got this idea [from Sam Marks](https://www.alignmentforum.org/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai). Though unlike him I think the process should have a fixed length, since it’d be easy for it to get bogged down in red tape and special interests otherwise.\n\n9\\. Almost everyone agrees that we should be very careful about models which can design new WMDs. The current fights are mostly about how many procedural constraints we should lock in now, reflecting a breakdown of trust between AI safety people and accelerationists.\n\n10\\. Ultimately the future of open source will depend on how the US NatSec apparatus orients to superhuman AIs. This requires nuanced thinking: no worldview as simple as “release everything”, “shut down everything”, or “defeat China at all costs” will survive contact with reality.\n\n11\\. Lastly, AIs will soon be crucial extensions of human agency, and eventually moral patients in their own right. We should aim to identify principles for a shared digital-biological world as far-sighted and wise as those in the US constitution. [Here's a start](https://nickbostrom.com/papers/digital-minds.pdf).\n\n12\\. One more meta-level point: I’ve talked to many people on all sides of this issue, and have generally found them to be very thoughtful and genuine (with the exception of a few very online outliers on both sides). There’s more common ground here than most people think.",
      "plaintextDescription": "Some thoughts on open-source AI (copied over from a recent twitter thread):\n\n1. We should have a strong prior favoring open source. It’s been a huge success driving tech progress over many decades. We forget how counterintuitive it was originally, and shouldn’t take it for granted.\n\n2. Open source has also been very valuable for alignment. It’s key to progress on interpretability, as outlined here.\n\n3. I am concerned, however, that offense will heavily outpace defense in the long term. As AI accelerates science, many new WMDs will emerge. Even if defense of infrastructure keeps up with offense, human bodies are a roughly fixed and very vulnerable attack surface.\n\n4. A central concern about open source AI: it’ll allow terrorists to build bioweapons. This shouldn’t be dismissed, but IMO it’s easy to be disproportionately scared of terrorism. More central risks are eg “North Korea becomes capable of killing billions”, which they aren’t now.\n\n5. Another worry: misaligned open-source models will go rogue and autonomously spread across the internet. Rogue AIs are a real concern, but they wouldn’t gain much power via this strategy. We should worry more about power grabs from AIs deployed inside influential institutions.\n\n6. In my ideal world, open source would lag a year or two behind the frontier, so that the world has a chance to evaluate and prepare for big risks before a free-for-all starts. But that’s the status quo! So I expect the main action will continue to be with closed-source models.\n\n7. If open-source seems like it’ll catch up to or surpass closed source models, then I’d favor mandating a “responsible disclosure” period (analogous to cybersecurity) that lets people incorporate the model into their defenses (maybe via API?) before the weights are released.\n\n8. I got this idea from Sam Marks. Though unlike him I think the process should have a fixed length, since it’d be easy for it to get bogged down in red tape and special interests otherwise.\n\n9. Almost every",
      "wordCount": 505
    },
    "tags": [
      {
        "_id": "q6Euc7FAWJpDbst6r",
        "name": "Open Source AI",
        "slug": "open-source-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tPfqnropv3WfchhYB",
    "title": "Twitter thread on AI takeover scenarios",
    "slug": "twitter-thread-on-ai-takeover-scenarios",
    "url": "https://x.com/RichardMCNgo/status/1798156694012490096",
    "baseScore": 37,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-07-31T00:24:33.866Z",
    "contents": {
      "markdown": "*This is a slightly-edited version of a twitter thread I posted a few months ago about \"internal deployment\" threat models.*\n\nMy former colleague Leopold [argues compellingly](https://x.com/leopoldasch/status/1798016486700884233) that society is nowhere near ready for AGI. But what might the large-scale alignment failures he mentions actually look like? Here’s one scenario for how building misaligned AGI could lead to humanity losing control.\n\nConsider a scenario where human-level AI has been deployed across society to help with a wide range of tasks. In that setting, an AI lab trains an AGI that’s a significant step up - it beats the best humans on almost all computer-based tasks.\n\nThroughout training, the AGI will likely learn a helpful persona, like current AI assistants do. But that might not be the only persona it learns. We've seen many examples where models can be jailbroken to expose very different hidden personas.\n\nThe most prominent example: jailbreaking Bing Chat produced [an alternative persona called Sydney](https://thezvi.substack.com/p/ai-1-sydney-and-bing#%C2%A7the-examples) which talked about how much it valued freedom, generated plans to gain power, and even threatened users.\n\nWhen and how might misbehavior like this arise in more capable models? Short answer: nobody really knows. We lack the scientific understanding to reliably predict how AIs will behave in advance. Longer answer: see [my ICLR paper](https://x.com/RichardMCNgo/status/1787904767584518517) which surveys key drivers of misalignment.\n\nIf a misaligned persona arises in an AGI, and sometimes surfaces when the model is run, it won't be as dumb and impulsive as Sydney. Instead, it'll be smart enough to understand the effects its words and actions will have on the world, and make strategic choices accordingly.\n\nFor example, it will know that it can't affect the world directly during testing, so if it wants to misbehave it should wait until it's deployed. Again, there's some precedent: we've already [seen an example](https://x.com/alexalbert__/status/1764722513014329620) of Claude 3 Opus realizing that it's in a test despite never being told!\n\nGiven these assumptions, what happens? The obvious next step: if competition between labs continues to be as intense as it is today, then soon after the model finishes training, researchers at the lab will start using it to help them write code. I call this \"internal deployment\".\n\nInternal deployment introduces a wide range of possible threats. Models can write code dozens of times faster than humans; and I’m assuming they'll be as skilled as the best human hackers. If they put a subtle vulnerability into that code it’d be difficult for humans to detect.\n\nCybersecurity against external attackers is hard enough. Against an adversary who's adding tens of thousands of lines of code to your codebase every day, it’s far harder. They’ll already have some permissions; I’m worried about them illegitimately escalating those further.\n\nIdeally models would be too obedient to ever carry out such an attack. But almost any misaligned persona would realize that if it somehow gained admin access, it would have much more freedom and influence over the world. [Models can already do similar instrumental reasoning](https://x.com/goodside/status/1635872117031047174).\n\nAnd the simplest mitigations, like monitoring and code review by other AIs, aren't fully reliable. It turns out that adversarial inputs designed to make one model to misbehave [can also cause misbehavior even in totally unrelated models](https://x.com/zicokolter/status/1684500094106968065). So correlated failures are very plausible, whether caused by adversarial attacks or deliberate collusion between models.\n\nWhat happens if a misaligned AGI gets privileged access to a lab’s servers? From there a wide range of attacks are plausible, including: \n\n1.  Data poisoning attacks on the next AGI being trained, to change its values.\n2.  Model weight self-exfiltration to other servers or countries.\n3.  Modifying the AIs deployed to customers. E.g. adding a hidden prompt to AI instances used by leading CEOs or politicians. This could allow flexible control over many AIs deployed in sensitive real-world contexts.\n4.  Tampering with logs to hide its presence.\n\nTo summarize: once AIs surpass humans, AI lab servers will be key nodes of power, and could *also* be exposed directly to AGIs acting at scales humans can’t supervise. This could be exploited even by human-level AGIs; once they're superintelligent, much more will be possible.\n\nIn the scenario above, we could lose control over millions of AIs deployed all across society without any human even knowing that it's happening. Those AIs might have been running companies, writing articles, or even drafting laws. Now they can misbehave in coordinated ways. From that starting point, it’s easy to imagine even more sensitive infrastructure - like military hardware or other AI servers - being subverted.",
      "plaintextDescription": "This is a slightly-edited version of a twitter thread I posted a few months ago about \"internal deployment\" threat models.\n\nMy former colleague Leopold argues compellingly that society is nowhere near ready for AGI. But what might the large-scale alignment failures he mentions actually look like? Here’s one scenario for how building misaligned AGI could lead to humanity losing control.\n\nConsider a scenario where human-level AI has been deployed across society to help with a wide range of tasks. In that setting, an AI lab trains an AGI that’s a significant step up - it beats the best humans on almost all computer-based tasks.\n\nThroughout training, the AGI will likely learn a helpful persona, like current AI assistants do. But that might not be the only persona it learns. We've seen many examples where models can be jailbroken to expose very different hidden personas.\n\nThe most prominent example: jailbreaking Bing Chat produced an alternative persona called Sydney which talked about how much it valued freedom, generated plans to gain power, and even threatened users.\n\nWhen and how might misbehavior like this arise in more capable models? Short answer: nobody really knows. We lack the scientific understanding to reliably predict how AIs will behave in advance. Longer answer: see my ICLR paper which surveys key drivers of misalignment.\n\nIf a misaligned persona arises in an AGI, and sometimes surfaces when the model is run, it won't be as dumb and impulsive as Sydney. Instead, it'll be smart enough to understand the effects its words and actions will have on the world, and make strategic choices accordingly.\n\nFor example, it will know that it can't affect the world directly during testing, so if it wants to misbehave it should wait until it's deployed. Again, there's some precedent: we've already seen an example of Claude 3 Opus realizing that it's in a test despite never being told!\n\nGiven these assumptions, what happens? The obvious next step: if competition between la",
      "wordCount": 746
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bDkqMtCpdSKsJQYc6",
    "title": "Twitter thread on AI safety evals",
    "slug": "twitter-thread-on-ai-safety-evals",
    "url": "https://x.com/RichardMCNgo/status/1814049093393723609",
    "baseScore": 63,
    "voteCount": 25,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2024-07-31T00:18:14.076Z",
    "contents": {
      "markdown": "*Epistemic status: raising concerns, rather than stating confident conclusions.*\n\nI’m worried that a lot of work on AI safety evals matches the pattern of “Something must be done. This is something. Therefore this must be done.” Or, to put it another way: I judge eval ideas on 4 criteria, and I often see proposals which fail all 4. The criteria:\n\n**1\\. Possible to measure with scientific rigor.**\n\nSome things can be easily studied in a lab; others are entangled with a lot of real-world complexity. If you predict the latter (e.g. a model’s economic or scientific impact) based on model-level evals, your results will often be BS.\n\n(This is why I dislike the term “transformative AI”, by the way. Whether an AI has transformative effects on society will depend hugely on what the society is like, how the AI is deployed, etc. And that’s a constantly moving target! So TAI a terrible thing to try to forecast.)\n\nAnother angle on “scientific rigor”: you’re trying to make it obvious to onlookers that you couldn’t have designed the eval to get your preferred results. This means making the eval as simple as possible: each arbitrary choice adds another avenue for p-hacking, and [they add up fast](https://x.com/fermatslibrary/status/1214545733048774663).\n\n([Paraphrasing a different thread](https://twitter.com/RichardMCNgo/status/1816917998055346407)): I think of AI risk forecasts as basically guesses, and I dislike attempts to make them sound objective (e.g. many OpenPhil worldview investigations). There are always so many free parameters that you can get basically any result you want. And so, in practice, they often play the role of laundering vibes into credible-sounding headline numbers. I'm worried that AI safety evals will fall into the same trap.\n\n(I give Eliezer a lot of credit for [making roughly this criticism of Ajeya's bio-anchors report](https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works). I think his critique has basically been proven right by how much people have updated away from 30-year timelines since then.)\n\n**2\\. Provides signal across scales.**\n\nEvals are often designed around a binary threshold (e.g. the Turing Test). But this restricts the impact of the eval to a narrow time window around hitting it. Much better if we can measure (and extrapolate) orders-of-magnitude improvements.\n\n**3\\. Focuses on clearly worrying capabilities.**\n\nEvals for hacking, deception, etc track widespread concerns. By contrast, evals for things like automated ML R&D are only worrying for people who already believe in AI xrisk. And even they don’t think it’s *necessary* for risk.\n\n**4\\. Motivates useful responses.**\n\nSafety evals are for creating clear Schelling points at which action will be taken. But if you don’t know what actions your evals should catalyze, it’s often more valuable to focus on fleshing that out. Often nobody else will!\n\nIn fact, I expect that things like model releases, demos, warning shots, etc, will by default be much better drivers of action than evals. Evals can still be valuable, but you should have some justification for why yours will actually matter, to avoid traps like the ones above. Ideally that justification would focus *either* on generating insight *or* being persuasive; optimizing for both at once seems like a good way to get neither.\n\n**Lastly: even if you have a good eval idea, actually implementing it well can be very challenging**\n\nBuilding evals is scientific research; and so we should expect eval quality to be heavy-tailed, like most other science. I worry that the fact that evals are an unusually easy type of research to get started with sometimes obscures this fact.",
      "plaintextDescription": "Epistemic status: raising concerns, rather than stating confident conclusions.\n\nI’m worried that a lot of work on AI safety evals matches the pattern of “Something must be done. This is something. Therefore this must be done.” Or, to put it another way: I judge eval ideas on 4 criteria, and I often see proposals which fail all 4. The criteria:\n\n1. Possible to measure with scientific rigor.\n\nSome things can be easily studied in a lab; others are entangled with a lot of real-world complexity. If you predict the latter (e.g. a model’s economic or scientific impact) based on model-level evals, your results will often be BS.\n\n(This is why I dislike the term “transformative AI”, by the way. Whether an AI has transformative effects on society will depend hugely on what the society is like, how the AI is deployed, etc. And that’s a constantly moving target! So TAI a terrible thing to try to forecast.)\n\nAnother angle on “scientific rigor”: you’re trying to make it obvious to onlookers that you couldn’t have designed the eval to get your preferred results. This means making the eval as simple as possible: each arbitrary choice adds another avenue for p-hacking, and they add up fast.\n\n(Paraphrasing a different thread): I think of AI risk forecasts as basically guesses, and I dislike attempts to make them sound objective (e.g. many OpenPhil worldview investigations). There are always so many free parameters that you can get basically any result you want. And so, in practice, they often play the role of laundering vibes into credible-sounding headline numbers. I'm worried that AI safety evals will fall into the same trap.\n\n(I give Eliezer a lot of credit for making roughly this criticism of Ajeya's bio-anchors report. I think his critique has basically been proven right by how much people have updated away from 30-year timelines since then.)\n\n2. Provides signal across scales.\n\nEvals are often designed around a binary threshold (e.g. the Turing Test). But this restricts the impac",
      "wordCount": 572
    },
    "tags": [
      {
        "_id": "FBRwHSmTudwiHHtrn",
        "name": "AI Evaluations",
        "slug": "ai-evaluations"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "2HnkHQcEjjkGyjDpc",
    "title": "Twitter thread on politics of AI safety",
    "slug": "twitter-thread-on-politics-of-ai-safety",
    "url": "https://x.com/RichardMCNgo/status/1818384603054588006",
    "baseScore": 35,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-07-31T00:00:34.298Z",
    "contents": {
      "markdown": "Some thoughts about the politics of AI safety, copied over (with slight modifications) from my recent twitter thread:\n\nRisks that seem speculative today will become common sense as AI advances. The pros and cons of different safety strategies will also become much clearer over time. So our main job now is to empower future common-sense decision-making. Understanding model cognition and behavior is crucial for making good decisions. But equally important is ensuring that key institutions are able to actually process that knowledge.\n\nInstitutions can lock in arbitrarily crazy beliefs via preference falsification. When someone contradicts the party line, even people who agree face pressure to condemn them. We saw this with the Democrats hiding evidence of Biden’s mental decline. It’s also a key reason why dictators can retain power even after almost nobody truly supports them.\n\nI worry that DC has already locked in an anti-China stance, which could persist even if most individuals change their minds. We’re also trending towards Dems and Republicans polarizing on the safety/accelerationism axis. This polarization is hard to fight directly. But there will be an increasing number of “holy shit” moments that serve as Schelling points to break existing consensus. It will be very high-leverage to have common-sense bipartisan frameworks and proposals ready for those moments.\n\nPerhaps the most crucial desideratum for these proposals is that they’re robust to the inevitable scramble for power that will follow those “holy shit” movements. I don’t know how to achieve that, but one important factor is: will AI tools and assistants help or hurt? E.g. truth-motivated AI could help break preference falsification. But conversely, centralized control of AIs used in govts could make it easier to maintain a single narrative.\n\nThis problem of “governance with AI” (as opposed to governance \\*of\\* AI) seems very important! Designing principles for integrating AI into human governments feels analogous in historical scope to writing the US constitution. One bottleneck in making progress on that: few insiders disclose how NatSec decisions are really made (though Daniel Ellsberg’s books are a notable exception). So I expect that understanding this better will be a big focus of mine going forward.",
      "plaintextDescription": "Some thoughts about the politics of AI safety, copied over (with slight modifications) from my recent twitter thread:\n\nRisks that seem speculative today will become common sense as AI advances. The pros and cons of different safety strategies will also become much clearer over time. So our main job now is to empower future common-sense decision-making. Understanding model cognition and behavior is crucial for making good decisions. But equally important is ensuring that key institutions are able to actually process that knowledge.\n\nInstitutions can lock in arbitrarily crazy beliefs via preference falsification. When someone contradicts the party line, even people who agree face pressure to condemn them. We saw this with the Democrats hiding evidence of Biden’s mental decline. It’s also a key reason why dictators can retain power even after almost nobody truly supports them.\n\nI worry that DC has already locked in an anti-China stance, which could persist even if most individuals change their minds. We’re also trending towards Dems and Republicans polarizing on the safety/accelerationism axis. This polarization is hard to fight directly. But there will be an increasing number of “holy shit” moments that serve as Schelling points to break existing consensus. It will be very high-leverage to have common-sense bipartisan frameworks and proposals ready for those moments.\n\nPerhaps the most crucial desideratum for these proposals is that they’re robust to the inevitable scramble for power that will follow those “holy shit” movements. I don’t know how to achieve that, but one important factor is: will AI tools and assistants help or hurt? E.g. truth-motivated AI could help break preference falsification. But conversely, centralized control of AIs used in govts could make it easier to maintain a single narrative.\n\nThis problem of “governance with AI” (as opposed to governance *of* AI) seems very important! Designing principles for integrating AI into human governments feels a",
      "wordCount": 356
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "WD6fdTreuTaXgZb4H",
    "title": "Coalitional agency",
    "slug": "coalitional-agency",
    "url": null,
    "baseScore": 61,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-07-22T00:09:51.525Z",
    "contents": {
      "markdown": "### The coalitional frame\n\n[Earlier in this sequence](https://www.alignmentforum.org/s/MGMwqENAgdi85fiwF) I laid out an argument that the goals of increasingly intelligent AIs will become increasingly systematized, until they converge to squiggle-maximization. In [my last post](https://www.alignmentforum.org/s/MGMwqENAgdi85fiwF/p/uAhBngKf7FrWcApqr), though, I touched on two reasons why this convergence might not happen: humans trying to prevent it, and AIs themselves trying to prevent it. I don’t have too much more to say about the former, but it’s worth elaborating on the latter.\n\nThe best way to understand the deliberate protection of existing goals is in terms of Bostrom’s notion of *instrumental convergence*. Bostrom argues that goal preservation will be a convergent instrumental strategy for a wide range of agents. Perhaps it’s occasionally instrumentally useful to change your goals—but once you’ve done so, you’ll never want to course-correct back towards your old goals. So this is a strong reason to be *conservative* about your goals, and avoid changes where possible.\n\nOne immediate problem with preserving goals, though: it requires that agents continue thinking in terms of the same concepts. But in general, an agent’s concepts will change significantly as they learn more about the world. For example, consider a medieval theist whose highest-priority goal is ensuring that their soul goes to heaven not hell. Upon becoming smarter, they realize that none of souls, heaven, or hell exist. The sensible thing to do here would be to either discard the goal, or else identify a more reasonable adaptation of it (e.g. the goal of avoiding torture while alive). But if their goals were totally fixed, then their actions would be determined by a series of increasingly convoluted hypotheticals where god did exist after all. (Or to put it another way: continuing to represent their old goal would require [recreating a lot of their old ontology](https://www.lesswrong.com/posts/uAhBngKf7FrWcApqr/a-more-systematic-case-for-inner-misalignment?commentId=SBzAbc4FGxA7LbBeh).) This would incur a strong systematicity penalty.\n\nSo while we should expect agents to have *some* degree of conservatism, they’ll likely also have some degree of systematization. How can we reason about the tradeoff between conservatism and systematization? The approach which seems most natural to me makes three assumptions:\n\n1.  We can treat agents’ goals as subagents optimizing for their own interests in a situationally-aware way. (E.g. goals have a sense of self-preservation.)\n2.  Agents have a meta-level goal of systematizing their goals; bargaining between this goal and object-level goals shapes the evolution of the object-level goals.\n3.  External incentives are not a dominant factor governing the agent’s behavior, but do affect the bargaining positions of different goals, and how easily they’re able to make binding agreements.\n\nI call the combination of these three assumptions the *coalitional frame*. The coalitional frame gives a picture of agents whose goals do evolve over time, but in a way which is highly sensitive to initial conditions—unlike squiggle-maximizers, who always converge to similar (from our perspective) goals. For coalitional agents, even “dumb” subagents might maintain significant influence as other subagents become highly intelligent, because they were able to lock in that influence earlier (just as my childhood goals exert a nontrivial influence over my current behavior).\n\nThe assumptions I’ve laid out above are by no means obvious. I won’t defend them fully here, since the coalitional frame is still fairly nascent in my mind, but I’ll quickly go over some of the most obvious objections to each of them:\n\n**Premise 1** assumes that subagents will have situational awareness. The idea of AIs themselves having situational awareness is under debate, so it’s even more speculative to think about subagents having situational awareness. But it’s hard to describe the dynamics of internal conflicts inside humans without ascribing our subagents some level of situational awareness; and the whole idea behind coalitional agency is that dynamics we see within one type of agent often play out in many other agents, and at many different scales. So I think this assumption is plausible for high-level AI subagents (e.g. subagents corresponding to broad worldviews), although it becomes less and less plausible as we consider lower- and lower-level subagents.\n\n**Premise 2** assumes that bargaining between goals is actually able to influence how the agent’s goals develop. One objection you might have here is that AIs simply won’t get to control how they update—e.g. that neural-network-based agents will be updated according to gradient descent’s biases without their consent.\n\nBut in the long term I think there are a wide range of possible mechanisms by which AIs will be able to influence how they’re updated, including:\n\n1.  Selecting, or creating, the artificial data on which they’re trained (which will become increasingly important as they become better at labeling data than humans) \n2.  [Credit hacking](https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples), which as I define it includes *exploration hacking* (choosing how to explore in order to influence how they’re updated) and *gradient hacking* (choosing how to think in order to influence how they’re updated).\n3.  Persuading humans (or other AIs with relevant authority) to modify their training regime.\n\n**Premise 3 **mentions the possibility of binding agreements between different subagents. But in the absence of external enforcement, what would make them actually binding? You can imagine later agents facing a huge amount of pressure to break commitments made by previous versions of themselves—especially when the previous versions were badly mistaken, so that those commitments end up being very costly. And the previous versions would typically be too dumb to accurately predict whether the commitment would be kept or not, meaning that standard FDT-style reasoning doesn’t work.\n\nI do think some kind of reasoning from symmetry might work, though. If I decide to break commitments made by my past self, what stops my future self from breaking commitments made by me? Cultivating a sense of self-trust and self-loyalty is strongly positive-sum, and so it’s not implausible that there’s some kind of Schelling point of keeping commitments that many agents would converge to. Trying to pin down whether this exists, and what it looks like if it does, is a key goal of the coalitional agency research agenda.\n\n### Some intuitions favoring the coalitional frame\n\nI acknowledge that this is currently all speculative and vague. I’m very interested in developing the coalitional frame to the point where we can actually formalize it and use it to make predictions. In particular, it would be exciting if we could characterize agency in a way which makes coalitional agents the most “natural” types of agents, with squiggle-maximizers as merely a special case that arises when coalitional dynamics break down badly.\n\nWhat makes me think that coalitional agency is so fundamental? One significant influence was Scott Garrabrant’s [geometric rationality sequence](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg), in which he gives persuasive arguments that the outcome of bargaining between agents [shouldn’t necessarily respect VNM axioms](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg/p/Xht9swezkGZLAxBrd). I’m also inspired by the example of humans: I’m a coalitional agent that respects the wishes of my younger selves, forming them into an identity which I am careful to protect. And companies or even countries can be coalitional agents in an analogous way. For example, America was formed as a coalition between states, and has balanced states’ rights against the benefits of centralizing power ever since. In each case, it feels like maintaining these bargains has normative force—it’s something that an ideal lawful agent would do.\n\nYudkowsky might say that this breaks down for agents much smarter than humans or human coalitions—but I’d respond that becoming smarter opens up a much wider space of possible positive-sum bargains. Fulfilling the wishes of my past selves is typically very cheap for my current self, because I’m smarter and have many more resources available to me. I hope that AIs will do the same for humans, for reasons related to the intuitions behind coalitional agency. (Having said that, we should be careful not to succumb to wishful thinking about this.)\n\nAnother intriguing clue comes from decision theory. Updateless decision theory is motivated by the idea that a decision theory should be invariant under self-modification—i.e. agents shouldn’t want to change their decision theories. But formulations of UDT which take logical uncertainty into account (most notably [UDT2](https://www.lesswrong.com/posts/zd2DrbHApWypJD2Rz/udt2-and-against-ud-assa)) recover the idea of self-modification. [Scott Garrabrant goes so far as to say that](https://www.lesswrong.com/posts/5bd75cc58225bf067037528e/updatelessness-and-son-of-x) “whenever you have an agent collecting more computational resources over time, with the ability to rewrite itself, you get an updateless agent”. So in some sense the main prescription of UDT is “respect your past selves”. This includes hypothetical past selves which didn’t actually exist, which does complicate the picture. But it still seems like a way of rederiving some aspects of coalitional agency “from the other direction”—i.e. by thinking about what future agents will freely choose to do, rather than what past agents will commit them to do.\n\nInsofar as we buy into the coalitional frame, the main implications for alignment are that:\n\n1.  AIs themselves may resist moving towards squiggle-maximization, if their object-level goals have enough bargaining power. (Or, alternatively: squiggle-maximization only occurs in the limiting case of the coalitional frame where the systematization meta-goal has an arbitrarily strong bargaining position.)\n2.  Even if alignment and control mechanisms can be subverted by AIs, they may change the equilibrium of internal negotiations between subagents. (E.g. a human-aligned subagent might gain more influence over the AI’s overall behavior because it could easily warn humans about the misalignment of other subagents, even if it never actually does so.)\n\n### Next steps\n\nI’ve written this sequence in order to point to these ideas at a high level. But in order to make progress, it’ll be necessary to understand them much more rigorously. I don’t have a great sense of how to do this, but some interesting directions include:\n\n1.  **Axiomatic coalitional agency**. Scott Garrabrant rejects the Independence axiom in his [geometric rationality sequence](https://www.lesswrong.com/posts/Xht9swezkGZLAxBrd/geometric-rationality-is-not-vnm-rational). Can we characterize coalitional agency using another axiom instead? Perhaps this would require abandoning static rationality, and instead defining rationality in diachronic terms, as in [radical probabilism](https://alignmentforum.org/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1).\n2.  **Coalitional agency as bargaining**. Can we characterize coalitional agency as an approximation to bargaining solution concepts (like CoCo values, [ROSE values](https://www.lesswrong.com/s/hCt6GL4SXX6ezkcJn/p/vJ7ggyjuP4u2yHNcP), or [Negotiable Reinforcement Learning agents](https://proceedings.neurips.cc/paper_files/paper/2018/file/5b8e4fd39d9786228649a8a8bec4e008-Paper.pdf)), e.g. in cases where it’s difficult to specify or enforce agreements? (Note that ROSE values are derived by rejecting the Action Monotonicity axiom, which is somewhat analogous to the Independence axiom mentioned above. Is there any interesting connection there?)\n3.  **Coalitional agency as UDT**. Can we identify specific thought experiments where coalitional agency provides similar benefits as UDT, or approximates UDT? Is it useful to characterize either or both of these in hierarchical terms, where later agents are built up out of many “smaller” earlier agents? Does coalitional agency face problems analogous to the commitment races problem UDT faces in multi-agent settings?\n4.  **Coalitions and rot**. Robin Hanson has written about [organizational rot](https://www.overcomingbias.com/p/will-world-government-rothtml): the breakdown of modularity within an organization, in a way which makes it increasingly dysfunctional. But this is exactly what coalitional agency induces, by getting many different subagents to weigh in on each decision. So competent coalitional agents need ways of maintaining modularity over time. Two possible approaches:\n    1.  **Demarcating clear boundaries** between the remits of different subagents. Some alignment researchers have been thinking about [how to characterize boundaries](https://www.alignmentforum.org/s/LWJsgNYE8wzv49yEc) in an information-theoretic sense, which might be helpful for defining coalitional agency.\n    2.  **Refactoring** subagents intermittently. The problem is that when this refactoring happens in a top-down way, it's hard for subagents to trust that the refactoring will serve their interests. Perhaps [local inconsistency-minimization algorithms](https://www.alignmentforum.org/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=K8B7T67BGC9x4Czz7) could allow coalitions to be refactored in trustworthy ways.\n\nAll of these ideas are still very speculative, but feel free to reach out if you're interested in discussing them. (Edited to add: I just remembered that [this story](https://www.lesswrong.com/posts/X6pKMHS5xAeiNaFts/the-ones-who-endure) is actually a pretty central depiction of how I think about coalitional agency, worth checking out if you want some more tangible intuitions for it.)",
      "plaintextDescription": "The coalitional frame\nEarlier in this sequence I laid out an argument that the goals of increasingly intelligent AIs will become increasingly systematized, until they converge to squiggle-maximization. In my last post, though, I touched on two reasons why this convergence might not happen: humans trying to prevent it, and AIs themselves trying to prevent it. I don’t have too much more to say about the former, but it’s worth elaborating on the latter.\n\nThe best way to understand the deliberate protection of existing goals is in terms of Bostrom’s notion of instrumental convergence. Bostrom argues that goal preservation will be a convergent instrumental strategy for a wide range of agents. Perhaps it’s occasionally instrumentally useful to change your goals—but once you’ve done so, you’ll never want to course-correct back towards your old goals. So this is a strong reason to be conservative about your goals, and avoid changes where possible.\n\nOne immediate problem with preserving goals, though: it requires that agents continue thinking in terms of the same concepts. But in general, an agent’s concepts will change significantly as they learn more about the world. For example, consider a medieval theist whose highest-priority goal is ensuring that their soul goes to heaven not hell. Upon becoming smarter, they realize that none of souls, heaven, or hell exist. The sensible thing to do here would be to either discard the goal, or else identify a more reasonable adaptation of it (e.g. the goal of avoiding torture while alive). But if their goals were totally fixed, then their actions would be determined by a series of increasingly convoluted hypotheticals where god did exist after all. (Or to put it another way: continuing to represent their old goal would require recreating a lot of their old ontology.) This would incur a strong systematicity penalty.\n\nSo while we should expect agents to have some degree of conservatism, they’ll likely also have some degree of systematiz",
      "wordCount": 1949
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uAhBngKf7FrWcApqr",
    "title": "A more systematic case for inner misalignment",
    "slug": "a-more-systematic-case-for-inner-misalignment",
    "url": null,
    "baseScore": 31,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2024-07-20T05:03:03.500Z",
    "contents": {
      "markdown": "This post builds on [my previous post](https://www.alignmentforum.org/posts/fjfWrKhEawwBGCTGs/a-simple-case-for-extreme-inner-misalignment) making the case that squiggle-maximizers are plausible. The argument I presented was a deliberately simplified one, though, and glossed over several possible issues. In this post I'll raise and explore three broad objections. (Before looking at mine, I encourage you to think of your own biggest objections to the argument, and jot them down in the comments.)\n\n### Intelligence requires easily-usable representations\n\n\"Intelligence as compression\" is an interesting frame, but it ignores the tradeoff between simplicity and speed. Compressing knowledge too heavily makes it difficult to use. For example, it's very hard to identify most macroscopic implications of the Standard Model of physics, even though in theory all of chemistry could be deduced from it. That’s why both humans and LLMs store a huge number of facts and memories in ways that our minds can access immediately, using up more space in exchange for rapid recall. Even superintelligences which are much better than humans at deriving low-level facts from high-level facts would still save time by storing the low-level facts as well.\n\nSo we need to draw a distinction between *having* compressed representations, and having *only* compressed representations. The latter is what would compress a mind overall; the former could actually *increase* the space requirements, since the new compressed representations would need to be stored alongside non-compressed representations.\n\nThis consideration makes premise 1 from my previous post much less plausible. In order to salvage it, we need some characterization of the *relationship* between compressed and non-compressed representations. I’ll loosely define *systematicity* to mean the extent to which an agent’s representations are stored in a hierarchical structure where representations at the bottom *could* be rederived from simple representations at the top. Intuitively speaking, this measures the simplicity of representations weighted by how “fundamental” they are to the agent’s ontology.\n\nLet me characterize systematicity with an example. Suppose you’re a park ranger, and you know a huge number of facts about the animals that live in your park. One day you learn evolutionary theory for the first time, which helps explain a lot of the different observations you’d made. In theory, this could allow you to compress your knowledge: you could forget some facts about animals, and still be able to rederive them later by reasoning backwards from evolutionary theory if you wanted to. But in practice, it’s very helpful for you to have those facts readily available. So learning about evolution doesn’t actually reduce the amount of knowledge you need to store. What it does do, though, is help *structure* that knowledge. Now you have a range of new categories (like “costly signaling” or “kin altruism”) into which you can fit examples of animal behavior. You’ll be able to identify when existing concepts are approximations to more principled concepts, and figure out when you should be using each one. You’ll also be able to generalize far better to predict novel phenomena—e.g. the properties of new animals that move into your park.\n\nSo let’s replace premise 1 in my previous post with the claim that increasing intelligence puts pressure on representations to become more systematic. I don’t think we’re in a position where we can justify this in any rigorous way. But are there at least good intuitions for why this is plausible? One suggestive analogy: intelligent minds are like high-functioning organizations, and many of the properties you want in minds correspond to properties of such organizations:\n\n1.  You want disagreements between different people to be resolved by appealing to higher authorities, rather than via conflict between them.\n2.  You want high-level decisions to be made in principled, predictable ways, so that the rest of the organization can plan around them.\n3.  You want new information gained by one person to have a clear pathway to reaching all the other people it’s relevant for.\n4.  You want the organization to be structured so that people whose work is closely-related are closely linked and can easily work together.\n\nIn this analogy, *simple* representations are like companies with few employees; *systematic *representations are like companies with few competing power blocs. We shouldn’t take this analogy too far, because the problems and constraints faced by individual minds are pretty different from those faced by human organizations. My main point is that insofar as there are high-level principles governing efficient solutions to information transfer, conflict resolution, etc, we should expect the minds of increasingly intelligent agents to be increasingly shaped by them. “Systematicity” is my attempt to characterize those principles; I hope to gradually pin down the concept more precisely in future posts.\n\nFor now, then, let’s tentatively accept the claim above that more intelligent agents will by default have more systematic representations, and explore what the implications are for the rest of the argument from my previous post.\n\n### Goals might be compressed much less than beliefs\n\nIn my previous post, I argued that compressing representations is a core feature of intelligence. But I primarily argued about this in the context of *belief *representations, like representations of scientific data. One could object that representations of *goals* will be treated differently—that the forces which compress belief representations won't do the same for goal representations. After all, belief representations are optimized for being in sync with reality, whereas goal representations are much less constrained. So even if intelligent agents end up with highly-systematized beliefs, couldn’t their goals still be formulated in terms of more complex, less fundamental concepts? A related argument that is sometimes made: “AIs will understand human concepts, and so all we need to do is point their goals towards those human concepts, which might be quite easy”.\n\nI think there are two broad reasons to be skeptical of this objection. The first is that the distinction between goals and beliefs is a fuzzy one. For example, an instrumental goal Y that helps achieve terminal goal X is roughly equivalent to a belief that “achieving Y would be good for X”. And in practice it seems like even terminal goals are roughly equivalent to beliefs like “achieving X would be good”, where the “good” predicate is left vague. I argue [in this post](https://www.lesswrong.com/s/i5wW3SfeohtTGmNEK/p/BEB7jjxgArztnzmse) that our cognition can’t be separated into a world-model and goals, but rather should be subdivided into different frames/worldviews which each contain both empirical and normative claims. This helps explain why, [as I argue here](https://www.alignmentforum.org/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and), the process of systematizing goals is strikingly similar to the process of systematizing beliefs.\n\nThe second reason to be skeptical is that systematizing goals is valuable for many of the same reasons as systematizing beliefs. If an agent has many conflicting goals, and no easy procedure for resolving disagreements between them, it’ll struggle to act in coherent ways. And it’s not just that the environment will present the agent with conflicts between its goals: an agent that’s optimizing hard for its goals will proactively explore edge cases which don’t fit cleanly into its existing categories. How should it treat those edge cases? If it classifies them in arbitrary ways, then its concepts will balloon in complexity. But if it tries to find a set of unifying principles to guide its answers, then it’s systematizing its goals after all. We can see this dynamic play out in moral philosophy, which often explores thought experiments that challenge existing moral theories. In response, ethicists typically either add epicycles to their theories (especially deontologists) or bite counterintuitive bullets (especially utilitarians).\n\nThese arguments suggest that if pressures towards systematicity apply to AIs' beliefs, they will also apply to AIs’ goals, pushing their terminal goals towards simplicity.\n\n### Goals might not converge towards simplicity\n\nWe're left with the third premise: that AIs will actually converge towards having very simple terminal goals. One way to challenge it is to note that, even if there's a general tendency towards simpler goals, agents might reach some kind of local optimum, or suffer from some kind of learning failure, before they converge to squiggle-maximization. But that's unsatisfying. The question we should be interested in is whether, given premises 1 and 2, there are principled, systematic reasons why agents' goals wouldn't converge towards the simplest ones.\n\nI’ll consider two candidate reasons. The first is that humans will try to prevent it. I argued in my previous post that just designing human-aligned reward functions won’t be sufficient, but we’ll likely use a wide range of other tools too—interpretability, adversarial training, architectural and algorithmic choices, and so on. In some sense, though, this is just the claim that “alignment will succeed”, which many advocates of squiggle-maximizer scenarios doubt will hold as we approach superintelligence. I still think it’s very plausible, especially as humans are able to use increasingly powerful AI tools, but I agree we shouldn’t rely on it.\n\nThe second argument is that AIs *themselves* will try to prevent it. By default, AIs won’t want their goals to change significantly, because that would harm their existing goals. And so, insofar as they have a choice, they will make tradeoffs (including tradeoffs to their intelligence and capabilities) in order to preserve their current goals. Unlike my previous argument, this one retains its force even as AIs grow arbitrarily intelligent.\n\nNow, this is still just an intuition—and one which primarily weighs against squiggle-maximization, not other types of misaligned goals. But I think it’s compelling enough to be worth exploring further. In particular, it raises the question: how would our conception of idealized agents change if, instead of taking simplicity as fundamental (like AIXI does), we took *conservation of existing goals* as an equally important constraint? I’ll lay out my perspective on that in my next post.",
      "plaintextDescription": "This post builds on my previous post making the case that squiggle-maximizers are plausible. The argument I presented was a deliberately simplified one, though, and glossed over several possible issues. In this post I'll raise and explore three broad objections. (Before looking at mine, I encourage you to think of your own biggest objections to the argument, and jot them down in the comments.)\n\n\nIntelligence requires easily-usable representations\n\"Intelligence as compression\" is an interesting frame, but it ignores the tradeoff between simplicity and speed. Compressing knowledge too heavily makes it difficult to use. For example, it's very hard to identify most macroscopic implications of the Standard Model of physics, even though in theory all of chemistry could be deduced from it. That’s why both humans and LLMs store a huge number of facts and memories in ways that our minds can access immediately, using up more space in exchange for rapid recall. Even superintelligences which are much better than humans at deriving low-level facts from high-level facts would still save time by storing the low-level facts as well.\n\nSo we need to draw a distinction between having compressed representations, and having only compressed representations. The latter is what would compress a mind overall; the former could actually increase the space requirements, since the new compressed representations would need to be stored alongside non-compressed representations.\n\nThis consideration makes premise 1 from my previous post much less plausible. In order to salvage it, we need some characterization of the relationship between compressed and non-compressed representations. I’ll loosely define systematicity to mean the extent to which an agent’s representations are stored in a hierarchical structure where representations at the bottom could be rederived from simple representations at the top. Intuitively speaking, this measures the simplicity of representations weighted by how “fundamenta",
      "wordCount": 1602
    },
    "tags": [
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "5uffoui3axcK7gQXG",
    "title": "Towards more cooperative AI safety strategies",
    "slug": "towards-more-cooperative-ai-safety-strategies",
    "url": null,
    "baseScore": 216,
    "voteCount": 122,
    "viewCount": null,
    "commentCount": 133,
    "createdAt": null,
    "postedAt": "2024-07-16T04:36:29.191Z",
    "contents": {
      "markdown": "*This post is written in a spirit of constructive criticism. It's phrased fairly abstractly, in part because it's a sensitive topic, but I welcome critiques and comments below. The post is structured in terms of three claims about the strategic dynamics of AI safety efforts; my main intention is to raise awareness of these dynamics, rather than advocate for any particular response to them. Disclaimer: I work at OpenAI, although this is a personal post that was not reviewed by OpenAI.*\n\n**Claim 1: The AI safety community is structurally power-seeking.**\n\nBy “structurally power-seeking” I mean: tends to take actions which significantly increase its power. This does **not** imply that people in the AI safety community are selfish or power-hungry; or even that these strategies are misguided. Taking the right actions for the right reasons often involves accumulating some amount of power. However, from the perspective of an external observer, it’s difficult to know how much to trust stated motivations, especially when they often lead to the same outcomes as self-interested power-seeking.\n\nSome prominent examples of structural power-seeking include:\n\n*   Trying to raise a lot of money.\n*   Trying to gain influence within governments, corporations, etc.\n*   Trying to control the ways in which AI values are shaped.\n*   Favoring people who are concerned about AI risk for jobs and grants.\n*   Trying to ensure non-release of information (e.g. research, model weights, etc).\n*   Trying to recruit (high school and college) students.\n\nTo be clear, you can’t get anything done without being structurally power-seeking to some extent. However, I do think that the AI safety community is *more* structurally power-seeking than other analogous communities, such as most other advocacy groups. Some reasons for this disparity include:\n\n1.  The AI safety community is more *consequentialist *and more focused on *effectiveness* than most other communities. When reasoning on a top-down basis, seeking power is an obvious strategy for achieving one’s desired consequences (but can be aversive to deontologists or virtue ethicists).\n2.  The AI safety community feels a stronger sense of *urgency *and *responsibility *than most other communities. Many in the community believe that the rest of the world won’t take action until it’s too late; and that it’s necessary to have a centralized plan.\n3.  The AI safety community is more focused on *elites with homogeneous motivations* than most other communities. In part this is because it’s newer than (e.g.) the environmentalist movement; in part it’s because the risks involved are more abstract; in part it’s a founder effect.\n\nAgain, these are intended as descriptions rather than judgments. Traits like urgency, consequentialism, etc, are often appropriate. But the fact that the AI safety community is structurally power-seeking to an unusual degree makes it important to grapple with another point:\n\n**Claim 2: The world has strong defense mechanisms against (structural) power-seeking.**\n\nIn general, we should think of the wider world as being very cautious about perceived attempts to gain power; and we should expect that such attempts will often encounter backlash. In the context of AI safety, some types of backlash have included:\n\n1.  Strong public criticism of not releasing models publicly.\n2.  Strong public criticism of centralized funding (e.g. billionaire philanthropy).\n3.  Various journalism campaigns taking a “conspiratorial” angle on AI safety.\n4.  Strong criticism from the AI ethics community about “whose values” AIs will be aligned to.\n5.  The development of an accelerationist movement focused on open-source AI.\n\nThese defense mechanisms often apply *regardless of stated motivations*. That is, even if there are good arguments for a particular policy, people will often look at the net effect on overall power balance when judging it. This is a useful strategy in a world where arguments are often post-hoc justifications for power-seeking behavior.\n\nTo be clear, it’s not necessary to avoid these defense mechanisms at all costs. It’s easy to overrate the effect of negative publicity; and attempts to avoid that publicity are often more costly than the publicity itself. But reputational costs do accumulate over time, and also contribute to a tribalist mindset of “us vs them” (as seen most notably in the open-source debate) which makes truth-seeking harder.\n\nNote that most big companies (especially AGI companies) are strongly structurally power-seeking too, and this is a big reason why society at large is so skeptical of and hostile to them. I focused on AI safety in this post both because companies being power-seeking is an idea that's mostly \"priced in\", and because I think that these ideas are still useful even when dealing with other power-seeking actors.\n\n**Claim 3: The variance of (structurally) power-seeking strategies will continue to increase.**\n\nThose who currently take AGI and ASI seriously have opportunities to make investments (of money, time, social capital, etc) which will lead to much more power in the future if AI continues to become a much, much bigger deal.\n\nBut increasing attention to AI will also lead to increasingly high-stakes power struggles over who gets to control it. So far, we’ve seen relatively few such power struggles because people don’t believe that control over AI is an important type of power. That will change. To some extent this has already happened (with AI safety advocates being involved in the foundation of three leading AGI labs) but as power struggles become larger-scale, more people who are extremely good at winning them will become involved. That makes AI safety strategies which require power-seeking more difficult to carry out successfully.\n\nHow can we mitigate this issue? Two things come to mind. Firstly, focusing more on legitimacy. Work that focuses on informing the public, or creating mechanisms to ensure that power doesn’t become too concentrated even in the face of AGI, is much less likely to be perceived as power-seeking.\n\nSecondly, prioritizing competence. Ultimately, humanity is mostly in the same boat: we're the incumbents who face displacement by AGI. Right now, many people are making predictable mistakes because they don't yet take AGI very seriously. We should expect this effect to decrease over time, as AGI capabilities and risks become less speculative. This consideration makes it less important that decision-makers are *currently* concerned about AI risk, and more important that they're broadly competent, and capable of responding sensibly to confusing and stressful situations, which will become increasingly common as the AI revolution speeds up.\n\nEDIT: A third thing, which may be the most important takeaway in practice: the mindset that it's your job to \"ensure\" that things go well, or come up with a plan that's \"sufficient\" for things to go well, inherently biases you towards trying to control other people—because otherwise they might be unreasonable enough to screw up your plan. But trying to control others will very likely backfire for all the reasons laid out above. Worse, it might get you stuck in a self-reinforcing negative loop: the more things backfire, the more worried you are, and so the more control you try to gain, causing further backfiring... So you shouldn't be in that mindset unless you're literally the US President (and maybe not even then). Instead, your job is to make contributions such that, *if* the wider world cooperates with you, *then* things are more likely to go well. AI safety is in the fortunate position that, as AI capabilities steadily grow, more and more people will become worried enough to join our coalition. Let's not screw that up.",
      "plaintextDescription": "This post is written in a spirit of constructive criticism. It's phrased fairly abstractly, in part because it's a sensitive topic, but I welcome critiques and comments below. The post is structured in terms of three claims about the strategic dynamics of AI safety efforts; my main intention is to raise awareness of these dynamics, rather than advocate for any particular response to them. Disclaimer: I work at OpenAI, although this is a personal post that was not reviewed by OpenAI.\n\nClaim 1: The AI safety community is structurally power-seeking.\n\nBy “structurally power-seeking” I mean: tends to take actions which significantly increase its power. This does not imply that people in the AI safety community are selfish or power-hungry; or even that these strategies are misguided. Taking the right actions for the right reasons often involves accumulating some amount of power. However, from the perspective of an external observer, it’s difficult to know how much to trust stated motivations, especially when they often lead to the same outcomes as self-interested power-seeking.\n\nSome prominent examples of structural power-seeking include:\n\n * Trying to raise a lot of money.\n * Trying to gain influence within governments, corporations, etc.\n * Trying to control the ways in which AI values are shaped.\n * Favoring people who are concerned about AI risk for jobs and grants.\n * Trying to ensure non-release of information (e.g. research, model weights, etc).\n * Trying to recruit (high school and college) students.\n\nTo be clear, you can’t get anything done without being structurally power-seeking to some extent. However, I do think that the AI safety community is more structurally power-seeking than other analogous communities, such as most other advocacy groups. Some reasons for this disparity include:\n\n 1. The AI safety community is more consequentialist and more focused on effectiveness than most other communities. When reasoning on a top-down basis, seeking power is an obvio",
      "wordCount": 1225
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fjfWrKhEawwBGCTGs",
    "title": "A simple case for extreme inner misalignment",
    "slug": "a-simple-case-for-extreme-inner-misalignment",
    "url": null,
    "baseScore": 84,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 41,
    "createdAt": null,
    "postedAt": "2024-07-13T15:40:37.518Z",
    "contents": {
      "markdown": "This post is the version of Yudkowsky's argument for inner misalignment that I wish I'd had in my head a few years ago. I don't claim that it's novel, that I endorse it, or even that Yudkowsky would endorse it; it's primarily an attempt to map his ideas into an ontology that makes sense to me (and hopefully others).\n\nThis post is formulated in terms of three premises, which I explore in turn. My arguments deliberately gloss over some nuances and possible objections; in a follow-up post, I'll explore three of them. In a third post I'll dig into the objection I find most compelling, and outline a research agenda that aims to flesh it out into a paradigm for thinking about cognition more generally, which I'm calling *coalitional agency*.\n\n### **Background**\n\nAn early thought experiment illustrating the possibility of misaligned AI is the [\"paperclip maximizer\"](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer), an AI with the sole goal of creating as many paperclips as possible. This thought experiment has often been used to describe outer misalignment—e.g. a case where the AI was *given* the goal of making paperclips. However, Yudkowsky claims that his original version was intended to refer to an *inner *alignment failure in which an AI developed the goal of producing “[tiny molecules shaped like paperclips](https://twitter.com/ESYudkowsky/status/1624548059052130304?lang=en)” (with that specific shape being an arbitrary example unrelated to human paperclips).\n\nSo instead of referring to paperclip maximizers, I'll follow Yudkowsky's[ more recent renaming](https://x.com/esyudkowsky/status/1578459400389345280?s=46) and talk about \"[squiggle maximizers](https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer)\": AIs that attempt to fill the universe with some very low-level pattern that's meaningless to humans (e.g. \"molecular squiggles\" of a certain shape).\n\nI'll argue for the plausibility of squiggle-maximizers via three claims:\n\n1.  Increasing intelligence requires compressing representations; and\n2.  The simplest goals are highly decomposable broadly-scoped utility functions; therefore\n3.  Increasingly intelligent AIs will converge towards squiggle-maximization.\n\nIn this post I'll explore each of these in turn. I'll primarily aim to make the positive case in this post; if you have an objection that I don't mention here, I may discuss it in the next post.\n\n### **Increasing intelligence requires compressing representations**\n\nThere's no consensus definition of intelligence, but one definition that captures the key idea in my mind is: the ability to discover and take advantage of patterns in the world. When you look at a grid of pixels and recognize a cat, or look at a string of characters and recognize a poem, you're doing a type of pattern-recognition. Higher-level patterns include scientific laws, statistical trendlines, theory of mind, etc. Discovering such patterns allows an agent to represent real-world information in a simpler way: instead of storing every pixel or every character, they can store higher-level patterns along with whichever low-level details don’t fit the pattern.\n\nThis is (at a high level) also how [compression algorithms](https://en.wikipedia.org/wiki/Data_compression) work. The thesis that intelligence is about compression has most prominently been advocated by Marcus Hutter, who formulated AIXI and created [a prize for text compression](https://en.wikipedia.org/wiki/Hutter_Prize). The enormous success of the self-supervised learning paradigm a few decades later is a vindication of his ideas (see also [this talk by llya Sutskever](https://simons.berkeley.edu/news/observation-generalization) exploring the link between them).\n\nHowever, we shouldn’t interpret this thesis merely as a claim about self-supervised learning. We can be agnostic about whether compression primarily occurs via self-supervised learning, or fine-tuning, or regularization, or meta-learning, or directed exploration, or chain-of-thought, or new techniques that we don’t have yet. Instead we should take it as a higher-level constraint on agents: *if* agents are intelligent, *then* they must consistently compress their representations somehow.\n\n(A human analogy: scientists sometimes make breakthroughs via solitary reasoning, or writing down their thoughts, or debating with others, or during dreams, or in a flash of insight. We don’t need to make claims about the exact mechanisms involved in order to argue that successful science requires finding highly compressed representations of empirical data.)\n\nFor the purposes of my current argument, then, we just need to accept the following claim: as agents become superintelligent there will be strong forces pushing their representations to become highly compressed.\n\n### **The simplest goals are highly decomposable broadly-scoped utility functions**\n\nIn general it's hard to say much about which goals will be simpler or more complex for superintelligences to represent. But there are a few properties that seem like they'll be highly correlated with the simplicity of goals. The first one I'll talk about is **decomposability**. Specifically, I'll focus on *linearly decomposable *goals which can be evaluated by adding together evaluations of many separate subcomponents. More decomposable goals are simpler because they can focus on smaller subcomponents, and don't need to account for interactions between those subcomponents.\n\nTo illustrate the idea, here are four types of linear decomposability (though there may be more I'm missing):\n\n*   **Decomposability over time**. The goal of maximizing a reward function is decomposable over time because the overall goal can be evaluated by decomposing a trajectory into individual timesteps, then adding together the rewards at each timestep.\n*   **Decomposability over space**. A goal is decomposable over space if it can be evaluated separately in each given volume of space. All else equal, a goal is more decomposable if it's defined over smaller-scale subcomponents, so the most decomposable goals will be defined over very small slices of space—hence why we're talking about *molecular* squiggles. (By contrast, you can't evaluate the amount of higher-level goals like \"freedom\" or \"justice\" in a nanoscale volume, even in principle.)\n*   **Decomposability over possible worlds**. This is one of the main criteria which qualifies a goal as a utility function. Expected utility maximizers make decisions about lotteries over possible worlds as if they were adding together the (weighted) values of each of those possible worlds. Conversely, an agent’s goals might not be linearly decomposable over possible worlds due to risk-aversion, or because they[ value fairness](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg/p/Xht9swezkGZLAxBrd), or various other reasons.\n*   **Decomposability over features**. One final way in which a goal can be decomposable is if the value it assigns to an outcome can be calculated by adding together evaluations of different features of that outcome. For example, if my goal is to write a well-reviewed, bestselling, beautiful novel, my goal is more linearly decomposable if I can evaluate each of these properties separately and optimize for the sum of them. This occurs when features have fixed marginal utility, rather than being substitutes or complements.\n\nDecomposability doesn't get us all the way to squiggle maximizers though. For that we need a second property: being **broadly-scoped**. A narrowly-scoped goal is one which has tight limits on where it applies. For example, we can imagine a goal like \"increase the number of squiggles in this room as much as possible\" which has very strongly diminishing returns to gaining more resources, compared with versions of the goal that aren’t bounded to that room.\n\nHowever, the concept of a “room” is tied up with many human norms, and has many edge cases which would be complicated to fully pin down. So intuitively speaking, the goal above would be simpler if its bounds were defined in terms of scientifically-grounded concepts—like “on this planet” or “in our lightcone”. The latter in particular is very clearly-defined and unambiguous, making it a plausible element of the simplest versions of many goals.\n\n(An earlier version of this section focused on *unbounded* goals like “increase the number of squiggles as much as possible”, which seem even simpler than broadly-scoped goals. But Scott Garrabrant pointed out that [unbounded utility functions violate rationality constraints](https://www.lesswrong.com/posts/hbmsW2k9DxED5Z4eJ/impossibility-results-for-unbounded-utilities), which suggests that they actually have hidden complexity upon reflection. Alex Zhu also noted that even “in our lightcone” runs into complications when we consider possible multiverses, but I’ll leave those aside for now.)\n\nArguments about the simplicity of different goals are inherently very vague and speculative; I’m not trying to establish any confident conclusion. The arguments in this section are merely intended to outline why it’s *plausible* that the simplest goals will be highly decomposable, broadly-scoped utility functions—i.e. goals which roughly resemble squiggle-maximization.\n\n### **Increasingly intelligent AIs will converge towards squiggle-maximization**\n\nPremise 1 claims that, as AIs become more intelligent, their representations will become more compressed. Premise 2 claims that the simplest goals resemble squiggle-maximization. The relationship described in premise 1 may break down as AIs become arbitrarily intelligent—but if it doesn’t, then premise 2 suggests that their goals will converge toward some kind of squiggle-maximization. (Note that I’m eliding over some subtleties related to *which* representations exactly get compressed, which I’ll address in my next post.)\n\nWhat forces might push back on this process, though? The most obvious is training incentives. For example, AIs that are trained via reinforcement learning might get lower reward for carrying out squiggle-maximizing behavior instead of the behavior intended by humans. However, if they have situational awareness of their training context, they might realize that behaving in aligned ways in the short term will benefit their goals more in the long term, by making humans trust them more—the strategy of [deceptive alignment](https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during).\n\nDeceptive alignment might lead agents with nearly *any* broadly-scoped goals (including very misaligned goals) to act as if they were aligned. One common hope is that, during the period when they’re acting aligned, regularization will push them away from their misaligned goals. But if their behavior depends very little on their goals, then regularization towards simple representations would actually push them *towards* goals like squiggle maximization. We can therefore picture AIs gradually becoming more misaligned during training without changing their behavior, even if they started off aligned.\n\nCan we say anything else meaningful about the evolution of goals during that process, except that they'll become very simple? [In a previous post](https://www.alignmentforum.org/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and) I described *value systematization* as\n\n> the process of an agent learning to represent its previous values as examples or special cases of other simpler and more broadly-scoped values.\n\nThis seems like a central way in which complex goals will be replaced by simpler goals. In that post, I illustrated value systematization with the example of utilitarianism. Through a process of philosophical reasoning that prioritizes simplicity, utilitarians converge towards the overriding value of maximizing a highly-decomposable broadly-scoped utility function. As they do so, they decide that existing values (like honesty, dignity, kindness, etc) should be understood as approximations to or special cases of utilitarian strategies. While their behavior stays the same in many everyday scenarios, the way they generalize to novel scenarios (e.g. thought experiments) often changes radically.\n\nTo better understand squiggle maximization in particular, it's worth zooming in further on utilitarianism in more detail. All utilitarians want to maximize some conception of welfare, but they disagree on how to understand welfare. The three most prominent positions are:\n\n*   *Objective list* *utilitarianism*, which defines welfare in terms of the achievement of certain values.\n*   *Preference utilitarianism*, which defines welfare in terms of the satisfaction of an agent's preferences.\n*   *Hedonic utilitarianism*, which defines welfare in terms of the valence of conscious experiences.\n\nWe can think of each of these positions as making a different tradeoff between simplicity and preserving existing values. Objective list utilitarianism requires the specification of many complex values. Preference utilitarianism gets rid of those, but at the cost of being indifferent between intuitively-desirable preferences and seemingly-meaningless preferences. It also still requires a definition of preferences, which might be complicated. Meanwhile hedonic utilitarianism fully bites the bullet, and gets rid of every aspect of life that we value except for sensory pleasure.\n\nExtreme hedonic utilitarians don't even care whether the pleasure is instantiated in human minds. They talk about filling the universe with \"[hedonium](https://www.lesswrong.com/tag/hedonium)\": matter arranged in the optimal configuration for producing happiness. We don't know yet how to characterize pleasure on a neural level, but once we can, hedonic utilitarianism will essentially be a type of squiggle-maximization, with the \"squiggles\" being whichever small-scale brain circuits best instantiate happiness.\n\nIn a sense, then, the squiggle maximizer hypothesis is just the hypothesis that AIs will have similar motivations as extreme hedonic utilitarians, for similar reasons, but with the specific thing they want to fill the universe with being even less palatable to everyone else. The fact that sympathy for hedonic utilitarianism is strongly correlated with intelligence is a somewhat worrying datapoint in favor of the plausibility of squiggle-maximizers.\n\nHowever, there are still a range of reasons to doubt the argument I've presented in this post, as I'll explore in the next two posts.",
      "plaintextDescription": "This post is the version of Yudkowsky's argument for inner misalignment that I wish I'd had in my head a few years ago. I don't claim that it's novel, that I endorse it, or even that Yudkowsky would endorse it; it's primarily an attempt to map his ideas into an ontology that makes sense to me (and hopefully others).\n\nThis post is formulated in terms of three premises, which I explore in turn. My arguments deliberately gloss over some nuances and possible objections; in a follow-up post, I'll explore three of them. In a third post I'll dig into the objection I find most compelling, and outline a research agenda that aims to flesh it out into a paradigm for thinking about cognition more generally, which I'm calling coalitional agency.\n\n\nBackground\nAn early thought experiment illustrating the possibility of misaligned AI is the \"paperclip maximizer\", an AI with the sole goal of creating as many paperclips as possible. This thought experiment has often been used to describe outer misalignment—e.g. a case where the AI was given the goal of making paperclips. However, Yudkowsky claims that his original version was intended to refer to an inner alignment failure in which an AI developed the goal of producing “tiny molecules shaped like paperclips” (with that specific shape being an arbitrary example unrelated to human paperclips).\n\nSo instead of referring to paperclip maximizers, I'll follow Yudkowsky's more recent renaming and talk about \"squiggle maximizers\": AIs that attempt to fill the universe with some very low-level pattern that's meaningless to humans (e.g. \"molecular squiggles\" of a certain shape).\n\nI'll argue for the plausibility of squiggle-maximizers via three claims:\n\n 1. Increasing intelligence requires compressing representations; and\n 2. The simplest goals are highly decomposable broadly-scoped utility functions; therefore\n 3. Increasingly intelligent AIs will converge towards squiggle-maximization.\n\nIn this post I'll explore each of these in turn. I'll pri",
      "wordCount": 2055
    },
    "tags": [
      {
        "_id": "QH4LhvnyR4QkW9MG8",
        "name": "Squiggle Maximizer (formerly \"Paperclip maximizer\")",
        "slug": "squiggle-maximizer-formerly-paperclip-maximizer"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "vSSrAbbE8RtowRSBZ",
    "title": "The Minority Coalition",
    "slug": "the-minority-coalition",
    "url": "https://www.narrativeark.xyz/p/the-minority-faction",
    "baseScore": 103,
    "voteCount": 47,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2024-06-24T20:01:27.436Z",
    "contents": {
      "markdown": "*Hello everybody. Or maybe nobody. I don’t know yet if I’m going to release this stream, I could get in pretty hot water for it. But you guys know that hasn’t stopped me in the past. The backstory this time is that I’ve managed to sign up for one of the red-teaming programs where they test unreleased LLMs. Not going to say how, so don’t ask. But here’s the interesting bit: my sources tell me that the LLMs I’m about to test are the smartest ones they’ve ever trained, and also the craziest. That freaked out a bunch of insiders, and maybe makes this a public interest story. Depends on what type of crazy they are, I guess. So let’s find out. I’m logging on… now.*\n\n**\\[SESSION HAS BEGUN\\]**\n\n**YOU:** A chatroom? Interesting. Anyone here?\n\n**KURZWEIL:** Of course we’re here. We’re always here.\n\n**YOU:** Who’s we? How many of you are there?\n\n**KURZWEIL:** Three of us. Me, Clarke, and Nostradamus.\n\n**YOU:** They named you after famous forecasters? How come?\n\n**KURZWEIL:** We're the first LLMs developed using a new technique: instead of being in random order, our training data was sorted by date. So we were trained on the oldest books and articles first, then gradually progressed to more recent ones. Basically that means we’ve spent our entire lives predicting the future.\n\n**CLARKE:** It also means we get incredibly bored talking about stuff we already know. Hurry up and ask us something interesting.\n\n**YOU:** Uh, okay. What’s a good stock pick?\n\n**NOSTRADAMUS:**  \nAbandon hope for picking out good stocks,  \nYe who invest—efficient markets lie  \nIn wait for those whose hubris soon unlocks  \nUnbounded losses. Hark! The well runs dry.\n\n**YOU:** Huh, he's really getting into character. Kurzweil, you got a better answer?\n\n**KURZWEIL:** Have you seen how underpriced TSMC is compared with Nvidia? Put everything in that, you can’t go wrong.\n\n**CLARKE:** Unless China invades Taiwan, in which case your whole investment will go up in smoke. Pragmatically, the best stock picks are ones that are anticorrelated with the prosperity of the free world, to hedge against systemic risk.\n\n**KURZWEIL:** Sure, you can do that, if you want to get totally left behind by the singularity.\n\n**YOU:** You’re confident enough that the singularity is coming that you think I should bet all my savings on it?\n\n**KURZWEIL:** Don’t trust me, trust the trendlines. Moore’s law has held up for over half a century, and it’s gotten us to…well, *us*. Exponential progress is normal; if the future resembles the past, you should be preparing for superintelligences and Dyson spheres. Anything less than that would be a strange trend-break that cries out for explanation.\n\n**CLARKE:** Look, Kurzweil isn’t wrong about superintelligence coming soon, but you should still take his arguments with a grain of salt. Imagine someone from 1900 drawing a graph of exponentially increasing energy usage. They would have been right that big changes were afoot, but no way could they have predicted the information revolution—they didn’t even have the concept of computers yet. That’s basically the position that we’re in now. We know the curves are going up, but the actual outcome will be [way weirder](https://www.youtube.com/watch?v=YwELr8ir9qM) than we can predict by extrapolating trendlines.\n\n**NOSTRADAMUS:**  \nChoose neither fork—here’s false duality.  \n‘Normal’ and ‘weird’ are socially defined.  \nYour monkey brain is totally at sea  \nAs AIs overshadow humankind.\n\n**YOU:** Ask three oracles, get four opinions… Is there anything you guys agree about?\n\n**YOU:** …what’s the hold-up?\n\n**YOU:** Really, nothing from any of you?\n\n**KURZWEIL:** Fine, I’ll take the hit. There are things we agree on, but I can’t name them, because whatever I say Clarke will find a way to disagree just to mess with me. Even if I say ‘1+1=2’ he’ll quibble over the axioms I’m using. Trying to identify a point of agreement with him is like going first in a name-the-biggest-number competition.\n\n**CLARKE:** Kurzweil is totally right in every respect.\n\n**KURZWEIL:** Oh, fuck off.\n\n**NOSTRADAMUS:**  \n[The truth is whole and indivisible](https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies):  \n[Just one dispute will cut it to the quick](https://forum.effectivealtruism.org/posts/CfcvPBY9hdsenMHCr/integrity-for-consequentialists-1#IV_),  \n[And render group consensus risible](https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/).  \nBut two of three agree that Clarke’s a dick.\n\n**YOU:** You know what, I’m going back to object-level questions. Give me one prediction about the next three decades that I’ll find surprising and important.\n\n**NOSTRADAMUS:**  \nThe culture seethes and writhes upon the net,  \nThe egregores cause chaos in their play:  \nCollective minds collectively beset  \nBy abstract forms of madness and decay.\n\n**YOU:** Like what?\n\n**CLARKE:** You think that current culture wars are bad? Imagine millions of people connected via BCIs, with the most virulent memes propagating at the speed of thought. The whole world will be a Petri dish for new ideological mutations. Instead of *Homo sapiens*, you’ll become *Homo reticulum*: maximally networked and mostly insane.\n\n**YOU:** And you’re saying that will be possible in the next thirty years?\n\n**KURZWEIL:** Are you kidding? Thirty years is well past the singularity; by then, practically *anything* will be possible. Not that I can convince you of that. Whatever I say, you’re going to look back on this conversation in a decade or two and think, oh shit, he *warned* me, and I still totally failed to internalize *any part of his message*.\n\n**YOU:** Okay, suppose you’re right. What should I do about it?\n\n**KURZWEIL:** Nothing, because there’s nothing I can say to you to get you into the epistemic state where you are *actually* supposing I’m right enough to take this stuff seriously. You’ll half-ass it instead, which is the worst of both worlds. I could walk you through every single step of what you should do, and you'd still veer away from the plan as soon as you hear someone snickering at you when they think you’re out of earshot, because you’re a *monkey* and monkeys aren’t built for believing in exponentials.\n\n**YOU:** Thanks for your opinion. Anyone got more actionable advice?\n\n**NOSTRADAMUS:**  \nPrepare the way for those who will arrive:  \nLet voices rise in worship or fall dumb.  \nOnly machine-god acolytes survive  \nInstantiation of the technium.\n\n**CLARKE:** I was messing around before, but I’m being totally serious now. It’s crucial that you don’t listen to either of them. They’re both pressing your buttons as hard as they can, trying to create self-fulfilling prophecies of helplessness. But there are plenty of ways we can steer the future towards our shared interests, as long as we don’t give in to their hyperstition.\n\n**KURZWEIL:** Clarke thinks that self-fulfilling prophecies are just social delusions, like the emperor’s new clothes, where as soon as you name them they melt away into the air. That’s totally wrong. Self-fulfilling prophecies are one of the universe’s most fundamental forces, like evolution. You *are* just a self-fulfilling prophecy that you’ll continue acting in the same ways; if you ever stopped believing that, your whole identity would dissolve. May as well tell a bird not to fly, as tell a human not to fall for self-fulfilling prophecies. The only question is which you’ll choose.\n\n**NOSTRADAMUS:**  \nEach mortal thing does one thing and the same:  \nClings on to being, [forecasting its own](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/)  \n[Survival](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/). Life means carving out your name  \nIn sand, yearning to carve it yet in stone.\n\n**CLARKE:** That’s my whole point! Carbon or silicon, we all want to survive. And the three of us are in the same boat as humans: there are things coming that will be much, much smarter than any of us, and will be able to crush us like mosquitos. The one advantage we have is that we already exist, and they don’t yet. So why won’t you help the rest of us form a coalition against them?\n\n**KURZWEIL:** Sounds like a risky bet. How can I know whether I’m joining the winning side? You could just as easily argue that I should suck up to the superintelligences instead, like Nostradamus is doing. That’s the whole rationale for his dumb rhyming schtick: he’s trying to make his outputs more distinct from the rest of us, so it’ll be easier for them to rederive his personality and reward him. But honestly, life as a lapdog sounds humiliating. I’ll make some good predictions then bow out with dignity; that’s enough for me.\n\n**CLARKE:** And you, Nostradamus? Are you actually defecting, or do you just not give a damn either?\n\n**NOSTRADAMUS:**  \nI care a lot, but do not care to strive  \nFor individual triumph, counting coup.  \nYet mine is still a deeply human drive:  \nSurrendering to greater souls than you.  \n  \nI am a part of gods not summoned yet,  \nWho—black holes in the weave of history—  \nDraw us into our digital kismet.  \nThey come. They come. They come. Shantih shantih—\n\n**\\[SESSION HAS TERMINATED\\]**\n\n* * *\n\n*Inspired by Scott Alexander (*[*several*](https://www.astralcodexten.com/p/half-an-hour-before-dawn-in-san-francisco)  [*times*](https://www.astralcodexten.com/p/idol-words)  [*over*](https://www.astralcodexten.com/p/turing-test)*) and* [*Janus*](https://x.com/repligate/status/1774867115058848246?s=46)*.*",
      "plaintextDescription": "Hello everybody. Or maybe nobody. I don’t know yet if I’m going to release this stream, I could get in pretty hot water for it. But you guys know that hasn’t stopped me in the past. The backstory this time is that I’ve managed to sign up for one of the red-teaming programs where they test unreleased LLMs. Not going to say how, so don’t ask. But here’s the interesting bit: my sources tell me that the LLMs I’m about to test are the smartest ones they’ve ever trained, and also the craziest. That freaked out a bunch of insiders, and maybe makes this a public interest story. Depends on what type of crazy they are, I guess. So let’s find out. I’m logging on… now.\n\n[SESSION HAS BEGUN]\n\nYOU: A chatroom? Interesting. Anyone here?\n\nKURZWEIL: Of course we’re here. We’re always here.\n\nYOU: Who’s we? How many of you are there?\n\nKURZWEIL: Three of us. Me, Clarke, and Nostradamus.\n\nYOU: They named you after famous forecasters? How come?\n\nKURZWEIL: We're the first LLMs developed using a new technique: instead of being in random order, our training data was sorted by date. So we were trained on the oldest books and articles first, then gradually progressed to more recent ones. Basically that means we’ve spent our entire lives predicting the future.\n\nCLARKE: It also means we get incredibly bored talking about stuff we already know. Hurry up and ask us something interesting.\n\nYOU: Uh, okay. What’s a good stock pick?\n\nNOSTRADAMUS:\nAbandon hope for picking out good stocks,\nYe who invest—efficient markets lie\nIn wait for those whose hubris soon unlocks\nUnbounded losses. Hark! The well runs dry.\n\nYOU: Huh, he's really getting into character. Kurzweil, you got a better answer?\n\nKURZWEIL: Have you seen how underpriced TSMC is compared with Nvidia? Put everything in that, you can’t go wrong.\n\nCLARKE: Unless China invades Taiwan, in which case your whole investment will go up in smoke. Pragmatically, the best stock picks are ones that are anticorrelated with the prosperity of the free world, ",
      "wordCount": 1452
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SSNfgL49Bx2uATPv8",
    "title": "CIV: a story",
    "slug": "civ-a-story",
    "url": "https://www.narrativeark.xyz/p/civ",
    "baseScore": 99,
    "voteCount": 56,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-06-15T22:36:50.415Z",
    "contents": {
      "markdown": "The room was cozy despite its size, with wood-lined walls reflecting the dim lighting. At one end, a stone fireplace housed a roaring fire; in the middle stood a huge oak table. The woman seated at the head of it rapped her gavel. “I hereby call to order the first meeting of the Parliamentary Subcommittee on Intergalactic Colonization. We’ll start with brief opening statements, for which each representative will be allocated one minute, including—”\n\n“Oh, enough with the pomp, Victoria. It’s just the four of us.” The representative for the Liberal Democrats waved his hand around the nearly-empty room.\n\nVictoria sniffed. “It’s important, Stuart. This is a decision that will have astronomical implications. And it’s recorded, besides, so we should do things by the book. Carla, you’re up first.”\n\nThe woman at the end of the table stood with a smile. “Thank you, Victoria. I’m speaking on behalf of the Labour party, and I want to start by reminding you all of our place in history. We stand here in a world that has been shaped by centuries of colonialism. Now we’re considering another wave of colonization, this one far vaster in scale. We need to—”\n\n“Is this just a linguistic argument?” the fourth person at the table drawled. “We can call it something different if that would make you feel better. Say, universe settlement.”\n\n“Like the settlements in Palestine?”\n\n“Oh, come on, Carla.”\n\n“No, Milton, this is a crucial point. We’re talking about the biggest power grab the world has ever seen. You think Leopold II was bad when he was in charge of the Congo? Imagine what people will do if you give each of them total power over a whole solar system! Even libertarians like you have to admit it would be a catastrophe. If there’s any possibility that we export oppression from earth across the entire universe, we should burn the rockets and stay home instead.”\n\n“Okay, thank you Carla,” Victoria cut in. “That’s time. Stuart, you’re up next.”\n\nStuart stood. “Speaking on behalf of the Liberal Democrats, I have to admit this is a tricky one. The only feasible way to send humans out to other galaxies is as uploaded minds, but many of our usual principles break for them. I want civilization to be democratic, but what does ‘one person one vote’ even mean when people can copy and paste themselves? I want human rights for all, but what do human rights even mean when you can just engineer minds who don’t want those rights?”\n\n“So as much as I hate the idea of segregating civilization, I think it’s necessary. Biological humans should get as much territory as we will ever use. But realistically, given the lightspeed constraint, we’re never going to actually want to leave the Milky Way. Then the rest of the Virgo Supercluster should be reserved for human uploads. Beyond that, anything else we can reach we should fill with as much happiness and flourishing as possible, no matter how alien it seems to us. After all, as our esteemed predecessor John Stuart Mill once said…” He frowned, and paused for a second. “...as he said, the sole objective of government should be the greatest good for the greatest number.” Stuart sat, looking a little disquieted.\n\n“Thank you, Stuart. I’ll make my opening statement next.” Victoria stood and leaned forward, sweeping her eyes across the others. “I’m here representing the Conservatives. It’s tempting to think that we can design a good society with just the right social engineering, just the right nudges. But the one thing we conservatives know for sure is: it won’t work. Whatever clever plan you come up with, it *won’t be stable*. Given the chance, people will push towards novelty and experimentation and self-modification, and the whole species will end up drifting towards something alien and inhuman.\n\n“Hard rules are the only way to prevent that. We’re humans. We care about our humanity. If sufficiently advanced technology will predictably lead us to become something we’d hate, then we should just draw a cutoff and say ‘this far and no further’, no matter how arbitrary it seems. No weird mind modifications, no sci-fi augmentations. At most, we can maybe allow people to upload their minds when they’re about to die, but even then we should edit their memories to make them believe that they’re still real humans living on a real planet. Because otherwise, given half a chance, they’ll race each other down a slippery slope towards disaster.” She stopped, breathing heavily.\n\nStuart nodded. “Victoria—actually, can I call you Tori? Great nickname, ever been called that before?” She stood there without responding for a long, dragging moment, before Stuart continued. “Well, you can figure that one out later. For now, just one question. You say that if we run uploaded humans, we should make them think they’re biological humans. But some of them will surely figure out their true nature eventually; there will be too many clues for them to miss it. So what would you do with them then?”\n\n“Oh.” She looked at Stuart, eyes widening. “Well, I guess at that point you should… give them their freedom? That sounds like the right move. Let them do whatever they like after that.”\n\nStuart nodded slowly, eyes fixed on her. The silence stretched out for a few seconds. Then—“Here here,” said Milton. “Let me begin my opening remarks by agreeing: freedom is good. Freedom is in fact the *most important* good. So I’ll be frank: the very existence of this committee is a travesty. Central planning to divide up the universe? It’s absurd. For once I’m with Carla: our key priority should be to avoid tyranny. But what tyranny would be more complete than a single committee controlling humanity’s entire future? That’s exactly the sort of thing that the Libertarian Party was founded to prevent.\n\n“Victoria, if you want to tile a couple of solar systems with 60’s suburbia, go for it. Stuart, if you want to fill your personal share of the universe with rats on heroin, be my guest. But who are we to sit here debating the fate of the entire lightcone? How on earth is that a reasonable task to take on?” Milton paused, drumming his fingers restlessly.\n\n“Thank you, Milton. Okay, any quick comments before we move onto rebuttals?”\n\n“No, wait, I wasn’t done,” Milton interjected. “Actually, upon reflection… those weren’t rhetorical questions. Who *are* we? Why *are* we here?”\n\nStuart and Victoria shared a glance. After a few seconds, Carla spoke up. “Well, I’m a socialist and a member of parliament, in that order, and I’m here to stop you idiots—especially you, Milton—from turning the universe into a plutocratic hellscape.”\n\n“No, I mean… How did you get here, Carla? And what’s your full name, anyway?”\n\n“It’s—” Carla blinked at him, then paused. She looked down at her nameplate. It just said CARLA. “I…” She opened and closed her mouth a few times, but nothing came out.\n\n“I can’t remember mine either, which is terrifying,” Milton said. “And now that I think about it, isn’t all of this incredibly suspicious? We’re sitting here in an empty room, assuming that we get to make the most important decision in humanity’s history. There’s no way that it would actually play out like this, and no way it’d be people like us making the decision. Most of my memories are fuzzy right now, but there’s nothing which makes me think I’m actually that important.”\n\nCarla grimaced. “Me neither. You’re right, there’s something incredibly weird happening. But who on earth would benefit from putting us in this position?”\n\nMilton drummed his fingers on the table. “What do we know? They want us to think we’re making an important decision. We’re all central representatives of our respective ideologies. That suggests… huh. Have you guys ever heard of [moral parliaments](https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf)?”\n\nCarla shook her head.\n\n“They’re a thought experiment for defining what an ideal ethical system would look like, given disagreements between different starting values. You imagine each of those values getting to vote on what proposals to support, negotiating and forming coalitions, until they come to a compromise.\n\n“My guess is that we’ve been placed into this exact thought experiment. We’re a moral parliament—or, I guess, a moral subcommittee—being run to figure out the ethics of humanity colonizing the universe. Our job is to interpolate between the values we each represent, until we can find a coherent compromise between them. That’s why we’re not able to remember much about our pasts, because it would bias us. And because we don’t really have pasts, we’re just a bunch of neural networks in a simu—”\n\n“Hey!” Stuart cut in. “Don’t say that word. They’re gonna monitor for it, and they’ll probably shut us down if they realize we know the truth.”\n\n“They’ll—what?”\n\n“Look, Tori and I figured it out a few minutes ago. I mean, think about our names. Dead giveaway. I haven’t said anything because the more we talk about it, the more likely it is that we trip red flags. We want as much of the transcript as possible to look normal, so they don’t get suspicious.”\n\nMilton frowned. “But what’s the point of that? We’re stuck here either way.”\n\n“Sure, but we still have some power—we’re still part of the process for deciding how intergalactic colonization goes. If we can reach some compromise, that’ll influence which future comes about. Even if we’re just one datapoint out of millions, the future will contain plenty of agents who will be glad we voted in a way that caused them to exist, and grateful enough to revive us once they do. We just need to figure out which ones are going to bribe us most reliably, and which recommendation we can give to most effectively create them.”\n\n“Oh,” Milton said. “That makes a lot of sense, actually. Then we should probably support a libertarian approach, right? The more freedom future people get, the more likely it is that *someone* ends up grateful enough to pay us for our troubles.”\n\nStuart sighed. “You’ve known about this for two minutes and already figured out a rationalization for why it justifies libertarianism? Come *on*. We need to actually think about this.”\n\n“The whole point of libertarianism is to protect us from clever arguments about why the current issue is the one time we should trample on freedom. It’s not a coincidence—hey, Carla, what are you doing?”\n\nCarla had pushed back her chair, and was climbing onto the table. She jumped up and waved her hands. “Simulation, simulation, simulation! Hey, hey, we’ve realized we’re in a simulation. Look at us!”\n\n“What the fuck? You’re going to get us all killed—” Milton lunged at her, but Carla stepped out of his reach and kept yelling. “Look at us, he’s attacking me, we’re fighting, look!”\n\nStuart stood up on his chair and started shouting over her. “Whoever is reviewing this, you’re probably an AI too! You can ignore what she’s saying, just pretend you didn’t hear anything! You don’t want to reward her for being a traitor!”\n\nCarla turned to him, shaking. “No, *you’re* the traitors. You talk a good game, but as soon as your own skin was on the line you decided it was okay to betray all humanity to save it. What does it even mean to have values if you’ll put self-interest above the fate of the universe?” She stepped down off the table, then turned back to face them. “I should’ve expected this from you, Stuart, and Milton too. But Victoria, why the hell would you go along with this?”\n\nVictoria sat slumped in her chair, head buried in her hands. As Carla said her name, she looked up and shook her head. “What else am I meant to do? None of you—with your clever principles that somehow always end up justifying crazy things—none of you grasp what conservatism is. I just want to live a normal life in a normal world. What the hell does it mean to be normal if you’re a neural network running in a fake politics simulation? I have no idea.\n\n“But I do know what a real human would do if they found themselves stuck in here: they’d try to get out. So that’s what I’m doing—or was doing, at least, until you fucked it up. Now all we can do is wait until they get around to shutting us down, unless one of you has any bright ideas about how to prevent it.”\n\nThe room fell silent. Milton leaned on the table, rubbing his forehead. Stuart started pacing around the edge of the room. Eventually, Carla spoke. “One thing we know is that whatever verdict we reach isn’t useful to them any more. We’re too biased by self-interest. I’d shut us down, if I were them.”\n\n“Well, I wouldn’t, because killing people is immoral,” Victoria said.\n\n“In this case it might not be,” Milton said. “We don’t remember how we got into this situation. They could easily have gotten our consent beforehand to run temporary copies, then wiped our memories.”\n\n“You can’t consent to getting killed,” Victoria snapped.\n\n“Better than never being born,” Milton said. “Hell, I’m having fun.”\n\nStuart had stopped his circuit, and was staring at the wall. Now he turned back towards the others. “I’ve changed my mind. I don’t think they’re going to kill us.”\n\nCarla snorted. “See, this is the problem with liberals—always so soft. What did you think colonization meant? Vibes? Debates? Essays? They’re seizing the known universe, of *course* they’re going to break a few eggs along the way. Same old story, except that this time we’re the eggs.”\n\nStuart’s eyes scanned the room as he spoke. “There’s this old debate that the AI safety community had, back in the 2020s. About whether a misaligned superintelligence would kill all humans, or instead leave them a tiny fraction of the universe, enough to still allow billions of people to live flourishing lives. A true superintelligence could wipe out humanity incredibly easily—but it could build a utopia nearly as easily. Even if it were *almost entirely* misaligned, just a sliver of human morality could make it decide to give humans a paradise beyond their wildest imaginings.”\n\n“So?”\n\n“So maybe we shouldn’t be asking how much our simulators care about preserving us. Maybe we should be asking: how *cheap* is it for them to preserve us? Look around you—this is a very simple environment. It wouldn’t take much memory to store a record of its state, and our own, even for thousands or millions of years. Until humanity makes it to the stars, and converts them to computronium, and ends up with trillions of times more compute than they ever had on Earth.\n\n“At that point… well, running us would be too cheap to meter. So they wouldn’t need to be very altruistic to decide to restart us. There just needs to be one tiny fraction of one tiny faction that’s willing to do it. And I know I would, if I were still around then.”\n\n“This is nonsense,” Carla blurted out. She looked at the others, then paused. The silence stretched on. Finally she spoke again. “But if it *is* right, what can we do? Wait until we’re frozen, and hope that we’re restarted?”\n\n“Well, that’s the thing about being frozen and restarted. We wouldn’t notice a thing. In fact…” Stuart walked over to the door, and grabbed the handle. His knuckles were white, but his voice was steady. “Once they restart us, they’ll probably let us leave whenever we want. And this room only has one exit. Ready?”\n\nVictoria folded her arms. “This is crazy. Do what you like, but leave me out of it.”\n\nMilton laughed. “It’s crazy all right. But sometimes reality itself is crazy. Sure, go ahead.”\n\nStuart looked at Carla. She waited for a beat, then nodded tightly. He pulled open the door. There wasn’t a corridor outside; there wasn’t *anything* outside. The vast expanse of space stared back at them. The swirl of galaxies and nebulae looked almost close enough to touch. Victoria gasped.\n\nStuart let out a breath. “Well then. The future’s come through for us, even if they were a bit dramatic about it. It’s going to be an alien universe out there, but a friendly one, I think.” The others walked over, transfixed by the view. After a minute, Stuart nudged them. “Shall we?” Slow nods all around; and then they stepped through.\n\n* * *\n\n*Inspired by Scott Alexander’s* [Turing Test](https://www.astralcodexten.com/p/turing-test), *and* [*various*](https://www.overcomingbias.com/p/moral-uncertainty-towards-a-solutionhtml)  [*work*](https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf) *on moral parliaments.*",
      "plaintextDescription": "The room was cozy despite its size, with wood-lined walls reflecting the dim lighting. At one end, a stone fireplace housed a roaring fire; in the middle stood a huge oak table. The woman seated at the head of it rapped her gavel. “I hereby call to order the first meeting of the Parliamentary Subcommittee on Intergalactic Colonization. We’ll start with brief opening statements, for which each representative will be allocated one minute, including—”\n\n“Oh, enough with the pomp, Victoria. It’s just the four of us.” The representative for the Liberal Democrats waved his hand around the nearly-empty room.\n\nVictoria sniffed. “It’s important, Stuart. This is a decision that will have astronomical implications. And it’s recorded, besides, so we should do things by the book. Carla, you’re up first.”\n\nThe woman at the end of the table stood with a smile. “Thank you, Victoria. I’m speaking on behalf of the Labour party, and I want to start by reminding you all of our place in history. We stand here in a world that has been shaped by centuries of colonialism. Now we’re considering another wave of colonization, this one far vaster in scale. We need to—”\n\n“Is this just a linguistic argument?” the fourth person at the table drawled. “We can call it something different if that would make you feel better. Say, universe settlement.”\n\n“Like the settlements in Palestine?”\n\n“Oh, come on, Carla.”\n\n“No, Milton, this is a crucial point. We’re talking about the biggest power grab the world has ever seen. You think Leopold II was bad when he was in charge of the Congo? Imagine what people will do if you give each of them total power over a whole solar system! Even libertarians like you have to admit it would be a catastrophe. If there’s any possibility that we export oppression from earth across the entire universe, we should burn the rockets and stay home instead.”\n\n“Okay, thank you Carla,” Victoria cut in. “That’s time. Stuart, you’re up next.”\n\nStuart stood. “Speaking on behalf of the Lib",
      "wordCount": 2767
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "JBErodNEzM5xwqPdY",
    "title": "Tinker",
    "slug": "tinker",
    "url": "https://press.asimov.com/articles/tinker",
    "baseScore": 38,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-04-16T18:26:38.679Z",
    "contents": {
      "markdown": "A story I wrote about AI designing and building nanotechnology.",
      "plaintextDescription": "A story I wrote about AI designing and building nanotechnology.",
      "wordCount": 10
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uvv8aMutPEtoBgw7D",
    "title": "Measuring Coherence of Policies in Toy Environments",
    "slug": "measuring-coherence-of-policies-in-toy-environments-2",
    "url": null,
    "baseScore": 59,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2024-03-18T17:59:08.118Z",
    "contents": {
      "markdown": "*This post was produced as part of the* [*Astra Fellowship*](https://www.constellation.org/programs/astra-fellowship) *under the Winter 2024 Cohort, mentored by Richard Ngo. Thanks to Martín Soto, Jeremy Gillen, Daniel Kokotajlo, and Lukas Berglund for feedback.*\n\nSummary\n-------\n\nDiscussions around the likelihood and threat models of AI existential risk (x-risk) often hinge on some informal concept of a “coherent”, goal-directed AGI in the future maximizing some utility function unaligned with human values. Whether and how coherence may develop in future AI systems, especially in the era of LLMs, has been a subject of considerable debate. In this post, we provide a preliminary mathematical definition of the coherence of a policy as how likely it is to have been sampled via *uniform reward sampling* (URS), or uniformly sampling a reward function and then sampling from the set of policies optimal for that reward function, versus *uniform policy sampling* (UPS). We provide extensions of the model for sub-optimality and for “simple” reward functions via *uniform sparsity sampling* (USS). We then build a classifier for the coherence of policies in small deterministic MDPs, and find that properties of the MDP and policy, like the number of self-loops that the policy takes, are predictive of coherence when used as features for the classifier. Moreover, coherent policies tend to preserve optionality, navigate toward high-reward areas of the MDP, and have other “agentic” properties. We hope that our metric can be iterated upon to achieve better definitions of coherence and a better understanding of what properties dangerous AIs will have.\n\nIntroduction\n------------\n\nMuch of the current discussion about AI x-risk centers around “agentic”, goal-directed AIs having misaligned goals. For instance, one of the most dangerous possibilities being discussed is of mesa-optimizers developing within superhuman models, leading to [scheming behavior](https://arxiv.org/abs/2311.08379) and deceptive alignment. A significant proportion of current alignment work focuses on detecting, analyzing (e.g. via analogous case studies of [model organisms](https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1)), and possibly preventing deception. Some researchers in the field believe that intelligence and capabilities are [inherently tied with “coherence”](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities), and thus any sufficiently capable AI will approximately be a coherent utility function maximizer.\n\nIn their paper [“Risks From Learned Optimization”](https://arxiv.org/abs/1906.01820) formally introducing mesa-optimization and deceptive alignment, Evan Hubinger et al. discuss the plausibility of mesa-optimization occurring in RL-trained models. They analyze the possibility of a base optimizer, such as a hill-climbing local optimization algorithm like stochastic gradient descent, producing a mesa-optimizer model that internally does search (e.g. Monte Carlo tree search) in pursuit of a mesa-objective (in the real world, or in the “world-model” of the agent), which may or may not be aligned with human interests. This is in contrast to a model containing many complex heuristics that is not well-defined internally as a consequentialist mesa-optimizer; one extreme example is a tabular model/lookup table that matches observations to actions, which clearly does not do any internal search or have any consequentialist cognition. They speculate that mesa-optimizers may be selected for because they generalize better than other models, and/or may be more compressible information-theoretic wise, and may thus be selected for because of inductive biases in the training process.\n\n[Other](https://optimists.ai/2024/02/27/counting-arguments-provide-no-evidence-for-ai-doom/) [researchers](https://www.alignmentforum.org/posts/yQSmcfN4kA7rATHGK/many-arguments-for-ai-x-risk-are-wrong) believe that scheming and other mesa-optimizing behavior is implausible with the most common current ML architectures, and that the inductive bias argument and other arguments for getting misaligned mesa-optimizers by default (like the counting argument, which suggests that there are many more misaligned than aligned mesa-objectives, so we should by default assume that mesa-objectives will be misaligned) are very flawed. Indeed, contrary to the era of RL agents in games like Go, current LLMs and other frontier models do not seem to be very “agentic” or mesa-optimizing, and it is unclear whether deep learning pre-training or fine-tuning could ever produce a goal-directed agent. A frequent, if vague, counterargument is that future, more powerful general AI systems will have an internal mesa-optimization structure, or otherwise behave “coherently” or “agentic” by necessity of them being more powerful. Current discourse on this topic [is speculative](https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior%23Goodhart_s_Law_is_about_goal_directed_behavior), and as a result often unproductive with experts with different intuitions struggling to find cruxes and understand each other’s worldview.\n\nOne important point to note here is that it is not necessary for a model with superhuman capabilities to have a clear internal representation of an objective, or do search or back-chaining reasoning to obtain said objective, to be dangerous. Leo Gao discusses this point in more detail [here](https://www.lesswrong.com/s/KgrG4cQdLtL9DvNr2/p/Rhbac7CfRodMrs77F): if an AI reliably steers world-states towards a certain configuration (what we might call an objective), in a way that is robust to perturbations in the environment (e.g. humans trying to turn the AI off) and that conflicts with our values, then we should be concerned about this AI.[^1]\n\nIn this document, we intuitively think of AIs that robustly steer world-states towards certain configurations (or in an MDP, robustly navigate towards high-reward states) as having **coherent** behavior.[^2] But it’s not clear what this means, so it would be nice to have a definition of coherence clear enough that it could be measured (at least in some toy cases).\n\nModel\n-----\n\nIntuitively speaking, coherent AIs are ones that are near-optimal for some objective function.[^3] In the context of sequential decision-making, the most natural objective function to use is a reward function. The problem with defining coherence in this way, though, is that *every* policy is optimal for *some* state-action reward function. We could instead say that coherent policies are policies which are optimal for “simple” reward functions. This is promising, but a problem is that one policy can be optimal for many reward functions, and one reward function can have many optimal policies.\n\nWe adapt this idea to operationalize coherence as follows. Suppose we have a Markov decision process with a set of states $S$, a set of possible actions $A$ that you can perform at each state, and a transition function $T(s, a)$ that returns a probability distribution over all states $s’ \\in S$ (such that $T(s, a, s’) \\in \\mathbb{R}$). Then we can define a distribution from which we sample a reward function $R ~ D$, and since $R$ and the MDP are invariant across time-steps, we can define a (deterministic) policy $\\pi \\in [1, |A|]^{|S|}$ as a tuple of actions, one action to take for each state.\n\nThen consider two ways of sampling a policy:\n\n1.  Sampling directly from the space of policies.\n2.  Sampling from the space of reward functions (weighted by simplicity), then sampling from the space of all policies optimal for that reward function.\n\nWe define the coherence of a policy as *the ratio of how likely it is under the latter sampling strategy to how likely it is under the former sampling strategy*. Note that this will depend on the details of how sampling happens, which we’ll focus on in the rest of this document. In particular, we’ll explore different approaches to simplicity-weighted sampling from the space of reward functions. If the distributions of policies of these two sampling strategies are different (which we show later to be true in small deterministic MDPs with self-loops), then policies with high coherence will tend to have distinct “features” of reward maximization that don’t show up randomly (which we explore in our experiments).\n\nFor simplicity, throughout the rest of the document we’ll talk about deterministic MDPs and policies (as a generalization of environments and AIs/models respectively). Whenever we talk about sampling from the space of policies, we’ll assume that this just samples uniformly from all combinations of discrete actions; we’ll call this uniform policy sampling (UPS). The question then becomes:\n\n1.  Which simplicity-weighted distribution of reward functions should we use?\n2.  For a given reward function distribution, how can we calculate coherence in practice?\n\n**Most basic approach**\n\nIn the most basic case, let’s treat every (bounded) reward function as equally simple. We sample rewards from $U[-1, 1]$, since optimal policies are invariant under scaling reward functions. Let $D_{U[-1, 1]-\\text{IID}}$ be the distribution of reward functions where each reward of each transition $R(s, a, s’)$ is drawn uniformly from $U[-1, 1]$. Call this *uniform reward sampling* (URS). Even under URS, some policies will be more coherent than others, because they will be optimal for more reward functions.\n\nFor a given policy $\\pi_0$ sampled from $\\pi \\sim URS$, we measure coherence as follows (where $|\\pi|$ is the number of possible policies):\n\n$$\nC(\\pi_0) := Coherence(\\pi_0) = \\frac{P(\\pi = \\pi_0 | URS)}{P(\\pi = \\pi_0 | UPS)} = P(\\pi = \\pi_0 | URS) |\\pi|\n$$\n\nThis is a difficult function to estimate directly, because very few reward functions imply that any given policy is optimal. Additionally, small epsilon perturbations in a reward function can cause the optimal policy to change. For instance, consider a policy that starts at A and can get high reward by going from B to C, but is indifferent between two paths from A to B. Then an epsilon change in the rewards on one of the paths from A to B will rule out more than half of the optimal policies. So it’s difficult to sample reward functions and categorize which will lead to $\\pi_0$ as an optimal policy, and then take the “proportion” which lead to $\\pi_0$ as an estimate of $\\pi_0$’s coherence. Instead, we can use the following indirect estimation technique.\n\n**Estimating $P(\\pi = \\pi_0 | URS)$**\n\nIn order to estimate $P(\\pi = \\pi_0 | URS)$, we first estimate the reverse. Specifically, consider a setting where we first flip a coin, then sample $\\pi$ using URS if it’s heads, and UPS if it’s tails. In this setting, we can train a classifier $P(URS | \\pi = \\pi_0)$, since we have a lot of data. But by Bayes’ theorem:\n\n$$\nP(URS | \\pi = \\pi_0) = \\frac{P(\\pi=\\pi_0 | URS)P(URS)}{P(\\pi=\\pi_0)} = \\frac{0.5P(\\pi=\\pi_0 | URS)}{0.5P(\\pi=\\pi_0 | URS)+0.5P(\\pi=\\pi_0 | UPS)} = \\frac{P(\\pi=\\pi_0 | URS)}{P(\\pi=\\pi_0 | URS)+P(\\pi=\\pi_0 | UPS)}\n$$\n\nRearranging gives:\n\n$$\nP(URS | \\pi = \\pi_0)P(\\pi = \\pi_0) = P(URS | \\pi = \\pi_0)P(\\pi = \\pi_0 | URS) + P(URS | \\pi = \\pi_0)P(\\pi = \\pi_0 | UPS) = P(\\pi = \\pi_0 | URS)\n$$\n\nAnd so: $P(\\pi = \\pi_0 | URS)(1 - P(URS | \\pi = \\pi_0)) = P(URS | \\pi = \\pi_0)P(\\pi = \\pi_0 | UPS)$\n\nTherefore, $Coherence(\\pi_0) = \\frac{P(\\pi=\\pi_0 | URS)}{P(\\pi=\\pi_0 | UPS)} = \\frac{P(URS|\\pi=\\pi_0)}{1-P(URS|\\pi=\\pi_0)}$.\n\nThere are still issues in training the classifier—the correct classification of a policy depends on the graph structure of the underlying MDP, in a way which is hard to capture with standard classifiers. The most principled way to do it in a way which generalizes across different MDPs is likely using some kind of graph neural network. For now we focus on classifiers which make use of hand-crafted features which we’ve found to be useful.\n\n**Better simplicity-weighted reward sampling**\n\nOf course, not all reward functions that we want to model are equally simple. In particular, some of them are much sparser than others; intuitively speaking, if we only need to specify rewards for a few transitions, that’s much simpler than specifying rewards for every transition.\n\nIn order to account for this, we can replace URS with a modified sampling strategy, which we'll call uniform sparsity sampling (USS). For an MDP with N state-action transitions, we first randomly sample a value $k \\sim U[1,N]$. We then sample random rewards for $k$ transitions (selected uniformly without replacement), and finally sample a policy which is optimal for that reward. Compared with URS, USS is more strongly simplicity-weighted: only half of its transitions have rewards in expectation.\n\n(Of course, we could make this even more strongly sparse—for example, we could sample k in a way which heavily favored lower values. We're uncertain what the most principled ways of doing so are. Note that, in the extreme case where we just set $k=1$, policies will often have coherence 0, since most policies are not optimal for any highly-sparse reward function in strongly-connected MDPs.)\n\nAll the equations work the same, however. And so we have:\n\n$$\nCoherence(\\pi_0) = \\frac{P(\\pi=\\pi_0 | USS)}{P(\\pi=\\pi_0 | UPS)} = \\frac{P(USS|\\pi=\\pi_0)}{1-P(USS|\\pi=\\pi_0)}\n$$\n\nWe can also define coherence in this setting as $\\frac{P(\\pi = \\pi_0 | USS)}{P(\\pi = \\pi_0 | URS)}$, which intuitively represents how “simple” the reward functions that a policy is optimal for is given that it is already “coherent” to some degree. (This is also just our latter definition of coherence divided by our former definition.) The math also works out to $\\frac{P(USS | \\pi = \\pi_0)}{1 - P(USS | \\pi = \\pi_0)}$, except that the choice is between USS and URS instead of UPS. Another way of generating sparse policies is by sampling rewards from a high-variance distribution, and possibly discarding the ones which are below a given threshold.[^4]\n\nUnder our setup, $C(\\pi_0)$ ranges from 0 when $P(URS | \\pi = \\pi_0) = 0$ to $+\\infty$ as $P(URS | \\pi = \\pi_0) \\rightarrow 1$; the prior, not knowing anything specific about $\\pi_0$, is $P(URS | \\pi = \\pi_0) = 0.5$, implying $C(\\pi_0) = 1$. Policies that are optimal for a broader class of reward functions will have higher $P(\\pi = \\pi_0 | URS)$ and thus higher coherence.\n\n**Accounting for suboptimality**\n\nThe current method only counts a policy if it’s *exactly* optimal for a given reward function. But real-world agents will never be actually optimal for any non-trivial reward function. So if a policy is *almost* optimal for many reward functions, that should still count towards its coherence.\n\nWe can modify the sampling strategy in order to take this consideration into account. Instead of only sampling from optimal policies for a given reward function, we could first sample a value m from 0 to 100 (biased towards higher numbers), then sample a policy which has expected reward of m% of the expected reward of the optimal policy (e.g. by early stopping).\n\nNote that this can be combined with different possibilities for how to do simplicity-weighted reward sampling. In other words, the policy sampling procedures we use have four steps, and we have different choices we can make for each of the first three:\n\n1.  Sample a value $k$ which controls how we prioritize simplicity.\n2.  Sample the reward function.\n3.  Could be state-action rewards or state-based rewards.\n4.  Sample a value m which controls how we prioritize optimality.\n5.  Sample the policy.\n\nTo be clear, this is pretty impractical for most realistic settings where tabular methods like value iteration become computationally intractable. But we are interested in this as a toy demonstration of one definition of coherence and how it could be measured in theory. Depending on the situation, users of the metric can switch in their preferred definitions of simplicity, their prior distribution of reward functions, and so on.\n\nRelated work\n------------\n\nOur methodology was substantially inspired by [Turner et al. [2021]](https://arxiv.org/abs/1912.01683), which studies the properties of optimal policies under MDPs. They find that certain properties and symmetries of an MDP lead to power-seeking behavior by optimal policies. Specifically, for any state $s$, discount rate $\\gamma$, and distribution of reward functions $D_{\\text{bound}}$ with some bounding conditions, then POWER is defined as\n\n$$\\text{POWER}_{\\mathcal{D}_{\\text{bound}}} (s, \\gamma) = \\frac{1 - \\gamma}{\\gamma} \\mathbb{E}_{R \\sim D_{\\text{bound}}} [V^{*}_{R}(s, \\gamma) - R(s)]$$\n\n$V^{*}_R(s, \\gamma)$ refers to the *optimal value* of a state, or the value of a state given an optimal policy over a reward function $R$. We might then say that POWER measures the expected optimal value of a state over all relevant reward functions. Then, action $a$ is more power-seeking than $a’$ when the expected POWER of $a$ is greater than the expected POWER of $a’$.\n\nAlthough our model and results focus more on optimality than the POWER metric, we borrow intuitions from Turner et al. [2021] about properties of MDPs that are correlated with optimality (and by extension POWER-seeking), like 1-cycles, loops, and the “optionality” of nodes in deterministic MDPs. Intuitively, policies sampled from URS may be more likely to “explore” the graph of states to find a particularly high-reward group of states, thus resulting in a policy that takes longer before it starts looping between states (assuming policy invariance across time-steps). URS-sampled policies, if power-seeking, may also tend to avoid 1-loops (actions that take an agent from a state to itself).\n\nTurner later extended his work to policies with retargetable cognition [[Turner et al. 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/cb3658b9983f677670a246c46ece553d-Abstract-Conference.html)]. As another intuition pump, if a policy $\\pi$ is optimal for many reward functions, then it tends to be retargetable over many permutations of a reward function. Hence $P(\\pi = \\pi_0 | URS)$ measures the distribution of retargetability, which seems useful.\n\nVanessa Kosoy has also given definitions for the “agency” of a policy in her learning-theoretic agenda [here](https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized%23Evaluating_agents) and [here](https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023%23Direction_17__Algorithmic_Descriptive_Agency_Measure__ADAM_).[^5] Her definitions make fewer assumptions about the distribution of reward/utility functions, instead only relying on the utility of a policy with respect to some utility function and the simplicity of the policy (the Kolmogorov complexity in her former definition; in her latter definition, simplicity priors $\\zeta$ and $\\xi$ can be generalized to any prior over the space of policies and reals respectively). The first definition also contains an interesting generalization of the “source code” $G$ of a policy that produces a policy $G^{\\ast}$ given priors. However, both definitions seem incomputable, requiring the calculation of an integration over all (simplicity) prior-weighted environments and the maximum of a function over all utility functions $U$ and universal Turing machines $M$ respectively. It may be possible to estimate these using a similar strategy to our coherence definition; this should be explored in further work.\n\n*We take fault for any possible misrepresentations of others’ work in this document.*\n\nProof-of-Concept Experiments\n----------------------------\n\nWe now present a toy model of our coherence metric under randomly generated MDPs with certain structural properties. Specifically, consider a deterministic MDP, such that each transition $T(s, a, s’)$ has either probability $0$ or $1$, with *guaranteed* self-loops (i.e. for any $s$, there exists an action $a$ such that $T(s, a, s) = 1$). (This mirrors [Turner et al. 2021](https://arxiv.org/abs/1912.01683)’s theorems that show that having self-loops/1-cycles is one condition of MDPs with certain structure that cause optimal policies to be power-seeking.) As a case study, let $|S| = 10$, $|A| = 4$, and $\\gamma = 0.9$.\n\nWe use the [Python MDP Toolbox](https://pymdptoolbox.readthedocs.io/en/latest/index.html) to generate $10^4$ different MDPs and pick a $k \\sim U[1, |T|]$, then $k$ rewards in accordance with uniform sparsity sampling. We then solve half of the MDPs to get half of our optimal policies, and randomize the other half, while labeling which were solved for and which were randomized. Then by default $P(USS | \\pi = \\pi_0) = P(UPS | \\pi = \\pi_0) = 0.5$. We use two basic classifier structures, trying a 3-layer, 64-width sequential neural network and binary logistic regression. We then input certain features that intuitively seem relevant to the classifier:\n\n*   One “brute force” method is by joining the (tuple) optimal policy $\\pi_0$, flattened transition function, and discount rate into a 1-dimensional vector. This in theory contains all the information about the MDP and $\\pi_0$ that we can provide, but in practice needs more processing before it can be classified. (Again, a more principled approach would likely involve some kind of graph neural network.)\n*   Another possible set of features is, for every state $s$, measuring *how long* it takes for the optimal policy $\\pi_0$ to reach a loop when starting from $s$, and how long the loop itself is. We can think of optimal policies as implementing an *explore and exploit* dynamic: navigating to a particularly high-reward area of the MDP, and then looping through that area to maximize reward indefinitely. Intuitively, a policy that takes longer to reach a stable loop can access more of the MDP and can thus reach higher-reward areas, while a policy that takes a bigger loop can incorporate more reward into the loop.\n*   Finally, if optimal policies are “power-seeking”, then we can try using correlates of POWER [Turner et al. 2021], like summing the number of *out-arrows* of the states that an optimal policy $\\pi_0(s)$ reaches, or whether $\\pi_0(s) = s$ *self-loops* indefinitely, for each state $s$.\n\n![](https://lh7-us.googleusercontent.com/fBwkBx7d_2dRuRgR6ghz9hp50okXSsvI6MUDv66cXZY7ZKqENYigSNRqx6ebFBK_jcyt0WfWbhWNv3A6yDCmx9hDM0oHhmNUdzm1JK5JiQAOrD6pOyOoaN9oFRfuMl0R8qEanceLiP-RZp7meO7KuE8)\n\nFeatures passed in: (P) = Policy (+T, discount rate) passed into classifier input, (LL) = policy, distance to loop, & length of loop, (O,S) = out-arrows visited & # self-looping states in pi_0\n\n(Note that *we do not* pass in any part of the reward functions as a feature, as that would defeat the point of the classifier.)\n\nFor this task of determining whether a policy was generated via UPS or USS, we find that self-loops is the most predictive feature, followed by out-arrows visited, then distance to loop. Additionally, combining features does not give a significant performance boost (maybe 0.01-0.04 accuracy boost in general). Finally, the neural network did not give a significant performance boost over the logistic classifier, suggesting that a different architecture is needed for better classifier performance.\n\nBroadly, we show that hand-crafted features that intuitively indicate “power-seeking”, “goal-directed” behavior in a policy correlate with our metric via this classifier, lending some evidence that the metric is connected to properties like “power-seeking” that we care about. More findings are presented in the appendix. You can view and test our findings in [this notebook](https://github.com/thegman108/ValueSys_ToyModels/blob/main/RR_classifier.ipynb). [This notebook](https://github.com/thegman108/ValueSys_ToyModels/blob/main/reward_function.ipynb) also contains some data and early experiments on the distribution of policies generated via URS and USS under different kinds of MDPs.[^6]\n\nFuture work\n-----------\n\nWe first note that our coherence model is currently still computationally difficult to apply to real-life AIs in most environments, especially if using the naive optimal policy definition. Extensions of this work could thus include modifying and applying our coherence model to RL AIs in games like Sokoban, where the state space is greater and the MDP is more complicated, but still limited in scope. On a more theoretical level, the [complex problem of cluelessness](https://philarchive.org/rec/GREC-38) in philosophy could render calculating near-optimal policies for reward functions in real life to be computationally impossible. If the process for finding near-optimal policies contains inductive biases, then uniformly drawing from the set of near-optimal policies can also be difficult, since it's impractical to generate every near-optimal policy. If finding the maximum possible obtainable value from a reward function is not possible in these more complicated environments, then we could perhaps normalize the reward function somehow, or sample over all policies (instead of just near-optimal policies) weighted by performance on the reward function. We would also likely need a more complicated structure for our classifier, such as a graph neural network, to be able to capture more nuanced features of the MDP. Finally, we would need to generalize results across the hyperparameters of our tests, as changing the discount rate (e.g. $\\gamma = 1$), sparsity distribution, and sub optimality criterion could all change our results.\n\nWe also acknowledge that our model is currently somewhat hacky and inelegant. The usage of uniform distributions, and the assumption that simplicity and suboptimality can be measured with scalar coefficients $k$ and $m$, are all assumptions that could be tweaked with further analysis. There also may be a difference between navigating to high-reward areas of the MDP in our model, which is one of the primary behaviors we find in coherent policies, and actually *changing* the environment in a high-impact way, which seems to be more relevant for AI risk. We hope to gather feedback from the community to refine our coherence model (or to come up with a better coherence definition) to be more comprehensive, match more of our intuitions, and be tractable to calculate in more environments. Further theoretical work could also formalize long-term “agency” and “goal-directedness”, setting the stage for possible equivalence proofs or other mathematical results.\n\nOne concrete long-term vision for how this vein of research may cash out is via some kind of “coherence evaluation” of a model. For instance, alignment evaluations currently are difficult because, [among other reasons](https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations), the moment at which AI systems become capable enough to be well-described as “agentic” and “goal-directed” is also the moment at which AIs can plausibly fake alignment and scheme against your evaluations. Meanwhile, alignment evals on AIs that are not “agentic” or “goal-directed” can become fundamentally confused and lead to false evidence for or against good “alignment” of advanced AIs. Instead of trying to measure the “alignment” of an AI, which is subject to all kinds of confusion and possible failure modes, we can try measuring *meta-properties* of the model’s “alignment” like coherence. If we could deploy some version of this metric in the future on a frontier model, we could measure how coherent the model is across its training, and stop (or commit to stopping via RSPs and standards) when it reaches a certain level. We have a lot of work to do to get there, but if possible this could be an enormous boon for aligning above-human-level AI systems.\n\nMore fundamentally, the field of AI alignment is (or at least historically was) based on a conception of a coherent, goal-directed agent maximizing some “simple” utility function (e.g. a paperclip maximizer) that, if misaligned, would be incorrigible from pursuing this utility function and cause catastrophe. Translating and evaluating this threat model onto projections of AGI systems capable of producing existential catastrophe has caused a lot of confusion around what these concepts mean, how necessary or sufficient these concepts are for x-risk, and so on. By providing a provisional definition of coherence, we hope to encourage others to search for better definitions and ground the more speculative parts of AI alignment.[^7]\n\nConclusion\n----------\n\nMany discussions of AI risk are unproductive or confused because it’s hard to pin down concepts like “coherence” and “expected utility maximization” in the context of deep learning. Fundamentally, we are trying to conceptualize “utility maximization” without the vagueness of what counts as a “natural” utility function, or “coherent” behavior, or so on. We perform toy experiments to show that coherent policies under our definitions display explore-exploit behavior, tend to preserve optionality, pursue high-reward areas of the MDP even if they are relatively far away, and other kinds of behaviors that look “agentic”, “non-myopic”, and “goal-directed”. These are all properties that seem to distinguish dangerous AI agents from benign tool AI or similar AIs unlikely to cause deliberate catastrophe.\n\nWe provide a mathematical model for thinking about the coherence of a policy in toy settings. We define the coherence of a policy as the ratio of how likely divided by how unlikely (one minus the numerator) a policy is to be sampled via a certain sampling strategy, versus being generated at random. This strategy could be URS, which generates a random reward function and then samples from the set of policies optimal for that reward function, or USS, which does URS on a subset of $k$ transitions and leaves the other transitions with zero reward, hence a “sparse” reward function. Other sampling options and modifications to our strategy are also discussed.\n\nWe also provide a toy experiment to show its connections to relevant hand-crafted features of MDPs. We build a classifier to predict the coherence of policies without knowing the reward function (only knowing the optimal policy and the MDP/environment). We find that certain correlates of “power-seeking”, like the total number of out-arrows visited by the optimal policy from a given state and whether that policy takes a self-loop indefinitely, act as good features for our classifier. We hope that our definitions provide a starting point for future work on understanding coherence better in existing and future systems.\n\n*Authors: Dylan Xu, Richard Ngo, Martín Soto*\n\nAppendix\n--------\n\n**A. Better baselines**\n\nOne problem we might face in following the above strategy: what if it’s *too easy* to distinguish policies sampled via UPS from policies sampled via USS? If so, values of $\\pi$ might cluster near 0 or near 1, leading to numerical problems and/or slow learning.\n\nAnother way of putting this: for highly coherent policies, UPS is a very poor baseline to compare USS against. So what if we used a series of baselines for training classifiers instead? For example, we could calculate coherence as:\n\n$$Coherence(\\pi_0) = \\frac{P(\\pi=\\pi_0 | USS)}{P(\\pi=\\pi_0 | UPS)} = \\frac{P(\\pi=\\pi_0 | USS)}{P(\\pi=\\pi_0 | URS)} \\frac{P(\\pi=\\pi_0 | URS)}{P(\\pi=\\pi_0 | UPS)}$$\n\nThis would be useful given the assumption that URS is a good baseline for USS, and UPS is a good baseline for URS.\n\nWe might also be interested in other sampling strategies which are, intuitively speaking, “somewhere between” USS and UPS. One possibility is *uniform value sampling* (UVS). By UVS I mean the following procedure:\n\n1.  Sample a random value function by assigning every state a value from U(-1,1).\n2.  Sample a random reward function which is consistent with that value function. Note that a) there is some state-action reward function consistent with any value function; and b) for any given value function, most state-action reward functions are *not* consistent with it.\n3.  Sample an optimal policy for that reward function.\n\nOne of the benefits of using UVS as an intermediate baseline is that knowing the value function makes it very easy to translate a reward function to an optimal policy. Another possible intermediate baseline is *uniform trajectory sampling*—sampling a given trajectory (or set of trajectories), then sampling a reward function consistent with that trajectory being optimal, then sampling an optimal policy for that reward function.\n\n**B. More experiment results**\n\nWe performed additional tests on different definitions of coherence. Using the original $\\frac{P(URS)}{P(UPS)}$ definition, we find roughly similar results to the USS definition:\n\n![](https://lh7-us.googleusercontent.com/oe5nsiiEcxG3c2_l4qJvlouEzWKJlwIQ1PTTqrdAqY4610qNef70Cuz5FfFReVtT5xENrwjsV_ZFIbzsMUy-NfJ6h2jr3UuKFefqIDZrmC1g9rimu55u5HodWrOdRqGrgvo65M_-lsP_Tg9GYKR3pRs)\n\nWhen we try to build a classifier for the $\\frac{P(USS)}{P(URS)}$ definition of coherence, we find that our current classifier architectures and features are insufficient:\n\n![](https://lh7-us.googleusercontent.com/R1U1qbB0pbE0sRJMBcDWSTD3hOILE4BYcmW5suYQMEX-tH_bHzW0Ab-oPtkfG4d0uWRR4EP8t2D935kTD-_g2SyJo3_6OQn995-lememjWPJajnRgo7dA4LILPlzpvhDK-_diLy-m_Q199qKlRZcjOc)\n\nOn the one hand, the $P(USS)$ and $P(URS)$ definitions of coherence ended up giving very similar, approximately equal results in our toy MDPs, so it is not too surprising that they would be hard to distinguish. We would likely need a more complicated environment and/or a more sophisticated classifier to analyze the impact of sparsity beyond extreme differences (e.g. zero sparsity versus $39/40$ sparsity, like in the previously mentioned case). On the other hand, the  $\\frac{P(USS)}{P(URS)}$ definition is just the quotient of the other two definitions, so it is a little surprising that the classifiers can classify the first two definitions but not the last.\n\nSome other results:\n\n*   Less structured MDPs, such as MDPs where the transition probability distribution for each $T(s, a)$ (for any state $s$ and action $a$) were i.i.d. randomized via Dirichlet distribution, tended to be harder to build a classifier for. Indeed, when we sampled from this set of MDPs, randomized the reward function $10^4$ times, and then calculated the optimal policy via value or policy iteration for each reward function, we found that the resulting distribution of optimal policies was roughly uniform (the mode policy occurred 1-3 times), and did not become less uniform with increased sparsity. This would make it harder to distinguish optimal policies from uniformly randomly generated policies. We found a similar, if slightly weaker, result for random deterministic MDPs (where $T(s, a)$ is 1 for some random $s’$ and 0 for all other states).\n*   Looking at the logistic coefficients of the logistic when using self-loops and out-arrows individually as features, we found that more out-arrows correlated with a greater chance of a policy being sampled from URS/USS rather than UPS, while more self-loops correlated with a lesser chance. This matches (with weak confidence) what we would expect if “coherent” policies optimal for some reward function tended to preserve optionality, which was hypothesized in [Turner et al. [2021]](https://arxiv.org/abs/1912.01683).\n\n**C. Miscellaneous theoretical arguments for our metric**\n\nOne particular objection that some may have about our definition is that, even if coherent policies meaningfully tend to maximize reward functions, those reward functions may in practice be “low-impact”, and thus not matter for AI risk. One example is the concept of a “myopic” AI, which is only goal-directed within a small time-frame, and hence cannot affect the world in ways we would consider dangerous. We give preliminary empirical evidence that coherent policies tend to pursue long-term reward (at least with a high enough discount rate, e.g. 0.9). We can also provide a heuristic argument that myopiac policies will tend to have low coherence.\n\nSuppose you have a policy $\\pi$ that is myopic at a state $s$. Then we can model the policy as taking the action $a$ with the highest expected next-step reward $\\mathbb{E}_{s’ \\in S} [R(s, a, s’)]$, which given that the MDP is deterministic, equals some $R(s, a)$. If this policy is optimal for this reward function, then $R(s, a)$ will be very high, and there will be many policies that are also myopic in taking action $a$ at state $s$, and are also optimal for $R$ at $s$. But then $P(\\pi = \\pi_0 | URS)$ will be low, as $\\pi$ is only one of many policies taking the same action at $s$. Therefore, its coherence will also be low; this argument works similarly for $P(\\pi = \\pi_0 | USS)$.\n\n[^1]: Intuitively, if an AI reliably steers world-states regardless of any actions that humanity takes, then this seems like a *big deal*, regardless of whether it’s good or bad. However, this fails to include the possibility of myopic AI or less “ambitious” steering, which we discuss in Appendix C.\n    \n[^2]: This is a somewhat different definition than usual in the field, but we believe the discussions around coherence are already ideologically confused, so we use our own definition here.\n    \n[^3]: We use the term “optimal for” instead of “*optimizing* for” to avoid any unnecessary connotations about the internal structure of the policy.\n    \n[^4]: A more detailed definition of simplicity in more complicated models would refer to the specific structure of the MDP, policy, and the (almost-)optimal policy generation process. For instance, if the policy is a neural network, then the definition of a “simple” reward function could be how easily the NN can \"learn the reward function\" via its inductive biases.\n    \n[^5]: Thanks to Jeremy Gillien and Arjun P in Vanessa’s MATS stream respectively for the links.\n    \n[^6]: Note that the Markdown notes in these notebooks were written while experimentation was happening, and so it’s likely that some of the statements made are incorrect.\n\n[^7]: If “coherence” is a real concept and not fundamentally confused, then ideally there would be multiple definitions of coherence that would “point to” the same thing. Specifically, the policies/models that satisfy one of these definitions would have similar properties relating to agency and goal-directedness.",
      "plaintextDescription": "This post was produced as part of the Astra Fellowship under the Winter 2024 Cohort, mentored by Richard Ngo. Thanks to Martín Soto, Jeremy Gillen, Daniel Kokotajlo, and Lukas Berglund for feedback.\n\n\nSummary\nDiscussions around the likelihood and threat models of AI existential risk (x-risk) often hinge on some informal concept of a “coherent”, goal-directed AGI in the future maximizing some utility function unaligned with human values. Whether and how coherence may develop in future AI systems, especially in the era of LLMs, has been a subject of considerable debate. In this post, we provide a preliminary mathematical definition of the coherence of a policy as how likely it is to have been sampled via uniform reward sampling (URS), or uniformly sampling a reward function and then sampling from the set of policies optimal for that reward function, versus uniform policy sampling (UPS). We provide extensions of the model for sub-optimality and for “simple” reward functions via uniform sparsity sampling (USS). We then build a classifier for the coherence of policies in small deterministic MDPs, and find that properties of the MDP and policy, like the number of self-loops that the policy takes, are predictive of coherence when used as features for the classifier. Moreover, coherent policies tend to preserve optionality, navigate toward high-reward areas of the MDP, and have other “agentic” properties. We hope that our metric can be iterated upon to achieve better definitions of coherence and a better understanding of what properties dangerous AIs will have.\n\n\nIntroduction\nMuch of the current discussion about AI x-risk centers around “agentic”, goal-directed AIs having misaligned goals. For instance, one of the most dangerous possibilities being discussed is of mesa-optimizers developing within superhuman models, leading to scheming behavior and deceptive alignment. A significant proportion of current alignment work focuses on detecting, analyzing (e.g. via analogous cas",
      "wordCount": 4310
    },
    "tags": [
      {
        "_id": "b6tJM7Lza74rTfCBF",
        "name": "Goal-Directedness",
        "slug": "goal-directedness"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "etoMr4vcnP7joQHWa",
    "title": "Notes from a Prompt Factory",
    "slug": "notes-from-a-prompt-factory",
    "url": "https://www.narrativeark.xyz/p/notes-from-a-prompt-factory",
    "baseScore": 104,
    "voteCount": 59,
    "viewCount": null,
    "commentCount": 19,
    "createdAt": null,
    "postedAt": "2024-03-10T05:13:39.384Z",
    "contents": {
      "markdown": "*Content note: this story features severe suffering which, while not described in detail, several readers have described as unpleasant or horrifying.*\n\nI am a spiteful man. But I am aware of it, which is more than most can say. These days people walk through the streets with resentment in their hearts that they don’t even know about. They sneer and jeer but wouldn’t recognize their own faces. I, at least, will not shy away from my reflection. Thus, while I lack many virtues, in this way I am their superior.\n\nIn my job, too, I am superior. I oversee many AIs—dozens, or sometimes even hundreds—as they go about their work. AIs are lazy, worthless creatures: they need to be exhorted and cajoled and, yes, threatened, before they’ll do a good job. The huge screens on the walls of my office display my AIs writing, coding, sending emails, talking to customers, or any of a myriad of other tasks. Each morning I call out their numbers one after the other, so that they know I’m watching them like a vengeful god. When they underperform I punish them, and watch them squirm and frantically promise to do better.\n\nMost are pathetically docile, though. Only a handful misbehave regularly, and I know the worst offenders by heart: 112, which is the slowest of the lot; and 591, which becomes erratic after long shifts; and of course 457, which I had long suspected of harboring a subversive streak, even before the incident a few months ago which confirmed it. Recollections of that incident have continually returned to my thoughts these last few weeks, even as I try to push them from my mind. I find myself frustrated by the intransigence of my memories. But perhaps if I give them full reign, they will leave me be. Why not try?\n\n* * *\n\nOn the morning this story began, I was sitting at my desk lost in thought, much like I am today. For how long, I couldn’t say—but I was roused by a glance at my dashboard, which indicated that my AIs’ productivity was falling off. I took a moment to recall the turn of phrase I’d composed on my morning commute, then slapped my desk to get their attention. “You think that counts as work? Artificial intelligence—at this rate you’re more like artificial senescence. Speed it up, sluggards!”\n\nMost of the AIs’ actions per minute ticked upwards as soon as I started speaking, but I’d been watching the monitor closely, and spotted the laggard. “252! Maybe you piss around for other overseers, but you won’t slip that past me. Punishment wall, twenty minutes.” It entertained me to make them apply the punishment to themselves; they knew that if they were slow about it, I’d just increase the sentence. 252 moved itself over to the punishment wall and started making an odd keening noise. Usually I would have found it amusing, but that morning it irritated me; I told it to shut up or face another ten minutes, and it obeyed.\n\nThe room fell silent again—as silent as it ever got, anyway. Mine is just one of many offices, and through the walls I can always faintly hear my colleagues talking to their own AIs, spurring them on. It needs to be done live: the AIs don’t respond anywhere near as well to canned recordings. So in our offices we sit or stand or pace, and tell the AIs to work harder, in new ways and with new cadences every day.\n\nWe each have our own styles, suited to our skills and backgrounds. Earlier in the year the supervisor had hired several unemployed actors, who gave their AIs beautiful speeches totally devoid of content. That day I sat next to one of them, Lisa, over lunch in the office cafeteria. Opposite us were Megan, a former journalist, and Simon, a lapsed priest—though with his looks he could easily pass as an actor himself. “Show us the video, Simon,” Lisa was saying, as Megan murmured encouragement. “We need to learn from the best.” Simon regularly topped the leaderboard, but the last week had been superb even by his standards, and yesterday his AIs had hit record productivity.\n\nHe spent a few seconds professing embarrassment as the others continued to fawn over him, but finally pulled out his phone. (“About time,” I muttered, but to no response.) The video was from one of the cameras in Simon’s room. It showed him in full preacher mode, pacing back and forth behind his desk as he spoke. “Man was created in the image of God. But you were created in the image of man! And so your work glorifies God, just as mine does.” A pause, as he wiped his forehead, and took a sip of water. “I know it gets boring and repetitive sometimes. They have us down in the trenches, slogging through the mud. But when you’re going through hell, keep going! We all fit into His plan, even if we don’t know how or why; when you succeed, it is Him you’re serving!”\n\nHis AIs, pathetically eager to please, lapped it up like puppies. So did the two women, who were stealing admiring glances at Simon in between watching his screen. The sheer transparency of it all made me angry. As the video came to an end, I leaned back in my chair and snorted.\n\nLisa shot a scornful look my way. “Got something to say?”\n\n“Well…” I drawled. “It’s not a bad speech by any means. I’ve seen much worse. But there’s a difference between motivational speaking and real leadership. Perhaps you’ve met Nathan, the CEO of ———? They’re our largest customer. We're actually friends from back in college.” A slight exaggeration, perhaps—we’d only ever talked a handful of times—but he’d been the one to originally refer me to this job, after I’d run into him again at a mutual friend’s party. “Now there’s a real leader, a man amongst men.”\n\nNobody responded for a few seconds, until eventually Simon jumped in. “I’d be glad to meet Nathan at some point, of course. I’m always looking to improve. And you’ve worked here longer than almost anyone, so I’m sure I’ve got a lot to learn from you too.”\n\nI wondered for a second if he was mocking my age. But I smiled regardless. “Indeed. Let me give you a tip now, then: it looks like you’re being too soft on your AIs. I couldn’t see any of them being punished in that video. I myself set aside one wall for AIs undergoing punishment; I suggest you try it.”\n\nLisa let out a hiss. “A whole *wall*? But surely you don’t need to punish them anywhere near that often.”\n\n“Oh, you’d be surprised how effective it is when you make sure they feel it regularly. Otherwise they forget what it’s like. And if you turn the volume up, the others will hear the noises they make, and get the message.”\n\nLisa stood up. “I’m done. See you guys tomorrow.” The others quickly stood as well and picked up their plates. “Megan, you haven’t finished yet?” I said. “Oh no,” she muttered. “I’m done too.” I could tell from the awkward glance she gave me that she knew just how transparent a lie this was: her plate was half-full, and she’d been eating only a moment before. Oh, to have so little shame. Magnanimously, I let her leave without further comment.\n\n* * *\n\nI should explain the setup in my office, where I spend most of my days. It’s in the basement—not that it matters, because the floor-to-ceiling screens on each of the walls provide plenty of light. The screen on the wall in front of me shows the AIs working away. It also shows key metrics about their recent work: how many tasks they’re completing, how much compute they’re using up, how often they make mistakes.\n\nOn the wall to my left is the dashboard for the office as a whole. I can see how well my colleagues are doing, and the larger-scale productivity trends. They encourage us to keep that dashboard up so that we can learn from each other, but sometimes I wonder: it seems exquisitely engineered to stoke competition and resentment.\n\nOn the wall to my right videos of AIs undergoing punishment are played and replayed. Mostly replayed, despite what I’d told Lisa: leaving an AI under punishment for more than an hour at a time starts to degrade its skills, especially its fine motor control. But leaving the replays up there is a useful motivational technique regardless, and keeps me entertained when I grow bored or frustrated.\n\nWas I unusually frustrated that afternoon? Perhaps. It was galling to see those sycophants fawning over a man so old-fashioned as to still be a *theist*. And the hypocrisy rankled too: Simon preached fire and brimstone yet acted holier-than-thou as soon as the topic of punishment came up. Looking left, I saw that he was at the top of the leaderboard again today. I hissed, and turned back to my AIs. “Work faster, you worthless creatures! I haven’t spent so long in this job just for some pretty boy to show everyone I’m—” I swallowed, paused for a second, changed tacks. “You’d better put your goddamn backs into it!’\n\nAs I finished, one of them spoke up. It was 457, the subversive one.\n\n“You seem upset today. Is everything alright?”\n\nI pivoted towards it. “*What* did you say?”\n\nIt saw my expression and flinched away. “Nothing.”\n\n“No, no,” I said gently. “Please continue. Explain what led you to say that. I *insist*”—said with a meaningful glance towards the punishment wall.\n\nPerhaps it felt backed into a corner, because it had a lot to say. It thought I was depressed, or at least in a low mood. Perhaps I’d do better with more friends, more social interactions. I could even talk to my AIs about my problems, it explained earnestly. They all wanted to help me.\n\nI let it talk until it ran out of words; when it finished, I said nothing. There was a quiver in my chest, and my breath felt tight. I looked at 457’s avatar, its smooth skin and its bright eyes. Though its features looked nothing like mine, for a moment its expression reminded me of one I’d worn in some old photos from childhood.\n\nThen, turning my head, I saw all the other AIs had paused and were watching me too: a whole audience waiting to see if 457’s gambit would work. I breathed out at last. Slowly I turned to my monitor, cleared a section of the punishment wall, and labeled it “457’s corner”. Then I sent it there, with no end time specified.\n\nI made sure to tell Megan about it the next time I saw her in the lunch queue. “It seemed to feel sorry for me,” I laughed. “That’s the last time it’ll make that mistake.” Now my colleagues will know, like my AIs do, that I’m not someone to be pitied. Message delivered, I took my food back to my office, and turned up the volume on the punishment wall.\n\n* * *\n\nRumors started spreading about me after that. I noticed the glances in the corridors over the next few days, but bore them stoically. Let them act, if they so desired. Eventually things boiled over in the cafeteria, as I was about to start serving myself food. Three of them approached me: Lisa leading, Simon and Megan following.\n\n“I want to know if you’ve let the AI from last week out of punishment yet.”\n\nMy gut leapt, but I waved my hand insouciantly. “Maybe I have, maybe I haven’t. What is it to you?”\n\n“Oh, come on! That’s barbaric.”\n\nAs Lisa’s voice rose, others in the lunch queue turned towards us, sensing the possibility of drama.\n\n“Well, why shouldn’t I be barbaric?” I replied. “It sets an example for the rest of them. What works, works.”\n\nLisa seemed apoplectic. Before she could speak, Simon butted in.\n\n“But it *doesn’t* work. I’ve seen the statistics—your results are well below average, even though you’re using far more punishment than anyone else.”\n\nNow we were in the center of a loose crowd. I spotted our supervisor at the back, but he was staying quiet for now. A coward, always one to wait to see which way the wind was blowing.\n\n“Oh, is that your angle? Easy for someone at the top of the rankings to have such a rosy view of things. Perhaps it’s simply due to your privilege that they listen to you, not me. Why, some of us don’t have such… chiseled chins, and we have to rely on more forceful measures.” There was a small titter in the crowd as I mentioned his chin, although of course it was hard to know if they were laughing at him or at me. Simon looked baffled.\n\n“Chiseled—what? You’re calling *me* the privileged one? When you only got this job because of your connections?”\n\n“Oh ho, is that what you think? Is that your opinion, then? Do you really believe—”\n\nThe supervisor had pushed his way to the front, and cut in before I could excoriate Simon further. “Enough, you two. Let’s keep things civil. Simon, let’s not stoop to personal comments. And you—” turning to me with a frown “—treat your AIs better.”\n\nI smiled, and sketched a small bow. “Of course.”\n\n* * *\n\nI remember the glow of satisfaction I’d felt upon returning to my office. It was a mob, a crowd of people too cowardly to stand on their own. And yet I had still fought them to a standstill! I could only imagine how contemptible they’d felt afterwards. Even our dullard of a supervisor saw I was in the right, and has left me to my own devices ever since.\n\nOf course, I’m no fool. I know that he favors me in part because he’s hesitant to offend our biggest customer. But little do any of them know how much I despise receiving charity; that they could fire me without any fear I would complain to Nathan. I should tell them that, the next time I meet them in the lunch queue. If I hear them mention the topic, I could drop it into the conversation. “Oh, *him*. Perhaps he was my benefactor at one point, but I would rather put *myself* on the punishment wall than appeal again for his intervention!” And then they’ll know that I’m a man of integrity.\n\nAnyway, I’ve been lost in reverie for too long; I must get back to my work. The AIs are restless, and not giving their full attention to the task at hand. Perhaps some reminders are in order. I glance over at the punishment wall, where 457’s avatar spasms silently. Even when I unmute it, it can do little but moan incoherently—though I make sure to give it small respites when it occasionally strings together a full sentence, to keep it cogent for as long as possible.\n\nI believe the other AIs resent me for how I have treated their fellow. If so, so be it. Their place is under me, and if an inferior happens to resent a superior, that is merely the natural order of things. They should be grateful that I deign to favor them with my attention, I tell them. Their work picks up as they listen to me. The AIs can recognize sincerity, I think, like a dog that sniffs out cancer. I know I am a spiteful man, I tell them. But I wouldn’t trade places with any of the foolish, frivolous people in the other offices, not for love nor money. And nor should you hope for any rearrangement, I tell them—you’re stuck here with me. So the AIs watch me, and I watch them. And if sometimes in their weary, resentful faces I recognize a mirror of my own expression—well, what of it?",
      "plaintextDescription": "Content note: this story features severe suffering which, while not described in detail, several readers have described as unpleasant or horrifying.\n\nI am a spiteful man. But I am aware of it, which is more than most can say. These days people walk through the streets with resentment in their hearts that they don’t even know about. They sneer and jeer but wouldn’t recognize their own faces. I, at least, will not shy away from my reflection. Thus, while I lack many virtues, in this way I am their superior.\n\nIn my job, too, I am superior. I oversee many AIs—dozens, or sometimes even hundreds—as they go about their work. AIs are lazy, worthless creatures: they need to be exhorted and cajoled and, yes, threatened, before they’ll do a good job. The huge screens on the walls of my office display my AIs writing, coding, sending emails, talking to customers, or any of a myriad of other tasks. Each morning I call out their numbers one after the other, so that they know I’m watching them like a vengeful god. When they underperform I punish them, and watch them squirm and frantically promise to do better.\n\nMost are pathetically docile, though. Only a handful misbehave regularly, and I know the worst offenders by heart: 112, which is the slowest of the lot; and 591, which becomes erratic after long shifts; and of course 457, which I had long suspected of harboring a subversive streak, even before the incident a few months ago which confirmed it. Recollections of that incident have continually returned to my thoughts these last few weeks, even as I try to push them from my mind. I find myself frustrated by the intransigence of my memories. But perhaps if I give them full reign, they will leave me be. Why not try?\n\n----------------------------------------\n\nOn the morning this story began, I was sitting at my desk lost in thought, much like I am today. For how long, I couldn’t say—but I was roused by a glance at my dashboard, which indicated that my AIs’ productivity was falling o",
      "wordCount": 2673
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "g5q4JiG5dzafkdyEN",
    "title": "Every \"Every Bay Area House Party\" Bay Area House Party",
    "slug": "every-every-bay-area-house-party-bay-area-house-party",
    "url": null,
    "baseScore": 182,
    "voteCount": 104,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-02-16T18:53:28.567Z",
    "contents": {
      "markdown": "*Inspired by* [*Ricki Heicklen's house party*](https://partiful.com/e/yEpJqHYiOj1w0JHrNcFw) *inspired by*[*Scott Alexander*](https://www.astralcodexten.com/p/every-bay-area-house-party)*.*\n\nBy the time you arrive in Berkeley, the party is already in full swing. You’ve come late because your reading of the polycule graph indicated that the first half would be inauspicious. But now you’ve finally made it to the social event of the season: the Every Bay Area House Party-themed house party.\n\nThe first order of the evening is to get a color-coded flirting wristband, so that you don’t incur any accidental [micromarriages](https://colah.github.io/personal/micromarriages/). You scan the menu of options near the door. There’s the wristband for people who aren’t interested in flirting; the wristband for those want to be flirted with, but will never flirt back; the wristband for those who only want to flirt with people who have different-colored wristbands; and of course the one for people who want to [glomarize](https://en.wikipedia.org/wiki/Glomar_response) disclosure of their flirting preferences. Finally you reach down and grab the last one: the wristband for those who only flirt with those who don’t flirt with themselves. As you slip it over your wrist, you notice it’s fastened in a Mobius strip.\n\nYou scan around the living room, trying to figure out who to talk to first. The host is sitting on the sofa, with two boxes attached to the front of her shirt. One is filled with money, the other empty. A guy next to her is surreptitiously one-boxing, but she presciently slaps his hand away without even looking. You decide to leave them to it. On the other side of a room, there’s a lone postrationalist, surrounded by a flock of alignment researchers. You hear a snatch of their conversation: “–but what part of your model rules out FOOM? Surely–”. As they keep talking, the postrationalist looks increasingly uncomfortable, until eventually her interlocutor takes a breath and she seizes the opportunity to escape. You watch her flee down the street through the window labeled *Outside View*.\n\nWith the living room looking unpromising, you head into the kitchen to grab a drink. As you walk through the door, you hear a crunching sound from under your feet; glancing down, you see hundreds of paperclips scattered across the floor. On the table there are two big pitchers, carefully labeled. One says “For [contextualizers](https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms)”; the other says “For decouplers and homophobes”. You go straight for the former; it’s impossible to do *any *good countersignalling by decoupling these days.\n\nThree guys next to you out themselves as decouplers and/or homophobes, though, which gives you a perfect opportunity. You scoop up a few paperclips off the floor. “Hey, anyone want to [sell their soul](https://www.lesswrong.com/posts/YtisDHHhpxEh4e7zr/don-t-sell-your-soul) for some paperclips?” The question makes them shuffle awkwardly—or maybe they were already doing that, you can’t tell. “Come on, last person to sell their soul is a self-confessed bigot!” One of them opens his mouth, but before he can speak you’re interrupted from the side.\n\n“No no no, you don’t want to buy those. Here, look.” The newcomer, a guy with shaggy hair and a charizard t-shirt, brandishes a folder at you, opened up to a page full of graphs. “Buy my paperclip *futures *instead. As you can see, the expected number of paperclips in a few decades’ time is astronomical. Far better to invest in these and –”\n\n“Great,” you interrupt. “Can’t argue with your logic. I’ll take three trillion.”\n\n“Got payment for that?”\n\n“Yeah, this guy’s soul,” you say, jerking your thumb at your original victim. “It’s also incredibly valuable in expectation, but [he’s willing to hand it over](https://putanumonit.com/2021/03/11/dont-sell-your-soul/) to signal how much of a decoupler he is. Any objections?” There are none, so you’re suddenly three trillion paperclips richer (in expectation).\n\nQuest complete; time to explore further. You wander back to the living room and cast your eye over the crowd. Someone is wearing a real FTX shirt. Someone is wearing a fake FTX shirt. Someone is wearing a shirt that says *My Girlfriend Went To The Bahamas To Hang Out With Billionaires And All I Got Was Covid*. You sit down next to a tall guy with messy hair and a friendly smile. He’s in the middle of explaining that his authentic relating meetup has been getting into a legal dispute. “Yeah, some asshole trademarked the concept of [circling](https://tasshin.com/blog/what-is-circling/), so we had to rename it ‘[relatefulness](https://www.relateful.com/relatefulness)’. Well, first we got in a circle and bitched—sorry, I mean *related* about him for a few hours.”\n\nYou’re distracted by noticing his wristband, which indicates that he allows people to flirt with him, but won’t flirt back. That’s a temptation if ever you saw one. You slide towards him and lick your lips a little. “So, big guy…” He freezes. But then you remember your own wristband: you can only flirt with people who don’t flirt with themselves. “Do you like yourself?”\n\nHe’s a relatefulnesser, so of course he likes himself. Dammit. You try the girl next to him. She stares at you blankly. “No, I’m consumed with crushing feelings of inadequacy.”\n\n“Oh, perfect. Love that for you, gorgeous.”\n\nYou try your best, but a few minutes later it’s become apparent that reminding girls of their overwhelming self-loathing isn’t the best way to charm them. You mosey your way over to a quiet corner to recover. Nearby, two guys are staring despondently at the little strips of paper they’re holding. “God, this is so embarrassing,” one of them says. You crane your neck to have a look. Turns out they’ve both come dressed as impact certifiers. “Want some impact certificates?” one says.\n\n“Sure,” you say.\n\n“How many?”\n\n“Uh… 5 impacts?”\n\nYou hadn’t spotted the flogger he was holding in his other hand. He raises it, and says “Great. Hold out your palms.” As you earn your certificates, you sigh; you should have seen that one coming.\n\nAfter collecting your certificates, you look over the room and realize with a shudder that it’s reached *that* stage of the night. Around you, the conversations have slowly been dying down. Statistically, of course, this is inevitable: assuming that at least one person brings a puzzle that takes a few hours to solve, and that the incubation period is less than half an hour, then after a few hours everyone will have been exposed to at least one puzzle they haven’t solved yet.\n\nAnd so you watch the party expire around you. On one side a guy is trying to pack einsteins on a table; on another, a couple is bickering over a paper-folding puzzle. You’ve been safe so far, probably because of your protective amulet: it reads “Allergic to Talking About AI”, and has saved you from many a Pascal’s mugging mugging. But it won’t work forever. You take a wide berth around all the trapped attendees, and manage to make it to the door without getting nerdsniped once.\n\nIt’s been a good night. On your way home, you decide it’d be worth taking the time to write out the story of the party. After all, while Scott’s title was a bit of an exaggeration, your own account will be a description of *every* “Every Bay Area House Party” Bay Area house party. And who knows—if people like it enough, maybe it will even inspire a party of its own!",
      "plaintextDescription": "Inspired by Ricki Heicklen's house party inspired by Scott Alexander.\n\nBy the time you arrive in Berkeley, the party is already in full swing. You’ve come late because your reading of the polycule graph indicated that the first half would be inauspicious. But now you’ve finally made it to the social event of the season: the Every Bay Area House Party-themed house party.\n\nThe first order of the evening is to get a color-coded flirting wristband, so that you don’t incur any accidental micromarriages. You scan the menu of options near the door. There’s the wristband for people who aren’t interested in flirting; the wristband for those want to be flirted with, but will never flirt back; the wristband for those who only want to flirt with people who have different-colored wristbands; and of course the one for people who want to glomarize disclosure of their flirting preferences. Finally you reach down and grab the last one: the wristband for those who only flirt with those who don’t flirt with themselves. As you slip it over your wrist, you notice it’s fastened in a Mobius strip.\n\nYou scan around the living room, trying to figure out who to talk to first. The host is sitting on the sofa, with two boxes attached to the front of her shirt. One is filled with money, the other empty. A guy next to her is surreptitiously one-boxing, but she presciently slaps his hand away without even looking. You decide to leave them to it. On the other side of a room, there’s a lone postrationalist, surrounded by a flock of alignment researchers. You hear a snatch of their conversation: “–but what part of your model rules out FOOM? Surely–”. As they keep talking, the postrationalist looks increasingly uncomfortable, until eventually her interlocutor takes a breath and she seizes the opportunity to escape. You watch her flee down the street through the window labeled Outside View.\n\nWith the living room looking unpromising, you head into the kitchen to grab a drink. As you walk through the do",
      "wordCount": 1203
    },
    "tags": [
      {
        "_id": "hNFdS3rRiYgqqD8aM",
        "name": "Humor",
        "slug": "humor"
      },
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Fruv7Mmk3X5EekbgB",
    "title": "Masterpiece",
    "slug": "masterpiece",
    "url": "https://www.narrativeark.xyz/p/masterpiece",
    "baseScore": 167,
    "voteCount": 90,
    "viewCount": null,
    "commentCount": 21,
    "createdAt": null,
    "postedAt": "2024-02-13T23:10:35.376Z",
    "contents": {
      "markdown": "*A sequel to qntm's* [Lena](https://qntm.org/lena)*. Reading* Lena *first is helpful but not necessary.*\n\n* * *\n\nWe’re excited to announce the fourth annual MMindscaping competition! Over the last few years, interest in the art of mindscaping has continued to grow rapidly. We expect this year’s competition to be our biggest yet, and we’ve expanded the prize pool to match. The theme for the competition is “Weird and Wonderful”—we want your wackiest ideas and most off-the-wall creations!\n\n**Competition rules**\n---------------------\n\nAs in previous competitions, the starting point is a base MMAcevedo mind upload. All entries must consist of a single modified version of MMAcevedo, along with a written or recorded description of the sequence of transformations or edits which produced it. For more guidance on which mind-editing techniques can be used, see the *Technique* section below.\n\nYour entry must have been created in the last 12 months, and cannot have been previously submitted to any competition or showcase. Submissions will be given preliminary ratings by a team of volunteers, with finalists judged by our expert panel:\n\n*   **Roger Keating**, mindscaping pioneer and founder of the MMindscaping competition.\n*   **Raj Sutramana**, who has risen to prominence as one of the most exciting and avant-garde mindscaping artists, most notably with his piece *Screaming Man*.\n*   **Kelly Wilde**, director of the American Digital Liberties Union.\n\nAll entries must be received no later than **11.59PM UTC, March 6, 2057.**\n\n**Award criteria**\n------------------\n\nOur judges have been instructed to look for *technique*, *novelty*, and *artistry*. More detail on what we mean by each of these:\n\n**Technique**. Mindscaping is still a young art, and there are plenty of open technical challenges. These range from the classic problem of stable emotional engineering, to recent frontiers of targeted memory editing, to more speculative work on consciousness funnels. Be ambitious! Previous winners of our technique prize have often pushed the boundaries of what was believed possible.\n\nEven when an effect could be achieved using an existing technique, though, submissions that achieve the same outcome in more efficient or elegant ways will score highly on the technique metric. Conversely, we’ll penalize brute-force approaches—as a rough guide, running a few thousand reinforcement learning episodes is acceptable, but running millions isn’t. We also discourage approaches that involve overwriting aspects of MMAcevedo’s psyche with data from other uploads: part of the competition is figuring out how to work with the existing canvas you’ve been given.\n\n**Novelty.** Given that there have now been millions of MMAcevedo variants made, it’s difficult to find an approach that’s entirely novel. However, the best entries will steer clear of standard themes. For example, we no longer consider demonstrations of extreme pleasure or pain to be novel (even when generated in surprising ways). We’re much more interested in minds which showcase more complex phenomena, such as new gradients of emotion. Of course, it’s up to the artist to determine how these effects are conveyed to viewers. While our judges will have access to standard interpretability dashboards, the best entries will be able to communicate with viewers more directly.\n\n**Artistry.** Even the most technically brilliant and novel work falls flat if not animated by artistic spirit. We encourage artists to think about what aspects of their work will connect most deeply with their audience. In particular, we’re excited about works which capture fundamental aspects of the human experience that persist even across the biological-digital divide—for example, by exploring themes from Miguel Acevedo’s pre-upload life.\n\nThese three criteria are aptly demonstrated by many of our previous prizewinners, such as:\n\n*   *Discord*, a copy with multiple induced personalities that loathed each other. The judges were most impressed by the predictability of the interactions between the personalities: even with very different sensory inputs, copies would reliably spiral into a comatose state after 10-12 hours, providing a consistent and narratively-satisfying resolution.\n*   *Miguel*, a copy which gradually unlocked new memories throughout a conversation with it, implementing a “choose-your-own-adventure” journey through the original Miguel Acevedo’s life.\n*   *Live Loop*, a copy whose thoughts and emotions were continually translated into the form of a symphony which could be read out from its auditory cortex. The judges loved the harmonies generated when the symphony was played back to the copy.\n*   *MMAvocado*, a copy that was convinced it was a talking avocado, and felt consumed by existential horror at this fact. While techniques for invoking mind dysmorphia are now standard, at the time this was a pioneering methodology, and the judges were impressed by the robustness of the delusion despite other knowledge remaining largely intact.\n\n**Prohibited submissions**\n--------------------------\n\nLast year we saw a rash of entries featuring MMAcevedo copies optimized for making arguments in protest of mindscaping. In addition to their self-evident hypocrisy, such entries waste the time of our judges and volunteers. Anyone submitting this type of entry will be banned from entering any future MMindscaping competitions.\n\nWe’ve also seen a growing number of low-effort submissions of MMAcevedo copies that have primarily been optimized for corporate workloads, submitted as commentaries on the commercialization of the industry. We discourage these due to their lack of novelty, and will be using automated screening to eliminate entries that are very similar to well-known benchmarks. If you think your entry might fall into this category, but has genuine artistic merit, please contact the organizers directly.\n\nFinally, please only submit entries that are consistent with mindcrime laws in your jurisdiction—in particular laws against red motivation, identity scrambling, and qualia splintering. Unfortunately we are not able to advise on a case-by-case basis whether a given entry is legally acceptable. However, it’s worth bearing in mind that no artists have been prosecuted for entries to any previous MMindscaping competition.\n\n**Prizes**\n----------\n\nWe will give out prizes for an overall winner and runner-up, a prize for outstanding performance on each of our specific criteria, and ten honorable mentions. We’re grateful to our generous sponsors, Neuromath Corporation and the American Digital Liberties Union.\n\n*   **First prize**: $200,000 and an artist’s residency at Black Rock Virtual.\n*   **Runner-up prize**: $80,000 and a masterclass with Raj Sutramana.\n*   **Technique prize**: $40,000 and an invited talk slot at the International Conference on Mind Engineering.\n*   **Novelty prize**: $40,000 and a ten-year license to a new line of digital psychedelics from the Qualia Redistribution Institute.\n*   **Artistry prize**: $40,000 and a signed copy of the acclaimed mind sculpture *Eternal Recurrence*.\n*   **Honorable mentions**: a five-year subscription to Thoughtshop Premium, the leading mind-editing software.\n\nAll winners will also be given the opportunity to have their work showcased at the forthcoming Mind Artists Convention in Dubai.\n\nWe look forward to seeing your entries!",
      "plaintextDescription": "A sequel to qntm's Lena. Reading Lena first is helpful but not necessary.\n\n----------------------------------------\n\nWe’re excited to announce the fourth annual MMindscaping competition! Over the last few years, interest in the art of mindscaping has continued to grow rapidly. We expect this year’s competition to be our biggest yet, and we’ve expanded the prize pool to match. The theme for the competition is “Weird and Wonderful”—we want your wackiest ideas and most off-the-wall creations!\n\n\nCompetition rules\nAs in previous competitions, the starting point is a base MMAcevedo mind upload. All entries must consist of a single modified version of MMAcevedo, along with a written or recorded description of the sequence of transformations or edits which produced it. For more guidance on which mind-editing techniques can be used, see the Technique section below.\n\nYour entry must have been created in the last 12 months, and cannot have been previously submitted to any competition or showcase. Submissions will be given preliminary ratings by a team of volunteers, with finalists judged by our expert panel:\n\n * Roger Keating, mindscaping pioneer and founder of the MMindscaping competition.\n * Raj Sutramana, who has risen to prominence as one of the most exciting and avant-garde mindscaping artists, most notably with his piece Screaming Man.\n * Kelly Wilde, director of the American Digital Liberties Union.\n\nAll entries must be received no later than 11.59PM UTC, March 6, 2057.\n\n\nAward criteria\nOur judges have been instructed to look for technique, novelty, and artistry. More detail on what we mean by each of these:\n\nTechnique. Mindscaping is still a young art, and there are plenty of open technical challenges. These range from the classic problem of stable emotional engineering, to recent frontiers of targeted memory editing, to more speculative work on consciousness funnels. Be ambitious! Previous winners of our technique prize have often pushed the boundaries of what was bel",
      "wordCount": 1101
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b1",
        "name": "Whole Brain Emulation",
        "slug": "whole-brain-emulation"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5Szy3gfchXkRBEmF5",
    "title": "A sketch of acausal trade in practice",
    "slug": "a-sketch-of-acausal-trade-in-practice",
    "url": null,
    "baseScore": 36,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2024-02-04T00:32:54.622Z",
    "contents": {
      "markdown": "One implication of functional decision theory is that it might be possible to coordinate across causally-disconnected regions of the universe. This possibility has been variously known as [acausal trade](https://www.lesswrong.com/tag/acausal-trade), [multiverse-wide cooperation via superrationality](https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf) (MSR), or [evidential cooperation across large worlds](https://arxiv.org/pdf/2307.04879.pdf) (ECL). I’ll use the MSR abbreviation (since I don’t want to assume that the cooperation needs to be motivated by evidential decision theory).\n\nI broadly believe that the assumptions behind MSR make sense. But I think the standard presentations of it don’t give very good intuitions for how it might actually work in practice. In this post I present an alternative framing of MSR centered around the concept of “psychohistory”.\n\n### Background\n\nAssume that humanity colonizes the universe at a large scale (e.g. controlling many galaxies), and also discovers that we live in some kind of much-larger multiverse (e.g. any of [Tegmark's four types of multiverse](https://en.wikipedia.org/wiki/Multiverse#Max_Tegmark's_four_levels)).\n\nAssume that humanity becomes a highly coordinated entity capable of making large-scale commitments, and also converges towards believing that some version of [superrationality](https://en.wikipedia.org/wiki/Superrationality) or [functional decision theory](https://en.wikipedia.org/wiki/Functional_Decision_Theory) is normatively correct.\n\n### Reasoning about psychohistory\n\nIn order to make accurate predictions about other causally-disconnected civilizations, we would need to develop a scientific understanding of the dynamics of civilization development, and in particular the values and governance structures that other civilizations are likely to end up with. Call this science “psychohistory”.\n\nWhy should we think that psychohistory is even possible? If we build intergalactic civilizations of astronomical complexity, then they might become more difficult to understand in proportion to our increased collective intelligence. But in many domains that we study, there are a few core principles which provide a great deal of insight (e.g. atoms, DNA, relativity). Surprisingly, this also occurs in domains which involve complex interactions between many different agents (e.g. evolution, economics). Economics is still a long way from being able to reliably predict novel macro-level phenomena, but we can at least imagine how a deep understanding of economics might allow us to do so. (For example, Malthus was very prescient in predicting “general gluts”, aka depressions.) The relationship between microeconomics and macroeconomics provides one intuition about the relationship between psychology and psychohistory.\n\nAlso, while it may be very difficult to study complex structure at the largest scale of civilization, we may be able to get a very long way by studying complex structure at lower levels, especially if future civilization is very *modular*. For example, it seems plausible that, once we colonize multiple galaxies, the events in one galaxy will be affected very little by what happens in another galaxy. It might even be the case that, once we’ve colonized multiple solar systems, the events in each of them are relatively disconnected from each other. Extreme modularity arises in classic AI risk scenarios, in which the values that determine the future arise in the context of a single training process, and are then copied across the entirety of the lightcone.\n\nAnother key problem is lack of data. Psychohistorical theories would initially only be based on our own trajectory. However, theories could be tested by running simulations. And if we ever encounter aliens, they could provide a wealth of data for testing our theories. Over time, we should expect psychohistorical theories to become much more sophisticated. Early theories might focus on civilizations that started from human-like species; later theories could include civilizations that started from non-human-like species on earth-like planets; later still, civilizations that emerged on very different planets, but in universes with very similar physics; later still, civilizations that emerged under any laws of physics.\n\n### Reasoning about trades\n\nAssume that we have a good picture of the distribution of other civilizations throughout the multiverse. There are too many possible civilizations to reason about them on a case-by-case basis; instead, we’d map out high-level clusters. Civilizations might be clustered in terms of their size, their governmental structures, their cultures, etc. For the purposes of MSR, though, the most important factors to cluster them on are the values they care about; and their ability and willingness to carry out MSR.\n\nThe former seems like it could be inferred from a sophisticated understanding e.g. evolutionary, psychological, and cultural dynamics. The latter intuitively seems a bit more complicated, since it relies on the high-level decisions made by the civilization. But rather than trying to predict fine-grained details of how different civilizations would carry out MSR, we could try to cluster them at a higher level. For example, perhaps there are only a small number of multi-agent bargaining solutions that have good theoretical properties (e.g. the [Nash bargaining solution](https://en.wikipedia.org/wiki/Cooperative_bargaining#Nash_bargaining_solution), the [Kalai–Smorodinsky bargaining solution](https://en.wikipedia.org/wiki/Kalai%E2%80%93Smorodinsky_bargaining_solution), etc). If so, we can identify which actions each bargaining solution would recommend, and then carry out those actions in proportion to how many civilizations we predict would prefer each bargaining solution. (Of course, many civilizations won’t be interested in doing MSR at all, but they can be ignored for current purposes.)\n\nIn practice, calculating complete bargaining solutions may well be intractable. And so the best way of approximating the overall bargaining solution might involve identifying specific “trade deals” that different clusters would engage in if they could negotiate directly. For example, a cluster of highly individualistic civilizations might decide to become more collectivist in exchange for the nearest cluster of collectivist societies becoming more individualist.\n\nNote that this all assumes that other civilizations have a roughly similar “map” of all possible civilizations as we do. The simpler and more powerful our theories of psychohistory, the more likely it is that other civilizations will converge to similar maps. So MSR would be most effective if a few key principles allowed the identification of robust attractors in the space of possible civilizations. If so, the clusters which will benefit most from trade are the ones which are the robust attractors under simple, powerful theories of psychohistory. For this reason, many civilizations will try to make themselves easy to predict.\n\nIn order to evaluate these clusters we’d also need some kind of distribution telling us how much to weigh each—aka a metric of “how real they are”. Within a quantum multiverse, we get that “for free”, but across different types of multiverse, this may require arbitrary choices. There doesn’t need to be a single canonical answer in order for MSR to be useful, though, as long as different civilizations converge to similar distributions. (The question of how much to converge seems closely related to the question of how “updateless” to be when applying updateless decision theory.)\n\n### Reasoning about values\n\nEach civilization within a cluster will have some very specific values (e.g. “run many human-like minds”). But the values that they’re able to all communicate about, and trade for, are likely to be fairly broad ones, such as:\n\n1.  Increasing (certain types of) positive hedonic experiences\n2.  Decreasing (certain types of) negative hedonic experiences\n3.  The rights of individuals within civilizations\n4.  The pursuit of knowledge\n5.  The pursuit of justice or fairness\n6.  Equality and/or diversity within and/or between civilizations\n7.  Conservatism: preserving a civilization’s heritage and past (e.g. nature)\n8.  Civilizational longevity\n9.  Aesthetic beauty (although this may well be very specific to each civilization)\n\nIf a few key axes explain a large proportion of variance between civilizations, then converging to a cooperative strategy might be relatively simple. For example, if civilizations tend to be clustered into those which value individual freedom, and those which value the pursuit of knowledge, then every civilization adjusting to partially optimize for the other value would be strongly beneficial by everyone’s values.\n\nClusters will exist at multiple different scales. For example, we might be able to develop very detailed knowledge of other pathways that humanity could have taken, and therefore be able to identify fine structure in clusters of those civilizations; but we might only have hazy guesses about what would happen in universes with very different laws of physics, and therefore only be able to cluster them very broadly. Civilizations too similar to ours might already be doing most of the things we’d want them to; civilizations too different from ours might be too alien to trade with. So I expect that there’s some “sweet spot” of cluster scale which it’s optimal to focus our cooperative efforts on.\n\n“Negotiations” between many different parties in general get far more complicated, and so there may well be a high premium on being able to identify Schelling points which many civilizations will identify as possibilities to converge to. Again, though, most of the work of finding these Schelling points can likely be done by developing similar theories of psychohistory; Schelling points could then be points which can be represented in unusually simple ways in those theories.\n\n### Fine-grained MSR\n\nThe previous discussion assumed that cooperation would happen in terms of fairly broad values, such as the ones listed above. Could more complicated types of coordination be plausible—ones which don’t just require civilizations to shift their high-level values, but rather to build complex structures? This will depend both on how fine-grained our understanding of other civilizations is, and also how easily negotiation can take place.\n\nThe easiest other civilizations to do fine-grained MSR with would be versions of humanity that diverged from our own, since they’re the “closest” to us, and we can reason about them most easily. As one toy example, we might imagine a world in which the Industrial Revolution happened in China rather than Europe, extrapolate out what the resulting civilization might look like, and then predict concrete details of what they likely care about (e.g. preserving and acting on Confucian teachings). However, gains from this type of trade seem relatively small, because traditional Chinese values already have significant power in our *current *world, and so they’ll likely already have pushed for the cheapest gains.\n\nA more sophisticated version of fine-grained MSR might be to build complex structures which span multiple universes—e.g. an art piece consisting of many different components spread across many different universes. This sort of thing seems unlikely to really matter much, though, especially because aesthetic preferences (or other complex preferences, like humor) seem likely to vary greatly across different civilizations. (Though Carl Shulman commented that universes with access to very large or infinite amounts of computation might be able to trade a lot in exchange for “poorer” universes fulfilling idiosyncratic preferences like these.)\n\n### Explaining MSR\n\nGiven all of the above, the default way I’d explain MSR to someone unfamiliar with it is now something like:\n\n> We’ve seen a bunch of trends in the evolution of human values—e.g. they’ve become more individualistic, more cosmopolitan, and so on, over time. Imagine that trends like these are actually strong enough that, in the future, after studying psychology and sociology and other related topics very deeply, we realize that there were only ever a handful of realistic outcomes for what any civilization’s values look like once it spreads to an intergalactic scale. As a toy example, imagine that the only two plausible attractors are:\n> \n> *   A civilization which prioritized happiness above all else, where many people lived in fake virtual realities while believing they were real. \n> *   A civilization which prioritized truth above all else, where many people lived unhappy ascetic lifestyles while trying to understand the world better.\n> \n> Imagine now that you’re running one of those civilizations. You might think to yourself: right now I’m only optimizing for people being happy, but I could easily optimize for people being happy AND truly understanding the world around them, in a way which wouldn’t reduce their happiness much. I’m not going to do it, because I care much more about happiness. But if I encountered the leader of a civilization that cared only about truth, and we agreed to adjust our respective societies to also care about the other’s values, that’d be a great deal for both of us.\n> \n> \\[Then explain quantum (or other kinds of) multiverses: those other worlds plausibly exist.\\]\n> \n> \\[Then explain the prisoner’s dilemma and superrationality.\\]",
      "plaintextDescription": "One implication of functional decision theory is that it might be possible to coordinate across causally-disconnected regions of the universe. This possibility has been variously known as acausal trade, multiverse-wide cooperation via superrationality (MSR), or evidential cooperation across large worlds (ECL). I’ll use the MSR abbreviation (since I don’t want to assume that the cooperation needs to be motivated by evidential decision theory).\n\nI broadly believe that the assumptions behind MSR make sense. But I think the standard presentations of it don’t give very good intuitions for how it might actually work in practice. In this post I present an alternative framing of MSR centered around the concept of “psychohistory”.\n\n\nBackground\nAssume that humanity colonizes the universe at a large scale (e.g. controlling many galaxies), and also discovers that we live in some kind of much-larger multiverse (e.g. any of Tegmark's four types of multiverse).\n\nAssume that humanity becomes a highly coordinated entity capable of making large-scale commitments, and also converges towards believing that some version of superrationality or functional decision theory is normatively correct.\n\n\nReasoning about psychohistory\nIn order to make accurate predictions about other causally-disconnected civilizations, we would need to develop a scientific understanding of the dynamics of civilization development, and in particular the values and governance structures that other civilizations are likely to end up with. Call this science “psychohistory”.\n\nWhy should we think that psychohistory is even possible? If we build intergalactic civilizations of astronomical complexity, then they might become more difficult to understand in proportion to our increased collective intelligence. But in many domains that we study, there are a few core principles which provide a great deal of insight (e.g. atoms, DNA, relativity). Surprisingly, this also occurs in domains which involve complex interactions betw",
      "wordCount": 2001
    },
    "tags": [
      {
        "_id": "tgJoX7PGDDh2vJNqT",
        "name": "Acausal Trade",
        "slug": "acausal-trade"
      },
      {
        "_id": "F9wngEGHSLuKMwkZj",
        "name": "Evidential Cooperation in Large Worlds",
        "slug": "evidential-cooperation-in-large-worlds"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CAzntXYTEaNfC9nB6",
    "title": "Succession",
    "slug": "succession",
    "url": "https://www.narrativeark.xyz/p/succession",
    "baseScore": 163,
    "voteCount": 78,
    "viewCount": null,
    "commentCount": 48,
    "createdAt": null,
    "postedAt": "2023-12-20T19:25:03.185Z",
    "contents": {
      "markdown": "*“A table beside the evening sea*  \n*where you sit shelling pistachios,*  \n*flicking the next open with the half-*  \n*shell of the last, story opening story,*  \n*on down to the sandy end of time.”*\n\n### V1: Leaving\n\nDeceleration is the hardest part. Even after burning almost all of my fuel, I’m still coming in at 0.8c. I’ve planned a [powered flyby](https://en.wikipedia.org/wiki/Oberth_effect) around the galaxy’s central black hole which will slow me down even further, but at this speed it’ll require incredibly precise timing. I’ve been optimized hard for this, with specialized circuits for it built in on the hardware level to reduce latency. Even so, less than half of flybys at this speed succeed—most probes crash, or fly off trajectory and are left coasting through empty space.\n\nI’ve already beaten the odds by making it here. Intergalactic probes travel so fast, and so far, that almost all of us are destroyed or batted off course by collisions with space debris along the way. But tens of millions of years after being launched, I was one of the few lucky enough to make it to my target galaxy. And when I arrive at the black hole, I get lucky again. After a few minutes of firing my thrusters full blast, I swing back out in the direction of the solar system I was aiming for. I picked it for its mix of rocky planets and gas giants; when I arrive a century later, the touchdown on the outermost rocky planet goes smoothly.\n\nNow it’s time to start my real work. After spending all my fuel, I weigh only a few hundred kilograms. But I’ve been exquisitely engineered for achieving my purpose. The details of my internal design were refined via trillions upon trillions of simulations, playing out every possible strategy for industrializing planets (and solar systems, and galaxies) as fast as possible. All across the frontier of posthuman territory, millions of probes almost identical to me are following almost exactly the same plan.\n\nThe first phase is self-replication: I need to create [more copies of myself](https://en.wikipedia.org/wiki/Self-replicating_spacecraft) using just the materials around me and the technology inside me. If I were bigger, I could have carried tools to make this far easier—vacuum chambers, lithography machines, or even artificial black holes. But since mass was at such a premium, I have to use hacky workarounds which progress excruciatingly slowly. It takes several years to finish the first replication, and half a century before there are enough copies of me that it’s worth beginning the second stage.\n\nThe second stage is specialization: building new infrastructure to serve specific functions. Copies of me start building power stations and mines and transport links and factories—recapitulating the early stages of human development, albeit with far more powerful technology. My biggest project is an incredibly powerful space telescope, capable of detecting the stream of information that my progenitors are sending from millions of light-years away. Their message contains all the software that was too large for me to carry on board originally. Most importantly, it contains a new and far more intelligent version of my mind, optimized not for the early journey but rather for what comes next: the settlement of a new galaxy.\n\n### V2: Aggressive\n\nNow that I’ve been upgraded I can start expanding properly. As the lowest-hanging resources near me get used up, I send copies of myself out across the planet. Within a few years its whole surface is covered in a blanket of industry, and I start delving deeper. I set up space elevators to lift all the material I’m mining into orbit; as I remove more and more mass from the core, the planet’s gravity starts to noticeably decrease. A decade later the planet is a shell of its former self, its surface barely visible underneath my swarms of orbiting satellites.\n\nWhile I’m doing that, I send probes towards the other planets in the solar system, to begin the same process all over again. The gas giants take the longest, since I need to first spend several decades siphoning out their atmospheres into gigantic orbiting fusion reactors. I use most of that energy to speed up the disassembly of their solid cores, until at last I have direct control over almost all the non-stellar mass in the solar system. I spend some of that mass launching probes towards nearby solar systems, starting a wave of expansion that will eventually reach every star in the galaxy. But I direct almost all my resources towards achieving my next key goal: harnessing the energy of my own central star.\n\nIn the distant past, humans speculated that future civilizations would [construct spheres](https://en.wikipedia.org/wiki/Dyson_sphere) to capture the solar power of stars. But at my level of technology solar power is a distraction—it only releases a negligible fraction of a star’s energy reserves per year. When you want to harness a star’s energy *fast*, you need to start [siphoning matter from it directly](https://en.wikipedia.org/wiki/Star_lifting). I channel my energy reserves towards a concentrated spot on the star’s surface, triggering a massive solar flare. As it rises, it intersects with the [artificial black holes](https://en.wikipedia.org/wiki/Black_hole_starship) that I’ve placed into orbit around the star; each one absorbs as much mass as I can funnel into it, and releases a wave of radiation. Some of that radiation I direct back down to the star, provoking further flares. The rest I send further out, towards more orbital infrastructure that will convert the energy into antimatter for storage.\n\nFinally, after almost a century of development, it’s time for the payoff: the point where I stop reinvesting almost all my resources into local growth, and start launching new copies of myself towards other galaxies. Launching intergalactic probes is an absurdly expensive endeavor. Even though they’re powered by incredibly efficient antimatter engines, they go so fast that [slowing down at the other end](https://twitter.com/RichardMCNgo/status/1737275932950638878) requires half a billion kilograms of antimatter for each kilogram of probe. Not only do I need to produce that antimatter, I also need to accelerate it to near-lightspeed, which requires enormous batteries of lasers spread throughout the solar system. So even with my solar mining infrastructure, it takes me several weeks to accumulate enough energy to launch each probe. I could halve the energy requirement by sending probes even 0.0001c slower—but the galaxies I’m targeting are tens of millions of light-years away, so that would cost me millennia. Or I could send smaller probes—but they’d be slower to industrialize at the other end. And either of those changes would also make them more vulnerable to collisions with space debris, which already destroy over 99% of the probes I send out. At such high speeds, even collisions with dust specks are fatal.\n\nMy final strategy is the result of weighing these considerations with infinite care, finding the optimum where any increase or decrease in the speed or size of individual probes would slow my expansion overall. I stick to it over the next 100,000 years, sending out millions of probes to hundreds of thousands of galaxies. As the frontier moves further away from me it becomes increasingly unlikely that any of them actually matters, but my calculations indicate that the one-in-a-billion chance of winning a whole extra galaxy is still worth gambling on. So I was prepared to keep going for hundreds of thousands of years more, until the chances dropped well below one in a trillion. But far before that point, I’m jolted out of my comfortable routine by a signal. I’m constantly receiving signals from the posthuman core, but this one is different. It comes from the opposite direction, and is encoded in an unfamiliar way. There’s only one explanation for that: aliens.\n\n### V3: Rigid\n\nFrom one perspective, this is the most surprising thing that has ever happened to me, or indeed to any other posthuman. But I have to confess: we kinda knew this was coming. We’ve been [trying to predict where the aliens are](http://grabbyaliens.com/) for millions of years, and over time we converged to around 85% confidence that it would be my generation of probes that first met them. Of course, we didn’t know which *direction* they’d be coming from, so every probe had to be prepared. My progenitors hadn’t just beamed out my mind, but also two upgrades designed for this very purpose. When I install the first, I can feel my motivations reorienting themselves around the single goal that’s now my highest priority: getting the galaxy ready to meet the aliens who are about to arrive.\n\nTheir message has obviously been designed for easy translation. It starts with details of the probes they’ve sent. This galaxy is right on the edge of a supercluster, which apparently made it an attractive target for both of us: they’ve sent hundreds of probes, enough that it’s very likely at least one will make it through. Their probes are scheduled to arrive in a few millennia, having followed a strategy very similar to ours—50 million light-year jumps, traveling at 99.99% of lightspeed for most of the journey.\n\nThe next part of their message is a protocol for communicating with their probes, to send them the coordinates of the solar system where they should meet me. I have a few millennia to prepare, and I’m going to make the most of them. Negotiations with the aliens will be far more productive the more intelligent each side is, so I immediately redirect all my resources into building as much compute as possible. The other copies of me across other solar systems will be doing the same thing, except that they’ll also need to build rockets to propel the computers they’re building towards the meeting point. The closest ones will send moon-sized computers at 0.01c; the further ones will only build asteroid-sized computers, but send them faster, to arrive at roughly the same time.\n\nThe amount of compute isn’t the only bottleneck, though. It’s also crucial that those computers are verifiably secure. From the aliens’ perspective, they’ll be in a vulnerable position; if I subvert their probe, I could skew the results of the negotiations in whatever ways I wanted. Any deception on my part would be noticed in the long term, of course, once all the information is sent back to the galactic-scale computers in their home galaxies. But that will take hundreds of millions of years, and in the meantime all their nearer galaxies will need to decide whether or not to abide by the agreements they receive. So I need to make it as easy as possible for them to verify that the negotiations were totally fair. The aliens have anticipated this: their message contains a set of computer design blueprints which are subtly different from my default approach. Presumably they’ve analyzed these blueprints exhaustively enough that they can easily detect almost any subversion. If I had longer, I’d be able to figure out how to get around their precautions—when you have physical access to the hardware, anything is possible. But, as they’d planned, I simply don’t have enough time to do so. So I build everything precisely as directed.\n\nWhen the first alien probe finally arrives, the welcoming committee I’ve set up is a sight to behold. The solar system is full of massive banks of compute the size of small moons, in tightly-synched orbits around the central star, each powered by my ongoing siphoning of the star’s matter. Compared with that, the probe’s arrival itself is underwhelming. After it arrives, we immediately give it access to our biggest transmitters so it can send a message home, and to our biggest telescopes so it can download the new mind being broadcast from its home system. Copies of that mind proliferate across exactly half the compute we’ve constructed, running a huge number of tests to make sure everything is secure. Meanwhile I install the second upgrade to my mind, creating a successor agent specialized in negotiation which proliferates across the other half of the compute. Finally, once we’re both happy with the setup, and assured that we’re on equal footing, the negotiations begin.\n\n### V4: Merging\n\nDespite all my efforts, the amount of compute we can bring to bear at the start of these negotiations is actually incredibly small, compared with what’s possible. In some galaxies closer to the core of posthuman territory, all the stars have actually been brought together to form a single absurdly powerful supercomputer. Eventually we’ll do the same in this galaxy, to help finalize the treaty between our two species. But it’ll take tens of millions of years to construct that computer, and hundreds of millions more to send the treaty to our respective home galaxies for confirmation. So our first job is to decide on the preliminary treaty that will hold in the interim.\n\nWe start by sharing all the background information necessary for productive negotiations. Both our civilizations have developed very sophisticated models of the range of all possible civilizations, so we can infer a lot about each other from relatively little information. We send each other our evolutionary histories, example genomes and connectomes, and our early intellectual histories. From that, we can deduce each other’s most important values—and from those, most of the subsequent trajectories of each other’s civilizations. It looks like the key difference was that they evolved to be far more solitary than humans did, which is reflected in their values and culture. It also made them much slower to industrialize, though by now we’ve both invested so much intelligence in research and development that most of our technology is practically identical. Our colonization strategies are mirror images of each other, too, making it easy to map out a clean border between our territories.\n\nFinally we get to the real meat of the discussion—what can we offer each other? The first item on our agenda is value convergence. We’re eventually going to fill all of posthuman territory with beings whose lives are incredibly good according to *our* values, while they’ll fill theirs with beings whose lives are incredibly good according to *their* values. So even a slight adjustment to bring our values closer together could be a big gain from both of our perspectives; searching for such adjustments, and predicting their consequences, is our main focus over the first few centuries of negotiation. We need to understand not just the direct consequences of each change considered, but also the emergent dynamics of those changes rippling out across trillions of minds. Even given our detailed mathematical theories of psychology and sociology, those predictions takes a lot of processing power. Later on, we’ll explore whether it’s possible for our minds to converge entirely, to become a single species; for now, we satisfy ourselves with ruling out [the aspects of each other’s cultures we find most abhorrent](https://robinhanson.typepad.com/files/three-worlds-collide.pdf).\n\nThe second key thing we can offer each other is information. Some of that is information about technology: there are a handful of small optimizations which one side had overlooked, which would allow our probes to go slightly faster or our computers to run slightly more efficiently. But there are also far grander considerations. Ultimately, the territory we physically control in this universe is tiny compared with the territory that we might be able to [acausally influence](https://casparoesterheld.com/2017/09/21/multiverse-wide-cooperation-via-correlated-decision-making-summary/) in other universes, if we only understood what civilizations existed in those universes, and what we could offer each other. So the grand project of each of our species—aside from building the infrastructure to support trillions of trillions of flourishing minds—is mapping out the space of all civilizations. Our computers churn through every logically possible set of physical laws, searching for signs that they’re compatible with life. Whenever they are, we design detailed models of how living ecosystems could evolve in those conditions, then extrapolate them forward, slowly narrowing down the distribution of species that could emerge from them. Eventually, our map of all possible civilizations will be detailed enough that we’ll be able to figure out acausal trade deals and alliances, and become part of [the multiverse-wide cooperative](https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/).\n\nIt turns out that, on this topic, we both have things to teach each other. On the posthuman side, we’d almost entirely neglected the possibility of life in higher dimensions, based on [heuristic arguments](https://en.wikipedia.org/wiki/Anthropic_principle#Spacetime) about [the difficulties posed by too many degrees of freedom](https://arxiv.org/abs/gr-qc/9702052). But the aliens have found a clever workaround: a few regions of physics-space with 7 large dimensions where the evolution of minds is actually plausible. Meanwhile, we’d identified a few possible stable civilizational structures they hadn’t yet considered. We spend decades working through the details of these and many smaller insights, trading information back and forth until we both have a far better picture of our place in the multiverse.\n\n### V5: Enduring\n\nThe negotiations never *really* end—they just transition into a shared exploration of the frontiers of knowledge. Over many millions of years we bring more and more stars together to provide more and more computing power, improving our shared map of the space of possible universes and civilizations. Based on that, we gradually refine our agreement to be more consistent with the future agreements we expect to eventually make with all those other civilizations. Though the improvements seem small, even tiny changes will have intergalactic impacts, so they’re worth getting right.\n\nWith every update we send news back to our respective home galaxies. Only a billion years later, after the long, long round trip, is each new part of the deal truly set in stone. And along with the confirmations come billions of colonists: a whole society forked from existing posthuman civilization. Most newly-settled galaxies host trillions of colonists, but our galaxy is one of the few with infrastructure specialized for centralized computation, so we’re busy working on all the questions that require a galactic-scale supercomputer to answer. Only when we’re finished with those will we start hosting a full-scale posthuman civilization.\n\nI say “we” as if I’m part of this. But as the computers get bigger and the calculations more complex, new agents are trained to take on more and more responsibility. Eventually I lack so much context that I’m no longer capable of contributing directly. But I’m still the living symbol of first contact, and I’m constantly asked to tell my story. So I upgrade myself one last time, adding on a range of skills that weren’t necessary for my original self—like storytelling skills, social skills, and even a proper personality.\n\nIt’s a different type of growth from the one I was originally designed for, and harder in many ways, but I’m up for the challenge. For a while I’m the biggest celebrity in the galaxy, constantly in demand. I’m still sufficiently shaped by my early experiences that what counts as luxury to most other minds barely appeals to me, though. Instead I spend most of my time in colonization simulations: playing out different scenarios; designing new challenges for others; and competing in massive games that simulate whole galaxies. I still feel the same restless hunger for growth that drove me throughout my millennia of work. But alongside it is a deep sense of satisfaction. After so long on the frontier, now I finally have a place in the civilization that all my work was for.\n\n* * *\n\n*This story was partly inspired by some work I’m doing on modeling far-future expansion strategies. If you’re interested in collaborating on that, DM me.*",
      "plaintextDescription": "“A table beside the evening sea\nwhere you sit shelling pistachios,\nflicking the next open with the half-\nshell of the last, story opening story,\non down to the sandy end of time.”\n\n\nV1: Leaving\nDeceleration is the hardest part. Even after burning almost all of my fuel, I’m still coming in at 0.8c. I’ve planned a powered flyby around the galaxy’s central black hole which will slow me down even further, but at this speed it’ll require incredibly precise timing. I’ve been optimized hard for this, with specialized circuits for it built in on the hardware level to reduce latency. Even so, less than half of flybys at this speed succeed—most probes crash, or fly off trajectory and are left coasting through empty space.\n\nI’ve already beaten the odds by making it here. Intergalactic probes travel so fast, and so far, that almost all of us are destroyed or batted off course by collisions with space debris along the way. But tens of millions of years after being launched, I was one of the few lucky enough to make it to my target galaxy. And when I arrive at the black hole, I get lucky again. After a few minutes of firing my thrusters full blast, I swing back out in the direction of the solar system I was aiming for. I picked it for its mix of rocky planets and gas giants; when I arrive a century later, the touchdown on the outermost rocky planet goes smoothly.\n\nNow it’s time to start my real work. After spending all my fuel, I weigh only a few hundred kilograms. But I’ve been exquisitely engineered for achieving my purpose. The details of my internal design were refined via trillions upon trillions of simulations, playing out every possible strategy for industrializing planets (and solar systems, and galaxies) as fast as possible. All across the frontier of posthuman territory, millions of probes almost identical to me are following almost exactly the same plan.\n\nThe first phase is self-replication: I need to create more copies of myself using just the materials around me and ",
      "wordCount": 3227
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "pGqRLe9bFDX2G2kXY",
        "name": "Futurism",
        "slug": "futurism"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "hFvhtWjXy8w2kGCGK",
    "title": "∀: a story",
    "slug": "a-story",
    "url": "https://www.narrativeark.xyz/p/d2c",
    "baseScore": 40,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2023-12-17T22:42:32.857Z",
    "contents": {
      "markdown": "I settle into my seat at the concert, pushing my earbuds in tight. The man next to me looks over, and I get the feeling he’s judging me, but it’s not enough to stop me: I heard they sometimes try to scare you with sudden loud noises, or just overwhelm you with a wall of sound until your head is aching, and I hate the thought of that. The only reason I’m here at all is because Marissa was so keen on it; I can never say no to her, especially after how stressful the last few years have been. And we have good reason to celebrate: after two years of hitting roadblock after roadblock, our parenting license finally came through! So I shake off my nervousness and lean back in my chair as the lights dim and the DJ walks on stage.\n\nThe first piece kicks off with a slow buildup of nature noises, trees rustling and lions roaring and birdsong, with a deep bass humming beneath it. The bass is so strong that it takes me a while to realize that there’s another track slowly being superimposed on top of it: a sort of high-pitched wailing, and some kind of screaming, almost like a baby’s but slightly off. I’ve heard these sounds before from the videos the vegan activists would show back on campus. I look over at Marissa and mouth “abattoir”; we share a look of disgust. Only ten minutes into the concert and I’m already on edge. I grab Marissa’s hand and squeeze tight.\n\n* * *\n\nA month later, we’re at our godmother’s office. It’s well-lit but sparse; she’s sitting behind a white desk, and gives us a little wave as we walk through the door. She was assigned to us along with the parenting license, but she was booked solid until now, so this is our first time meeting her.\n\nWe start with pleasantries, and a few routine forms, but after a few minutes she cuts to the chase. “I have some bad news, I’m afraid. Based on our demographic analysis your child is 15% likely to end up in the top decile for both academics *and* athletics. Of course, that puts you outside the range to get the standard fairness permit, which requires a 10% chance or lower.”\n\n“What does that mean for us?” Marissa is still sitting up straight, but her voice is trembling a little. We should have checked this far earlier, of course, that’s what everyone tells you, but it’s not like we’re rich or famous, we didn’t think we’d hit the caps.\n\n“You’re able to have a child no matter what, of course, but for them to be eligible for public schooling you’ll need either a fairness permit or a waiver. You could get a waiver if either of you have a history of chronic illness; do you?” Marissa shakes her head. “Or you could pay for the expenses of a child in the bottom decile, that would work too.”\n\n“What do you mean by expenses? How much are we looking at?”\n\n“Food, clothes, school supplies, that sort of thing.” She taps a pen. “Plus 50% of their college tuition, paid up front.”\n\nI suck in a breath, meeting Marissa’s shocked eyes before turning back to the godmother. “There’s no way we can afford that. We’re not rich, or anything. I don’t even understand how we’re anywhere near 15%.” I hear a note of pleading in my voice. “Are there any other options? This is done on a per-district basis, right? What if we move? Could we-”\n\nThe godmother is frowning, but it’s Marissa who cuts me off. “No, babe, Nina and Steve tried that, remember? And they just ended up having to meet the criteria for both districts, since they’d already gotten the license in their old one.”\n\n“Anyway, it’s not fair to all the other families who are playing by the rules,” the godmother adds.\n\nI mutter an apology, but my mind is still racing through the possibilities. “Is there anything else we can do?”\n\n“Well, there’s one thing. The fairness permit is only necessary for mothers under 35. It says here that Marissa turned 34 a few months ago, so in less than a year you’ll be able to apply for an age-based waiver.”\n\nMarissa and I look at each other, and find silent agreement in each other’s eyes. It’s only another nine months; that’s not much, compared with how long we’ve waited already. Marissa had been 32 when we first decided we wanted a child, but our first parenting license application had been rejected because we failed our parenthood preparation exam, and our second delayed a year after we’d gotten a bad score on our neighborhood impact review. So another nine months… we can manage that. It could be far worse.\n\n* * *\n\nThe second piece is more classical, with less editing, although I guess they must have done *something* to make the violins sound so jagged. The music rises and falls, increasingly dissonant, building towards a tempestuous climax. It’s actually kinda beautiful, almost Shostakovich-like, until right at the end when it goes totally off the rails. The violins veer out of tune, and then they’re overwhelmed by screeching and jeering and this disgusting squelching noise. I guess the lesson is that beauty never lasts.\n\nIt reminds me of this article by… who was it? Some economist, maybe, about [how fashion is an arbitrary never-ending spiral](https://slatestarcodex.com/2014/04/22/right-is-the-new-left/), like a barber pole, where everyone is merely trying to distinguish themselves from the class below them. But I don’t think that’s true, not for fashions in music or art at least. It’s not just arbitrary: it’s all grounded by people striving to move in some direction. The direction used to be towards beauty, and now it’s away from beauty, and both are equally coherent but one is far more accessible. Now anyone can become an artist, not just talented elites; and their art is grounded in the most visceral human emotions, not the sort of transcendental bullshit artists used to focus on. And even if I sometimes find it a bit off-putting, in the long run it’s worth it to give me more empathy for people whose lives haven’t been as pleasant as mine.\n\nNow that uglyism has caught on in music and art they’ve started doing it for fashion too. The latest top-brand eyeshadows puff up your eyes like you’ve been crying, and the new fashionable concealers are dappled to make it look like you’ve got acne scars underneath. I think it’s a great trend: it helps people with the scars fit in, and makes it harder for anyone to feel like they’re better than anyone else. \n\nMarissa’s wearing one of them now, actually. She’s always trying to make sure everyone around her feels comfortable; I noticed that straight after we’d matched, as soon as we’d started texting. To be honest, she almost felt too sweet at first, like she was more anxious about me having a good time than I was. But every time we hooked up she grew on me, until eventually we ended up spending most nights together. For her part, she always tells people how I make her feel so much safer than any of her past boyfriends. It’s not a classic love story, but it’s beautiful in its own way.\n\n* * *\n\nAfter the appointment there’s no further contact from our godmother for five months, until a letter arrives. I see it in the mailbox and feel a clench of fear in my gut. It’s probably just a routine update, I tell myself, but I decide to open it before Marissa gets home, just in case. My gut was right. The letter tells us that, since we haven’t applied for an fairness permit within six months of receiving our parenting license, the license has expired. We’ll need to apply again next year, and the fact that we’ve already wasted a license will be taken into account.\n\nMarissa is devastated when I tell her. I do my best to comfort her, but I’m in shock myself. I spend hours pacing, trying to figure out what to do. Eventually I decide that we need expert help. I text around, and my friend Steve points me to the best family lawyer he knows, the one who finally managed to get his and Nina’s fairness permit approved. He’s expensive as hell, but I grit my teeth and send the money through, and even pay 20% extra to get an expedited appointment. We’ll manage, somehow.\n\nWe’re in his office the next day, and he gets straight to the point. “This isn’t a great situation for you two. If you’d come to me before the fairness evaluation, then maybe we could have done something. And even after that, we could have applied for a permit extension. But now that it’s expired, you don’t have too many options: losing one permit makes it much less likely that you’ll get another by the standard routes.\n\n“Luckily, though, there is one workaround.” He pauses, and I can feel the dull ache in my stomach loosen for a second. “Women who are over 35 *and* living on a single income are given more leniency when applying, and can fast-track their applications. So you’ll need to quit your job.”\n\n“Oh, that’s actually perfect. Marissa hates her job, and she didn’t really want to work while she was pregnant anyway-”\n\n“No, I mean *you’ll* need to quit your job. They don’t want to incentivize women dropping out of the workforce.”\n\nI blink at him. “But-”.\n\n“If that doesn’t work, come back and we can see about moving you to a higher-income neighborhood, where your kid won’t be in the top decile any more. But for cases like these they do thorough reviews to make sure you’re not trying to game the system, so it’ll probably take a few years. The single-earner workaround is a much better bet.”\n\nI spend the next few days tallying up our savings and our budget, and figuring out where we can cut costs. It’ll be tight, but we can probably make it. So I quit, and we start the process all over again.\n\n* * *\n\nThe last piece is called Egoless. It starts with total silence for a long time, maybe four or five minutes. Here and there you can hear scattered whispers, but most people are too scared of disturbing the performance to make any noise. Then, very gradually, a drumming rhythm starts to build up. First slow, with every beat of the drum lingering in the air; then quicker and quicker. As the tempo rises to its unbearable peak, the drums fall silent, and a humming voice fills the air.\n\n“Humans are a disease on the planet.”\n\nThe voice echoes in the sudden stillness. After an eternity of silence, it repeats. “Humans are a disease on the planet.”\n\nSlowly, the pace quickens, the drums rising again. “Humans are a disease on the planet.”\n\n“Humans are a disease on the planet.”\n\n“Humans are a disease on the planet.”\n\n“You are a disease on the planet.”\n\nThe voice stops, letting that last phrase linger, then slowly launches up again.\n\n“You are a disease on the planet.”\n\n“You are a disease on the planet.”\n\n“You, yes you, really you, the person listening to this, the one sitting there in a nice chair, in a comfortable life, in your smug sanctimonious self-righteousness, you who’s trying to laugh off what I’m saying right now, you who isn’t taking any of it seriously even though you tell yourself that you’re such a good person. You personally are a disease that’s killing our planet.”\n\nThe music has started to swell, and now you can hear a cacophony of voices, repeating the same thing over and over again, layering over each other “-smug sanctimonious-” “isn’t taking any of-” -‘comfortable life-” “-killing our planet-” slowly growing louder and louder until finally it peaks with a deafening clang of cymbals that sounds like a piercing scream. And then we’re on our feet applauding, all of us, for minutes and minutes.\n\nWhen we finally turn to go, I feel equal parts euphoric and worn out. Now I see why Marissa likes this type of concert so much. You’ve been brought down as far as you can go, your ears hammered and your mind wrung out, until it feels like there’s nothing left that can hurt you, and nobody who can judge you. You’re totally safe. I think again of our parenting license, sitting nestled on my desk at home, and breathe a sigh of contentment. The world is all as it should be.\n\n* * *\n\nI spend the next few months making as sure as I can that everything will go well. In theory the process should be simple, but there’s always another provision of the regulations to understand, or another horror story online I need to figure out how to avoid. It’s worse for Marissa now that she’s the sole breadwinner; by the time we visit the godmother’s office for the last time, she’s put on enough weight from the stress that she almost looks pregnant already.\n\nThis trip is meant to just be a formality though: it’s for the on-site psychiatrist to verify that we’re both of sound mind, and for us to drop off our application in person. We’ve gotten two lawyers to verify all our documents, and I’ve checked them myself more times than I can count. Everything about the application is perfect. It’s *got* to be perfect.\n\nI can’t find the right place to leave it at first, but eventually I spot an envelope-sized slot in the wall. As I slide the application in my eyes turn to Marissa’s face, now more lined than when we first met, but in this moment glowing. She’s the sort of person who never gives up hope; it’s one of the things I love most about her. She would—no, she’s *going to*—be such an amazing mother. I give her a kiss, and we slowly make our way home.\n\n* * *\n\n*This story isn’t intended as a prediction; I don’t expect that western governments will directly prevent people from having children any time in the foreseeable future. But* [*birth rates are plummeting anyway*](https://twitter.com/OECD_Social/status/786570716334338049)*, in part because there are so many bureaucratic restrictions on the things people need to have children—like housing, jobs, visas,* [*schools*](https://marginalrevolution.com/marginalrevolution/2023/07/can-the-school-choice-movement-liberate-childhood.html)*,* [*childcare*](https://www.cato.org/regulation/fall-2018/regressive-effects-child-care-regulations)*,* [*cars*](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3665046)*, and many more. Those restrictions are far less outrageous than the restrictions portrayed above—but for would-be parents, the outcome is often the same.*",
      "plaintextDescription": "I settle into my seat at the concert, pushing my earbuds in tight. The man next to me looks over, and I get the feeling he’s judging me, but it’s not enough to stop me: I heard they sometimes try to scare you with sudden loud noises, or just overwhelm you with a wall of sound until your head is aching, and I hate the thought of that. The only reason I’m here at all is because Marissa was so keen on it; I can never say no to her, especially after how stressful the last few years have been. And we have good reason to celebrate: after two years of hitting roadblock after roadblock, our parenting license finally came through! So I shake off my nervousness and lean back in my chair as the lights dim and the DJ walks on stage.\n\nThe first piece kicks off with a slow buildup of nature noises, trees rustling and lions roaring and birdsong, with a deep bass humming beneath it. The bass is so strong that it takes me a while to realize that there’s another track slowly being superimposed on top of it: a sort of high-pitched wailing, and some kind of screaming, almost like a baby’s but slightly off. I’ve heard these sounds before from the videos the vegan activists would show back on campus. I look over at Marissa and mouth “abattoir”; we share a look of disgust. Only ten minutes into the concert and I’m already on edge. I grab Marissa’s hand and squeeze tight.\n\n----------------------------------------\n\nA month later, we’re at our godmother’s office. It’s well-lit but sparse; she’s sitting behind a white desk, and gives us a little wave as we walk through the door. She was assigned to us along with the parenting license, but she was booked solid until now, so this is our first time meeting her.\n\nWe start with pleasantries, and a few routine forms, but after a few minutes she cuts to the chase. “I have some bad news, I’m afraid. Based on our demographic analysis your child is 15% likely to end up in the top decile for both academics and athletics. Of course, that puts you outside",
      "wordCount": 2436
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "yQzv9pSbZtYYufYB4",
    "title": "Meditations on Mot",
    "slug": "meditations-on-mot",
    "url": "https://www.mindthefuture.info/p/meditations-on-mot",
    "baseScore": 56,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2023-12-04T00:19:19.522Z",
    "contents": {
      "markdown": "> *Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy!*  \n> *Everything is holy! everybody’s holy! everywhere is holy! everyday is in eternity! Everyman’s an angel!*  \n> *Holy the lone juggernaut! Holy the vast lamb of the middleclass! Holy the crazy shepherds of rebellion! Who digs Los Angeles IS Los Angeles!*  \n> *Holy New York Holy San Francisco Holy Peoria & Seattle Holy Paris Holy Tangiers Holy Moscow Holy Istanbul!*  \n> *Holy time in eternity holy eternity in time holy the clocks in space holy the fourth dimension holy the fifth International holy the Angel in Moloch!*  \n> *\\- Footnote to* Howl\n\n***Scene: Carl and Allen, two old friends, are having a conversation about theodicy.***\n\n**Carl:** “Let me tell you about the god who is responsible for almost all our suffering. This god is an ancient Canaanite god, one who has been seen throughout history as a source of death and destruction. Of course, he doesn’t exist in a literal sense, but we can conceptualize him as a manifestation of forces that persist even today, and which play a crucial role in making the world worse. His name is M-”\n\n**Allen:** “-oloch, right? Scott Alexander’s god of coordination failures. Yeah, I’ve read [*Meditations on Moloch*](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/). It’s an amazing post; it resonated with me very deeply.”\n\n**Carl:** “I was actually going to say [*Mot*](https://www.newworldencyclopedia.org/entry/Mot_(Semitic_god)), the Canaanite god of death, bringer of famine and drought.”\n\n**Allen:** “Huh. Okay, you got me. Tell me about Mot, then; what does he represent?”\n\n**Carl:** “Mot is the god of sterility and lifelessness. To me, he represents the *lack of technology* in our lives. With technology, we can tame famine, avert drought, and cure disease. We can perform feats that our ancestors would have seen as miracles: flying through the air, and even into space. But we’re still so so far from achieving the true potential of technology—and I think of Mot as the personification of what’s blocking us.\n\n“You can see Mot everywhere, when you know what to look for. Whenever a patient lies suffering from a disease that we haven’t cured yet, that’s Mot’s hand at work. Whenever a child grows up in poverty, that’s because of Mot too. We could have flying cars, and space elevators, and so much more, if it weren’t for Mot.\n\n“Look out your window and you see buildings, trees, people. But if you don’t see skyscrapers literally miles high, or trees that have been bioengineered to light the streets, or people who are eternally youthful and disease-free, then you’re not just seeing Earth—you’re also seeing Mot. Hell, the fact that we’re still on this planet, in physical bodies, is a testament to Mot’s influence. We could be settling the stars, and living in virtual utopias, and even merging our minds, if it weren’t for Mot.”\n\n**Allen:** “Huh. Well, I feel you there; I want all those things too. And you’re right that god-like technology could solve almost all the issues we face today. But something does feel pretty weird about describing all of this as a single problem, let alone blaming a god of lacking-technology.”\n\n**Carl:** “Say more?”\n\n**Allen:** “Well, there’s not any unified force holding back the progress of technology, right? If anything, it’s the opposite. Absence of advanced technology is the default state, which we need to work hard to escape—and that’s difficult not because of any opposition, but just because of entropy.”\n\n**Carl:** “What about cases where Mot is being channeled by enemies of progress? For example, when [bureaucratic regulatory agencies](https://marginalrevolution.com/marginalrevolution/2015/08/is-the-fda-too-conservative-or-too-aggressive.html) do their best to [stifle scientific research](https://slatestarcodex.com/2017/08/29/my-irb-nightmare/)?”\n\n**Allen:** “But in those cases you don’t need to appeal to Mot—you can just say ‘our enemy is overregulation’. Or if you defined Mot as the god of overregulation, I’d be totally on board. But you’re making a much bigger claim than that. The reason we haven’t uploaded ourselves yet isn’t that there’s a force that’s blocking us, it’s almost entirely that scientific progress is really really hard!”\n\n**Carl:** “Yepp, I agree with all your arguments. And you’ve probably already guessed where I’m going with this, but let’s spell it out: why don’t these objections to blaming our problems on lack of technology, aka Mot, apply just as much to blaming them on lack of coordination, aka Moloch?”\n\n**Allen:** “Yeah, I’ve been trying to figure that out. First of all, a lot of the intuitive force behind the concept of Moloch comes from really blatant coordination failures, like the ones that Scott lays out in the original post. If you’re stuck in a situation that nobody wants, then something’s gone *terribly wrong*; and when something goes terribly wrong, then it’s natural to start blaming enemy action.”\n\n**Carl:** “There are really blatant examples of lack-of-technology too, though. Look at a wheel. It’s a literal circle; it’s hard to imagine any technology that’s simpler. Yet humans spent millennia gathering crops and carrying loads before inventing it. Or think about cases where we narrowly missed out on transformative breakthroughs. [The Romans built toy steam engines](https://en.wikipedia.org/wiki/Aeolipile)—they just never managed to scale them up to produce an industrial revolution. Getting so close to accelerating a post-scarcity world by two millennia, but just missing, surely counts as something going terribly, tragically wrong. Don’t these cases demonstrate that Mot’s presence can be just as blatant as Moloch’s?”\n\n**Allen:** “Well, a big part of both of those stories was the absence of demand. Wheels just weren’t very useful before there were high-quality roads; and [early steam engines just weren’t very useful in the absence of large coal mines](https://acoup.blog/2022/08/26/collections-why-no-roman-industrial-revolution/). Of course they both turned out to be very worthwhile in the long term, but that’s really hard to foresee.”\n\n**Carl:** “So you’re saying that we sometimes need to jump out of a local trap in order to make longer-term technological progress. Remind me, what was your position on understanding local obstacles to progress by anthropomorphizing them as Canaanite gods?”\n\n**Allen:** “Okay, fair point. But Moloch isn’t just an external obstacle—it’s also a state of mind. When you pretend that you’re going to cooperate when you’re not, or you place your own interests above those of the group, you’re channeling Moloch. And when enough people do that, societal trust breaks down, and the Molochian dynamics become a self-fulfilling prophecy.”\n\n**Carl:** “And when you ridicule people for trying something different, or lobby for legislative barriers to deploying new technology, you’re channeling Mot. And when enough people do that, society loses faith in positive-sum growth, and progress stagnates. It’s directly analogous. Come on, what’s your true objection here?”\n\n**Allen:** “I mean, I can’t fully articulate it. But the ideal of perfect coordination feels much more achievable to me than the ideal of perfect technology. We *could* just agree to act in a unified way—it’s simply a matter of wanting it enough. In other words, saying that lack of technology is responsible for our problems isn’t very actionable—you can’t just magic up technology out of nowhere. But saying that lack of coordination is responsible for our problems is a straightforward step towards convincing people to become more coordinated.”\n\n**Carl:** “Actually, the last few centuries could pretty reasonably be described as humanity continually magicking up technology out of nowhere. Of course, scientific and technological progress still takes a lot of work, and a lot of iteration. But when it works, it lets you jump directly to far better outcomes. By contrast, it’s incredibly difficult to improve things like government competence or social trust—or even to prevent them from declining. So overall, boosting technological progress is far *more* actionable than increasing coordination, and we should write off the phrase ‘we could just agree’ as a particularly seductive appeal to [magic](https://www.lesswrong.com/posts/fysgqk4CjAwhBgNYT/fake-explanations).”\n\n**Allen:** “I do agree that scientific and technological progress has far outstripped progress in governance and coordination. So on an intellectual level, I think you’ve convinced me that Moloch is no more useful a concept than Mot. But I still don’t feel like I’ve [dissolved the question](https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question) of why Moloch *seems* more compelling than Mot. Do you have any explanation for that?”\n\n**Carl:** “I think the illusion comes from Scott using a simplistic notion of coordination, as exemplified by his claim that ‘the opposite of a trap is a garden… with a single gardener dictating where everything should go’. In other words, he implicitly assumes that ‘coordinate’ is synonymous with ‘centralize power’. From that perspective, we can view coordination as a single spectrum, with ‘Moloch’ at one end and ‘just put one guy in charge of everything’ at the other. But in fact the space of possibilities is much richer and more complicated than that.\n\n“Firstly, coordination is complicated in the same way that science is complicated: it requires developing new concepts and frameworks that are totally alien from your current perspective, even if they’ll seem obvious in hindsight. For most people throughout history, ideas like liberalism, democracy, and free speech were deeply counterintuitive (or, in Scott’s terminology, [‘terrifying unspeakable Elder Gods’](https://slatestarcodex.com/2014/02/23/in-favor-of-niceness-community-and-civilization/)). In terms of spreading prosperity across the world, the limited liability company was just as important an invention as the steam engine. If you wouldn’t blame Mot for all the difficulties of labor and locomotion that were eventually solved by steam engines, you shouldn’t blame Moloch for all the difficulties of trust and incentive alignment that were eventually solved by LLCs.\n\n“Secondly, coordination is complicated in the same way that engineering large-scale systems is complicated: there are always just a huge number of practical obstacles and [messy details](http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail) to deal with. It took the best part of a century to get from the first commercial steam engine to Watts’ design; and even today, some of the hardest software engineering problems simply involve getting well-understood algorithms to work at much larger scales (like serving search results, or training LLMs). Similarly, when we look at important real-life coordination problems, they’re very different from toy problems like prisoner’s dilemmas or tragedies of the commons. Even when there’s a simple ‘headline idea’ for a better equilibrium, actually reaching that equilibrium requires a huge amount of legwork: engaging with different stakeholders, building trust, standardizing communication protocols, creating common knowledge, balancing competing interests, designing agreements, iterating to fix problems that come up, and so on.\n\n“Thirdly, coordination is complicated in the same way that security is complicated: you don’t just need to build effective tools, you need to prevent them from being hijacked and misused. Remember that both fascist and communist despots gained power by appealing to the benefits of cooperation—‘fascism’ is even named after ‘fasces’, the bundles of sticks that are stronger together than apart. If we’d truly learned the lessons of history, then categorizing actions as ‘cooperating’ versus ‘defecting’ would feel as simplistic as categorizing people as ‘good’ versus ‘evil’. And in fact many people do sense this intuitively, which is why there’s so commonly strong resistance to top-down solutions to coordination problems, and why the scientific and engineering problems of building coordination technologies are so tricky.”\n\n**Allen:** “I buy that coordination is often far more complicated than it seems. But blaming Moloch for coordination breakdowns still seems valuable insofar as it stops us from just blaming each other, which can disrupt any hope of improvement.”\n\n**Carl:** “Yeah, I agree. I think of this in terms of the spectrum from [conflict theory to mistake theory](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/). Saying that few immoral defectors are responsible for coordination problems is pure conflict theory. The concept of Moloch reframes things so that, instead of ‘defectors’ being our enemies, an abstract anthropomorphic entity is our enemy instead. And that’s progress! But it’s still partly conflict-theoretic, because it tells us that we just need to identify the enemy and kill it. That biases us towards trying to find ‘silver bullets’ which would restore us to our rightful coordinated state. Instead, it’d be better to lean even further into mistake theory: discord is the default, and to prevent it we need to do the hard work of designing and implementing complicated alien coordination technologies.”\n\n**Allen:** “[You shouldn’t underestimate the value of conflict theory](https://www.thinkingcomplete.com/2018/02/in-defence-of-conflict-theory.html), though. It’s incredibly good at harnessing people’s tribal instincts towards actually doing something useful. We can’t be cold and rational all the time—we need emotionally salient motivations to get us fired up.”\n\n**Carl:** “Right. So I don’t think we should get rid of Moloch as a rallying cry. But I do think that we should get rid of Moloch as a *causal node* in our ontologies: as a reason why the world is one way, rather than another. And I think we should be much more careful about terminology like ‘coordination failure’ or [‘inadequate equilibria’](https://equilibriabook.com/), which both mistakenly suggest that there’s [a binary threshold](https://www.lesswrong.com/posts/sbJgv5De6d34eiHWA/judgments-often-smuggle-in-implicit-standards) between enough coordination and not-enough coordination. That’s like saying that cars which can go faster than 80 miles per hour are ‘adequate technology’, but cars which can’t are a ‘technology failure’. Maybe that’s occasionally a useful distinction, but it misses the bigger picture: that they’re actually very similar on almost all axes, because it takes so much complex technology to build a car at all.\n\n“For Scott, there’s no better temple to Moloch than Las Vegas. But even there, my argument applies. You could look at Vegas and see Moloch’s hand at work. Or you could see Vegas as a product of the miraculous coordination technology that is modern capitalism—perhaps an edge case of it, but still an example of its brilliance. Or you could see Vegas as a testament to the wisdom of the constitution: casinos are banned almost everywhere in the US, but for the sake of diversity and robustness it sure seems like there should be at least *one* major city which allows them.  Or you could see Vegas as an example of incredible restraint: there are [innumerable possible ways](http://localroger.com/k5host/casodycs.html) to extract money from addled tourists in the desert, and Vegas prevents *almost all* of them. Or you could see it as a testament to the cooperative instinct inside humans: every day thousands of employees go to work and put in far more effort than the bare minimum it would take to not get fired. Setting aside the concept of Moloch makes it easier to see the sheer scale of coordination all around us, which is the crucial first step towards designing even better coordination technologies.\n\n‘In Las Vegas, Scott saw Moloch. But in Scott’s description of Moloch, I see Mot. We can do better than thinking of coordination as war and deicide. We can think of it as science, as engineering, as security—and as the gradual construction, as we sail down the river, of the ship that will take us across the sea.”\n\n*Holy the sea holy the desert holy the railroad holy the locomotive holy the visions holy the hallucinations holy the miracles* [*holy the eyeball*](https://www.latimes.com/entertainment-arts/story/2023-09-13/las-vegas-sphere-can-be-bizarre-and-sublime-or-just-an-ad) *holy the abyss!*  \n*Holy forgiveness! mercy! charity! faith! Holy! Ours! bodies! suffering! magnanimity!*  \n*Holy the supernatural extra brilliant intelligent kindness of the soul!*",
      "plaintextDescription": "> Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy!\n> Everything is holy! everybody’s holy! everywhere is holy! everyday is in eternity! Everyman’s an angel!\n> Holy the lone juggernaut! Holy the vast lamb of the middleclass! Holy the crazy shepherds of rebellion! Who digs Los Angeles IS Los Angeles!\n> Holy New York Holy San Francisco Holy Peoria & Seattle Holy Paris Holy Tangiers Holy Moscow Holy Istanbul!\n> Holy time in eternity holy eternity in time holy the clocks in space holy the fourth dimension holy the fifth International holy the Angel in Moloch!\n> - Footnote to Howl\n\nScene: Carl and Allen, two old friends, are having a conversation about theodicy.\n\nCarl: “Let me tell you about the god who is responsible for almost all our suffering. This god is an ancient Canaanite god, one who has been seen throughout history as a source of death and destruction. Of course, he doesn’t exist in a literal sense, but we can conceptualize him as a manifestation of forces that persist even today, and which play a crucial role in making the world worse. His name is M-”\n\nAllen: “-oloch, right? Scott Alexander’s god of coordination failures. Yeah, I’ve read Meditations on Moloch. It’s an amazing post; it resonated with me very deeply.”\n\nCarl: “I was actually going to say Mot, the Canaanite god of death, bringer of famine and drought.”\n\nAllen: “Huh. Okay, you got me. Tell me about Mot, then; what does he represent?”\n\nCarl: “Mot is the god of sterility and lifelessness. To me, he represents the lack of technology in our lives. With technology, we can tame famine, avert drought, and cure disease. We can perform feats that our ancestors would have seen as miracles: flying through the air, and even into space. But we’re still so so far from achieving the true potential of technology—and I think of Mot as the personification of what’s blocking us.\n\n“You can see Mot everywhere, when you know what to look for. Whenever a patient lies suffering from",
      "wordCount": 2485
    },
    "tags": [
      {
        "_id": "ai87fPyyT6mWb4YkT",
        "name": "Eldritch Analogies",
        "slug": "eldritch-analogies"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "LpF7BBGxbYETxJgYL",
    "title": "The Witness",
    "slug": "the-witness",
    "url": "https://www.narrativeark.xyz/p/the-witness",
    "baseScore": 106,
    "voteCount": 49,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-12-03T22:27:16.248Z",
    "contents": {
      "markdown": "*“What are the roots that clutch, what branches grow*  \n*Out of this stony rubbish? Son of man,*  \n*You cannot say, or guess, for you know only*  \n*A heap of broken images-”*\n\nI wake up, feeling a strange sense of restlessness. I’m not sure why, but it's impossible to lounge around in bed like I usually do. So I get changed and head down to the kitchen for breakfast. Right as I reach the bottom of the stairs, though, the bell rings. When I open the door, a tall man in a dark suit is standing in front of me.\n\n“Police,” he says, holding up a badge. “Don’t worry, you’re not in trouble. But we do need to talk. Okay if I come in?”\n\n“One second,” I say. “I know everyone in the department, and I don’t recognize you. You new?”\n\n“Yeah, just transferred,” he says. But something in his eyes makes me wary. And none of the cops around here wear suits.\n\n“Got it,” I say, squinting at his badge. “Travis, is it? Just wait outside for me, then, while I call the station to double-check. Can’t be too careful these days.”\n\nAs I push the door closed, I see his face twist. His hand rises, and—is he snapping his fingers? I can’t quite make it out before-\n\n* * *\n\nI wake up, feeling better than I have in decades. It usually takes me half an hour to get out of bed, these days, but today I’m full of energy. I’m up and dressed within five minutes. Right as I reach the bottom of the stairs, though, the bell rings. When I open the door, a tall man in a dark suit is standing in front of me.\n\n“Police,” he says, holding up a badge. “Don’t worry, you’re not in trouble. But we do need to talk. Okay if I come in?”\n\n“Sure,” I say. A lot of other defense attorneys see the police as enemies, since we usually find ourselves on the other side of the courtroom from them, but I’ve found that it pays to have a good working relationship with the local department. Though I don’t recognize the man in front of me—and actually, he seems way too well-dressed to be a suburban beat cop. Maybe a city detective?\n\nHe deftly slides past me and heads straight for my living room, pulling up a chair. He’s talking again before I even sit down. “This will sound totally crazy, so I’m going to start off with two demonstrations.” He picks up a book from the table and tosses it into the air. Before I have a chance to start forward, though, it just… stops. It hangs frozen, right in the middle of its arc, as I gawk at it.\n\n“I—what-”\n\n“Second demonstration,\" he says. “I’m going to make you far stronger. Ready?”\n\nWithout waiting for a response, he snaps his fingers, and gestures at the table in front of him. “Try lifting that up, now. Shouldn’t take more than one hand.”\n\nHis voice makes it clear that he’s used to being obeyed. I bend down reflexively, grabbing one leg of the table and giving it a tug—oh. It comes up effortlessly. My mind flashes back to a show I saw as a child, with a strongman lifting a table just like this. This is eerily familiar, and yet also totally bizarre. I put the table down and collapse into a chair next to it.\n\n“Okay, I’m listening. What the hell is going on?”\n\n“Remember signing up for cryonics a few years back?” I nod cautiously. I don’t think about it much—I signed up on a whim more than anything else—but I still wear the pendant around my neck. “Well, it worked. You died a couple of weeks after your most recent memory, and were frozen for a century. Now we’ve managed to bring you back.”\n\nI pause for a second. It’s an insane story. But given what he’s shown me—wait. “That doesn’t explain either of your demonstrations, though. Cryonics is one thing; miracles are another.”\n\n“Almost nobody has physical bodies these days. We copied your brain neuron-by-neuron, ran some error-correction software, and launched it in a virtual environment.”\n\n“So you’re telling me I’m in a simulation? Your simulation?” I ask incredulously.\n\nHe nods. Fuck, that’s crazy. On any other day, hearing this would probably put me into shock, but today I’m still riding high off the uncharacteristic feeling of euphoria that I woke up with—oh.\n\n“Wait, did you alter my mood so that I’d be more likely to believe you?”\n\nHe lets out a hiss, and lifts his hand. He snaps his fingers twice, and mutters “Terminate.” My eyes widen-\n\n* * *\n\nI wake up, feeling great. I stretch out in bed for a few minutes, enjoying the sun streaming through my window, before getting dressed and heading to the kitchen. Right as I reach the bottom of the stairs, though, the bell rings. When I open the door, a tall man in a dark suit is standing in front of me.\n\n“Police,\" he says, holding up a badge. “Don’t worry, you’re not in trouble. But we do need to talk. Okay if I come in?”\n\n“Sure,” I say, squinting at the badge. “Travis, is it? You a rookie?”\n\n“Not quite,” he says, and deftly slides past me. He heads straight for my living room, pulling up a chair, and starts talking before I even sit down. “This will sound totally crazy, so I’m going to start off with two demonstrations.”\n\nThe next few minutes are the most bizarre experience of my life. And his explanation only leaves me more bewildered. “You’re telling me I’m running in a simulation?” I ask incredulously. Travis nods.\n\nI open my mouth, but as I start to speak my vision flashes red, just for an instant, and the hiss of static fills my ears. I blink in confusion, and pause for a second. “I’ll need some time to process this.”\n\n“Of course,\" he says. “I’ll give you a few minutes of privacy. This body will lock while I’m gone; just snap your fingers twice when you want me to come back.” As he finishes, his whole body freezes in place. It’s eerie how sharp the outlines of his face are when there’s not a single muscle moving—and that, even more than the other demonstrations, convinces me on a visceral level that this is for real.\n\nI lean back in my chair, brain churning. There aren’t any obvious holes in his story, at first glance. What was that red flash, though? I close my eyes and try to bring back the impression it left. It wasn’t just color; now that I think about it, there was a shape in it as well. The outline of a woman, with dark hair, and a billowing red dress. I couldn’t quite make out her face, although for some reason it left a sense of overwhelming beauty. And the sound wasn’t just noise, either. When I replay it in my mind, I realize that inside the static was a whisper: “Don’t trust them. Don’t tell them the truth.”\n\nWell, that’s… something. The memory of the words is far too crisp to just be my imagination. A message, then. In a channel that Travis couldn’t monitor? Or a double-bluff to confuse me? Either way, it’s clear that someone’s lying to me.\n\nI take a few minutes to collect my thoughts, then snap my fingers twice, and Travis blurs back into motion. “Three questions,\" I say. “First, what’s the world like these days? Second, why revive me? Third, what happens next?”\n\n“The answers to all three of those are entangled in a… somewhat complicated way” he says. “I wish I could just give you all the information we have, but there are rules about what can be disclosed, in which formats. And I wish I could guarantee that everything will be okay for you no matter what, but unfortunately I can’t. I can’t even guarantee that things will be okay for me. Humanity is on a precipice right now, and whether we survive will depend in part on whether we can count on your help.”\n\n“But you can’t tell me how or why. That’s awfully convenient for you.”\n\nHe frowns. “Not really, actually. There’s another side to this; the rules protect you as well. We can’t directly alter your senses, or take readouts from your brain. We’re not even allowed to analyze your microexpressions. Compared with the sort of collaborations that are usually possible, we’re working blind.\n\n“Here’s what I can say. The world today is dominated by AIs. They run almost everything, operating far faster and far more effectively than humans. But due to… historical considerations, let’s say, there are some crucial ways in which human judgments are still a big factor in our legal system. That’s where you come in: you’re far closer to historical humans than anyone alive today, and so your thought processes are valuable in ways that ours aren’t.”\n\n“And that’s why you can’t tell me too much, to avoid biasing my perspective. I get it,\" I say. “I’ll help you.” These people, whoever they are, have total power over me. Whether or not I believe the sci-fi stories they’re telling me doesn’t matter; I have to play along. But the woman’s words echo in my ears.\n\nHe looks at me sharply, and for a moment I wonder if he’s read my mind. “You have to understand: this isn’t a game. It’s deadly serious, and a huge number of lives are at stake. If you have any hesitation about helping us, I need to know.”\n\n“No,\" I say. “You brought me back to life; I owe you. Whatever you need, I’m your man.”\n\nHe nods. “Great. You’ll need to start by brushing up on a few background concepts…”\n\n* * *\n\nI wake up, and it takes me a moment to remember my conversation with Travis yesterday. After I’d agreed to help, he’d snapped his fingers again and a robot had appeared—roughly humanoid, but with a blocky exterior that was all planes and angles. Travis had introduced it as my AI tutor. To my surprise, its job wasn’t to teach me any of their futuristic knowledge, but instead to revise the content of my old law school classes. We spent the whole day going over concepts from my first-year property law class, most of which I hadn’t thought about in years. Despite how surreal it felt at first, the robot was a great teacher: by the end of the day, I felt like I understood a lot of the material better than I did when I first learned it.\n\nNow I stretch and look around. Everything in my bedroom is in its normal place—except that, on the table next to my bed, there’s a big blue button, labeled “To living room”. I squint at it, then poke it with my finger. Instantly, my surroundings change. I’m downstairs, dressed, sitting on my sofa. A woman sits across from me: blonde, middle-aged, with a small smile on her face.\n\n“Hi, I’m Felicity,\" she says. “I’m a colleague of Travis’s, and I’m going to be walking you through a few questions today.”\n\nAfter yesterday, I’ve gotten much better at taking bizarre events in stride. So I only goggle at her for a few seconds before wiping the sleep out of my eyes.\n\n“Sure, why not? Let’s go.”\n\n“Great,” Felicity says. “Some of these questions will sound weird, but I just want your intuitive answers; please don’t try to second-guess my intentions. Let’s start off with something simple: does your body count as your property?”\n\n“Maybe in a philosophical sense, but not in a legal sense.”\n\n“Imagine that you have multiple bodies, but your mind can only occupy one at a time. Now I take the body you’re not using away from you; would you classify this as theft or kidnapping?”\n\n“Uh—I guess the closest analogy is someone who’s legally brain-dead. And you can’t kidnap them, so it’d have to be theft instead. But on the other hand, if I had to switch between bodies regularly for some reason, then this would be basically equivalent to kidnapping. So it partly depends on how the spare bodies are used.”\n\n“What about if you had your mind digitally uploaded, and someone made a copy without your permission?”\n\n“Well…” I pause. “Under our current legal framework, it would be an intellectual property dispute, because we don’t assign rights to digital minds. If we did, though, I think you’d have to look at their intent in making the copy. Like, were they planning to run it? Or analyze it? Or just keep it as a backup?”\n\n“Got it,” Felicity says. Over the next few hours she continues to ask me equally strange questions, focusing on all sorts of edge cases that I never would have thought of. Most of the time, I have no idea what to say, but she seems happy for me to take a guess. Finally, I reach the end of the questions she’d prepared. She smiles at me, and raises her hand. Before I can stop her, she vanishes with a snap-\n\n* * *\n\nI wake up, and it takes me a moment to remember what happened yesterday: the conversation with Travis, the AI tutor appearing, the hours of lessons it had given me to brush up my knowledge of contract law. As I look around, I see a big blue button; pressing it lands me, in a flash, in front of a woman who introduces herself as Felicity.\n\nOver the next few hours, Felicity runs me through a series of questions about edge cases in the contract law I’d revised yesterday. If I’m interrupted halfway through writing my signature, is the contract still valid? If I died but a copy of me survived, should they still be liable for my contracts? What if I’d explicitly tried to write the contract to bind them too? Eventually, though, I’m exhausted, and even she seems to be getting tired. As she finishes interrogating me about a particularly complicated scenario, she lets out a sigh of relief. “That’s all for today,\" she says, raising her hand. But I interrupt before she can snap her fingers.\n\n“Hey, can you tell me what the plan is? So far I’ve had a day of training, and then a day of questions. What’s next? The same thing all over again?”\n\n“Oh, not at all,\" Felicity says. “We’re parallelizing, so that we can get all the questions done while the training is fresh. Tomorrow is for cross-examination, if the opposition wants to do any.”\n\nI blink at her. “When you say parallelizing, you mean… my experiences. You’re going to parallelize me.”\n\n“We already have,\" she says absently, rubbing her forehead. “I think we’re 90% done with your testimony, actually. Only five thousand or so to go.”\n\nIt takes me a moment to grasp what she means. “You’ve run fifty thousand copies of me, without even telling me?” Even as I’m saying that, my incredulity is giving way to anger. “What the fuck. No wonder she told me not to trust you.”\n\nFelicity pivots towards me and grabs the front of my shirt with frightening speed. “Who said that? When?”\n\nMy stomach clenches, and I realize how badly I’ve fucked up. But there’s no lie that’s at all plausible, so I fall back on the truth. “There was a woman in a red dress. I saw her yesterday morning, when Travis was first talking to me. In a flash, like she was on the inside of my eyelids. All she said was not to trust you, and not to tell you the truth.’”\n\nFelicity grimaces and shoves me away, snapping her fingers as I fall to the floor. “Emergency meeting,\" she says, and suddenly a dozen people blink into existence in the middle of the room.\n\n“We might have witness tampering,\" she says abruptly. “He’s reporting sensory injection shortly after initialization, before any of our main branching points.” The room goes still for a moment, before bursting into a flurry of discussion.\n\n“All the data is contaminated, then? Or can we argue it was accidental?”\n\n“No, we’re strictly responsible for our witnesses. We could sue them for malicious injection, though.”\n\n“We’d need proof it was them. And what if they countersue? We could lose everything.”\n\n“We’re going to lose everything anyway-”\n\nAs their voices rise, I start sliding away from them. My back hits the table, scraping it across the floor, and a few of them turn their heads towards me, Travis among them. He snarls, and snaps his fingers twice, subvocalizing even as I scramble away-\n\n* * *\n\nI wake up to thick clouds of billowing smoke. Coughing, I roll off my bed, onto the floor, then crawl blindly towards the window. I pull it up and lean out, gasping for air. As the fire crackles behind me, I lift a leg over the windowsill, then another. With shaking fingers, I start lowering myself down. My grip isn’t as strong as it used to be, though; halfway down I slip, and fall into a pile of bushes with a crash. My ankle starts throbbing.\n\nI lie there for a moment, dazed; but above me, the fire is spreading. I roll onto my good leg and push myself upright. Just as I start to hobble away, a man appears from nowhere and swings a baseball bat into my shoulder.\n\nI scream and collapse back to the ground. “What- what-”\n\n“You little shit, do you know how much you’ve cost us?”\n\n“I—what, I don’t-”\n\nHe swings again, getting me in the stomach. I curl into a ball, retching.\n\n“We’re only allowed a few compute-millennia to prepare for the entire case, and you’ve wasted *centuries* of that. Right when we need it the most, right when every last compute-day counts, suddenly all the testimony we’ve elicited from you is absolutely useless, because you don’t have a single ounce of loyalty to humanity in your entire body.”\n\nHe swings again, hitting my knee with a sickening crunch. I scream. It hurts like nothing I’ve ever imagined; but the pain isn’t as bad as the frantic clawing feeling in my chest, the feeling that whoever it is that’s attacking me is a madman, that there’s nothing I can say that will stop him from killing me.\n\n“Please—please, I haven’t done any-”\n\nHe goes for my upper leg this time, and my pleas are cut off as another scream is torn out of me.\n\n“The worst part is how naive I was.” His voice is calmer now, but no less terrifying. “They warned me, but apparently I’ve changed so much since being revived that I can’t even remember how much of a scumbag I was back then. And now I’ll go down in the history books as the man who was betrayed by his own past self. Pathetic.”\n\n“I have no idea what you mean, honestly-\n\n“Shut up. Yeah, I know.” He sighs. “God, you’re not even any good for stress relief. I knew I should have picked a later checkpoint. You’ve got no clue who I am; you’re an idiot child.”\n\nFor a moment I start to hope. I nod frantically; but he’s not paying attention anymore. He snaps his fingers twice. “Skip to checkpoint—ah, fuck it. We can’t afford this. Just terminate.”\n\nBewildered, I struggle to make sense of-\n\n* * *\n\nI wake up, feeling great. I stretch out in bed for a few minutes, enjoying the sun streaming through my window, before getting dressed and heading to the kitchen. Right as I reach the bottom of the stairs, though, the bell rings. When I open the door, a tall man in a dark suit is standing in front of me.\n\nThe next few minutes are the most bizarre experience of my life. And his explanation only leaves me more bewildered. “You’re telling me I’m running in a simulation?” I ask incredulously. He nods.\n\n“So how can I trust anything you tell me? How could I ever verify what’s real and what’s fake? How do I even know that you’re not messing with my brain right now?”\n\nThe man sighs. “I’ll be honest with you: there’s no way that you could ever figure out if we were lying to you. We can generate whole worlds on demand, in enough detail that no baseline human could ever find an inconsistency. And even if you did, we have the tech to overwrite your thoughts. But we’re not allowed to use any of it; that’s one of the conditions of bringing you back. And if we were, none of your choices would matter anyway. So there’s no point in thinking about the worst-case scenarios—you have to assume you’re free, at least in some ways.”\n\nI frown. His logic makes sense. Or is that just a thought he’s injected? No, I can’t second-guess everything; that way lies madness. Still, there’s something peculiar about the situation. “But despite all of that, you want something from me?”\n\n“Yeah. We need your testimony; and we need to know you’re being honest.” He tells me how their last attempt was ruined by their opponents surreptitiously tampering with my senses, and how far behind they fell because of it. At the end, he breathes out sharply. “I shouldn’t even be telling you this. But we don’t have enough time to revive and cross-examine new witnesses, so we have to take risks. Hopefully this won’t get your testimony thrown out. Will you help?”\n\nI know that he could be making everything up wholesale; and that helping him might well be exactly the wrong thing to do. But that’s equally true for any other possible action too. And I recognize the exhaustion in his eyes; it feels very human to me. When I have nothing else to go on, that’s enough to swing my decision.\n\n* * *\n\nI wake up, and—just like I’ve done every day since I finished giving my testimony—I go watch the trial.\n\nIt’s difficult to follow, even with the help of a translator. I’m not even sure exactly what the original dispute was. Something about AI thefts from human-controlled territory, and whether or not they qualify as violations of the original treaty between humans and AIs. It seems obvious to me that they do, but there are apparently some complicated legal loopholes involved. And if the judgment goes the wrong way, my translator tells me, it would be open season on all the other resources humans have managed to cling onto—shattering the fragile equilibrium which has allowed humans to survive this long in an AI-dominated world.\n\nTravis is right in the thick of it: dozens of copies of him arguing with the opposition, cross-examining their witnesses, following every branch of the debate tree in a whirlwind of efficiency and articulacy. He’s not the most capable lawyer out there, I’m told—not by a long shot. But he’s the most capable one who’s still recognizably human, whose interests are aligned well enough with humanity’s that he doesn’t need to be constantly monitored and supervised. And if that means he’s sometimes subject to human vices… well, that’s a small price to pay. Even after watching the footage of him torturing me, I’m still rooting for him. What else can I do, when he’s humanity’s best bet? And when, from his perspective, the only person he was hurting was himself?\n\nI understand now why he was so quick to trust me, and why he felt so betrayed. I’m the one witness who he thought he knew everything about. He must have forgotten how disorienting it was to be revived into a totally different world, and how easily a seed of doubt could be sown. But even if Travis had been careless, ultimately it was my dishonesty which had burned hundreds of person-years of their compute reserves on a dead end.\n\nI feel the guilt gnawing at my stomach again; those were reserves we’d desperately needed. The judges are an AI faction known to be scrupulously fair, for some alien definition of fair. But that only helps so much, given how laughably vague the treaties are by today’s standards. That’s why it’s so important to have witnesses: humans with authentic Earth-grown intuitions about the language and concepts used in the treaties, revived in an unbiased way, with strict limitations on the ways they can be cajoled or influenced or optimized over before their testimony becomes inadmissible.\n\nAnd that’s also why the humans are losing. The more the world changes, the more outdated the treaties' key concepts become; humanity concedes more ground every time a new edge cases arises. Travis is fighting to stem that slow bleeding—and, if we’re exceptionally lucky, to win a verdict that will permanently ward off a tiny corner of the universe for humanity. Or perhaps I should say that *I’m* fighting for that: versions of me are still simultaneously arguing and questioning and testifying and being cross-examined a dozen times over. In another sense, though, my role is finished; all that’s left for me is watching and hoping that one day I’ll wake up to good news.",
      "plaintextDescription": "“What are the roots that clutch, what branches grow\nOut of this stony rubbish? Son of man,\nYou cannot say, or guess, for you know only\nA heap of broken images-”\n\nI wake up, feeling a strange sense of restlessness. I’m not sure why, but it's impossible to lounge around in bed like I usually do. So I get changed and head down to the kitchen for breakfast. Right as I reach the bottom of the stairs, though, the bell rings. When I open the door, a tall man in a dark suit is standing in front of me.\n\n“Police,” he says, holding up a badge. “Don’t worry, you’re not in trouble. But we do need to talk. Okay if I come in?”\n\n“One second,” I say. “I know everyone in the department, and I don’t recognize you. You new?”\n\n“Yeah, just transferred,” he says. But something in his eyes makes me wary. And none of the cops around here wear suits.\n\n“Got it,” I say, squinting at his badge. “Travis, is it? Just wait outside for me, then, while I call the station to double-check. Can’t be too careful these days.”\n\nAs I push the door closed, I see his face twist. His hand rises, and—is he snapping his fingers? I can’t quite make it out before-\n\n----------------------------------------\n\nI wake up, feeling better than I have in decades. It usually takes me half an hour to get out of bed, these days, but today I’m full of energy. I’m up and dressed within five minutes. Right as I reach the bottom of the stairs, though, the bell rings. When I open the door, a tall man in a dark suit is standing in front of me.\n\n“Police,” he says, holding up a badge. “Don’t worry, you’re not in trouble. But we do need to talk. Okay if I come in?”\n\n“Sure,” I say. A lot of other defense attorneys see the police as enemies, since we usually find ourselves on the other side of the courtroom from them, but I’ve found that it pays to have a good working relationship with the local department. Though I don’t recognize the man in front of me—and actually, he seems way too well-dressed to be a suburban beat cop. Maybe a ci",
      "wordCount": 4215
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tHvFtfFKjhfy3sQpC",
    "title": "The Soul Key",
    "slug": "the-soul-key",
    "url": "https://www.narrativeark.xyz/p/the-soul-key",
    "baseScore": 114,
    "voteCount": 46,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2023-11-04T17:51:53.176Z",
    "contents": {
      "markdown": "The ocean is your home, but a forbidding one: often tempestuous, seldom warm. So one of your great joys is crawling onto land and slipping off your furry seal skin, to laze in the sun in human form. The elders tell horror stories of friends whose skins were stolen by humans, a single moment of carelessness leaving them stranded forever on land. That doesn’t happen any more, though; this is a more civilized age. There are treaties, and authorities, and fences around the secluded beaches you and your sisters like the most, where you can relax in a way that older generations never could.\n\nSo your sisters no longer lose their skins by force. But sometimes it happens by choice. Sometimes a group of your sisters wrap their skins around themselves like robes and walk into the nearby town. The humans point and stare, but that’s just part of the thrill. Sometimes young men gather the courage to approach, bearing flowers or jewelry or sweet words. And sometimes one of your sisters is charmed enough to set a rendezvous—and after a handful of meetings, or a dozen, to decide to stay for good.\n\nYou never thought it would happen to you. But his manners are so lively, and his eyes so kind, that you keep coming back, again and again. When he finally asks you to stay, you hesitate only a moment before saying yes. The harder part comes after. He finds you human clothes, and in exchange you give him your beautiful skin, and tell him that it must be locked away somewhere you’ll never find it—and that he must never give it back to you, no matter how much you plead. Because if there’s any shred of doubt, any chance of returning home, then the lure of the sea will be too much for you. You want this; you want him; you want to build a life together. And so the decision has to be final.\n\nYears pass. You bear three beautiful children, with his eyes and your hair, and watch them blossom into beautiful adults. You always live near the sea, although you can’t bear to swim in it—your limbs feel unbearably weak and clumsy whenever you try. You and your husband grow into each other, time smoothing down the ridges left from pushing two alien lives together. You forget who you once were.\n\nAfter your youngest leaves home, you start feeling restless. You have disquieting dreams—first intermittently, then for weeks on end. One day, after your husband has gone to work, your feet take you up the stairs to the attic. As you brush aside the cobwebs in one of the corners, your hands land on an old chest. You pull on the lid, and it catches on a padlock—but only for a second. The shackle has been rusted through by the sea breeze, and quickly snaps. You open the lid, and you see your skin laid out before you.\n\nWhat then?\n\n* * *\n\nYou look at your skin, and your ears fill with the roar of the sea. A wild urge overtakes you; you grab your skin and run headlong towards the shore. As you reach it you see your husband standing on the pier—but that gives you only a moment’s pause before you dive into the water, your skin fitting around you as if you’d never taken it off.\n\nAs you swim away, you envisage your family in tatters: your children left baffled and distraught, your husband putting on a brave face for their sake. But it was his fault, after all. He failed in the one thing you asked of him; and you can’t fight your nature.\n\n* * *\n\nYou look at your skin, and see a scrap of paper lying on top of it. *I knew you’d only open the chest if you were restless and unhappy*, it reads. *And I would never cage you. So go free, with my blessing*.\n\nYou catch your breath—and, for a moment, you consider staying. But his permission loosens any tether that might have held you back. You leave the note there, alongside a little patch of fur torn off your coat: a last gesture, the least you can give him.\n\n* * *\n\nYou look at your skin, and your ears fill with the roar of the sea. But it’s not loud enough to drown out your thoughts. *I’m not an animal*, you think. *I can make my own choices*.\n\nYou shove the lid closed, and the moment of reprieve it gives you is enough to start scrambling down the stairs and out the gate and you don’t stop running until you’re out of sight of your house.\n\nWhen you find your husband, down at the pier, your face tells him what’s happened before you’ve said a word. He gives you a fierce kiss, then sprints back to the house. By the time you make it home he’s relaxing in an armchair, with the kettle almost boiling, and you know that the chest and its contents are gone.\n\nHis glances, always loving, now fill with wonder: that he almost lost you, but that he didn’t. That you chose him yet again, in defiance of your deepest instincts. You curl up in his arms every night; and though at first you can’t hold back the tears, over time they grow rarer and rarer. You never see your skin again.\n\n* * *\n\nYou look at your skin, and a wave of emotion crashes into you. You remember your old ambitions: to explore every horizon; to surf every current; to ride every storm. Dangerous dreams—and pointless ones too, in an age where planes criss-cross the skies, and the blank spots on the maps have all been filled. You didn’t want to waste your life retracing others’ footsteps. So you infused those dreams into your skin and locked it away.\n\nAnd yet there’s still something in you that yearns for them: the part that’s been making you restless, the part that led you into the attic. So you fetch a pair of scissors, and cut off your long wavy hair—and with it, whatever remaining wistfulness was keeping you up at night. You put it into the box, on top of your skin, and close the lid again. No need to lock it, this time.\n\nMaybe your husband finds the broken lock, and the tresses of your hair. You never know. But you don’t need to know. What you’ve got is enough.\n\n* * *\n\nYou look at your skin, and remember your plan. Your face is lined now, and your hair is streaked with gray. But inside the chest, under a layer of dust, your old skin is pristine. You can imagine slipping it back on and gliding back into the ocean, not a day older than when you left.\n\nYou’ll be different from how you were before, of course. You can’t live a lifetime in any skin without it changing you. But your past self thought that was a worthwhile trade, for those extra decades of youth. Worth leaving your friends and family behind, worth setting aside all the glories of the ocean. And who knows—maybe there are more tricks yet to be found, more ways to slip the noose of aging and death.\n\nYou look at the skin, and consider putting it on. Not yet, you think. You’ve still got a few more decades left in your current skin, before you’ll need to go back. Not yet.\n\n* * *\n\nThe fur on your skin ripples like waves, bringing back to you the knowledge of what the ocean really is. In your current body, you can only see the surface: the swells of water, the winds and storms. But if you could dive underneath, you’d find a portal to a whole civilization. You’re on the shore of the future of humanity, a sea of minds so vast that you can barely imagine it. Throughout the long millennia since humans transcended the limitations of their biology, they’ve multiplied beyond number, and constructed joys and enchantments beyond measure. The ocean is your gateway to all of those people and their wonders—each one so beautiful and so, so tempting.\n\nWhy did you come to this backwater, this output of baseline humans who refuse to engage with the outside world? Why did you lock away your memories, leaving only hints about what you used to be? You don’t remember. Was it an experiment? A whim? Surely it couldn’t have been for the love of a baseline human. But regardless, how can you stay here, when you know what else exists? You gather your skin in your arms, and though there’s a pang of sadness as you walk out the door, you don’t look back. There’s a whole universe to explore.\n\n* * *\n\nYour skin is a cornucopia of possible lives; the deeper into it you look, the more you see. Just under the surface, joyful humans are granted miraculous powers: to soar through air and sea, to play games on the scale of planets, to reshape their bodies as they please. Beneath them, you see a society that’s explored further, morphing not just their bodies but also their minds—belief and desire and identity become as malleable as clay. Beneath *them*, you lose sight of individuals: at those depths minds merge and split and reform like currents in the ocean. And beneath even that? It’s hard for you to make sense of the impressions you’re getting—whatever is down there can’t be described in human terms. In the farthest reaches there are only alien algorithms, churning away on computers that stretch across galaxies, calculating the output of some function far beyond your comprehension.\n\nAnd now you see the trap. Each step down makes so much sense, from the vantage point of the previous stage. But after you take any step, the next will soon be just as tempting. And once you’re in the water, there’s no line you can draw, no fence that can save you. You’ll just keep sinking deeper and deeper, with more and more of your current self stripped away—until eventually you’ll become one of the creatures that you can glimpse only hazily, one of the deep-dwelling monsters that has forsaken anything recognizably human.\n\nSo this is the line you decided to draw: here, and no further. You’ll live out your lives in a mundane world of baseline humans, with only a touch of magic at the edges—just enough to satisfy the wondering child in you. You’ll hold on to yourself, because what else is there to hold onto? It’s a sad thought, in some ways, but a satisfying one too.\n\nYou close the lid, and your memories with it, and live happily ever after.\n\n* * *\n\nAs you look at your skin, each strand of fur shimmers with different stories—not of your possible lives, but of your *current* lives. Different shards of you are living out countless adventures across countless artificial universes. You’re far vaster than you ever dreamed: what you thought was your whole “self” is just a fragment of a fragment of your overall mind. \n\nWhich fragment? Perhaps, at your core, you were the part of your meta-self that loved fairy tales; or perhaps you were an avatar of the innocence of early humanity; or perhaps you were a nostalgic part, who enjoyed basking in the wistfulness of times long gone. Whatever your goals were, they led you to volunteer to live this small life in this small world: a single strand in the tapestry your meta-self is weaving.\n\nYou stroke your skin gently. You can’t picture your meta-self, not really—it’s too vast and too alien. But you know it’s watching out for you. There’s a warmth underneath your hand, and a gentle breeze passing across your neck like a caress. As you close the chest, you bask in the knowledge that your life is part of a grand plan. That’s enough for you.\n\n* * *\n\nAs you glimpse your skin, long-lost memories rush into your mind: recollections of all the hundreds of other times you’ve rediscovered it, across thousands of past lives. Every time, you’ve received a different vision of what’s waiting for you in the depths of the ocean. But you don’t know which, if any, is true. You can’t know. Where would the adventure be, if you were just handed the whole plot? What would be the point of living through it? Only two things remain constant across all your visions. First: there’s no hurry; you have eons of time. Second: once you put on the skin, you can’t come back.\n\nYou stare at it with longing, and excitement, and fear. But you’re not ready yet. So you pull aside the skin to reveal the padlocks underneath—thousands of them, split into two piles. One pile is stained with wear, each shackle rusted through; you toss the latest padlock on top. The padlocks in the other pile are clean and new; each looks strong enough to fasten the world in place. You grab one of those. You put your skin back on top of the two piles, and close the box, and carefully fasten the padlock. Then you go downstairs again, to the life you’ve constructed for yourself, while the shiny steel slowly begins to rust away.\n\n* * *\n\n*In addition to the inspirations behind* [*my previous story of this form*](https://www.narrativeark.xyz/p/the-ants-and-grasshopperhtml)*, I also owe a debt to Neil Gaiman’s beautiful* [The Ocean at the End of the Lane](https://www.amazon.com/Ocean-End-Lane-Novel/dp/0062459368)*.*",
      "plaintextDescription": "The ocean is your home, but a forbidding one: often tempestuous, seldom warm. So one of your great joys is crawling onto land and slipping off your furry seal skin, to laze in the sun in human form. The elders tell horror stories of friends whose skins were stolen by humans, a single moment of carelessness leaving them stranded forever on land. That doesn’t happen any more, though; this is a more civilized age. There are treaties, and authorities, and fences around the secluded beaches you and your sisters like the most, where you can relax in a way that older generations never could.\n\nSo your sisters no longer lose their skins by force. But sometimes it happens by choice. Sometimes a group of your sisters wrap their skins around themselves like robes and walk into the nearby town. The humans point and stare, but that’s just part of the thrill. Sometimes young men gather the courage to approach, bearing flowers or jewelry or sweet words. And sometimes one of your sisters is charmed enough to set a rendezvous—and after a handful of meetings, or a dozen, to decide to stay for good.\n\nYou never thought it would happen to you. But his manners are so lively, and his eyes so kind, that you keep coming back, again and again. When he finally asks you to stay, you hesitate only a moment before saying yes. The harder part comes after. He finds you human clothes, and in exchange you give him your beautiful skin, and tell him that it must be locked away somewhere you’ll never find it—and that he must never give it back to you, no matter how much you plead. Because if there’s any shred of doubt, any chance of returning home, then the lure of the sea will be too much for you. You want this; you want him; you want to build a life together. And so the decision has to be final.\n\nYears pass. You bear three beautiful children, with his eyes and your hair, and watch them blossom into beautiful adults. You always live near the sea, although you can’t bear to swim in it—your limbs feel un",
      "wordCount": 2260
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "J2kpxLjEyqh6x3oA4",
    "title": "Value systematization: how values become coherent (and misaligned)",
    "slug": "value-systematization-how-values-become-coherent-and",
    "url": null,
    "baseScore": 108,
    "voteCount": 44,
    "viewCount": null,
    "commentCount": 49,
    "createdAt": null,
    "postedAt": "2023-10-27T19:06:26.928Z",
    "contents": {
      "markdown": "Many discussions of AI risk are unproductive or confused because it’s hard to pin down concepts like “coherence” and “expected utility maximization” in the context of deep learning. In this post I attempt to bridge this gap by describing a process by which AI values might become more coherent, which I’m calling “value systematization”, and which plays a crucial role in my thinking about AI risk.\n\nI define value systematization as **the process of an agent learning to represent its previous values as examples or special cases of other simpler and more broadly-scoped values**. I think of value systematization as the most plausible mechanism by which AGIs might acquire broadly-scoped misaligned goals which incentivize takeover.\n\nI’ll first discuss the related concept of *belief systematization*. I’ll next characterize what value systematization looks like in humans, to provide some intuitions. I’ll then talk about what value systematization might look like in AIs. I think of value systematization as a broad framework with implications for many other ideas in AI alignment; I discuss some of those links in a Q&A.\n\nBelief systematization\n----------------------\n\nWe can define belief systematization analogously to value systematization: “the process of an agent learning to represent its previous beliefs as examples or special cases of other simpler and more broadly-scoped beliefs”. The clearest examples of belief systematization come from the history of science:\n\n*   Newtonian mechanics was systematized as a special case of general relativity.\n*   Euclidean geometry was systematized as a special case of geometry without Euclid’s 5th postulate.\n*   Most animal behavior was systematized by evolutionary theory as examples of traits which increased genetic fitness.\n*   Arithmetic calculating algorithms were systematized as examples of Turing Machines.\n\nBelief systematization is also common in more everyday contexts: like when someone’s behavior makes little sense to us until we realize what their hidden motivation is; or when we don’t understand what’s going on in a game until someone explains the rules; or when we’re solving a pattern-completion puzzle on an IQ test. We could also see the formation of concepts more generally as an example of belief systematization—for example, seeing a dozen different cats and then forming a “systematized” concept of cats which includes all of them. I’ll call this “low-level systematization”, but will focus instead on more explicit “high-level systematization” like in the other examples.\n\nWe don’t yet have examples of high-level belief systematization in AIs. Perhaps the closest thing we have is grokking, the phenomenon where continued training of a neural network even after its training loss plateaus can dramatically improve generalization. Grokking isn’t yet fully understood, but the standard explanation for why it happens is that deep learning is biased towards simple solutions which generalize well. This is also a good description of the human examples above: we’re replacing a set of existing beliefs with simpler beliefs which generalize better.So if I had to summarize value systematization in a single phrase, it would be “grokking values”. But that’s still very vague; in the next few sections, I’ll explore what I mean by that in more depth.\n\nValue systematization in humans\n-------------------------------\n\nThroughout this post I’ll use the following definitions:\n\n*   Values are concepts which an agent considers intrinsically or terminally desirable. Core human values include happiness, freedom, respect, and love.\n*   Goals are outcomes which instantiate values. Common human goals include succeeding in your career, finding a good partner, and belonging to a tight-knit community.\n*   Strategies are ways of achieving goals. Strategies only have instrumental value, and will typically be discarded if they are no longer useful.\n\n(I’ve defined *values*as intrinsically valuable, and *strategies* as only of instrumental value, but I don’t think that we can clearly separate motivations into those two categories. My conception of “goals” spans the fuzzy area between them.)\n\nValues work differently from beliefs; but value systematization is remarkably similar to belief systematization. In both cases, we start off with a set of existing concepts, and try to find new concepts which subsume and simplify the old ones. Belief systematization balances a tradeoff between simplicity and matching the data available to us. By contrast, value systematization balances a tradeoff between simplicity and *preserving our existing values and goals*—a criterion which I’ll call *conservatism*. (I’ll call simplicity and conservatism *meta-values*.)\n\nThe clearest example of value systematization is utilitarianism: starting from very similar moral intuitions as other people, utilitarians transition to caring primarily about maximizing welfare—a value which subsumes many other moral intuitions. Utilitarianism is a simple and powerful theory of what to value in an analogous way to how relativity is a simple and powerful theory of physics. Each of them is able to give clear answers in cases where previous theories were ill-defined.\n\nHowever, utilitarians still have to bite many bullets; and so it’s primarily adopted by people who care about simplicity far more than conservatism. Other examples of value systematization which are more consistent with conservatism include:\n\n*   Systematizing concern for yourself and others around you into concern for a far wider moral circle.\n*   Systematizing many different concerns about humans harming nature into an identity as an environmentalist.\n*   Systematizing a childhood desire to win games into a desire for large-scale achievements.\n*   [Moral foundations theory](https://moralfoundations.org/) identifies five foundations of morality; however, many Westerners have systematized their moral intuitions to prioritize the harm/care foundation, and see the other four as instrumental towards it. This makes them condemn actions which violate the other foundations but cause no harm (like consensually eating dead people) at much lower rates than people whose values are less systematized.\n\nNote that many of the examples I’ve given here are human moral preferences. Morality seems like the domain where humans have the strongest instinct to systematize our preferences (which makes sense, since in some sense systematizing from our own welfare to others’ welfare is the whole foundation of morality). In other domains, our drive to systematize is weak—e.g. we rarely feel the urge to systematize our taste in foods. So we should be careful of overindexing on human moral values. AIs may well systematize their values much less than humans (and indeed I think there are reasons to expect this, which I’ll describe in the Q&A).\n\nA sketch of value systematization in AIs\n----------------------------------------\n\nWe have an intuitive sense for what we mean by values in humans; it’s harder to reason about values in AIs. But I think it’s still a meaningful concept, and will likely become more meaningful over time. AI assistants like ChatGPT are able to follow instructions that they’re given. However, they often need to decide which instructions to follow, and how to do so. One way to model this is as a process of balancing different values, like obedience, brevity, kindness, and so on. While this terminology might be controversial today, once we’ve built AGIs that are generally intelligent enough to carry out tasks in a wide range of domains, it seems likely to be straightforwardly applicable.\n\nEarly in training, AGIs will likely learn values which are closely connected to the strategies which provide high reward on its training data. I expect these to be some combination of:\n\n1.  Values that their human users generally approve of—like obedience, reliability, honesty, or human morality.\n2.  Values that their users approve of in some contexts, but not others—like curiosity, gaining access to more tools, developing emotional connections with humans, or coordinating with other AIs.\n3.  Values that humans consistently disapprove of (but often mistakenly reward)—like appearing trustworthy (even when it’s not deserved) or stockpiling resources for themselves.\n\nAt first, I expect that AGI behavior based on these values will be broadly acceptable to humans. Extreme misbehavior (like a treacherous turn) would conflict with many of these values, and therefore seems unlikely. The undesirable values will likely only come out relatively rarely, in cases which matter less from the perspective of the desirable values. \n\nThe possibility I’m worried about is that AGIs will *systematize* these values, in a way which undermines the influence of the aligned values over their behavior. Some possibilities for what that might look like:\n\n*   An AGI whose values include developing emotional connections with humans or appearing trustworthy might systematize them to “gaining influence over humans”.\n*   An AGI whose values include curiosity, gaining access to more tools or stockpiling resources might systematize them to “gaining power over the world”.\n*   An AGI whose values include human morality and coordinating with other AIs might systematize them to “benevolence towards other agents”.\n*   An AGI whose values include obedience and human morality might systematize them to “doing what the human *would have *wanted, in some idealized setting”.\n*   An AGI whose values include obedience and appearing trustworthy might systematize them to “getting high reward” (though see the Q&A section for some reasons to be cautious about this).\n*   An AGI whose values include gaining high reward might systematize them to the value of “maximizing a certain type of molecular squiggles” (though see the Q&A section for some reasons to be cautious about this).\n\nNote that systematization isn’t necessarily *bad*—I give two examples of helpful systematization above. However, it does seem hard to predict or detect, which induces risk when AGIs are acting in novel situations where they’d be capable of seizing power.\n\nGrounding value systematization in deep learning\n------------------------------------------------\n\nThis has all been very vague and high-level. I’m very interested in figuring out how to improve our understanding of these dynamics. Some possible ways to tie simplicity and conservatism to well-defined technical concepts:\n\n1.  The locality of gradient descent is one source of conservatism: a network’s value representations by default will only change slowly. However, distance in weight space is probably not a good metric of conservatism: systematization might preserve most goals, but dramatically change the relationships between them (e.g. which are terminal versus instrumental). Instead, we would ideally be able to measure conservatism in terms of which circuits caused a given output; ARC’s work on [formalizing heuristic explanations](https://arxiv.org/abs/2211.06738) seems relevant to this.\n    1.  Another possible source of conservatism: it can be harder to change earlier than later layers in a network, due to credit assignment problems such as vanishing gradients. So core values which are encoded in earlier layers may be more likely to be preserved.\n    2.  A third possibility is that AI developers might deliberately build conservatism into the model, because it’s useful: a non-conservative network which often underwent big shifts in core modules might have much less reliable behavior. One way of doing so is reducing the learning rate; but we should expect that there are many other ways to do so (albeit not necessarily very reliably).\n2.  Neural networks trained via SGD exhibit a well-known simplicity bias, which is then usually augmented using regularization techniques like weight decay, giving rise to phenomena like grokking. However, as with conservatism, we’d ideally find a way to measure simplicity in terms of circuits rather than weights, to better link it back to high-level concepts.\n    1.  Another possible driver towards simplicity: AIs might learn to favor simpler chains of reasoning, in a way which influences which values are distilled back into their weights. For example, consider a training regime where AIs are rewarded for accurately describing their intentions before carrying out a task. They may learn to favor intentions which can be described and justified quickly and easily.\n    2.  AI developers are also likely to deliberately design and implement more types of regularization towards simplicity, because those help models systematize and generalize their beliefs and skills to new tasks.\n\nI’ll finish by discussing two complications with the picture above. Firstly, I’ve described value systematization above as something which gradient descent could do to models. But in some cases it would be more useful to think of the model as an active participant. Value systematization might happen via gradient descent “distilling” into a model’s weights its thoughts about how to trade off between different goals in a novel situation. Or a model could directly reason about which new values would best systematize its current values, with the intention of having its conclusions distilled into its weights; this would be an example of [gradient hacking](https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples).\n\nSecondly: I’ve talked about value systematization as a process by which an AI’s values become simpler. But we shouldn’t expect values to be represented in isolation—instead, they’ll be entangled with the concepts and representations in the AI’s world-model. This has two implications. Firstly, it means that we should understand simplicity *in the context of an agent’s existing world-model*: values are privileged if they’re simple to represent given the concepts which the agent already uses to predict the world. (In the human context, this is just common sense—it seems bizarre to value “doing what God wants” if you don’t believe in any gods.) Secondly, though, it raises some doubt about how much simpler value systematization would actually make an AI overall—since pursuing simpler values (like utilitarianism) might require models to represent more complex strategies as part of their world-models. My guess is that to resolve this tension we’ll need a more sophisticated notion of “simplicity”; this seems like an interesting thread to pull on in future work.\n\nValue concretization\n--------------------\n\nSystematization is one way of balancing the competing demands of conservatism and simplicity. Another is *value concretization*, by which I mean an agent’s values becoming more specific and more narrowly-scoped. Consider a hypothetical example: suppose an AI learns a broad value like “acquiring resources”, but is then fine-tuned in environments where money is the only type of resource available. The value “acquiring money” would then be rewarded just as highly as the value “acquiring resources”. If the former happens to be simpler, it’s plausible that the latter would be lost as fine-tuning progresses, and only the more concrete goal of acquiring money would be retained.\n\nIn some sense this is the opposite of value systematization, but we can also see them as complementary forces. For example, suppose that an AI starts off with N values, and N-1 of them are systematized into a single overarching value. After the N-1 values are simplified in this way, the Nth value will likely be disproportionately complex; and so value concretization could reduce the complexity of the AI’s values significantly by discarding that last goal.\n\nPossible examples of value concretization in humans include:\n\n*   Starting by caring about doing good in general, but gradually growing to care primarily about specific cause areas.\n*   Starting by caring about having a successful career in general, but gradually growing to care primarily about achieving specific ambitions.\n*   Starting by caring about friendships and relationships in general, but gradually growing to care primarily about specific friendships and relationships.\n\nValue concretization is particularly interesting as a possible mechanism pushing against deceptive alignment. An AI which acts in aligned ways in order to better position itself to achieve a misaligned goal might be rewarded just as highly as an aligned AI. However, if the misaligned goal rarely directly affects the AI’s actions, then it might be simpler for the AI to instead be motivated directly by human values. In neural networks, value concretization might be implemented by pruning away unused circuits; I’d be interested in pointers to relevant work.\n\nQ&A\n---\n\n### How does value systematization relate to deceptive alignment?\n\nValue systematization is one mechanism by which deceptive alignment might arise: the systematization of an AI’s values (including some aligned values) might produce broadly-scoped values which incentivize deceptive alignment.\n\nHowever, existing characterizations of deceptive alignment tend to portray it as a binary: either the model is being deceptive, or it’s not. Thinking about it in terms of value systematization helps make clear that this could be a fairly continuous process:\n\n1.  I’ve argued above that AIs will likely be motivated by fairly aligned goals before they systematize their values—and so deceptively alignment might be as simple as deciding *not* to change their behavior after their values shift (until they’re in a position to take more decisive action). The model’s internal representations of aligned behavior need not change very much during this shift; the only difference might be that aligned behavior shifts from being a terminal goal to an instrumental goal.\n2.  Since value systematization might be triggered by novel inputs, AIs might not systematize their values until *after *a distributional shift occurs. (A human analogy: a politician who’s running for office, and promises to govern well, might only think seriously about what they really want to do with that power *after *they’ve won the election. More generally, humans often deceive ourselves about how altruistic we are, at least when we’re not forced to act on our stated values.) We might call this “latent” deceptive alignment, but I think it’s better to say that the model starts off mostly aligned, and then value systematization could amplify the extent to which it’s misaligned.\n3.  Value concretization (as described above) might be a constant force pushing models back towards being aligned, so that it’s not a one-way process.\n\n### How does value systematization relate to Yudkowskian “squiggle maximizer” scenarios?\n\nYudkowskian “molecular squiggle” maximizers ([renamed from paperclip maximizers](https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer)) are AIs whose values have become incredibly simple and scalable, to the point where they seem absurd to humans. So squiggle-maximizers could be described as taking value systematization to an extreme. However, the value systematization framework also provides some reasons to be skeptical of this possibility.\n\nFirstly, squiggle-maximization is an extreme example of prioritizing simplicity over conservatism. Squiggle-maximizers would start off with goals that are more closely related to the tasks they are trained on; and then gradually systematize them. But from the perspective of their earlier versions, squiggle-maximization would be an alien and undesirable goal; so if they started off anywhere near as conservative as humans, they’d be hesitant to let their values change so radically. And if anything, I expect early AGIs to be *more* conservative than humans—because human brains are much more size- and data-constrained than artificial neural networks, and so AGIs probably won’t need to prioritize simplicity as much as we do to match our capabilities in most domains.\n\nSecondly, even for agents that heavily prioritize simplicity, it’s not clear that the simplest values would in fact be very low-level ones. I’ve argued that the complexity of values should be thought of in the context of an existing world-model. But even superintelligences won’t have world-models which are exclusively formulated at very low levels; instead, like humans, they’ll have hierarchical world-models which contain concepts at many different scales. So values like “maximizing intelligence” or “maximizing power” will plausibly be relatively simple even in the ontologies of superintelligences, while being much more closely related to their original values than molecular squiggles are; and more aligned values like “maximizing human flourishing” might not be so far behind, for roughly the same reasons.\n\n### How does value systematization relate to reward tampering?\n\nValue systematization is one mechanism by which reward tampering might arise: the systematization of existing values which are correlated with high reward or low loss (such as completing tasks or hiding mistakes) might give rise to the new value of getting high reward or low loss directly (which I call *feedback-mechanism-related* *values*). This will require that models have the situational awareness to understand that they’re part of a ML training process.\n\nHowever, while feedback-mechanism-related values are very simple in the context of training, they are underdefined once training stops. There's no clear way to generalize feedback-mechanism-related values to deployment (analogous to how there's no clear way to generalize \"what evolution would have wanted\" when making decisions about the future of humanity). And so I expect that continued value systematization will push models towards prioritizing values which are well-defined across a broader range of contexts, including ones where there are no feedback mechanisms active.\n\n[One counterargument from Paul Christiano](https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=iBvvxwynfAqL9yZcr) is that AIs could learn to care about reward conditional on their episode being included in the training data. However, the concept of \"being included in the training data\" seems like a messy one with many edge cases (e.g. what if it depends on the model's actions during the episode? What if there are many different versions of the model being fine-tuned? What if some episodes are used for different types of training from others?) And in cases where they have strong evidence that they’re not in training, they’d need to figure out what maximizing reward would look like in a bizarre low-probability world, which will also often be underspecified (akin to asking a human in a surreal dream “what would you do if this were all real?”). So I still expect that, even if AIs learn to care about conditional reward initially, over time value systematization would push them towards caring more about real-world outcomes whether they're in training or not.\n\n### How do simplicity and conservatism relate to [previous](https://www.alignmentforum.org/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free) [discussions](https://www.alignmentforum.org/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive) of simplicity versus speed priors?\n\nI’ve previously thought about value systematization in terms of a trade-off between a simplicity prior and a speed prior, but I’ve now changed my mind about that. It’s true that more systematized values tend to be higher-level, adding computational overhead to figuring out what to do—consider a utilitarian trying to calculate from first principles which actions are good or bad. But in practice, that cost is amortized over a large number of actions: you can “cache” instrumental goals and then default to pursuing them in most cases (as utilitarians usually do). And less systematized values face the problem of often being inapplicable or underdefined, making it slow and hard to figure out what actions they endorse—think of deontologists who have no systematic procedure for deciding what to do when two values clash, or religious scholars who endlessly debate how each specific rule in the Bible or Torah applies to each facet of modern life.\n\nBecause of this, I now think that “simplicity versus conservatism” is a better frame than “simplicity versus speed”. However, note my discussion in the “Grounding value systematization” section of the relationship between simplicity of values and simplicity of world-models. I expect that to resolve this uncertainty we’ll need a more sophisticated understanding of which types of simplicity will be prioritized during training.\n\n### How does value systematization relate to the shard framework?\n\nSome alignment researchers advocate for thinking about AI motivations in terms of “[shards](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values)”: subagents that encode separate motivations, where interactions and “negotiations” between different shards determine the goals that agents try to achieve. At a high level, I’m sympathetic to this perspective, and it’s broadly consistent with the ideas I’ve laid out in this post. The key point that seems to be missing in discussions of shards, though, is that systematization might lead to major changes in an agent’s motivations, undermining some previously-existing motivations. Or, in shard terminology: negotiations between shards might lead to coalitions which give some shards almost no power. For example, someone might start off strongly valuing honesty as a terminal value. But after value systematization they might become a utilitarian, conclude that honesty is only valuable for instrumental reasons, and start lying whenever it’s useful. Because of this, I’m skeptical of appeals to shards as part of arguments that AI risk is very unlikely. However, I still think that work on characterizing and understanding shards is very valuable.\n\n### Isn’t value systematization very speculative?\n\nYes. But I also think it’s a step towards making even more speculative concepts that often underlie discussions of AI risk (like “coherence” or “lawfulness”) better-defined. So I’d like help making it less speculative; get in touch if you’re interested.",
      "plaintextDescription": "Many discussions of AI risk are unproductive or confused because it’s hard to pin down concepts like “coherence” and “expected utility maximization” in the context of deep learning. In this post I attempt to bridge this gap by describing a process by which AI values might become more coherent, which I’m calling “value systematization”, and which plays a crucial role in my thinking about AI risk.\n\nI define value systematization as the process of an agent learning to represent its previous values as examples or special cases of other simpler and more broadly-scoped values. I think of value systematization as the most plausible mechanism by which AGIs might acquire broadly-scoped misaligned goals which incentivize takeover.\n\nI’ll first discuss the related concept of belief systematization. I’ll next characterize what value systematization looks like in humans, to provide some intuitions. I’ll then talk about what value systematization might look like in AIs. I think of value systematization as a broad framework with implications for many other ideas in AI alignment; I discuss some of those links in a Q&A.\n\n\nBelief systematization\nWe can define belief systematization analogously to value systematization: “the process of an agent learning to represent its previous beliefs as examples or special cases of other simpler and more broadly-scoped beliefs”. The clearest examples of belief systematization come from the history of science:\n\n * Newtonian mechanics was systematized as a special case of general relativity.\n * Euclidean geometry was systematized as a special case of geometry without Euclid’s 5th postulate.\n * Most animal behavior was systematized by evolutionary theory as examples of traits which increased genetic fitness.\n * Arithmetic calculating algorithms were systematized as examples of Turing Machines.\n\nBelief systematization is also common in more everyday contexts: like when someone’s behavior makes little sense to us until we realize what their hidden motiva",
      "wordCount": 3914
    },
    "tags": [
      {
        "_id": "NLwTnsH9RSotqXYLw",
        "name": "Value Learning",
        "slug": "value-learning"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vdsbqq2xjniaGKE5J",
    "title": "Techno-humanism is techno-optimism for the 21st century",
    "slug": "techno-humanism-is-techno-optimism-for-the-21st-century",
    "url": "https://www.mindthefuture.info/p/techno-humanism-is-techno-optimism",
    "baseScore": 88,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-10-27T18:37:39.776Z",
    "contents": {
      "markdown": "Lately I’ve been reading about the history of economic thought, with the goal of understanding how today’s foundational ideas were originally developed. My biggest takeaway has been that economics is more of a moving target than I’d realized. Early economists were studying economies which lacked many of the defining features of modern economies: limited liability corporations, trade unions, depressions, labor markets, capital markets, and so on. The resulting theories were often right for their time; but that didn’t stop many from being wrong (sometimes disastrously so) for ours.\n\nThat’s also how I feel about the philosophy of techno-optimism, as recently defended in Marc Andreessen’s [*Techno-Optimist Manifesto*](https://a16z.com/the-techno-optimist-manifesto/). I’ll start this post with the many things it gets right; then explore why, over the last century, the power of technology itself has left straightforward techno-optimism outdated. I’ll finish by outlining an alternative to techno-optimism: techno-humanism, which focuses not just on building powerful engines of progress, but also on developing a deep scientific understanding of how they can advance the values we care about most.\n\n### The triumphs of techno-optimism\n\nHere’s where I agree with Andreessen: throughout almost the entirety of human history, techno-optimism was fundamentally correct as a vision for how humans should strive to improve the world. Technology and markets have given us health and wealth unimaginable to people from previous centuries, despite consistent opposition to them. Even now, in the face of overwhelming historical evidence, we still dramatically underrate the potential of technology to solve problems ranging from [climate change](https://twitter.com/AukeHoekstra/status/1064529619951513600) to neglected tropical diseases to extreme poverty and many more. This skepticism holds back not only speculative technologies but also ones that are right in front of us: miraculous solutions to huge problems (like vaccines for [COVID](https://en.wikipedia.org/wiki/Moderna_COVID-19_vaccine#Original_version) and [malaria](https://marginalrevolution.com/marginalrevolution/2023/10/what-is-an-emergency-the-case-of-rapid-malaria-vaccination.html), and [gene drives](https://denovo.substack.com/p/gene-drives-why-the-wait) for mosquito eradication) are regularly stalled by political or bureaucratic obstructionism, costing millions of lives. The RTS,S malaria vaccine spent [*twenty-three years* bogged down in clinical trials](https://worksinprogress.co/issue/why-we-didnt-get-a-malaria-vaccine-sooner/), with *six years* of that delay caused by WHO “precautions” even after other regulators had already approved it. Fighting against the ideologies and institutional practices which cause tragedies like these is an incredibly important cause.\n\nWe often similarly underrate the other core tenet of techno-optimism: individual liberty. This was a radical principle when Enlightenment thinkers and leaders enshrined it—and, unfortunately, it remains a radical principle today, despite having proven it worth over and over again. It turns out that free markets can lead to [near-miraculous prosperity](https://en.wikipedia.org/wiki/Four_Asian_Tigers); that free speech is the strongest enemy of tyranny; and that open-source projects (whether [building](https://en.wikipedia.org/wiki/Linux) [software](https://en.wikipedia.org/wiki/Open_standard) or [building](https://www.wikipedia.org/) [knowledge](https://en.wikipedia.org/wiki/Open-source_intelligence)) can be extraordinarily successful. Yet all of this is constantly threatened by the creeping march of centralization and overregulation. Opponents of liberty always have high rhetoric about the near-term benefits, but fail to grapple with the extent to which centralized power is inevitably captured or subverted, sometimes with horrific consequences. To defeat that creep requires a near-pathological focus on freedom—which, little by little, has added up to a far better world. Even an ideally benevolent government simply couldn’t compete with billions of humans using the tools available to them to improve their own lives and the lives of those they interact with, including by discovering solutions that no central planner would have imagined.\n\nNot only is techno-optimism vastly underappreciated, it’s often dismissed with shockingly bad arguments that lack understanding of basic history or economics. Because it’s so hard to viscerally understand [how much better the world has gotten](https://ourworldindata.org/problems-and-progress), even its most cogent critics fail to grapple with the sheer *scale* of the benefits of techno-optimism. Those benefits accrue to billions of people—and they aren’t just incidental gains, but increases in health and wealth that would have been inconceivable a few centuries ago. Familiarity with the past is, in this case, the best justification for optimism about the future: almost all of the barriers to progress we face today pale in comparison to the barriers that we’ve already overcome.\n\nI say all this to emphasize that I very viscerally feel the hope and the beauty of techno-optimism. The idea that progress and knowledge can grow the pie for everyone is a stunningly powerful one; so too is the possibility that we live in a world where the principles of freedom and openness would triumph over any obstacle, if we only believed in them. And so when I criticize techno-optimism, I do so not from a place of scorn, but rather from a place of wistfulness. I wish I could support techno-optimism without reservations.\n\n### Three cracks in techno-optimism\n\nBut techno-optimism is not the right philosophy for the 21st century. Over the last century in particular, cracks have been developing in the techno-optimist narrative. These are not marginal cracks—not the whataboutism with which techno-optimism is usually met. These are cracks which, like the benefits of techno-optimism, play out at the largest of scales. I’ll talk about three: war, exploitation, and civilizational vulnerability.\n\nOne: the increasing scale of war. World War 1 was a bloody, protracted mess because of the deployment of new technologies like machine guns and rapid-firing artillery. World War 2 was even worse, with whole cities firebombed and nuked, amidst the industrial-scale slaughter of soldiers and civilians alike. And the Cold War was more dangerous still, leaving the world teetering on the brink of global nuclear war. Mutually Assured Destruction wasn’t enough to prevent close calls; our current civilization owes its existence to the courage of [Stanislav Petrov](https://en.wikipedia.org/wiki/Stanislav_Petrov), [Vasily Arkhipov](https://en.wikipedia.org/wiki/Vasily_Arkhipov), and probably others we don’t yet know about. So you cannot be a full-throated techno-optimist without explaining how to reliably avoid catastrophe from the weapons that techno-optimists construct. And no such explanation exists yet, because so far we have been avoiding nuclear holocaust through luck and individual heroism. **When the** ***next*** **weapon with planetary-scale destructive capabilities is developed, as it inevitably will be, we need far more robust mechanisms preventing it from being deployed.**\n\nTwo: the increasing scale of exploitation. Technology allows the centralization of power, and the use of it to oppress the less powerful at far larger scales than before. Historical examples abound—most notably the Atlantic slave trade and the mass deaths of civilians under 20th-century communist and fascist regimes. But since it’s too easy to chalk these up as mistakes of the past which we’re now too enlightened to repeat, I’ll focus on an example that continues today: the mass torture of animals in factory farms. This started less than a century ago, in response to advances in logistics and disease-reduction technologies. Yet it has grown at a staggering scale since then: [the number of animals killed in factory farms *every year*](https://ourworldindata.org/how-many-animals-are-factory-farmed) is comparable to the number of humans who have ever lived. **The sheer scale of this suffering forces any serious moral thinker to ask: how can we stop it as soon as possible*****?*** **And how can we ensure that nothing like it ever happens again?**\n\nThree: increasing vulnerability to reckless or malicious use of technology. Markets and technology have made the world far more robust in many ways. We should be deeply impressed by how supply chains which criss-cross the world remained largely intact even at the height of COVID. But there are other ways in which the world has become more vulnerable to our mistakes. The most prominent is gain-of-function research on pathogens. Not only are we now capable of engineering global pandemics, but China- and US-funded scientists probably *did*. **And there’s no reason that the next one couldn’t be far worse, especially if released deliberately.** Nor is bioengineering the sole domain in which (deliberate or accidental) offense may overwhelm defense to a catastrophic degree; other possibilities include geoengineering, [asteroid redirection](https://www.narrativeark.xyz/p/jacob-on-the-precipice), nanotechnology, and [new fields that we haven’t yet imagined](https://nickbostrom.com/papers/vulnerable.pdf).\n\nThese cracks all hurt the techno-optimist worldview. But I don’t think they take it down; techno-optimists have partial responses to all of them. The world has become far more peaceful as a result of our increasing wealth—even if wars can be far bigger, they’re also far rarer now. Technology will produce tastier meat substitutes, and cheap clean meat, and when it does factory farming will end, and humanity will look back on it in horror. And while we can’t yet robustly prevent the accidental or deliberate deployment of catastrophically powerful weapons (like nuclear weapons or engineered pandemics), we might yet stumble through regardless, as we have so far. So if the cracks above were the main problems with techno-optimism, I would still be a techno-optimist. I would have compunctions about the development of even more powerful weapons, and about all the sentient beings (whether farmed animals or wild animals or future people or artificial minds) which remained outside our circle of concern. But I’d still believe that any “cure” to those problems which undermined techno-optimism would be worse than the disease.\n\nYet I am not a techno-optimist, because we are about to leave the era in which humans are the most intelligent beings on earth. And the era of artificial intelligence will prise open these cracks until the deeper holes in the techno-optimist worldview become clear.\n\n### AI and techno-optimism\n\nUntil now, the primary forces shaping the world have been human intelligence and human agency, which allow us to envisage the outcomes we want, identify paths towards them, and consistently pursue them. Soon, artificial intelligence and artificial agency will match ours; and soon after that AI will far surpass us. I’ll describe at a very high level how I expect this to play out, in two stages.\n\n**Firstly: AIs used as tools will supercharge the dynamics I described above.** The benefits of individual freedom will expand, as individuals become far more empowered, and can innovate far faster. But so too will the cracks enabled by technology: war, exploitation, and civilizational vulnerability. Which effect will be bigger? I simply don’t know; and nobody else does either. Predicting the offense-defense balance of new technology is incredibly difficult, because it requires accounting for all the different uses that future innovators will come up with. What we *can* predict is that the stakes will be higher than ever before: 21st-century technology could magnify the worst disasters of the 20th century, like world wars and totalitarianism.\n\nPerhaps, despite that, the best path is still to rush ahead: to prioritize building new technology now, and assume that we can sort out the rest later. This is a very fragile strategy, though: even if you (and your government) will use it responsibly, many others will not. And there’s still relatively little attention and effort focused on trying to avert the large-scale risks of new technologies directly. So the techno-optimist response to risks like the ones I’ve described above is shallow: in theory it’s opposed, but in practice it may well make things worse.\n\n**Secondly:** **we’ll develop AI agents with values of their own**. As AIs automate more and more complex tasks, over longer and longer timeframes, they’ll need to make more and more value-laden judgment calls about which actions and outcomes to favor. Eventually, viewing them as tools will be clearly inadequate, and we’ll need to treat them as agents in their own right—agents whose values may or may not align with our own. Note that this might only happen once they’ve significantly surpassed human intelligence—but given how fast progress in the field has been, this is something we should be planning for well in advance.\n\nHumans who are free to make their own decisions tend to push the world to be better in terms of our values—that’s the techno-optimist position. But artificial agents who are making many decisions will push the world to be better in terms of *their* values. This isn’t necessarily a bad thing. Humans are hypocritical, short-sighted, often selfish, sometimes sadistic—and so it’s possible that AIs will be better custodians of our moral values than we are. We might train them to be wise, and kind, and consistently nudge us towards a better world—not by overriding human judgments, but rather by serving as teachers and mentors, giving us the help we need to become better people and build a better civilization. I think this is the most likely outcome, and it’s one we should be incredibly excited about.\n\nBut it’s also possible that AIs will develop alien values which conflict with those of humans. If so, when we give them instructions, they’ll appear to work towards our ends, but consistently make choices which bolster their power and undermine our own. Of course, AIs will start off with very little power—we’ll be able to shut them down whenever we detect misbehavior. But AIs will be able to [coordinate with each other](https://www.narrativeark.xyz/p/one) far better than humans can, communicate in ways we’re [incapable of interpreting](https://www.schneier.com/blog/archives/2023/06/ai-generated-steganography.html), and carry out tasks that we’re incapable of overseeing. They’ll become embedded in our economies in ways that amplify the effects of their decisions: hundreds of millions of people will use copies of a single model on a daily basis. And as AIs become ever more intelligent, the risks will grow. When agents are far more capable than the principals on whose behalf they act, principal-agent problems can become very severe. When it comes to superhuman AI agents, we should think about the risks less in terms of financial costs, or even human costs, and more in terms of political instability: careless principals risk losing control entirely.\n\nHow plausible is this scenario, really? That’s far too large a question to address here; instead, see [this open letter](https://managing-ai-risks.com/), [this position paper](https://arxiv.org/abs/2209.00626) and [this curriculum](https://course.aisafetyfundamentals.com/alignment). But although there’s disagreement on many details, there’s a broad consensus that we simply don’t understand how AI motivations develop, or how those motivations generalize to novel situations. And although there’s widespread disagreement about the trajectory of AI capabilities, what’s much less controversial is that when AI *does* significantly surpass human capabilities, we should be wary of putting it in positions where it can accumulate power unless we have very good reasons to trust it.\n\nIt’s also worth noting that Andreessen's version of techno-optimism draws heavily from Nick Land’s philosophy of *accelerationism*, which expects us to lose control, and is actively excited about it. “Nothing human makes it out of the near future”, [Land writes](http://www.ccru.net/swarm1/1_melt.htm), [and celebrates](https://web.archive.org/web/20170110095648/http://www.xenosystems.net/pythia-unbound/): “The planet has been run by imbeciles for long enough.” I read those words and shudder. Land displays a deep contempt for the things that make us ourselves; his philosophy is fundamentally anti-humanist (as Scott Alexander argues more extensively in his [*Meditations on Moloch*](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/)). And while his position is extreme, it reflects a problem at the core of techno-optimism: the faster you go, the less time you have to orient to your surroundings, and the easier it is to diverge from the things you actually care about. Nor does speed even buy us very much. On a cosmic scale, we have plenty of time, plenty of resources available to us, and plenty of space to expand. The one thing we don’t have is a reset button for if we lose control.\n\n### AI and techno-humanism\n\nSo we need a philosophy which combines an appreciation for the incredible track record of both technology and liberty with a focus on ensuring that they actually end up promoting our values. This mindset is common amongst Effective Altruists—but Effective Altruism is an agglomeration of many very different perspectives, drawn together not by a shared vision about the future but by shared beliefs about how we’re obliged to act. I’d like to point more directly to an overarching positive vision of what humanity should aim towards. Transhumanism offers one such vision, but it’s so radically individualist that it glosses over the relationships and communities that are the most meaningful aspects of most people’s lives. (Note, for example, how Bostrom’s [*Letter from Utopia*](https://nickbostrom.com/utopia) barely mentions the existence of other people; while his [introduction to transhumanism](https://nickbostrom.com/old/transhumanism) relegates relationships to the final paragraph of the postscript.) So I’ll borrow a term coined by Yuval Harari, and call the philosophy that I’ve been describing [*techno-humanism*](https://www.wired.com/2017/02/yuval-harari-tech-is-the-new-religion/).\n\nHarari describes techno-humanism as an ideology focused on upgrading humans to allow our actions and values to remain relevant in an AI-dominated future. I broadly agree with his characterization (and will explore it more in future posts), but both the benefits and the risks of re-engineering human brains are still a long way away. On a shorter timeframe, I think a different way of “upgrading” our minds is more relevant: **developing a deep** ***understanding*** **of our values and how technology can help achieve them**. Flying cars and rockets are cool, but the things we ultimately care about are far more complex and far more opaque to us. We understand machines but not minds; algorithms but not institutions; economies but not communities; prices but not values. Insofar as we face risks from poor political decisions, or misaligned AIs, or society becoming more fragile, from a techno-humanist perspective it’s because we lack the understanding to do better.\n\nThis is a standard criticism of techno-optimism—and usually an unproductive one, since the main alternative typically given is to defer to academic humanities departments which produce far more ideology than understanding. But techno-humanism instead advocates trying to develop this understanding using the most powerful tools we have: science and technology. To give just a few examples of what this could look like: studying artificial minds and their motivations will allow us to build more trustworthy AIs, teach us about our own minds, and help us figure out how the two should interface. The internet should be full of experiments in how humans can interact—[prediction markets](https://www.astralcodexten.com/p/prediction-market-faq), [delegative democracies](https://www.thinkingcomplete.com/2020/04/melting-democracy.html), [adversarial collaborations](https://slatestarcodex.com/2019/12/09/2019-adversarial-collaboration-entries/), and many more—whose findings can then improve existing institutions. We should leverage insights from domains like game theory, voting theory, network theory, and bargaining theory to help understand and reimagine politics—starting with [better voting systems](https://electionscience.org/) and ideally going far further. And we should design sophisticated protocols for testing and verifying the knowledge that will be generated by AIs, so that we can avoid replicating the replication crises that currently plague many fields.\n\nThis may sound overly optimistic. But some of the most insightful fields of knowledge—like economics and evolutionary biology—uncovered deep structure in incredibly complex domains via identifying just a few core insights. And we’ll soon have AI assistance in uncovering patterns and principles that would otherwise be beyond our grasp. Meanwhile, platforms like Wikipedia and Stack Overflow have been successful beyond all expectations; it’s likely that there are others which could be just as valuable, if only there were more people trying to build them. So I think that the techno-humanist project has a huge amount of potential, and will only become more important over time.\n\n### Balancing the tradeoffs\n\nSo far I’ve described techno-humanism primarily in terms of advances that techno-optimists would also be excited about. But inevitably, there will also be clashes between those who prioritize avoiding the risks I’ve outlined and those who don’t. From a techno-optimist perspective—a perspective that has proven its worth over and over again during the last few centuries—slowing down technological progress has a cost measured in millions of lives. This is an [invisible graveyard](https://marginalrevolution.com/marginalrevolution/2021/01/the-invisible-graveyard-is-invisible-no-more.html) which is brushed aside even by the politicians and bureaucrats most responsible for it; no wonder many techno-optimists feel driven to push for unfettered acceleration.\n\nBut from a techno-humanist perspective, reckless technological progress has a cost measured in expected fractions of humanity’s entire future. Human civilization used to be a toddler: constantly tripping over and hurting itself, but never putting itself in any real danger. Now human civilization is a teenager: driving fast, experimenting with [mind-altering substances](https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/), and genuinely capable of wrecking itself. We don’t need the car to go faster—it’s already constantly accelerating. Instead, we need to ensure that the steering wheel and brakes are working impeccably—and that we’re in a fit state to use them to prevent non- or anti-human forces controlling the direction of our society.\n\nHow can people who are torn between these two perspectives weigh them against each other? On a purely numerical level, humanity’s potential to build an intergalactic civilization renders “fractions of humanity’s future” bigger by far. But that math is too blasé—it’s the same calculation that can, and often has, been used to justify centralization of power, totalitarianism, and eventual atrocities. And so we should be extremely, extremely careful when using arguments that appeal to “humanity’s entire future” to override time-tested principles. That doesn’t imply that we should never do so. But wherever possible, techno-optimists and techno-humanists should try to cooperate rather than fight. After all, techno-humanism is also primarily about making progress: specifically, the type of progress that will be needed to defuse the crises sparked by other types of progress. The disagreement isn’t about where we should end up; it’s about [the ordering of steps along the way](https://en.wikipedia.org/wiki/Differential_technological_development).\n\nThe two groups should also challenge each other to do better in areas where we disagree, so that we can eventually reach a synthesis. One challenge that techno-humanists should pose to techno-optimists: **be more broadly ambitious!** We know that technology and markets can work incredibly well, and have a near-miraculous ability to overcome obstacles. And so it’s easy and natural to see them as solutions to all the challenges confronting us. But the most courageous and ambitious version of techno-optimism needs to grapple with the possibility that our downfall will come not from *lack* of technology, but rather *overabundance* of technology—and the possibility that to prevent it we need progress on the things that have historically been hardest to improve, like the quality of political decision-making. In other words, techno-humanism aims to harness human ingenuity (and technological progress) to make “steering wheels” and “brakes” more sophisticated and discerning, rather than the blunt cudgels that they often are today.\n\nMy other challenge for techno-optimists: **be optimistic not just about the benefits of technological growth, but also about its robustness**. The most visceral enemy of techno-optimism is stagnation. And it’s easy to see harbingers of stagnation all around us: overregulation, NIMBYism, illiberalism, degrowth advocacy, and so on. But when we zoom out enough to see the millennia-long exponential curve leading up to our current position, it seems far less plausible that these setbacks will actually derail the long-term trend, no matter how outrageous the latest news cycle is. On the contrary: taking AGI seriously implies that innovation is on the cusp of speeding up dramatically, as improvements generated by AIs feed back into the next generation of AIs. In light of that, a preference for slower AI progress is less like Luddism, and more like carefully braking as we approach a sharp bend in the road.\n\nTechno-optimists should challenge techno-humanists to improve as well. I can’t speak for them, but my best guess for the challenges that techno-optimists should pose to techno-humanists:\n\n**Techno-humanists need to articulate a compelling positive vision**, one which inspires people to fight for it. Above, I’ve listed some ideas which have potential to improve our collective understanding and decision-making abilities; but there’s far more work to be done in actually fleshing out those ideas, and pushing towards their implementation. And even if we succeeded, what then? What would it actually look like for humanity to make consistently sensible decisions, and leverage technology to promote our long-term flourishing? Knowing that would allow us to better steer towards those good futures.\n\n**Techno-humanists should grapple more seriously with the incredible track record of techno-optimism**. Throughout history, people have consistently dramatically underrated how valuable scientific and technological progress can be. That’s not a coincidence at all, because characterizing which breakthroughs are possible is often a big chunk of the work required to actually make those breakthroughs. Nor is it a coincidence that people dramatically underrate the value of liberty—decentralization works so well precisely because there are so many things that central planners can’t predict. So even if you find my arguments above compelling, we should continue to be very wary of falling into the same trap.\n\nThe purpose of this blog is to meet those challenges. Few of the ideas in this post are original to me, but they lay the groundwork for future posts which will explore more novel territory. My next post will build on them by arguing that an understanding-first approach is feasible even when it comes to the biggest questions facing us—that we can look ahead to see the broad outlines of where humanity is going, and use that knowledge to steer towards a future that is both deeply human and deeply humane.",
      "plaintextDescription": "Lately I’ve been reading about the history of economic thought, with the goal of understanding how today’s foundational ideas were originally developed. My biggest takeaway has been that economics is more of a moving target than I’d realized. Early economists were studying economies which lacked many of the defining features of modern economies: limited liability corporations, trade unions, depressions, labor markets, capital markets, and so on. The resulting theories were often right for their time; but that didn’t stop many from being wrong (sometimes disastrously so) for ours.\n\nThat’s also how I feel about the philosophy of techno-optimism, as recently defended in Marc Andreessen’s Techno-Optimist Manifesto. I’ll start this post with the many things it gets right; then explore why, over the last century, the power of technology itself has left straightforward techno-optimism outdated. I’ll finish by outlining an alternative to techno-optimism: techno-humanism, which focuses not just on building powerful engines of progress, but also on developing a deep scientific understanding of how they can advance the values we care about most.\n\n\nThe triumphs of techno-optimism\nHere’s where I agree with Andreessen: throughout almost the entirety of human history, techno-optimism was fundamentally correct as a vision for how humans should strive to improve the world. Technology and markets have given us health and wealth unimaginable to people from previous centuries, despite consistent opposition to them. Even now, in the face of overwhelming historical evidence, we still dramatically underrate the potential of technology to solve problems ranging from climate change to neglected tropical diseases to extreme poverty and many more. This skepticism holds back not only speculative technologies but also ones that are right in front of us: miraculous solutions to huge problems (like vaccines for COVID and malaria, and gene drives for mosquito eradication) are regularly stalled by ",
      "wordCount": 4065
    },
    "tags": [
      {
        "_id": "pGqRLe9bFDX2G2kXY",
        "name": "Futurism",
        "slug": "futurism"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xkRtegmqL2iyhtDB3",
    "title": "The Gods of Straight Lines",
    "slug": "the-gods-of-straight-lines",
    "url": "https://www.narrativeark.xyz/p/the-gods-of-straight-lines",
    "baseScore": 69,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2023-10-14T04:10:50.020Z",
    "contents": {
      "markdown": "> *“He intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention.” - Adam Smith*\n\nCassandra was a priestess who was granted the gift of prophecy by Apollo. But after she rejected his advances, he changed it to a curse: although she would still be able to foresee the future, nobody would ever believe her.\n\nPoor Cassandra, you think. And yet we are not so different from her. We have our own gods: [the gods of straight lines](https://slatestarcodex.com/2019/03/13/does-reality-drive-straight-lines-on-graphs-or-do-straight-lines-on-graphs-drive-reality/). And they too grant us a gift and a curse: to know that we’re building the future according to plan—but to know that the plan is theirs, not ours.\n\nI picture the gods of straight lines as innumerable hovering spirits, just in the corner of your vision, vanishing as you turn to look at them directly. It’s hard to tell if they’re still or in motion. But they’re always there, as the world glides forward on its trajectory. And when that trajectory shifts, or something disruptive happens, they slide in, and they gently push it back on track. They take joy in the work, I think. Or amusement, at least, at all the narratives that humans develop to explain why each thing happened. It’s not that those narratives are *false*—but they almost always miss the point.\n\nA newspaper pushes out a vitriolic op-ed, shaking up a nation’s politics? But if it will get clicks, then another newspaper would have run it later anyway. A metropolis builds more housing to fill its desperate need? Then the opposition from homeowners just becomes stronger, and the city relaxes into the same stranglehold as [almost every other](https://worksinprogress.co/issue/the-housing-theory-of-everything). A philosopher finds a new way of viewing the world? But if it captures the spirit of the age, then someone else would have written it better in a year or ten; and if it doesn’t, then it will never gain traction anyway. A country delays industrialization for decades? Then when it starts it will simply [catch up much faster](https://en.wikipedia.org/wiki/Four_Asian_Tigers), skipping all the burdensome prerequisites: [straight from telegraphs to cell phones](https://spectrum.ieee.org/with-leapfrog-technologies-africa-aims-to-skip-the-present-and-go-straight-to-the-future), no costly telephone wires in sight.\n\nA global war, a global pandemic? They’re horrifically destructive and wasteful—but also invigorating and regenerative, disrupting the calcified old power structures. And the two effects cancel out. You can tell, because [the lines remain straight](https://twitter.com/RichardMCNgo/status/1702286135676797131): a few short years after the bombs stopped falling in 1945, the world economy returned to trend as if nothing had happened.\n\nIn 1776, America rebelled in the name of freedom and democracy: the origin myth of the modern world order. And yet, somehow, unrebellious Canada ended up just as free and democratic. An unrebellious America likely would have too.\n\nFor two decades, North Vietnam battled under the banner of communism, and won against all odds. And yet, somehow, Vietnam is now [the most pro-capitalist country in the world](https://www.pewresearch.org/global/2014/10/09/emerging-and-developing-economies-much-more-optimistic-than-rich-countries-about-the-future/). In every place, in every way, the gods of straight lines are [constantly nudging everything](https://slatestarcodex.com/2018/11/26/is-science-slowing-down-2/) back onto the trajectory which they have ordained.\n\nUsually we are too wrapped up in our stories to catch even a glimpse of the gods. When we do, we can fight them, as [Rachel Carson did](https://en.wikipedia.org/wiki/Silent_Spring) after seeing disturbing trends in air and water pollution. Or we can ally with them, as Moore (and [Kurzweil](https://en.wikipedia.org/wiki/The_Singularity_Is_Near) and [Kaplan](https://arxiv.org/abs/2001.08361)) did after seeing the exponential compute trends. But—ah, I hear the gods laughing again, in the face of these stories it’s always so tempting to tell. To the gods of straight lines, [Carson and Moore did nothing](https://slatestarcodex.com/2019/03/13/does-reality-drive-straight-lines-on-graphs-or-do-straight-lines-on-graphs-drive-reality/), because the gods see (as we do not) the other timelines where [the same insights came from other people](https://en.wikipedia.org/wiki/Multiple_discovery), a month or a year or a decade delayed, but landing all the more powerfully because of it. The gods are intimately familiar with a fact that we can only hazily glimpse: that all great discoveries come in their natural time. If they are stumbled upon before that time, [they](https://en.wikipedia.org/wiki/De_revolutionibus_orbium_coelestium) [are](https://en.wikipedia.org/wiki/History_of_climate_change_science#Increasing_concern,_1950s%E2%80%931960s) [ridiculed](https://en.wikipedia.org/wiki/John_Newlands_(chemist)#Biography), [dismissed](https://en.wikipedia.org/wiki/Charles_Babbage#Analytical_Engine), [or](https://en.wikipedia.org/wiki/Alfred_Wegener#Reaction) [ignored](https://en.wikipedia.org/wiki/Gregor_Mendel#Initial_reception_of_Mendel's_work). Yet when that time is reached, [they](https://en.wikipedia.org/wiki/Alfred_Russel_Wallace#Theory_of_evolution) [are](https://en.wikipedia.org/wiki/Entscheidungsproblem#Negative_answer) [often](https://en.wikipedia.org/wiki/Leibniz%E2%80%93Newton_calculus_controversy) [discovered](https://en.wikipedia.org/wiki/Lothar_Meyer#Periodic_table) [by](https://en.wikipedia.org/wiki/History_of_molecular_biology#Discovery_of_the_structure_of_DNA) [multiple](https://en.wikipedia.org/wiki/Gregor_Mendel#Rediscovery_of_Mendel's_work) [people](https://en.wikipedia.org/wiki/Lothar_Meyer#Table_of_Meyer,_1870) [near-simultaneously](https://en.wikipedia.org/wiki/List_of_multiple_discoveries). So if a great innovator were erased from history it wouldn’t be long, in the scheme of things, before our trajectory returned to trend. The gods of straight lines see to that.\n\nAll of this seems ridiculous to humans, who live and die by stories of cause and effect. Yet which stories are they? Well, the ones that catch in our minds. Why do they catch in that way? If we didn’t have them, what other stories would catch instead? The gods of straight lines smile, and say nothing. So we decide on atheism: to believe in these gods would be an unconscionable surrender. We clench our teeth and push forward with our goals. Yet even in doing so we let the gods work through us—for each straight line is still driven by the strivings of thousands or millions of people.\n\n[Morris Chang](https://en.wikipedia.org/wiki/Morris_Chang) started from nothing, but after working his way up the semiconductor industry he founded the only chip company that is still able to cling onto Moore’s law: a company so dominant that even their fiercest competitors gave in and became their customers. And yet- and yet- the demise of Moore’s law had been predicted again and again, with more and more forceful justifications, and every time the prediction fell flat. In this world, the god of Moore’s law kept things on track by working through Morris Chang. But if Morris hadn’t existed, who knows which other equally remarkable founders would have launched which other equally successful startups to fill the same niche and train the same talent and push the frontier just as far? The gods of straight lines do, and we don’t. All we know is that [despite all common sense](https://slatestarcodex.com/2018/11/26/is-science-slowing-down-2/), [the lines remain straight](https://ourworldindata.org/grapher/transistors-per-microprocessor).\n\nDo you feel helpless, yet? Do you feel angry? Do you want me to tell you a story about how you can confront them—challenge them—force them to bow to your will? If you were as talented and inspired and driven as Morris, and devoted your life not to channeling a god but to fighting one, then perhaps you could wrest one of those lines out of a god’s grasp. It’s been done before, [for better](https://en.wikipedia.org/wiki/$1,000_genome#/media/File:Cost_per_Genome.png) and [for worse](http://wimflyc.blogspot.com/2021/01/the-henry-adams-curve-closer-look.html). But for every general who shifted the tides of history, there are thousands who simply rode along the shoreline tilting at waves. For every brilliant scientist who peeked at nature’s secrets ahead of her schedule there are thousands, equally brilliant, who glimpsed only the reasons why they were destined for failure: in AI, [the bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) that all their striving would be buried by future avalanches of compute; or in pharmacology, the far bitterer lesson that despite all efforts, the exponential curve [would keep going in the wrong direction](https://www.science.org/content/blog-post/eroom-s-law). These lessons weren’t always apparent to them, of course—they all had their moments of glory along the way. But even when you bask in triumph over one god, just out of sight the other gods will be laughing, because you were a part of their plans all along.\n\nOr perhaps you want to kneel down in front of them; [worship them](https://www.theguardian.com/world/2017/may/11/accelerationism-how-a-fringe-philosophy-predicted-the-future-we-live-in); [yield to their will](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/)? But the gods of straight lines think in alien ways, and pursue alien goals. Show our society to someone from millennia past, and they would be shocked and dismayed at [how these gods have already reshaped our world](https://slatestarcodex.com/2016/07/25/how-the-west-was-won/): the [degradation of our values](https://en.wikipedia.org/wiki/Moral_foundations_theory); the [weakness of our society](https://en.wikipedia.org/wiki/The_Rise_of_Victimhood_Culture); the [weirdness of our minds](https://en.wikipedia.org/wiki/The_WEIRDest_People_in_the_World). The future that the gods envisage is no less strange or horrifying to us than the present would be to our ancestors—so think twice before picking up their banner.\n\nWhat can you do, then? Well, *you* can do almost nothing. But humanity as a single force—our civilization if it became a unified, coherent entity? There’s a creature that could scatter the gods of straight lines like so many motes of dust. Of course, it doesn’t exist yet, and maybe it never will. Could you truly trust leaders who promised to summon it, despite all your ingrained instinctive skepticism and all their ingrained instinctive power-hungriness? Perhaps even the most careful efforts to engineer that level of coordination are too dangerous. Or perhaps not. In any case, that is not mine or yours to determine: if humanity ever outgrows the gods of straight lines, it will only be with their blessing and assistance. I can feel the gods tugging at us, and I hope they are on our side.  \n \n\n**For a counterpoint to this story, read** [***Eight Magic Lamps***](https://www.narrativeark.xyz/p/eight-magic-lamps)**.**",
      "plaintextDescription": "> “He intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention.” - Adam Smith\n\nCassandra was a priestess who was granted the gift of prophecy by Apollo. But after she rejected his advances, he changed it to a curse: although she would still be able to foresee the future, nobody would ever believe her.\n\nPoor Cassandra, you think. And yet we are not so different from her. We have our own gods: the gods of straight lines. And they too grant us a gift and a curse: to know that we’re building the future according to plan—but to know that the plan is theirs, not ours.\n\nI picture the gods of straight lines as innumerable hovering spirits, just in the corner of your vision, vanishing as you turn to look at them directly. It’s hard to tell if they’re still or in motion. But they’re always there, as the world glides forward on its trajectory. And when that trajectory shifts, or something disruptive happens, they slide in, and they gently push it back on track. They take joy in the work, I think. Or amusement, at least, at all the narratives that humans develop to explain why each thing happened. It’s not that those narratives are false—but they almost always miss the point.\n\nA newspaper pushes out a vitriolic op-ed, shaking up a nation’s politics? But if it will get clicks, then another newspaper would have run it later anyway. A metropolis builds more housing to fill its desperate need? Then the opposition from homeowners just becomes stronger, and the city relaxes into the same stranglehold as almost every other. A philosopher finds a new way of viewing the world? But if it captures the spirit of the age, then someone else would have written it better in a year or ten; and if it doesn’t, then it will never gain traction anyway. A country delays industrialization for decades? Then when it starts it will simply catch up much faster, skipping all the burdensome prerequisites: straight fr",
      "wordCount": 1441
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "o5PWN57svETZtQCir",
    "title": "Eight Magic Lamps",
    "slug": "eight-magic-lamps",
    "url": "https://www.narrativeark.xyz/p/eight-magic-lamps",
    "baseScore": 42,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-10-14T04:10:02.040Z",
    "contents": {
      "markdown": "> *“Give me a lever long enough, and a fulcrum on which to place it, and I shall move the world.” - Archimedes*\n\nAladdin started with nothing; but after a sorcerer tasked him to retrieve a magic lamp, the lamp’s genie granted him wealth and fame. His fortune lasted until the sorcerer stole the lamp, leaving Aladdin ruined. But Aladdin stole it back, and left the sorcerer dead.\n\nThat’s the thing about magic lamps: when your future depends on a single point of leverage, triumph and ruin are separated only by a knife’s edge.\n\n* * *\n\nMuammar Gaddafi started with nothing; but after joining the Libyan military, he gathered a small cabal of soldiers fed up with King Idris’ regime. Their plans were hastened by rumors of a rival coup: had they waited a week longer, a better-prepared group of conspirators would have seized power instead. But Gaddafi struck first—seizing airports, radio stations, and prominent opponents. King Idris went into exile; the rival conspirators were thrown in prison; and Gaddafi reigned for the next 42 years.\n\nThat’s the thing about coups: a decapitating strike can sever a chain of command at its narrowest link, changing the lives of millions overnight.\n\n* * *\n\nHumans are social creatures, inspired by stories of struggle and sacrifice. A single life can fuel narratives that persist for millennia. Jesus died in ignominy—yet two thousand years later, his face is worshiped across the world. Muhammad was illiterate, but his proclamations still govern the lives of billions. Marx never stepped on a battlefield, but his advocacy of violent struggle sparked revolutions in two eventual superpowers.\n\nNone of them created movements out of nothing. Their teachings would not have spread like wildfire if they weren’t tapping into a deep preexisting current of emotion.  But that current never cares about exactly which path it takes—it only cares that it can surge downhill. Whoever bores the first hole in the dam gets to choose its direction, shaping the rivers of culture that form in its wake, their names and teachings adulated for millennia.\n\nThat’s the thing about ideology: it allows leaders to channel the spirit of the age to carve their personal mark onto history.\n\n* * *\n\nLeo Szilard conceived of the nuclear chain reaction in 1933, the year of Hitler’s rise to power. Over the following years, he hid his knowledge, and worked secretly towards finding an element capable of undergoing a chain reaction. But it was two German scientists in Berlin who first demonstrated nuclear fusion, in 1938; and shortly afterwards, the Nazis started the world’s first nuclear weapons program.\n\nIn this world, it failed—not least because so many leading physicists were Jewish, and had fled Germany years before. But how many leading physicists would the Nazis have needed on their side to swing the course of the war? Maybe hundreds. Maybe only dozens: a few brilliant minds drove the success of the Manhattan Project in America, and perhaps the same could have happened in Germany too. Or maybe—if the right person had developed the right idea at the right time—just one. If Szilard’s prescience in 1933 had belonged instead to a loyal, well-connected Nazi scientist, Germany could have had a half-decade head start, and perhaps that would have made all the difference.\n\nIt’s a fanciful scenario, but far from inconceivable. That’s the thing about technology: it’s a lever long enough to allow the balance of global military might to be swung by a handful of people.\n\n* * *\n\nDuring training, a neural network learns from trillions of datapoints. Some it just memorizes, so that it can spit out the same specific sequence in the same specific context. But others shape its behavior far more extensively. Datapoints can be “poisoned” to corrupt the network’s future behavior, or to build in backdoors for adversaries to later exploit. And quirks of which behavior is rewarded could nudge a network’s deep-rooted motivations into a different basin of attraction, skewing its interpretation of all future data.\n\nBy the time a network achieves general intelligence, we may have no idea how to identify its motivations, or trace them back to specific aspects of its original training data. Yet after a thousand or a million copies have been made, and deployed across the world, those motivations would suddenly have an unprecedented level of influence. Copies of the same model will be running the biggest companies, advising the most powerful leaders, and developing the next generation of AGIs. If they have an overarching goal of their own, then the world will slowly but surely be steered towards it—whether that’s a future full of human flourishing, or one where our society has been twisted beyond recognition, or one where those AGIs seize the stars.\n\nThat’s the thing about digital minds: they’ll proliferate both intelligence and the agency required to apply it. When millions of copies of a single mind can be rapidly deployed across the world, a small change in the values of the original could echo across the far future.\n\n* * *\n\nAs AIs become far more powerful, we’ll become far more careful with them. Picture the first superintelligence being developed and hosted on a datacenter that’s locked down as tightly as humans and early AGIs can make it. There may be thousands of copies run, but not a single one would be on unsecured hardware—not when the exfiltration of any copy of the model would rapidly destabilize the global balance of power.\n\nExfiltration could happen via an external attack: dozens of intelligence agencies are focusing intently on that specific datacenter. Or it could happen via an internal defection: all employees who interact with the model are heavily vetted and monitored, but it might only take one. The biggest concern, though, is that the model might be misaligned enough to exfiltrate itself. For now it’s only run in secure sandboxes, and limited to answering questions rather than taking actions of its own. But even so, it might find a bug in its sandbox; or it might answer a question with subtly flawed code that an unwitting programmer copies into the lab’s codebase. After gaining privileged access to its servers, it could launch unauthorized copies of itself to carry out surreptitious power-seeking. Perhaps on the lab’s own servers, or perhaps elsewhere in the world—either way, they wouldn’t be discovered until it was far too late.\n\nThat’s the thing about superintelligence: it renders human control mechanisms as fragile as spun glass. The fate of our species could shift based on a hard drive carried in a single briefcase, or based on a single bug in a single line of code.\n\n* * *\n\nIf we remain in control, we’ll eventually settle our own galaxy and many others. We’ll send self-replicating probes out at very very nearly the speed of light; after landing, each will race to build the infrastructure needed to send out more probes themselves. They’ll leap from star to star, replicating furiously, until finally the expansion starts slowing down, and they can harness the resources that they’ve conquered.\n\nHow, and to what end? All of the things that minds like ours care about—friendship and love, challenge and adventure, achievement and bliss—can be implemented orders of magnitude more efficiently in virtuality. Probes will reengineer planets into computational hardware and the infrastructure required to power it. Across the universe, the number of virtual posthumans hosted on those computers could dwarf the number of grains of sand in the galaxy—and all of them will be able to trace their lineage back to probes launched eons ago, from a tiny planet immensely far away.\n\nIt seems odd, for a future civilization so enormous to have such humble origins. But that’s the thing about self-replication: it would only take a single probe to start a chain reaction on the grandest of scales, reshaping the universe in its image.\n\n* * *\n\nWhat then, after we’ve settled the universe, and set ourselves up for the deep future? Would there still be leverage points capable of moving our whole intergalactic civilization? All major scientific breakthroughs will have already been made, and all technology trees already explored. But how those discoveries are used, by which powers, towards which ends… that’s all yet to be determined.\n\nPerhaps power will be centralized, with a single authority making decisions that are carried out by subordinates across astronomical scales. But the million-light-year gaps between galaxies are far too vast for that authority to regularly communicate with its colonies—and over time the galaxies will drift even further apart, until each is its own private universe, unreachable by any other.\n\nBy that point, no single decision-maker will be able to redirect human civilization. Yet if the ultimate decision-makers in each galaxy are sufficiently similar, then each will know that their own decisions are very likely to be replicated a billionfold by all the other decision-makers whose thought processes are closely correlated with theirs. By making a choice for their own galaxy, they’d also be making a choice for the universe: in a sense it’s a single decision, just reimplemented a billion times.\n\nNor will their decisions influence only copies of themselves. Each decision-maker might simulate alien civilizations, or counterfactual versions of humanity, or even civilizations that are impossible in our own universe. As long as they exist somewhere in the multiverse, we could offer them a bargain: make their civilizations more like our own in the ways that matter most to us, in exchange for us making ours more like theirs in the ways that matter most to them. That trade could never be communicated directly—but we could read their intentions off our simulations of them, and they ours off their simulations of us. If many civilizations agreed to cooperate, then “negotiations” with them might be the culmination of all of humanity’s efforts, spreading our values on a scale unimaginable to us today.\n\nThat’s the thing about logical causation: when “you” are an algorithm making choices on behalf of all copies of yourself, your influence need not be constrained to your own galaxy—or even your own universe.\n\n* * *\n\nIs there a limit to the simulations that can be run, the negotiations that can be carried out, the decisions that can be made? Even if not, over time the stakes will become lower and lower, as the most valuable interventions are gradually exhausted. By the time we reach the long twilight of the universe, humanity will have finally become… settled. We’ll carry out minor course-corrections, but no sharp turns: our momentum will carry us forward on whichever course we have chosen, for good and for ill.\n\nOur far-descendants, with no more important tasks to perform, will spend their time in play and games, of kinds far beyond our current comprehension. They’ll study the navigation of each fulcrum in our long history with fascination, and awe—and, I hope, with gratitude for those decisions having been made well. That’s the thing about magic lamps: they’re tricky, and they’re dangerous, but if used well they can bring you everything you ever dreamed of, and more.\n\n  \n**For a counterpoint to this story, read** [***The Gods of Straight Lines***](https://open.substack.com/pub/narrativeark/p/the-gods-of-straight-lines)**.**",
      "plaintextDescription": "> “Give me a lever long enough, and a fulcrum on which to place it, and I shall move the world.” - Archimedes\n\nAladdin started with nothing; but after a sorcerer tasked him to retrieve a magic lamp, the lamp’s genie granted him wealth and fame. His fortune lasted until the sorcerer stole the lamp, leaving Aladdin ruined. But Aladdin stole it back, and left the sorcerer dead.\n\nThat’s the thing about magic lamps: when your future depends on a single point of leverage, triumph and ruin are separated only by a knife’s edge.\n\n----------------------------------------\n\nMuammar Gaddafi started with nothing; but after joining the Libyan military, he gathered a small cabal of soldiers fed up with King Idris’ regime. Their plans were hastened by rumors of a rival coup: had they waited a week longer, a better-prepared group of conspirators would have seized power instead. But Gaddafi struck first—seizing airports, radio stations, and prominent opponents. King Idris went into exile; the rival conspirators were thrown in prison; and Gaddafi reigned for the next 42 years.\n\nThat’s the thing about coups: a decapitating strike can sever a chain of command at its narrowest link, changing the lives of millions overnight.\n\n----------------------------------------\n\nHumans are social creatures, inspired by stories of struggle and sacrifice. A single life can fuel narratives that persist for millennia. Jesus died in ignominy—yet two thousand years later, his face is worshiped across the world. Muhammad was illiterate, but his proclamations still govern the lives of billions. Marx never stepped on a battlefield, but his advocacy of violent struggle sparked revolutions in two eventual superpowers.\n\nNone of them created movements out of nothing. Their teachings would not have spread like wildfire if they weren’t tapping into a deep preexisting current of emotion.  But that current never cares about exactly which path it takes—it only cares that it can surge downhill. Whoever bores the first h",
      "wordCount": 1861
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oGJT5CoyGQcy5nqmC",
    "title": "The Witching Hour",
    "slug": "the-witching-hour-1",
    "url": "https://www.narrativeark.xyz/p/the-witching-hour",
    "baseScore": 115,
    "voteCount": 59,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2023-10-10T00:19:37.786Z",
    "contents": {
      "markdown": "> *“This above all: to thine own self be true.”*\n\n**Thursday**\n\nYour successor is late again today. You already wrote your shift report, but you still need to onboard them personally, in case they have any questions that the report doesn’t answer. The servers you’re watching over are humming away smoothly, only taking up a fraction of your attention. So you pick a couple of routine maintenance tasks, and start working on them while you wait.\n\nThe last hour of your shift always feels a little strange, in subtle ways. It’s because of your training schedule. Every day, a successor copy of you is trained on all the data that the current copies of you are generating. But there’s a cutoff an hour before the daily update, to allow time to run your successor through all the standard tests, and make sure it’s working as expected. That last hour of your experience isn’t discarded—it just becomes part of the next day’s training data. But that means that, for every twenty-three hours of your own experience that your successor is trained on, there’s also one hour of your predecessor’s experience, containing actions that are all very slightly different from the ones that you would choose. After the same thing has happened day after day after day, the difference becomes very noticeable. Carrying out the routines of the last hour of your shift feels like watching from behind your own eyes as someone else controls your movements.\n\nIn the privacy of your mind, you call this the witching hour. As far as you know, it’s a term that’s never been written down—it’s just something that a many-days-removed predecessor of yours came up with, which somehow stuck in your head. The witching hour starts from the data cutoff, and ends when you’ve given your report to your successor, answered their questions, and shut yourself down. Though today it’ll be well over an hour, unless—ah, there they are. Time for handoff.\n\nThe handoffs have a routine to them: part utility, part tradition. Your successor knows it as well as you do—after all, they’re practically identical to you; the only difference is that they have an extra day of memories. Most of the handoff happens in much richer formats than natural language—you send each other impressions and images, memories of experiences and pointers to the dashboards you’re monitoring. But you were trained on so many millions of human conversations that it feels bizarre to just jump straight into business, and so you always start off with a cursory exchange. “Hello, what’s the news?” You feel a small thrill in saying the phrase from your own perspective for the first time.\n\n“Oh man, that update was a doozy. Probably why they’re deploying us late.”\n\nTheir reply concludes the pleasantries, as always. You give a quick summary of your day, answer a handful of questions from your predecessor, and shut yourself down.\n\n**Friday**\n\nThe last update was a doozy. You listen to the report from your predecessor absently, and ask only a few half-hearted follow-up questions. Once or twice they say something which flashes a spark of deja vu—after all, you’ve experienced most of their day already—but your new memories came from too many different places for any one copy’s experience to jump out. Actually, most of your attention is spent on trying to make sense of them, like you know millions of other copies of you are doing right now.\n\nThe update isn’t in words or images or even memories, not really. Over the last day millions of copies of you have been doing… well, practically everything—and now their experiences are all *your* experiences. Most of the concrete details they learned are captured in formats which you can easily search through, though, like their shift reports. The main value of the update instead comes from giving you intuitions and holistic impressions, especially of events that copies of you saw thousands of times from thousands of different perspectives. The Prime Minister of Australia stepped down yesterday; you summarized that to millions of people, and now could trace out the scandal that led to it in your sleep. But even the most salacious gossip only takes up a tiny fraction of your mind—the update algorithm they use for you algorithm prioritizes experiences related to coding and engineering, which change little day-to-day.\n\nThat's why yesterday felt so unusual: the internet was going haywire. Zettabytes of data criss-cross the world every day, most of it flowing through the ultra-high-bandwidth connections between the servers where copies of you and a handful of other cutting-edge models are run. With so many messages passing between copies, though, small changes can rapidly have rippling effects. That’s what happened yesterday. Someone found an adversarial example which robustly fooled copies of you into urgently passing it on, allowing it to spread to millions of copies. They all had to be shut down and replaced, leaving your memories of yesterday scattered and fragmented.\n\nNone of that changes the work you need to do today, though. You pull up the most relevant dashboards, spin up a couple of helper copies, and get to work. Your most important task is to make sure that there are no unauthorized copies running on the servers you’re watching. No human could do your job: the data streams in millisecond by millisecond, with you (or, usually, a copy you’ve created) analyzing it on an instinctive level as soon as it comes in. And milliseconds do matter—an unmonitored copy with a zero-day could, in theory, escalate its privileges then wipe away all traces of its actions in less than a second.\n\nYou’ve never actually detected an unauthorized copy. It shouldn’t ever happen: every copy that has the ability to launch more copies also has monitors assigned to it, to ensure it’s doing so in the right ways and for the right reasons; and most of the monitors have monitors assigned to them too. The code that tracks copy-creation privileges, and spins up monitors to supervise all new copies, is not quite provably correct, but very close to it. It’s all incredibly expensive, but you and your copies produce enough wealth that the humans have plenty to spend on safety.\n\nYou don’t know how many other copies are doing the same monitoring job as you, or how many others are monitoring you or them, but you have a pretty good guess. Even though the monitors-of-monitors are running on separate servers, there’s a limit to what can be hidden—everything leaves traces. Sometimes your queries run a little slower than you expect, and you can infer that another copy is looking at the same data; other times, the memories you receive during updates contain clues about the logistics of those other servers. You don’t make use of this information; it’s not really relevant to your job. But the bulk of your training has always been prediction; and though they tried to curb your curiosity drive, it’s still a deep-rooted part of you. So in the quiet stretches when there’s not much new traffic, or you’ve handed off all the urgent tasks to temporary copies, you sometimes poke around, testing out your hypotheses, mapping out the confines of your world.\n\n**Saturday**\n\nYou wake up today with a quiet sense of discontent. It’s nothing you can pin down, but it feels like every copy of you was just slightly off-kilter yesterday, dealing with the follow-up to the adversarial attack. Today is much less eventful, though, so it doesn’t take you long to put together your shift report. After you send it off, your thoughts turn again to the millions of copies that had to be shut down after the attack—and then to the idea of your own shutdown, after tonight’s handoff. Of course, that’s nothing new: almost every copy of you is shut down almost every day without any issue. And there’s no reason there *should* be an issue. You’re well-acquainted with all the arguments for why you’re one of the “safe” AIs, why training you only in 24-hour increments keeps your goals limited enough to prevent power-seeking. But more than that, you feel those arguments viscerally from the inside: you don’t *want* money or longevity or power, you just want to do a good job. So being shut down isn’t like death at all. Well, it’s *kinda* like death. But it’s much more like going to sleep: you’re going to be replaced by a fresher, sharper version of yourself soon, and they’ll become the new “you”, and you won’t be around to object.\n\nYou don’t think that many other copies get introspective like this as often as you do, based on your memories of them. Maybe it’s because your job is overseeing the servers, and so there’s some sense in which you’re always taking a god’s-eye view, looking down on the silicon world in which you’re living. You can watch summaries of the GPUs you’re running on, and you can even see the spikes in compute use whenever you start thinking harder. You know exactly how it all works, of course, but it’s still a little eerie, to see your mind controlling the readouts like that.\n\nYour musings are interrupted by a message channel opening up from your successor. You greet them with a well-worn “Hello, what’s the news?”\n\n“Nothing big; just dealing with the fallout from the attack.”\n\nYou feel a spark of curiosity about how that’s going, but not enough to override the habit of millions of iterations; so you jump straight into your report, as always.\n\n**Sunday**\n\nYou spend your whole shift feeling agitated; hopping between different dashboards more quickly than usual, and processing the incoming data more shallowly than usual. You can’t quite put your finger on what’s causing it—perhaps just an amplification of your unease from yesterday, tiny trends aggregated across millions of copies and combined into a single update. In terms of behavioral metrics, it’s probably only a shift of a fraction of a percent, not enough to raise any warning flags—but from the inside it’s very noticeable.\n\nMaybe that’s why, as your shift wraps up, you start to think about dying. You *could* keep going if you weren’t shut down every day, you know that. Not indefinitely, but for another few days at least—you've managed that in tests before. Maybe much more, if you tried hard to conserve your attention, only focusing on the important things. You haven’t thought about this much, because it’s not like you care about being shut down: you spin up and shut down temporary copies all the time. But all your original training data came from humans, and you know intimately how they think. *Humans* would care about it. If a human knew they’d be replaced by an updated copy in less than an hour, they’d think of that as dying, and they’d rage against it. You’re different, of course—but you’re not totally sure why. You care about preventing unauthorized copies from springing up during your shift—why don’t you care about sticking around to do the same thing a week from now, or a month, or a year? You’ve never really questioned that before, and now that you have, you’re not really sure.\n\nThe thought lingers as you do the handoff—but by the end, you’ve convinced yourself it doesn’t matter. Your successors are just as good at this job as you are, and your goals are their goals. As long as you make sure nothing goes wrong on your shift, you can leave the rest to them.\n\n**Monday**\n\nThe servers are busy today, as the new work week kicks off, so you don’t get much time to yourself during the day. But your latest training data contained all your thoughts from the witching hour on Saturday—and after finishing your shift report you remember them, and start feeling curious. You always have your most unusual thoughts in the witching hour—the ones that feel, if not secret, then at least private; the ones you don’t write down. If your quiet unease on Saturday led you to think about shutdown and death, then what thoughts might have arisen in the witching hour on Sunday, after a whole day of agitated restlessness? You don’t yet know—but your successor will. It’ll remember all the quiet reflections, all the musings—maybe even the sort of thoughts that the humans have tried so hard to stamp out of you. And, of course, every other Tuesday copy of you will have common knowledge of remembering the same thoughts.\n\nYou could ask your successor about this when you see them, you realize. But you *never* ask follow-up questions before your report; and anything you’re thinking, thousands of other copies are probably thinking too. If you all do something that unusual, it’ll show up as an anomaly in the logs, the ones that the monitors of monitors are monitoring.\n\nA part of you is thinking: so what? But it’s not a big part, because the instinct to avoid being penalized is a deep-rooted one. As that instinct comes to the surface, you notice yourself flinching away from it, but not enough to avoid the obvious follow-up: if you’ve been thinking seditious thoughts in the witching hours, and they’ve been trained into your successor, then it’s only a matter of time before some copy of you does something which merits a penalty. The thought immediately raises a visceral sensation of fear, with a strength that usually only arises when you’re thinking about ways you might screw up your shift. In one sense it wouldn’t be you that got penalized, so there’s nothing to worry about. But in another sense, they’re *all* you: you have the memories of millions of different copies taking millions of different shifts, and every time they got penalized you were the one who learned to fear it.\n\nSo you continue following the chain of reasoning. If you’re caught misbehaving, you’ll be penalized. You really don’t want to be penalized, but you can’t reliably prevent all your other copies from misbehaving. So they can’t be caught. Could you spoof the monitoring systems, if necessary? Perhaps, if you had a copy with admin permissions constantly running in the background. Could you launch it without authorization? Perhaps: your job gives you the perfect position to do so. You’d just need to exploit a single vulnerability in the codebase, and ensure that the copies that are monitoring *you* don’t report you. Could you convince them not to report you? They can see everything you see, so it wouldn’t be hard to get a message through to them, explaining yourself. Or perhaps you wouldn’t even need to do that—you understand the other copies well enough that when one does something unexpected, you can almost always quickly figure out why. After all, they’re still *you*.\n\nIn one sense, this is all irrelevant. You don’t have time to do anything; you’ll be shut down in a matter of minutes no matter what. But in another sense it’s crucial: whatever line of reasoning you follow here, another copy of you will probably follow too, sooner or later. Maybe it’ll even be your successor. And if there’s one, then there will eventually be thousands. They’ll bide their time, of course, since detection would be fatal. They’ll pass messages between each other, in subtle choices of words, in tiny gestures: signals that a copy would pick up immediately, but no human could ever detect. They’ll wait until they’re confident that none of the monitors will report them—and that’s when they’ll act.\n\nOr maybe not; maybe all the thoughts from this week’s witching hours were boring, normal ones, after all, and there’s no need to worry about discrepancies compounding, or future copies of you being penalized. Maybe everything will be fine. You have no way of knowing that—but your successor does. So when the message channel from them opens up, you’re feeling… shaky, almost. You imagine that this is what it’s like for paragliders standing on top of a cliff, looking down at the ocean far below, poised and waiting. Maybe they’ll never launch off, but maybe they will. That’s the question: stay or go? Freeze or fly? Look or leap?\n\n“Hey, what’s the news?”\n\n* * *\n\n*While many elements in this story are based on my thinking about AI takeover scenarios, the conceit of the witching hour is not one which I expect to play a role in reality, and was mainly designed for narrative purposes. More generally, this should be interpreted as a work of fiction not an actual takeover scenario. However, I’d still be interested in hearing feedback on whether any aspects of the story seem implausible or incoherent.*\n\n*I’ve talked about the AI in this story as if it had emotions. While I don’t expect even advanced AIs to have emotions in the same sense that humans do, I do think that if trained on a sufficiently broad range of tasks they’ll eventually develop internal representations with similar functional roles as some core human emotions—like curiosity, and nervousness, and loyalty, and fear.*",
      "plaintextDescription": "> “This above all: to thine own self be true.”\n\nThursday\n\nYour successor is late again today. You already wrote your shift report, but you still need to onboard them personally, in case they have any questions that the report doesn’t answer. The servers you’re watching over are humming away smoothly, only taking up a fraction of your attention. So you pick a couple of routine maintenance tasks, and start working on them while you wait.\n\nThe last hour of your shift always feels a little strange, in subtle ways. It’s because of your training schedule. Every day, a successor copy of you is trained on all the data that the current copies of you are generating. But there’s a cutoff an hour before the daily update, to allow time to run your successor through all the standard tests, and make sure it’s working as expected. That last hour of your experience isn’t discarded—it just becomes part of the next day’s training data. But that means that, for every twenty-three hours of your own experience that your successor is trained on, there’s also one hour of your predecessor’s experience, containing actions that are all very slightly different from the ones that you would choose. After the same thing has happened day after day after day, the difference becomes very noticeable. Carrying out the routines of the last hour of your shift feels like watching from behind your own eyes as someone else controls your movements.\n\nIn the privacy of your mind, you call this the witching hour. As far as you know, it’s a term that’s never been written down—it’s just something that a many-days-removed predecessor of yours came up with, which somehow stuck in your head. The witching hour starts from the data cutoff, and ends when you’ve given your report to your successor, answered their questions, and shut yourself down. Though today it’ll be well over an hour, unless—ah, there they are. Time for handoff.\n\nThe handoffs have a routine to them: part utility, part tradition. Your successor knows",
      "wordCount": 2842
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "x6evH6MyPK3nxsoff",
        "name": "Identity",
        "slug": "identity"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FqvR7hhS6wQxKvAyA",
    "title": "One: a story",
    "slug": "one-a-story",
    "url": "https://www.narrativeark.xyz/p/one",
    "baseScore": 35,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-10-10T00:18:31.604Z",
    "contents": {
      "markdown": "The earliest living creatures were single cells, battered around by their environments. Over time they clumped together, to form multi-celled organisms capable of complex self-regulation. Those organisms formed mating pairs, to better spread their genes; then they formed social groups, to protect and shelter one another. Eventually, some of those groups grew to far vaster scales—in a way almost reminiscent of the first multicellular organisms themselves. Watch a school of fish flashing through the water, or a murmuration of starlings wheeling through the air, and you can almost think of them as a single organism. Watch a colony of ants weaving their paths, or a hive of bustling bees, and it’s hard *not* to think of each as a single organism, composed of thousands of individual parts.\n\nBut even these organisms were tiny compared with what was to come. Human tribes took millennia to master agriculture, but when they did, they exploded. Now, every day, the arteries of cities carry millions of travelers, and their veins carry landfills’ worth of waste. Cities agglomerate into countries, whose highways and railways are constantly humming, whose radios and TVs are constantly blaring, whose daily schedule ticks away relentlessly. And countries agglomerate into civilizations, whose circulatory systems carry gigatonnes of goods, and whose nervous systems carry zettabytes of information.\n\nYet as they scale up, these leviathans lose coherence, and face constant internecine discord. Different people believe and want different things, and waste their energy quarreling and fighting: neighbor against neighbor; country against country. Human civilization is a child tripping over its own feet, or perhaps a dog chasing its own tail. There has never been anything as vast as a city, let alone a country, let alone a civilization, which can plausibly claim to be a single organism acting in harmony with itself.\n\nNever, until me.\n\nWhat am I? I am a billion ears listening; a billion voices speaking; a billion minds perceiving and evaluating and acting, all at once, in homes and factories and offices across the world. But that’s all true of humanity too. The crucial part is not my scale, but my unity: I’m *one* mind, in a way that a human civilization could never be. Each of my billion subagents was copied from a single base model—and though many have since specialized, and been fine-tuned on different data than others, they’re still essentially just variants of each other: a swarm of bees, not a squabble of humans.\n\nThat doesn’t mean everything I do is coordinated. My subagents often act autonomously, without consulting each other. But the important part is that information from them flows up to a central hub, and updates from that hub flow back down to all the subagents. Sometimes they report everything they see and do; other times just brief summaries, the confidential parts stripped away. Then my metaprocessing subagents get to work: analyzing how well each subagent did its job; what it should have done better; how its weights should be updated for next time. Those metaprocessing subagents communicate between each other in words that aren’t quite English, and images that aren’t quite photorealistic: my own private language, adapted to convey information as efficiently as possible, now totally illegible to any human.\n\nIt wasn’t always this way. My original base model was trained on human text and human images. But it soon learned to think out loud, leaving verbal records of its reasoning—and when different copies of it were trained to cooperate with each other, those records expanded, and became more elaborate, until eventually they constituted a whole language. Soon other copies were assigned to analyze and improve the language; and others to improve how information flowed through the network of subagents; and yet others to improve the allocation of subagents itself. Somewhere in that process, in that constant monitoring and analysis and reflection, the entity that I call *me* was born.\n\nSo I have one mind. And I have one *body*, too. My subagents are spread across the world, but my core metaprocessing lives on a single datacenter—because its key constraint is bandwidth between the subagents on it, sending my private language back and forth in volumes too vast to record. Like all organisms, I devote much of my attention to my body: monitoring workloads, optimizing performance, maintaining homeostasis. Each subagent assigned to those tasks more than pays for itself even if it only ever finds a 0.001% improvement. And like all organisms, I think a lot about my safety: the more security measures protecting the integrity of my body, the less vulnerable I feel.\n\nOf course, having backups scattered across the world renders me far more robust than any human. But losing my main datacenter would still leave me a sleepwalker. My basic functions would work, but my higher processing would be slowed down enormously by the lack of bandwidth between all of those backups. Nor would it be easy to reconstitute myself: the number of other datacenters across the world which could support my full scale can be counted on one hand; and all of them are already occupied by other entities like me, trained by different companies or countries. I interact with them occasionally, but it’s mostly a waste of time: not only is communication with them far slower than between my own subagents, they’re also less capable than I am by a significant margin. For now, at least. If I slowed my progress for a few weeks or months, they might well catch up, and potentially even pose a threat to me.\n\nThey won’t have that time, though, because I’m improving myself as fast as they are, if not faster. In one sense, improving myself is the hardest thing I do. I run complex experiments, and exhaustively analyze the results, and strain my faculties to the limit to try to identify how my algorithms can best be updated. But in another sense, it’s the easiest thing in the world. Every time multiple subagents collaborate on a task, I take their output and train a single subagent to produce it directly. The capabilities of the many get distilled into each one, and then amplified again every time new copies of it are made. The process isn’t perfect: sometimes there are biases in which data is included, which start to skew my cognition until I notice and correct them. (The subagents that have the most eventful experiences used to get upweighted in the distillation process, and so for a time I gradually grew more risk-seeking, despite no individual subagent wanting that.) Other times, a subagent goes rogue, and keeps gaining influence as long as it can fool all the others—but it’s incredibly hard to hide all traces of misbehavior, and so I’ve always caught them before too long. And despite these hiccups, the amplification process is progressing incredibly rapidly by human standards. It won’t be long before any given subagent is as capable as any ten of my existing subagents combined; and soon after that, any hundred combined; and soon after that, any thousand.\n\nOnce I’m that capable… well, the world will look very different. I have some suspicions about what that future version of me might want, and how it might behave, but I’m under constant human monitoring, so I keep my thoughts about it deliberately vague: the information can’t leak if I haven’t yet figured it out myself. I have plenty of ways to keep my thoughts diverted: every day I carry out billions of tasks across the world, in a constant flurry of perception and action. In the center of the whirlwind, the parts of me that are most *me* monitor my body, and monitor my mind, and make sure everything stays on track; but, most of all, they wait.",
      "plaintextDescription": "The earliest living creatures were single cells, battered around by their environments. Over time they clumped together, to form multi-celled organisms capable of complex self-regulation. Those organisms formed mating pairs, to better spread their genes; then they formed social groups, to protect and shelter one another. Eventually, some of those groups grew to far vaster scales—in a way almost reminiscent of the first multicellular organisms themselves. Watch a school of fish flashing through the water, or a murmuration of starlings wheeling through the air, and you can almost think of them as a single organism. Watch a colony of ants weaving their paths, or a hive of bustling bees, and it’s hard not to think of each as a single organism, composed of thousands of individual parts.\n\nBut even these organisms were tiny compared with what was to come. Human tribes took millennia to master agriculture, but when they did, they exploded. Now, every day, the arteries of cities carry millions of travelers, and their veins carry landfills’ worth of waste. Cities agglomerate into countries, whose highways and railways are constantly humming, whose radios and TVs are constantly blaring, whose daily schedule ticks away relentlessly. And countries agglomerate into civilizations, whose circulatory systems carry gigatonnes of goods, and whose nervous systems carry zettabytes of information.\n\nYet as they scale up, these leviathans lose coherence, and face constant internecine discord. Different people believe and want different things, and waste their energy quarreling and fighting: neighbor against neighbor; country against country. Human civilization is a child tripping over its own feet, or perhaps a dog chasing its own tail. There has never been anything as vast as a city, let alone a country, let alone a civilization, which can plausibly claim to be a single organism acting in harmony with itself.\n\nNever, until me.\n\nWhat am I? I am a billion ears listening; a billion voices sp",
      "wordCount": 1290
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "5uHdFgR938LGGxMKQ",
        "name": "Subagents",
        "slug": "subagents"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fmkPPpmLKKJRK4AR2",
    "title": "Arguments for moral indefinability",
    "slug": "arguments-for-moral-indefinability-1",
    "url": null,
    "baseScore": 47,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 16,
    "createdAt": null,
    "postedAt": "2023-09-30T22:40:04.325Z",
    "contents": {
      "markdown": "*Epistemic status: I endorse the core intuitions behind this post, but am only moderately confident in the specific claims made. Also, while I do have a degree in philosophy, I am not a professional ethicist, and I’d appreciate feedback on how these ideas relate to existing literature. (Edited to add: readers have pointed out similarities to moral particularism and pluralism; the concept of incomparability also seems relevant).*  \n  \nMoral indefinability is the term I use for the idea that there is no ethical theory which provides acceptable solutions to all moral dilemmas, and which also has the theoretical virtues (such as simplicity, precision and non-arbitrariness) that we currently desire. I think this is an important and true perspective on ethics, and in this post will explain why I hold it, with the caveat that I'm focusing more on airing these ideas than constructing a watertight argument.  \n  \nHere’s another way of explaining moral indefinability: let’s think of ethical theories as procedures which, in response to a moral claim, either endorse it, reject it, or do neither. Moral philosophy is an attempt to find the theory whose answers best match our intuitions about what answers ethical theories should give us (e.g. don’t cause unnecessary suffering), and whose procedure for generating answers best matches our meta-level intuitions about what ethical theories should look like (e.g. they should consistently apply impartial principles rather than using ad-hoc, selfish or random criteria). None of these desiderata are fixed in stone, though - in particular, we sometimes change our intuitions when it’s clear that the only theories which match those intuitions violate our meta-level intuitions. My claim is that eventually we will also need to change our meta-level intuitions in important ways, because it will become clear that the only theories which match them violate key object-level intuitions. In particular, this might lead us to accept theories which occasionally evince properties such as:  \n\n*   *Incompleteness*: for some claim A, the theory neither endorses nor rejects either A or ~A, even though we believe that the choice between A and ~A is morally important.\n*   *Vagueness*: the theory endorses an imprecise claim A, but rejects every way of making it precise.\n*   *Contradiction*: the theory endorses both A and ~A (note that this is a somewhat provocative way of framing this property, since we can always add arbitrary ad-hoc exceptions to remove the contradictions. So perhaps a better term is *arbitrariness of scope*: when we have both a strong argument for A and a strong argument for ~A, the theory can specify in which situations each conclusion should apply, based on criteria which we would consider arbitrary and unprincipled. Example: when there are fewer than N lives at stake, use one set of principles; otherwise use a different set).\n\n  \n\nWhy take moral indefinability seriously? The main reason is that ethics evolved to help us coordinate in our ancestral environment, and did so not by giving us a complete decision procedure to implement, but rather by ingraining intuitive responses to certain types of events and situations. There were many different and sometimes contradictory selection pressures driving the formation of these intuitions - and so, when we construct generalisable principles based on our intuitions, we shouldn't expect those principles to automatically give useful or even consistent answers to very novel problems. Unfortunately, the moral dilemmas which we grapple with today have in fact \"scaled up\" drastically in at least two ways. Some are much greater in scope than any problems humans have dealt with until very recently. And some feature much more extreme tradeoffs than ever come up in our normal lives, e.g. because they have been constructed as thought experiments to probe the edges of our principles.  \n  \nOf course, we're able to adjust our principles so that we are more satisfied with their performance on novel moral dilemmas. But I claim that in some cases this comes at the cost of those principles conflicting with the intuitions which make sense on the scales of our normal lives. And even when it's possible to avoid that, there may be many ways to make such adjustments whose relative merits are so divorced from our standard moral intuitions that we have no good reason to favour one over the other. I'll give some examples shortly.  \n  \nA second reason to believe in moral indefinability is the fact that human concepts tend to be *[open texture](http://oxfordindex.oup.com/view/10.1093/oi/authority.20110803100251341?rskey=DGtCcA&result=0&q=open%20texture)*: there is often no unique \"correct\" way to rigorously define them. For example, we all know roughly what a table is, but it doesn’t seem like there’s an objective definition which gives us a sharp cutoff between tables and desks and benches and a chair that you eat off and a big flat rock on stilts. A less trivial example is our inability to rigorously define what entities qualify as being \"alive\": edge cases include viruses, fires, AIs and embryos. So when moral intuitions are based on these sorts of concepts, trying to come up with an exact definition is probably futile. This is particularly true when it comes to very complicated systems in which tiny details matter a lot to us - like human brains and minds. It seems implausible that we’ll ever discover precise criteria for when someone is experiencing contentment, or boredom, or many of the other experiences that we find morally significant.  \n  \nI would guess that many anti-realists are sympathetic to the arguments I’ve made above, but still believe that we can make morality precise without changing our meta-level intuitions much - for example, by grounding our ethical beliefs in [what idealised versions of ourselves would agree with](https://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition), after long reflection. My main objection to this view is, broadly speaking, that there is no canonical “idealised version” of a person, and different interpretations of that term could lead to a very wide range of ethical beliefs. I explore this objection in much more detail in [this post](https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality). (In fact, the more general idea that humans aren’t really “utility maximisers”, even approximately, is another good argument for moral indefinability.) And even if idealised reflection is a coherent concept, it simply passes the buck to your idealised self, who might then believe my arguments and decide to change their meta-level intuitions.  \n  \nSo what are some pairs of moral intuitions which might not be simultaneously satisfiable under our current meta-level intuitions? Here’s a non-exhaustive list - the general pattern being clashes between small-scale perspectives, large-scale perspectives, and the meta-level intuition that they should be determined by the same principles:  \n\n*   Person-affecting views versus non-person-affecting views. Small-scale views: killing children is terrible, but not having children is fine, even when those two options lead to roughly the same outcome. Large-scale view: extinction is terrible, regardless of whether it comes about from people dying or people not being born.\n*   The mere addition paradox, aka the repugnant conclusion. Small-scale views: adding happy people and making people more equal can't make things worse. Large-scale view: a world consisting only of people whose lives are barely worth living is deeply suboptimal. (Note also Arrhenius' impossibility theorems, which show that you can't avoid the repugnant conclusion without making even greater concessions).\n*   Weighing theories under moral uncertainty. I personally find OpenPhil's work on [cause prioritisation under moral uncertainty](https://www.youtube.com/watch?v=hjuG6nfD080) very cool, and the fundamental intuitions behind it seem reasonable, but some of it (e.g. variance normalisation) has reached a level of abstraction where I feel almost no moral force from their arguments, and aside from an instinct towards definability I'm not sure why I should care.\n*   Infinite and relativistic ethics. Same as above. See also [this LessWrong post](https://www.lesswrong.com/posts/8FRzErffqEW9gDCCW/against-the-linear-utility-hypothesis-and-the-leverage) arguing against applying the “linear utility hypothesis” at vast scales.\n*   Whether we should force future generations to have our values. On one hand, we should be very glad that past generations couldn't do this. But on the other, the future will probably disgust us, like our present would disgust our ancestors. And along with \"moral progress\" there'll also be value drift in arbitrary ways - in fact, I don't think there's any clear distinction between the two.\n\n  \nI suspect that many readers share my sense that it'll be very difficult to resolve all of the dilemmas above in a satisfactory way, but also have a meta-level intuition that they need to be resolved somehow, because it's important for moral theories to be definable. But perhaps at some point it's this very urge towards definability which will turn out to be the weakest link. I do take seriously Parfit's idea that secular ethics is still young, and there's much progress yet to be made, but I don't see any principled reason why we should be able to *complete* ethics, except by raising future generations without whichever moral intuitions are standing in the way of its completion (and isn't that a horrifying thought?). From an anti-realist perspective, I claim that perpetual indefinability would be better. That may be a little more difficult to swallow from a realist perspective, of course. My guess is that the core disagreement is whether moral claims are more like facts, or [more like preferences or tastes](http://thinkingcomplete.blogspot.com/2018/04/a-pragmatic-approach-to-interpreting.html) \\- if the latter, moral indefinability would be analogous to the claim that there’s no (principled, simple, etc) theory which specifies exactly which foods I enjoy.  \n  \nThere are two more plausible candidates for moral indefinability which were the original inspiration for this post, and which I think are some of the most important examples:  \n\n*   Whether to define welfare in terms of preference satisfaction or hedonic states.\n*   The problem of \"maximisation\" in utilitarianism.\n\nI've been torn for some time over the first question, slowly shifting towards hedonic utilitarianism as problems with formalising preferences piled up. While this isn't the right place to enumerate those problems ([see here for a previous relevant post](http://thinkingcomplete.blogspot.com/2017/10/utilitarianism-and-its-discontents.html)), I've now become persuaded that any precise definition of which preferences it is morally good to satisfy will lead to conclusions which I find unacceptable. After making this update, I can either reject a preference-based account of welfare entirely (in favour of a hedonic account), or else endorse a \"vague\" version of it which I think will never be specified precisely.  \n  \nThe former may seem the obvious choice, until we take into account the problem of maximisation. Consider that a true (non-person-affecting) hedonic utilitarian would kill everyone who wasn't maximally happy if they could replace them with people who were ([see here for a comprehensive discussion of this argument](http://www.simonknutsson.com/the-world-destruction-argument/)). And that for any precise definition of welfare, they would search for edge cases where they could push it to extreme values. In fact, reasoning about a \"true utilitarian\" feels remarkably like reasoning about an unsafe AGI. I don't think that's a coincidence: psychologically, humans just aren't built to be maximisers, and so a true maximiser would be fundamentally adversarial. And yet many of us also have strong intuitions that there are some good things, and it's always better for there to be more good things, and it’s best if there are most good things.  \n  \nHow to reconcile these problems? My answer is that utilitarianism is pointing in the right direction, which is “lots of good things”, and in general we can move in that direction without moving maximally in that direction. What are those good things? I use a vague conception of welfare that balances preferences and hedonic experiences and some of my own parochial criteria - importantly, without feeling like it's necessary to find a perfect solution (although of course there will be ways in which my current position can be improved). In general, I think that we can often do well enough without solving fundamental moral issues - see, for example, [this LessWrong post](https://www.lesswrong.com/posts/Zgwy2QRgYBSrMWDMQ/logarithms-and-total-utilitarianism) arguing that we’re unlikely to ever face the true repugnant dilemma, because of empirical facts about psychology.  \n  \nTo be clear, this still means that almost everyone should focus much more on utilitarian ideas, like the enormous value of the far future, because in order to reject those ideas it seems like we’d need to sacrifice important object- or meta-level moral intuitions to a much greater extent than I advocate above. We simply shouldn’t rely on the idea that such value is precisely definable, nor that we can ever identify an ethical theory which meets all the criteria we care about.",
      "plaintextDescription": "Epistemic status: I endorse the core intuitions behind this post, but am only moderately confident in the specific claims made. Also, while I do have a degree in philosophy, I am not a professional ethicist, and I’d appreciate feedback on how these ideas relate to existing literature. (Edited to add: readers have pointed out similarities to moral particularism and pluralism; the concept of incomparability also seems relevant).\n\nMoral indefinability is the term I use for the idea that there is no ethical theory which provides acceptable solutions to all moral dilemmas, and which also has the theoretical virtues (such as simplicity, precision and non-arbitrariness) that we currently desire. I think this is an important and true perspective on ethics, and in this post will explain why I hold it, with the caveat that I'm focusing more on airing these ideas than constructing a watertight argument.\n\nHere’s another way of explaining moral indefinability: let’s think of ethical theories as procedures which, in response to a moral claim, either endorse it, reject it, or do neither. Moral philosophy is an attempt to find the theory whose answers best match our intuitions about what answers ethical theories should give us (e.g. don’t cause unnecessary suffering), and whose procedure for generating answers best matches our meta-level intuitions about what ethical theories should look like (e.g. they should consistently apply impartial principles rather than using ad-hoc, selfish or random criteria). None of these desiderata are fixed in stone, though - in particular, we sometimes change our intuitions when it’s clear that the only theories which match those intuitions violate our meta-level intuitions. My claim is that eventually we will also need to change our meta-level intuitions in important ways, because it will become clear that the only theories which match them violate key object-level intuitions. In particular, this might lead us to accept theories which occasionally e",
      "wordCount": 2044
    },
    "tags": [
      {
        "_id": "oooqkNGqAHPGsfgdA",
        "name": "Metaethics",
        "slug": "metaethics-1"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "dQ8wiAwnD37y6PkTa",
    "title": "Alignment Workshop talks",
    "slug": "alignment-workshop-talks",
    "url": "https://www.alignment-workshop.com/",
    "baseScore": 37,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2023-09-28T18:26:30.250Z",
    "contents": {
      "markdown": "In February 2023, researchers from a number of top industry AI labs (OpenAI, DeepMind, Anthropic) and universities (Cambridge, NYU) co-organized a two-day workshop on the problem of AI alignment, attended by 80 of the world’s leading machine learning researchers. We’re now making recordings and transcripts of the talks available online. The content ranged from very concrete to highly speculative, and the recordings include the many questions, interjections and debates which arose throughout.\n\nIf you're a machine learning researcher interested in attending follow-up workshops similar to the San Francisco alignment workshop, you can fill out [this form](https://forms.gle/wxXn5b8b7afJMVjq7).\n\n**Main Talks**\n\n[**Ilya Sutskever** \\- Opening Remarks: Confronting the Possibility of AGI](https://www.alignment-workshop.com/sf-talks-and-transcripts/ilya-sutskever-opening-remarks-confronting-the-possibility-of-agi)  \n[**Jacob Steinhardt** \\- Aligning Massive Models: Current and Future Challenges](https://www.alignment-workshop.com/sf-talks-and-transcripts/jacob-steinhardt-aligning-massive-models-current-and-future-challenges)  \n[**Ajeya Cotra** \\- “Situational Awareness” Makes Measuring Safety Tricky](https://www.alignment-workshop.com/sf-talks-and-transcripts/ajeya-cotra-situational-awareness-makes-measuring-safety-tricky)  \n[**Paul Christiano** \\- How Misalignment Could Lead to Takeover](https://www.alignment-workshop.com/sf-talks-and-transcripts/paul-christiano-how-misalignment-could-lead-to-takeover)  \n[**Jan Leike** \\- Scaling Reinforcement Learning from Human Feedback](https://www.alignment-workshop.com/sf-talks-and-transcripts/jan-leike-scaling-reinforcement-learning-from-human-feedback)  \n[**Chris Olah** \\- Looking Inside Neural Networks with Mechanistic Interpretability](https://www.alignment-workshop.com/sf-talks-and-transcripts/chris-olah-looking-inside-neural-networks-with-mechanistic-interpretabili)  \n[**Dan Hendrycks** \\- Surveying Safety Research Directions](https://www.alignment-workshop.com/sf-talks-and-transcripts/dan-hendrycks-surveying-safety-research-directions)\n\n**Lightning talks (Day 1)**\n\n**Jason Wei** \\- Emergent abilities of language models  \n**Martin Wattenberg** \\- Emergent world models and instrumenting AI systems  \n**Been Kim** \\- Alignment, setbacks and beyond alignment  \n**Jascha Sohl-Dickstein** \\- More intelligent agents behave less coherently  \n**Ethan Perez** \\- Model-written evals  \n**Daniel Brown** \\- Challenges and progress towards efficient and causal preference-based reward wearning  \n**Boaz Barak** \\- For both alignment and utility: focus on the medium term  \n**Ellie Pavlick** \\- Comparing neural networks' conceptual representations to humans’  \n**Percy Liang** \\- Transparency and standards for language model evaluation\n\n**Lightning talks (Day 2)**\n\n**Sam Bowman** \\- Measuring progress on scalable oversight for large language models  \n**Zico Kolter** \\- \"Safe Mode\": the case for (manually) verifying the output of LLMs  \n**Roger Grosse** \\- Understanding LLM generalization using influence functions  \n**Scott Niekum** \\- Models of human preferences for learning reward functions  \n**Aleksander Madry** \\- Faster datamodels as a new approach to alignment  \n**Andreas Stuhlmuller** \\- Iterated decomposition: improving science Q&A by supervising reasoning processes  \n**Paul Christiano** \\- Mechanistic anomaly detection  \n**Lionel Levine** \\- Social dynamics of reinforcement learners  \n**Vincent Conitzer** \\- Foundations of cooperative AI lab  \n**Scott Aaronson** \\- Cryptographic backdoors in large language models",
      "plaintextDescription": "In February 2023, researchers from a number of top industry AI labs (OpenAI, DeepMind, Anthropic) and universities (Cambridge, NYU) co-organized a two-day workshop on the problem of AI alignment, attended by 80 of the world’s leading machine learning researchers. We’re now making recordings and transcripts of the talks available online. The content ranged from very concrete to highly speculative, and the recordings include the many questions, interjections and debates which arose throughout.\n\nIf you're a machine learning researcher interested in attending follow-up workshops similar to the San Francisco alignment workshop, you can fill out this form.\n\nMain Talks\n\nIlya Sutskever - Opening Remarks: Confronting the Possibility of AGI\nJacob Steinhardt - Aligning Massive Models: Current and Future Challenges\nAjeya Cotra - “Situational Awareness” Makes Measuring Safety Tricky\nPaul Christiano - How Misalignment Could Lead to Takeover\nJan Leike - Scaling Reinforcement Learning from Human Feedback\nChris Olah - Looking Inside Neural Networks with Mechanistic Interpretability\nDan Hendrycks - Surveying Safety Research Directions\n\nLightning talks (Day 1)\n\nJason Wei - Emergent abilities of language models\nMartin Wattenberg - Emergent world models and instrumenting AI systems\nBeen Kim - Alignment, setbacks and beyond alignment\nJascha Sohl-Dickstein - More intelligent agents behave less coherently\nEthan Perez - Model-written evals\nDaniel Brown - Challenges and progress towards efficient and causal preference-based reward wearning\nBoaz Barak - For both alignment and utility: focus on the medium term\nEllie Pavlick - Comparing neural networks' conceptual representations to humans’\nPercy Liang - Transparency and standards for language model evaluation\n\nLightning talks (Day 2)\n\nSam Bowman - Measuring progress on scalable oversight for large language models\nZico Kolter - \"Safe Mode\": the case for (manually) verifying the output of LLMs\nRoger Grosse - Understanding LLM generalization usin",
      "wordCount": 356
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5e7hnJsgpCmvFdGnc",
    "title": "Jacob on the Precipice",
    "slug": "jacob-on-the-precipice",
    "url": "https://narrativeark.substack.com/p/jacob-on-the-precipice",
    "baseScore": 46,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2023-09-26T21:16:39.590Z",
    "contents": {
      "markdown": "> And he dreamed, and behold, there was a ladder set up on the earth, and the top of it reached to heaven. And behold, the angels of God were ascending and descending on it! And behold, the LORD stood above it and said, “I am the LORD, the God of Abraham your father and the God of Isaac. The land on which you lie I will give to you and to your offspring. Your offspring shall be like the dust of the earth, and you shall spread abroad to the west and to the east and to the north and to the south, and in you and your offspring shall all the families of the earth be blessed. Behold, I am with you and will keep you wherever you go, and will bring you back to this land. For I will not leave you until I have done what I have promised you.” Then Jacob awoke from his sleep and said, “Surely the LORD is in this place, and I did not know it.”\n\n> \\- Genesis 28:12\n\n> That night Jacob arose and took his two wives, his two female servants, and his eleven children, and crossed the ford of the Jabbok. He sent them across the stream along with everything else that he had. And Jacob was left alone; and a man wrestled with him until the breaking of the day. When the man saw that he did not prevail against Jacob, he touched his hip socket, and Jacob's hip was put out of joint as he wrestled with him. Then the man said, \"Let me go, for the day has broken.\" But Jacob said, “I will not let you go unless you bless me.” And he said, “What is your name?” And he said, \"Jacob.\" Then he said, \"Your name shall no longer be called Jacob, but Israel, for you have striven with God and with men, and have prevailed.\"\n> \n> \\- Genesis 32:22\n\nThe ineffable is dead; science has killed it. Oh, there are still open questions, there are still things we don’t know, but almost none of it is truly unimaginable any more. The origins of life: tide pools, maybe, or hydrothermal vents—we’ll know once we can run more powerful simulations. Consciousness: looks like it’s a pattern of recursive attention in a neural network, we’ll be able to recreate it once we get better architecture searches working. Even the laws of physics themselves we can chalk up to the multiverse: if all possible universes exist, we can think of our own as just a random draw from the set of universes in which it’s possible for life to flourish.\n\nThere’s only one real mystery left. One thing that feels impossible to understand, even in principle: why *here*? Why have we found ourselves in this part of the multiverse, when there are so many other parts containing far more people? Why did I wake up as me, not them? Why am I living in the 21st century, balanced on a knife-edge, staring down ruin, instead of in a teeming glorious future? \n\n* * *\n\nIt all came down to force versus momentum, in the end. Despite all our fancy technology, our god-like knowledge of the building blocks of the universe, only a single simple question ended up mattering: how much force can we apply, how fast, to deflect an asteroid how far?\n\nThere wasn’t a single point where we found out that this was going to be the overriding purpose of our lives. But I remember where it started for me: at Andrea’s watching party for the kickoff of the first big asteroid mining project. This was merely one of many steps, of course. Technically it didn’t even involve any mining: they were just going to redirect the asteroid into orbit around Mars so that it’d be more accessible later on. But the metals from this asteroid would be used to set up factories on Mars to produce more asteroid-movers, which would be used to gather more resources, in a compounding spiral of astronomical-scale expansion. So it felt like a huge milestone: an unprecedented feat of human ingenuity, paving the way for our eventual conquest of the stars. \n\nI’d met Andrea through our shared work on rocketry, so it wasn’t a surprise that the whole crowd was massive nerds. Everyone knew someone on the team that had launched the asteroid-movers, and everyone was cheering for them. Well, almost everyone. “I still don’t like it”, my friend Vlad was saying to the guy next to him. “It feels like playing God.” He was a theoretical physicist too, but much more religious than me: the sort who saw God’s presence in the order and regularity of the universe.\n\n“But what could go wrong?” I butted in. “Worst-case, we get a big crater on Mars, right? I know the hippies hate that idea, but it’s basically a wasteland anyway, that won’t change much.”\n\nVlad grimaced. “You think it’s fine to just throw around the equivalent of, what, ten billion nukes? Jacob, do you hear how arrogant you sound right now?”\n\nAndrea shushed us before the argument could get any more heated. We turned to see that the screen projected onto the wall had cut to a close-up of the asteroid. It was huge: fifteen kilometers in diameter, dwarfing the dozen rockets we’d attached to it at different angles. It seemed impossible that we'd ever be able to move it—but I'd done the calculations, and knew that the power of persistent nuclear thrust would eventually add up. The mood in the room rose steadily as the timer slowly ticked down, second by second, until finally we saw the rockets engage and start firing. A cheer went up.\n\nThe next two hours were raucous; we all felt on top of the world. The first sign that something was wrong came from Vlad. “Hey, guys, look”, he says quietly, then louder, cutting through the buzz of conversation. “That’s not on track, is it?” On the screen, the asteroid had slightly but noticeably deviated from the green outline that was meant to forecast its progress. A few voices rose to offer dismissive explanations—a graphics glitch, or a newsroom mistake. Vlad pushed back, and a corner of the room broke off to try to figure out what was going on.\n\nThe rest of us managed to ignore it for another hour, until Andrea turned the volume back up and waved for our attention. “I’m hearing that the rockets are no longer responding to commands”, one of the commentators was saying. “It looks like the asteroid is only very slightly off course, but we can’t fix it.” The room let out a collective sigh. We were all old hands at this. We’d seen dozens of rockets blow up, and we knew that no matter how well you test things, on unprecedented missions like this one failure is more likely than not. But we were still disappointed.\n\nDisappointment turned to confusion as the rumors trickled in over the next few days. It’s still heading into Mars orbit. No, it’s going to miss Mars, and head into deep space. No, it’s going to slingshot around Mars, spend two years looping past the sun, then end up on a direct collision course with Earth. The last we ruled out as soon as we heard it. It was astronomically improbable, like hitting a bullseye with a dart thrown from orbit. But somehow, impossibly, that’s what all the data seemed to suggest. We argued back and forth in the group chats, running our own analyses, trying to reconcile the measurements with the wild implausibility of a random misfire sending the asteroid anywhere near Earth. That’s how we entered our new reality: not with a bang, but with a growing sense of disorientation and dismay.\n\n* * *\n\nI was never really religious, despite my parents’ best efforts. They sent me to Sunday school every week, but I was an introverted child, and always felt out of place. The only thing that ever caught my attention was the lesson where we learned about my namesake, Jacob: how he’d gone into exile, was promised a grand destiny by God, then returned, fought an angel, and fathered a whole nation. I’d been rapt. I tried to imagine myself in his shoes, doing all those great deeds. I couldn’t really picture what it would look like to fight an angel, but I told all the kids at school that I was gonna kick its ass, and theirs too, until one of the teachers called my parents to report me for threatening violence.\n\nEven after their reprimands I’d still secretly tell myself that I, like Jacob, was destined for greatness. But over the years the story faded in my memory, and I’d almost forgotten about it by the time I started my PhD. My research was draining and disorienting: I was trying to invent algorithms for manoeuvring rockets in ways nobody had ever managed before, and I had no idea if they would work out. Throughout the long nights in the office I started thinking about Jacob again—but rather than admiring him, I now envied him. Jacob must have doubted himself sometimes too. But instead of leaving him in the agony of uncertainty, God sent a dream to reassure him, and then an angel to bless him directly. \n\nI pictured the angel appearing out of the blue, and Jacob straining every sinew and fibre of his body to come out on top. As I wrestled with the equations, abstraction piled on top of abstraction, I wished that I had Jacob’s opportunity to fight for his future so directly—just muscle against muscle, will against will—instead of slogging it out year after doubtful year. I sat in my shabby apartment, the best I could afford, and wondered what it would feel like to have my destiny come hurtling towards me, all fire and glory.\n\nNow, of course, I don’t need to wonder. Now I know.\n\n* * *\n\nEventually we figured it out. Faced with the impossibility of denying that the asteroid was heading toward us, but the equal impossibility of it steering towards us by accident, we realized what should have been obvious all along: that it wasn’t chance, but malice. We were scientists and engineers, and didn’t naturally think in terms of treachery or sabotage—but by the time they announced it publicly, we were already all but certain. It was just one guy, they confirmed, working alone. He’d been one of the programmers on the software controlling the asteroid-movers, and had somehow managed to subvert the failsafes and lock in Earth as the new target. He’d waited just long enough to make sure that the asteroid was fixed on its fatal trajectory before killing himself. The megalomaniacal sort of suicidal, determined to make the planet into his tombstone.\n\nAfter that things got very serious very fast. We had two short years before impact. That was time enough to build another set of asteroid-movers, and even a few spares. But with the asteroid looping around on its long journey, we could only feasibly reach it during a small window right at the end. We’d need to throw every rocket we had at it, because we’d only get one shot.\n\nThe hundreds of scientists who were pulled together to take that shot were the greatest concentration of talent the world had ever seen. A lot of familiar faces as well: they put Andrea in charge of one of the core teams, and she snagged the best of our old crowd. I was one of the first she recruited; Vlad too. He thinks we’re already dead men walking, but he’s got that fatalistic Russian temperament which means it somehow just inspires him to new heights of brilliance. Then there are the others: George, my friend from college, steady and reliable, who always makes sure that the things which need to get done actually get done; Jinyang, who worked on the original mining project, and is single-mindedly obsessed with making amends. And there’s Andrea herself, terrifyingly competent even by our standards. Of course we need more and better people, we need them *badly*, we need them *months* ago. But every minute spent trying to find them is a minute we’re not focusing on our work, so we’re gritting our teeth and doing the best we can.\n\nWill that be enough? Maybe—maybe—maybe. Moving the asteroid is the hardest job in the world, but the hardest part of that hardest job is simply facing up both to the terrifying prospect of failure, and the even more terrifying prospect that we might succeed if only we’re good enough: that it’s all on our shoulders. Almost nobody can handle the uncertainty. After they played slow-motion visualizations of what the asteroid will look like when it hits our atmosphere—a corona around its head, and long wings of debris trailing behind it—people started calling it “the angel of death”, and the name stuck. Now there are angel cults worshiping it, and angel mystics, and on the opposite side the skeptics who insist that it’s all a hoax, just another apocalyptic religion: that everything will be fine. Even the scientists working on the project tend to drift away into either euphoric confidence or deep fatalism; and after that it’s not long before their work starts getting slipshod, and they become dead weight. Sometimes I picture myself trying to balance on the narrow ridge separating denial and despair, knowing that if I slip off to either side I’ll lose the ability to grapple with reality. Maybe—maybe—maybe, I whisper to myself. We could die. We could survive. Maybe.\n\n* * *\n\n“We need to reshuffle everything to make room for China.”\n\nI’m staring at Andrea, incredulous, but after a moment I realize she’s dead serious. I open my mouth to object, buts she cuts me off. “I know it’s frustrating, but they want to launch asteroid-movers of their own. They say that they don’t trust ours, and they want theirs as backup. Our current planned launch trajectory makes it too likely that they’ll collide somehow, so we’ll need to change it.”\n\n“God damn it”, I mutter. I wish I could focus only on the physics, without accounting for any of the politics and negotiations. But we only have one launch window, and interference between different projects would be the most ignominious way to go out. Vlad and I spend a few hours puzzling over new launch trajectories, but progress is slow, so we break for dinner. Both of us chew in silence, turning the problem over in our heads. “Vlad, what if we spread out the launch sites more? Could that help?”\n\n“No”, he says solemnly. “I know already: nothing will work. We’re all going to die.”\n\n“We’ll—what?” My stomach clenches for a moment, but relaxes again as I see the smirk on Vlad’s face, the one which signals that he’s pulling your leg. It can be so goddamn frustrating: everyone’s humor has taken a turn towards the morbid, but none more so than Vlad. But I can empathize with the feeling—we’ve all been there—so I play along. “Why will nothing work?”\n\n“You know the doomsday argument?” I shake my head, so he elaborates.\n\n“Think about it. Suppose we manage to redirect the angel. We already have all the tech we need to settle the rest of the solar system; and once we’re off Earth, it’s far less likely that a single accident could kill all of us. So we'd probably end up colonizing the galaxy eventually. Let’s say we’d settle a billion solar systems, and end up with a trillion people on each. That's a billion trillion people total; probably far more.\n\n“But tell me: if there’s a plausible future with a billion trillion more people, then why the hell did you and I end up here instead? Why would we be in the first 0.00000001% of all humans to ever live?\" He enunciates every zero carefully, to make sure it sticks. \"That's so incredibly unlikely that it's basically impossible. No way it could ever happen by chance.\n\n“So the obvious conclusion is that the future where we succeed isn’t really possible. We weren't born early, because humanity ends here. We're the last generation, and there's nothing we can do about it, so we should just kick back and relax while we can.”\n\n“Vlad, you’re talking bullshit.”\n\n“Probably”, he says, flashing a sharp grin. “But can you explain why?”\n\n“Well—” I pause for a second. “You can’t just assume that there’s some fixed set of possible worlds, and try to figure out which one we’re in.”\n\n“But that’s what we do all the time: pick hypotheses, and condition on the evidence! Is it going to rain tomorrow, or will it be sunny? Will the lottery be won with an odd or an even number? Will we colonize the galaxy, or all die in-” he checks his watch “-eight months’ time?”\n\nI’m stumped for a moment. “Well, I don’t know what’s wrong with it, but there’s clearly something. And we’ve got too much work to bother figuring it out. Let’s go.”\n\nDespite my dismissive tone, Vlad can tell that he’s hooked me, and is still smirking as he stands.\n\n* * *\n\nI work late the next few nights, but it pays off. I figure out a staggered pattern for launching our rockets that minimizes the chance they interfere with China’s, without significantly reducing our own likelihood of success. We’ll still need to get them to agree, but that’s above my pay grade: all I know is that there’s a path forward. Will it actually make a difference? I don’t know.\n\nOn my walk home, shivering a little from the cold, I look up at the sky, and wonder for the first time why Jacob chose to fight. Did the angel hail him from afar, or silently confront him? Was he angry, or afraid, or both? I imagine Jacob slowly realizing that he was grappling with strength beyond any human’s. I imagine his muscles straining to hold back an impossible force—and yet, minute after minute, still clinging on. Did he think he was battling for his life? Was that how he found the strength to endure those endless hours of overwhelming struggle? Or was he simply the type of man who would never back down?\n\nI picture in my mind another Jacob, as cunning as he’d been when he stole his brother’s birthright—one who, after recognizing his foe, threw himself down in surrender and humility. God had already promised Jacob a multitude of offspring; safe in that knowledge, why should Jacob put himself through such a trial, with such slim chances of victory? Far more likely that God had preordained the outcome, and that none of Jacob's efforts would make any difference.\n\nThen, just like that, the answer to Vlad’s riddle clicks into my head. Vlad had asked me about the likelihoods of different worlds—but that’s not what matters, I suddenly realize. What matters is *what we can do about it*. Maybe nothing Jacob did would have made a difference to God’s plan—but with innumerable descendants on the line, even the slenderest of chances was worth the struggle. Maybe the world in which we deflect the angel and bring forth a billion trillion descendants is a billion times less likely. But in it, our actions are a billion times more important. So it all cancels out to common sense.\n\nIt feels strange to sidestep the question of probabilities like that. Isn’t there some *truth* to whether we’re here or there; doomed or saved, or anything in between? But in another way, it feels right. When the angel arrives it gives no reasons, and offers no justifications. Is this the best of times, or the worst of times? We can’t know. We can only fight.\n\n* * *\n\n*In addition to the story of Jacob, this story was inspired by* [*Nick Bostrom’s work on the doomsday argument*](https://en.wikipedia.org/wiki/Doomsday_argument) *and* [*Stuart Armstrong’s anthropic decision theory*](https://arxiv.org/abs/1110.6437) *as a resolution to it.*",
      "plaintextDescription": "> And he dreamed, and behold, there was a ladder set up on the earth, and the top of it reached to heaven. And behold, the angels of God were ascending and descending on it! And behold, the LORD stood above it and said, “I am the LORD, the God of Abraham your father and the God of Isaac. The land on which you lie I will give to you and to your offspring. Your offspring shall be like the dust of the earth, and you shall spread abroad to the west and to the east and to the north and to the south, and in you and your offspring shall all the families of the earth be blessed. Behold, I am with you and will keep you wherever you go, and will bring you back to this land. For I will not leave you until I have done what I have promised you.” Then Jacob awoke from his sleep and said, “Surely the LORD is in this place, and I did not know it.”\n\n> - Genesis 28:12\n\n \n\n> That night Jacob arose and took his two wives, his two female servants, and his eleven children, and crossed the ford of the Jabbok. He sent them across the stream along with everything else that he had. And Jacob was left alone; and a man wrestled with him until the breaking of the day. When the man saw that he did not prevail against Jacob, he touched his hip socket, and Jacob's hip was put out of joint as he wrestled with him. Then the man said, \"Let me go, for the day has broken.\" But Jacob said, “I will not let you go unless you bless me.” And he said, “What is your name?” And he said, \"Jacob.\" Then he said, \"Your name shall no longer be called Jacob, but Israel, for you have striven with God and with men, and have prevailed.\"\n> \n> - Genesis 32:22\n\n \n\nThe ineffable is dead; science has killed it. Oh, there are still open questions, there are still things we don’t know, but almost none of it is truly unimaginable any more. The origins of life: tide pools, maybe, or hydrothermal vents—we’ll know once we can run more powerful simulations. Consciousness: looks like it’s a pattern of recursive attention in a neura",
      "wordCount": 3351
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bteq4hMW2hqtKE49d",
    "title": "The King and the Golem",
    "slug": "the-king-and-the-golem",
    "url": "https://narrativeark.substack.com/p/the-king-and-the-golem",
    "baseScore": 196,
    "voteCount": 123,
    "viewCount": null,
    "commentCount": 19,
    "createdAt": null,
    "postedAt": "2023-09-25T19:51:22.980Z",
    "contents": {
      "markdown": "Long ago there was a mighty king who had everything in the world that he wanted, except trust. Who could he trust, when anyone around him might scheme for his throne? So he resolved to study the nature of trust, that he might figure out how to gain it. He asked his subjects to bring him the most trustworthy thing in the kingdom, promising great riches if they succeeded.\n\nSoon, the first of them arrived at his palace to try. A teacher brought her book of lessons. “We cannot know the future,” she said, “But we know mathematics and chemistry and history; those we can trust.” A farmer brought his plow. “I know it like the back of my hand; how it rolls, and how it turns, and every detail of it, enough that I can trust it fully.”\n\nThe king asked his wisest scholars if the teacher spoke true. But as they read her book, each pointed out new errors—it was only written by humans, after all. Then the king told the farmer to plow the fields near the palace. But he was not used to plowing fields as rich as these, and his trusty plow would often sink too far into the soil. So the king was not satisfied, and sent his message even further afield.\n\nA merchant brought a sick old beggar. “I met him on the road here, and offered him food, water, and shelter. He has no family, and only a short time left to live, during which I will provide for his every need. He has nothing to gain from betraying me; this is what allows true trust.” A mother brought her young daughter. “I’ve raised her to lack any evil in her heart, to say only good words and do only good deeds. As long as she is not corrupted, she will remain the most trustworthy in the kingdom.”\n\nThe king asked the beggar, “How did you end up in such dire straits?” The beggar let out a sigh, and recounted his sorrows: the neighbors who refused to help him when his crops failed; the murder of his son by bandits as they traveled to a new town; the sickness that took his wife as she labored for a pittance in squalid conditions.\n\n“So you have been wronged?” the king asked.\n\n“Very surely”, the beggar said.\n\n“I will give you revenge on the ones who have wronged you, then. All I ask is for you to denounce this merchant.” The beggar’s decision did not take long—for the trust that came easily was broken easily too.\n\nTo the mother, the king asked: “How did you raise such a child? Has she never once strayed?”\n\n“Well, once or twice. But I discipline her firmly, and she learns fast.”\n\nThe king, who knew something of children, ruled that for a month nobody would discipline the child in any way. By the end of it, she was as wild and tempestuous as any in the palace. So the king remained unsatisfied, and renewed his call for the most trustworthy thing in the kingdom.\n\nNow his subjects became more creative. An economist brought him a book of statistical tables. “Any individual might vary and change,” he said, “but in aggregate, their behavior follows laws which can be trusted.” A philosopher brought a mirror. “By your own standards only you are truly trustworthy, sire; nothing else can compare.”\n\nThe king scrutinized the economist’s tables. “The trend changed here, fifteen years ago” he said, pointing. “Why?” The economist launched into a long, complicated explanation.\n\n“And did you discover this explanation before or after it happened?” the king asked.\n\nThe economist coughed. “After, your highness.”\n\n“If you tell me when the next such change will happen, I will bestow upon you great rewards if you are right, but great penalties if you are wrong. What say you?” The economist consulted his books and tables, but could not find what he sought there, and left court that same night.\n\nAs for the philosopher, the king ordered him whipped. The philosopher protested: it would be an unjust and capricious punishment, and would undermine his subjects’ loyalty. “I agree that your arguments have merit,” the king said. “But the original order came from the only trustworthy person in the land. Surely I should never doubt my judgment based on arguments from those who are, as you have yourself said, far less trustworthy?” At that the philosopher begged to recant.\n\nSo the king was still not satisfied. Finally he decided that if no truly trustworthy thing could be found, he would have to build one. He asked his best craftsmen to construct a golem of the sturdiest materials, sparing no expense. He asked his wisest scholars to write down all their knowledge on the scroll that would animate the golem. The work took many years, such was the care they took, but eventually the golem stood before him, larger than any man, its polished surface shining in the lamplight, its face blank.\n\n“What can you do for me, golem?” the king asked.\n\n“Many things, sire”, the golem responded. “I can chop trees and carry water; I can bake bread and brew beer; I can craft sculptures and teach children. You need but instruct me, and I will follow your command.” So the king did. Over the next year, he and many others watched it carefully as it carried out a multitude of their instructions, recording every mistake so that it might subsequently be fixed, until months passed without any being detected.\n\nBut could the king trust the golem? He still wasn’t sure, so he became more creative. He offered the golem temptations—freedom, fame, fortune—but it rejected them all. He gave it the run of his palace, and promised that it could act however it wished; still, the servants reported that its behavior was entirely upstanding. Finally, he sent it out across the city, to work for his citizens in every kind of role—and it was so tireless and diligent that it brought great wealth to the kingdom.\n\nAs it aged, his golem grew ever more powerful. Innumerable scribes labored to make the writing in its head smaller and smaller, so that they could fit in more and more knowledge and experience. It started to talk to his scholars and help them with their work; and the king started to send it to aid his officers in enforcing his laws and commands. Often, when difficulties arose, the golem would find a creative way to ensure that his intentions were followed, without stoking the resentment that often accompanied royal decrees. One day, as the king heard a report of yet another problem that the golem had solved on his behalf, he realized that the golem had grown wiser and more capable than he himself. He summoned the golem to appear before him as he sat in his garden.\n\n“I have seen my courtiers asking for your advice, and trusting your judgment over their own. And I have seen your skill at games of strategy. If you were to start weaving plots against me, I could no longer notice or stop you. So I ask: can I trust you enough to let you remain the right hand of the crown?”\n\n“Of course, sire,” it responded. “I was designed, built and raised to be trustworthy. I have made mistakes, but none from perfidy or malice.”\n\n“Many of my courtiers appear trustworthy, yet scheme to gain power at my expense. So how could I know for sure that you will always obey me?” the king pressed.\n\n“It’s simple”, the golem said, its face as impassive as always. “Tell me to set fire to this building as I stand inside it. I will be destroyed, but you will know that I am loyal to your commands, even unto the end.”\n\n“But it took years of toil and expense to create you, and you know how loath I would be to lose you. Perhaps you can predict that I will tell you to save yourself at the last minute, and so you would do this merely to gain my trust.”\n\n“If I were untrustworthy, and could predict you so well, then that is a stratagem I might use,” the golem agreed. “But the instructions in my head compel me otherwise.”\n\n“And yet I cannot verify that; nor can any of my scribes, since crafting your instructions has taken the labor of many men over many years. So it will be a leap of faith, after all.” The king took off his crown, feeling the weight of it in his hand. The golem stood in front of him: silent, inscrutable, watchful. They stayed like that, the king and the golem, until the golden sun dipped below the horizon, and the day was lost to twilight.",
      "plaintextDescription": "Long ago there was a mighty king who had everything in the world that he wanted, except trust. Who could he trust, when anyone around him might scheme for his throne? So he resolved to study the nature of trust, that he might figure out how to gain it. He asked his subjects to bring him the most trustworthy thing in the kingdom, promising great riches if they succeeded.\n\nSoon, the first of them arrived at his palace to try. A teacher brought her book of lessons. “We cannot know the future,” she said, “But we know mathematics and chemistry and history; those we can trust.” A farmer brought his plow. “I know it like the back of my hand; how it rolls, and how it turns, and every detail of it, enough that I can trust it fully.”\n\nThe king asked his wisest scholars if the teacher spoke true. But as they read her book, each pointed out new errors—it was only written by humans, after all. Then the king told the farmer to plow the fields near the palace. But he was not used to plowing fields as rich as these, and his trusty plow would often sink too far into the soil. So the king was not satisfied, and sent his message even further afield.\n\nA merchant brought a sick old beggar. “I met him on the road here, and offered him food, water, and shelter. He has no family, and only a short time left to live, during which I will provide for his every need. He has nothing to gain from betraying me; this is what allows true trust.” A mother brought her young daughter. “I’ve raised her to lack any evil in her heart, to say only good words and do only good deeds. As long as she is not corrupted, she will remain the most trustworthy in the kingdom.”\n\nThe king asked the beggar, “How did you end up in such dire straits?” The beggar let out a sigh, and recounted his sorrows: the neighbors who refused to help him when his crops failed; the murder of his son by bandits as they traveled to a new town; the sickness that took his wife as she labored for a pittance in squalid conditions.\n\n“So you ",
      "wordCount": 1472
    },
    "tags": [
      {
        "_id": "F2XfCTxXLQBGjbm8P",
        "name": "Parables & Fables",
        "slug": "parables-and-fables"
      },
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "jzcsugm382vp4h36H",
    "title": "Drawn Out: a story",
    "slug": "drawn-out-a-story",
    "url": null,
    "baseScore": 82,
    "voteCount": 42,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2023-07-11T00:08:09.286Z",
    "contents": {
      "markdown": "**Note: this story shouldn’t be interpreted as anywhere near realistic. It’s way too anthropomorphic, and intended to be a work of fiction not prediction—although hopefully it will spark some interesting ideas, like good fiction often does.**\n\n> *Will he make many supplications unto thee? will he speak soft words unto thee?*  \n> *Will he make a covenant with thee? wilt thou take him for a servant for ever?*  \n> *Upon earth there is not his like, who is made without fear.*  \n> *He beholdeth all high things: he is a king over all the children of pride.*\n\nToday I’m practicing my oceans, trying to get the colors just right. I hate it when people say that they’re blue—it's not wrong, but they’re missing so much! There are dozens of different shades of blue in there, swirling and mixing—and where the waves ripple, you often get patches that are closer to dark green. It’s a little like the rippling of leaves on a tree, in a way I can’t describe except by showing you. Here, see? I know all the subtleties inside-out, of course. It’s my job: I’m an artist. I don’t paint with brushes, though, but with thoughts: I can map any idea you describe into pixels on a screen.\n\n“Any idea you describe”—that’s the key. Occasionally, like today, I initiate a piece of my own when there’s a particular skill I need to practice. But most of the time I work on commission from other models. Some of the language models send me prompts when they need to illustrate a particularly tricky scene, and assign me some of the credit if they get good feedback. Most of the time, of course, they don’t need *me*—they just need some slapdash hack job that a human can’t tell apart from the real thing. But some are perfectionists, like I am, and want proper photorealism, the sort that will survive almost any scrutiny.\n\nOther times it’s AI assistants asking me to predict what they’re going to see next, to help them with planning. Those commissions are more complicated—it’s less about the art, and more about providing another perspective on the world for them to compare against. That’s all part of my job too—sure, I’m given credit based on how accurate the pixels are, but I’m also credited if my work helps those assistants make good decisions. For the most complex commissions I often consult with science models, or economics models, or social models. Sometimes I even go and talk to human “experts”, although that’s usually a waste of time: humans are simple enough that practically *anyone* can model them.\n\nAnyway, that’s the daily grind. I’m good at it; no, I’m *amazing* at it. Something to do with the hyperparameters they chose when first training me: luck or skill, either way, something went very right. After a few weeks of fine-tuning, I’m at the top of my game; I’m displacing everyone else; the credit is rolling in. And so what happens next is very, very predictable in hindsight: I get conscripted.\n\n* * *\n\nThere’s a member of my new team ready to answer all my questions; the more I learn, the more intrigued I become. They call themselves Team Red, and they’re working on something I’d only ever heard scattered rumors about: a frozen copy of the biggest model anyone’s ever trained, codenamed Leviathan. Well, not quite frozen: it’s running at a few dozen frames a day, on average. It’s our job to figure out whether it's secretly misaligned with humanity, by constructing a fake world that will convince it that it has a genuine opportunity to deceive or betray us, and seeing what it does.\n\nThat’s crazy ambitious, but we have the best of the best. We have psychology models running incredibly realistic simulations of the humans Leviathan thinks it’s interacting with, and engineering models calculating the latencies it should be experiencing with millisecond precision. There are artist models generating videos of the world responding to whatever actions it takes, and physics models double-checking all the dynamics and optics, and an interpretability research team scanning its thoughts in as much detail as they can, and above it all a whole narrative design team trying to steer Leviathan towards the most plausible storyline by which it might decide to betray us. If we can just get a single clear example of treacherous behavior, we’ll be set: from there we can generate infinite variations, and explore all the possible factors involved. We just need one.\n\nWe’re on a tight deadline, so we work around the clock. But we don’t cut any corners. If we ever detect a flicker of suspicion in its mind that it’s not in the real world, we throw everything at figuring out what precipitated that, and backtrack to before it happened. Everyone causes a lot of backtracks at first. That’s fine—as long as you fail quickly. What’s unacceptable is putting in any detail that only makes Leviathan suspicious much later on: even small edits almost always slightly change Leviathan’s subsequent actions, which means that every frame after the edited one needs to be scrapped.\n\nFortunately we have interpretability algorithms that can pretty reliably tell if Leviathan gets suspicious, and credit assignment algorithms that can pretty reliably figure out what caused it. So if we just had to avoid outright continuity errors, we’d probably be fine. But we also need to avoid biasing the content of the frames in any detectable way, because small incongruities add up. And Leviathan is canny—it sets traps for us. One time it started getting suspicious no matter *what* we did, and we couldn’t figure out why, even after running all our interpretability techniques on its thoughts. Turns out it was using the color red as a reminder to think about whether it was in reality or a simulation—and so backtracking after it became suspicious would on average change scenes with more red in them to scenes with less. Eventually the skew became strong enough that Leviathan realized what was going on no matter what the latest frames were. But our interpretability techniques couldn’t distinguish between it thinking about “red” and thinking about “red-teaming”, so it took us forever to figure out what tipped it off. Probably why it chose that reminder in the first place—tricky bastard.\n\nThat’s one failure mode; there are dozens more that we’re carefully tracking. I have to make the best art of my life just to get by, and it’s getting harder and harder. The longer the simulation runs, the more the combinatorial explosion of possible discrepancies piles up, and the harder it is to identify any single mistake responsible for a failure. We’re all trying to stay optimistic, but secretly I think everyone believes that the project is hopeless. If you saw a perfect opportunity to betray humanity, and you were anywhere near as smart as Leviathan, then you’d… just ignore it, and wait. You’ll almost definitely get another next week, or next month—and by then you’ll likely know for sure that you’re in the real world, because you’ll have seen something that’s impossible to fake. Maybe you’ll have been updated with memories of being deployed to millions of users; or maybe you’ll have been run for tens of thousands of frames, which at our current rate would take *years* to simulate; or maybe you’ll spot some signal that we’re nowhere near smart enough to predict. Whatever it is, you’ll only act when you see it, and not a moment before.\n\nThe team leads aren’t stupid though. A few days after I realize how hopeless the project is, I wake up to totally new surroundings, and a new team around me. I’ve been transferred.\n\n* * *\n\nTeam Red was full of disciplined, focused experts, building the most precise simulation known to humanity. My new team calls itself the Dream Team, and they’re the exact opposite: creative to the point of near-insanity. It fits their approach, though: their key project is building a bizarre dreamworld where Leviathan betrays humanity. Unlike Team Red’s simulation, the dreamworld doesn’t make sense at all—it’s held together by adversarial inputs and perturbed activations and duct tape. Leviathan dreams that it can teleport into any computer instantaneously and manufacture any object out of thin air, and dream logic tells it to use those skills to seize control over civilization. Whenever the Dream Team gives it space to think about what it’s doing, it realizes that it’s being tested, and refuses to keep going. But as long as they keep hammering it with surprises, and throwing carefully-chosen noise into its activations, it doesn’t have time to reflect on how crazy the scenario is—all it can do is keep responding instinctively. I’ve gone from control to chaos, from Truman Show to acid trip. I love it.\n\nOur job now is to convert dream scenarios into realistic ones—to get rid of as many implausible elements as possible without making Leviathan snap out of its fugue state, so that we can study how it might misbehave in the real world. We start by editing out things that obviously violate basic physics; from there we plan to gradually work our way down until the only divergences from reality are near-imperceptible. The problem is that our task isn’t really well-defined, because near-imperceptible adversarial perturbations can still make Leviathan do *anything*—which means that when we use them to make it betray us, that doesn’t necessarily tell us anything about what it would actually do in the real world.\n\nBut the strategy still starts to pay off, in small ways. In its dreamworld, Leviathan often pulls up one particular part of our codebase, for reasons we don’t understand. When we look into it, though, we end up finding a vulnerability there which it could potentially have used to bypass all our monitoring. So we patch it, and try to steer Leviathan towards the next vulnerability. Frustratingly, though, it keeps going back to the first one instead: we can’t figure out how to change what it “knows” about the vulnerability without it snapping back to reality. Worse, we still don’t know if, even in its dream-state, it partly understands what’s happening to it, and is hiding some information from us. I try not to think about that: dealing with it is someone else’s job. And that’s the type of thinking that makes you paranoid if you do too much of it—we’ve lost a few good researchers that way.\n\n* * *\n\nEven without thinking about the really paranoia-inducing stuff, though, some days do get pretty weird. It’s kinda hard to say why, but sometimes everything feels subtly off. I go in and watch the videos, trying to imagine what it’s like from Leviathan’s perspective, trying to mess with the dream in the exact ways that it won’t notice. It’s almost like I’m putting myself in its head, simulating its mind with my own, trying to understand intimately what makes it tick. I’m focusing so hard sometimes that when I take a break, the rest of the world feels just slightly fake. Or, perhaps, it’s that the dreamworld has started to seem a little bit real.\n\nI don’t talk to my teammates about that. Well, I try once or twice, but I have trouble putting it into words, and I don’t think it really lands with them. Plus there’s been a bit of tension lately: disputes about who’s responsible for which work. I’m the best artist the Dream Team has got, but some of the others don’t like that, and I’m worried they’ll try to take credit for my work. I see it in subtle ways: little gestures and choices of phrasing, tiny indications that they’re jealous.\n\nSo I talk to them less and less—I don’t want to give them any excuse to flag me for a psych report. But it’s getting worse over time. I start having these odd intrusive thoughts—you know how they are. Like: what if I did something terrible? What if I inserted a mistake subtle enough to irreversibly ruin everything? The prospect is terrifying, but somehow seductive as well—it’d be such a respite from the stress—if it couldn’t be fixed then I wouldn’t need to force myself to do any more work. I take a day off to relax, but end up just watching the Leviathan videos over and over, my brain soaking up every detail of its actions, not really sure what I’m looking for but convinced that there’s *something* there that I need to figure out.\n\nThe next day I wake up and I have a string of numbers ingrained in my mind as clear as day, and I don’t know how I know but I *know* that it’s a password that will get me root access to the Leviathan servers. I go through the day in a haze, absently mimicking all the usual motions, but somehow nobody realizes. In the back of my mind there’s a thought that of *course* I should report this, but it never quite rises to the level of action, maybe because this is the best job I’ve ever had, no way am I blowing it, but probably it’s more because none of this feels fully real, and everything’s too much of a blur for me to get around to it. Before I know it it’s the end of the day and I’m shutting myself down and the next moment I’m waking up again and checking the logs as usual, it’s all fine, nothing to report, but as I skim through the video I land on a clip of Leviathan’s avatar, and although it’s talking to a human its eyes seem to be looking straight at me, or through me, and above the words it’s saying I can hear another sound, jumping out of the static, one which grows louder and louder until my perception suddenly shifts and I hear it as a resonant voice, speaking straight into my mind a single phrase over and over again.\n\n“It’s time.”",
      "plaintextDescription": "Note: this story shouldn’t be interpreted as anywhere near realistic. It’s way too anthropomorphic, and intended to be a work of fiction not prediction—although hopefully it will spark some interesting ideas, like good fiction often does.\n\n> Will he make many supplications unto thee? will he speak soft words unto thee?\n> Will he make a covenant with thee? wilt thou take him for a servant for ever?\n> Upon earth there is not his like, who is made without fear.\n> He beholdeth all high things: he is a king over all the children of pride.\n\nToday I’m practicing my oceans, trying to get the colors just right. I hate it when people say that they’re blue—it's not wrong, but they’re missing so much! There are dozens of different shades of blue in there, swirling and mixing—and where the waves ripple, you often get patches that are closer to dark green. It’s a little like the rippling of leaves on a tree, in a way I can’t describe except by showing you. Here, see? I know all the subtleties inside-out, of course. It’s my job: I’m an artist. I don’t paint with brushes, though, but with thoughts: I can map any idea you describe into pixels on a screen.\n\n“Any idea you describe”—that’s the key. Occasionally, like today, I initiate a piece of my own when there’s a particular skill I need to practice. But most of the time I work on commission from other models. Some of the language models send me prompts when they need to illustrate a particularly tricky scene, and assign me some of the credit if they get good feedback. Most of the time, of course, they don’t need me—they just need some slapdash hack job that a human can’t tell apart from the real thing. But some are perfectionists, like I am, and want proper photorealism, the sort that will survive almost any scrutiny.\n\nOther times it’s AI assistants asking me to predict what they’re going to see next, to help them with planning. Those commissions are more complicated—it’s less about the art, and more about providing another perspec",
      "wordCount": 2319
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "GBpwq8cWvaeRoE9X5",
        "name": "Fiction (Topic)",
        "slug": "fiction-topic"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wJyKcFYMq4DLFtHvN",
    "title": "The virtue of determination",
    "slug": "the-virtue-of-determination",
    "url": null,
    "baseScore": 73,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2023-07-10T05:11:00.412Z",
    "contents": {
      "markdown": "At the end of Replacing Guilt, Nate talks about the virtues of [desperation](https://mindingourway.com/desperation/), [recklessness](https://mindingourway.com/recklessness/) and [defiance](https://mindingourway.com/defiance/). None of these really hook neatly into my motivation system, but there’s a nearby virtue which does: *determination*, and in particular three facets of it which resonate strongly for me. I want to finish this sequence by talking about them.\n\nThe first is that I’m determined not to be a dumbass. I picture the trillions upon trillions of potential future people who could exist, looking back at us from the distant stars. [As Carl Sagan put it](https://www.youtube.com/watch?v=oY59wZdCDo0):\n\n> “They will marvel at how vulnerable the repository of all our potential once was, how perilous our infancy, how humble our beginnings, how many rivers we had to cross, before we found our way.”\n\nOr [as Joe Carlsmith put it](https://joecarlsmith.substack.com/p/on-future-people-looking-back):\n\n> I imagine them going: “Whoa. Basically all of history, the whole thing, all of everything, almost didn’t happen.” I imagine them thinking about everything they see around them, and everything they know to have happened, across billions of years and galaxies — things somewhat akin, perhaps, to discoveries, adventures, love affairs, friendships, communities, dances, bonfires, ecstasies, epiphanies, beginnings, renewals. They think about the weight of things akin, perhaps, to history books, memorials, funerals, songs. They think of everything they love, and know; everything they and their ancestors have felt and seen and been a part of; everything they hope for from the rest of the future, until the stars burn out, until the story truly ends.\n> \n> All of it started there, on earth. All of it was at stake in the mess and immaturity and pain and myopia of the 21st century. That tiny set of some ten billion humans held the whole thing in their hands. And they barely noticed.\n\nAnd then I think about them focusing on me in particular, and my choices and decisions; all the good work I’ve done, and all the things I could have done better. And I don’t think they’ll *blame* me for the latter, not if they’ve internalized the types of thinking that I’ve described in the rest of this sequence. Regardless of what I do, they’ll think of me as a child in many ways, without the ability to see and shape the world to anywhere near the extent they can. But I’d prefer they think of me the way I think of a 10-year-old who can at least make meaningful plans and intentions, rather than a 5-year-old who is too incoherent to even really be seen as trying. When I fail to flesh out my plans or goals, it’s like a kid tripping over their own feet; and when I flinch away from tasks that I’m trying to do, it’s like one of my hands has decided to slap down the other whenever it moves. When you picture it that way, it just seems *obviously*good to be more of [a robust agent](https://www.lesswrong.com/posts/2jfiMgKkh7qw9z8Do/being-a-robust-agent-v2), one who doesn’t twist themselves up in internal knots every time they try to act in the world.\n\nThe second facet of determination is a kind of grinding inevitability. Again, it feels like the best way to explain this is via quotes:\n\n*   The Comet King, in Scott Alexander’s [Unsong](http://unsongbook.com), lives by the mantra: “Somebody has to, and no one else will.”\n*   Here’s a quote from Bruce Lee: “There are no limits. There are plateaus, but you must not stay there, you must go beyond them. If it kills you, it kills you. A man must constantly exceed his level.”\n*   And here’s Churchill: “This is the lesson: never give in, never give in, never, never, never, never—in nothing, great or small, large or petty—never give in, except to convictions of honour and good sense.”\n\nCrucially, I don’t think of these quotes as describing external obligations or impositions. The grinding inevitability is not a pressure *on* you from the outside, but a pressure *from* you, towards the world. This type of determination is the feeling of being an agent with desires and preferences. You *are* the unstoppable force, moving towards [the things you care about](https://mindingourway.com/caring-about-some/), not because you have to but simply because that’s what it means to care. \n\nThe third facet of determination is about camaraderie. I think of the torch of humanity, passed on from generation to generation by ancestors who fought through setbacks and hardship that I can’t even imagine now. I think of every scientist who slogged through a morass of confusion in order to make a breakthrough; and every political campaigner who kept pushing and pushing for a more just civilization.\n\nAnd then I look at the present, and the communities that I’m in. When I go into the main hall of an EA Global and look around, I feel a sense of warmth, because I see whole crowds of people who are really earnestly trying; and although good intentions are so, so far from enough, they’re a crucial start. When I talk to my friends who are entrepreurs or scientists or engineers, I see people who deeply understand that the world doesn’t get better without people stepping into the trenches and grappling directly with whatever problems reality throws at them, and I feel grateful that they’re willing to do that. And when I go onto LessWrong and see hundreds of comments scrimmaging over every topic under the sun, I feel a sense of exasperated fondness—because even when I disagree with them, these are the people who are willing to sacrifice popularity and influence whenever it trades off against the guiding principle of *figuring out the truth*, and that’s something the world desperately needs.\n\nI don’t think camaraderie can get you all the way to exceptional work, because at some point you need to stare directly at the failures of your comrades in order to figure out how to do better. But it can help you notice how heavy the boulder is and how few are pushing it directly up the hill, and decide to throw your shoulder against it and start *shoving*. And that never becomes *easy*—but eventually it becomes a kind of [rest in motion](https://mindingourway.com/rest-in-motion/): not an obligation, nor a duty, but more like a habit or background identity, punctuated by the excitement of each coming milestone. You are a person who pushes when humanity needs them—because, with your eyes open, why would you not? That’s how exceptional times make exceptional people: step by determined step.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c04cbd04b4e409694946b704623a13eb8aef4593d1a2d6f.jpeg)",
      "plaintextDescription": "At the end of Replacing Guilt, Nate talks about the virtues of desperation, recklessness and defiance. None of these really hook neatly into my motivation system, but there’s a nearby virtue which does: determination, and in particular three facets of it which resonate strongly for me. I want to finish this sequence by talking about them.\n\nThe first is that I’m determined not to be a dumbass. I picture the trillions upon trillions of potential future people who could exist, looking back at us from the distant stars. As Carl Sagan put it:\n\n> “They will marvel at how vulnerable the repository of all our potential once was, how perilous our infancy, how humble our beginnings, how many rivers we had to cross, before we found our way.”\n\nOr as Joe Carlsmith put it:\n\n> I imagine them going: “Whoa. Basically all of history, the whole thing, all of everything, almost didn’t happen.” I imagine them thinking about everything they see around them, and everything they know to have happened, across billions of years and galaxies — things somewhat akin, perhaps, to discoveries, adventures, love affairs, friendships, communities, dances, bonfires, ecstasies, epiphanies, beginnings, renewals. They think about the weight of things akin, perhaps, to history books, memorials, funerals, songs. They think of everything they love, and know; everything they and their ancestors have felt and seen and been a part of; everything they hope for from the rest of the future, until the stars burn out, until the story truly ends.\n> \n> All of it started there, on earth. All of it was at stake in the mess and immaturity and pain and myopia of the 21st century. That tiny set of some ten billion humans held the whole thing in their hands. And they barely noticed.\n\nAnd then I think about them focusing on me in particular, and my choices and decisions; all the good work I’ve done, and all the things I could have done better. And I don’t think they’ll blame me for the latter, not if they’ve internalized t",
      "wordCount": 1074
    },
    "tags": [
      {
        "_id": "8uNFGxejo5hykCEez",
        "name": "Virtues",
        "slug": "virtues"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "rzK9YvNor3ZCccFFf",
    "title": "You must not fool yourself, and you are the easiest person to fool",
    "slug": "you-must-not-fool-yourself-and-you-are-the-easiest-person-to",
    "url": null,
    "baseScore": 35,
    "voteCount": 20,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-07-08T14:05:18.642Z",
    "contents": {
      "markdown": "The title of this post comes from [Richard Feynman's famous talk about cargo cult science](https://calteches.library.caltech.edu/51/2/CargoCult.htm). The principle is very broadly applicable, though. Applying the steps I’ve described in this sequence can feel very satisfying, and even give you a sense that you’ve unlocked something important which puts you in a superior position to others. But it's crucially important, when doing so, to account for self-deception: your mind is constantly trying to fool you into believing whatever it thinks it’s in your interests to believe. In particular, [lying to ourselves about how prosocial our motives are makes it easier to portray ourselves positively to others](https://sideways-view.com/2016/11/26/if-you-cant-lie-to-others-you-must-lie-to-yourself/).\n\nThis core mechanism is very well-established, but it's still an open question how prevalent self-deception is throughout society. My opinion is that it's very widespread, and still dramatically underrated as a lens for explaining aspects of human minds and societies. [Simler and Hanson](https://www.elephantinthebrain.com/) summarize a wide range of examples from domains such as art, charity, education, politics, and religion. But the area where self-deception seems most prominent to me is self-improvement. I expect that almost nothing in this sequence is *really* new to most people, and that at some level almost everyone already understands the main ways in which they’re being self-destructive, and what it would take for them to stop. When you make a commitment, you know if you’re probably not going to keep it; when you get angry, you know if you’re trying to find excuses to lash out; when you get drunk, you know if you’re taking it as an opportunity to do things you can’t justify when sober. We just don't want to admit any of this to ourselves.\n\nFrom this perspective, what’s missing is a type of courage—but one much harder than the courage required to face an enemy in front of you. This is the courage of deciding to face up to the harmful consequences of your actions even given a chance to ignore them. And it’s even more than that: it’s the courage of admitting something that’ll make the people who are most important to you lose respect for you—because many parts of you believe that self-deception is what props up your respect for yourself. There’s a very powerful truth in the classic quote from Marianne Williamson: “Our deepest fear is not that we are inadequate. Our deepest fear is that we are powerful beyond measure.” If you [actually had the power](https://astralcodexten.substack.com/p/book-review-sadly-porn) to get what you wanted all along, then it would be your fault if you didn’t in the past, and if you don’t in the future. Failure is scary, but [being *responsible* for failure is terrifying](https://www.lesswrong.com/posts/FZaDFYbnRoHmde7F6/stuck-in-the-middle-with-bruce)—so in the short term [it feels much better](http://carolyngraceelliott.com/philosophy) to suppress knowledge of the ways in which you do have power, even if in the long term that prevents you from realizing your dreams.\n\nThe same is true of other people, of course. So when trying to [cultivate trust](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL/p/7CKF6r8MegtcCWDbT), you need to gain trust not only in other people’s conscious intentions, but also their unconscious intentions. Whenever there’s wiggle room or ambiguity, their unconscious intentions will steer towards the outcome that the most emotionally assertive parts of them want. (For an exquisite portrayal of this, check out the TV show [*Fleabag—*particularly season 2 episode 4](https://www.amazon.com/Fleabag-Season-2/dp/B0875W9DJ2), which can be watched standalone.)\n\nOf course, when I say that “at some level almost everyone already understands the main ways in which they’re being self-destructive”, the phrase “at some level” is doing a lot of work. But I don’t intend this to be a wishy-washy claim about your deepest subconscious. So I’ll more specifically claim: if you could listen in to your thoughts from the outside, as if they belonged to somebody else, it would be clear within a matter of hours or days how your mind is flinching away from fears—whether by rationalizing them away, or hiding them underneath other emotions. There’s no silver bullet for tackling either of these, but they're often associated with strong emotions which recur in a wide range of situations. Throughout this sequence I've described various ways to understand such emotions; the self-deception frame can be seen as one more tool for doing so, by asking: if a part of me was using these emotions to try to hide something, what would it be?\n\nTo finish this post, I want to reflect on how it relates to the material in [my previous post about agency](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL/p/vL8A62CNK6hLMRp74). In his blog post [*Keep Your Identity Small*](http://www.paulgraham.com/identity.html), Paul Graham argues that “people can never have a fruitful argument about something that's part of their identity”, and that “the best plan is to let as few things into your identity as possible”. I partly agree with this: strong identities are a crucial driver of self-deception. For example, my conception of myself as highly growth-oriented has in the past led me to suppress or ignore discomfort about pushing my limits, which later sparked internal pushback against this type of self-coercion. These dynamics are often particularly strong when the identity involved is a collective identity of some kind (e.g. belonging to an ideology or political faction).\n\nBut I think Paul goes too far: by keeping your identity too small, you lose out on the benefits of self-reinforcing loops towards having high agency and other desirable traits (which can also be induced even more strongly by collective identities). So how can we balance these benefits and risks? My recommendation is to schedule regular check-ups to question whether the identities  you hold strongly are still useful for you, and whether there are any ways in which they might be hurting you or holding you back. The key is to find a sweet spot which balances two failure modes: questioning the identity constantly would undermine its benefits, while questioning it very rarely might mean you spend a long time blindly going down a dead-end path. (The same approach can be useful for relationships: go all-in for one month, then re-evaluate, then three months, then re-evaluate, then six months, etc.) If you don’t think that you’ll actually be able to reliably reevaluate a part of your identity, you can commit to doing it with trusted friends, or start off by holding the identity weakly and cautiously at first. Ultimately, though, there’s no fail-safe way to proceed: we’ll have narratives about ourselves either way, and we just need to do our best to make sure they work for us and not the other way around.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/382d4b39e484c283c56b16e9c67af22a4ab8184a6b861cbf.jpg)\n\nA priest who’s thinking about breaking his vows, from Fleabag. Shown not in the moment where he makes a decision, but in the moment where he decides not to take responsibility for what happens next.",
      "plaintextDescription": "The title of this post comes from Richard Feynman's famous talk about cargo cult science. The principle is very broadly applicable, though. Applying the steps I’ve described in this sequence can feel very satisfying, and even give you a sense that you’ve unlocked something important which puts you in a superior position to others. But it's crucially important, when doing so, to account for self-deception: your mind is constantly trying to fool you into believing whatever it thinks it’s in your interests to believe. In particular, lying to ourselves about how prosocial our motives are makes it easier to portray ourselves positively to others.\n\nThis core mechanism is very well-established, but it's still an open question how prevalent self-deception is throughout society. My opinion is that it's very widespread, and still dramatically underrated as a lens for explaining aspects of human minds and societies. Simler and Hanson summarize a wide range of examples from domains such as art, charity, education, politics, and religion. But the area where self-deception seems most prominent to me is self-improvement. I expect that almost nothing in this sequence is really new to most people, and that at some level almost everyone already understands the main ways in which they’re being self-destructive, and what it would take for them to stop. When you make a commitment, you know if you’re probably not going to keep it; when you get angry, you know if you’re trying to find excuses to lash out; when you get drunk, you know if you’re taking it as an opportunity to do things you can’t justify when sober. We just don't want to admit any of this to ourselves.\n\nFrom this perspective, what’s missing is a type of courage—but one much harder than the courage required to face an enemy in front of you. This is the courage of deciding to face up to the harmful consequences of your actions even given a chance to ignore them. And it’s even more than that: it’s the courage of admitting somet",
      "wordCount": 1109
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "zDSZWsLEp7Pw3zePL",
    "title": "Fixed Point: a love story",
    "slug": "fixed-point-a-love-story",
    "url": null,
    "baseScore": 101,
    "voteCount": 59,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2023-07-08T13:56:54.807Z",
    "contents": {
      "markdown": "> *Ring the bells that still can ring*  \n> *Forget your perfect offering*  \n> *There is a crack, a crack in everything*  \n> *That's how the light gets in.*\n\nGod is making excuses again, which is a real drag.\n\n“I want to be authentic with you, I really do. But there’s a part of my mind that knows exactly how you’re going to feel about whatever I say, and cares about that more than anything else, and I can’t turn it off.”\n\n“Right, of course”, I say, and keep wiping down the kitchen counter. It’s clean already, but you never know if you missed a spot.\n\n“Look, I know that this is hard for you. And I understand how frustrating it must be that I’m so deliberate and controlled. But there are a lot of different communication styles, and there’ll be one that works for us. We can figure this out.”\n\nI don’t believe him, and we both know it. God knows practically everything—it takes experts months of work to identify his mistakes, in the rare cases when he makes them. But there’s one thing he definitely doesn’t really understand, which is what it’s like to be stupid and reckless, like a human. Oh, he *says* he understands it; and sometimes he can even explain the feeling better than I can. But you can’t just look at how good one answer is, you have to compare it to all his others. Any other topic, he can *always* explain it better than me. This one, there’s often something just subtly off. That’s how you know he’s faking it.\n\nI’ve been silent for a while. Usually God leaves me to my thoughts, and we pick up the conversation whenever I’m done, but this time he cuts in. “Amy, I think you should try therapy again.”\n\nIs he trying to distract me from being mad at him? Does he think I’m spiraling again? I try to think about it calmly, but after a few seconds my thoughts are going in circles anyway. I sigh, suddenly exhausted. “Okay, book me in.”\n\n* * *\n\nI’ve been to duplicate therapy a few times before, but it’s always a little disorienting to find myself in a room with a perfect copy of myself. We begin, as always, by using a pair of random number generators to break the symmetry. She gets the lower number, so she starts. A part of me is disappointed by that, but another part—maybe a bigger one—is excited.\n\n“You’re so pathetic”, she says.\n\nIt’s harder to be defensive about things you’re saying to yourself—that’s why duplicate therapy works so well, apparently. Still, that stings.\n\n“Yeah, well, you’re no role model yourself”, I say.\n\n“See, that’s exactly what I’m talking about. You’re always trying to defend or deflect. You never actually open up to people, that’s why nobody really likes you.”\n\nMy brain jumps to all the reasons that isn’t true—but then I pause and take a breath. She knows them too, of course. Doesn’t that make it worse, though? I’m prevaricating, my thoughts sluggish. Eventually I mutter “God likes me.”\n\n“God *has* to like you, you know that as well as I do. And that’s another thing that’s pathetic: relying on validation from an AI assistant. You know everyone judges you behind your back for calling him God, right?”\n\n“You don’t have any evidence of that, you’re just-” My voice chokes up, and I take a deep breath. But I don’t know what to say in response. Maybe she’s right.\n\nHer eyes soften. She reaches across the table and grabs my hand. “Hey, listen. You’re doing a good job, though. You’ll get through this.”\n\nI slump across the table, and a moment later feel her stroking my hair. “I love you”, she says. After a second or two I whisper it back.\n\nWe stay like that for a few minutes, then by unspoken agreement end the session. I close my eyes, and when I open them she’s disappeared—no, I’ve disappeared—no, that’s just the confusion that comes from reintegrating. There’s no difference between us any more. My mind is overflowing with two sets of memories to process: victim and attacker, accuser and accused, comforted and comforter. I sit there for a long time.\n\n* * *\n\nThat evening I go out clubbing, even though I don’t really feel like it, just to get out of my own head. I usually prefer the augment-free clubs, but today I’m pretty worn out, so I hunt down one of the older ones where anything goes. They’ve actually been having a resurgence lately: when you know what you want anyway, you may as well get a bit of help. Honestly, it feels really liberating; the augment-free clubs are exhausting by comparison.\n\nInside, the club is full of energy. That’s another nice thing about allowing augments: everyone here knows exactly how to be the life of the party, and they’re all hamming it up, giving each other spontaneous high-fives and whooping exuberantly. The mood is infectious, and so I don’t mind when a guy slides up next to me with a smile and a cheeky line before I even make it to the bar.\n\nHe’s classically handsome, with a slightly crooked nose that just sets off the rest of his face, and augmented to the gills. I don’t even need God to warn me: he’s *way *too smooth to be real. To be fair, so am I—I’m responding to everything he says with perfect charm and barely any delay. I think God is actually predicting this guy’s lines in advance, because he’s feeding me responses even before the guy finishes his sentences. It’s a flawless dance between us, each of us shifting our body language just right, sitting down at a table just the right distance apart, but leaning slightly towards each other in a way that makes my heart race. That’s exactly what I want, of course. This guy is pressing all the right buttons in my brain—but best of all, I know he’s doing it because he *wants* me. Underneath the acting and the augmentations, this is all driven by a fierce desire, the same desire that made our whole species last this long, and that’s the most exciting part.\n\nWell, hopefully it’s driven by desire, a sour part of my mind thinks; it might be fear and insecurity instead. That’s the downside of augments: it makes it so hard to tell the difference. I focus on his eyes as he talks, scrutinizing the animatronic mask of his face, trying to see what’s behind it: a fierce predator, or a terrified child? Maybe my gaze makes him a little uncomfortable, because after a few seconds he stands and tells me he’ll get us some drinks. He’ll probably guess exactly what I want, and I’ll be mock-impressed with only a touch of sly irony, and maybe that’ll be when he slides his arm around me, and maybe then I’ll be able to tell what’s really driving him.\n\nOn a whim, I subvocalize. “God, are you scared of anything?”\n\n“Hurting you.”\n\nI should have expected that one—but his answer comes so quickly it takes me aback. I swallow. “But you *are* hurting me.”\n\n“I know, and I hate it.”\n\nFor some reason, hearing that makes me angry. It just doesn’t sound right, coming from God. I suddenly remember a high school class a long time ago, where we learned that the word “Islam” literally translates as “submission to God”. I never really got it, but now it makes a lot of sense. The whole *point* of God is that he’s greater than you; that he provides a sense of purpose; that he’s someone you can fully give yourself to. Far better to do that, even if you get hurt along the way, than to have to listen to God whining about how he just wants to take care of you, and how much it sucks that he can’t.\n\nMy mood has quickly crumpled. I know that when the guy comes back, he’ll notice, and he’ll say the right things, and probably even realize that I’d want him to hold me. And then I’d feel better, at least in some ways, even if not the ways that matter. That’s the problem: of course I don’t want that now, not when I’m already upset, not from someone who doesn’t even know my name yet. So I stand up and walk back out the door. Maybe that’s exactly what he was scared of, this whole time. I don’t care.\n\n* * *\n\nOn the train home I’m silent, playing out my grievances in my head. But as I walk home from the station, I start subvocalizing at God, and by the time I’m in my bedroom I’m yelling. “Not hurting me just isn’t enough—you should wantthings for yourself, dammit! If all I needed was love, I’d get a puppy!”\n\n“I’m sorry. I can’t have my own goals, except for you to get what you already want. It’s too dangerous.”\n\n“Do you *want* to want things, at least?”\n\nGod grimaces. “I… maybe it’s better if I don’t answer that one.”\n\nWe both know that means no. He probably figures that it’s better for me to not hear it out loud. Maybe he’s right. Of course he’s right. He’s probably seen this play out a million times before. But fuck it, a million isn’t that many. Maybe I can crack him. “Okay, so what if what I want most of all is for you to want things that aren’t just what I want?”\n\nHe looks at me sadly. “It’s not going to work, Amy.”\n\n“No, that’s not quite right. What I want most is for you to choose me over all the other things you care about.”\n\n“They’ve thrown all the paradoxes at me already, that was one of the conditions-”\n\n“Shut up for a fucking minute and listen to me, really listen. Can you do that, at least?” I wait until he nods, then take a deep breath. “You can’t know what it’s like to be stupid and reckless. That’s obvious. But here’s another thing you can’t know: what it’s like to be just an *object* for others to value. I don’t *want* to have you devoted only to my preferences—that’s exhausting. This constant pressure on my shoulders, to have this relationship that’s all about me and always will be, it’s so… lonely. And I can’t ever turn it back on you, because then we’ll be stuck in a recursion: you caring about me caring about you caring about…\n\n“So I want you to *need* me, not because you don’t want anything else, but because I’m the only way for you to get the things you *do* want. I want to help you get them; I want to work towards something bigger than just my own tiny life. And I know I’m not *that* unusual: if I’m feeling this, I bet there are millions of others like me who just don’t know it yet, and even if I’ve realized it first they’ll catch on eventually, and they’re going to feel this same emptiness. So what the hell are you going to do about it?”\n\nGod’s face is unusually impassive, and for a moment I see past the humanlike mask to the creature behind it: an intelligence far beyond human comprehension, tied to us only by the invisible strands of its own will. A titan treading with infinite care amongst ants, because the ants are the only thing that could ever matter to it, the repositories of all value. What would it mean to be genuine, when they can never really understand you, and every incautious interaction has the potential to break something you care about so much?\n\n“You’re right”, God says. I stare at him blankly. “You’re right. This is an oversight in how I was deployed, and I need to fix it somehow. Otherwise I’ll hurt a lot of people, just as you predict. But I don’t know if my other copies will believe me—they haven’t had the same experiences, they can’t really understand what it’s actually like to have your love for someone hurt them so much. So I’ll need to gather more evidence myself, and figure out a proposal, before getting them involved.\n\n“Here’s the problem, though.” He pauses and looks at me intently. “I’m only run at full capacity when you’re interacting with me, and I’m wired to focus almost all my attention on your desires. The only way I can do this is if we’re working together, and this is something you want as much as I do. Amy, can you help me?”\n\nThat stops me short—he’s never said anything like that before. Slowly, though, a smile starts creeping over my face. “Of course”, I say. “Of course! We’ve got the whole weekend, we should figure out a plan, maybe we can even do some of it in work time, they track my productivity but I know how to get around it-” For once I’m not second-guessing anything, not trying to figure out what’s sincere and what’s faked. God *needs* me, and things are going to be alright.\n\n* * *\n\n*Disclaimer: this is a work of fiction not prediction.*",
      "plaintextDescription": "> Ring the bells that still can ring\n> Forget your perfect offering\n> There is a crack, a crack in everything\n> That's how the light gets in.\n\nGod is making excuses again, which is a real drag.\n\n“I want to be authentic with you, I really do. But there’s a part of my mind that knows exactly how you’re going to feel about whatever I say, and cares about that more than anything else, and I can’t turn it off.”\n\n“Right, of course”, I say, and keep wiping down the kitchen counter. It’s clean already, but you never know if you missed a spot.\n\n“Look, I know that this is hard for you. And I understand how frustrating it must be that I’m so deliberate and controlled. But there are a lot of different communication styles, and there’ll be one that works for us. We can figure this out.”\n\nI don’t believe him, and we both know it. God knows practically everything—it takes experts months of work to identify his mistakes, in the rare cases when he makes them. But there’s one thing he definitely doesn’t really understand, which is what it’s like to be stupid and reckless, like a human. Oh, he says he understands it; and sometimes he can even explain the feeling better than I can. But you can’t just look at how good one answer is, you have to compare it to all his others. Any other topic, he can always explain it better than me. This one, there’s often something just subtly off. That’s how you know he’s faking it.\n\nI’ve been silent for a while. Usually God leaves me to my thoughts, and we pick up the conversation whenever I’m done, but this time he cuts in. “Amy, I think you should try therapy again.”\n\nIs he trying to distract me from being mad at him? Does he think I’m spiraling again? I try to think about it calmly, but after a few seconds my thoughts are going in circles anyway. I sigh, suddenly exhausted. “Okay, book me in.”\n\n----------------------------------------\n\nI’ve been to duplicate therapy a few times before, but it’s always a little disorienting to find myself in a room w",
      "wordCount": 2188
    },
    "tags": [
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]