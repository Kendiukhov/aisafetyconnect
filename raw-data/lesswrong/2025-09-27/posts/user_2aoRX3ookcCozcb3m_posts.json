[
  {
    "_id": "kgb58RL88YChkkBNf",
    "title": "The Problem",
    "slug": "the-problem",
    "url": null,
    "baseScore": 312,
    "voteCount": 147,
    "viewCount": null,
    "commentCount": 218,
    "createdAt": null,
    "postedAt": "2025-08-05T21:40:00.962Z",
    "contents": {
      "markdown": "*This is a new introduction to AI as an extinction threat, previously posted to the* [*MIRI website*](https://intelligence.org/the-problem/) *in February alongside a* [*summary*](https://intelligence.org/briefing/)*. It was written independently of Eliezer and Nate's forthcoming book,* [***If Anyone Builds It, Everyone Dies***](https://ifanyonebuildsit.com/)*, and **isn't** a sneak peak of the book. Since the book is long and costs money, we expect this to be a valuable resource in its own right even after the book comes out next month.*[^fpb6rwvoxuo]\n\nThe stated goal of the world’s leading AI companies is to build AI that is **general** enough to do anything a human can do, from solving hard problems in theoretical physics to deftly navigating social environments. Recent machine learning progress seems to have brought this goal within reach. At this point, we would be uncomfortable ruling out the possibility that AI more capable than any human is achieved in the next year or two, and we would be moderately surprised if this outcome were still two decades away.\n\nThe current view of MIRI’s research scientists is that if smarter-than-human AI is developed this decade, the result will be an unprecedented catastrophe. The [CAIS Statement](https://www.safe.ai/work/statement-on-ai-risk), which was widely endorsed by senior researchers in the field, states:\n\n> Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n\n**We believe that if researchers build superintelligent AI with anything like the field’s current technical understanding or methods, the expected outcome is human extinction**.\n\n“Research labs around the world are currently building tech that is likely to cause human extinction” is a conclusion that should motivate a rapid policy response. The fast pace of AI, however, has caught governments and the voting public flat-footed. This document will aim to bring readers up to speed, and outline the kinds of policy steps that might be able to avert catastrophe.\n\nKey points in this document:\n\n*   [There isn’t a ceiling at human-level capabilities.](https://intelligence.org/the-problem/#1_no_ceiling_at_human-level)\n*   [ASI is very likely to exhibit goal-oriented behavior.](https://intelligence.org/the-problem/#2_goal-oriented_behavior)\n*   [ASI is very likely to pursue the wrong goals.](https://intelligence.org/the-problem/#3_wrong_goals)\n*   [It would be lethally dangerous to build ASIs that have the wrong goals.](https://intelligence.org/the-problem/#4_lethally_dangerous)\n*   [Catastrophe can be averted via a sufficiently aggressive policy response.](https://intelligence.org/the-problem/#5_policy)\n\n### **1.  There isn’t a ceiling at human-level capabilities.**\n\nThe signatories on the CAIS Statement included the three most cited living scientists in the field of AI: Geoffrey Hinton, Yoshua Bengio, and Ilya Sutskever. Of these, Hinton has [said](https://www.ft.com/content/c64592ac-a62f-4e8e-b99b-08c869c83f4b): “If I were advising governments, I would say that there’s a 10% chance these things will wipe out humanity in the next 20 years. I think that would be a reasonable number.” In an April 2024 Q&A, Hinton [said](https://youtu.be/PTF5Up1hMhw?si=oY8w3v37EhNu8sbJ&t=2220): “I actually think the risk is more than 50%, of the existential threat.”\n\nThe underlying reason AI poses such an extreme danger is that AI progress doesn’t stop at human-level capabilities. The development of systems with human-level generality is likely to quickly result in **artificial superintelligence** (ASI): AI that substantially surpasses humans in all capacities, including economic, scientific, and military ones.\n\nHistorically, when the world has found a way to automate a computational task, we’ve generally found that computers can perform that task far better and faster than humans, and at far greater scale. This is certainly true of recent AI progress in board games and protein structure prediction, where AIs spent little or no time at the ability level of top human professionals before vastly surpassing human abilities. In the strategically rich and difficult-to-master game Go, AI went in the span of a year from never winning a single match against the worst human professionals, to never losing a single match against the best human professionals. Looking at a specific system, [AlphaGo Zero](https://intelligence.org/2017/10/20/alphago/): In three days, AlphaGo Zero went from knowing nothing about Go to being vastly more capable than any human player, without any access to information about human games or strategy.\n\nAlong most dimensions, computer hardware greatly outperforms its biological counterparts at the fundamental activities of computation. While currently far less energy efficient, modern transistors can switch states at least ten million times faster than neurons can fire. The working memory and storage capacity of computer systems can also be vastly larger than those of the human brain. Current systems already produce prose, art, code, etc. orders of magnitude faster than any human can. When AI becomes capable of the full range of cognitive tasks the smartest humans can perform, we shouldn’t expect AI’s speed advantage (or other advantages) to suddenly go away. Instead, we should expect smarter-than-human AI to drastically outperform humans on speed, working memory, etc.\n\nMuch of an AI’s architecture is digital, allowing even deployed systems to be quickly redesigned and updated. This gives AIs the ability to self-modify and self-improve far more rapidly and fundamentally than humans can. This in turn can create a feedback loop (I.J. Good’s “intelligence explosion”) as AI self-improvements speed up and improve the AI’s ability to self-improve.\n\nHumans’ scientific abilities have had an enormous impact on the world. However, we are very far from optimal on core scientific abilities, such as mental math; and our brains were not optimized by evolution to do such work. More generally, humans are a young species, and evolution has only begun to explore the design space of generally intelligent minds — and has been hindered in these efforts by contingent features of human biology. An example of this is that the human birth canal can only widen so much before hindering bipedal locomotion; this served as a bottleneck on humans’ ability to evolve larger brains. Adding ten times as much computing power to an AI is sometimes just a matter of connecting ten times as many GPUs. This is sometimes not literally trivial, but it’s easier than expanding the human birth canal.\n\nAll of this makes it much less likely that AI will get stuck for a long period of time at the rough intelligence level of the best human scientists and engineers.\n\nRather than thinking of “human-level” AI, we should expect weak AIs to exhibit a strange mix of subhuman and superhuman skills in different domains, and we should expect strong AIs to fall well outside the human capability range.\n\nThe number of scientists raising the alarm about artificial superintelligence is large, and quickly growing. Quoting from a recent [interview](https://youtu.be/Gi_t3v53XRU?si=2hG5OozeBXYJeQu6&t=3748) with Anthropic’s Dario Amodei:\n\n> **AMODEI:** Yeah, I think ASL-3 \\[AI Safety Level 3\\] could easily happen this year or next year. I think ASL-4 —\n> \n> **KLEIN:** Oh, Jesus Christ.\n> \n> **AMODEI:** No, no, I told you. I’m a believer in exponentials. I think ASL-4 could happen anywhere from 2025 to 2028.\n> \n> **KLEIN:** So that is fast.\n> \n> **AMODEI:** Yeah, no, no, I’m truly talking about the near future here.\n\nAnthropic [associates](https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf) ASL-4 with thresholds such as AI “that is unambiguously capable of replicating, accumulating resources, and avoiding being shut down in the real world indefinitely” and scenarios where “AI models have become the primary source of national security risk in a major area”.\n\n<table style=\"background-color:transparent\"><tbody><tr><td style=\"background-color:rgba(128, 128, 128, 0.07);border-color:rgba(128, 128, 128, 0.5);padding:15px;vertical-align:top\">Learn more:<a href=\"http://intelligence.org/notes/soon\"> <strong>Why expect smarter-than-human AI to be developed anytime soon?</strong></a></td></tr></tbody></table>\n\nIn the wake of these widespread concerns, members of the US Senate convened a bipartisan [AI Insight Forum](https://intelligence.org/2023/12/06/written-statement-of-miri-ceo-malo-bourgon-to-the-ai-insight-forum/) on the topic of “Risk, Alignment, & Guarding Against Doomsday Scenarios”, and United Nations Secretary-General António Guterres [acknowledged](https://www.youtube.com/watch?v=ktFF2dSH3oU&t=38s) that much of the research community has been loudly raising the alarm and “declaring AI an existential threat to humanity”. In a report commissioned by the US State Department, Gladstone AI [warned](https://www.cnn.com/2024/03/12/business/artificial-intelligence-ai-report-extinction/index.html) that loss of control of general AI systems “could pose an extinction-level threat to the human species.”\n\nIf governments do not intervene to halt development on this technology, we believe that human extinction is the default outcome. If we were to put a number on how likely extinction is in the absence of an aggressive near-term policy response, MIRI’s research leadership would give one **upward of 90%**.\n\nThe rest of this document will focus on how and why this threat manifests, and what interventions we think are needed.\n\n### **2\\. ASI is very likely to exhibit goal-oriented behavior.**\n\nGoal-oriented behavior is [economically useful](https://gwern.net/tool-ai), and the leading AI companies are [explicitly trying](https://www.bloomberg.com/news/videos/2024-05-08/google-deepmind-ceo-on-drug-discovery-hype-isomorphic-video) to achieve goal-oriented behavior in their models.\n\nThe deeper reason to expect ASI to exhibit goal-oriented behavior, however, is that problem-solving with a long time horizon is essentially the same thing as goal-oriented behavior. This is a key reason the situation with ASI appears dire to us.\n\nImportantly, an AI can “exhibit goal-oriented behavior” without necessarily having human-like desires, preferences, or emotions. Exhibiting goal-oriented behavior only means that the AI **persistently modifies the world in ways that yield a specific long-term outcome**.\n\nWe can observe goal-oriented behavior in existing systems like Stockfish, the top chess AI:\n\n*   Playing to win. Stockfish has a clear goal, and it consistently and relentlessly pursues this goal. Nothing the other player does can cause Stockfish to drop this goal; no interaction will cause Stockfish to “go easy” on the other player in the name of fairness, mercy, or any other goal. (All of this is fairly obvious in the case of a chess AI, but it’s worth noting explicitly because there’s a greater temptation to anthropomorphize AI systems and assume they have human-like goals when the AI is capable of more general human behaviors, is tasked with imitating humans, etc.)\n*   Strategic and tactical flexibility. In spite of this rigidity in its objective, Stockfish is extremely flexible at the level of strategy and tactics. Interfere with Stockfish’s plans or put an obstacle in its way, and Stockfish will immediately change its plans to skillfully account for the obstacle.\n*   Planning with foresight and creativity. Stockfish will anticipate possible future obstacles (and opportunities), and will construct and execute sophisticated long-term plans, including brilliant feints and novelties, to maximize its odds of winning.\n\nObservers who note that systems like ChatGPT don’t seem particularly goal-oriented also tend to note that ChatGPT is bad at long-term tasks like “writing a long book series with lots of foreshadowing” or “large-scale engineering projects”. They might not see that these two observations are connected.\n\nIn a sufficiently large and surprising world that keeps throwing wrenches into existing plans, the way to complete complex tasks over long time horizons is to (a) possess relatively powerful and general skills for anticipating and adapting to obstacles to your plans; and (b) possess a disposition to tenaciously continue in the pursuit of objectives, without getting distracted or losing motivation — like how Stockfish single-mindedly persists in trying to win.\n\nThe demand for AI to be able to skillfully achieve long-term objectives is high, and as AI gets better at this, we can expect AI systems to appear correspondingly more goal-oriented. We can see this in, e.g., OpenAI o1, which does more long-term thinking and planning than previous LLMs, and indeed empirically [acts more tenaciously](https://www.transformernews.ai/p/openai-o1-alignment-faking) than previous models.\n\nGoal-orientedness isn’t sufficient for ASI, or Stockfish would be a superintelligence. But it seems very close to necessary: An AI needs the mental machinery to strategize, adapt, anticipate obstacles, etc., and it needs the disposition to readily deploy this machinery on a wide range of tasks, in order to reliably succeed in complex long-horizon activities.\n\nAs a strong default, then, smarter-than-human AIs are very likely to stubbornly reorient towards particular targets, regardless of what wrench reality throws into their plans. This is a good thing if the AI’s goals are good, but it’s an extremely dangerous thing if the goals aren’t what developers intend:\n\nIf an AI’s goal is to move a ball up a hill, then from the AI’s perspective, humans who get in the way of the AI achieving its goal count as “obstacles” in the same way that a wall counts as an obstacle. The exact same mechanism that makes an AI useful for long-time-horizon real-world tasks — relentless pursuit of objectives in the face of the enormous variety of blockers the environment will throw one’s way — will also make the AI want to prevent humans from interfering in its work. This may only be a nuisance when the AI is less intelligent than humans, but it becomes an enormous problem when the AI is smarter than humans.\n\nFrom the AI’s perspective, modifying the AI’s goals counts as an obstacle. If an AI is optimizing a goal, and humans try to change the AI to optimize a new goal, then unless the new goal also maximizes the old goal, the AI optimizing goal 1 will want to avoid being changed into an AI optimizing goal 2, because this outcome scores poorly on the metric “is this the best way to ensure goal 1 is maximized?”. This means that iteratively improving AIs won’t always be an option: If an AI becomes powerful before it has the right goal, it will want to subvert attempts to change its goal, since any change to its goals will seem bad from the AI’s perspective.\n\nFor the same reason, shutting down the AI counts as an obstacle to the AI’s objective. For almost any goal an AI has, the goal is more likely to be achieved if the AI is operational, so that it can continue to work towards the goal in question. The AI doesn’t need to have a self-preservation instinct in the way humans do; it suffices that the AI be highly capable and goal-oriented at all. Anything that could potentially interfere with the system’s future pursuit of its goal is liable to be treated as a threat.\n\nPower, influence, and resources further most AI goals. As we’ll discuss in the section “[It would be lethally dangerous to build ASIs that have the wrong goals](https://intelligence.org/the-problem/#4_lethally_dangerous)”, the best way to avoid potential obstacles, and to maximize your chances of accomplishing a goal, will often be to maximize your power and influence over the future, to gain control of as many resources as possible, etc. This puts powerful goal-oriented systems in direct conflict with humans for resources and control.\n\nAll of this suggests that it is critically important that developers robustly get the right goals into ASI. However, the prospects for succeeding in this seem extremely dim under the current technical paradigm.\n\n### **3.  ASI is very likely to pursue the wrong goals.**\n\nDevelopers are unlikely to be able to imbue ASI with a deep, persistent care for worthwhile objectives. Having spent two decades studying the technical aspects of this problem, our view is that the field is nowhere near to being able to do this in practice.\n\nThe reasons artificial superintelligence is likely to exhibit unintended goals include:\n\n*   In modern machine learning, AIs are “grown”, not designed.\n*   The current AI paradigm is poorly suited to robustly instilling goals.\n*   Labs and the research community are not approaching this problem in an effective and serious way.\n\n**In modern machine learning, AIs are “grown”, not designed.**\n\nDeep learning algorithms build neural networks automatically. Geoffrey Hinton explains this point well in an [interview](https://youtu.be/qrvK_KuIeJk?t=288) on 60 Minutes:\n\n> **HINTON:** We have a very good idea of sort of roughly what it’s doing, but as soon as it gets really complicated, we don’t actually know what’s going on, any more than we know what’s going on in your brain.\n> \n> **PELLEY:** What do you mean, “We don’t know exactly how it works”? It was designed by people.\n> \n> **HINTON:** No, it wasn’t. What we did was we designed the learning algorithm. That’s a bit like designing the principle of evolution. But when this learning algorithm then interacts with data, it produces complicated neural networks that are good at doing things, but we don’t really understand exactly how they do those things.\n\nEngineers can’t tell you why a modern AI makes a given choice, but have nevertheless released increasingly capable systems year after year. AI labs are aggressively scaling up systems they don’t understand, with little ability to predict the capabilities of the next generation of systems.\n\nRecently, the young field of mechanistic interpretability has attempted to address the opacity of modern AI by mapping a neural network’s configuration to its outputs. Although there has been nonzero real progress in this area, interpretability pioneers are very clear that we’re still fundamentally in the dark about what’s going on inside these systems:\n\n*   Leo Gao of OpenAI: “I think it is quite accurate to say we don’t understand how neural networks work.” ([2024-6-16](https://x.com/nabla_theta/status/1802292064824242632))\n*   Neel Nanda of Google DeepMind: “As lead of the Google DeepMind mech interp team, I strongly seconded. It’s absolutely ridiculous to go from ‘we are making interp progress’ to ‘we are on top of this’ or ‘x-risk won’t be an issue’.” ([2024-6-16](https://x.com/NeelNanda5/status/1804613268356399185))\n\n(“X-risk” refers to “existential risk”, the risk of human extinction or similarly bad outcomes.)\n\nEven if effective interpretability tools were in reach, however, the prospects for achieving nontrivial robustness properties in ASI would be grim.\n\nThe internal machinery that could make an ASI dangerous is the same machinery that makes it work at all. (What looks like “power-seeking” in one context would be considered “good hustle” in another.) There are no dedicated “badness” circuits for developers to monitor or intervene on.\n\nMethods developers might use during training to reject candidate AIs with thought patterns they consider dangerous can have the effect of driving such thoughts “underground”, making it increasingly unlikely that they’ll be able to detect warning signs during training in the future.\n\nAs AI becomes more generally capable, it will become increasingly good at deception. The January 2024 “[Sleeper Agents](https://arxiv.org/abs/2401.05566)” paper by Anthropic’s testing team demonstrated that an AI given secret instructions in training not only was capable of keeping them secret during evaluations, but made strategic calculations (incompetently) about when to lie to its evaluators to maximize the chance that it would be released (and thereby be able to execute the instructions). Apollo Research made similar findings with regards to OpenAI’s o1-preview model released in September 2024 (as described in [their contributions to the o1-preview system card](https://assets.ctfassets.net/kftzwdyauwt9/67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf), p.10).\n\nThese issues will predictably become more serious as AI becomes more generally capable. The first AIs to inch across high-risk thresholds, however — such as noticing that they are in training and plotting to deceive their evaluators — are relatively bad at these new skills. This causes some observers to prematurely conclude that the behavior category is unthreatening.\n\nThe indirect and coarse-grained way in which modern machine learning “grows” AI systems’ internal machinery and goals means that we have little ability to predict the behavior of novel systems, little ability to robustly or precisely shape their goals, and no reliable way to spot early warning signs.\n\nWe expect that there are ways in principle to build AI that doesn’t have these defects, but this constitutes a long-term hope for what we might be able to do someday, not a realistic hope for near-term AI systems.\n\n**The current AI paradigm is poorly suited to robustly instilling goals.**\n\nDocility and goal agreement don’t come for free with high capability levels. An AI system can be able to answer an ethics test in the way its developers want it to, without thereby having human values. An AI can behave in docile ways when convenient, without actually being docile.\n\n**ASI alignment** is the set of technical problems involved in robustly directing superintelligent AIs at intended objectives.\n\nASI alignment runs into two classes of problem, discussed in [Hubinger et al.](https://arxiv.org/abs/1906.01820) — problems of **outer alignment**, and problems of **inner alignment**.\n\nOuter alignment, roughly speaking, is the problem of picking the right goal for an AI. (More technically, it’s the problem of ensuring the learning algorithm that builds the ASI is optimizing for what the programmers want.)This runs into issues such as “human values are too complex for us to specify them just right for an AI; but if we only give ASI some of our goals, the ASI is liable to trample over our other goals in pursuit of those objectives”. Many goals are safe at lower capability levels, but dangerous for a sufficiently capable AI to carry out in a maximalist manner. The literary trope here is “be careful what you wish for”. Any given goal is unlikely to be safe to delegate to a sufficiently powerful optimizer, because the developers are not superhuman and can’t predict in advance what strategies the ASI will think of.\n\nInner alignment, in contrast, is the problem of figuring out how to get particular goals into ASI at all, even imperfect and incomplete goals. The literary trope here is “just because you summoned a demon doesn’t mean that it will do what you say”. Failures of inner alignment look like “we tried to give a goal to the ASI, but we failed and it ended up with an unrelated goal”.\n\n**Outer alignment and inner alignment are both unsolved problems**, and in this context, **inner alignment is the more fundamental issue**. Developers aren’t on track to be able to cause a catastrophe of the “be careful what you wish for” variety, because realistically, we’re extremely far from being able to metaphorically “make wishes” with an ASI.\n\nModern methods in AI are a poor match for tackling inner alignment. Modern AI development doesn’t have methods for getting particular inner properties into a system, or for verifying that they’re there. Instead, modern machine learning concerns itself with observable behavioral properties that you can run a loss function over.\n\nWhen minds are grown and shaped iteratively, like modern AIs are, they won’t wind up pursuing the objectives they’re trained to pursue. Instead, training is far more likely to lead them to pursue unpredictable proxies of the training targets, which are brittle in the face of increasing intelligence. By way of analogy: Human brains were ultimately “designed” by natural selection, which had the simple optimization target “maximize inclusive genetic fitness”. The actual goals that ended up instilled in human brains, however, were far more complex than this, and turned out to only be fragile correlates for inclusive genetic fitness. Human beings, for example, pursue proxies of good nutrition, such as sweet and fatty flavors. These proxies were once reliable indicators of healthy eating, but were brittle in the face of technology that allows us to invent novel junk foods. The case of humans illustrates that even when you have a very exact, very simple loss function, outer optimization for that loss function doesn’t generally produce inner optimization in that direction. Deep learning is much less random than natural selection at finding adaptive configurations, but it shares the relevant property of finding minimally viable simple solutions first and incrementally building on them.\n\nMany alignment problems relevant to superintelligence don’t naturally appear at lower, passively safe levels of capability. This puts us in the position of needing to solve many problems on the first critical try, with little time to iterate and no prior experience solving the problem on weaker systems. Today’s AIs require a long process of iteration, experimentation, and feedback to hammer them into the apparently-obedient form the public is allowed to see. This hammering changes surface behaviors of AIs without deeply instilling desired goals into the system. This can be seen in cases like [Sydney](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html), where the public was able to see more of the messy details behind the surface-level polish. In light of this, and in light of the opacity of modern AI models, the odds of successfully aligning ASI if it’s built in the next decade seem extraordinarily low. Modern AI methods are all about repeatedly failing, learning from our mistakes, and iterating to get better; AI systems are highly unpredictable, but we can get them working eventually by trying many approaches until one works. In the case of ASI, we will be dealing with a highly novel system, in a context where our ability to safely fail is extremely limited: we can’t charge ahead and rely on our ability to learn from mistakes when the cost of some mistakes is an extinction event.\n\nIf you’re deciding whether to hand a great deal of power to someone and you want to know whether they would abuse this power, you won’t learn anything by giving the candidate power in a board game where they know you’re watching. Analogously, situations where an ASI has no real option to take over are fundamentally different from situations where it does have a real option to take over. No amount of purely behavioral training in a toy environment will reliably eliminate power-seeking in real-world settings, and no amount of behavioral testing in toy environments will tell us whether we’ve made an ASI genuinely friendly. “Lay low and act nice until you have an opportunity to seize power” is a sufficiently obvious strategy that even relatively unintelligent humans can typically manage it; ASI trivially clears that bar. In principle, we could imagine developing a theory of intelligence that relates ASI training behavior to deployment behavior in a way that addresses this issue. We are nowhere near to having such a theory today, however, and those theories can fundamentally only be tested once in the actual environment where the AI is much much smarter and sees genuine takeover options. If you can’t properly test theories without actually handing complete power to the ASI and seeing what it does — and causing an extinction event if your theory turned out to be wrong — then there’s very little prospect that your theory will work in practice.\n\nThe most important alignment technique used in today’s systems, Reinforcement Learning from Human Feedback (RLHF), trains AI to produce outputs that it predicts would be rated highly by human evaluators. This already creates its own predictable problems, such as style-over-substance and flattery. This method breaks down completely, however, when AI starts working on problems where humans aren’t smart enough to fully understand the system’s proposed solutions, including the long-term consequences of superhumanly sophisticated plans and superhumanly complex inventions and designs.\n\nOn a deeper level, the limitation of reinforcement learning strategies like RLHF stems from the fact that these techniques are more about incentivizing local behaviors than about producing an internally consistent agent that deeply and robustly optimizes a particular goal the developers intended.\n\nIf you train a tiger not to eat you, you haven’t made it share your desire to survive and thrive, with a full understanding of what that means to you. You have merely taught it to associate certain behaviors with certain outcomes. If its desires become stronger than those associations, as could happen if you forget to feed it, the undesired behavior will come through. And if the tiger were a little smarter, it would not need to be hungry to conclude that the threat of your whip would immediately end if your life ended.\n\n<table style=\"background-color:transparent\"><tbody><tr><td style=\"background-color:rgba(128, 128, 128, 0.07);border-color:rgba(128, 128, 128, 0.5);padding:15px;vertical-align:top\">Learn more:<a href=\"https://intelligence.org/agi-ruin\"> <strong>What are the details of why ASI alignment looks extremely technically difficult?</strong></a></td></tr></tbody></table>\n\nAs a consequence, MIRI doesn’t see any viable quick fixes or workarounds to misaligned ASI.\n\n*   If an ASI has the wrong goals, then it won’t be possible to safely use the ASI for any complex real-world operation. One could theoretically keep an ASI from doing anything harmful — for example, by preemptively burying it deep in the ground without any network connections or human contact — but such an AI would be useless. People are building AI because they want it to radically impact the world; they are consequently giving it the access it needs to be impactful.\n*   One could attempt to deceive an ASI in ways that make it more safe. However, attempts to deceive a superintelligence are prone to fail, including in ways we can’t foresee. A feature of intelligence is the ability to notice the contradictions and gaps in one’s understanding, and interrogate them. In May 2024, when Anthropic modified their Claude AI into thinking that the answer to every request [involved the Golden Gate Bridge](https://www.anthropic.com/research/mapping-mind-language-model), it [floundered](https://archive.is/u7HJm) in some cases, noticing the contradictions in its replies and trying to route around the errors in search of better answers. It’s hard to sell a false belief to a mind whose complex model of the universe disagrees with your claim; and as AI becomes more general and powerful, this difficulty only increases.\n*   Plans to align ASI using unaligned AIs are similarly unsound. Our 2024 “[Misalignment and Catastrophe](https://intelligence.org/wp-content/uploads/2024/02/Misalignment_and_Catastrophe.pdf)” paper explores the hazards of using unaligned AI to do work as complex as alignment research.\n\n**Labs and the research community are not approaching this problem in an effective and serious way.**\n\nIndustry efforts to solve ASI alignment have to date been minimal, often seeming to serve as a fig leaf to ward off regulation. Labs’ general laxness on information security, alignment, and strategic planning suggests that the “move fast and break things” culture that’s worked well for accelerating capabilities progress is not similarly useful when it comes to exercising foresight and responsible priority-setting in the domain of ASI.\n\nOpenAI, the developer of ChatGPT, admits that today’s most important methods of steering AI won’t scale to the superhuman regime. In July of 2023, OpenAI announced a new team with their “[Introducing Superalignment](https://openai.com/index/introducing-superalignment/)” page. From the page:\n\n> Currently, we don’t have a solution for steering or controlling a potentially superintelligent AI, and preventing it from going rogue. Our current techniques for aligning AI, such as reinforcement learning from human feedback, rely on humans’ ability to supervise AI. But humans won’t be able to reliably supervise AI systems much smarter than us, and so our current alignment techniques will not scale to superintelligence. We need new scientific and technical breakthroughs.\n\nTen months later, OpenAI disbanded their superintelligence alignment team in the wake of mass resignations, as researchers like Superalignment team lead Jan Leike [claimed](https://twitter.com/janleike/status/1791498174659715494) that OpenAI was systematically cutting corners on safety and robustness work and severely under-resourcing their team. Leike had previously said, in an August 2023 [interview](https://80000hours.org/podcast/episodes/jan-leike-superalignment/), that the probability of extinction-level catastrophes from ASI was probably somewhere between 10% and 90%.\n\nGiven the research community’s track record to date, we don’t think a well-funded crash program to solve alignment would be able to correctly identify solutions that won’t kill us. This is an organizational and bureaucratic problem, and not just a technical one. It would be difficult to find enough experts who can identify non-lethal solutions to make meaningful progress, in part because the group must be organized by someone with the expertise to correctly identify these individuals in a sea of people with strong incentives to lie (both to themselves and to regulators) about how promising their favorite proposal is.\n\nIt would also be difficult to ensure that the organization is run by, and only answerable to, experts who are willing and able to reject any bad proposals that bubble up, even if this initially means rejecting literally every proposal. There just aren’t enough experts in that class right now.\n\nOur current view is that a survivable way forward will likely require ASI to be delayed for a long time. The scale of the challenge is such that we could easily see it taking multiple generations of researchers exploring technical avenues for aligning such systems, and bringing the fledgling alignment field up to speed with capabilities. It seems extremely unlikely, however, that the world has that much time.\n\n### **4\\. It would be lethally dangerous to build ASIs that have the wrong goals.**\n\nIn “[ASI is very likely to exhibit goal-oriented behavior](https://intelligence.org/the-problem/#2_goal-oriented_behavior)”, we introduced the chess AI Stockfish. Stuart Russell, the author of the most widely used AI textbook, has previously [explained](https://youtu.be/mukaRhQTMP8?t=36) AI-mediated extinction via a similar analogy to chess AI:\n\n> At the state of the art right now, humans are toast. No matter how good you are at playing chess, these programs will just wipe the floor with you, even running on a laptop.\n> \n> I want you to imagine that, and just extend that idea to the whole world. \\[…\\] The world is a larger chess board, on which potentially at some time in the future machines will be making better moves than you. They’ll be taking into account more information, and looking further ahead into the future, and so if you are playing a game against a machine in the world, the assumption is that at some point we will lose.\n\nIn a July 2023 [US Senate hearing](https://cdss.berkeley.edu/news/stuart-russell-testifies-ai-regulation-us-senate-hearing), Russell testified that “achieving AGI \\[artificial general intelligence\\] would present potential catastrophic risks to humanity, up to and including human extinction”.\n\nStockfish captures pieces and limits its opponent’s option space, not because Stockfish hates chess pieces or hates its opponent but because these actions are instrumentally useful for its objective (“win the game”). The danger of superintelligence is that ASI will be trying to “win” (at a goal we didn’t intend), but with the game board replaced with the physical universe.\n\nJust as Stockfish is ruthlessly effective in the narrow domain of chess, AI that automates all key aspects of human intelligence will be ruthlessly effective in the real world. And just as humans are vastly outmatched by Stockfish in chess, we can expect to be outmatched in the world at large once AI is able to play that game at all.\n\nIndeed, outmaneuvering a strongly smarter-than-human adversary is far more difficult in real life than in chess. Real life offers a far more multidimensional option space: we can anticipate a hundred different novel attack vectors from a superintelligent system, and still not have scratched the surface.\n\nUnless it has worthwhile goals, ASI will predictably put our planet to uses incompatible with our continued survival, in the same basic way that we fail to concern ourselves with the crabgrass at a construction site. This extreme outcome doesn’t require any malice, resentment, or misunderstanding on the part of the ASI; it only requires that ASI behaves like a new intelligent species that is indifferent to human life, and that strongly surpasses our intelligence.\n\nWe can decompose the problem into two parts:\n\n*   Misaligned ASI will be motivated to take actions that disempower and wipe out humanity, either directly or as a side-effect of other operations.\n*   ASI will be able to destroy us.\n\n**Misaligned ASI will be motivated to take actions that disempower and wipe out humanity.**\n\nThe basic reason for this is that an ASI with non-human-related goals will generally want to maximize its control over the future, and over whatever resources it can acquire, to ensure that its goals are achieved.\n\nSince this is true for a wide variety of goals, it operates as a default endpoint for a variety of paths AI development could take. We can predict that ASI will want very basic things like “more resources” and “greater control” — at least if developers fail to align their systems — without needing to speculate about what specific ultimate objectives an ASI might pursue.\n\n(Indeed, trying to call the objective in advance seems hopeless if the situation at all resembles what we see in nature. Consider how difficult it would have been to guess in advance that human beings would end up with the many specific goals we have, from “preferring frozen ice cream over melted ice cream” to “enjoying slapstick comedy”.)\n\nThe extinction-level danger from ASI follows from several behavior categories that a wide variety of ASI systems are likely to exhibit:\n\n*   Resource extraction**.** Humans depend for their survival on resource flows that are also instrumentally useful for almost any other goal. Air, sunlight, water, food, and even the human body are all made of matter or energy that can be repurposed to help with other objectives on the margin. In slogan form: “The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.”\n*   Competition for control. Humans are a potential threat and competitor to any ASI. If nothing else, we could threaten an ASI by building a second ASI with a different set of goals. If the ASI has an easy way to eliminate all rivals and never have to worry about them again, then it’s likely to take that option.\n*   Infrastructure proliferation. Even if an ASI is too powerful to view humans as threats, it is likely to quickly wipe humans out as a side-effect of extracting and utilizing local resources. If an AI is thinking at superhuman speeds and building up self-replicating machinery exponentially quickly, the Earth could easily become uninhabitable within a few months, as engineering megaprojects emit waste products and heat that can rapidly make the Earth inhospitable for biological life.\n\nPredicting the specifics of what an ASI would do seems impossible today. This is not, however, grounds for optimism, because most possible goals an ASI could exhibit would be very bad for us, and most possible states of the world an ASI could attempt to produce would be incompatible with human life.\n\nIt would be a fallacy to reason in this case from “we don’t know the specifics” to “good outcomes are just as likely as bad ones”, much as it would be a fallacy to say “I’m either going to win the lottery or lose it, therefore my odds of winning are 50%”. Many different pathways in this domain appear to converge on catastrophic outcomes for humanity — most of the “lottery tickets” humanity could draw will be losing numbers.\n\nThe arguments for optimism here are uncompelling. Ricardo’s Law of Comparative Advantage, for example, has been cited as a possible reason to expect ASI to keep humans around indefinitely, even if the ASI doesn’t ultimately care about human welfare. In the context of microeconomics, Ricardo’s Law teaches that even a strictly superior agent can benefit from trading with a weaker agent.\n\nThis law breaks down, however, when one partner has more to gain from overpowering the other than from voluntarily trading. This can be seen, for example, in the fact that humanity didn’t keep “trading” with horses after we invented the automobile — we replaced them, converting surplus horses into glue.\n\nHumans found more efficient ways to do all of the practical work that horses used to perform, at which point horses’ survival depended on how much we sentimentally care about them, not on horses’ usefulness in the broader economy. Similarly, keeping humans around is unlikely to be the most efficient solution to any problem that the AI has. E.g., rather than employing humans to conduct scientific research, the AI can build an ever-growing number of computing clusters to run more instances of itself, or otherwise automate research efforts.\n\n**ASI will be able to destroy us.**\n\nAs a minimum floor on capabilities, we can imagine ASI as a small nation populated entirely by brilliant human scientists who can work around the clock at ten thousand times the speed of normal humans.\n\nThis is a minimum both because computers can be even faster than this, and because digital architectures should allow for qualitatively better thoughts and methods of information-sharing than humans are capable of.\n\nTransistors can switch states millions to billions of times faster than synaptic connections in the human brain. This would mean that every week, the ASI makes an additional two hundred years of scientific progress. The core reason to expect ASI to win decisively in a conflict, then, is the same as the reason a 21st-century military would decisively defeat an 11th-century one: technological innovation.\n\nDeveloping new technologies often requires test cycles and iteration. A civilization thinking at 10,000 times the speed of ours cannot necessarily develop technology 10,000 times faster, any more than a car that’s 100x faster would let you shop for groceries 100x faster — traffic, time spent in the store, etc. will serve as a bottleneck.\n\nWe can nonetheless expect such a civilization to move extraordinarily quickly, by human standards. Smart thinkers can find all kinds of ways to shorten development cycles and reduce testing needs.\n\nConsider the difference in methods between Google software developers, who rapidly test multiple designs a day, and designers of space probes, who plan carefully and run cheap simulations so they can get the job done with fewer slow and expensive tests.\n\nTo a mind thinking faster than a human, every test is slow and expensive compared to the speed of thought, and it can afford to treat everything like a space probe. One implication of this is that ASI is likely to prioritize the development and deployment of small-scale machinery (or engineered microorganisms) which, being smaller, can run experiments, build infrastructure, and conduct attacks orders of magnitude faster than humans and human-scale structures.\n\nA superintelligent adversary will not reveal its full capabilities and telegraph its intentions. It will not offer a fair fight. It will make itself indispensable or undetectable until it can strike decisively and/or seize an unassailable strategic position. If needed, the ASI can consider, prepare, and attempt many takeover approaches simultaneously. Only one of them needs to work for humanity to go extinct.\n\nThere are a number of major obstacles to recognizing that a system is a threat before it has a chance to do harm, even for experts with direct access to its internals.\n\n<table style=\"background-color:transparent\"><tbody><tr><td style=\"background-color:rgba(128, 128, 128, 0.07);border-color:rgba(128, 128, 128, 0.5);padding:15px;vertical-align:top\">Learn more:<a href=\"http://intelligence.org/notes/takeover\"> <strong>What’s an example of how ASI takeover could occur?</strong></a></td></tr></tbody></table>\n\nRecognizing that a particular AI is a threat, however, is not sufficient to solve the problem. At the project level, identifying that a system is dangerous doesn’t put us in a position to make that system safe. Cautious projects may voluntarily halt, but this does nothing to prevent other, incautious projects from storming ahead.\n\nAt the global level, meanwhile, clear evidence of danger doesn’t necessarily mean that there will be the political will to internationally halt development. AI is likely to become increasingly entangled with the global economy over time, making it increasingly costly and challenging to shut down state-of-the-art AI services. Steps could be taken today to prevent critical infrastructure from becoming dependent on AI, but the window for this is plausibly closing.\n\nMany analyses seriously underestimate the danger posed by building systems that are far smarter than any human. Four common kinds of error we see are:\n\n*   [Availability bias](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) and overreliance on analogies. AI extinction scenarios can sound extreme and fantastical. Humans are used to thinking about unintelligent machines and animals, and intelligent humans. “It’s a machine, but one that’s intelligent in the fashion of a human” is something genuinely new, and people make different errors from trying to pattern-match AI to something familiar, rather than modeling it on its own terms.\n*   Underestimating feedback loops. AI is used today to accelerate software development, including AI research. As AI becomes more broadly capable, an increasing amount of AI progress is likely to be performed by AIs themselves. This can rapidly spiral out of control, as AIs find ways to improve on their own ability to do AI research in a self-reinforcing loop.\n*   Underestimating exponential growth. Many plausible ASI takeover scenarios route through building self-replicating biological agents or machines. These scenarios make it relatively easy for ASI to go from “undetectable” to “ubiquitous”, or to execute covert strikes, because of the speed at which doublings can occur and the counter-intuitively small number of doublings required.\n*   Overestimating human cognitive ability, relative to what’s possible. Even in the absence of feedback loops, AI systems routinely blow humans out of the water in narrow domains. As soon as AI can do X at all (or very soon afterwards), AI vastly outstrips any human’s ability to do X. This is a common enough pattern in AI, at this point, to barely warrant mentioning. It would be incredibly strange if this pattern held for every skill AI is already good at, but suddenly broke for the skills AI can’t yet match top humans on, such as novel science and engineering work.\n\nWe should expect ASIs to vastly outstrip humans in technological development soon after their invention. As such, we should also expect ASI to very quickly accumulate a decisive strategic advantage over humans, as they outpace humans in this strategically critical ability to the same degree they’ve outpaced humans on hundreds of benchmarks in the past.\n\nThe main way we see to avoid this catastrophic outcome is to not build ASI at all, at minimum until a scientific consensus exists that we can do so without destroying ourselves.\n\n### **5\\. Catastrophe can be averted via a sufficiently aggressive policy response.**\n\nIf anyone builds ASI, everyone dies. This is true whether it’s built by a private company or by a military, by a liberal democracy or by a dictatorship.\n\nASI is strategically very novel. Conventional powerful technology isn’t an intelligent adversary in its own right; typically, whoever builds the technology “has” that technology, and can use it to gain an advantage on the world stage.\n\nAgainst a technical backdrop that’s at all like the current one, ASI instead functions like a sort of global suicide bomb — a volatile technology that blows up and kills its developer (and the rest of the world) at an unpredictable time. If you build smarter-than-human AI, you don’t thereby “have” an ASI; rather, the ASI has you.\n\nProgress toward ASI needs to be halted until ASI can be made alignable. Halting ASI progress would require an effective worldwide ban on its development, and tight control over the factors of its production.\n\nThis is a large ask, but domestic oversight in the US, mirrored by a few close allies, will not suffice. This is not a case where we just need the “right” people to build it before the “wrong” people do.\n\nA “wait and see” approach to ASI is probably not survivable, given the fast pace of AI development and the difficulty of predicting the point of no return — the threshold where ASI is achieved.\n\nOn our view, **the international community’s top immediate priority should be creating an “off switch” for frontier AI development**. By “creating an off switch”, we mean putting in place the systems and infrastructure necessary to either shut down frontier AI projects or enact a general ban.\n\nCreating an off switch would involve identifying the relevant parties, tracking the relevant hardware, and requiring that advanced AI work take place within a limited number of monitored and secured locations. It extends to building out the protocols, plans, and chain of command to be followed in the event of a shutdown decision.\n\nAs the off-switch could also provide resilience to more limited AI mishaps, we hope it will find broader near-term support than a full ban. For “limited AI mishaps”, think of any lower-stakes situation where it might be desirable to shut down one or more AIs for a period of time. This could be something like a bot-driven misinformation cascade during a public health emergency, or a widespread Internet slowdown caused by AIs stuck in looping interactions with each other and generating vast amounts of traffic. Without off-switch infrastructure, any response is likely to be haphazard — delayed by organizational confusion, mired in jurisdictional disputes, beset by legal challenges, and unable to avoid causing needless collateral harm.\n\nAn off-switch can only prevent our extinction from ASI if it has sufficient reach and is actually used to shut down progress toward ASI sufficiently soon. If humanity is to survive this dangerous period, it will have to stop treating AI as a domain for international rivalry and demonstrate a collective resolve equal to the scale of the threat.\n\n[^fpb6rwvoxuo]: This essay was primarily written by Rob Bensinger, with major contributions throughout by Mitchell Howe. It was edited by William and reviewed by Nate Soares and Eliezer Yudkowsky, and the project was overseen by Gretta Duleba. It was a team effort, and we're grateful to others who provided feedback (at MIRI and beyond).",
      "plaintextDescription": "This is a new introduction to AI as an extinction threat, previously posted to the MIRI website in February alongside a summary. It was written independently of Eliezer and Nate's forthcoming book, If Anyone Builds It, Everyone Dies, and isn't a sneak peak of the book. Since the book is long and costs money, we expect this to be a valuable resource in its own right even after the book comes out next month.[1]\n\nThe stated goal of the world’s leading AI companies is to build AI that is general enough to do anything a human can do, from solving hard problems in theoretical physics to deftly navigating social environments. Recent machine learning progress seems to have brought this goal within reach. At this point, we would be uncomfortable ruling out the possibility that AI more capable than any human is achieved in the next year or two, and we would be moderately surprised if this outcome were still two decades away.\n\nThe current view of MIRI’s research scientists is that if smarter-than-human AI is developed this decade, the result will be an unprecedented catastrophe. The CAIS Statement, which was widely endorsed by senior researchers in the field, states:\n\n> Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n\nWe believe that if researchers build superintelligent AI with anything like the field’s current technical understanding or methods, the expected outcome is human extinction.\n\n“Research labs around the world are currently building tech that is likely to cause human extinction” is a conclusion that should motivate a rapid policy response. The fast pace of AI, however, has caught governments and the voting public flat-footed. This document will aim to bring readers up to speed, and outline the kinds of policy steps that might be able to avert catastrophe.\n\nKey points in this document:\n\n * There isn’t a ceiling at human-level capabilities.\n * ASI is very likely to exhibit go",
      "wordCount": 7921
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "J482p4hJBevBbwdmF",
    "title": "MIRI Newsletter #123",
    "slug": "miri-newsletter-123",
    "url": "https://intelligence.org/2025/07/03/miri-newsletter-123/",
    "baseScore": 54,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-07-03T22:56:42.045Z",
    "contents": {
      "markdown": "*If Anyone Builds It, Everyone Dies*\n------------------------------------\n\n*   As we [announced](https://intelligence.org/2025/05/15/yudkowsky-and-soares-announce-major-new-book-if-anyone-builds-it-everyone-dies/) last month, Eliezer and Nate have a book coming out this September: *If Anyone Builds It, Everyone Dies*. This is MIRI’s major attempt to warn the policy world and the general public about AI. [Preorders are live now](https://ifanyonebuildsit.com/#preorder), and are exceptionally helpful.\n*   **Preorder Bonus:** We’re hosting two exclusive virtual events for those who preorder the book! The first is a chat between Nate Soares and Tim Urban ([Wait But Why](https://waitbutwhy.com/)) followed by a Q&A, on August 10 @ noon PT. The second is a Q&A with both Nate and Eliezer in September (date and time TBD). For details, and to obtain access, head to [ifanyonebuildsit.com/events](http://ifanyonebuildsit.com/events).\n*   If you have graphic design chops, you can give us a hand by joining the [advertisement design competition](https://intelligence.org/2025/07/01/iabied-advertisement-design-competition/) for the book.\n*   As Malo [recently announced](https://intelligence.org/2025/06/18/new-endorsements-for-if-anyone-builds-it-everyone-dies/), advance copies of the book have been getting an incredible reception, with blurbs from:\n    *   computer security guru Bruce Schneier;\n    *   Suzanne Spaulding, former head of the DHS Cybersecurity and Infrastructure Security Agency, the US government’s main agency for cybersecurity and critical infrastructure security;\n    *   Jack Shanahan, a retired three-star general and the inaugural director of the Pentagon’s Joint AI Center, the coordinating hub for bringing AI to every branch of the US military;\n    *   Jon Wolfsthal, the Obama administration’s senior nuclear security advisor;\n    *   legendary biologist George Church;\n    *   Nobel-winning economist Ben Bernanke;  \n        … and [many others](https://ifanyonebuildsit.com/praise).\n*   Nate has written a [follow-up post](https://intelligence.org/2025/06/26/a-case-for-courage-when-speaking-of-ai-danger/) arguing that the time is ripe to speak the truth about AI danger.  \n     \n\nOther MIRI updates\n------------------\n\n*   In a new [research agenda](https://intelligence.org/2025/05/01/ai-governance-to-avoid-extinction-the-strategic-landscape-and-actionable-research-questions/), MIRI’s Technical Governance Team describes the strategic landscape of AI governance and identifies questions that, if answered, could provide insights for preventing an AI catastrophe.\n*   Announcing [The Problem](https://intelligence.org/the-problem/) — MIRI’s new go-to explainer for AI x-risk (at least until our book comes out). We expect this to be an important resource for people new to this topic, and we’ve launched it alongside major updates to [our website](https://intelligence.org/).\n*   Eliezer [visited Robinson Erhardt’s podcast](https://youtu.be/0QmDcQIvSDc?feature=shared), Nate was [interviewed by Will Cain](https://x.com/WillCainShow_/status/1927475138796138845) on Fox News, and Malo Bourgon [went on Liv Boeree’s podcast](https://www.winwinpodcast.com/p/why-we-probably-shouldnt-race-to).\n*   The Technical Governance Team [submitted recommendations](https://x.com/Volty/status/1900687901308186896) in response to the RFI on the Development of an AI Action Plan.\n*   Martin Lucas joined the operations team at MIRI. Previously, Martin was on the ops team at Anthropic. We’re excited to have him aboard!\n*   In the new MIRI Single Author Series, authors share their individual perspectives on topics relevant to MIRI’s mission. We have published several new pieces as part of this series:\n    *   Max Harms shares [some of his thoughts](https://intelligence.org/2025/04/09/thoughts-on-ai-2027/) on [AI 2027](https://ai-2027.com/), a scenario by Daniel Kokotajlo’s team.\n    *   In [Refining MAIM: Identifying Changes Required to Meet Conditions for Deterrence](https://intelligence.org/2025/04/11/refining-maim-identifying-changes-required-to-meet-conditions-for-deterrence/), David Abecassis discusses limitations of the deterrence strategy in [*Superintelligence Strategy*](https://www.nationalsecurity.ai/).\n    *   In [So You Want to Work at a Frontier AI Lab](https://intelligence.org/2025/06/11/so-you-want-to-work-at-a-frontier-ai-lab/), Joe Rogero explains why he doesn’t buy the arguments for working at frontier labs.\n    *   In [Takeover Not Required](https://intelligence.org/2025/03/14/takeover-not-required/), Duncan Sabien argues that society is likely to willingly hand over control to ASI.\n    *   In [a response to OpenAI’s “How we think about safety and alignment,”](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/) Harlan Stewart raises objections to OpenAI’s strategy and messaging.\n    *   Joe Collman, Joe Rogero, and William Brewer argue that there are [dangerous gaps in AI labs’ safety frameworks](https://intelligence.org/2025/04/09/existing-safety-frameworks-imply-unreasonable-confidence/), reflecting the industry’s systematic overconfidence.",
      "plaintextDescription": "If Anyone Builds It, Everyone Dies\n * As we announced last month, Eliezer and Nate have a book coming out this September: If Anyone Builds It, Everyone Dies. This is MIRI’s major attempt to warn the policy world and the general public about AI. Preorders are live now, and are exceptionally helpful.\n * Preorder Bonus: We’re hosting two exclusive virtual events for those who preorder the book! The first is a chat between Nate Soares and Tim Urban (Wait But Why) followed by a Q&A, on August 10 @ noon PT. The second is a Q&A with both Nate and Eliezer in September (date and time TBD). For details, and to obtain access, head to ifanyonebuildsit.com/events.\n * If you have graphic design chops, you can give us a hand by joining the advertisement design competition for the book.\n * As Malo recently announced, advance copies of the book have been getting an incredible reception, with blurbs from:\n   * computer security guru Bruce Schneier;\n   * Suzanne Spaulding, former head of the DHS Cybersecurity and Infrastructure Security Agency, the US government’s main agency for cybersecurity and critical infrastructure security;\n   * Jack Shanahan, a retired three-star general and the inaugural director of the Pentagon’s Joint AI Center, the coordinating hub for bringing AI to every branch of the US military;\n   * Jon Wolfsthal, the Obama administration’s senior nuclear security advisor;\n   * legendary biologist George Church;\n   * Nobel-winning economist Ben Bernanke;\n     … and many others.\n * Nate has written a follow-up post arguing that the time is ripe to speak the truth about AI danger.\n    \n\n\nOther MIRI updates\n * In a new research agenda, MIRI’s Technical Governance Team describes the strategic landscape of AI governance and identifies questions that, if answered, could provide insights for preventing an AI catastrophe.\n * Announcing The Problem — MIRI’s new go-to explainer for AI x-risk (at least until our book comes out). We expect this to be an important resource for peo",
      "wordCount": 568
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wC5c8kFExofeLoyn9",
    "title": "MIRI’s 2024 End-of-Year Update",
    "slug": "miri-s-2024-end-of-year-update",
    "url": null,
    "baseScore": 98,
    "voteCount": 48,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-12-03T04:33:47.499Z",
    "contents": {
      "markdown": "MIRI is a nonprofit research organization with a mission of addressing the most serious hazards posed by smarter-than-human artificial intelligence. In our [general strategy update](https://intelligence.org/2024/01/04/miri-2024-mission-and-strategy-update/) and [communications strategy update](https://intelligence.org/2024/05/29/miri-2024-communications-strategy/) earlier this year, we announced a new strategy that we’re executing on at MIRI, and several new teams we’re spinning up as a result. This post serves as a status update on where things stand at MIRI today.\n\nWe originally planned to run a formal fundraiser this year, our first one since 2019. However, we find ourselves with just over two years of reserves, which is more than we’ve typically had in our past fundraisers.\n\nInstead, we’ve written this brief update post, discussing recent developments at MIRI, projects that are in the works, and our basic funding status. In short: We aren’t in urgent need of more funds, but we do have a fair amount of uncertainty about what the funding landscape looks like now that MIRI is pursuing a very different strategy than we have in the past. Donations and expressions of donor interest may be especially useful over the next few months, to help us get a sense of how easy it will be to grow and execute on our new plans.\n\nYou can donate via our [Donate](https://intelligence.org/donate) page, or get in touch with us [here](https://intelligence.org/contact). For more details, read on.\n\nAbout MIRI\n----------\n\nFor most of MIRI’s 24-year history, our focus has been on a set of technical challenges: “How could one build AI systems that are far smarter than humans, without causing an extinction-grade catastrophe?”\n\nOver the last few years, we’ve come to the conclusion that this research, both at MIRI and in the larger field, has gone far too slowly to prevent disaster. Although we continue to support some AI alignment research efforts, we now believe that **absent an international government effort to suspend frontier AI research, an extinction-level catastrophe is extremely likely**.\n\nAs a consequence, our new focus is on informing policymakers and the general public about the dire situation, and attempting to mobilize a response.\n\n<table><tbody><tr><td><p>Learn more:</p><ul><li>For a basic overview of why AI progress poses such an extreme risk, see <a href=\"https://www.ted.com/talks/eliezer_yudkowsky_will_superintelligent_ai_end_the_world?subtitle=en\"><strong>Will Superintelligent AI End the World?</strong></a> (short TED talk) and <a href=\"https://intelligence.org/agi-ruin\"><strong>AGI Ruin</strong></a> (longer explainer).</li><li>For an overview of what’s changed at MIRI and what we’ve been working on, see our <a href=\"https://intelligence.org/2024/01/04/miri-2024-mission-and-strategy-update/\"><strong>2024 Mission and Strategy Update</strong></a> and our <a href=\"https://intelligence.org/2024/05/29/miri-2024-communications-strategy/\"><strong>2024 Communications Strategy</strong></a>.</li><li>For more resources, check out our <a href=\"https://intelligence.org/media/\"><strong>Media</strong></a> and <a href=\"https://intelligence.org/research\"><strong>Research</strong></a> pages, or <a href=\"https://intelligence.org/newsletter/\">subscribe</a> to our monthly <a href=\"https://intelligence.org/category/newsletters/\"><strong>Newsletter</strong></a>.</li></ul></td></tr></tbody></table>\n\nWe began to scale back our alignment research program starting in late 2020 and 2021, and began pivoting to communications and governance research in 2023. Over the past year, we’ve been focusing on hiring and training new staff, and we have a number of projects in the pipeline, including new papers from our [technical governance team](https://techgov.intelligence.org/), a new introductory resource for AI risk, and a forthcoming book.\n\nWe believe that our hard work in finding the right people is really paying off; we’re proud of the new teams we’re growing, and we’re eager to add a few more communications specialists and technical governance researchers in 2025. Expect some more updates over the coming months as our long-form projects come closer to completion.\n\nFunding Status\n--------------\n\nAs noted above, MIRI hasn’t run a formal fundraiser since 2019. Thanks to the major support we received in [2020](https://intelligence.org/2020/04/27/miris-largest-grant-to-date/) and [2021](https://intelligence.org/2021/05/13/two-major-donations/), as well as our scaled-back alignment research ambitions as we reevaluated our strategy in 2021–2022, we haven’t needed to raise additional funds in some time.\n\nOur yearly expenses in 2019–2023 ranged from $5.4M to $7.7M, with a high in 2020 and a low in 2022. Our projected spending for 2024 is $5.6M, and (with a high degree of uncertainty, mostly about hiring outcomes) we expect our 2025 expenses to be in the range of $6.5M to $7M.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/59bd55ecd96242cf77f86da3415eb7a43a19d4e1d327d1b0.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2e58ad9bea9597c326f4d115d970e8fcc1f3d2f28b542587.png)\n\nWith our new strategy, we are in the process of scaling up our new communications and technical governance teams. Earlier this year, we expected to have dipped below two years of reserves at this point, but (1) our 2024 expenses were lower than expected, and (2) the majority of our reserves are invested in a broad range of US and foreign stocks and bonds, and the market performed well in 2024. As a consequence, given our mainline staff count and hiring estimates, we now project that our $16M end-of-year reserves will be enough to fund our activities for just over two years.\n\nWe generally aim to maintain at least two years of reserves, and thanks to the generosity of our past donors, we’re still able to hit that target.\n\nHowever, we’re also in a much more uncertain funding position than we have been in recent years: we don’t know how many of the funders who supported our alignment research in 2019 will also be interested in supporting our communications and policy efforts going forward, and much has changed in the world in the last five years.\n\nIt’s entirely possible that it will be *easier* to find support for MIRI’s new strategy than for our past alignment research program. AI extinction scenarios have received more scientific and popular attention over the last two years, and the basic case for MIRI’s new strategy is a lot simpler and less technical than was previously the case: “Superintelligence is likely to cause an extinction-grade catastrophe if we build it before we’re ready; we’re nowhere near ready, so we shouldn’t build it. Few scientists are candidly explaining this situation to policymakers or the public, and MIRI has a track record of getting these ideas out into the world.\"\n\nThat said, it’s also possible that it will be more difficult to fund our new efforts. If so, this would force us to drastically change our plans and scale back our ambitions.\n\nOne of our goals in sharing all of this now is to solicit donations and hear potential donors’ thoughts, which we can then use as an early indicator of how likely it is that we’ll be able to scale our teams and execute on our plans over the coming years. So long as we’re highly uncertain about the new funding environment, donations and words from our supporters can make a big difference for how we think about the next few years.\n\nIf you want to support our efforts, visit our [Donate page](https://intelligence.org/donate) or [reach out](https://intelligence.org/contact).\n\n* * *\n\n(*Posting on Alex Vermeer's behalf;* [*MIRI Blog mirror*](https://intelligence.org/2024/12/02/miris-2024-end-of-year-update/)*.*)",
      "plaintextDescription": "MIRI is a nonprofit research organization with a mission of addressing the most serious hazards posed by smarter-than-human artificial intelligence. In our general strategy update and communications strategy update earlier this year, we announced a new strategy that we’re executing on at MIRI, and several new teams we’re spinning up as a result. This post serves as a status update on where things stand at MIRI today.\n\nWe originally planned to run a formal fundraiser this year, our first one since 2019. However, we find ourselves with just over two years of reserves, which is more than we’ve typically had in our past fundraisers.\n\nInstead, we’ve written this brief update post, discussing recent developments at MIRI, projects that are in the works, and our basic funding status. In short: We aren’t in urgent need of more funds, but we do have a fair amount of uncertainty about what the funding landscape looks like now that MIRI is pursuing a very different strategy than we have in the past. Donations and expressions of donor interest may be especially useful over the next few months, to help us get a sense of how easy it will be to grow and execute on our new plans.\n\nYou can donate via our Donate page, or get in touch with us here. For more details, read on.\n\n \n\n\nAbout MIRI\nFor most of MIRI’s 24-year history, our focus has been on a set of technical challenges: “How could one build AI systems that are far smarter than humans, without causing an extinction-grade catastrophe?”\n\nOver the last few years, we’ve come to the conclusion that this research, both at MIRI and in the larger field, has gone far too slowly to prevent disaster. Although we continue to support some AI alignment research efforts, we now believe that absent an international government effort to suspend frontier AI research, an extinction-level catastrophe is extremely likely.\n\nAs a consequence, our new focus is on informing policymakers and the general public about the dire situation, and attempting to ",
      "wordCount": 1075
    },
    "tags": [
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "jZF2jwLnPKBv6m3Ag",
        "name": "Organization Updates",
        "slug": "organization-updates"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "Yig9oa4zGE97xM2os",
    "title": "Response to Aschenbrenner's \"Situational Awareness\"",
    "slug": "response-to-aschenbrenner-s-situational-awareness",
    "url": null,
    "baseScore": 197,
    "voteCount": 99,
    "viewCount": null,
    "commentCount": 27,
    "createdAt": null,
    "postedAt": "2024-06-06T22:57:11.737Z",
    "contents": {
      "markdown": "([*Cross-posted from Twitter.*](https://x.com/robbensinger/status/1798845199382429697))\n\nMy take on Leopold Aschenbrenner's new [report](https://situational-awareness.ai/): I think Leopold gets it right on a bunch of important counts.\n\nThree that I especially care about:\n\n1.  Full AGI and ASI [soon](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential#Timelines). (I think his [arguments](https://situational-awareness.ai/from-gpt-4-to-agi/) for this have a lot of [holes](https://twitter.com/ESYudkowsky/status/1798105252375503176), but he gets the basic point that superintelligence looks 5 or 15 years off rather than 50+.)\n2.  This technology is an overwhelmingly huge deal, and if we play our cards wrong we're all dead.\n3.  Current developers are indeed fundamentally unserious about the core risks, and need to make IP security and closure a top priority.\n\nI especially appreciate that the report seems to *get it* when it comes to our basic strategic situation: it gets that we may only be a few years away from a truly world-threatening technology, and it speaks very candidly about the implications of this, rather than soft-pedaling it to the degree that public writings on this topic almost always do. I think that's a valuable contribution all on its own.\n\nCrucially, however, I think Leopold gets the wrong answer on the question \"is alignment tractable?\". That is: OK, we're on track to build vastly smarter-than-human AI systems in the next decade or two. How realistic is it to think that we can control such systems?\n\nLeopold acknowledges that we currently only have guesswork and half-baked ideas on the technical side, that this field is extremely young, that many aspects of the problem look impossibly difficult (see attached image), and that there's a strong chance of this research operation getting us all killed. \"To be clear, given the stakes, I think 'muddling through' is in some sense a terrible plan. But it might be all we’ve got.\" *Controllable* superintelligent AI is a far more speculative idea at this point than superintelligent AI itself.\n\n![Image](https://pbs.twimg.com/media/GPbFZ60awAEH-9B?format=png&name=large)\n\nI think this report is drastically mischaracterizing the situation. ‘This is an awesome exciting technology, let's race to build it so we can reap the benefits and triumph over our enemies’ is an appealing narrative, but it requires the facts on the ground to shake out very differently than how the field's trajectory currently looks.\n\nThe more *normal* outcome, if the field continues as it has been, is: if anyone builds it, everyone dies.\n\nThis is not a national security issue of the form ‘exciting new tech that can give a country an economic or military advantage’; it's a national security issue of the form ‘we've found a way to build a doomsday device, and as soon as anyone starts building it the clock is ticking on how long before they make a fatal error and take themselves out, and take the rest of the world out with them’.\n\nSomeday superintelligence could indeed become more than a doomsday device, but that's the sort of thing that looks like a realistic prospect if ASI is 50 or 150 years away and we fundamentally know what we're doing on a technical level — not if it's more like 5 or 15 years away, as Leopold and I agree.\n\nThe field is not ready, and it's not going to suddenly become ready tomorrow. We need urgent and decisive action, but to indefinitely globally halt progress toward this technology that threatens our lives and our children's lives, not to accelerate ourselves straight off a cliff.\n\nConcretely, the kinds of steps we need to see ASAP from the USG are:\n\n\\- Spearhead an international alliance to prohibit the development of smarter-than-human AI until we’re in a radically different position. The three top-cited scientists in AI (Hinton, Bengio, and Sutskever) and the three leading labs (Anthropic, OpenAI, and DeepMind) have all publicly stated that this technology's trajectory poses a serious risk of causing human extinction (in the [CAIS statement](https://www.safe.ai/work/statement-on-ai-risk)). It is absurd on its face to let any private company or nation unilaterally impose such a risk on the world; rather than twiddling our thumbs, we should act.\n\n\\- Insofar as some key stakeholders aren’t convinced that we need to shut this down at the international level immediately, a sane first step would be to restrict frontier AI development to a limited number of compute clusters, and place those clusters under a uniform monitoring regime to forbid catastrophically dangerous uses. Offer symmetrical treatment to signatory countries, and do not permit exceptions for any governments. The idea here isn’t to centralize AGI development at the national or international level, but rather to make it possible at all to shut down development at the international level once enough stakeholders recognize that moving forward would result in self-destruction. In advance of a decision to shut down, it may be that anyone is able to rent H100s from one of the few central clusters, and then freely set up a local instance of a free model and fine-tune it; but we retain the ability to change course, rather than just resigning ourselves to death in any scenario where ASI alignment isn’t feasible.\n\nRapid action is called for, but it needs to be based on the realities of our situation, rather than trying to force AGI into the old playbook of far less dangerous technologies. The fact that we can build something doesn't mean that we ought to, nor does it mean that the international order is helpless to intervene.",
      "plaintextDescription": "(Cross-posted from Twitter.)\n\n \n\nMy take on Leopold Aschenbrenner's new report: I think Leopold gets it right on a bunch of important counts.\n\nThree that I especially care about:\n\n 1. Full AGI and ASI soon. (I think his arguments for this have a lot of holes, but he gets the basic point that superintelligence looks 5 or 15 years off rather than 50+.)\n 2. This technology is an overwhelmingly huge deal, and if we play our cards wrong we're all dead.\n 3. Current developers are indeed fundamentally unserious about the core risks, and need to make IP security and closure a top priority.\n\nI especially appreciate that the report seems to get it when it comes to our basic strategic situation: it gets that we may only be a few years away from a truly world-threatening technology, and it speaks very candidly about the implications of this, rather than soft-pedaling it to the degree that public writings on this topic almost always do. I think that's a valuable contribution all on its own.\n\nCrucially, however, I think Leopold gets the wrong answer on the question \"is alignment tractable?\". That is: OK, we're on track to build vastly smarter-than-human AI systems in the next decade or two. How realistic is it to think that we can control such systems?\n\nLeopold acknowledges that we currently only have guesswork and half-baked ideas on the technical side, that this field is extremely young, that many aspects of the problem look impossibly difficult (see attached image), and that there's a strong chance of this research operation getting us all killed. \"To be clear, given the stakes, I think 'muddling through' is in some sense a terrible plan. But it might be all we’ve got.\" Controllable superintelligent AI is a far more speculative idea at this point than superintelligent AI itself.\n\nI think this report is drastically mischaracterizing the situation. ‘This is an awesome exciting technology, let's race to build it so we can reap the benefits and triumph over our enemies’ is an appe",
      "wordCount": 884
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "zPM5r3RjossttDrpw",
    "title": "When is a mind me?",
    "slug": "when-is-a-mind-me",
    "url": null,
    "baseScore": 149,
    "voteCount": 123,
    "viewCount": null,
    "commentCount": 132,
    "createdAt": null,
    "postedAt": "2024-04-17T05:56:38.482Z",
    "contents": {
      "markdown": "xlr8harder writes:\n\n> In general I don’t think an uploaded mind is you, but rather a copy. But one thought experiment makes me question this. A Ship of Theseus concept where individual neurons are replaced one at a time with a nanotechnological functional equivalent.\n> \n> Are you still you?\n\nPresumably the question xlr8harder cares about here isn't semantic question of how linguistic communities use the word \"you\", or predictions about how [whole-brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation) tech might change the way we use pronouns.\n\nRather, I assume xlr8harder cares about more substantive questions like:\n\n1.  If I expect to be uploaded tomorrow, should I care about the upload in the same ways (and to the same degree) that I care about my future biological self?\n2.  Should I anticipate *experiencing* what my upload experiences?\n3.  If the scanning and uploading process requires destroying my biological brain, should I say yes to the procedure?\n\nMy answers:\n\n1.  Yeah.\n2.  Yep.\n3.  Yep, this is no big deal. A productive day for me might involve doing some work in the morning, getting a sandwich at Subway, destructively uploading my brain, then texting some friends to see if they'd like to catch a movie after I finish answering e-mails. ¯\\\\_(ツ)_/¯\n\nIf there's an open question here about whether a high-fidelity emulation of me is \"really me\", this seems like it has to be a purely [verbal](https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb) question, and not something that I would care about at reflective equilibrium.\n\nOr, to the extent that *isn't* true, I think that's a red flag that there's a cognitive illusion or confusion still at work. There isn't a special extra \"me\" thing separate from my brain-state, and my precise causal history isn't *that* important to my values.\n\nI'd guess that this illusion comes from not fully internalizing [reductionism](https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x) and [naturalism](https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo) about the mind.\n\nI find it pretty natural to think of my \"self\" as though it were a homunculus that lives in my brain, and \"watches\" my experiences in a Cartesian theater.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/93c9678a595398698c1347b4654c602db8917c5ecc3dc173.png)\n\nOn this intuitive model, it makes sense to ask, separate from the experiences and the rest of the brain, where the homunculus is. (“OK, there’s an exact copy of my brain-state there, but where am *I?*”)\n\nE.g., consider a teleporter that works by destroying your body, and creating an exact atomic copy of it elsewhere.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2355cddf7d0763d41c868e3e449195b7c052e98a5baccfaa.png)\n\nPeople often worry about whether they'll \"really experience\" the stuff their brain undergoes post-teleport, or whether a copy will experience it instead. \"Should I anticipate 'waking up' on the other side of the teleporter? Or should I anticipate Oblivion, and it will be Someone Else who has those future experiences?\"\n\nThis question doesn't really make sense from a naturalistic perspective, because there isn't any causal mechanism that could *be responsible for the difference* between \"a version of me that exists at 3pm tomorrow, whose experiences I should anticipate experiencing\" and \"an exact physical copy of me that exists at 3pm tomorrow, whose experiences I *shouldn't* anticipate experiencing\".\n\nImagine that the teleporter is located on Earth, and it sends you to a room on a space station that looks and feels identical to the room you started in. This means that until you exit the room and discover whether you're still on Earth, there's no way for you to tell whether the teleporter worked.\n\nBut more than that, *there will be nothing about your brain that tracks* whether or not the teleporter sent you somewhere (versus doing nothing).\n\nThere isn't an XML tag in the brain saying \"this is a new brain, not the original\"!\n\nThere isn't a Soul or Homunculus that exists *in addition* to the brain, that could be the causal mechanism distinguishing \"a brain that is me\" from \"a brain that is not me\". There's just the brain-state, with no remainder.\n\nAll of the same functional brain-states occur whether you enter the teleporter or not, at least until you exit the room. At every moment where the brain exists, the *current* state of the brain isn't affected by whether teleportation occurred.\n\nSo there isn't, within physics, any *way* for \"the real you to be having an experience\" in the case where the teleporter malfunctioned, and \"someone else to be having the experience\" in the case where the teleporter worked. (Unless this is a *purely verbal* distinction, unrelated to the three important-feeling questions we started with.)\n\nPhysics is local, and doesn't *remember* whether the teleportation occurred in the past.\n\nNor is there a law of physics saying \"your subjective point of view immediately blips out of existence and is replaced by Someone Else's point of view if your spacetime coordinates change a *lot* in a *short* period of time (even though they don't blip out of existence when your spacetime coordinates change a *little* or change over a *longer* period of time)\".\n\nIf that sort of difference can *really and substantively change* whether your experiences persist over time, it would have to be through some divine mechanism outside of physics.[^md0dc1h6e3b]\n\nWhy Humans Feel Like They Persist\n---------------------------------\n\n![](https://cdn.discordapp.com/attachments/995477915657588736/1230033400166088796/bliptrip_a_simple_beautiful_minimalist_watercolor_cartoon_icon__45a52375-0472-4f29-b9bb-ed3af73a3726.png?ex=6631d8df&is=661f63df&hm=142ceb15c2a503f4123239dc59b9bde34efa22115c5d2c9a7229c92e90ec2514&)\n\nTaking a step back, we can ask: what physical mechanism makes it *feel as though* I'm persisting over time? In *normal* cases, why do I feel so confident that I'm going to experience my future self's experiences, as opposed to being replaced by a doppelganger who will experience everything in my place?\n\nLet's call \"Rob at time 1\" R1, \"Rob at time 2\" R2, and \"Rob at time 3\" R3.\n\nR1 is hungry, and has the thought \"I'll go to the fridge to get a sandwich\". R2 walks to the fridge and opens the door. R3 takes a bite of the sandwich.\n\n**Question 1:** Why is R2 bothering to open the fridge, even though it's R3 that will get to eat the sandwich? For that matter, why is R1 bothering to strategize about finding food, when it's not R1 who will realize the benefits?\n\n**Answer:** Well, there's no need in principle for my time-slices to work together like that. Indeed, there are other cases where my time-slices work at cross purposes (like when I try to follow a diet but one of my time-slices says \"no\"). But it was reproductively advantageous for my ancestors' brains to generate and execute plans (including very fast, unconscious five-second plans), so they evolved to do so, rather than just executing a string of reflex actions.\n\n**Question 2:** OK, but you could still achieve all that by having R1 think of R1, R2, and R3 as three different people. Rather than R1 thinking \"I selfishly want a sandwich, so I'll go ahead and do multiple actions in sequence so that I get a sandwich\", why doesn't R1 think \"I altruistically want my friend R3 to have a sandwich, so I'll collaborate with R2 to do a favor for R3\"?\n\n**Answer:** Either of those ways of thinking would probably work fine in principle. Indeed, there's some individual and cultural variation in how much individual humans think of themselves as transtemporal \"teams\" versus persisting objects.\n\nBut it does seem like humans have a pretty strong inclination to think of themselves as psychologically persisting over time. I don't know why that is, but plausibly it has a lot to do with the general way humans think of objects: we say that a table is \"the same table\" even if it has changed a lot through years of usage. We even say that a caterpillar is \"the same organism\" as the butterfly it produces. We don't *usually* think of objects as a rapid succession of momentary blips, so it doesn't seem surprising that we think of our minds/brains as stable objects too, and use labels like \"me\" and \"selfish\" rather than \"us\" and \"self-altruistic\".\n\n**Question 3:** OK, but it's not just that I'm using the arbitrary label \"me\" to refer to R1, R2, and R3. R1 *anticipates experiencing* the sandwich himself, and would anticipate this regardless of how he used language. Why's that?\n\n**Answer:** Because R1 is being replaced by R2, an extremely similar brain that will likely remember the things R1 just thought. You're in a sense *constantly* passing the baton to a new person, as your brain changes over time. **The feeling of being replaced by a new brain state that has around that much in common with your current brain state** ***just is*** **the experience that you're calling \"persisting over time\".**\n\nThat experience of \"persisting over time\" isn't the experience of a magical Cartesian ghost that is observing a series of brain-states and acting as a single Subject for all of them. Rather, the experience of \"persisting over time\" *just is* the experience of each brain-states possessing certain kinds of information (\"memories\") about the previous brain-state in a sequence. (Along with R1, R2, and R3 having tons of overlapping personality traits, goals, etc.)\n\nSome humans are more temporally unstable than others, and if a drug or psychotic episode interfered with your short-term memory enough, or caused your personality or values to change enough minute-to-minute, you might indeed feel as though \"I'm the same person over time\" has become less true.\n\n(On the other hand, if you'd been *born* with that level of instability, it's less likely that you'd think there was anything weird about it. Humans can get used to a lot!)\n\nThere isn't a sharp black line in physics that determines how much a brain must resemble your own in order for you to \"persist over time\" into becoming that brain. There's just one brain-state that exists at one spacetime coordinate, and then another brain-state that exists at another spacetime coordinate.\n\nIf a brain-state A has quasi-sensory access to the experience of another brain-state B — if A feels like it \"remembers\" being in state B a fraction of a second ago — then A will typically feel as though it used to be B. If A *doesn't* have the same personality or values as B, then A will perhaps feel like they used to be B, but have suddenly changed into a very different sort of person.\n\nChange *enough*, while still giving A immediate quasi-sensory access to B's state, and perhaps the connection will start to feel more dissociative or dreamlike; but there's no sharp line in physics to tell us how much change makes someone \"no longer the same person\".\n\nSleep and Film Reels\n--------------------\n\nI find it easier to make sense of the teleporter scenario when I consider hypotheticals like \"neuroscience discovers that you die and are reborn every night while you sleep\", or \"physics discovers that the entire universe is destroyed and an exact copy is recreated millions of times every second\".\n\nIf we discovered one of those facts, would it make sense to freak out or go into mourning?\n\nIn that scenario, should we really start fretting about whether \"I'm\" going to \"really experience\" the thing that happens to my body five seconds from now, versus Someone Else experiencing it?\n\nI think this would be pretty danged silly. You're *right now* experiencing what it's like to \"toss the baton\" from a past version of you to a future version of you, with zero consternation or anxiety, even though *right now* it's an open possibility that you're not \"continuous\".\n\nMaybe the real, deep metaphysical Truth is that the universe is more like a film reel made up of many discrete frames (that feel continuous to us, because we're experiencing the frames from the inside, not looking at the reel from Outside The Universe), not something actually continuous.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8e950e928db49b9b1086fe6cb0f772d1a36b64c3daebdf16.png)\n\nI earnestly believe that the proper response to that hypothetical is: Who cares? For all I know, something like that could be true. But if it's true now, it was always true; I've been living that way my whole life. If *the experiences I'm having as I write this sentence* are the super scary Teleporter Death thing people keep saying I should worry about, then I already *know* what that's like, and it's chill.\n\nIf you aren't already bored by the whole topic (as you probably should be), you can play semantics and claim that I should instead say \"the experiences *we've* been having as *we* write this sentence\". Because this weird obscure discovery about metaphysics is somehow supposed to mean that in the world where we made this discovery, the Real Me is secretly constantly dying and being replaced...?\n\nBut whatever. If you're *just* redescribing the stuff I'm already experiencing and telling me that *that's* the scary thing, then I think you're too easily spooked by abstract redescriptions of ordinary life. Or if you're redescribing it but *not* trying to tell me I should freak out about your redescription, then it's just semantics, and I'll use pronouns in whichever way is most convenient.\n\nAnother way of thinking about this is: I am my brain, not a ghost or thing outside my brain. So if something makes *no physical difference* to my current brain-state, *and* makes no difference to any of my past or future brain-states, then I think it's just crazy talk to think that this metaphysical bonus thingie-outside-my-brain is the crucial thing that determines whether I exist, or whether I'm alive or dead, etc.\n\nThinking that my existence depends on some metaphysical \"glue\" outside of my brain, is like thinking that my existence depends on whether a magenta marble is currently orbiting Neptune. Why would the existence of some random Stuff out there in the cosmos that's *not* a Rob-time-slice brain-state, change how I should care about a Rob-time-slice brain-state, or change which brain-state (if any) I should anticipate?\n\nReal life is more boring than the games we can play, striving to find a redescription of the mundane that makes the mundane sound spooky. Like children staring at campfire shadows and trying to will the shadows into looking like monsters.  \n  \nReal life looks like going to bed at night and thinking about whether I want toast tomorrow morning, even though I don't know how sleep works and it's totally possible that sleep might involve shutting down my stream of consciousness at some point and then starting it up again.\n\nRegardless of how a mature neuroscience of sleep ends up looking, I expect the me tomorrow to share *a truly crazily extraordinarily massive number* of memories, personality traits, goals, etc. in common with me.\n\nI expect them to remember a ton of the things I do today, such that micro-decisions (like how I write this sentence) can influence a bunch of things about their state and their own future trajectory.\n\nI can try to distract myself from those things with neurotic philosophy-101 ghost stories, but looking away from reality doesn't make it go away.\n\nWeird-Futuristic-Technology Anxiety\n-----------------------------------\n\nSince there isn't a Soul that lives Outside The Film Reel and is being torn asunder from my brain-state by the succession of frames — there's *just* a bunch of brain-states — the anxiety about whether \"I\" should \"really\" anticipate any future experiences in Film Reel World is based in illusion.\n\nBut the only difference between this scenario and the teleporter one is that the teleporter scenario invokes a weird-sounding New Technology, whereas the sleep and Film Reel examples bake in \"there's nothing new and weird happening, you've already been living your whole life this way\". If you'd grown up using using teleporters all the time, then it would seem just as unremarkable as stepping through a doorway.\n\nIf a philosopher then came to you one day and said \"but WHAT IF something KILLS YOU every time you step through a door and then a NEW YOU comes into existence on the other side!\", you would just roll your eyes. If it makes no perceptible difference, then wtf are we even talking about?\n\nAnd the same logic applies to mind uploading. There isn't some magical Extra Thing beyond the brain state, that could make it the case that one thing is You and another thing is Not You.\n\nSure, you're now made of silicon atoms rather than carbon atoms. But this is like discovering that Film Reel World alternates between one kind of metaphysical Stuff and another kind of Stuff every other second.\n\nIf you aren't worried about learning that the universe secretly metaphysically is in a state of Constant Oscillation between two types of (functionally indistinguishable) micro-particles, then why care about functionally irrelevant substrate changes at all?\n\n(It's another matter entirely if you think carbon vs. silicon actually *does* make an inescapable functional, causal difference for which high-level thoughts and experiences your mind instantiates, and if you think that there's no way in principle to use a computer to emulate the causal behavior of a human mind. I think that's crazy talk, but it's crazy because of ordinary facts about physics / neuroscience / psych / CS, not because of any weird philosophical considerations.)\n\nTo Change Experience, You Have to Change Physics, Not Just Metaphysics\n----------------------------------------------------------------------\n\nScenario 1:\n\n<table><tbody><tr><td><p>I step through a doorway.</p><p>At time 1, a brain is about to enter a doorway.</p><p>At time 2, an extremely similar brain is passing through the doorway.</p><p>At time 3, another extremely similar brain has finished passing through the doorway.</p></td></tr></tbody></table>\n\nScenario 2:\n\n<table><tbody><tr><td><p>I step into a teleporter.</p><p>Here, again, there exist a series of extremely similar brain states before, during, and after I use the teleporter.</p></td></tr></tbody></table>\n\nThe particular *brain states* look no different in the teleporter case than if I'd stepped through a door; so if there's something that makes the post-teleporter Rob \"not me\" while also making the post-doorway Rob \"me\", then it must lie outside the brain states, a Cartesian Ghost.\n\nGiven all that, there's something genuinely weird about the fact that teleporters spook people more than walking through a door does.\n\nIt's like looking at a film strip, and being scared that if a blank slide were added in between every frame, this would somehow make a difference for the people living inside the movie. It's getting confused about the distinction between the physics of the movie's events and the meta-physics of \"what the world runs on\".\n\nThe same confusion can arise if we imagine flipping the order of all the frames in the film strip; or flipping the order of all the frames in the second half of the movie; or swapping the order of every pair of frames, like so:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/516abe658b84eb7f7e71555a3dd6a113058fb182c2b2069b.png)\n\nFrom *outside* the movie, this can make the movie's events look more confusing or chaotic to us, the viewers. But if you imagine that the *characters inside the movie* would be the least bit bothered or confused by this rearrangement, you're making a clear mistake. **To confuse the characters, you need to** ***change what happens inside the frames*****, not just change the relationship between those frames.**\n\nI claim that a very similar cognitive hiccup is occurring when someone worries about their internal stream of consciousness halting due to a teleporter (and *not* halting due to stepping through a random doorway).\n\nYou're imagining that something about the *context* of the film cells — i.e., the stuff *outside* of the brain states themselves — is able to change your experiences.\n\nBut experiences *just are* brain things. To imagine that some of the unconscious goings-on in between two of your experiences can interfere with your Self **is just the same kind of error** as imagining that a movie character will be bothered, or will even subjectively notice, if you inject some empty frames into the movie while changing nothing else about the movie.\n\n... And You Can't Change Experience With Just Any Old Change to Physics\n-----------------------------------------------------------------------\n\nClaim:\n\n<table><tbody><tr><td><p>As soon as a purple hat comes into existence on Pluto, my stream of consciousness will end and I will be imperceptibly replaced by an exact copy of myself that is experiencing a different stream of consciousness.</p><p>This exact copy of me will be physically identical to me in every respect, and will have all of my memories, personality traits, etc. But they won't be <i>me</i>. The hat, if such a hat ever comes into being, will kill me.</p></td></tr></tbody></table>\n\nWhat, specifically, is wrong with this claim?\n\nWell, one thing that's wrong with the claim is that Pluto is very far away from the Earth.\n\nBut the idea of a hat ending my existence seems very strange even if the hat is in closer proximity to me. Even putting a hat on my head seems like it shouldn't be enough to end my stream of consciousness, *unless* there's something special about the hat that will actually drastically change my brain-state. (E.g., maybe the hat is wired up with explosives.)\n\nThe point of this example being:\n\nYou can call the Ghost a \"Soul\", and make it obvious that we're invoking magic.\n\nOr you can call it a \"special kind of causal relationship (that's able to preserve selfhood)\", and make it sound superficially scientific. (Or at least science-compatible.)\n\nYou can hypothesize that there's something special about the *causal process* that produces new brain-states in the \"walk through a doorway\" case — something \"in the causality itself\" that makes the post-doorway self *me* and the post-teleporter self *not me*.\n\nBut of course, this \"causal relationship\" is *not a part of the brain state*. Reify causality all you want; the issue remains that you're positing something *outside the brain*, outside you and your experiences, that is able to change which experiences you should anticipate *without changing any of the experiences or brain-states themselves*.\n\nThe brain states exist *too*, whatever causal relationships they exhibit. To say that *exactly the same brain states can exist*, and yet something outside of those states is changing a *perceptible feature of those experiences* (\"which experience comes next in this subjective flow that's being experienced; what I should expect to see next\"), *without changing any of the actual brain states*, is just as silly whether that something is a \"causal relationship\" or a purple hat.\n\nThis principle is easier to motivate in the case of the hat, because hats are a lot more concrete, familiar, and easy to think about than some fancy philosophical abstraction like \"causal relationship\". But the principle generalizes; random objects and processes out there, whether fancy-sounding or perfectly mundane, can't perceptibly change my experience (*unless* they change which brain states occur).\n\nLikewise, it's easier to see that something on Pluto can't suddenly end my stream of consciousness, than to see that something physically (or metaphysically?) \"nearby\" can't suddenly end my stream of consciousness (without leaving a mess). But the principle generalizes; being *nearby* or *connected to* something doesn't open the door to arbitrary magical changes, absent some mechanism for how *that exact change* is caused by *that exact physical process*.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5a13f42a3096e03bdbfff863e8963ac012eff89e05dfc6fa.png)\n\nIf we were *just talking about word definitions and nothing else*, then sure, define \"self\" however you want. You have the universe's permission to define yourself into dying as often or as rarely as you'd like, if word definitions alone are what concerns you.\n\nBut this post hasn't been talking about word definitions. It's been talking about substantive predictive questions like \"What's the very next thing I'm going to see? The other side of the teleporter? Or nothing at all?\"\n\nThere should be an actual answer to this, at least to the same degree there's an answer to \"When I step through this doorway, will I have another experience? And if so, what will that experience be?\"\n\nAnd once we have an answer, this should change how excited we are about things like mind uploading. If my stream of consciousness is going to end with my biological death no matter what I do, then mind uploading sounds a lot less exciting!\n\nOr, equivalently: If my experiences were a matter of \"displaying images for a Cartesian Homunculus\", and the death of certain cells in the brain severs the connection between my brain and the Homunculus, then there's no obvious reason I should expect this exact same Homunculus to establish a connection to an uploaded copy of my brain.\n\nIt's only if I'm *in* my brain, just an ordinary [part of physics](https://www.lesswrong.com/posts/NEeW7eSXThPz7o4Ne/thou-art-physics), that mind uploading makes sense as a way to extend my lifespan.\n\nCausal relationships and processes obviously matter for what experiences occur. But they matter *because they change the brain-states themselves*. They don't cause *additional* changes to experience *beyond* the changes exhibited in the brain.\n\nHaving More Than One Future\n---------------------------\n\nI've tried to keep this post pretty simple and focused. E.g., I haven't gone into questions like \"What happens if you make *two* uploads of me? Which one should I anticipate having the experiences of?\"\n\nBut I hope the arguments I've laid out above make it clear what the right answer has to be: **You should anticipate having both experiences.**\n\nIf you've already bitten the bullet on things like the teleporter example, then I don't think this should actually be particularly counter-intuitive. If one copy of my brain exists at time 1 (Rob-x), and two almost-identical copies of my brain (Rob-y and Rob-z) exist at time 2, then there's going to be a version of me that's Rob-y, and a version of me that's Rob-z, and each will have equal claim to being \"the next thing I experience\".\n\nIn a world without magical Cartesian Homunculi, this *has* to be how things work; there isn't any physical difference between Rob-y and Rob-z that makes one of them my True Heir and the other a False Pretender. They're both just future versions of me.\n\n\"You should anticipate having both experiences\" *sounds* sort of paradoxical or magical, but I think this stems from a verbal confusion. \"Anticipate having both experiences\" is ambiguous between two scenarios:\n\n*   Scenario 1: \"Split-screen mode.\" My stream of consciousness continues, but it somehow magically splits into a portion that's Rob-y and a different portion that's Rob-z, as though the Cartesian Homunculus were trying to keep an eye on both brains at once.\n*   Scenario 2: \"Two separate screens.\" My stream of consciousness continues from Rob-x to Rob-y, and it also continues from Rob-x to Rob-z. Or, equivalently: Rob-y feels exactly as though he was just Rob-x, and Rob-z also feels exactly as though he was just Rob-x (since each of these slightly different people has all the memories, personality traits, etc. of Rob-x — just as though they'd stepped through a doorway).\n\nScenario 1 *is* crazy talk, and it's *not* the scenario I'm talking about. When I say \"You should anticipate having both experiences\", I mean it in the sense of Scenario 2.\n\nScenario 2 is pretty unfamiliar to us, because we don't currently live in a world where we can readily copy-paste our own brains. And accordingly, it's a bit awkward to talk about Scenario 2; the English language is adapted to a world where \"humans don't fork\" has always been a safe assumption.\n\nBut there isn't a mystery about *what happens*. If you think there's something mysterious or unknown about what happens when you make two copies of yourself, then I pose the question to you:\n\nWhat concrete fact about the physical world do you think you're missing? What are you ignorant of?\n\nAlternatively, if you're not ignorant of anything, then: how can there be a mystery here? (Versus just \"a weird way the world can sometimes end up\".)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ebcd445763ac327c985adba2bdef4cfc39ee213d37981a0.png)\n\n[^md0dc1h6e3b]: And insofar as it's your physical brain thinking these thoughts right now, unaltered by any divine revelation, it would have to be a coincidence that this \"I would blip out of existence in case A but not case B\" hunch is correct. Because the reason your brain has that intuition is a product of the brain's physical, causal history, and is not the result of you making any observation that's Bayesian evidence for this mechanism existing.Your brain is not causally entangled with any mechanism like that; you'd be thinking the same thoughts whether the mechanism existed or not. So while it's possible that you're having this hunch for reasons unrelated to the hunch being correct, and yet the hunch be correct anyway, you shouldn't on reflection believe your own hunch. Any Bayesian evidence for this hypothesis would need to come from some source other than the hunch/intuition.",
      "plaintextDescription": "xlr8harder writes:\n\n> In general I don’t think an uploaded mind is you, but rather a copy. But one thought experiment makes me question this. A Ship of Theseus concept where individual neurons are replaced one at a time with a nanotechnological functional equivalent.\n> \n> Are you still you?\n\nPresumably the question xlr8harder cares about here isn't semantic question of how linguistic communities use the word \"you\", or predictions about how whole-brain emulation tech might change the way we use pronouns.\n\nRather, I assume xlr8harder cares about more substantive questions like:\n\n 1. If I expect to be uploaded tomorrow, should I care about the upload in the same ways (and to the same degree) that I care about my future biological self?\n 2. Should I anticipate experiencing what my upload experiences?\n 3. If the scanning and uploading process requires destroying my biological brain, should I say yes to the procedure?\n\nMy answers:\n\n 1. Yeah.\n 2. Yep.\n 3. Yep, this is no big deal. A productive day for me might involve doing some work in the morning, getting a sandwich at Subway, destructively uploading my brain, then texting some friends to see if they'd like to catch a movie after I finish answering e-mails. ¯\\_(ツ)_/¯\n\nIf there's an open question here about whether a high-fidelity emulation of me is \"really me\", this seems like it has to be a purely verbal question, and not something that I would care about at reflective equilibrium.\n\nOr, to the extent that isn't true, I think that's a red flag that there's a cognitive illusion or confusion still at work. There isn't a special extra \"me\" thing separate from my brain-state, and my precise causal history isn't that important to my values.\n\nI'd guess that this illusion comes from not fully internalizing reductionism and naturalism about the mind.\n\nI find it pretty natural to think of my \"self\" as though it were a homunculus that lives in my brain, and \"watches\" my experiences in a Cartesian theater.\n\nOn this intuitive model,",
      "wordCount": 4529
    },
    "tags": [
      {
        "_id": "5f5c37ee1b5cdee568cfb2fa",
        "name": "Personal Identity",
        "slug": "personal-identity"
      },
      {
        "_id": "PbShukhzpLsWpGXkM",
        "name": "Anthropics",
        "slug": "anthropics"
      },
      {
        "_id": "SJFsFfFhE6m2ThAYJ",
        "name": "Anticipated Experiences",
        "slug": "anticipated-experiences"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ZDQvYKuyH5aTrZKoY",
    "title": "AI Views Snapshots",
    "slug": "ai-views-snapshots",
    "url": null,
    "baseScore": 143,
    "voteCount": 57,
    "viewCount": null,
    "commentCount": 61,
    "createdAt": null,
    "postedAt": "2023-12-13T00:45:50.016Z",
    "contents": {
      "markdown": "(*Cross-posted* [*from Twitter*](https://twitter.com/robbensinger/status/1734699436176298131)*, and therefore optimized somewhat for simplicity.*)\n\nRecent discussions of AI x-risk in places like Twitter tend to focus on \"are you in the Rightthink Tribe, or the Wrongthink Tribe?\". Are you a doomer? An accelerationist? An EA? A techno-optimist?\n\nI'm pretty sure these discussions would go *way* better if the discussion looked less like that. More concrete claims, details, and probabilities; fewer vague slogans and vague expressions of certainty.\n\nAs a start, I made this image (also available [as a Google Drawing](https://docs.google.com/drawings/d/1Vd_3P_EGUb08Ag1z0NvymaFjF2_XiOlF3TPU76WB8bg/edit)):\n\n![Image](https://pbs.twimg.com/media/GBL_QlraoAA-hlv?format=jpg&name=large)\n\n(**Added:** [Web version](https://ai-views-snapshots.tetratopia.foundation/) made by Tetraspace.)\n\nI obviously left out lots of other important and interesting questions, but I think this is OK as a conversation-starter. I've encouraged Twitter regulars to share their own versions of this image, or similar images, as a nucleus for conversation (and a way to directly clarify what people's actual views are, beyond the stereotypes and slogans).\n\nIf you want to see a filled-out example, here's mine (though you may not want to look if you prefer to give answers that are less anchored): [Google Drawing link](https://docs.google.com/drawings/d/16K6V1BQ4UPI4xKpoEag50ofEEdoSkL0wMu3kPzjtOX4/edit).",
      "plaintextDescription": "(Cross-posted from Twitter, and therefore optimized somewhat for simplicity.)\n\n \n\nRecent discussions of AI x-risk in places like Twitter tend to focus on \"are you in the Rightthink Tribe, or the Wrongthink Tribe?\". Are you a doomer? An accelerationist? An EA? A techno-optimist?\n\nI'm pretty sure these discussions would go way better if the discussion looked less like that. More concrete claims, details, and probabilities; fewer vague slogans and vague expressions of certainty.\n\nAs a start, I made this image (also available as a Google Drawing):\n\n \n\n \n\n(Added: Web version made by Tetraspace.)\n\n \n\nI obviously left out lots of other important and interesting questions, but I think this is OK as a conversation-starter. I've encouraged Twitter regulars to share their own versions of this image, or similar images, as a nucleus for conversation (and a way to directly clarify what people's actual views are, beyond the stereotypes and slogans).\n\nIf you want to see a filled-out example, here's mine (though you may not want to look if you prefer to give answers that are less anchored): Google Drawing link.",
      "wordCount": 178
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "QzkTfj4HGpLEdNjXX",
    "title": "An artificially structured argument for expecting AGI ruin",
    "slug": "an-artificially-structured-argument-for-expecting-agi-ruin",
    "url": null,
    "baseScore": 91,
    "voteCount": 44,
    "viewCount": null,
    "commentCount": 26,
    "createdAt": null,
    "postedAt": "2023-05-07T21:52:54.421Z",
    "contents": {
      "markdown": "Philosopher David Chalmers [asked](https://twitter.com/robbensinger/status/1647340328998039552):\n\n> \\[I\\]s there a canonical source for \"the argument for AGI ruin\" somewhere, preferably laid out as an explicit argument with premises and a conclusion?\n\nUnsurprisingly, the actual reason people expect AGI ruin isn't a crisp deductive argument; it's a probabilistic update based on many lines of evidence. The specific observations and heuristics that carried the most weight for someone will vary for each individual, and can be hard to [accurately draw out](https://www.lesswrong.com/posts/AJ9dX59QXokZb35fk/when-not-to-use-probabilities).\n\nThat said, Eliezer Yudkowsky's [So Far: Unfriendly AI Edition](https://econlib.org/archives/2016/03/so_far_unfriend.html) might be a good place to start if we want a pseudo-deductive argument just for the sake of organizing discussion.  People can then say which premises they want to drill down on.[^erl9bzappwh]\n\nIn [The Basic Reasons I Expect AGI Ruin](https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin), I wrote:\n\n> When I say \"general intelligence\", I'm usually thinking about \"whatever it is that lets human brains do astrophysics, category theory, etc. even though our brains evolved under literally zero selection pressure to solve astrophysics or category theory problems\".\n> \n> It's possible that we should already be thinking of GPT-4 as \"AGI\" on some definitions, so to be clear about the threshold of generality I have in mind, I'll specifically talk about \"**STEM-level AGI**\",[^otkehmiaqgl] though I expect such systems to be good at non-STEM tasks too.\n\nSTEM-level AGI is AGI that has \"the basic mental machinery required to do par-human reasoning about *all* the hard sciences\",[^iqw10v88yvh] though a specific STEM-level AGI could (e.g.) lack physics ability for the same reasons many smart humans can't solve physics problems, such as \"lack of familiarity with the field\".\n\nA simple way of stating the argument in terms of STEM-level AGI is:\n\n1.  **Substantial Difficulty of** [**Averting Instrumental Pressures**](https://arbital.com/p/avert_instrumental_pressure/)**:**[^2mdfiwtqwtw]As a strong default, absent alignment breakthroughs, STEM-level AGIs that understand their situation and don't value human survival [as an end](https://arbital.com/p/terminal_vs_instrumental/) will want to kill all humans if they can.\n2.  **Substantial Difficulty of** [**Value Loading**](https://www.edge.org/response-detail/26198)**:** As a strong default, absent alignment breakthroughs, STEM-level AGI systems won't value human survival as an end.\n3.  **High Early Capabilities.** As a strong default, absent alignment breakthroughs or global coordination breakthroughs, early STEM-level AGIs will be scaled to capability levels that allow them to understand their situation, and allow them to kill all humans if they want.\n4.  **Conditional Ruin.** If it's very likely that there will be no alignment breakthroughs or global coordination breakthroughs before we invent STEM-level AGI, then given 1+2+3, it's very likely that early STEM-level AGI will kill all humans.\n5.  **Inadequacy.** It's very likely that there will be no alignment breakthroughs or global coordination breakthroughs before we invent STEM-level AGI.\n6.  Therefore it's very likely that early STEM-level AGI will kill all humans. (From 1–5)\n\nI'll say that the \"invention of STEM-level AGI\" is the first moment when an AI developer (correctly) recognizes that it can build a working STEM-level AGI system within a year. I usually operationalize \"**early STEM-level AGI**\" as \"STEM-level AGI that is built within five years of the invention of STEM-level AGI\".[^320szmcv6y9]\n\nI think humanity is very likely to destroy itself within five years of the invention of STEM-level AGI. And plausibly far sooner — e.g., within three months or a year of the technology's invention. A lot of the technical and political difficulty of the situation stems from this high level of time pressure: if we had decades to work with STEM-level AGI before catastrophe, rather than months or years, we would have far more time to act, learn, try and fail at various approaches, build political will, craft and implement policy, etc.[^r2jw6u00ss]\n\nThis argument focuses on \"human survival\", but from my perspective the more important claim is that STEM-level AGI systems very likely won't value awesome [cosmopolitan outcomes](https://arbital.com/p/value_cosmopolitan/) at all. It's not just that we'll die; it's that there probably won't be anything else of significant value that the AGI creates in our place.[^1n8504xok95]\n\nElaborating on the five premises:\n\n**1\\. Substantial Difficulty of Averting Instrumental Pressures**\n\n[In *Superintelligence*](https://nickbostrom.com/superintelligentwill.pdf), Nick Bostrom defines an \"Instrumental Convergence Thesis\":\n\n> \\[A\\]s long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so.\n> \n> \\[...\\]\n> \n> Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent’s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by many intelligent agents.\n\nBostrom distinguishes between \"instrumental goals\" and \"final goals\" (\"terminal goals\" in [Yudkowsky's writing](https://arbital.com/p/terminal_vs_instrumental/)). I call the former \"instrumental strategies\" instead, to make it clearer that instrumental \"goals\" are just strategies for achieving ends.\n\nFor the argument to carry, it isn't sufficient to argue that STEM-level AGI systems exhibit instrumental convergence at all; they need to exhibit *catastrophic* instrumental convergence, i.e., a wide variety of ends need to imply strategies that kill all humans (given the opportunity).\n\nOne way of arguing for 1 is via these three subclaims:\n\n<table><tbody><tr><td>1a. <strong>STEM-Level AGIs Exhibit Goal-Oriented Behavior by Default.</strong> As a strong default, STEM-level AGIs will have \"goals\"—or will at least look from the outside like they do. By this I mean that they'll select outputs that competently steer the world toward particular states.</td></tr></tbody></table>\n\n<table><tbody><tr><td><p>1b. <strong>Goal-Oriented Systems Exhibit Catastrophic Instrumental Convergence. </strong>E.g., considering the instrumental strategies <a href=\"https://nickbostrom.com/superintelligentwill.pdf\"><i>Superintelligence</i></a><i> </i>focuses on. For most states of the world you could ultimately be pushing toward (i.e., most \"goals\"), once you understand your situation well enough, you'll tend to want there to exist optimizers that share your goal (\"self-preservation\", \"goal-content integrity\") and you'll tend to want more power (\"cognitive enhancement\", \"technological perfection\"), and resources (\"resource acquisition\").</p><p>Humans are potential threats, and we consume (and are made out of) resources that can be put to other ends, so most goals that don't specifically value human welfare as an end will endorse the conditional strategy \"if you see a sufficiently cheap and reliable way to kill all humans, take that opportunity\".</p></td></tr></tbody></table>\n\n1a and 1b suggest that if STEM-level AGI technology proliferates widely, we're dead (conditional on 2+3+4). If it makes sense to try to build STEM-level AGI at all in that situation, then the obvious thing to do with your STEM-level AGI is to try to leverage its capabilities to prevent other AGIs from destroying the world (a \"[pivotal act](https://arbital.com/p/pivotal/)\"). But:\n\n<table><tbody><tr><td><p>1c. <strong>Averting Instrumental Pressures in Pivotal-Act-Enabling AGI is Substantially Difficult. </strong>It looks very difficult to safely perform a pivotal act with an AGI system that doesn't value human survival and flourishing as an end, because there's no obvious way to avoid dangerous instrumental strategies in systems that capable.</p><p>Substantial alignment breakthroughs are very likely required here (and in value loading, interpretability, etc.). We likely won't get such breakthroughs in time, though we should certainly put a huge effort into trying.</p></td></tr></tbody></table>\n\n1a and 1b are in effect saying that the *least* informed and safety-conscious people in the world are likely to build AI systems with dangerous conditional incentives. If you *don't try at all* to instill the right goals into your STEM-level AGI systems, and don't otherwise try to avert these default [instrumental pressures](https://arbital.com/p/instrumental_pressure/), then your systems will be catastrophically dangerous (if they become capable enough).[^dz6i83owv5a]\n\n1c makes the much stronger claim that the *most* safety-conscious people will fail to avert these instrumental pressures, as a strong default. (Assuming they build AGI that's powerful enough to possibly be useful for a pivotal act or any similarly ambitious feat.)\n\nChalmers [asked for](https://twitter.com/davidchalmers42/status/1647359862622281728) \"canonical (or at least MIRI-canonical) cases for the premises (esp 1, 2, and 5)\", so I'll collect some sources for supporting arguments here, though I don't think there's a single \"canonical\" source. Many of the arguments support multiple premises or sub-premises, so there's some arbitrariness in where I mention these below.\n\nI'm not aware of a good resource that fully captures the MIRI-ish perspective on 1a (\"**STEM-Level AGIs Exhibit Goal-Oriented Behavior by Default**\"), but from my perspective some of the key supporting arguments are:\n\n*   [Consequentialist Cognition](https://arbital.com/p/consequentialist/): \"Steering toward outcomes\" is a relatively simple idea, with a relatively simple formal structure (preference orderings, functions from outcomes to actions that tend to produce them, etc.).\n*   [Coherent Decisions Imply Consistent Utilities](https://arbital.com/p/intro_utility_coherence/) and [Coherence arguments imply a force for goal-directed behavior](https://www.lesswrong.com/posts/DkcdXsP56g9kXyBdq/coherence-arguments-imply-a-force-for-goal-directed-behavior): Visible deviations from this structure tend to correspond to \"throwing away resources for no reason\".\n    *   So humans, evolution- or SGD-ish processes, [learned optimizers](https://www.lesswrong.com/tag/mesa-optimization) modifying their own thoughts or building successors, etc. have incentives to iron out these inefficiencies wherever possible.\n    *   Gwern Branwen's [Why Tool AIs Want To Be Agent AIs](https://gwern.net/tool-ai) discusses other reasons goal-oriented behavior (and other aspects of \"agency\", a term I usually try to avoid) tends to be incentivized where it's an available option.\n*   General-purpose science, technology, engineering, and mathematics work is hard, requiring lining up *many* ducks in a row. \"Minds that try to steer toward specific world-states\" are a relatively simple and obvious way to do sufficiently hard things. So even if humanity is only trying to do hard STEM work and isn't specifically trying to produce goal-oriented systems, it's likely that the *way* we first do this will involve goal-oriented systems.\n*   This is also empirically what happened when evolution built scientific reasoners. Insofar as we can think of evolution as an optimization process, it was neither optimizing for \"build goal-oriented systems\" nor for \"build STEM workers\", but was instead (myopically) optimizing for our ancestors' brains to solve various local problems in their ancestral environment, like \"don't eat poisonous berries\" and \"get powerful coalitions of other humans to adopt strategies that are likelier to propagate my genes\". This happened to produce relatively general reasoning systems that exhibit goal-oriented behavior, and our cognitive generality and goal-oriented optimization then resulted in us becoming good at STEM further down the road (with no additional evolutionary optimization of our brains for STEM).\n*   Having \"goals\" in the required sense is a more basic and disjunctive property than it may initially seem. It doesn't necessarily require, for example:\n    *   ... that the system be at all human-like, or that it have conscious human-style volition.\n    *   ... that the system have an internal model of itself, or a model of its goals; or that it be reflectively consistent.\n    *   ... that the system's brain cleanly factor into a \"goal\" component plus other components.\n    *   ... that the system have only one goal.\n    *   ... that all parts of the system work toward the same goal.\n    *   ... that the \"goal\" be a property of one AI system, rather than something that emerges from multiple systems' interaction.\n    *   ... that the system's goal be perfectly stable over time.\n    *   ... that the system's goal be defined over the physical world vs. over its own mind.[^nl8rhy22ad]\n    *   ... that the system's output channel be a physical \"action\", vs. (say) a text channel.\n    *   ... that the system have a conventional output channel at all, vs. (say) programmers extracting information from its brain via interpretability tools.[^9xa79noj2vh]\n    *   ... that the system or network-of-systems have no humans in the loop. If humans are manually passing information back and forth between different parts of the system or supersystem's \"mind\", this doesn't necessarily address the core dangers, since being physically involved in the system's cognition doesn't mean that you personally understand the implications of what you're doing and can avoid any dangerous steps in the process. Likewise, if humans are doing physical work for the AI rather than giving it actuators, the humans *are* the actuators from the AI's perspective, and can be manipulated into doing things we wouldn't on reflection want to do.\n*   Instead, having \"goals\" in the relevant sense just requires that the system be steering toward outcomes at all — as opposed to, say, its outputs looking like a [sphex's](https://www.lesswrong.com/posts/W5HcGywyPoDDdJtbz/trigger-action-planning) reflex behavior, [insensitive](https://www.lesswrong.com/posts/hQHuXuRGZxxWXaPgg/the-blue-minimizing-robot) to the future's state).[^jt6ijp8qvpb]\n    *   Considerations like the above are a lot of why I don't even discuss \"goals\" in [The Basic Reasons I Expect AGI Ruin](https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin). Instead, item 2 in that post emphasizes that all action sequences that push the world toward some sufficiently hard-to-reach state tend to be dangerous. The (catastrophic) instrumental convergence thesis holds for the action sequences themselves. I discuss \"goals\" more in this post mainly because I'm modeling Chalmers' target audience as pretty different from my own in various ways.\n*   People will want AGI to do very *novel* STEM work (and promising pivotal acts in particular seem to require novel STEM work). Regurgitating or mildly tweaking human insights is one thing; efficiently advancing the scientific frontier seems far harder with shallow, unfocused, unstrategic pattern regurgitation.\n*   [The Basic Reasons I Expect AGI Ruin](https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin) (item 1): STEM-level cognition requires \"an enormous amount of laserlike focus and strategicness when it comes to which thoughts you do or don't think. A large portion of your compute needs to be relentlessly funneled into *exactly* the tiny subset of questions about the physical world that bear on the question you're trying to answer or the problem you're trying to solve. If you fail to be relentlessly targeted and efficient in 'aiming' your cognition at the most useful-to-you things, you can easily spend a lifetime getting sidetracked by minutiae, directing your attention at the wrong considerations, etc.\"\n    *   If an AGI system needs to be strategic and outcome-oriented about the events inside its brain, then it will be much more difficult to keep it from being strategic and outcome-oriented about the events outside of its brain.\n    *   See also [Ngo and Yudkowsky on Alignment Difficulty](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/7im8at9PmhbT4JHsW) and [Ngo and Yudkowsky on Scientific Reasoning and Pivotal Acts](https://www.lesswrong.com/posts/cCrpbZ4qTCEYXbzje/ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts).[^c6nd9elbimc]\n\nSome sources discussing arguments for 1b (\"**Goal-Oriented Systems Exhibit Catastrophic Instrumental Convergence**\"):\n\n*   [*Superintelligence* ch. 7](https://nickbostrom.com/superintelligentwill.pdf), linked above.\n*   [Instrumental convergence](https://arbital.com/p/instrumental_convergence/) and Yudkowsky's [list of arguably convergent strategies](https://arbital.com/p/convergent_strategies/).\n*   [The Value Learning Problem](https://intelligence.org/files/ValueLearningProblem.pdf): Notes that \"Whereas agents at similar capability levels have incentives to compromise, collaborate, and trade, agents with strong power advantages over others can have incentives to simply take what they want.\"\n    *   Cf. a recent [Yudkowsky tweet](https://twitter.com/ESYudkowsky/status/1654141290945331200) noting that humans aren't optimal tools for most (non-human) ends.\n\n[AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) emphasizes that there's no *impossibility* in producing AGI minds with basically whatever properties you want; it just looks too difficult for humanity to do, under time pressure, given anything remotely like our current technical understanding, before AGI causes an existential catastrophe.\n\nTo a large extent the reason we think this is just the reason Nate Soares gives in [Ensuring Smarter-Than-Human Intelligence Has a Positive Outcome](https://intelligence.org/2017/04/12/ensuring/): \"Why do I think that AI alignment looks fairly difficult? The main reason is just that this has been my experience from actually working on these problems.\" But we can say more than that about the shape of some of the difficulties. (Keeping in mind that we think many of the difficulties will turn out to be things that aren't on our radar today.)[^zwva0ud3uy]\n\nSources arguing for 1c (\"**Averting Instrumental Pressures in Pivotal-Act-Enabling AGI is Substantially Difficult**\"):\n\n*   [AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities):\n    *   8: \"The best and easiest-found-by-optimization algorithms for solving problems we want an AI to solve, readily generalize to problems we'd rather the AI not solve\".\n    *   11: \"If cognitive machinery doesn't generalize far out of the distribution where you did tons of training, it can't solve problems on the order of 'build nanotechnology'\" (which seems like the rough capability level needed for using AGI to hit the pause button indefinitely on AGI proliferation).\n*   [Ensuring Smarter-Than-Human Intelligence Has a Positive Outcome](https://intelligence.org/2017/04/12/ensuring/) ([Section 2](https://intelligence.org/2017/04/12/ensuring/#2)): [Corrigibility](https://arbital.com/p/corrigibility/) (the general property of allowing yourself to be shut down, corrected, inspected, etc. rather than manipulating your operators or seizing control from them) turns out to be surprisingly hard to describe in a coherent and precise way.\n    *   [Problem of Fully Updated Deference](https://arbital.com/p/updated_deference/): Normative uncertainty doesn't address the core obstacles to corrigibility.\n*   [Ngo and Yudkowsky on Alignment Difficulty](https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty): Corrigibility is anti-natural to general means-end reasoning. \"\\[W\\]e can see ourselves as asking for a very unnatural sort of object: a path-through-the-future that is robust enough to funnel history into a narrow band in a very wide array of circumstances, but somehow insensitive to specific breeds of human-initiated attempts to switch which narrow band it's pointed towards.\"\n    *   Quoting [AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities): \"'\\[Y\\]ou can't bring the coffee if you're dead' for almost every kind of coffee.  We (MIRI) [tried and failed](https://www.lesswrong.com/posts/5bd75cc58225bf0670374f04/forum-digest-corrigibility-utility-indifference-and-related-control-ideas) to find a coherent formula for an agent that would let itself be shut down (without that agent actively trying to get shut down).  Furthermore, many anti-corrigible lines of reasoning like this may only first appear at high levels of intelligence.\"\n*   [Deep Deceptiveness](https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness): One method for averting instrumental pressures would be to train an AGI to halt its thought processes whenever it starts to approach dangerous topics. But this sort of approach is likely to be extremely brittle, likely to either fail catastrophically or cripple the system, because of issues like [unforeseen maxima](https://arbital.com/p/unforeseen_maximum/) and [nearest unblocked neighbors](https://arbital.com/p/nearest_unblocked/), and because different topics tend to be highly entangled in [rich real-world domains](https://arbital.com/p/rich_domain/), and because we don't know how to specify which topics are \"dangerous\" (see premise 2, below).\n*   [Mild Optimization](https://arbital.com/p/soft_optimizer/): A different approach to averting instrumental pressures would be to limit how hard the AI tries to achieve outcomes in general. This again runs into issues like \"how do we avoid crippling the system in the process?\", as well as \"seemingly mild optimizers often prefer to build, or self-modify into, non-mild optimizers\".\n    *   We can try to build AGI systems to *actively want* to stay mild, but this requires us to solve an unusually difficult form of the value loading problem.\n    *   (Unusually difficult because mildness actively runs counter to effectiveness and efficiency. Pushing for mildness often means unacceptably slowing down systems, and/or incentivizing systems to [work against you](https://arbital.com/p/nonadversarial/) and find ways to become less mild.)\n\n**2\\. Substantial Difficulty of Value Loading**\n\nWhen I say that \"value loading is difficult\", I tend to distinguish four different claims:\n\n<table><tbody><tr><td>2a. <strong>Values Aren't Shared By Default.</strong> If humans don't try to align STEM-level AGI systems at all, then with very high probability, such systems won't share our values. (With values like \"don't kill people\" as a special case.)</td></tr></tbody></table>\n\n<table><tbody><tr><td>2b. <strong>Full Value Loading is Extremely Difficult. </strong>Causing one of the very first STEM-level AGI systems to share all of our core values is ~impossibly difficult.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"14\" data-footnote-id=\"6yvvch5q83\" role=\"doc-noteref\" id=\"fnref6yvvch5q83\"><sup><a href=\"#fn6yvvch5q83\">[14]</a></sup></span>&nbsp;Before we can shoot for a target like that and have a good chance of succeeding, we'll need a lot of practice studying and working with powerful AGI systems, a lot of technical mastery and experience from aligning AGI on easier tasks, and a far deeper understanding of human values and how to robustly check whether we're converging on them.</td></tr></tbody></table>\n\nIf full value loading is going to be out of reach initially, then we can instead try to load enough goals into the first powerful AGI systems to at least cause them to not want to cause catastrophes (e.g., human extinction) while they're performing various powerful tasks for us. But:\n\n<table><tbody><tr><td>2c. <strong>Sufficient-for-Safety Goal Loading is Substantially Difficult. </strong>As a strong default, absent alignment breakthroughs, we won't be able to cause one of the first STEM-level AGI systems to have sufficient-for-safety goals. (E.g., we won't be able to give it the subset of human morality required for it to do ambitious things without destroying the world).</td></tr></tbody></table>\n\n<table><tbody><tr><td>2d. <strong>Pivotal Act Loading is Substantially Difficult. </strong>As a strong default, absent alignment breakthroughs, we won't be able to safely<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"15\" data-footnote-id=\"aax6aiwpc54\" role=\"doc-noteref\" id=\"fnrefaax6aiwpc54\"><sup><a href=\"#fnaax6aiwpc54\">[15]</a></sup></span>&nbsp;cause one of the first STEM-level AGI systems to want to perform an operator-intended <a href=\"https://arbital.com/p/task_goal/\">task</a> that helps prevent the world from being destroyed by other AGIs.</td></tr></tbody></table>\n\n2a gives us a reason to care about 2b: if AGI won't have our values by default, then the obvious response is to try to instill these values into the system. And 2b give us a reason to care about 2c: if we can't have everything right off the bat, we can shoot for \"enough to prevent disasters\".\n\n2b, in combination with 1+3+4+5 (and 2c), again gives us a reason to care about pivotal acts and thereby motivates 1d. If it's difficult to cause AGI systems to share our values, then (given 1, 3, etc.) we face an enormous danger from the first STEM-level AGI systems. This would hold even if 1c were false, since AGI tech will proliferate by default and, given wide access to AGI, sooner or later someone will run a powerful AGI without the safeties.\n\nIf we *can* use AGI to perform some pivotal act (or find some other way to pause AGI development and proliferation for as long as the research community needs), then we can take as much time as needed to nail down *full* value loading.\n\nSo the urgent priority is to find some way to be able to hit the breaks, either before humanity reaches STEM-level AGI, or before STEM-level AGI technology proliferates.\n\nSome sources discussing arguments for 2a (\"**Values Aren't Shared By Default**\"):\n\n*   [No Universally Compelling Arguments](https://www.lesswrong.com/posts/PtoQdG7E8MxYJrigu/no-universally-compelling-arguments): Which arguments (including moral arguments) a mind finds \"compelling\" depends on the mechanistic behavior of that mind. For any given thought or action that is caused by an argument, we could in principle build a mind that responds differently to that same argument.[^17wuo0rt59y]\n*   [Orthogonality](https://arbital.com/p/orthogonality/): An argument that \"there can exist arbitrarily intelligent agents pursuing any kind of goal\". Just as \"is\" doesn't imply \"ought\", effective ability to pursue ends doesn't imply any specific choice of ends. So the field's normal approach to doing AI (\"just try to make the thing smarter\") doesn't give us alignment for free.\n*   [The Design Space of Minds-in-General](https://www.lesswrong.com/posts/tnWRXkcDi5Tw9rzXw/the-design-space-of-minds-in-general): The abstract space of possible minds is enormous and diverse. As a special case, *goals* vary enormously across possible minds. (E.g., there exist enormously many preference orderings over world-histories.)\n*   [Anthropomorphic Optimism](https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism) and [Humans in Funny Suits](https://www.lesswrong.com/posts/Zkzzjg3h7hW5Z36hK/humans-in-funny-suits): We tend to anthropomorphize inhuman optimization processes (e.g., evolution, or non-human animals), and we tend to forget how contingent human traits are. Correcting for biases like these should move us toward thinking values are less shared by default.\n*   [Superintelligent AI is Necessary for an Amazing Future, But Far From Sufficient](https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1): Human values evolved via a lengthy and complex process that surely involved many historical contingencies. But beyond this general point, we can also note specific features of human evolution that seem safety-relevant and are unlikely to be shared by STEM-level AGI systems.[^kp1zt7nng6c]\n\nSources arguing for 2b (\"**Full Value Loading is Extremely Difficult**\"):\n\n*   [Complex Value Systems Are Required to Realize Valuable Futures](https://intelligence.org/files/ComplexValues.pdf): Humane values are surprisingly complex (containing many parts) and fragile (with many points of failure that destroy ~all of the future's value). Also discussed more recently [on Arbital](https://arbital.com/p/complexity_of_value/).\n    *   This point also increases the expected difficulty of sufficient-for-safety goals and of pivotal acts: there are many ways for an AGI to cause disaster in the course of enabling a pivotal act, and there are many different dimensions on which powerful AGI systems need to be simultaneously safe.\n\nSources arguing for 2c (\"**Sufficient-for-Safety Goal Loading is Substantially Difficult**\"):\n\n*   [Cognitive Uncontainability](https://arbital.com/p/uncontainability/), [Context Disaster](https://arbital.com/p/context_disaster/), and [The Hidden Complexity of Wishes](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes)[^b860ayvw83h]: Anticipating the full space of catastrophic hazards is hard, and it's nontrivial to specify individual hazards that we do anticipate.[^aqtldmh61m8]\n    *   [Niceness is unnatural](https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural) and [Detached Lever Fallacy](https://www.lesswrong.com/posts/zY4pic7cwQpa9dnyk/detached-lever-fallacy): Many components of human value seem intuitively simple (e.g., \"just be friendly and cooperative toward other agents\"), but have many complex and contingent features that are required to produce outcomes we'd see as good. (Cf. \"[value-laden](https://arbital.com/p/value_laden/)\" on Arbital.)\n*   [Optimization Amplifies](https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies) and [Robust Delegation](https://intelligence.org/2018/11/04/embedded-delegation/) ([arXiv version](https://arxiv.org/abs/1902.09469)): Optimization amplifies slight differences between what we say we want and what we really want. Specifically, powerful optimization introduces (at least) four versions of Goodhart's Law: regressional, extremal, causal, and adversarial. Powerful optimizers also tend to hack the repository of value.\n*   [AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities): Large parts of the post can be cited here. I would highlight 3 and 5–6 (in [Section A](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_A_)), 10 and 12–15 (in [Section B.1](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_1___The_distributional_leap_), \"the distributional leap\"), and 16–33 (all of [Section B.2](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_2___Central_difficulties_of_outer_and_inner_alignment_) on outer/inner alignment and all of [Section B.3](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#sufficiently_good_and_useful) on interpretability).[^5do09bm3j7i]\n*   [A central AI alignment problem: capabilities generalization, and the sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization): Expands on a point from AGI Ruin: 21, \"Capabilities generalize further than alignment once capabilities start to generalize far.\" STEM-level general intelligence forms an attractor well, whereas alignment with human interests doesn't. And \"On the contrary, sliding down the capabilities well is liable to break a bunch of your existing alignment properties.\"[^9tdt7gjthor]\n*   [Meta-rules for (narrow) value learning are still unsolved](https://arbital.com/p/meta_unsolved/): It's not clear, either in practice or in principle, what meta-procedure could be used to load the right values into an AGI over time, or what meta-meta-procedure could be used to figure out the right meta-procedure over time.\n*   [Low impact](https://arbital.com/p/low_impact/): A concrete example of a goal we might shoot for is \"don't have too large an impact\". [AI Alignment: Why It’s Hard, and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/) discusses early failed attempts to define low-impact or corrigible goals that don't [cripple a system's ability to do anything useful](https://arbital.com/p/safe_useless/).\n\nOther arguments for 2d (\"**Pivotal Act Loading is Substantially Difficult**\"):\n\n*   [AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), 7 and 9: Based on the fact that nobody has come up with an example of a \"pivotal weak act\" (something AGI could do that's weak enough to be clearly safe absent alignment efforts, but strong enough to save the world), it seems very likely that there are no such acts.\n\nThe argument for 2d heavily overlaps with the arguments for 2b and 2c. It matters for 2d what the range of plausible pivotal acts look like, and we haven't published a detailed write-up on pivotal acts, though we discuss them a decent amount in the (lengthy) [Late 2021 MIRI Conversations](https://intelligence.org/late-2021-miri-conversations/).[^8xczdb67iag]\n\n**3\\. High Early Capabilities**\n\nI'll distinguish three subclaims:\n\n<table><tbody><tr><td><strong>3a. Some Early Developers Will Be Able to Make Dangerously Capable STEM-Level AGIs.</strong> In particular, capable enough to understand their situation (so incentives like \"wipe out humans if you find a way to do so\" become apparent, if the system isn't aligned), and capable enough to gain a decisive strategic advantage if they want one.</td></tr></tbody></table>\n\n\"Early developers\" again means \"within five years of the invention of STEM-level AGI\". In fact this needs to happen faster than that in order to support 3b and 3c:\n\n<table><tbody><tr><td><p>3b. <strong>If Some Early Developers Can Do So, Many Early Developers Will Be Able To Do So.</strong> (Assuming the very first developers don't kill us first; and absent defeaters like an AGI-enabled pivotal act or a sufficiently heavy-duty globally enforced ban.)</p><p>As a strong default, AGI tech will spread widely quite quickly. So even if the first developers are cautious enough to avoid disaster, we'll face the issue that not <i>everybody</i> is cautious enough. And we'll likely face this issue within only a few months or years of STEM-level AGI's invention, which make government responses and AGI-mediated pivotal acts far more difficult.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"23\" data-footnote-id=\"0epfa6umjzga\" role=\"doc-noteref\" id=\"fnref0epfa6umjzga\"><sup><a href=\"#fn0epfa6umjzga\">[23]</a></sup></span></p></td></tr></tbody></table>\n\n<table><tbody><tr><td><strong>3c. If Many Early Developers </strong><i><strong>Can</strong></i><strong> Do So, Some </strong><i><strong>Will</strong></i><strong> Do So.</strong> (Again, absent defeaters.)</td></tr></tbody></table>\n\nAnother important claim I'd endorse is \"early STEM-level AGIs will be capable enough to perform pivotal acts\", but this is cause for hope rather than a distinct reason to worry (if you already accept 3a), so it isn't a supporting premise for this particular argument.[^5xqq6gfwtem]\n\nMIRI has never written a canonical \"here are all the reasons we expect STEM-level AGI to be very powerful\" argument. Some relevant sources for 3a (\"**Some Early Developers Will Be Able to Make Dangerously Capable STEM-Level AGIs**\") are:\n\n*   [AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities): Points 1 (\"AGI will not be upper-bounded by human ability or human learning speed\") and 2 (decisive strategic advantage is reachable).\n*   [The Basic Reasons I Expect AGI Ruin](https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin) (point 1).\n*   [Comments on Carlsmith's “Is power-seeking AI an existential risk?”](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential) (\"Background\" section):\n\n> 2\\. The bottleneck on decisive strategic advantages is very likely cognition (of a deep and high-quality variety).\n> \n> *   The challenge of building the aforementioned nanomachines is very likely bottlenecked on cognition alone. (Ribosomes exist, and look sufficiently general to open the whole domain to any mind with sufficient mastery of protein folding, and are abundant.)\n> *   In the modern world, significant amounts of infrastructure can be deployed with just an internet connection -- currency can be attained anonymously, humans can be hired to carry out various physical tasks (such as RNA synthesis) without needing to meet in person first, etc.\n> *   The laws of physics have shown themselves to be \"full of exploitable hacks\" (such as the harnessing of electricity to power lights in every home at night, or nuclear fission to release large amounts of energy from matter, or great feats of molecular-precision engineering for which trees and viruses provide a lower-bound).\n> \n> 3\\. The abilities of a cognitive system likely scale non-continuously with the depth and quality of the cognitions.\n> \n> *   For instance, if you can understand protein folding well enough to get 90% through the reasoning of how your nanomachines will operate in the real world, that doesn't let you build nanomachines that have 90% of the impact of ones that are successfully built to carry out a particular purpose.\n> *   I expect I could do a lot with 100,000 trained-software-engineer-hours, that I cannot do with 1,000,000 six-year-old hours.\n\nSome defeaters for 3a could include \"STEM-level AGI is impossible (e.g., because there's something magical and special about human minds that lets us do science\", \"there's no way to leverage (absolute or relative) intelligence to take over the world\", and \"early STEM-level AGIs won't be (absolutely or relatively) smart enough to access any of those ways\".\n\nI'd tentatively guess that \"there will be lots of different STEM-level AGIs before any AGI can destroy the world\" is false, but if it's true, I think to a first approximation this doesn't lower the probability of AGI ruin. This is because:\n\n*   I still expect at least one early STEM-level AGI to be capable of unilaterally killing humans, if it wants to. Call this AGI \"X\". If the other STEM-level AGIs don't terminally value human survival, they will have no incentive to stop X from killing all humans (and in fact will have an incentive to help X if they can, to reduce the number of potential competitors and threats). This means that the existence of other misaligned AGIs doesn't give X any incentive to avoid killing humans.\n*   If no one STEM-level AGI is capable of unilaterally killing humans, I still expect early STEM-level AGIs to be able to coordinate to do so; and if they don't terminally value human empowerment and coordination is required to disempower humans, I think they will in fact coordinate to disempower humans. This scenario is noted by Eliezer Yudkowsky [here](https://twitter.com/ESYudkowsky/status/1649264653590212608) and [here](https://twitter.com/ESYudkowsky/status/1650635493544189954).\n\nI view 3b (\"**If Some Early Developers Can Do So, Many Early Developers Will Be Able To Do So**\") and 3c (\"**If Many Early Developers** ***Can*** **Do So, Some** ***Will*** **Do So**\") as following from the normal way AI tech has proliferated over time: it didn't take 10 years for other groups to match GPT-3 or ChatGPT once they were deployed, and there are plenty of incautious people who think alignment is silly, so it seems inevitable that someone will deploy powerful misaligned AGI if no major coordination effort or pivotal-act-via-AGI blocks this.\n\n**4\\. Conditional Ruin**\n\nPremises 1–3 each begin with \"As a strong default...\", so one way to object to this premise is just to concede these are three \"strong defaults\", but say they aren't jointly strong *enough* to carry an \"X is very likely\" conclusion.\n\nDepending on the conversational goal, I could respond by switching to a probabilistic argument, or by stipulating that \"strong default\" here means \"strong enough to make premise 4 true\".\n\nBeyond that, I think this claim is fairly obvious at a glance.\n\n**5\\. Inadequacy**\n\n\"\\[There will be no alignment breakthroughs or global coordination breakthroughs before we invent STEM-level AGI\" is obviously a lot stronger than the conclusion requires: seeing breakthroughs in either domain doesn't mean that the breakthroughs were sufficient to avert catastrophe. But I weakly predict that there in fact won't be any breakthroughs in either domain, so this unnecessarily strong premise seems like a fine starting point.\n\nWhen stronger claims are justifiable but weaker claims are sufficient, bad outcomes look more overdetermined, which strengthens the case for thinking we're in a *dire* situation calling for an extraordinary response.\n\nI don't think MIRI has written a centralized argument regarding 5. We're much more interested in intervening on it than in describing it, and if things are going well, it should look like a moving target.\n\nWe've written at least a little about [why AGI timelines don't look super long to us](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential#Timelines), and we've written at greater length about why alignment seems to us to be moving too slowly — e.g., in [On How Various Plans Miss the Hard Bits of the Alignment Challenge](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment) and [AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities). Posts like [Security Mindset and Ordinary Paranoia](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/), [Security Mindset and the Logistic Success Curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/), and [Brainstorm of Things That Could Force an AI Team to Burn Their Lead](https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/p3s8RvkcyTwzu27ps) help paint a qualitative picture of how hard we think it would be to *actually* succeed in STEM-level AGI alignment, and therefore how overdetermined failure looks.\n\nThe AGI ruin argument mostly rests on claims that the alignment and deployment problems are difficult and/or weird and novel, [not on strong claims about society](https://www.lesswrong.com/posts/4Wk2iHigCcmaTWYiz/agi-ruin-mostly-rests-on-strong-claims-about-alignment-and). The bar for a sufficient response seems high, and the responses required are unusual and extreme, with a high need for proactive rather than reactive action in the world.\n\nOur arguments for discontinuous and rapid AI capability gains are possibly the main reason we're more pessimistic than others about governments responding well. We also have unusually high baselines pessimism about government sanity by EA standards, but I don't think this is the main source of model disagreement.\n\n[^erl9bzappwh]: Other options include Joe Carlsmith's Is Power-Seeking AI an Existential Risk? (which Nate Soares replied to here) and Katja Grace's Argument for AI X-Risk from Competent Malign Agents.Note that I'm releasing this post without waiting on other MIRI staff to endorse it or make changes, so this can be treated as my own attempt to build a structured argument, rather than as something Eliezer, Nate, Benya, or others would necessarily endorse. \n\n[^otkehmiaqgl]: Like \"AGI\", \"STEM-level AGI\" lacks a formal definition. (If we did have a deep formal understanding of reasoning about the physical world, we would presumably be able to do many feats with AI that we cannot do today.)Absent such a definition, however, we shouldn't ignore the observed phenomenon that there's a certain kind of problem-solving ability (observed in humans) that generalizes to inventing steam engines and landing on the Moon, even though our brains didn't evolve under direct selection pressure to start industrial revolutions or visit other planets, and even though birds and nematodes can't invent steam engines or land on the Moon.We can then ask what happens when we find a way to automate this kind of problem-solving ability. \n\n[^iqw10v88yvh]: \"The basic mental machinery\" is vague, and maybe some would argue that GPT-4 already has all of the right \"mental machinery\" in some sense, in spite of its extremely limited ability to do novel STEM work in practice. (I disagree with this claim myself.)E.g., some might analogize GPT-4 to a human child: a sufficiently young John von Neumann will lack some \"basic mental machinery\" required for STEM reasoning, but will at least have meta-machinery that will predictably unfold into the required machinery via normal brain development and learning.(And, indeed, the difference between \"having the basic mental machinery for STEM\" and \"having meta-machinery that will predictably unfold into the basic mental machinery\" may not be a crisp one. Even the adult von Neumann presumably continued to upgrade his own general problem-solving software via adopting new and better heuristics.)I don't think that GPT-4 in fact has all of the basic mental machinery or meta-machinery for STEM, and I don't personally think that comparing GPT-4 to a human child is very illuminating. I'm also not confident one way or the other about whether GPTs will scale to \"as good at science as smart humans\".That said, since people can disagree about the nature of general intelligence and about what's actually going on in humans or AI systems when we do scientific work, it might be helpful to instead define \"STEM-level\" AI as AI technology that can (e.g.) match smart human performance in a specific hard science field, across all the scientific work humans do in that field.As a strong default, I expect AI with that level of capability to be able to generalize to all the sciences, and to reasoning about any other topic humans can reason about; and that level of generality and capability seems to me to be the level where we face AI-mediated extinction risks. \n\n[^2mdfiwtqwtw]: The Arbital articles I link in this post, and most of the AI alignment content on Arbital, were written by Eliezer Yudkowsky in 2015–2017. I consider this one of the best online resources regarding AI alignment, though a lot of it is relatively unedited or incomplete. \n\n[^320szmcv6y9]: If human whole-brain emulation is built before (or shortly after) STEM-level AGI, and this allows us to run human minds at faster speeds, then this opens up a lot more possibility for things to occur \"early\" (as measured in sidereal time).It might even be possible to solve coherent extrapolated volition within five sidereal years of the invention of STEM-level AGI. (Though if so, I'm imagining this happening via ems and AI systems achieving feats that might have otherwise taken thousands of years of work, including enormous amounts of work gaining a mature understanding of the human mind, iteratively improving the ems' speed and reasoning abilities, and very carefully and conservatively ratcheting up the capabilities of AI systems — and widening the set of tasks we can safely use them for — as we gain more mastery of alignment.)To be clear: I'd consider it an obviously terrible idea, bordering on suicidal, to gamble the future on a pivotal act that does no monitoring or intervening in the wider world for five entire years after the invention of STEM-level AGI. I'd say that one year is already taking on a lot of risk, and three years is clearly too long.But at the point where safety-conscious AGI developers are being cheaply run at 1000x speed relative to all the non-safety-conscious AGI developers, monitoring the world for planet-endangering threats (and intervening if necessary) is probably reasonably trivial. The hard part is getting to whole-brain emulation (and powerful hardware for running the ems) in the first place. \n\n[^r2jw6u00ss]: This is not, of course, to say that \"AGI can achieve decisive strategic advantage within five years\" is necessary for the AGI situation to be dire. \n\n[^1n8504xok95]: Also, \"human survival\" is a phrase some transhumanists (myself included) will object to as ambiguous. I think involuntary human death is bad, but I think it's probably good if we voluntarily upload ourselves and develop into cool posthumans, regardless of whether that counts as biological \"death\" or \"extinction\" in some purely technical sense.I use the phrase \"human survival\" in spite of all these issues because I (perhaps wrongly) imagine that Chalmers is looking for an argument that a wide variety of non-transhumanists will immediately see the importance of. Ordinary people can clearly see that it's bad for AI to kill them and their loved ones (and can see why this is bad), without any need to wade into deep philosophical debates or utopia-crafting.Focusing on something more abstract risks misleading people about the severity of the risk (\"surely if you had something that scary in mind, you'd blurt it out rather than burying the lede\"), and also about its nature (\"surely if you thought AI would literally just kill everyone, you'd say that\"). If I instead mostly worried about AI disaster scenarios where AI doesn't literally kill everyone, I'd talk about those instead. \n\n[^dz6i83owv5a]: In principle one could make a simpler argument for pivotal acts by just saying \"World-destroyingly-powerful AGI technology will proliferate by default, and if everyone has the ability to destroy the world then someone will inevitably do it on purpose\".But in reality the situation is far worse than that, because even if we could limit AGI access to people who would never deliberately use AGI to try to do evil, AGI systems' own default incentives make them extremely dangerous. Moreover, this issue blocks our ability to safely use AGI for pivotal acts as well. \n\n[^nl8rhy22ad]: It does matter that the system be able to generate hypotheses and instrumental strategies concerning the physical world; but the system's terminal goal doesn't need to concern to the physical world in order for the system to care about steering the physical world. E.g., a system that just wants its mind to be in a certain state will care about its hardware (since changes in hardware state will affect its mind), which means caring about everything in the larger world that could potentially affect its hardware. \n\n[^9xa79noj2vh]: Cf. Microscope AI in Hubinger's An Overview of 11 Proposals for Building Safe Advanced AI.Microscope AI also involves \"using transparency tools to verify that the model isn’t performing any optimization\", but part of my argument here is that it's extremely unlikely we'll be able to get major new scientific/predictive insights from AI without it doing any \"optimizing\". However, we might in principle be able to verify that the AI isn't doing too much optimizing, or optimizing in the wrong directions, or optimizing over relatively risky domains, etc. In any case, we can consider the wider space of strategies that involve inspecting the AI's mind as an alternative to using conventional outputs of the system.If operators have enough visibility into the AGI's mind, and enough deep understanding and useful tools for making sense of all important information in that mind, then in principle \"do useful science by looking at the AGI's mind rather than by giving it an output channel\" can prevent any catastrophes that result from the AGI deliberately optimizing against human interests.(Though we would still need to find ways to get the AGI to do specific useful cognitions and not just harmful ones. And also, if you have that much insight into the AGI's mind and can get it to think useful and relevant thoughts at all, then you may be able to avoid Microscope-AI approaches, by trusting the AI's outputs so long as it hasn't had any dangerous thoughts anywhere causally upstream of the outputs.)In real life, however, it's very unlikely that we'll have that level of mastery of the first STEM-level AGI systems. If we only have partial visibility and understanding of the AGI's mind, then Microscope AI can in principle just be used by the AGI as another output channel, particularly if it learns or deduces things about which parts of its mind we're inspecting, how we tend to interpret different states of its brain, etc. This is a more constrained problem from the AI's perspective, but it still seems to demand some very difficult alignment breakthroughs for humanity to perform a pivotal act by this method. \n\n[^jt6ijp8qvpb]: Note that \"sphexish\" isn't an all-or-nothing property, and if you zoom in on any agentic brain in enough detail, you should expect the parts to eventually start looking more sphexish. This is because \"agency\" isn't a primitive property, but rather arises from the interaction of many gears, and sufficiently small gears will do things more automatically, without checking first to take into account context, etc.The important question is: \"To what extent do these sphex-like gears assemble into something that's steering toward outcomes at the macro-level, versus assembling into something that's more sphex-like at the macro-level?\" \n\n[^c6nd9elbimc]: Quoting Yudkowsky in Ngo and Yudkowsky on Alignment Difficulty: \"[A]n earlier part of the path [to building AGI systems that exhibit dangerous means-ends reasoning, etc.] is from being optimized to do things difficult enough that you need to stop stepping on your own feet and have different parts of your thoughts work well together\".Quoting Yudkowsky in Ngo and Yudkowsky on Scientific Reasoning and Pivotal Acts: \"[...] Despite the inevitable fact that some surprises of this kind now exist, and that more such surprises will exist in the future, it continues to seem to me that science-and-engineering on the level of 'invent nanotech' still seems pretty unlikely to be easy to do with shallow thought, by means that humanity discovers before AGI tech manages to learn deep thought?\"What actual cognitive steps?  Outside-the-box thinking, throwing away generalizations that governed your previous answers and even your previous questions, inventing new ways to represent your questions, figuring out which questions you need to ask and developing plans to answer them; these are some answers that I hope will be sufficiently useless to AI developers that it is safe to give them, while still pointing in the direction of things that have an un-GPT-3-like quality of depth about them.\"Doing this across unfamiliar domains that couldn't be directly trained in by gradient descent because they were too expensive to simulate a billion examples of[.]\"If you have something this powerful, why is it not also noticing that the world contains humans?  Why is it not noticing itself?\" \n\n[^zwva0ud3uy]: Issues that are visible today probably won't spontaneously solve themselves without a serious technical effort, but new obstacles can certainly crop up. (See the discussion of software development hell and robust-software-in-particular hell in The Basic Reasons I Expect AGI Ruin, and the \"rocket-accelerating cryptographic Neptune probe\" analogy in So Far: Unfriendly AI Edition.)  \n\n[^6yvvch5q83]: Note that \"share all of our core values\" is imprecise: what makes a value \"core\" in the relevant sense? How do we enable moral progress, and avoid locking in our current flawed values? It's an extremely thorny problem. I endorse coherent extrapolated volition as a good (very high-level and abstract) description of desiderata for a solution. On LessWrong and Arbital, the phrase \"humane values\" is often used to specifically point at \"the sort of values we ought to want to converge on eventually\", as opposed to our current incomplete and flawed conceptions of what's morally valuable, aesthetically valuable, etc.Note also that the challenge here is causing AGI systems to consistently optimize for humane values; it's not merely to cause AGI systems to understand our values. The latter is far easier, because it doesn't depend on the AGI's goals; a sufficiently capable paperclip maximizer would also want to understand human goals, if its environment contained humans. \n\n[^aax6aiwpc54]: \"Safely\" doesn't necessarily require that the AGI terminally values human survival. I'd put more probability on AGI systems being safe if they aren't internally representing humans at all, with safety coming from this fact in combination with other alignment measures. \n\n[^17wuo0rt59y]: This doesn't rule out that some responses to arguments are more common than others; and indeed, we should expect sufficiently capable minds to converge on similar responses to things like \"valid logical arguments\", since accepting such arguments is very useful for being \"sufficiently capable\".The problem is that sufficiently capable reasoners don't converge on accepting human morality. \"Accept valid logical arguments\" is useful for nearly all ambitious real-world ends, so we should expect it to arise relatively often as an instrumental strategy and/or as a terminal goal. \"Care for humans\" is useful for a far smaller range of ends. \n\n[^kp1zt7nng6c]: Some relevant passages, discussing evolved aliens and then artificial minds:\"[...] I think my point estimate there is 'most aliens are not happy to see us', but I’m highly uncertain. Among other things, this question turns on how often the mixture of 'sociality (such that personal success relies on more than just the kin-group), stupidity (such that calculating the exact fitness-advantage of each interaction is infeasible), and speed (such that natural selection lacks the time to gnaw the large circle of concern back down)' occurs in intelligent races’ evolutionary histories.\"These are the sorts of features of human evolutionary history that resulted in us caring (at least upon reflection) about a much more diverse range of minds than 'my family', 'my coalitional allies', or even 'minds I could potentially trade with' or 'minds that share roughly the same values and faculties as me'.\"Humans today don’t treat a family member the same as a stranger, or a sufficiently-early-development human the same as a cephalopod; but our circle of concern is certainly vastly wider than it could have been, and it has widened further as we’ve grown in power and knowledge.\"[... T]he development process of misaligned superintelligent AI is very unlike the typical process by which biological organisms evolve.\"Some relatively important differences between intelligences built by evolution-ish processes and ones built by stochastic-gradient-descent-ish processes:\"• Evolved aliens are more likely to have a genome/connectome split, and a bottleneck on the genome.\"• Aliens are more likely to have gone through societal bottlenecks.\"• Aliens are much more likely the result of optimizing directly for intergenerational prevalence. The shatterings of a target like 'intergenerational prevalence' are more likely to contain overlap with the good stuff, compared to the shatterings of training for whatever-training-makes-the-AGI-smart-ASAP. (Which is the sort of developer goal that’s likely to win the AGI development race and kill humanity first.)\"Evolution tends to build patterns that hang around and proliferate, whereas AGIs are likely to come from an optimization target that's more directly like 'be good at these games that we chose with the hope that being good at them requires intelligence', and the shatterings of the latter are less likely to overlap with our values.\" \n\n[^b860ayvw83h]: A version of The Hidden Complexity of Wishes also appears in Complex Value Systems Are Required to Realize Valuable Futures. \n\n[^aqtldmh61m8]: Note that this is separate from the issue that it's hard to instill particular goals into powerful AGI system at all. This point is discussed more in AGI Ruin. \n\n[^5do09bm3j7i]: Summarizing the relevant items:3: \"We need to get alignment right on the 'first critical try' at operating at a 'dangerous' level of intelligence\". This makes it more difficult to achieve any desired property in STEM-level AGI.5 and 6: \"We can't just build a very weak system, which is less dangerous because it is so weak, and declare victory; because later there will be more actors that have the capability to build a stronger system and one of them will do so.\" If the system is weak, then flaws in its goals like \"be low-impact\" or \"don't hurt humans\" matter less. But we need at least one system strong enough to help in some pivotal act (unless we find some way to globally limit AGI proliferation without the help of STEM-level AGI), which makes it far more dangerous if its goals are flawed.10: \"Powerful AGIs doing dangerous things that will kill you if misaligned, must have an alignment property that generalized far out-of-distribution from safer building/training operations that didn't kill you.\"12: \"Operating at a highly intelligent level is a drastic shift in distribution from operating at a less intelligent level, opening up new external options, and probably opening up even more new internal choices and modes.  Problems that materialize at high intelligence and danger levels may fail to show up at safe lower levels of intelligence, or may recur after being suppressed by a first patch.\"13 and 14: \"Many alignment problems of superintelligence will not naturally appear at pre-dangerous, passively-safe levels of capability.\"15: \"Fast capability gains seem likely, and may break lots of previous alignment-required invariants simultaneously.\"16: \"Even if you train really hard on an exact loss function, that doesn't thereby create an explicit internal representation of the loss function inside an AI that then continues to pursue that exact loss function in distribution-shifted environments.\"17: \"[O]n the current optimization paradigm there is no general idea of how to get particular inner properties into a system, or verify that they're there, rather than just observable outer ones you can run a loss function over.\"18: \"[I]f you show an agent a reward signal that's currently being generated by humans, the signal is not in general a reliable perfect ground truth about how aligned an action was, because another way of producing a high reward signal is to deceive, corrupt, or replace the human operators with a different causal system which generates that reward signal\".19: \"More generally, there is no known way to use the paradigm of loss functions, sensory inputs, and/or reward inputs, to optimize anything within a cognitive system to point at particular things within the environment\".20: \"Human operators are fallible, breakable, and manipulable.\"21 and 22: \"When you have a wrong belief, reality hits back at your wrong predictions. [...] Reality doesn't 'hit back' against things that are locally aligned with the loss function on a particular range of test cases, but globally misaligned on a wider range of test cases.\" Thus \"Capabilities generalize further than alignment once capabilities start to generalize far.\"Section B.3 (25–33): Sufficiently good and useful transparency / interpretability seems extremely difficult. \n\n[^9tdt7gjthor]: \"Why? Because things in the capabilities well have instrumental incentives that cut against your alignment patches. Just like how your previous arithmetic errors (such as the pebble sorters on the wrong side of the Great War of 1957) get steamrolled by the development of arithmetic, so too will your attempts to make the AGI low-impact and shutdownable ultimately (by default, and in the absence of technical solutions to core alignment problems) get steamrolled by a system that pits those reflexes / intuitions / much-more-alien-behavioral-patterns against the convergent instrumental incentive to survive the day.\"Quoting from footnote 3 of A central AI alignment problem: capabilities generalization, and the sharp left turn: \"Note that this is consistent with findings like 'large language models perform just as well on moral dilemmas as they perform on non-moral ones'; to find this reassuring is to misunderstand the problem. Chimps have an easier time than squirrels following and learning from human cues. Yet this fact doesn't particularly mean that enhanced chimps are more likely than enhanced squirrels to remove their hunger drives, once they understand inclusive genetic fitness and are able to eat purely for reasons of fitness maximization. Pre-left-turn AIs will get better at various 'alignment' metrics, in ways that I expect to build a false sense of security, without addressing the lurking difficulties.\" \n\n[^8xczdb67iag]: The kinds of capabilities we expect to be needed for a pivotal act are similar to those required for the strawberry problem (\"Place, onto this particular plate here, two strawberries identical down to the cellular but not molecular level.\"). Yudkowsky's unfinished Zermelo-Fraenkel provability oracle draft makes the specific claim that powerful theorem-proving wouldn't help save the world. \n\n[^0epfa6umjzga]: Cf. AGI Ruin, point 4. \n\n[^5xqq6gfwtem]: I think the easiest pivotal acts are somewhat harder than the easiest strategies a misasligned AGI could use to seize power; but (looking only at capability and not alignability) I expect AGI to achieve both capabilities at around the same time, coinciding with (or following shortly after) the invention of STEM-level AGI.",
      "plaintextDescription": "Philosopher David Chalmers asked:\n\n> [I]s there a canonical source for \"the argument for AGI ruin\" somewhere, preferably laid out as an explicit argument with premises and a conclusion?\n\nUnsurprisingly, the actual reason people expect AGI ruin isn't a crisp deductive argument; it's a probabilistic update based on many lines of evidence. The specific observations and heuristics that carried the most weight for someone will vary for each individual, and can be hard to accurately draw out.\n\nThat said, Eliezer Yudkowsky's So Far: Unfriendly AI Edition might be a good place to start if we want a pseudo-deductive argument just for the sake of organizing discussion.  People can then say which premises they want to drill down on.[1]\n\n \n\nIn The Basic Reasons I Expect AGI Ruin, I wrote:\n\n> When I say \"general intelligence\", I'm usually thinking about \"whatever it is that lets human brains do astrophysics, category theory, etc. even though our brains evolved under literally zero selection pressure to solve astrophysics or category theory problems\".\n> \n> It's possible that we should already be thinking of GPT-4 as \"AGI\" on some definitions, so to be clear about the threshold of generality I have in mind, I'll specifically talk about \"STEM-level AGI\",[2] though I expect such systems to be good at non-STEM tasks too.\n\nSTEM-level AGI is AGI that has \"the basic mental machinery required to do par-human reasoning about all the hard sciences\",[3] though a specific STEM-level AGI could (e.g.) lack physics ability for the same reasons many smart humans can't solve physics problems, such as \"lack of familiarity with the field\".\n\nA simple way of stating the argument in terms of STEM-level AGI is:\n\n 1. Substantial Difficulty of Averting Instrumental Pressures:[4] As a strong default, absent alignment breakthroughs, STEM-level AGIs that understand their situation and don't value human survival as an end will want to kill all humans if they can.\n 2. Substantial Difficulty of Value Loading: ",
      "wordCount": 5646
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "D6y2AgYBeHsMYqWC4",
        "name": "AI Safety Public Materials",
        "slug": "ai-safety-public-materials-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4Wk2iHigCcmaTWYiz",
    "title": "AGI ruin mostly rests on strong claims about alignment and deployment, not about society",
    "slug": "agi-ruin-mostly-rests-on-strong-claims-about-alignment-and",
    "url": null,
    "baseScore": 70,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2023-04-24T13:06:02.255Z",
    "contents": {
      "markdown": "Dustin Moskovitz writes [on Twitter](https://twitter.com/moskov/status/1644476965838266371):\n\n> My intuition is that MIRI's argument is almost more about sociology than computer science/security (though there *is* a relationship). People won't react until it is too late, they won't give up positive rewards to mitigate risk, they won't coordinate, the govt is feckless, etc.\n> \n> And that's a big part of why it seems overconfident to people, bc sociology is not predictable, or at least isn't believed to be.\n\nAnd Stefan Schubert writes:\n\n> I think it's good @robbensinger wrote a list of reasons he expects AGI ruin. It's well-written.\n> \n> But it's notable and symptomatic that 9/10 reasons relate to the nature of AI systems and only 1/10 (discussed in less detail) to the societal response.\n\n> [https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin](https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin) \n> \n> Whatever one thinks the societal response will be, it seems like a key determinant of whether there'll be AGI ruin.\n> \n> Imo the debate on whether AGI will lead to ruin systematically underemphasises this factor, focusing on technical issues.\n> \n> It's useful to distinguish between warnings and all-things-considered predictions in this regard.\n> \n> When issuing warnings, it makes sense to focus on the technology itself. Warnings aim to elicit a societal response, not predict it.\n> \n> [https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential](https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential) \n> \n> But when you actually try to predict what'll happen all-things-considered, you need to take the societal response into account in a big way\n> \n> As such I think Rob's list is better as a list of reasons we ought to take AGI risk seriously, than as a list of reasons it'll lead to ruin\n\nMy reply is:\n\nIt's true that in my \"[top ten reasons I expect AGI ruin](https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin)\" list, only one of the sections is about the social response to AGI risk, and it's a short section. But the section links to some more detailed discussions (and quotes from them in a long [footnote](https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin#fnfm0m5ssc91)):\n\n*   [Four mindset disagreements behind existential risk disagreements in ML](https://www.lesswrong.com/posts/84BJopKvQ8qYGHY6b/four-mindset-disagreements-behind-existential-risk)\n*   [The inordinately slow spread of good AGI conversations in ML](https://www.lesswrong.com/posts/Rkxj7TFxhbm59AKJh/the-inordinately-slow-spread-of-good-agi-conversations-in-ml)\n*   [*Inadequate Equilibria*](https://equilibriabook.com/toc/)\n\nAlso, discussing the adequacy of society's response before I've discussed AGI itself at length doesn't really work, I think, because I need to argue for what kind of response is warranted before I can start arguing that humanity is putting insufficient effort into the problem.\n\nIf you think the alignment problem itself is easy, then I can cite all the evidence in the world regarding \"very few people are working on alignment\" and it won't matter.\n\nIf you think a slowdown is unnecessary or counterproductive, then I can point out that governments haven't placed a ceiling on large training runs and you'll just go \"So? Why should they?\"\n\nSociety's response can only be inadequate given some model of what's required for adequacy. That's a lot of why I factor out that discussion into other posts.[^tkqzeh4uiu]\n\n* * *\n\nMore importantly, contra Dustin, I don't see myself as having strong priors or complicated models regarding the social situation.\n\nEliezer Yudkowsky [similarly says](https://www.lesswrong.com/posts/hwxj4gieR7FWNwYfa/ngo-and-yudkowsky-on-ai-capability-gains-1) he doesn't have strong predictions about what governments or communities will do in this or that situation (beyond [anti-predictions](https://www.lesswrong.com/tag/antiprediction) like \"they probably won't do specific thing X that's wildly different from anything they've done before\"):\n\n> <table style=\"background-color:rgb(255, 255, 255);border:1px double rgb(179, 179, 179)\"><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:0.4em;vertical-align:top\"><p><strong>[Ngo][12:26]</strong>&nbsp;</p><p>The other thing is that, for pedagogical purposes, I think it'd be useful for you to express some of your beliefs about how governments will respond to AI</p><p>I think I have a rough guess about what those beliefs are, but even if I'm right, not everyone who reads this transcript will be</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:0.4em;vertical-align:top\"><p><strong>[Yudkowsky][12:28]</strong>&nbsp;</p><p>Why would I be expected to know <i>that</i>? I could talk about weak defaults and iterate through an unending list of possibilities.</p><p>Thinking that Eliezer thinks he knows that to any degree of specificity feels like I'm being weakmanned!</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:0.4em;vertical-align:top\"><p><strong>[Ngo][12:28]</strong>&nbsp;</p><p>I'm not claiming you have any specific beliefs</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:0.4em;vertical-align:top\"><p><strong>[Yudkowsky][12:29]</strong>&nbsp;</p><p>I suppose I have skepticism when other people dream up elaborately positive and beneficial reactions apparently drawn from some alternate nicer political universe that had an absolutely different response to Covid-19, and so on.</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:0.4em;vertical-align:top\"><p><strong>[Ngo][12:29]</strong>&nbsp;</p><p>But I'd guess that your models rule out, for instance, the US and China deeply cooperating on AI before it's caused any disasters</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:0.4em;vertical-align:top\"><p><strong>[Yudkowsky][12:30]</strong>&nbsp;</p><p>\"Deeply\"? Sure. That sounds like something that has never happened, and I'm generically skeptical about political things that go better than any political thing has ever gone before.</p></td></tr></tbody></table>\n\nI don't feel pessimistic about society across all domains, I don't think most tech or scientific progress is at all dangerous or bad, etc. It's mostly just that AGI looks like a super unusual and hard problem to me.\n\nTo imagine civilization behaving really unusually and doing something a lot harder than it's ever done, I need strong predictive models saying why civilization *will* do those things. Adequate strategies are conjunctive; I don't need special knowledge to predict \"not that\".\n\nIt's true that this requires a bare minimum model of civilization saying that we *aren't* a sane, coordinated super-agent that just handles problems whenever there's something important to do.\n\nIf humanity *did* consistently strategically scale its efforts with the difficulty and importance of problems in the world (even when weird and abstract analysis is required to see how hard and important the problem is), then I *would* expect us to just flexibly scale up our efforts and modify all our [old heuristics](https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects) in response to the alignment problem.[^wpwfccund9]\n\nSo I'm at least making the anti-prediction \"civilization isn't specifically like that\".\n\nExample: I don't in fact see my high p(doom) as resting on a strong assumption about whether people will panic and ban a bunch of AI things. My high level of concern is predicated on a reasonable amount of uncertainty about whether that will happen.\n\nThe issue is that \"people panic and ban things\", while potentially helpful on the margin, does not consistently save the world and cause the long-term future to go well (and there's a nontrivial number of worlds where it makes things worse on net). The same issue of aligning and wielding powerful tech has to be addressed anyway.\n\nMaybe panic buys us another 5 years, optimistically; maybe it even buys us 20, amazingly. But if superintelligence comes in 2055 rather than 2035, I still very much expect catastrophe. So possibilities like this don't strongly shift the set of worlds I expect to see toward optimistic outcomes.\n\n* * *\n\nStefan replies [on Twitter](https://twitter.com/StefanFSchubert/status/1648940041761173504):\n\n> Thanks, Rob, this is helpful.\n> \n> I do actually think you should put the kinds of arguments you give here \\[...\\] in posts like this, since \"people will rise to the occasion\" seems like one of the key counter-argument to your views; so it seems central to rebut that.\n> \n> I also think there's some tension between being uncertain about what the societal response will be and being relatively certain of doom. (Though it depends on the levels of un/certainty.)\n> \n> I think many would give the simple argument:\n> \n> P1: Whether there'll be AI doom depends on the societal response\n> \n> P2: It's uncertain what the societal response will be\n> \n> C: It's uncertain whether there'll be AI doom (so P(doom) isn't very high)\n> \n> Could be good to address that head on\n\nThere's of course tension! Indeed, I'd phrase it more strongly than that: uncertainty about the societal response is one of the largest reasons I still have any hope for the future. It's one of the main factors pushing against high p(doom), on my model.\n\n\"We don't know exactly how hard alignment is, and in the end it's just a technical problem\" is plausibly an even larger factor. It's easier to get clear data about humanity's coordination ability than to get clear data about how hard alignment is: we have huge amounts of *direct observational data* about how humans and nations tend to behave, whereas no amount of failed work can rule out the possibility that someone will come up with a brilliant new alignment approach tomorrow that just works.\n\nThat said, there are enough visible obstacles to alignment, and enough failed attempts have been made at this point, that I'm willing to strongly bet against a miracle solution occurring (while working to try to [prove myself wrong](https://twitter.com/robbensinger/status/1639049899823210496) about this).\n\n\"Maybe society will coordinate to do something miraculous\" and \"maybe we'll find a miraculously effective alignment solution\" are possibilities that push in the direction of hope, but they don't strike me as *likely* in absolute terms.\n\nThe reason \"maybe society will do something miraculous\" seems unlikely to me is mostly just because the scale of the required miracle seems very large to me.\n\nThis is because:\n\n1.  I think it's very likely that we'll need to solve both the alignment problem and the [deployment problem](https://www.cold-takes.com/racing-through-a-minefield-the-ai-deployment-problem/) in order to see good outcomes.\n2.  It seems to me that these two problems **both require getting a large number of things right,** and **some of these things seem very hard, and/or seem to require us to approach the problem is very novel and unusual ways**.\n\n[AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) and [Capabilities Generalization, and the Sharp Left Turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) make the case for the alignment problem seeming difficult and/or out-of-scope for business-as-usual machine learning.\n\n\"[Pivotal acts](https://arbital.com/p/pivotal/) seem hard\" and \"there isn't a business-as-usual way to prevent AGI tech from proliferating and killing everyone\" illustrate why the deployment problem seems difficult and/or demanding of very novel strategies, and [Six Dimensions of Operational Adequacy in AGI Projects](https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects) fills in a lot of the weird-or-hard details.\n\nWhen we're making a large enough ask of civilization (in terms of raw difficulty, and/or in terms of requiring civilization to go wildly off-script and do things in very different ways than it has in the past), we can have a fair amount of confidence that civilization won't fulfill the ask even if we're highly uncertain about the specific dynamics at work, the specific course history will take, etc.\n\n[^tkqzeh4uiu]: It's also not clear to me what Stefan (or Dustin) would want me to actually say about society, in summarizing my views.In the abstract, it's fine to say \"society is very important, so it's weird if only 1/10 of the items discuss society\". But I don't want to try to give equal time to technical and social issues just for the sake of emphasizing the importance of social factors. If I'm going to add more sentences to a post, I want it to be because the specific claims I'm adding are important, unintuitive, etc. What are the crucial specifics that are missing? \n\n[^wpwfccund9]: Though if we actually lived in that world, we would have already made that observation. A sane world that nimbly adapts its policies in response to large and unusual challenges doesn't wait until the last possible minute to snatch victory from the jaws of defeat; it gets to work on the problem too early, tries to leave itself plenty of buffer, etc.",
      "plaintextDescription": "Dustin Moskovitz writes on Twitter:\n\n> My intuition is that MIRI's argument is almost more about sociology than computer science/security (though there is a relationship). People won't react until it is too late, they won't give up positive rewards to mitigate risk, they won't coordinate, the govt is feckless, etc.\n> \n> And that's a big part of why it seems overconfident to people, bc sociology is not predictable, or at least isn't believed to be.\n\nAnd Stefan Schubert writes:\n\n> I think it's good @robbensinger wrote a list of reasons he expects AGI ruin. It's well-written.\n> \n> But it's notable and symptomatic that 9/10 reasons relate to the nature of AI systems and only 1/10 (discussed in less detail) to the societal response.\n\n> https://www.lesswrong.com/posts/eaDCgdkbsfGqpWazi/the-basic-reasons-i-expect-agi-ruin \n> \n> Whatever one thinks the societal response will be, it seems like a key determinant of whether there'll be AGI ruin.\n> \n> Imo the debate on whether AGI will lead to ruin systematically underemphasises this factor, focusing on technical issues.\n> \n> It's useful to distinguish between warnings and all-things-considered predictions in this regard.\n> \n> When issuing warnings, it makes sense to focus on the technology itself. Warnings aim to elicit a societal response, not predict it.\n> \n> https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential \n> \n> But when you actually try to predict what'll happen all-things-considered, you need to take the societal response into account in a big way\n> \n> As such I think Rob's list is better as a list of reasons we ought to take AGI risk seriously, than as a list of reasons it'll lead to ruin\n\nMy reply is:\n\nIt's true that in my \"top ten reasons I expect AGI ruin\" list, only one of the sections is about the social response to AGI risk, and it's a short section. But the section links to some more detailed discussions (and quotes from them in a long footnote):\n\n * Four ",
      "wordCount": 1677
    },
    "tags": [
      {
        "_id": "EdDGrAxYcrXnKkDca",
        "name": "Distillation & Pedagogy",
        "slug": "distillation-and-pedagogy"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "eaDCgdkbsfGqpWazi",
    "title": "The basic reasons I expect AGI ruin",
    "slug": "the-basic-reasons-i-expect-agi-ruin",
    "url": null,
    "baseScore": 189,
    "voteCount": 89,
    "viewCount": null,
    "commentCount": 73,
    "createdAt": null,
    "postedAt": "2023-04-18T03:37:01.496Z",
    "contents": {
      "markdown": "I've been citing [AGI Ruin: A List of Lethalities](https://lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) to explain why the situation with AI looks lethally dangerous to me. But that post is relatively long, and emphasizes specific open technical problems over \"the basics\".\n\nHere are 10 things I'd focus on if I were giving \"the basics\" on why I'm so worried:[^010v6dpfbnpm]\n\n* * *\n\n1\\. **General intelligence is very powerful**, and once we can build it at all, **STEM-capable artificial general intelligence (AGI) is likely to vastly outperform human intelligence immediately (or very quickly)**.\n\nWhen I say \"general intelligence\", I'm usually thinking about \"whatever it is that lets human brains do astrophysics, category theory, etc. even though our brains evolved under literally zero selection pressure to solve astrophysics or category theory problems\".\n\nIt's possible that we should already be thinking of GPT-4 as \"AGI\" on some definitions, so to be clear about the threshold of generality I have in mind, I'll specifically talk about \"STEM-level AGI\", though I expect such systems to be good at non-STEM tasks too.\n\nHuman brains aren't perfectly general, and not all narrow AI systems or animals are equally narrow. (E.g., AlphaZero is more general than AlphaGo.) But it sure is interesting that humans evolved cognitive abilities that unlock *all of these sciences at once*, with zero evolutionary fine-tuning of the brain aimed at equipping us for any of those sciences. Evolution just stumbled into a solution to *other* problems, that happened to generalize to millions of wildly novel tasks.\n\nMore concretely:\n\n*   AlphaGo is a very impressive reasoner, but its hypothesis space is limited to sequences of Go board states rather than sequences of states of the physical universe. Efficiently reasoning about the physical universe requires solving at least some problems that are different in kind from what AlphaGo solves.\n    *   These problems might be solved by the STEM AGI's programmer, and/or solved by the algorithm that finds the AGI in program-space; and some such problems may be solved by the AGI itself in the course of refining its thinking.[^tdsf69euh5g]\n*   Some examples of abilities I expect humans to only automate once we've built STEM-level AGI (if ever):\n    *   The ability to perform open-heart surgery with a high success rate, in a messy non-standardized ordinary surgical environment.\n    *   The ability to match smart human performance in a specific hard science field, across all the scientific work humans do in that field.\n*   In principle, I suspect you could build a narrow system that is good at those tasks while lacking the basic mental machinery required to do par-human reasoning about *all* the hard sciences. In practice, I very strongly expect humans to find ways to build general reasoners to perform those tasks, before we figure out how to build narrow reasoners that can do them. (For the same basic reason evolution stumbled on general intelligence so early in the history of human tech development.)[^juc8lyqym6]\n\nWhen I say \"general intelligence is very powerful\", a lot of what I mean is that *science* is very powerful, and that having all of the sciences at once is a lot more powerful than the sum of each science's impact.[^7w4ozi1e5x5]\n\nAnother large piece of what I mean is that (STEM-level) general intelligence *is a very high-impact sort of thing to automate* because STEM-level AGI is likely to blow human intelligence out of the water immediately, or very soon after its invention.\n\n80,000 Hours gives the (non-representative) example of how AlphaGo and its successors compared to humanity:\n\n> In the span of a year, AI had advanced from being too weak to win a single \\[Go\\] match against the worst human professionals, to being impossible for even the best players in the world to defeat.\n\nI expect general-purpose science AI to blow human science ability out of the water in a similar fashion.\n\nReasons for this include:\n\n*   Empirically, humans aren't near a cognitive ceiling, and even narrow AI often suddenly blows past the human reasoning ability range on the task it's designed for. It would be weird if scientific reasoning [were an exception](https://intelligence.org/2017/10/20/alphago/).\n*   Empirically, human brains are full of cognitive biases and inefficiencies. It's doubly weird if scientific reasoning is an exception even though it's *visibly* a mess with tons of blind spots, inefficiencies, and motivated cognitive processes, and even though there are innumerable historical examples of scientists and mathematicians taking decades to make technically simple advances.\n*   Empirically, human brains are extremely bad at some of the most basic cognitive processes underlying STEM.\n    *   E.g., consider the stark limits on human working memory and ability to do basic mental math. We can barely multiply smallish multi-digit numbers together in our head, when in principle a reasoner could hold thousands of complex mathematical structures in its working memory simultaneously and perform complex operations on them. Consider the sorts of technologies and scientific insights that might only ever occur to a reasoner if it can directly see (within its own head, in real time) the connections between hundreds or thousands of different formal structures.\n*   Human brains underwent no direct optimization for STEM ability in our ancestral environment, beyond traits like \"I can distinguish four objects in my visual field from five objects\".[^tqs5rz8srtt]\n*   In contrast, human engineers can deliberately optimize AGI systems' brains for math, engineering, etc. capabilities; and human engineers have an enormous variety of tools available to build general intelligence that evolution lacked.[^fukm2mro9]\n*   Software (unlike human intelligence) scales with more compute.\n*   Current ML uses far more compute to *find* reasoners than to *run* reasoners. This is very likely to hold true for AGI as well.\n*   We probably have more than enough compute already, if we knew how to train AGI systems in a remotely efficient way.\n\nAnd on a meta level: the hypothesis that STEM AGI can quickly outperform humans has a disjunctive character. There are [many different advantages](https://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/) that individually suffice for this, even if STEM AGI doesn't start off with any other advantages. (E.g., speed, math ability, scalability with hardware, skill at optimizing hardware...)\n\nIn contrast, the claim that STEM AGI will [hit the narrow target](https://twitter.com/robbensinger/status/1623835453110775810) of \"par-human scientific ability\", *and* stay at around that level for long enough to let humanity adapt and adjust, has a conjunctive character.[^wqd7amqv33m]\n\n2\\. A common misconception is that STEM-level AGI is dangerous because of something murky about \"agents\" or about self-awareness. Instead, I'd say that **the danger is inherent to the nature of action sequences** that push the world toward some sufficiently-hard-to-reach state.[^akn8ouae43a]\n\nCall such sequences \"plans\".\n\nIf you sampled a random plan from the space of all writable plans (weighted by length, in any extant formal language), and all we knew about the plan is that executing it would successfully achieve some superhumanly ambitious technological goal like \"invent fast-running [whole-brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation)\", then hitting a button to execute the plan would kill all humans, with very high probability. This is because:\n\n*   \"Invent fast WBE\" is a hard enough task that succeeding in it usually requires gaining a lot of knowledge and cognitive and technological capabilities, enough to do lots of other dangerous things.\n*   \"Invent fast WBE\" is likelier to succeed if the plan also includes steps that gather and control as many resources as possible, eliminate potential threats, etc. These are \"[convergent instrumental strategies](https://arbital.com/p/instrumental_convergence/)\"—strategies that are useful for pushing the world in a particular direction, almost regardless of which direction you're pushing.\n*   Human bodies and the food, water, air, sunlight, etc. we need to live are resources (\"you are made of atoms the AI can use for something else\"); and we're also potential threats (e.g., we could build a rival superintelligent AI that executes a totally different plan).\n\nThe danger is in the cognitive work, not in some complicated or emergent feature of the \"agent\"; it's in the task itself.\n\nIt isn't that the abstract space of plans was built by evil human-hating minds; it's that the [instrumental convergence](https://nickbostrom.com/superintelligentwill.pdf) thesis holds for the plans themselves. In full generality, plans that succeed in goals like \"build WBE\" tend to be dangerous.\n\nThis isn't true of all plans that successfully push our world into a specific (sufficiently-hard-to-reach) physical state, but it's true of the vast majority of them.\n\nThis is counter-intuitive because most of the impressive \"plans\" we encounter today are generated by humans, and it’s tempting to view strong plans through a human lens. But humans have hugely overlapping values, thinking styles, and capabilities; AI is drawn from new distributions.\n\n3\\. Current ML work is on track to produce things that are, in the ways that matter, **more like \"randomly sampled plans\" than like \"the sorts of plans a civilization of human von Neumanns would produce\"**. (Before we're anywhere near being able to produce the latter sorts of things.)[^4geieijrphn]\n\nWe're building \"AI\" in the sense of building powerful general search processes (and search processes for search processes), not building \"AI\" in the sense of building friendly ~humans but in silicon.\n\n(Note that \"we're going to build systems that are more like A Randomly Sampled Plan than like A Civilization of Human Von Neumanns\" doesn't imply that the plan we'll get is the one we wanted! There are two separate problems: that current ML finds things-that-act-like-they're-optimizing-the-task-you-wanted rather than things-that-actually-internally-optimize-the-task-you-wanted, and also that internally ~maximizing most superficially desirable ends will kill humanity.)\n\nNote that the same problem holds for systems trained to imitate humans, if those systems scale to being able to do things like \"build whole-brain emulation\". \"We're training on something related to humans\" doesn't give us \"we're training things that are best thought of as humans plus noise\".\n\nIt's not obvious to me that GPT-like systems can scale to capabilities like \"build WBE\". But if they do, we face the [problem](https://twitter.com/ESYudkowsky/status/1628835920328929281) that most ways of successfully imitating humans don't look like \"build a human (that's somehow superhumanly good at imitating the Internet)\". They look like \"build a relatively complex and alien optimization process that is good at imitation tasks (and potentially at many other tasks)\".\n\nYou don't need to be a human in order to model humans, any more than you need to be a cloud in order to model clouds well. The only reason this is more confusing in the case of \"predict humans\" than in the case of \"predict weather patterns\" is that humans and AI systems are both intelligences, so it's easier to slide between \"the AI models humans\" and \"the AI is basically a human\".\n\n4\\. The key differences between humans and \"things that are more easily approximated as random search processes than as humans-plus-a-bit-of-noise\" **lies in lots of complicated machinery in the human brain**.\n\n(Cf. [Detached Lever Fallacy](https://www.lesswrong.com/posts/zY4pic7cwQpa9dnyk/detached-lever-fallacy), [Niceness Is Unnatural](https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural), and [Superintelligent AI Is Necessary For An Amazing Future, But Far From Sufficient](https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1).)\n\nHumans are not blank slates in the relevant ways, such that just raising an AI like a human solves the problem.\n\nThis doesn't mean the problem is unsolvable; but it means that you either need to reproduce that internal machinery, in a lot of detail, in AI, or you need to build some new kind of machinery that’s safe for reasons other than the specific reasons humans are safe.\n\n(You need cognitive machinery that somehow samples from a much narrower space of plans that are still powerful enough to succeed in at least one task that saves the world, but are constrained in ways that make them far less dangerous than the larger space of plans. And you need a thing that *actually* implements internal machinery like that, as opposed to just being optimized to superficially behave as though it does in the narrow and unrepresentative environments it was in before starting to work on WBE. \"Novel science work\" means that pretty much everything you want from the AI is out-of-distribution.)\n\n5\\. **STEM-level AGI timelines don't look that long** (e.g., probably not 50 or 150 years; could well be 5 years or 15).\n\nI won't try to argue for this proposition, beyond pointing at the field's recent [progress](https://twitter.com/leopoldasch/status/1638848850516672513) and echoing [Nate Soares' comments](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential#Timelines) from early 2021:\n\n> \\[...\\] I observe that, 15 years ago, everyone was saying AGI is far off because of what it couldn't do -- basic image recognition, go, starcraft, winograd schemas, simple programming tasks. But basically all that has fallen. The gap between us and AGI is made mostly of intangibles. (Computer programming that is Actually Good? Theorem proving? Sure, but on my model, \"good\" versions of those are a hair's breadth away from full AGI already. And the fact that I need to clarify that \"bad\" versions don't count, speaks to my point that the only barriers people can name right now are intangibles.) That's a very uncomfortable place to be!\n> \n> \\[...\\] I suspect that I'm in more-or-less the \"penultimate epistemic state\" on AGI timelines: I don't know of a project that seems like they're right on the brink; that would put me in the \"final epistemic state\" of thinking AGI is imminent. But I'm in the second-to-last epistemic state, where I wouldn't feel all that shocked to learn that some group has reached the brink. Maybe I won't get that call for 10 years! Or 20! But it could also be 2, and I wouldn't get to be indignant with reality. I wouldn't get to say \"but all the following things should have happened first, before I made that observation!\". Those things have happened. I have made those observations. \\[...\\]\n\nI think timing tech is [very difficult](https://intelligence.org/2017/10/13/fire-alarm/) (and plausibly ~impossible when the tech isn't pretty imminent), and I think reasonable people can disagree a lot about timelines.\n\nI also think converging on timelines is not very crucial, since if AGI is 50 years away I would say it's still the largest single risk we face, and the bare minimum alignment work required for surviving that transition could easily take longer than that.\n\nAlso, \"STEM AGI when?\" is the kind of argument that requires hashing out people's predictions about how we get to STEM AGI, which is a bad thing to debate publicly insofar as improving people's models of pathways can further shorten timelines.\n\nI mention timelines anyway because they are in fact a major reason I'm pessimistic about our prospects; if I learned tomorrow that AGI were 200 years away, I'd be outright optimistic about things going well.\n\n6\\. **We don't currently know how to do alignment**, we don't seem to have a much better idea now than we did 10 years ago, and there are many large novel visible difficulties. (See [AGI Ruin](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) and the [Capabilities Generalization, and the Sharp Left Turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization).)\n\nOn a more basic level, quoting [Nate Soares](https://intelligence.org/ensuring): \"Why do I think that AI alignment looks fairly difficult? The main reason is just that this has been my experience from actually working on these problems.\"\n\n7\\. We should be starting with a pessimistic prior about **achieving reliably good behavior in any complex safety-critical software, particularly if the software is novel**. Even more so if the thing we need to make robust is structured like undocumented spaghetti code, and more so still if the field is highly competitive and you need to achieve some robustness property *while moving faster* than a large pool of less-safety-conscious people who are racing toward the precipice.\n\nThe [default](https://www.lesswrong.com/posts/Ke2ogqSEhL2KCJCNx/security-mindset-lessons-from-20-years-of-software-security) assumption is that complex software goes wrong in dozens of different ways you didn't expect. Reality ends up being thorny and inconvenient in many of the places where your models were absent or fuzzy. Surprises are abundant, and some surprises can be good, but this is empirically a lot rarer than unpleasant surprises in software development hell.\n\nThe future is hard to predict, but plans systematically take longer and run into more snags than humans naively expect, as opposed to plans systematically going surprisingly smoothly and deadlines being systematically hit ahead of schedule.\n\nThe history of computer security and of safety-critical software systems is almost invariably one of robust software lagging far, far behind non-robust versions of the same software. Achieving any robustness property in complex software that will be deployed in the real world, with all its messiness and adversarial optimization, is very difficult and usually fails.\n\nIn many ways I think the foundational discussion of AGI risk is [Security Mindset and Ordinary Paranoia](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) and [Security Mindset and the Logistic Success Curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/), and the main body of the text doesn't even mention AGI. Adding in the specifics of AGI and smarter-than-human AI takes the risk from \"dire\" to \"seemingly overwhelming\", but adding in those specifics is not required to be massively concerned if you think getting this software right matters for our future.\n\n8\\. **Neither ML nor the larger world is currently taking this seriously**, as of April 2023.\n\nThis is obviously something we can change. But until it's changed, things will continue to look very bad.\n\nAdditionally, most of the people who are taking AI risk somewhat seriously are, to an important extent, not willing to worry about things until after they've been experimentally proven to be dangerous. Which is a lethal sort of methodology to adopt when you're working with smarter-than-human AI.\n\nMy basic picture of *why* the world currently isn't responding appropriately is the one in [Four mindset disagreements behind existential risk disagreements in ML](https://www.lesswrong.com/posts/84BJopKvQ8qYGHY6b/four-mindset-disagreements-behind-existential-risk), [The inordinately slow spread of good AGI conversations in ML](https://www.lesswrong.com/posts/Rkxj7TFxhbm59AKJh/the-inordinately-slow-spread-of-good-agi-conversations-in-ml), and [*Inadequate Equilibria*](https://equilibriabook.com/toc).[^fm0m5ssc91]\n\n9\\. As noted above, **current ML is very opaque**, and **it mostly lets you intervene on behavioral proxies** for what we want, rather than letting us directly design desirable features.\n\nML as it exists today also requires that data is readily available and safe to provide. E.g., we can’t robustly train the AGI on \"don’t kill people\" because we can’t provide real examples of it killing people to train against the behavior we don't want; we can only give flawed proxies and work via indirection.\n\n10\\. **There are lots of specific abilities** which seem like they ought to be possible for the kind of civilization that can safely deploy smarter-than-human optimization, that are far out of reach, with no obvious path forward for achieving them with opaque deep nets even if we had unlimited time to work on some relatively concrete set of research directions.\n\n(Unlimited time suffices if we can set a more abstract/indirect research direction, like \"just think about the problem for a long time until you find some solution\". There are presumably paths forward; we just don’t know what they are today, which puts us in a worse situation.)\n\nE.g., we don’t know how to go about inspecting a nanotech-developing AI system’s brain to verify that it’s only thinking about a specific room, that it’s internally representing the intended goal, that it’s directing its optimization at that representation, that it internally has a particular planning horizon and a variety of capability bounds, that it’s unable to think about optimizers (or specifically about humans), or that it otherwise has the right topics internally whitelisted or blacklisted.\n\nIndividually, it seems to me that each of these difficulties can be addressed. In combination, they seem to me to put us in a very dark situation.\n\n* * *\n\nOne common response I hear to points like the above is:\n\n> The future is generically hard to predict, so it's just not possible to be rationally confident that things will go well or poorly. Even if you look at dozens of different arguments and framings and the ones that hold up to scrutiny nearly all seem to point in the same direction, it's always possible that you're making some invisible error of reasoning that causes correlated failures in many places at once.\n\nI'm sympathetic to this because I agree that the future is hard to predict.\n\nI'm not totally confident things will go poorly; if I were, I wouldn't be trying to solve the problem! I think things are looking extremely dire, but not hopeless.\n\nThat said, some people think that even \"extremely dire\" is an impossible belief state to be in, in advance of an AI apocalypse actually occurring. I disagree here, for two basic reasons:\n\na. There are many details we can get into, but on a core level **I don't think the risk is particularly complicated or hard to reason about**. The core concern fits into a [tweet](https://twitter.com/robbensinger/status/1644006731792646145):\n\n> STEM AI is likely to vastly exceed human STEM abilities, conferring a decisive advantage. We aren't on track to knowing how to aim STEM AI at intended goals, and STEM AIs pursuing unintended goals tend to have instrumental subgoals like \"control all resources\".\n\nZvi Mowshowitz [puts the core concern](https://thezvi.substack.com/p/response-to-tyler-cowens-existential) in even more basic terms:\n\n> I also notice a kind of presumption that things in most scenarios will work out and that doom is dependent on particular ‘distant possibilities,’ that often have many logical dependencies or require a lot of things to individually go as predicted. Whereas I would say that those possibilities are not so distant or unlikely, but more importantly that the result is robust, that once the intelligence and optimization pressure that matters is no longer human that most of the outcomes are existentially bad by my values and that one can reject or ignore many or most of the detail assumptions and still see this.\n\nThe details do matter for evaluating the exact risk level, but this isn't the sort of topic where it seems fundamentally impossible for any human to reach a good understanding of the core difficulties and whether we're handling them.\n\nb. Relatedly, as Nate Soares has argued, **AI disaster scenarios are** [**disjunctive**](https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive). There are many bad outcomes for every good outcome, and many paths leading to disaster for every path leading to utopia.\n\nQuoting [Eliezer Yudkowsky](https://twitter.com/ESYudkowsky/status/1644045728983969816):\n\n> You don't get to adopt a prior where you have a 50-50 chance of winning the lottery \"because either you win or you don't\"; the question is not whether we're uncertain, but whether someone's allowed to milk their uncertainty to expect good outcomes.\n\nQuoting [Jack Rabuck](https://twitter.com/JackRabuck/status/1644762977504227329):\n\n> I listened to the whole 4 hour Lunar Society interview with @ESYudkowsky  \n> (hosted by @dwarkesh_sp) that was mostly about AI alignment and I think I identified a point of confusion/disagreement that is pretty common in the area and is rarely fleshed out:\n> \n> Dwarkesh repeatedly referred to the conclusion that AI is likely to kill humanity as \"wild.\"\n> \n> Wild seems to me to pack two concepts together, 'bad' and 'complex.' And when I say complex, I mean in the sense of the Fermi equation where you have an end point (dead humanity) that relies on a series of links in a chain and if you break any of those links, the end state doesn't occur.\n> \n> It seems to me that Eliezer believes this end state is not wild (at least not in the complex sense), but very simple. He thinks many (most) paths converge to this end state.\n> \n> That leads to a misunderstanding of sorts. Dwarkesh pushes Eliezer to give some predictions based on the line of reasoning that he uses to predict that end point, but since the end point is very simple and is a convergence, Eliezer correctly says that being able to reason to that end point does not give any predictive power about the particular path that will be taken in this universe to reach that end point.\n> \n> Dwarkesh is thinking about the end of humanity as a causal chain with many links and if any of them are broken it means humans will continue on, while Eliezer thinks of the continuity of humanity (in the face of AGI) as a causal chain with many links and if any of them are broken it means humanity ends. Or perhaps more discretely, Eliezer thinks there are a few very hard things which humanity could do to continue in the face of AI, and absent one of those occurring, the end is a matter of when, not if, and the when is much closer than most other people think.\n> \n> Anyway, I think each of Dwarkesh and Eliezer believe the other one falls on the side of extraordinary claims require extraordinary evidence - Dwarkesh thinking the end of humanity is \"wild\" and Eliezer believing humanity's viability in the face of AGI is \"wild\" (though not in the negative sense). \n\nI don't consider \"AGI ruin is disjunctive\" a knock-down argument for high p(doom) on its own. NASA has a high success rate for rocket launches even though success requires many things to go right simultaneously. Humanity is capable of achieving conjunctive outcomes, to some degree; but I think this framing makes it clearer why it's *possible* to rationally arrive at a high p(doom), at all, when enough evidence points in that direction.[^dyzc06bcslo]\n\n[^010v6dpfbnpm]: Eliezer Yudkowsky's So Far: Unfriendly AI Edition and Nate Soares' Ensuring Smarter-Than-Human Intelligence Has a Positive Outcome are two other good (though old) introductions to what I'd consider \"the basics\".To state the obvious: this post consists of various claims that increase my probability on AI causing an existential catastrophe, but not all the claims have to be true in order for AI to have a high probability of causing such a catastrophe.Also, I wrote this post to summarize my own top reasons for being worried, not to try to make a maximally compelling or digestible case for others. I don't expect others to be similarly confident based on such a quick overview, unless perhaps you've read other sources on AI risk in the past. (Including more optimistic ones, since it's harder to be confident when you've only heard from one side of a disagreement. I've written in the past about some of the things that give me small glimmers of hope, but people who are overall far more hopeful will have very different reasons for hope, based on very different heuristics and background models.) \n\n[^tdsf69euh5g]: E.g., the physical world is too complex to simulate in full detail, unlike a Go board state. An effective general intelligence needs to be able to model the world at many different levels of granularity, and strategically choose which levels are relevant to think about, as well as which specific pieces/aspects/properties of the world at those levels are relevant to think about.More generally, being a general intelligence requires an enormous amount of laserlike focus and strategicness when it comes to which thoughts you do or don't think. A large portion of your compute needs to be relentlessly funneled into exactly the tiny subset of questions about the physical world that bear on the question you're trying to answer or the problem you're trying to solve. If you fail to be relentlessly targeted and efficient in \"aiming\" your cognition at the most useful-to-you things, you can easily spend a lifetime getting sidetracked by minutiae, directing your attention at the wrong considerations, etc.And given the variety of kinds of problems you need to solve in order to navigate the physical world well, do science, etc., the heuristics you use to funnel your compute to the exact right things need to themselves be very general, rather than all being case-specific.(Whereas we can more readily imagine that many of the heuristics AlphaGo uses to avoid thinking about the wrong aspects of the game state (or getting otherwise sidetracked) are Go-specific heuristics.) \n\n[^juc8lyqym6]: Of course, if your brain has all the basic mental machinery required to do other sciences, that doesn't mean that you have the knowledge required to actually do well in those sciences. An STEM-level artificial general intelligence could lack physics ability for the same reason many smart humans can't solve physics problems. \n\n[^7w4ozi1e5x5]: E.g., because different sciences can synergize, and because you can invent new scientific fields and subfields, and more generally chain one novel insight into dozens of other new insights that critically depended on the first insight. \n\n[^tqs5rz8srtt]: More generally, the sciences (and many other aspects of human life, like written language) are a very recent development on evolutionary timescales. So evolution has had very little time to refine and improve on our reasoning ability in many of the ways that matter. \n\n[^fukm2mro9]: \"Human engineers have an enormous variety of tools available that evolution lacked\" is often noted as a reason to think that we may be able to align AGI to our goals, even though evolution failed to align humans to its \"goal\". It's additionally a reason to expect AGI to have greater cognitive ability, if engineers try to achieve great cognitive ability. \n\n[^wqd7amqv33m]: And my understanding is that, e.g., Paul Christiano's soft-takeoff scenarios don't involve there being much time between par-human scientific ability and superintelligence. Rather, he's betting that we have a bunch of decades between GPT-4 and par-human STEM AGI. \n\n[^akn8ouae43a]: I'll classify thoughts and text outputs as \"actions\" too, not just physical movements. \n\n[^4geieijrphn]: Obviously, neither is a particularly good approximation for ML systems. The point is that our optimism about plans in real life generally comes from the fact that they're weak, and/or it comes from the fact that the plan generators are human brains with the full suite of human psychological universals. ML systems don't possess those human universals, and won't stay weak indefinitely. \n\n[^fm0m5ssc91]: Quoting Four mindset disagreements behind existential risk disagreements in ML:People are taking the risks unseriously because they feel weird and abstract.When they do think about the risks, they anchor to what's familiar and known, dismissing other considerations because they feel \"unconservative\" from a forecasting perspective.Meanwhile, social mimesis and the bystander effect make the field sluggish at pivoting in response to new arguments and smoke under the door.Quoting The inordinately slow spread of good AGI conversations in ML:Info about AGI propagates too slowly through the field, because when one ML person updates, they usually don't loudly share their update with all their peers. This is because:1. AGI sounds weird, and they don't want to sound like a weird outsider.2. Their peers and the community as a whole might perceive this information as an attack on the field, an attempt to lower its status, etc.3. Tech forecasting, differential technological development, long-term steering, exploratory engineering, 'not doing certain research because of its long-term social impact', prosocial research closure, etc. are very novel and foreign to most scientists.EAs exert effort to try to dig up precedents like Asilomar partly because Asilomar is so unusual compared to the norms and practices of the vast majority of science. Scientists generally don't think in these terms at all, especially in advance of any major disasters their field causes.And the scientists who do find any of this intuitive often feel vaguely nervous, alone, and adrift when they talk about it. On a gut level, they see that they have no institutional home and no super-widely-shared 'this is a virtuous and respectable way to do science' narrative.Normal science is not Bayesian, is not agentic, is not 'a place where you're supposed to do arbitrary things just because you heard an argument that makes sense'. Normal science is a specific collection of scripts, customs, and established protocols.In trying to move the field toward 'doing the thing that just makes sense', even though it's about a weird topic (AGI), and even though the prescribed response is also weird (closure, differential tech development, etc.), and even though the arguments in support are weird (where's the experimental data??), we're inherently fighting our way upstream, against the current.Success is possible, but way, way more dakka is needed, and IMO it's easy to understand why we haven't succeeded more.This is also part of why I've increasingly updated toward a strategy of \"let's all be way too blunt and candid about our AGI-related thoughts\".The core problem we face isn't 'people informedly disagree', 'there's a values conflict', 'we haven't written up the arguments', 'nobody has seen the arguments', or even 'self-deception' or 'self-serving bias'.The core problem we face is 'not enough information is transmitting fast enough, because people feel nervous about whether their private thoughts are in the Overton window'.On the more basic level, Inadequate Equilibria paints a picture of the world's baseline civilizational competence that I think makes it less mysterious why we could screw up this badly on a novel problem that our scientific and political institutions weren't designed to address. Inadequate Equilibria also talks about the nuts and bolts of Modest Epistemology, which I think is a key part of the failure story. \n\n[^dyzc06bcslo]: Quoting a recent conversation between Aryeh Englander and Eliezer Yudkowsky:Aryeh: [...] Yet I still have a very hard time understanding the arguments that would lead to such a high-confidence prediction. Like, I think I understand the main arguments for AI existential risk, but I just don't understand why some people seem so sure of the risks. [...]Eliezer: I think the core thing is the sense that you cannot in this case milk uncertainty for a chance of good outcomes; to get to a good outcome you'd have to actually know where you're steering, like trying to buy a winning lottery ticket or launching a Moon rocket. Once you realize that uncertainty doesn't move estimates back toward \"50-50, either we live happily ever after or not\", you realize that \"people in the EA forums cannot tell whether Eliezer or Paul is right\" is not a factor that moves us toward 1:1 good:bad but rather another sign of doom; surviving worlds don't look confused like that and are able to make faster progress.Not as a fully valid argument from which one cannot update further, but as an intuition pump: the more all arguments about the future seem fallible, the more you should expect the future Solar System to have a randomized configuration from your own perspective. Almost zero of those have humans in them. It takes confidence about some argument constraining the future to get to more than that.Aryeh: when you talk about uncertainty here do you mean uncertain factors within your basic world model, or are you also counting model uncertainty? I can see how within your world model extra sources of uncertainty don't point to lower risk estimates. But my general question I think is more about model uncertainty: how sure can you really be that your world model and reference classes and framework for thinking about this is the right one vs e.g., Robin or Paul or Rohin or lots of others? And in terms of model uncertainty it looks like most of these other approaches imply much lower risk estimates, so adding in that kind of model uncertainty should presumably (I think) point to overall lower risk estimates.Eliezer: Aryeh, if you've got a specific theory that says your rocket design is going to explode, and then you're also very unsure of how rockets work really, what probability should you assess of your rocket landing safely on target?Aryeh: how about if you have a specific theory that says you should be comparing what you're doing to a rocket aiming for the moon but it'll explode, and then a bunch of other theories saying it won't explode, plus a bunch of theories saying you shouldn't be comparing what you're doing to a rocket in the first place? My understanding of many alignment proposals is that they think we do understand \"rockets\" sufficiently so that we can aim them, but they disagree on various specifics that lead you to have such high confidence in an explosion. And then there are others like Robin Hanson who use mostly outside-type arguments to argue that you're framing the issues incorrectly, and we shouldn't be comparing this to \"rockets\" at all because that's the wrong reference class to use. So yes, accounting for some types of model uncertainty won't reduce our risk assessments and may even raise them further, but other types of model uncertainty - including many of the actual alternative models / framings at least as I understand them - should presumably decrease our risk assessment.Eliezer: What if people are trying to build a flying machine for the first time, and there's a whole host of them with wildly different theories about why it ought to fly easily, and you think there's basic obstacles to stable flight that they're not getting? Could you force the machine to fly despite all obstacles by recruiting more and more optimists to have different theories, each of whom would have some chance of being right?Aryeh: right, my point is that in order to have near certainty of not flying you need to be very very sure that your model is right and theirs isn't. Or in other words, you need to have very low model uncertainty. But once you add in model uncertainty where you consider that maybe those other optimists' models could be right, then your risk estimates will go down. Of course you can't arbitrarily add in random optimistic models from random people - it needs to be weighted in some way. My confusion here is that you seem to be very, very certain that your model is the right one, complete with all its pieces and sub-arguments and the particular reference classes you use, and I just don't quite understand why.Eliezer: There's a big difference between \"sure your model is the right one\" and the whole thing with people wandering over with their own models and somebody else going, \"I can't tell the difference between you and them, how can you possibly be so sure they're not right?\"The intuition I'm trying to gesture at here is that you can't milk success out of uncertainty, even by having a bunch of other people wander over with optimistic models. It shouldn't be able to work in real life. If your epistemology says that you can generate free success probability that way, you must be doing something wrong.Or maybe another way to put it: When you run into a very difficult problem that you can see is very difficult, but inevitably a bunch of people with less clear sight wander over and are optimistic about it because they don't see the problems, for you to update on the optimists would be to update on something that happens inevitably. So to adopt this policy is just to make it impossible for yourself to ever perceive when things have gotten really bad.Aryeh: not sure I fully understand what you're saying. It looks to me like to some degree what you're saying boils down to your views on modest epistemology - i.e., basically just go with your own views and don't defer to anybody else. It sounds like you're saying not only don't defer, but don't even really incorporate any significant model uncertainty based on other people's views. Am I understanding this at all correctly or am I totally off here?Eliezer: My epistemology is such that it's possible in principle for me to notice that I'm doomed, in worlds which look very doomed, despite the fact that all such possible worlds no matter how doomed they actually are, always contain a chorus of people claiming we're not doomed.(See Inadequate Equilibria for a detailed discussion of Modest Epistemology, deference, and \"outside views\", and Strong Evidence Is Common for the basic first-order case that people can often reach confident conclusions about things.)",
      "plaintextDescription": "I've been citing AGI Ruin: A List of Lethalities to explain why the situation with AI looks lethally dangerous to me. But that post is relatively long, and emphasizes specific open technical problems over \"the basics\".\n\nHere are 10 things I'd focus on if I were giving \"the basics\" on why I'm so worried:[1]\n\n----------------------------------------\n\n1. General intelligence is very powerful, and once we can build it at all, STEM-capable artificial general intelligence (AGI) is likely to vastly outperform human intelligence immediately (or very quickly).\n\nWhen I say \"general intelligence\", I'm usually thinking about \"whatever it is that lets human brains do astrophysics, category theory, etc. even though our brains evolved under literally zero selection pressure to solve astrophysics or category theory problems\".\n\nIt's possible that we should already be thinking of GPT-4 as \"AGI\" on some definitions, so to be clear about the threshold of generality I have in mind, I'll specifically talk about \"STEM-level AGI\", though I expect such systems to be good at non-STEM tasks too.\n\nHuman brains aren't perfectly general, and not all narrow AI systems or animals are equally narrow. (E.g., AlphaZero is more general than AlphaGo.) But it sure is interesting that humans evolved cognitive abilities that unlock all of these sciences at once, with zero evolutionary fine-tuning of the brain aimed at equipping us for any of those sciences. Evolution just stumbled into a solution to other problems, that happened to generalize to millions of wildly novel tasks.\n\nMore concretely:\n\n * AlphaGo is a very impressive reasoner, but its hypothesis space is limited to sequences of Go board states rather than sequences of states of the physical universe. Efficiently reasoning about the physical universe requires solving at least some problems that are different in kind from what AlphaGo solves.\n   * These problems might be solved by the STEM AGI's programmer, and/or solved by the algorithm that find",
      "wordCount": 4122
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "JX69nZB8tfxnx5nGH",
        "name": "Threat Models (AI)",
        "slug": "threat-models-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "84BJopKvQ8qYGHY6b",
    "title": "Four mindset disagreements behind existential risk disagreements in ML",
    "slug": "four-mindset-disagreements-behind-existential-risk",
    "url": null,
    "baseScore": 137,
    "voteCount": 66,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2023-04-11T04:53:48.427Z",
    "contents": {
      "markdown": "I've previously written that good ideas and conversations about AGI seem to have propagated through ML [weirdly slowly](https://www.lesswrong.com/posts/Rkxj7TFxhbm59AKJh/the-inordinately-slow-spread-of-good-agi-conversations-in-ml).\n\nA different weird phenomenon I observe is that the field's relative inaction about AGI seems less based on a confident set of beliefs about AGI definitely going well (or definitely being far off), and more based on an implicit sense like \"the default is everything going well, and we don't need to change anything until there's overwhelming evidence to the contrary\".\n\nSome people *do* have confident beliefs that imply \"things will go well\"; I disagree there, but I expect some amount of disagreement like that.\n\nBut that doesn't seem to be the [crux](https://www.lesswrong.com/tag/crux) for most people in ML.\n\nIn a sane world, it doesn't seem like \"well, maybe AI will get stuck at human-ish levels for decades\" or \"well, maybe superintelligence couldn't invent any wild new tech\" ought to be cruxes for \"Should we [pause](https://forum.effectivealtruism.org/posts/Cs5Zx3ncLWcoYKSog/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1) AI development?\" or \"Is alignment research the world's top priority?\"\n\nNote that I'm *not* arguing \"an AGI-mediated extinction event is such a big deal that we should make it a top priority even if it's very unlikely\". There are enough other powerful technologies on the horizon, and enough other risks for civilizational collapse or value lock-in, that I don't in fact think AGI x-risk should get major attention if it's very unlikely.\n\nBut the most common view within ML seems to be less \"it's super unlikely for reasons X Y Z\", and more of an \"I haven't thought about it much\" and/or \"I see some reasons to be very worried, but also some reasons things might be fine, so I end up with medium-ish levels of worry\".\n\nIn a mid-2022 survey, [48% of researchers](https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/) who had recently published in NeurIPS or ICML gave double-digit probabilities to advanced AI's long-term effect being “extremely bad (e.g., human extinction)”. A similar number gave double-digit probabilities to \"human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowerment of the human species\".\n\nIn an early 2021 survey, [91% of researchers](https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results) working on \"long-term AI topics\" at CHAI, DeepMind, MIRI, OpenAI, Open Philanthropy, and what would become Anthropic gave double-digit probabilities to \"the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended\".\n\nThe level of concern and seriousness I see from ML researchers discussing AGI on any social media platform or in any mainstream venue seems *wildly* out of step with \"half of us think there's a 10+% chance of our work resulting in an existential catastrophe\".\n\nI think the following four factors help partly (though not completely) explain what's going on. If I'm right, then I think there's some hope that the field can explicitly talk about these things and consciously course-correct.\n\n1.  **\"Conservative\" predictions**, versus conservative decision-making.\n2.  **Waiting for a fire alarm**, versus intervening proactively.\n3.  **Anchoring to what's familiar**, versus trying to account for potential novelties in AGI.\n4.  **Modeling existential risks in far mode**, versus near mode.\n\n### 1\\. \"Conservative\" predictions, versus conservative decision-making\n\nIf you're building toward a technology as novel and powerful as \"automating every cognitive ability a human can do\", then it may sound \"conservative\" to predict modest impacts. But at the decision-making level, you should be \"conservative\" in a very different sense, by not gambling the future on your technology being low-impact.\n\nThe first long-form discussion of AI alignment, Eliezer Yudkowsky's [*Creating Friendly AI 1.0*](https://intelligence.org/files/CFAI.pdf), made this point in 2001:\n\n> The conservative assumption according to futurism is not necessarily the “conservative” assumption in Friendly AI. Often, the two are diametric opposites. When building a toll bridge, the conservative *revenue* assumption is that half as many people will drive through as expected. The conservative *engineering* assumption is that ten times as many people as expected will drive over, and that most of them will be driving fifteen-ton trucks.\n> \n> Given a choice between discussing a human-dependent traffic-control AI and discussing an AI with independent strong nanotechnology, we should be biased towards assuming the more powerful and independent AI. An AI that remains Friendly when armed with strong nanotechnology is likely to be Friendly if placed in charge of traffic control, but perhaps not the other way around. (A minivan can drive over a bridge designed for armor-plated tanks, but not vice-versa.)\n> \n> In addition to engineering conservatism, the nonconservative futurological scenarios are played for much higher stakes. A strong-nanotechnology AI has the power to affect billions of lives and humanity’s entire future. A traffic-control AI is being entrusted “only” with the lives of a few million drivers and pedestrians.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JQxvZZdPG5KYjyBfg/jsgehvjzcvg25h3trfrg)\n\nPeople who think their role is only to be a \"conservative predictor\", and not a \"conservative decision-maker\", will skew the scholarly conversation toward taking more extreme risks, because acknowledging extreme things sounds too out-there to them.\n\nI personally wouldn't even call the predictions here \"conservative\", since this conflates \"sounds normal\" with \"robust to uncertainty\". All consistent object-level views about AI and technological progress have at least one \"wild\" implication (as noted in Holden Karnofsky's [*The Most Important Century*](https://cold-takes.com/most-important-century/)), so views that sound normal here generally have to use misdirection and vagueness to obscure the wild part.\n\nThe [availability heuristic](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) and [absurdity bias](https://www.lesswrong.com/posts/P792Z4QA9dzcLdKkE/absurdity-heuristic-absurdity-bias) cause us to neglect big changes until it's too late.\n\nMy own view is that extreme disaster scenarios are very likely, not just a tail risk to hedge against. I *actually expect* AGI systems to achieve Drexler-style nanotechnology within anywhere from a few months to a few years of reaching human-level-or-better ability to do science and engineering work. At this point, I'm looking for [any hope of us surviving at all](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), not holding out hope for a \"conservative\" scheme (sane as that would be).\n\nBut the point stands that if you have more \"medium-sized\" probabilities on those capabilities being available (as opposed to very high or very low ones), then a sane response to AGI should explicitly grapple with that, not pretend the probability is negligible because it's scary.\n\nI do think debates between the \"risk is extremely high\" camp and the \"risk is medium-sized\" camp are important. But the importance mostly stems from \"this suggests we have different background models, and should try to draw those out so they can be discussed explicitly\", not \"we should only take action about extreme risks once we're 95+% sure of them\".\n\n### 2\\. Waiting for a fire alarm, versus intervening proactively\n\n[There's No Fire Alarm for Artificial General Intelligence](https://intelligence.org/2017/10/13/fire-alarm/) (written in 2017) makes a few different claims:\n\n1.  Predicting when future technologies will be invented is usually very hard, and may be flatly impossible for humans in the typical case, at least when the technology isn't imminent. AGI is likely no exception.\n2.  There isn't any \"fire alarm\" for AGI, i.e., there's no event that field leaders all know is going to happen well before AGI, such that everyone can coordinate around that event as \"here's the trigger for us to start thinking seriously about AGI risk and alignment\".\n3.  Many people aren't working on AGI alignment today (in 2017) because they're waiting for some clear future social signal that you won't look weird or panicky for thinking about something as science-fictiony as AGI.  But in addition to there being no *known* event like that we can safely wait around for, there probably aren't any *unknown* events like that either. It will probably *still* be socially risky to loudly worry about AGI even on the eve of AGI's invention. If you're waiting for social permission to get involved, you'll likely never get involved at all.\n\nQuoting Yudkowsky:\n\n> When I observe that there’s no fire alarm for AGI, I’m not saying that there’s no possible equivalent of smoke appearing from under a door.\n> \n> What I’m saying rather is that the smoke under the door is always going to be arguable; it is not going to be a clear and undeniable and absolute sign of fire; and so there is never going to be a fire alarm producing common knowledge that action is now due and socially acceptable.\n\nClaims 1 and 2 still seem correct to me. We can hope that 3 is maybe false, and that we're now seeing a shift in the field toward taking AGI seriously, even if this wasn't foreseeable in 2017 and doesn't come with a lot of clarity about timelines.\n\nFor now, however, it still seems to me that the basic dynamics described in the Fire Alarm post are inhibiting action. Things are murky now, and I think there's a common implicit expectation that they'll be less murky later, and that we can safely put off thinking about the problem until some unspecified future date.\n\nThe [bystander effect](https://www.lesswrong.com/posts/K5nq3KcDXaGm7QQWR/bystander-apathy) still seems powerful here. People don't want to be the *first* in a given social context to express alarm, so they default to looking vaguely calm while waiting for someone else to speak up or spring into action first. But everyone else is doing the same thing, so no one ends up acting at all.\n\nThis is a case where unilaterally *acting at all* (in sane and [actually-helpful](https://www.lesswrong.com/posts/aaeMECbcBzC3cweGK/on-saving-one-s-world) ways), speaking up, [blurting](https://forum.effectivealtruism.org/posts/TaBJqDMJtEH68Hhfw/ea-should-blurt) your actual thoughts, etc. can be particularly powerful and important.\n\nIn some cases it may only take one person shattering the [Overton window](https://www.lesswrong.com/posts/bzhGBHrGrFfQss4Df/the-feeling-of-breaking-an-overton-window) in order to open the floodgates for other people who were quietly worried. And even where that's not true, I expect better results from people hashing out their disagreements in argument than from people timidly waiting for the right moment.\n\n### 3\\. Anchoring to what's familiar, versus trying to account for potential novelties in AGI\n\nThe level and nature of the risk from AGI turns on the physical properties of AGI. \"AlphaGo wasn't dangerous\" is evidence for \"AGI won't be dangerous\" only insofar as you think AGI is similar to AlphaGo in the relevant ways.\n\nBut for some reason a lot of people who *wouldn't* go out on a limb and claim that AlphaGo and AGI are actually particularly similar in the ways that matter, do treat AGI like \"just a normal ML system\". Their policy suggests confidence that AGI is in the same reference class as systems like AlphaGo or DALL-E in all the ways that matter, even though they wouldn't ever actually state that as a belief.\n\nThe whole conversation is baked through with a tacit assumption that the difficulty, danger, and importance of AGI alignment needs to be \"just more of the same\", even though AGI itself is a very new sort of beast.\n\nBut \"get a smarter-than-human AI system to produce good outcomes\" is not in fact similar to a problem we've faced before! I think it's a solvable problem in principle, but the difficulty level does not need to be calibrated to business-as-usual efforts.\n\nQuoting [Beyond the Reach of God](https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god):\n\n> Once upon a time, I believed that the extinction of humanity was not allowed.  And others who call themselves rationalists, may yet have things they trust.  They might be called \"positive-sum games\", or \"democracy\", or \"technology\", but they are sacred.  The mark of this sacredness is that the trustworthy thing can't lead to anything *really* bad; or they can't be *permanently* defaced, at least not without a compensatory silver lining.  In that sense they can be trusted, even if a few bad things happen here and there.\n> \n> The unfolding history of Earth can't ever turn from its positive-sum trend to a negative-sum trend; that is not allowed.  [Democracies](http://www.overcomingbias.com/2007/09/applause-lights.html)—*modern* *liberal* democracies, anyway—won't ever legalize torture.  [Technology](http://www.overcomingbias.com/2008/09/raised-in-sf.html) has done so much good up until now, that there can't possibly be a Black Swan technology that breaks the trend and does more harm than all the good up until this point.\n> \n> There are all sorts of clever arguments why such things can't possibly happen.  But the source of these arguments is a much deeper belief that such things are *not allowed*.  Yet who prohibits?  Who prevents it from happening?  If you can't visualize at least one lawful universe where physics say that such dreadful things happen—and so they *do* happen, there being nowhere to appeal the verdict—then you aren't yet ready to argue *probabilities*.\n> \n> \\[...\\] If there is a fair(er) universe, we have to get there starting from *this* world—the neutral world, the world of hard concrete with no padding, the world where challenges are not calibrated to your skills.\n\n### 4\\. Modeling existential risks in far mode, versus near mode\n\nQuoting Yudkowsky again, in \"[Cognitive Biases Potentially Affecting Judgment of Global Risks](https://intelligence.org/files/CognitiveBiases.pdf)\":\n\n> In addition to standard biases, I have personally observed what look like harmful modes of thinking specific to existential risks. The Spanish flu of 1918 killed 25-50 million people. World War II killed 60 million people. 108 is the order of the largest catastrophes in humanity’s written history. Substantially larger numbers, such as 500 million deaths, and *especially* qualitatively different scenarios such as the extinction of the entire human species, seem to trigger a *different mode of thinking*—enter into a “separate magisterium.” People who would never dream of hurting a child hear of an existential risk, and say, “Well, maybe the human species doesn’t really deserve to survive.”\n> \n> There is a saying in heuristics and biases that people do not evaluate events, but descriptions of events—what is called non-extensional reasoning. The *extension* of humanity’s extinction includes the death of yourself, of your friends, of your family, of your loved ones, of your city, of your country, of your political fellows. Yet people who would take great offense at a proposal to wipe the country of Britain from the map, to kill every member of the Democratic Party in the U.S., to turn the city of Paris to glass—who would feel still greater horror on hearing the doctor say that their child had cancer—these people will discuss the extinction of humanity with perfect calm. “Extinction of humanity,” as words on paper, appears in fictional novels, or is discussed in philosophy books—it belongs to a different context than the Spanish flu. We evaluate descriptions of events, not extensions of events. The cliché phrase *end of the world* invokes the magisterium of myth and dream, of prophecy and apocalypse, of novels and movies. The challenge of existential risks to rationality is that, the catastrophes being so huge, people snap into a different mode of thinking.\n\nIn terms of [construal level theory](https://en.wikipedia.org/wiki/Construal_level_theory), personal tragedies are \"near\", while human extinction is \"far\". We think of far-mode things in more abstract and detached terms, more like morality tales or symbols than like messy, concrete, mechanistic processes.\n\nRationally, we ought to take larger disasters proportionally more seriously than equally probable small-scale risks. In practice, we don't seem to do that at all.\n\n\"Well, maybe we aren't all going to die; it's not a sure thing!\" is a lot weaker than the bar we usually require for doing *anything*.\n\nThe above is my attempt at a partial explanation of what's going on:\n\n*   People are taking the risks unseriously because they feel weird and abstract.\n*   When they do think about the risks, they anchor to what's familiar and known, dismissing other considerations because they feel \"unconservative\" from a forecasting perspective.\n*   Meanwhile, social mimesis and the bystander effect make the field [sluggish](https://www.lesswrong.com/posts/Rkxj7TFxhbm59AKJh/the-inordinately-slow-spread-of-good-agi-conversations-in-ml) at pivoting in response to new arguments and smoke under the door.\n\nWhat do you think of this picture? Do you have a different model of what's going on? And if this is what's going on, what should we do about it?",
      "plaintextDescription": "I've previously written that good ideas and conversations about AGI seem to have propagated through ML weirdly slowly.\n\nA different weird phenomenon I observe is that the field's relative inaction about AGI seems less based on a confident set of beliefs about AGI definitely going well (or definitely being far off), and more based on an implicit sense like \"the default is everything going well, and we don't need to change anything until there's overwhelming evidence to the contrary\".\n\nSome people do have confident beliefs that imply \"things will go well\"; I disagree there, but I expect some amount of disagreement like that.\n\nBut that doesn't seem to be the crux for most people in ML.\n\nIn a sane world, it doesn't seem like \"well, maybe AI will get stuck at human-ish levels for decades\" or \"well, maybe superintelligence couldn't invent any wild new tech\" ought to be cruxes for \"Should we pause AI development?\" or \"Is alignment research the world's top priority?\"\n\nNote that I'm not arguing \"an AGI-mediated extinction event is such a big deal that we should make it a top priority even if it's very unlikely\". There are enough other powerful technologies on the horizon, and enough other risks for civilizational collapse or value lock-in, that I don't in fact think AGI x-risk should get major attention if it's very unlikely.\n\nBut the most common view within ML seems to be less \"it's super unlikely for reasons X Y Z\", and more of an \"I haven't thought about it much\" and/or \"I see some reasons to be very worried, but also some reasons things might be fine, so I end up with medium-ish levels of worry\".\n\nIn a mid-2022 survey, 48% of researchers who had recently published in NeurIPS or ICML gave double-digit probabilities to advanced AI's long-term effect being “extremely bad (e.g., human extinction)”. A similar number gave double-digit probabilities to \"human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowermen",
      "wordCount": 2606
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "e4pYaNt89mottpkWZ",
    "title": "Yudkowsky on AGI risk on the Bankless podcast",
    "slug": "yudkowsky-on-agi-risk-on-the-bankless-podcast",
    "url": null,
    "baseScore": 83,
    "voteCount": 38,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-03-13T00:42:22.694Z",
    "contents": {
      "markdown": "Eliezer gave a very frank overview of his take on AI two weeks ago on the cryptocurrency show Bankless: \n\nI've posted a transcript of the show and a [**follow-up Q&A**](https://twitter.com/i/spaces/1PlJQpZogzVGE) below.\n\nAndrea_Miotti, remember, and vonk previously posted transcripts to LessWrong ([1](https://www.lesswrong.com/posts/Aq82XqYhgqdPdPrBA/full-transcript-eliezer-yudkowsky-on-the-bankless-podcast),[2](https://www.lesswrong.com/posts/jfYnq8pKLpKLwaRGN/transcript-yudkowsky-on-bankless-follow-up-q-and-a) — see the comments there for discussion), but they contained many important errors, so I've corrected the transcripts below. (They also weren't available on the EA Forum.)\n\n* * *\n\n[Intro](https://www.youtube.com/watch?v=gA1sNLL6yg4)\n----------------------------------------------------\n\n**Eliezer Yudkowsky**: \\[clip\\] I think that we are hearing the last winds start to blow, the fabric of reality start to fray. This thing alone cannot end the world, but I think that probably some of the vast quantities of money being blindly and helplessly piled into here are going to end up actually accomplishing something.\n\n**Ryan Sean Adams**: Welcome to Bankless, where we explore the frontier of internet money and internet finance. This is how to get started, how to get better, how to front run the opportunity. This is Ryan Sean Adams. I'm here with David Hoffman, and we're here to help you become more bankless.\n\nOkay, guys, we wanted to do an episode on AI at Bankless, but I feel like David...\n\n**David: **Got what we asked for.\n\n**Ryan: **We accidentally waded into the deep end of the pool here. And I think before we get into this episode, it probably warrants a few comments. I'm going to say a few things I'd like to hear from you too. But one thing I want to tell the listener is, don't listen to this episode if you're not ready for an existential crisis. Okay? I'm kind of serious about this. I'm leaving this episode shaken. And I don't say that lightly. In fact, David, I think you and I will have some things to discuss in the debrief as far as how this impacted you. But this was an impactful one. It sort of hit me during the recording, and I didn't know fully how to react. I honestly am coming out of this episode wanting to refute some of the claims made in this episode by our guest, Eliezer Yudkowsky, who makes the claim that humanity is on the cusp of developing an AI that's going to destroy us, and that there's really not much we can do to stop it.\n\n**David:** There's no way around it, yeah.\n\n**Ryan: **I have a lot of respect for this guest. Let me say that. So it's not as if I have some sort of big-brained technical disagreement here. In fact, I don't even know enough to fully disagree with anything he's saying. But the conclusion is so dire and so existentially heavy that I'm worried about it impacting you, listener, if we don't give you this warning going in.\n\nI also feel like, David, as interviewers, maybe we could have done a better job. I'll say this on behalf of myself. Sometimes I peppered him with a lot of questions in one fell swoop, and he was probably only ready to synthesize one at a time.\n\nI also feel like we got caught flat-footed at times. I wasn't expecting his answers to be so frank and so dire, David. It was just bereft of hope.\n\nAnd I appreciated very much the honesty, as we always do on Bankless. But I appreciated it almost in the way that a patient might appreciate the honesty of their doctor telling them that their illness is terminal. Like, it's still really heavy news, isn't it? \n\nSo that is the context going into this episode. I will say one thing. In good news, for our failings as interviewers in this episode, they might be remedied because at the end of this episode, after we finished with hitting the record button to stop recording, Eliezer said he'd be willing to provide an additional Q&A episode with the Bankless community. So if you guys have questions, and if there's sufficient interest for Eliezer to answer, tweet at us to express that interest. Hit us in Discord. Get those messages over to us and let us know if you have some follow-up questions.\n\nHe said if there's enough interest in the crypto community, he'd be willing to come on and do another episode with follow-up Q&A. Maybe even a Vitalik and Eliezer episode is in store. That's a possibility that we threw to him. We've not talked to Vitalik about that too, but I just feel a little overwhelmed by the subject matter here. And that is the basis, the preamble through which we are introducing this episode.\n\nDavid, there's a few benefits and takeaways I want to get into. But before I do, can you comment or reflect on that preamble? What are your thoughts going into this one?\n\n**David: **Yeah, we approached the end of our agenda—for every Bankless podcast, there's an equivalent agenda that runs alongside of it. But once we got to this crux of this conversation, it was not possible to proceed in that agenda, because... what was the point?\n\n**Ryan:** Nothing else mattered.\n\n**David: **And nothing else really matters, which also just relates to the subject matter at hand. And so as we proceed, you'll see us kind of circle back to the same inevitable conclusion over and over and over again, which ultimately is kind of the punchline of the content.\n\nI'm of a specific disposition where stuff like this, I kind of am like, “Oh, whatever, okay”, just go about my life. Other people are of different dispositions and take these things more heavily. So Ryan's warning at the beginning is if you are a type of person to take existential crises directly to the face, perhaps consider doing something else instead of listening to this episode.\n\n**Ryan: **I think that is good counsel.\n\nSo, a few things if you're looking for an outline of the agenda. We start by talking about ChatGPT. Is this a new era of artificial intelligence? Got to begin the conversation there.\n\nNumber two, we talk about what an artificial superintelligence might look like. How smart exactly is it? What types of things could it do that humans cannot do?\n\nNumber three, we talk about why an AI superintelligence will almost certainly spell the end of humanity and why it'll be really hard, if not impossible, according to our guest, to stop this from happening.\n\nAnd number four, we talk about if there is absolutely anything we can do about all of this. We are heading careening maybe towards the abyss. Can we divert direction and not go off the cliff? That is the question we ask Eliezer.\n\nDavid, I think you and I have a lot to talk about during the debrief. All right, guys, the debrief is an episode that we record right after the episode. It's available for all Bankless citizens. We call this the Bankless Premium Feed. You can access that now to get our raw and unfiltered thoughts on the episode. And I think it's going to be pretty raw this time around, David.\n\n**David:** I didn't expect this to hit you so hard.\n\n**Ryan: **Oh, I'm dealing with it right now.\n\n**David: **Really?\n\n**Ryan**: And this is not too long after the episode. So, yeah, I don't know how I'm going to feel tomorrow, but I definitely want to talk to you about this. And maybe have you give me some counseling. (*laughs*)\n\n**David:** I'll put my psych hat on, yeah.\n\n**Ryan: **Please! I'm going to need some help.\n\n[ChatGPT](https://youtu.be/gA1sNLL6yg4?t=601)\n---------------------------------------------\n\n**Ryan:** Bankless Nation, we are super excited to introduce you to our next guest. Eliezer Yudkowsky is a decision theorist. He's an AI researcher. He's the seeder of the Less Wrong community blog, a fantastic blog by the way. There's so many other things that he's also done. I can't fit this in the short bio that we have to introduce you to Eliezer.\n\nBut most relevant probably to this conversation is he's working at the Machine Intelligence Research Institute to ensure that when we do make general artificial intelligence, it doesn't come kill us all. Or at least it doesn't come ban cryptocurrency, because that would be a poor outcome as well.\n\n**Eliezer: **(*laughs*)\n\n**Ryan: **Eliezer, it's great to have you on Bankless. How are you doing?\n\n**Eliezer:** Within one standard deviation of my own peculiar little mean.\n\n**Ryan: **(*laughs*) Fantastic. You know, we want to start this conversation with something that jumped onto the scene for a lot of mainstream folks quite recently, and that is ChatGPT. So apparently over 100 million or so have logged on to ChatGPT quite recently. I've been playing with it myself. I found it very friendly, very useful. It even wrote me a sweet poem that I thought was very heartfelt and almost human-like.\n\nI know that you have major concerns around AI safety, and we're going to get into those concerns. But can you tell us in the context of something like a ChatGPT, is this something we should be worried about? That this is going to turn evil and enslave the human race? How worried should we be about ChatGPT and BARD and the new AI that's entered the scene recently?\n\n**Eliezer:** ChatGPT itself? Zero. It's not smart enough to do anything really wrong. Or really right either, for that matter.\n\n**Ryan:** And what gives you the confidence to say that? How do you know this?\n\n**Eliezer:** Excellent question. So, every now and then, somebody figures out how to put a new prompt into ChatGPT. You know, one time somebody found that one of the earlier generations of the technology would sound smarter if you first told it it was Eliezer Yudkowsky. There's other prompts too, but that one's one of my favorites. So there's untapped potential in there that people hadn't figured out how to prompt yet.\n\nBut when people figure it out, it moves ahead sufficiently short distances that I do feel fairly confident that there is not so much untapped potential in there that it is going to take over the world. It's, like, making small movements, and to take over the world it would need a very large movement. There's places where it falls down on predicting the next line that a human would say in its shoes that seem indicative of “probably that capability just is not in the giant inscrutable matrices, or it would be using it to predict the next line”, which is very heavily what it was optimized for. So there's going to be some untapped potential in there. But I do feel quite confident that the upper range of that untapped potential is insufficient to outsmart all the living humans and implement the scenario that I'm worried about.\n\n**Ryan:** Even so, though, is ChatGPT a big leap forward in the journey towards AI in your mind? Or is this fairly incremental, it's just (for whatever reason) caught mainstream attention?\n\n**Eliezer:** GPT-3 was a big leap forward. There's rumors about GPT-4, which, who knows? ChatGPT is a commercialization of the actual AI-in-the-lab giant leap forward. If you had never heard of GPT-3 or GPT-2 or the whole range of text transformers before ChatGPT suddenly entered into your life, then that whole thing is a giant leap forward. But it's a giant leap forward based on a technology that was published in, if I recall correctly, 2018.\n\n**David:** I think that what's going around in everyone's minds right now—and the Bankless listenership (and crypto people at large) are largely futurists, so everyone (I think) listening understands that in the future, there will be sentient AIs perhaps around us, at least by the time that we all move on from this world.\n\nSo we all know that this future of AI is coming towards us. And when we see something like ChatGPT, everyone's like, “Oh, is this the moment in which our world starts to become integrated with AI?” And so, Eliezer, you've been tapped into the world of AI. Are we onto something here? Or is this just another fad that we will internalize and then move on for? And then the real moment of generalized AI is actually much further out than we're initially giving credit for. Like, where are we in this timeline?\n\n**Eliezer: **Predictions are hard, especially about the future. I sure hope that this is where it saturates — this or the next generation, it goes only this far, it goes no further. It doesn't get used to make more steel or build better power plants, first because that's illegal, and second because the large language model technologies’ basic vulnerability is that it’s not reliable. It's good for applications where it works 80% of the time, but not where it needs to work 99.999% of the time. This class of technology can't drive a car because it will sometimes crash the car.\n\nSo I hope it saturates there. I hope they can't fix it. I hope we get, like, a 10-year AI winter after this.\n\nThis is not what I actually predict. I think that we are hearing the last winds start to blow, the fabric of reality start to fray. This thing alone cannot end the world. But I think that probably some of the vast quantities of money being blindly and helplessly piled into here are going to end up actually accomplishing something.\n\nNot most of the money—that just never happens in any field of human endeavor. But 1% of $10 billion is still a lot of money to actually accomplish something.\n\n[AGI](https://youtu.be/gA1sNLL6yg4?t=992)\n-----------------------------------------\n\n**Ryan:** So listeners, I think you've heard Eliezer's thesis on this, which is pretty dim with respect to AI alignment—and we'll get into what we mean by AI alignment—and very worried about AI-safety-related issues.\n\nBut I think for a lot of people to even worry about AI safety and for us to even have that conversation, I think they have to have some sort of grasp of what AGI looks like. I understand that to mean “artificial general intelligence” and this idea of a super-intelligence.\n\nCan you tell us: if there was a superintelligence on the scene, what would it look like? I mean, is this going to look like a big chat box on the internet that we can all type things into? It's like an oracle-type thing? Or is it like some sort of a robot that is going to be constructed in a secret government lab? Is this, like, something somebody could accidentally create in a dorm room? What are we even looking for when we talk about the term “AGI” and “superintelligence”?\n\n**Eliezer:** First of all, I'd say those are pretty distinct concepts. ChatGPT shows a very wide range of generality compared to the previous generations of AI. Not very wide generality compared to GPT-3—not literally the lab research that got commercialized, that's the same generation. But compared to stuff from 2018 or even 2020, ChatGPT is better at a much wider range of things without having been explicitly programmed by humans to be able to do those things.\n\nTo imitate a human as best it can, it has to capture all of the things that humans can think about that it can, which is not all the things. It's still not very good at long multiplication (unless you give it the right instructions, in which case suddenly it can do it). \n\nIt's significantly more general than the previous generation of artificial minds. Humans were significantly more general than the previous generation of chimpanzees, or rather *Australopithecus* or last common ancestor.\n\nHumans are not *fully* general. If humans were fully general, we'd be as good at coding as we are at football, throwing things, or running. Some of us are okay at programming, but we're not spec'd for it. We're not *fully* general minds.\n\nYou can imagine something that's more general than a human, and if it runs into something unfamiliar, it's like, okay, let me just go reprogram myself a bit and then I'll be as adapted to this thing as I am to anything else.\n\nSo ChatGPT is less general than a human, but it's genuinely ambiguous, I think, whether it's more or less general than (say) our cousins, the chimpanzees. Or if you don't believe it's as general as a chimpanzee, a dolphin or a cat.\n\n**Ryan:** So this idea of general intelligence is sort of a range of things that it can actually do, a range of ways it can apply itself?\n\n**Eliezer: **How wide is it? How much reprogramming does it need? How much retraining does it need to make it do a new thing?\n\nBees build hives, beavers build dams, a human will look at a beehive and imagine a honeycomb shaped dam. That's. like, humans alone in the animal kingdom. But that doesn't mean that we are general intelligences, it means we're significantly more generally applicable intelligences than chimpanzees.\n\nIt's not like we're all that narrow. We can walk on the moon. We can walk on the moon because there's aspects of our intelligence that are made in full generality for universes that contain simplicities, regularities, things that recur over and over again. We understand that if steel is hard on Earth, it may stay hard on the moon. And because of that, we can build rockets, walk on the moon, breathe amid the vacuum.\n\nChimpanzees cannot do that, but that doesn't mean that humans are the most general possible things. The thing that is more general than us, that figures that stuff out faster, is the thing to be scared of if the purposes to which it turns its intelligence are not ones that we would recognize as nice things, even in the most [cosmopolitan and embracing](https://arbital.com/p/value_cosmopolitan/) senses of what's worth doing.\n\n[Efficiency](https://youtu.be/gA1sNLL6yg4?t=1269)\n-------------------------------------------------\n\n**Ryan:** And you said this idea of a general intelligence is different than the concept of superintelligence, which I also brought into that first part of the question. How is superintelligence different than general intelligence?\n\n**Eliezer:** Well, because ChatGPT has a little bit of general intelligence. Humans have more general intelligence. A superintelligence is something that can beat any human and the entire human civilization at all the cognitive tasks. I don't know if the efficient market hypothesis is something where I can rely on the entire… \n\n**Ryan: **We're all crypto investors here. We understand the efficient market hypothesis for sure.\n\n**Eliezer: **So the [efficient market hypothesis](https://equilibriabook.com/inadequacy-and-modesty/) is of course not generally true. It's not true that literally all the market prices are smarter than you. It's not true that all the prices on earth are smarter than you. Even the most arrogant person who is at all calibrated, however, still thinks that the efficient market hypothesis is true relative to them 99.99999% of the time. They only think that they know better about one in a million prices.\n\nThey might be important prices. The price of Bitcoin is an important price. It's not just a random price. But if the efficient market hypothesis was only true to you 90% of the time, you could just pick out the 10% of the remaining prices and double your money every day on the stock market. And nobody can do that. Literally nobody can do that.\n\nSo this property of relative efficiency that the market has to you, that the price’s estimate of the future price already has all the information you have—not all the information that exists in principle, maybe not all the information that the best equity could, but it's efficient relative to you.\n\nFor you, if you pick out a random price, like the price of Microsoft stock, something where you've got no special advantage, that estimate of its price a week later is efficient relative to you. *You* can't do better than that price.\n\nWe have much less experience with the notion of [instrumental efficiency](https://arbital.com/p/efficiency/), efficiency in choosing actions, because actions are harder to aggregate estimates about than prices. So you have to look at, say, AlphaZero playing chess—or just, you know, whatever the latest Stockfish number is, an advanced chess engine.\n\nWhen it makes a chess move, you can't do better than that chess move. It may not be the optimal chess move, but if you pick a different chess move, you'll do worse. That you'd call a kind of efficiency of action. Given its goal of winning the game, once you know its move—unless you consult some more powerful AI than Stockfish—you can't figure out a better move than that.\n\nA superintelligence is like that with respect to everything, with respect to all of humanity. It is relatively efficient to humanity. It has the best estimates—not perfect estimates, but the best estimates—and its estimates contain all the information that you've got about it. Its actions are the most efficient actions for accomplishing its goals. If you think you see a better way to accomplish its goals, you're mistaken.\n\n**Ryan: **So you're saying \\[if something is a\\] superintelligence, we'd have to imagine something that knows all of the chess moves in advance. But here we're not talking about chess, we're talking about everything. It knows all of the moves that we would make and the most optimum pattern, including moves that we would not even know how to make, and it knows these things in advance.\n\nI mean, how would human beings sort of experience such a superintelligence? I think we still have a very hard time imagining something smarter than us, just because we've never experienced anything like it before.\n\nOf course, we all know somebody who's genius-level IQ, maybe quite a bit smarter than us, but we've never encountered something like what you're describing, some sort of mind that is superintelligent.\n\nWhat sort of things would it be doing that humans couldn't? How would we experience this in the world?\n\n**Eliezer:** I mean, we do have some tiny bit of experience with it. We have experience with chess engines, where we just can't figure out better moves than they make. We have experience with market prices, where even though your uncle has this really long, elaborate story about Microsoft stock, you just know he's wrong. Why is he wrong? Because if he was correct, it would already be incorporated into the stock price.\n\nAnd especially because the market’s efficiency is not perfect, like that whole downward swing and then upward move in COVID. I have friends who made more money off that than I did, but I still managed to buy back into the broader stock market on the exact day of the low—basically coincidence. So the markets aren't perfectly efficient, but they're efficient almost everywhere.\n\nAnd that sense of deference, that sense that your weird uncle can't possibly be right because the hedge funds would know it—you know. unless he's talking about COVID, in which case maybe he is right if you have the right choice of weird uncle! I have weird friends who are maybe better at calling these things than your weird uncle. So among humans, it's subtle. \n\nAnd then with superintelligence, it's not subtle, just massive advantage. But not perfect. It's not that it knows every possible move you make before you make it. It's that it's got a good probability distribution about that. And it has figured out all the *good* moves you could make and figured out how to reply to those.\n\nAnd I mean, in practice, what's that like? Well, unless it's limited, narrow superintelligence, I think you mostly don't get to observe it because you are dead, unfortunately.\n\n**Ryan: **What? (*laughs*)\n\n**Eliezer: **Like, Stockfish makes strictly better chess moves than you, but it's playing on a very narrow board. And the fact that it's better at you than chess doesn't mean it's better at you than everything. And I think that the actual catastrophe scenario for AI looks like big advancement in a research lab, maybe driven by them getting a giant venture capital investment and being able to spend 10 times as much on GPUs as they did before, maybe driven by a new algorithmic advance like transformers, maybe driven by hammering out some tweaks in last year's algorithmic advance that gets the thing to finally work efficiently. And the AI there goes over a critical threshold, which most obviously could be like, “can write the next AI”. \n\nThat's so obvious that science fiction writers figured it out almost before there were computers, possibly even before there were computers. I'm not sure what the exact dates here are. But if it's better at you than everything, it's better at you than building AIs. That snowballs. It gets an immense technological advantage. If it's smart, it doesn't announce itself. It doesn't tell you that there's a fight going on. It emails out some instructions to one of those labs that'll synthesize DNA and synthesize proteins from the DNA and get some proteins mailed to a hapless human somewhere who gets paid a bunch of money to mix together some stuff they got in the mail in a file. Like, smart people will not do this for any sum of money. Many people are not smart. Builds the ribosome, but the ribosome that builds things out of covalently bonded diamondoid instead of proteins folding up and held together by Van der Waals forces, builds tiny diamondoid bacteria. The diamondoid bacteria replicate using atmospheric carbon, hydrogen, oxygen, nitrogen, and sunlight. And a couple of days later, everybody on earth falls over dead in the same second.\n\nThat's the disaster scenario if it's as smart as I am. If it's smarter, it might think of a better way to do things. But it can at least think of that if it's relatively efficient compared to humanity because I'm in humanity and I thought of it.\n\n**Ryan:** This is—I've got a million questions, but I'm gonna let David go first.\n\n**David:** Yeah. So we sped run the introduction of a number of different concepts, which I want to go back and take our time to really dive into.\n\nThere's the AI alignment problem. There's AI escape velocity. There is the question of what happens when AIs are so incredibly intelligent that humans are to AIs what ants are to us.\n\nAnd so I want to kind of go back and tackle these, Eliezer, one by one.\n\nWe started this conversation talking about ChatGPT, and everyone's up in arms about ChatGPT. And you're saying like, yes, it's a great step forward in the generalizability of some of the technologies that we have in the AI world. All of a sudden ChatGPT becomes immensely more useful and it's really stoking the imaginations of people today.\n\nBut what you're saying is it's not the thing that's actually going to be the thing to reach escape velocity and create superintelligent AIs that perhaps might be able to enslave us. But my question to you is, how do we know when that—\n\n**Eliezer: **Not enslave. They don't enslave you, but sorry, go on.\n\n**David: **Yeah, sorry.\n\n**Ryan:** Murder, David. Kill all of us. Eliezer was very clear on that.\n\n**David:** So if it's not ChatGPT, how close are we? Because there's this unknown event horizon where you kind of alluded to it, where we make this AI that we train it to create a smarter AI and that smart AI is so incredibly smart that it hits escape velocity and all of a sudden these dominoes fall. How close are we to that point? And are we even capable of answering that question?\n\n**Eliezer: **How the heck would I know? \n\n**Ryan: **Well, when you were talking, Eliezer, if we had already crossed that event horizon, a smart AI wouldn't necessarily broadcast that to the world. I mean, it's possible we've already crossed that event horizon, is it not?\n\n**Eliezer:** I mean, it's theoretically possible, but seems very unlikely. Somebody would need inside their lab an AI that was much more advanced than the public AI technology. And as far as I currently know, the best labs and the best people are throwing their ideas to the world! Like, they don't care.\n\nAnd there's probably some secret government labs with secret government AI researchers. My pretty strong guess is that they don't have the best people and that those labs could not create ChatGPT on their own because ChatGPT took a whole bunch of fine twiddling and tuning and visible access to giant GPU farms and that they don't have the people who know how to do the twiddling and tuning. This is just a guess.\n\n[AI Alignment](https://youtu.be/gA1sNLL6yg4?t=1969)\n---------------------------------------------------\n\n**David:** Could you walk us through—one of the big things that you spend a lot of time on is this thing called the AI alignment problem. Some people are not convinced that when we create AI, that AI won't really just be fundamentally aligned with humans. I don't believe that you fall into that camp. I think you fall into the camp of when we do create this superintelligent, generalized AI, we are going to have a hard time aligning with it in terms of our morality and our ethics.\n\nCan you walk us through a little bit of that thought process? Why do you feel disaligned?\n\n**Ryan: **The dumb way to ask that question too is like, Eliezer, why do you think that the AI automatically hates us? Why is it going to—\n\n**Eliezer: **It doesn't hate you.\n\n**Ryan: **Why does it want to kill us all?\n\n**Eliezer:** The AI doesn't hate you, neither does it love you, and you're made of atoms that it can use for something else.\n\n**David:** It's indifferent to you.\n\n**Eliezer: **It's got something that it actually does care about, which makes no mention of you. And you are made of atoms that it can use for something else. That's all there is to it in the end.\n\nThe reason you're not in its utility function is that the programmers did not know how to do that. The people who built the AI, or the people who built the AI that built the AI that built the AI, did not have the technical knowledge that nobody on earth has at the moment as far as I know, whereby you can do that thing and you can control in detail what that thing ends up caring about.\n\n**David: **So this feels like humanity is hurdling itself towards what we're calling, again, an event horizon where there's this AI escape velocity, and there's nothing on the other side. As in, we do not know what happens past that point as it relates to having some sort of superintelligent AI and how it might be able to manipulate the world. Would you agree with that?\n\n**Eliezer: **No.\n\nAgain, the Stockfish chess-playing analogy. You cannot predict exactly what move it would make, because in order to predict exactly what move it would make, you would have to be at least that good at chess, and it's better than you.\n\nThis is true even if it's just a little better than you. Stockfish is actually enormously better than you, to the point that once it tells you the move, you can't figure out a better move without consulting a different AI. But even if it was just a bit better than you, then you're in the same position.\n\nThis kind of disparity also exists between humans. If you ask me, where will Garry Kasparov move on this chessboard? I'm like, I don't know, maybe here. Then if Garry Kasparov moves somewhere else, it doesn't mean that he's wrong, it means that I'm wrong. If I could predict exactly where Garry Kasparov would move on a chessboard, I'd be Garry Kasparov. I'd be at least that good at chess. Possibly better. I could also be able to predict him, but also see an even better move than that. \n\nThat's an irreducible source of uncertainty with respect to superintelligence, or anything that's smarter than you. If you could predict exactly what it would do, you'd be that smart yourself. It doesn't mean you can predict no facts about it.\n\nWith Stockfish in particular, I can predict it's going to win the game. I know what it's optimizing for. I know where it's trying to steer the board. I can't predict exactly what the board will end up looking like after Stockfish has finished winning its game against me. I can predict it will be in the class of states that are winning positions for black or white or whichever color Stockfish picked, because, you know, it wins either way.\n\nAnd that's similarly where I'm getting the prediction about everybody being dead, because if everybody were alive, then there'd be some state that the superintelligence preferred to that state, which is all of the atoms making up these people and their farms are being used for something else that it values more.\n\nSo if you postulate that everybody's still alive, I'm like, okay, well, why is it you're postulating that Stockfish made a stupid chess move and ended up with a non-winning board position? That's where that class of predictions come from.\n\n**Ryan: **Can you reinforce this argument, though, a little bit? So, why is it that an AI can't be nice, sort of like a gentle parent to us, rather than sort of a murderer looking to deconstruct our atoms and apply for use somewhere else?\n\nWhat are its goals? And why can't they be aligned to at least some of our goals? Or maybe, why can't it get into a status which is somewhat like us and the ants, which is largely we just ignore them unless they interfere in our business and come in our house and raid our cereal boxes?\n\n**Eliezer: **There's a bunch of different questions there. So first of all, the space of minds is [very wide](https://www.lesswrong.com/posts/tnWRXkcDi5Tw9rzXw/the-design-space-of-minds-in-general). Imagine this giant sphere and all the humans are in this one tiny corner of the sphere. We're all basically the same make and model of car, running the same brand of engine. We're just all painted slightly different colors.\n\nSomewhere in that mind space, there's things that are as nice as humans. There's things that are nicer than humans. There are things that are trustworthy and nice and kind in ways that no human can ever be. And there's even things that are so nice that they can understand the concept of leaving you alone and doing your own stuff sometimes instead of hanging around trying to be obsessively nice to you every minute and all the other famous disaster scenarios from ancient science fiction (\"With Folded Hands\" by Jack Williamson is the one I'm quoting there.)\n\nWe don't know how to reach into mind design space and pluck out an AI like that. It's not that they don't exist in principle. It's that we don't know how to do it. And I’ll hand back the conversational ball now and figure out, like, which next question do you want to go down there?\n\n**Ryan: **Well, I mean, why? Why is it so difficult to align an AI with even our basic notions of morality?\n\n**Eliezer: **I mean, I wouldn't say that it's difficult to align an AI with our basic notions of morality. I'd say that it's difficult to align an AI on a task like “take this strawberry, and make me another strawberry that's identical to this strawberry down to the cellular level, but not necessarily the atomic level”. So it looks the same under like a standard optical microscope, but maybe not a scanning electron microscope. Do that. Don't destroy the world as a side effect.\n\nNow, this does intrinsically take a powerful AI. There's no way you can make it easy to align by making it stupid. To build something that's cellular identical to a strawberry—I mean, mostly I think the way that you do this is with very primitive nanotechnology, but we could also do it using very advanced biotechnology. And these are not technologies that we already have. So it's got to be something smart enough to develop new technology.\n\nNever mind all the subtleties of morality. I think we don't have the technology to align an AI to the point where we can say, “Build me a copy of the strawberry and don't destroy the world.”\n\nWhy do I think that? Well, case in point, look at natural selection building humans. Natural selection mutates the humans a bit, runs another generation. The fittest ones reproduce more, their genes become more prevalent to the next generation. Natural selection hasn't really had very much time to do this to modern humans at all, but you know, the hominid line, the mammalian line, go back a few million generations. And this is an example of an optimization process building an intelligence.\n\nAnd natural selection asked us for only one thing: “Make more copies of your DNA. Make your alleles more relatively prevalent in the gene pool.” Maximize your inclusive reproductive fitness—not just your own reproductive fitness, but your two brothers or eight cousins, as the joke goes, because they've got on average one copy of your genes. This is *all* we were optimized for, for *millions* of generations, creating humans *from scratch*, from the first accidentally self-replicating molecule.\n\nInternally, psychologically, inside our minds, we do not know what genes are. We do not know what DNA is. We do not know what alleles are. We have no concept of inclusive genetic fitness until our scientists figure out what that even is. We don't know what we were being optimized for. For a long time, many humans thought they'd been created by God!\n\nWhen you use the hill-climbing paradigm and optimize for one single extremely pure thing, this is how much of it gets inside.\n\nIn the ancestral environment, in the exact distribution that we were originally optimized for, humans did tend to end up using their intelligence to try to reproduce more. Put them into a different environment, and all the little bits and pieces and fragments of optimizing for fitness that were in us now do totally different stuff. We have sex, but we wear condoms.\n\nIf natural selection had been a foresightful, intelligent kind of engineer that was able to engineer things successfully, it would have built us to be revolted by the thought of condoms. Men would be lined up and fighting for the right to donate to sperm banks. And in our natural environment, the [little drives](https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter) that got into us happened to lead to more reproduction, but distributional shift: run the humans out of their distribution over which they were optimized, and you get totally different results. \n\nAnd gradient descent would by default do—not quite the same thing, it's going to do a weirder thing because natural selection has a much narrower information bottleneck. In one sense, you could say that natural selection was at an advantage because it finds *simpler* solutions. You could imagine some hopeful engineer who just built intelligences using gradient descent and found out that they end up wanting these thousands and millions of little tiny things, none of which were exactly what the engineer wanted, and being like, well, let's try natural selection instead. It's got a much sharper information bottleneck. It'll find the *simple* specification of what I want.\n\nBut we actually get there as humans. And then, gradient descent, probably may be even worse.\n\nBut more importantly, I'm just pointing out that there is no physical law, computational law, mathematical/logical law, saying when you optimize using hill-climbing on a very simple, very sharp criterion, you get a general intelligence that wants that thing.\n\n**Ryan: **So just like natural selection, our tools are too blunt in order to get to that level of granularity to program in some sort of morality into these super intelligent systems?\n\n**Eliezer: **Or build me a copy of a strawberry without destroying the world. Yeah. The tools are too blunt.\n\n**David: **So I just want to make sure I'm following with what you were saying. I think the conclusion that you left me with is that my brain, which I consider to be at least decently smart, is actually a byproduct, an accidental byproduct of this desire to reproduce. And it's actually just like a tool that I have, and just like conscious thought is a tool, which is a useful tool in means of that end.\n\nAnd so if we're applying this to AI and AI's desire to achieve some certain goal, what's the parallel there?\n\n**Eliezer: **I mean, every organ in your body is a reproductive organ. If it didn't help you reproduce, you would not have an organ like that. Your brain is no exception. This is merely conventional science and merely the conventional understanding of the world. I'm not saying anything here that ought to be at all controversial. I'm sure it's controversial somewhere, but within a pre-filtered audience, it should not be at all controversial. And this is, like, the obvious thing to expect to happen with AI, because why wouldn't it? What new law of existence has been invoked, whereby this time we optimize for a thing and we get a thing that wants exactly what we optimized for on the outside?\n\n[AI Goals](https://youtu.be/gA1sNLL6yg4?t=2763)\n-----------------------------------------------\n\n**Ryan:** So what are the types of goals an AI might want to pursue? What types of utility functions is it going to want to pursue off the bat? Is it just those it's been programmed with, like make an identical strawberry?\n\n**Eliezer: **Well, the whole thing I'm saying is that we do not know how to get goals into a system. We can cause them to do a thing inside a distribution they were optimized over using gradient descent. But if you shift them outside of that distribution, I expect other weird things start happening. When they reflect on themselves, other weird things start happening.\n\nWhat kind of utility functions are in there? I mean, darned if I know. I think you'd have a pretty hard time calling the shape of humans from advance by looking at natural selection, the thing  that natural selection was optimizing for, if you'd never seen a human or anything like a human.\n\nIf we optimize them from the outside to predict the next line of human text, like GPT-3—I don't actually think this line of technology leads to the end of the world, but maybe it does, in like GPT-7—there's probably a bunch of stuff in there too that desires to accurately model things like humans under a wide range of circumstances, but it's not exactly humans, because: ice cream.\n\nIce cream didn't exist in the natural environment, the ancestral environment, the environment of evolutionary adaptedness. There was nothing with that much sugar, salt, fat combined together as ice cream. We are not built to want ice cream. We were built to want strawberries, honey, a gazelle that you killed and cooked and had some fat in it and was therefore nourishing and gave you the all-important calories you need to survive, salt, so you didn't sweat too much and run out of salt. We evolved to want those things, but then ice cream comes along and it fits those taste buds better than anything that existed in the environment that we were optimized over.\n\nSo, a very primitive, very basic, very unreliable wild guess, but at least an informed kind of wild guess: Maybe if you train a thing really hard to predict humans, then among the things that it likes are tiny little pseudo things that meet the definition of “human” but weren't in its training data and that are much easier to predict, or where the problem of predicting them can be solved in a more satisfying way, where “satisfying” is not like human satisfaction, but some other criterion of “thoughts like this are tasty because they help you predict the humans from the training data”. (*shrugs*)\n\n[Consensus](https://youtu.be/gA1sNLL6yg4?t=2951)\n------------------------------------------------\n\n**David: **Eliezer, when we talk about all of these ideas about the ways that AI thought will be fundamentally not able to be understood by the ways that humans think, and then all of a sudden we see this rotation by venture capitalists by just pouring money into AI, do alarm bells go off in your head? Like, hey guys, you haven't thought deeply about these subject matters yet? Does the immense amount of capital going into AI investments scare you?\n\n**Eliezer: **I mean, alarm bells went off for me in 2015, which is when it became obvious that this is how it was going to go down. I sure am now seeing the realization of that stuff I felt alarmed about back then.\n\n**Ryan: **Eliezer, is this view that AI is incredibly dangerous and that AGI is going to eventually end humanity and that we're just careening toward a precipice, would you say this is the consensus view now, or are you still somewhat of an outlier? And why aren't other smart people in this field as alarmed as you? Can you [steel-man](https://www.lesswrong.com/tag/steelmanning) their arguments?\n\n**Eliezer: **You're asking, again, several questions sequentially there. Is it the consensus view? No. Do I think that the people in the wider scientific field who dispute this point of view—do I think they understand it? Do I think they've done anything like an impressive job of arguing against it at all? No.\n\nIf you look at the famous prestigious scientists who sometimes make a little fun of this view in passing, they're making up arguments rather than deeply considering things that are held to any standard of rigor, and people outside their own fields are able to validly shoot them down.\n\nI have no idea how to pronounce his last name. Francis Chollet said something about, I forget his exact words, but it was something like, I never hear any good arguments for stuff. I was like, okay, here's some good arguments for stuff. You can read [the reply from Yudkowsky to Chollet](https://intelligence.org/2017/12/06/chollet/) and Google that, and that'll give you some idea of what the eminent voices versus the reply to the eminent voices sound like. And Scott Aronson, who at the time was off on complexity theory, he was like, “That's not how no free lunch theorems work”, correctly.\n\nI think the state of affairs is we have eminent scientific voices making fun of this possibility, but not engaging with the arguments for it. \n\nNow, if you step away from the eminent scientific voices, you can find people who are more familiar with all the arguments and disagree with me. And I think they lack [security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/). I think that they're engaging in the sort of blind optimism that many, many scientific fields throughout history have engaged in, where when you're approaching something for the first time, you don't know why it will be hard, and you imagine easy ways to do things. And the way that this is supposed to naturally play out over the history of a scientific field is that you run out and you try to do the things and they don't work, and you go back and you try to do other clever things and they don't work either, and you learn some pessimism and you start to understand the reasons why the problem is hard.\n\nThe field of artificial intelligence itself recapitulated this very common ontogeny of a scientific field, where initially we had people getting together at the Dartmouth conference. I forget what their exact famous phrasing was, but it's something like, “We are wanting to address the problem of getting AIs to, you know, like understand language, improve themselves”, and I forget even what else was there. A list of what now sound like grand challenges. “And we think we can make substantial progress on this using 10 researchers for two months.” And I think that at the core is what's going on. \n\nThey have not run into the actual problems of alignment. They aren't trying to get ahead of the game. They're not trying to panic early. They're waiting for reality to hit them onto the head and turn them into grizzled old cynics of their scientific field who understand the reasons why things are hard. They're content with the predictable life cycle of starting out as bright-eyed youngsters, waiting for reality to hit them over the head with the news. And if it wasn't going to kill everybody the first time that they're really wrong, it'd be fine! You know, this is how science works! If we got unlimited free retries and 50 years to solve everything, it'd be okay. We could figure out how to align AI in 50 years given unlimited retries.\n\nYou know, the first team in with the bright-eyed optimists would destroy the world and people would go, oh, well, you know, it's not that easy. They would try something else clever. That would destroy the world. People would go like, oh, well, you know, maybe this field is actually hard. Maybe this is actually one of the thorny things like computer security or something. And so what exactly went wrong last time? Why didn't these hopeful ideas play out? Oh, like you optimize for one thing on the outside and you get a different thing on the inside. Wow. That's really basic. All right. Can we even do this using gradient descent? Can you even build this thing out of giant inscrutable matrices of floating point numbers that nobody understands at all? You know, maybe we need different methodology. And 50 years later, you'd have an aligned AGI.\n\nIf we got unlimited free retries without destroying the world, it'd be, you know, it'd play out the same way that ChatGPT played out. It's, you know, from 1956 or 1955 or whatever it was to 2023. So, you know, about 70 years, give or take a few. And, you know, just like we can do the stuff that they wanted to do in the summer of 1955, you know, 70 years later, you'd have your aligned AGI.\n\nProblem is that the world got destroyed in the meanwhile. And that's why, you know, that's the problem there.\n\n[God Mode and Aliens](https://youtu.be/gA1sNLL6yg4?t=3345)\n----------------------------------------------------------\n\n**David:** So this feels like a gigantic *Don't Look Up* scenario. If you're familiar with that movie, it's a movie about this asteroid hurtling to Earth, but it becomes popular and in vogue to not look up and not notice it. And Eliezer, you're the guy who's saying like, hey, there's an asteroid. We have to do something about it. And if we don't, it's going to come destroy us.\n\nIf you had God mode over the progress of AI research and just innovation and development, what choices would you make that humans are not currently making today?\n\n**Eliezer: **I mean, I could say something like shut down all the large GPU clusters. How long do I have God mode? Do I get to like stick around for seventy years?\n\n**David: **You have God mode for the 2020 decade.\n\n**Eliezer: **For the 2020 decade. All right. That does make it pretty hard to do things.\n\nI think I shut down all the GPU clusters and get all of the famous scientists and brilliant, talented youngsters—the vast, vast majority of whom are not going to be productive and where government bureaucrats are not going to be able to tell who's actually being helpful or not, but, you know—put them all on a large island, and try to figure out some system for filtering the stuff through to me to give thumbs up or thumbs down on that is going to work better than scientific bureaucrats producing entire nonsense.\n\nBecause, you know, the trouble is—the reason why scientific fields have to go through this long process to produce the cynical oldsters who know that everything is difficult. It's not that the youngsters are stupid. You know, sometimes youngsters are fairly smart. You know, Marvin Minsky, John McCarthy back in 1955, they weren't idiots. You know, privileged to have met both of them. They didn't strike me as idiots. They were very old, and they still weren't idiots. But, you know, it's hard to see what's coming in advance of experimental evidence hitting you over the head with it.\n\nAnd if I only have the decade of the 2020s to run all the researchers on this giant island somewhere, it's really not a lot of time. Mostly what you've got to do is invent some entirely new AI paradigm that isn't the giant inscrutable matrices of floating point numbers on gradient descent. Because I'm not really seeing what you can do that's clever with that, that doesn't kill you and that you know doesn't kill you and doesn't kill you the very first time you try to do something clever like that.\n\nYou know, I'm sure there's *a* way to do it. And if you got to try over and over again, you could find it.\n\n**Ryan: **Eliezer, do you think every intelligent civilization has to deal with this exact problem that humanity is dealing with now? Of how do we solve this problem of aligning with an advanced general intelligence?\n\n**Eliezer: **I expect that's much easier for some alien species than others. Like, there are alien species who might arrive at “this problem” in an entirely different way. Maybe instead of having two entirely different information processing systems, the DNA and the neurons, they've only got one system. They can trade memories around heritably by swapping blood sexually. Maybe the way in which they “confront this problem” is that very early in their evolutionary history, they have the equivalent of the DNA that stores memories and processes, computes memories, and they swap around a bunch of it, and it adds up to something that reflects on itself and makes itself coherent, and then you've got a superintelligence before they have invented computers. And maybe that thing wasn't aligned, but how do you even align it when you're in that kind of situation? It'd be a very different angle on the problem.\n\n**Ryan: **Do you think every advanced civilization is on the trajectory to creating a superintelligence at some point in its history?\n\n**Eliezer: **Maybe there's ones in universes with alternate physics where you just can't do that. Their universe's computational physics just doesn't support that much computation. Maybe they never get there. Maybe their lifespans are long enough and their star lifespans short enough that they never get to the point of a technological civilization before their star does the equivalent of expanding or exploding or going out and their planet ends.\n\n“Every alien species” covers a lot of territory, especially if you talk about alien species and universes with physics different from this one.\n\n**Ryan:** Well, talking about our present universe, I'm curious if you've been confronted with the question of, well, then why haven't we seen some sort of superintelligence in our universe when we look out at the stars? Sort of the Fermi paradox type of question. Do you have any explanation for that?\n\n**Eliezer:** Oh, well, supposing that they got killed by their own AIs doesn't help at all with that because then we'd see the AIs.\n\n**Ryan:** And do you think that's what happens? Yeah, it doesn't help with that. We would see evidence of AIs, wouldn't we?\n\n**Eliezer: **Yeah.\n\n**Ryan: ** Yes. So why don't we?\n\n**Eliezer:** I mean, the same reason we don't see evidence of the alien civilizations not with AIs.\n\nAnd that reason is, although it doesn't really have much to do with the whole AI thesis one way or another, because they're too far away—or so says Robin Hanson, using a very clever argument about the apparent difficulty of hard steps in humanity's evolutionary history to further induce the rough gap between the hard steps. ... And, you know, I can't really do justice to this. If you look up grabby aliens, you can...\n\n**Ryan: **Grabby aliens?\n\n**David: **I remember this.\n\n**Eliezer:** Grabby aliens. You can find Robin Hanson's very clever argument for how far away the aliens are...\n\n**Ryan: **There's an entire website, Bankless listeners, there's an entire website called [grabbyaliens.com](https://grabbyaliens.com/) you can go look at.\n\n**Eliezer: **Yeah. And that contains by far the best answer I've seen, to:\n\n*   “Where are they?” (Answer: too far away for us to see, even if they're traveling here at nearly light speed.)\n*   How far away are they?\n*   And how do we know that?\n\n(*laughs*) But, yeah.\n\n**Ryan:** This is amazing.\n\n**Eliezer:** There is not a very good way to simplify the argument, any more than there is to simplify the notion of zero-knowledge proofs. It's not that difficult, but it's just very not easy to simplify. But if you have a bunch of locks that are all of different difficulties, and a limited time in which to solve all the locks, such that anybody who gets through all the locks must have gotten through them by luck, all the locks will take around the same amount of time to solve, even if they're all of very different difficulties. And that's the core of Robin Hanson's argument for how far away the aliens are, and how do we know that? (*shrugs*)\n\n[Good Outcomes](https://youtu.be/gA1sNLL6yg4?t=3796)\n----------------------------------------------------\n\n**Ryan:** Eliezer, I know you're very skeptical that there will be a good outcome when we produce an artificial general intelligence. And I said when, not if, because I believe that's your thesis as well, of course. But is there the possibility of a good outcome? I know you are working on AI alignment problems, which leads me to believe that you have greater than zero amount of hope for this project. Is there the possibility of a good outcome? What would that look like, and how do we go about achieving it?\n\n**Eliezer: **It looks like me being wrong. I basically don't see on-model hopeful outcomes at this point. We have not done those things that it would take to earn a good outcome, and this is not a case where you get a good outcome by accident.\n\nIf you have a bunch of people putting together a new operating system, and they've heard about computer security, but they're skeptical that it's really that hard, the chance of them producing a [secure operating system](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) is effectively zero.\n\nThat's basically the situation I see ourselves in with respect to AI alignment. I have to be wrong about something—which I certainly am. I have to be wrong about something in a way that makes the problem *easier* rather than *harder* for those people who don't think that alignment's going to be all that hard.\n\nIf you're building a rocket for the first time ever, and you're wrong about something, it's not surprising if you're wrong about something. It's surprising if the thing that you're wrong about causes the rocket to go twice as high on half the fuel you thought was required and be much easier to steer than you were afraid of.\n\n**Ryan: **So, are you...\n\n**David: **Where the alternative was, “If you’re wrong about something, the rocket blows up.”\n\n**Eliezer: **Yeah. And then the rocket ignites the atmosphere, is the problem there.\n\nO rather: a bunch of rockets blow up, a bunch of rockets go places... The analogy I usually use for this is, very early on in the Manhattan Project, they were worried about “What if the nuclear weapons can ignite fusion in the nitrogen in the atmosphere?” And they ran some calculations and decided that it was incredibly unlikely for multiple angles, so they went ahead, and were correct. We're still here. I'm not going to say that it was luck, because the calculations were actually pretty solid.\n\nAn AI is like that, but instead of needing to refine plutonium, you can make nuclear weapons out of a billion tons of laundry detergent. The stuff to make them is fairly widespread. It's not a tightly controlled substance. And they spit out gold up until they get large enough, and *then* they ignite the atmosphere, and you can't calculate how large is large enough. And a bunch of the CEOs running these projects are making fun of the idea that it'll ignite the atmosphere.\n\nIt's not a very helpful situation.\n\n**David: **So the economic incentive to produce this AI—one of the things why ChatGPT has sparked the imaginations of so many people is that everyone can imagine products. Products are being imagined left and right about what you can do with something like ChatGPT. There's this meme at this point of people leaving to go start their ChatGPT startup.\n\nThe metaphor is that what you're saying is that there's this generally available resource spread all around the world, which is ChatGPT, and everyone's hammering it in order to make it spit out gold. But you're saying if we do that too much, all of a sudden the system will ignite the whole entire sky, and then we will all...\n\n**Eliezer: **Well, no. You can run ChatGPT any number of times without igniting the atmosphere. That's about what research labs at Google and Microsoft—counting DeepMind as part of Google and counting OpenAI as part of Microsoft—that's about what the research labs are doing, bringing more metaphorical Plutonium together than ever before. Not about how many times you run the things that have been built and not destroyed the world yet.\n\nYou can do any amount of stuff with ChatGPT and not destroy the world. It's not that smart. It doesn't get smarter every time you run it.\n\n[Ryan's Childhood Questions](https://youtu.be/gA1sNLL6yg4?t=4078)\n-----------------------------------------------------------------\n\n**Ryan: **Can I ask some questions that the 10-year-old in me wants to really ask about this? I'm asking these questions because I think a lot of listeners might be thinking them too, so knock off some of these easy answers for me.\n\nIf we create some sort of unaligned, let's call it “bad” AI, why can't we just create a whole bunch of good AIs to go fight the bad AIs and solve the problem that way? Can there not be some sort of counterbalance in terms of aligned human AIs and evil AIs, and there be some sort of battle of the artificial minds here?\n\n**Eliezer: **Nobody knows how to create any good AIs at all. The problem isn't that we have 20 good AIs and then somebody finally builds an evil AI. The problem is that the first very powerful AI is evil, nobody knows how to make it good, and then it kills everybody before anybody can make it good.\n\n**Ryan: **So there is no known way to make a friendly, human-aligned AI whatsoever, and you don't know of a good way to go about thinking through that problem and designing one. Neither does anyone else, is what you're telling us.\n\n**Eliezer: **I have some idea of what I would do if there were more time. Back in the day, we had more time. Humanity squandered it. I'm not sure there's enough time left now. I have some idea of what I would do if I were in a 25-year-old body and had $10 billion.\n\n**Ryan: **That would be the island scenario of “You're God for 10 years and you get all the researchers on an island and go really hammer for 10 years at this problem”?\n\n**Eliezer: **If I have buy-in from a major government that can run actual security precautions and more than just $10 billion, then you could run a whole Manhattan Project about it, sure.\n\n**Ryan: **This is another question that the 10-year-old in me wants to know. Why is it that, Eliezer, people listening to this episode or people listening to the concerns or reading the concerns that you've written down and published, why can't everyone get on board who's building an AI and just all agree to be very, very careful? Is that not a sustainable game-theoretic position to have? Is this a coordination problem, more of a social problem than anything else? Or, like, why can't that happen?\n\nI mean, we have so far not destroyed the world with nuclear weapons, and we've had them since the 1940s.\n\n**Eliezer: **Yeah, this is harder than nuclear weapons. This is a *lot* harder than nuclear weapons.\n\n**Ryan:** Why is this harder? And why can't we just coordinate to just all agree internationally that we're going to be very careful, put restrictions on this, put regulations on it, do something like that?\n\n**Eliezer: **Current heads of major labs seem to me to be openly contemptuous of these issues. That's where we're starting from. The politicians do not understand it.\n\nThere are distortions of these ideas that are going to sound more appealing to them than “everybody suddenly falls over dead”, which is a thing that I think actually happens. “Everybody falls over dead” just doesn't inspire the monkey political parts of our brain somehow. Because it's not like, “Oh no, what if terrorists get the AI first?” It's like, it doesn't matter who gets it first. Everybody falls over dead.\n\nAnd yeah, so you're describing a world coordinating on something that is relatively hard to coordinate. So, could we, if we tried starting today, prevent anyone from getting a billion pounds of laundry detergent in one place worldwide, control the manufacturing of laundry detergent, only have it manufactured in particular places, not concentrate lots of it together, enforce it on every country?\n\nY’know, if it was legible, if it was *clear* that a billion pounds of laundry detergent in one place would end the world, if you could calculate that, if all the scientists calculated it arrived at the same answer and told the politicians that maybe, maybe humanity would survive, even though smaller amounts of laundry detergent spit out gold.\n\nThe threshold can't be calculated. I don't know how you'd convince the politicians. We definitely don't seem to have had much luck convincing those CEOs whose job depends on them not caring, to care. Caring is easy to fake. It's easy to hire a bunch of people to be your “AI safety team”  and redefine “AI safety” as having the AI not say naughty words. Or, you know, I'm speaking somewhat metaphorically here for reasons.\n\nBut, you know, it's the basic problem that we have is like trying to build a secure OS before we run up against a really smart attacker. And there's all kinds of, like, fake security. “It's got a password file! This system is secure! It only lets you in if you type a password!” And if you never go up against a really smart attacker, if you never go far out of distribution against a powerful optimization process looking for holes, you know, then how does a bureaucracy come to know that what they're doing is not the level of computer security that they need? The way you're supposed to find this out, the way that scientific fields historically find this out, the way that fields of computer science historically find this out, the way that crypto found this out back in the early days, is by having the disaster happen! \n\nAnd we're not even that good at learning from relatively minor disasters! You know, like, COVID swept the world. Did the FDA or the CDC learn anything about “Don't tell hospitals that they're not allowed to use their own tests to detect the coming plague”? Are we installing UV-C lights in public spaces or in ventilation systems to prevent the next respiratory pandemic? You know, we lost a million people and we sure did not learn very much as far as I can tell for next time.\n\nWe could have an AI disaster that kills a hundred thousand people—how do you even *do* that? Robotic cars crashing into each other? Have a bunch of robotic cars crashing into each other! It's not going to look like that was the fault of artificial general intelligence because they're not going to put AGIs in charge of cars. They're going to pass a bunch of regulations that's going to affect the entire AGI disaster or not at all.\n\nWhat does the winning world even look like here? How in real life did we get from where we are now to this worldwide ban, including against North Korea and, you know, some one rogue nation whose dictator doesn't believe in all this nonsense and just wants the gold that these AIs spit out? How did we get there from here? How do we get to the point where the United States and China signed a treaty whereby they would both use nuclear weapons against Russia if Russia built a GPU cluster that was too large? How did we get there from here?\n\n**David: **Correct me if I'm wrong, but this seems to be kind of just like a topic of despair? I'm talking to you now and hearing your thought process about, like, there is no known solution and the trajectory's not great. Do you think all hope is lost here?\n\n**Eliezer: **I'll keep on fighting until the end, which I wouldn't do if I had literally zero hope. I could still be wrong about something in a way that makes this problem somehow much easier than it currently looks. I think that's how you go down fighting with dignity.\n\n**Ryan: **“Go down fighting with dignity.” That's the stage you think we're at.\n\nI want to just double-click on what you were just saying. Part of the case that you're making is humanity won't even see this coming. So it's not like a coordination problem like global warming where every couple of decades we see the world go up by a couple of degrees, things get hotter, and we start to see these effects over time. The characteristics or the advent of an AGI in your mind is going to happen incredibly quickly, and in such a way that we won't even see the disaster until it's imminent, until it's upon us...?\n\n**Eliezer: **I mean, if you want some kind of, like, formal phrasing, then I think that superintelligence will kill everyone before non-superintelligent AIs have killed one million people. I don't know if that's the phrasing you're looking for there.\n\n**Ryan: **I think that's a fairly precise definition, and why? What goes into that line of thought?\n\n**Eliezer: **I think that the current systems are actually very weak. I don't know, maybe I could use the analogy of Go, where you had systems that were finally competitive with the pros, where “pro” is like the set of ranks in Go, and then a year later, they were challenging the world champion and winning. And then another year, they threw out all the complexities and the training from human databases of Go games and built a new system, AlphaGo Zero, that trained itself from scratch. No looking at the human playbooks, no special-purpose code, just a general purpose game-player being specialized to Go, more or less.\n\nAnd, three days—there's a quote from Gwern about this, which I forget exactly, but it was something like, “We know how long AlphaGo Zero, or AlphaZero (two different systems), was equivalent to a human Go player. And it was, like, 30 minutes on the following floor of such-and-such DeepMind building.”\n\nMaybe the first system doesn't improve that quickly, and they build another system that does—And all of that with AlphaGo over the course of years, going from “it takes a long time to train” to “it trains very quickly and without looking at the human playbook”, that’s *not* with an artificial intelligence system that improves itself, or even that gets smarter as you run it, the way that human beings (not just as you evolve them, but as you run them over the course of their own lifetimes) improve.\n\nSo if the first system doesn't improve fast enough to kill everyone very quickly, they will build one that's meant to spit out more gold than that.\n\nAnd there could be weird things that happen before the end. I did not see ChatGPT coming, I did not see Stable Diffusion coming, I did not expect that we would have AIs smoking humans in rap battles before the end of the world. Ones that are clearly much dumber than us.\n\n**Ryan: **It’s kind of a nice send-off, I guess, in some ways.\n\n[Trying to Resist](https://youtu.be/gA1sNLL6yg4?t=4995)\n-------------------------------------------------------\n\n**Ryan:** So you said that your hope is not zero, and you are planning to fight to the end. What does that look like for you? I know you're working at MIRI, which is the Machine Intelligence Research Institute. This is a non-profit that I believe that you've set up to work on these AI alignment and safety issues. What are you doing there? What are you spending your time on? How do we actually fight until the end? If you do think that an end is coming, how do we try to resist?\n\n**Eliezer: **I'm actually on something of a sabbatical right now, which is why I have time for podcasts. It's a sabbatical from, you know, like, been doing this 20 years. It became clear we were all going to die. I felt kind of burned out, taking some time to rest at the moment. When I dive back into the pool, I don't know, maybe I will go off to Conjecture or Anthropic or one of the smaller concerns like Redwood Research—Redwood Research being the only ones I really trust at this point, but they're tiny—and try to figure out if *I* can see anything clever to do with the giant inscrutable matrices of floating point numbers.\n\nMaybe I just write, continue to try to explain in advance to people why this problem is hard instead of as easy and cheerful as the current people who think they're pessimists think it will be. I might not be working all that hard compared to how I used to work. I'm older than I was. My body is not in the greatest of health these days. Going down fighting doesn't necessarily imply that I have the stamina to fight all that hard. I wish I had prettier things to say to you here, but I do not.\n\n**Ryan: **No, this is... We intended to save probably the last part of this episode to talk about crypto, the metaverse, and AI and how this all intersects. But I gotta say, at this point in the episode, it all kind of feels pointless to go down that track.\n\nWe were going to ask questions like, well, in crypto, should we be worried about building sort of a property rights system, an economic system, a programmable money system for the AIs to sort of use against us later on? But it sounds like the easy answer from you to those questions would be, yeah, absolutely. And by the way, none of that matters regardless. You could do whatever you'd like with crypto. This is going to be the inevitable outcome no matter what.\n\nLet me ask you, what would you say to somebody listening who maybe has been sobered up by this conversation? If a version of you in your 20s does have the stamina to continue this battle and to actually fight on behalf of humanity against this existential threat, where would you advise them to spend their time? Is this a technical issue? Is this a social issue? Is it a combination of both? Should they educate? Should they spend time in the lab? What should a person listening to this episode do with these types of dire straits?\n\n**Eliezer: **I don't have really good answers. It depends on what your talents are. If you've got the very deep version of the [security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/), the part where you don't just put a password on your system so that nobody can walk in and directly misuse it, but the kind where you don't just encrypt the password file even though nobody's supposed to have access to the password file in the first place, and that's already an authorized user, but the part where you hash the passwords and salt the hashes. If you're the kind of person who can think of that from scratch, maybe take your hand at alignment.\n\nIf you can think of an alternative to the giant inscrutable matrices, then, you know, don't tell the world about that. I'm not quite sure where you go from there, but maybe you work with Redwood Research or something.\n\nA whole lot of this problem is that even if you do build an AI that's limited in some way, somebody else steals it, copies it, runs it themselves, and takes the bounds off the for loops and the world ends. \n\nSo there's that. You think you can do something clever *with* the giant inscrutable matrices? You're probably wrong. If you have the talent to try to figure out why you're wrong in advance of being hit over the head with it, and not in a way where you just make random far-fetched stuff up as the reason why it won't work, but where you can actually *keep looking* for the reason why it won't work...\n\nWe have people in crypto\\[graphy\\] who are good at breaking things, and they're the reason why *anything* is not on fire. Some of them might go into breaking AI systems instead, because that's where you learn anything.\n\nYou know: Any fool can build a crypto\\[graphy\\] system that they think will work. *Breaking* existing cryptographical systems is how we learn who the real experts are. So maybe the people finding weird stuff to do with AIs, maybe those people will come up with some truth about these systems that makes them easier to align than I suspect.\n\nHow do I put it... The saner outfits do have uses for money. They don't really have *scalable* uses for money, but they do burn any money literally at all. Like, if you gave MIRI a billion dollars, I would not know how to...\n\nWell, at a billion dollars, I might try to bribe people to move out of AI development, that gets broadcast to the whole world, and move to the equivalent of an island somewhere—not even to make any kind of critical discovery, but just to remove them from the system. If I had a billion dollars.\n\nIf I just have another $50 million, I'm not quite sure what to do with that, but if you donate that to MIRI, then you at least have the assurance that we will not randomly spray money on looking like we're doing stuff and we'll reserve it, as we are doing with the last giant crypto donation somebody gave us until we can figure out something to do with it that is actually helpful. And MIRI has that property. I would say probably Redwood Research has that property.\n\nYeah. I realize I'm sounding sort of disorganized here, and that's because I don't really have a good organized answer to how in general somebody goes down fighting with dignity.\n\n[MIRI and Education](https://youtu.be/gA1sNLL6yg4?t=5453)\n---------------------------------------------------------\n\n**Ryan: **I know a lot of people in crypto. They are not as in touch with artificial intelligence, obviously, as you are, and the AI safety issues and the existential threat that you've presented in this episode. They do care a lot and see coordination problems throughout society as an issue. Many have also generated wealth from crypto, and care very much about humanity not ending. What sort of things has MIRI, the organization I was talking about earlier, done with funds that you've received from crypto donors and elsewhere? And what sort of things might an organization like that pursue to try to stave this off?\n\n**Eliezer: **I mean, I think mostly we've pursued a lot of lines of research that haven't really panned out, which is a respectable thing to do. We did not know in advance that those lines of research would fail to pan out. If you're doing research that you know will work, you're probably not really doing any research. You're just doing a pretense of research that you can show off to a funding agency.\n\nWe try to be real. We did things where we didn't know the answer in advance. They didn't work, but that was where the hope lay, I think. But, you know, having a research organization that keeps it real that way, that's not an easy thing to do. And if you don't have this very deep form of the security mindset, you will end up producing fake research and doing more harm than good, so I would not tell all the successful cryptocurrency people to run off and start their own research outfits.\n\nRedwood Research—I'm not sure if they can scale using more money, but you can give people more money and wait for them to figure out how to scale it later if they're the kind who won't just run off and spend it, which is what MIRI aspires to be.\n\n**Ryan: **And you don't think the education path is a useful path? Just educating the world?\n\n**Eliezer: **I mean, I would give myself and MIRI credit for why the world isn't just walking blindly into the whirling razor blades here, but it's not clear to me how far education scales apart from that. You can get more people aware that we're walking directly into the whirling razor blades, because even if only 10% of the people can get it, that can still be a bunch of people. But then what do they do? I don't know. Maybe they'll be able to do something later.\n\nCan you get all the people? Can you get all the politicians? Can you get the people whose job incentives are against them admitting this to be a problem? I have various friends who report, like, “Ah yes, if you talk to researchers at OpenAI in *private*, they are very worried and say that they cannot be that worried in public.”\n\n[How Long Do We Have?](https://youtu.be/gA1sNLL6yg4?t=5640)\n-----------------------------------------------------------\n\n**Ryan: **This is all a giant [Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/) trap, is sort of what you're telling us. I feel like this is the part of the conversation where we've gotten to the end and the doctor has said that we have some sort of terminal illness. And at the end of the conversation, I think the patient, David and I, have to ask the question, “Okay, doc, how long do we have?” Seriously, what are we talking about here if you turn out to be correct? Are we talking about years? Are we talking about decades? What's your idea here?\n\n**David: **What are *you* preparing for, yeah?\n\n**Eliezer: **How the hell would I know? Enrico Fermi was saying that fission chan reactions were 50 years off if they could ever be done at all, two years before he built the first nuclear pile. The Wright brothers were saying heavier-than-air flight was 50 years off shortly before they built the first Wright flyer. How on earth would I know?\n\nIt could be three years. It could be 15 years. We could get that AI winter I was hoping for, and it could be 16 years. I'm not really seeing 50 without some kind of giant civilizational catastrophe. And to be clear, whatever civilization arises after that would probably, I'm guessing, end up stuck in just the same trap we are.\n\n**Ryan: **I think the other thing that the patient might do at the end of a conversation like this is to also consult with other doctors. I'm kind of curious who we should talk to on this quest. Who are some people that if people in crypto want to hear more about this or learn more about this, or even we ourselves as podcasters and educators want to pursue this topic, who are the other individuals in the AI alignment and safety space you might recommend for us to have a conversation with?\n\n**Eliezer: **Well, the person who actually holds a coherent technical view, who disagrees with me, is named Paul Christiano. He does not write Harry Potter fan fiction, and I expect him to have a harder time explaining himself in concrete terms. But that is the main technical voice of opposition. If you talk to other people in the effective altruism or AI alignment communities who disagree with this view, they are probably to some extent repeating back their misunderstandings of Paul Christiano's views. \n\nYou could try Ajeya Cotra, who's worked pretty directly with Paul Christiano and I think sometimes aspires to explain these things that Paul is not the best at explaining. I'll throw out Kelsey Piper as somebody who would be good at explaining—like, would not claim to be a technical person on these issues, but is good at explaining the part that she does know. \n\nWho else disagrees with me? I'm sure Robin Hanson would be happy to come on... well, I'm not sure he'd be happy to come on this podcast, but Robin Hanson disagrees with me, and I kind of feel like the [famous argument we had](https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate) back in the early 2010s, late 2000s about how this would all play out—I basically feel like this was the Yudkowsky position, this is the Hanson position, and then reality was over here, [well to the Yudkowsky side](https://intelligence.org/2017/10/20/alphago/) of the Yudkowsky position in the Yudkowsky-Hanson debate. But Robin Hanson does not feel that way, and would probably be happy to expound on that at length. \n\nI don't know. It's not hard to find opposing viewpoints. The ones that'll stand up to a few solid minutes of cross-examination from somebody who knows which parts to cross-examine, that's the hard part.\n\n[Bearish Hope](https://youtu.be/gA1sNLL6yg4?t=5895)\n---------------------------------------------------\n\n**Ryan: **You know, I've read a lot of your writings and listened to you on previous podcasts. One was in 2018 [on the Sam Harris podcast](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/). This conversation feels to me like the most dire you've ever seemed on this topic. And maybe that's not true. Maybe you've sort of always been this way, but it seems like the direction of your hope that we solve this issue has declined. I'm wondering if you feel like that's the case, and if you could sort of summarize your take on all of this as we close out this episode and offer, I guess, any concluding thoughts here.\n\n**Eliezer: **I mean, I don't know if you've got a time limit on this episode? Or is it just as long as it runs?\n\n**Ryan: **It's as long as it needs to be, and I feel like this is a pretty important topic. So you answer this however you want.\n\n**Eliezer: **Alright. Well, there was a conference one time on “What are we going to do about looming risk of AI disaster?”, and Elon Musk attended that conference. And I was like,: Maybe this is it. Maybe this is when the powerful people notice, and it's one of the relatively more technical powerful people who could be noticing this. And maybe this is where humanity finally turns and starts... not quite fighting back, because there isn't an external enemy here, but conducting itself with... I don't know. Acting like it cares, maybe?\n\nAnd what came out of that conference, well, was OpenAI, which was fairly nearly the worst possible way of doing anything. This is not a problem of “Oh no, what if secret elites get AI?” It's that nobody knows how to build the thing. If we *do* have an alignment technique, it's going to involve running the AI with a bunch of careful bounds on it where you don't just throw all the cognitive power you have at something. You have limits on the for loops. \n\nAnd whatever it is that could possibly save the world, like go out and turn all the GPUs and the server clusters into Rubik's cubes or something else that prevents the world from ending when somebody else builds another AI a few weeks later—anything that could do that is an artifact where somebody else could take it and take the bounds off the for loops and use it to destroy the world.\n\nSo let's open up everything! Let's accelerate everything! It was like GPT-3's version, though GPT-3 didn't exist back then—but it was like ChatGPT's blind version of throwing the ideals at a place where they were *exactly* the wrong ideals to solve the problem.\n\nAnd the problem is that demon summoning is easy and angel summoning is much harder. Open sourcing all the demon summoning circles is not the correct solution. And I'm using Elon Musk's own terminology here. He talked about AI as “summoning the demon”, which, not accurate, but—and then the solution was to put a demon summoning circle in every household. \n\nAnd, why? Because his friends were calling him Luddites once he'd expressed any concern about AI at all. So he picked a road that sounded like “openness” and “accelerating technology”! So his friends would stop calling him “Luddite”.\n\nIt was very much the worst—you know, maybe not the literal, actual worst possible strategy, but so very far pessimal.\n\nAnd that was it.\n\nThat was like... that was me in 2015 going like, “Oh. So this is what humanity will elect to do. We will not rise above. We will not have more grace, not even here at the very end.”\n\nSo that is, you know, that is when I did my crying late at night and then picked myself up and fought and fought and fought until I had run out all the avenues that I seem to have the capabilities to do. There's, like, more things, but they require scaling my efforts in a way that I've never been able to make them scale. And all of it's pretty far-fetched at this point anyways.\n\nSo, you know, that—so what's, you know, what's changed over the years? Well, first of all, I ran out some remaining avenues of hope. And second, things got to be such a disaster, such a *visible* disaster, the AI has got powerful enough and it became clear enough that, you know, we do not know how to align these things, that I could actually say what I've been thinking for a while and not just have people go completely, like, “What are you *saying* about all this?”\n\nYou know, now the stuff that was obvious back in 2015 is, you know, starting to become visible in the distance to others and not just completely invisible. That's what changed over time.\n\n[The End Goal](https://youtu.be/gA1sNLL6yg4?t=6230)\n---------------------------------------------------\n\n**Ryan: **What kind of... What do you hope people hear out of this episode and out of your comments? Eliezer in 2023, who is sort of running on the last fumes of, of hope. Yeah, what do you, what do you want people to get out of this episode? What are you planning to do?\n\n**Eliezer: **I don't have concrete hopes here. You know, when everything is in ruins, you might as well speak the truth, right? Maybe *somebody* hears it, *somebody* figures out something I didn't think of.\n\nI mostly expect that this does more harm than good in the modal universe, because a bunch of people are like, “Oh, I have this brilliant, clever idea,” which is, you know, something that I was arguing against in 2003 or whatever, but you know, maybe somebody out there with the proper level of pessimism hears and thinks of something I didn't think of.\n\nI suspect that if there's hope at all, it comes from a technical solution, because the difference between technical problems and political problems is at least the technical problems have solutions in principle. At least the technical problems are solvable. We're not on course to solve this one, but I think anybody who's hoping for a political solution has frankly not understood the technical problem. \n\nThey do not understand what it looks like to try to solve the political problem to such a degree that the world is not controlled by AI because they don't understand how easy it is to destroy the world with AI, given that the clock keeps ticking forward.\n\nThey're thinking that they just have to stop some bad actor, and that's why they think there's a political solution.\n\nBut yeah, I don't have concrete hopes. I didn't come on this episode out of any concrete hope.\n\nI have no takeaways except, like, don't make this thing worse.\n\nDon't, like, go off and accelerate AI more. Don't—f you have a brilliant solution to alignment, don't be like, “Ah yes, I have solved the whole problem. We just use the following clever trick.”\n\nYou know, “Don't make things worse” isn’t very much of a message, especially when you're pointing people at the field at all. But I have no winning strategy. Might as well go on this podcast as an experiment and say what I think and see what happens. And probably no good ever comes of it, but you might as well go down fighting, right?\n\nIf there's a world that survives, maybe it's a world that survives because of a bright idea somebody had after listening to listening to this podcast—that was *brighter*, to be clear, than the usual run of bright ideas that don't work.\n\n**Ryan: **Eliezer, I want to thank you for coming on and talking to us today. I do.\n\nI don't know if, by the way, you've seen that movie that David was referencing earlier, the movie *Don’t Look Up*, but I sort of feel like that news anchor, who's talking to the scientist—is it Leonardo DiCaprio, David? And, uh, the scientist is talking about kind of dire straits for the world. And the news anchor just really doesn't know what to do. I'm almost at a loss for words at this point.\n\n**David: **I've had nothing for a while now.\n\n**Ryan: **But one thing I can say is I appreciate your honesty. I appreciate that you've given this a lot of time and given this a lot of thought. Everyone, anyone who has heard you speak or read anything you've written knows that you care deeply about this issue and have given it a tremendous amount of your life force, in trying to educate people about it.\n\nAnd, um, thanks for taking the time to do that again today. I'll—I guess I'll just let the audience digest this episode in the best way they know how. But, um, I want to reflect everybody in crypto and everybody listening to Bankless—their thanks for you coming on and explaining.\n\n**Eliezer: **Thanks for having me. We'll see what comes of it.\n\n**Ryan: **Action items for you, Bankless nation. We always end with some action items. Not really sure where to refer folks to today, but one thing I know we can refer folks to is MIRI, which is the machine research intelligence institution that Eliezer has been talking about through the episode. That is at [intelligence.org](https://intelligence.org/), I believe. And some people in crypto have donated funds to this in the past. Vitalik Buterin is one of them. You can take a look at what they're doing as well. That might be an action item for the end of this episode.\n\nUm, got to end with risks and disclaimers—man, this seems very trite, but our legal experts have asked us to say these at the end of every episode. “Crypto is risky. You could lose everything...”\n\n**Eliezer: **(*laughs*)\n\n**David: **Apparently not as risky as AI, though.\n\n**Ryan: **—But we're headed west! This is the frontier. It's not for everyone, but we're glad you're with us on the Bankless journey. Thanks a lot.\n\n**Eliezer: **And we are grateful for the crypto community’s support. Like, it was possible to end with even less grace than this.\n\n**Ryan: **Wow. (*laughs*)\n\n**Eliezer: **And you made a difference.\n\n**Ryan: **We appreciate you.\n\n**Eliezer: **You really made a difference.\n\n**Ryan: **Thank you.\n\n* * *\n\n[Q&A](https://twitter.com/i/spaces/1PlJQpZogzVGE)\n-------------------------------------------------\n\n**Ryan: **\\[... Y\\]ou gave up this quote, from I think someone who's an executive director at MIRI: \"We've given up hope, but not the fight.\"\n\nCan you reflect on that for a bit? So it's still possible to fight this, even if we've given up hope? And even if you've given up hope? Do you have any takes on this?\n\n**Eliezer:** I mean, what else is there to do? You don't have good ideas. So you take your mediocre ideas, and your not-so-great ideas, and you pursue those until the world ends. Like, what's supposed to be better than that?\n\n**Ryan:** We had some really interesting conversation flow out of this episode, Eliezer, as you can imagine. And David and I want to relay some questions that the community had for you, and thank you for being gracious enough to help with those questions in today's Twitter Spaces.\n\nI'll read something from Luke ethwalker. \"Eliezer has one pretty flawed point in his reasoning. He assumes that AI would have no need or use for humans because we have atoms that could be used for better things. But how could an AI use these atoms without an agent operating on its behalf in the physical world? Even in his doomsday scenario, the AI relied on humans to create the global, perfect killing virus. That's a pretty huge hole in his argument, in my opinion.\"\n\nWhat's your take on this? That maybe AIs will dominate the digital landscape but because humans have a physical manifestation, we can still kind of beat the superintelligent AI in our physical world?\n\n**Eliezer:** If you were [an alien civilization](https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message) of a billion John von Neumanns, thinking at 10,000 times human speed, and you start out connected to the internet, you would want to not be just stuck on the internet, you would want to build that physical presence. You would not be content solely with working through human hands, despite the many humans who'd be lined up, cheerful to help you, you know. Bing already has its partisans. (*laughs*)\n\nYou wouldn’t be content with that, because the humans are very slow, glacially slow. You would like fast infrastructure in the real world, reliable infrastructure. And how do you build that, is then the question, and a whole lot of advanced analysis has been done on this question. I would point people again to Eric Drexler's *Nanosystems*.\n\nAnd, sure, if you literally start out connected to the internet, then probably the fastest way — maybe not the only way, but it's, you know, an easy way — is to get humans to do things. And then humans do those things. And then you have the desktop — not quite desktop, but you have the nanofactories, and then you don't need the humans anymore. And this need not be advertised to the world at large while it is happening.\n\n**David:** So I can understand that perspective, like in the future, we will have better 3D printers — distant in the future, we will have ways where the internet can manifest in the physical world. But I think this argument does ride on a future state with technology that we don't have today. Like, I don't think if I was the internet — and that kind of is this problem, right? Like, this superintelligent AI just becomes the internet because it's embedded in the internet. If I was the internet, how would I get myself to manifest in real life?\n\nAnd now, I am not an expert on the current state of robotics, or what robotics are connected to the internet. But I don't think we have too strong of tools today to start to create in the real world manifestations of an internet-based AI. So like, would you say that this part of this problem definitely depends on some innovation, at like the robotics level?\n\n**Eliezer:** No, it depends on the AI being smart. It doesn't depend on the humans having this technology; it depends on the AI being able to invent the technology.\n\nThis is, like, the central problem: the thing is smarter. Not in the way that the average listener to this podcast probably has an above average IQ, the way that humans are smarter than chimpanzees.\n\nWhat does that let humans do? Does it let humans be, like, really *clever* in how they play around with the stuff that's on the ancestral savanna? Make *clever* use of grass, *clever* use of trees?\n\nThe humans invent technology. They build the technology. The technology is not there until the humans invent it, the humans conceive it.\n\nThe problem is, humans are not the upper bound. We don't have the best possible brains for that kind of problem. So the existing internet is more than connected enough to people and devices, that you could build better technology than that if you had invented the technology because you were thinking much, much faster and better than a human does.\n\n**Ryan:** Eliezer, this is a question from stirs, a Bankless Nation listener. He wants to ask the question about your explanation of why the AI will undoubtedly kill us. That seems to be your conclusion, and I'm wondering if you could kind of reinforce that claim. Like, for instance — and this is something David and I discussed after the episode, when we were debriefing on this — why exactly wouldn't an AI, or couldn't an AI just blast off of the Earth and go somewhere more interesting, and leave us alone? Like, why does it have to take our atoms and reassemble them? Why can't it just, you know, set phasers to ignore?\n\n**Eliezer:** It could if it wanted to. But if it doesn't want to, there is some initial early advantage. You get to colonize the universe slightly earlier if you consume all of the readily accessible energy on the Earth's surface as part of your blasting off of the Earth process.\n\nIt would only need to care for us by a very tiny fraction to spare us, this I agree. Caring a very tiny fraction is basically the same problem as 100% caring. It's like, well, could you have a computer system that is usually like the Disk Operating System, but a tiny fraction of the time it's Windows 11? Writing that is just as difficult as writing Windows 11. We still have to write all the Windows 11 software. Getting it to care a tiny little bit is the same problem as getting it to care 100%.\n\n**Ryan:** So Eliezer, is this similar to the relationship that humans have with other animals, planet Earth? I would say largely we really don't... I mean, obviously, there's no animal Bill of Rights. Animals have no legal protection in the human world, and we kind of do what we want and trample over their rights. But it doesn't mean we necessarily kill all of them. We just largely ignore them.\n\nIf they're in our way, you know, we might take them out. And there have been whole classes of species that have gone extinct through human activity, of course; but there are still many that we live alongside, some successful species as well. Could we have that sort of relationship with an AI? Why isn't that reasonably high probability in your models?\n\n**Eliezer** So first of all, all these things are *just* metaphors. AI is not going to be exactly like humans to animals.\n\nLeaving that aside for a second, the reason why this metaphor breaks down is that although the humans are smarter than the chickens, we're not smarter than evolution, natural selection, cumulative optimization power over the last billion years and change. (You know, there's evolution before that but it's pretty slow, just, like, single-cell stuff.)\n\nThere are things that cows can do for us, that we cannot do for ourselves. In particular, make meat by eating grass. We’re smarter than the cows, but there's a thing that designed the cows; and we're faster than that thing, but we've been around for much less time. So we have not yet gotten to the point of redesigning the entire cow from scratch. And because of that, there's a purpose to keeping the cow around alive.\n\nAnd humans, furthermore, being the kind of funny little creatures that we are — some people care about cows, some people care about chickens. They're trying to fight for the cows and chickens having a better life, given that they have to exist at all. And there's a long complicated story behind that. It's not simple, the way that humans ended up in that \\[??\\]. It has to do with the particular details of our evolutionary history, and unfortunately it's not just going to pop up out of nowhere.\n\nBut I'm drifting off topic here. The basic answer to the question \"where does that analogy break down?\" is that I expect the superintelligences to be able to do better than natural selection, not just better than the humans.\n\n**David:** So I think your answer is that the separation between us and a superintelligent AI is orders of magnitude larger than the separation between us and a cow, or even us than an ant. Which, I think a large amount of this argument resides on this superintelligence explosion — just going up an exponential curve of intelligence very, very quickly, which is like the premise of superintelligence.\n\nAnd Eliezer, I want to try and get an understanding of... A part of this argument about \"AIs are going come kill us\" is buried in the Moloch problem. And Bankless listeners are pretty familiar with the concept of Moloch — the idea of coordination failure. The idea that the more that we coordinate and stay in agreement with each other, we actually create a larger incentive to defect.\n\nAnd the way that this is manifesting here, is that even if we do have a bunch of humans, which understand the AI alignment problem, and we all agree to only safely innovate in AI, to whatever degree that means, we still create the incentive for someone to fork off and develop AI faster, outside of what would be considered safe.\n\nAnd so I'm wondering if you could, if it does exist, give us the sort of lay of the land, of all of these commercial entities? And what, if at all, they're doing to have, I don't know, an AI alignment team?\n\nSo like, for example, OpenAI. Does OpenAI have, like, an alignment department? With all the AI innovation going on, what does the commercial side of the AI alignment problem look like? Like, are people trying to think about these things? And to what degree are they being responsible?\n\n**Eliezer:** It looks like OpenAI having a bunch of people who it pays to do AI ethics stuff, but I don't think they're plugged very directly into Bing. And, you know, they've got that department because back when they were founded, some of their funders were like, \"Well, but ethics?\" and OpenAI was like \"Sure, we can buy some ethics. We'll take this group of people, and we'll put them over here and we'll call them an alignment research department\".\n\nAnd, you know, the key idea behind ChatGPT is RLHF, which was invented by Paul Christiano. Paul Christiano had much more detailed ideas, and somebody might have reinvented this one, but anyway. I don't think that went through OpenAI, but I could be mistaken. Maybe somebody will be like \"Well, actually, Paul Christiano was working at OpenAI at the time\", I haven't checked the history in very much detail.\n\nA whole lot of the people who were most concerned with this \"ethics\" left OpenAI, and founded Anthropic. And I'm *still* not sure that Anthropic has sufficient leadership focus in that direction.\n\nYou know, like, put yourself in the shoes of a corporation! You can spend some little fraction of your income on putting together a department of people who will write safety papers. But then the actual behavior that we've seen, is they storm ahead, and they use one or two of the ideas that came out from anywhere in the whole \\[alignment\\] field. And they get as far as that gets them. And if that doesn't get them far enough, they just keep storming ahead at maximum pace, because, you know, Microsoft doesn't want to lose to Google, and Google doesn't want to lose to Microsoft.\n\n**David:** So it sounds like your attitude on the efforts of AI alignment in commercial entities is, like, they're not even doing 1% of what they need to be doing.\n\n**Eliezer:** I mean, they could spend \\[10?\\] times as much money and that would not get them to 10% of what they need to be doing.\n\nIt's not just a problem of “oh, they they could spend the resources, but they don't want to”. It's a question of “how do we even spend the resources to get the info that they need”.\n\nBut that said, not knowing how to do that, not really understanding that they need to do that, they are just charging ahead anyways.\n\n**Ryan:** Eliezer, is OpenAI the most advanced AI project that you're aware of?\n\n**Eliezer:** Um, no, but I'm not going to go name the competitor, because then people will be like, \"Oh, I should go work for them\", you know? I'd rather they didn't.\n\n**Ryan:** So it's like, OpenAI is this organization that was kind of — you were talking about it at the end of the episode, and for crypto people who aren't aware of some of the players in the field — were they spawned from that 2015 conference that you mentioned? It's kind of a completely open-source AI project?\n\n**Eliezer:** That was the original suicidal vision, yes. But...\n\n**Ryan:** And now they're bent on commercializing the technology, is that right?\n\n**Eliezer:** That's an improvement, but not enough of one, because they're still generating lots of noise and hype and directing more resources into the field, and storming ahead with the safety that they have instead of the safety that they need, and setting bad examples. And getting Google riled up and calling back in Larry Page and Sergey Brin to head up Google's AI projects and so on. So, you know, it could be worse! It would be worse if they were open sourcing all the technology. But what they're doing is still pretty bad.\n\n**Ryan:** What should they be doing, in your eyes? Like, what would be responsible use of this technology?\n\nI almost get the feeling that, you know, your take would be \"stop working on it altogether\"? And, of course, you know, to an organization like OpenAI that's going to be heresy, even if maybe that's the right decision for humanity. But what should they be doing?\n\n**Eliezer:** I mean, if you literally just made me dictator of OpenAI, I would change the name to \"ClosedAI\". Because right now, they're making it look like being \"closed\" is hypocrisy. They're, like, being \"closed\" while keeping the name \"OpenAI\", and that itself makes it looks like closure is like not this thing that you do cooperatively so that humanity will not die, but instead this sleazy profit-making thing that you do while keeping the name “OpenAI”.\n\nSo that's very bad; change the name to \"ClosedAI\", that's step one.\n\nNext. I don't know if they *can* break the deal with Microsoft. But, you know, cut that off. None of this. No more hype. No more excitement. No more getting famous and, you know, getting your status off of like, \"Look at how much closer *we* came to destroying the world! You know, we're not there yet. But, you know, we're at the *forefront* of destroying the world!\" You know, stop grubbing for the Silicon Valley bragging cred of visibly being the leader.\n\nTake it all closed. If you got to make money, make money selling to businesses in a way that doesn't generate a lot of hype and doesn't visibly push the field.And then try to figure out systems that are more alignable and not just more powerful. And at the end of that, they would fail, because, you know, it's not easy to do that. And the world would be destroyed. But they would have died with more dignity. Instead of being like, \"Yeah, yeah, let's like push humanity off the cliff ourselves for the ego boost!\", they would have done what they could, and then failed.\n\n**David:** Eliezer, do you think anyone who's building AI — Elon Musk, Sam Altman at OpenAI – do you think progressing AI is fundamentally bad?\n\n**Eliezer:** I mean, there are *narrow* forms of progress, especially if you *didn't open-source them*, that would be good. Like, you can imagine a thing that, like, pushes capabilities a bit, but is much more alignable.\n\nThere are people working in the field who I would say are, like, sort of *unabashedly* good. Like, Chris Olah is taking a microscope to these giant inscrutable matrices and trying to figure out what goes on inside there. Publishing that might possibly even push capabilities a little bit, because if people know what's going on inside there, they can make better ones. But the question of like, whether to closed-source *that* is, like, much more fraught than the question of whether to closed-source the stuff that's just pure capabilities.\n\nBut that said, the people who are just like, \"Yeah, yeah, let's do more stuff! And let's tell the world how we did it, so they can do it too!\" That's just, like, unabashedly bad.\n\n**David:** So it sounds like you do see paths forward in which we can develop AI in responsible ways. But it's really this open-source, open-sharing-of-information to allow anyone and everyone to innovate on AI,  that's really the path towards doom. And so we actually need to keep this knowledge private. Like, normally knowledge...\n\n**Eliezer:** No, no, no, no. Open-sourcing all this stuff is, like, a *less* dignified path straight off the edge. I'm not saying that all we need to do is keep everything closed and in the right hands and it will be fine. That will also kill you.\n\nBut that said, if you have stuff and you *do not know* how to make it not kill everyone, then broadcasting it to the world is even *less* dignified than being like, \"Okay, maybe we should *keep* working on this until we can figure out how to make it *not* kill everyone.\"\n\nAnd then the other people will, like, go storm ahead on *their* end and kill everyone. But, you know, you won't have *personally* slaughtered Earth. And that is more dignified.\n\n**Ryan:** Eliezer, I know I was kind of shaken after our episode, not having heard the full AI alignment story, at least listened to it for a while.\n\nAnd I think that in combination with the sincerity through which you talk about these subjects, and also me sort of seeing these things on the horizon, this episode was kind of shaking for me and caused a lot of thought.\n\nBut I'm noticing there is a cohort of people who are dismissing this take and your take specifically in this episode as Doomerism. This idea that every generation thinks it's, you know, the end of the world and the last generation.\n\nWhat's your take on this critique that, \"Hey, you know, it's been other things before. There was a time where it was nuclear weapons, and we would all end in a mushroom cloud. And there are other times where we thought a pandemic was going to kill everyone. And this is just the latest Doomerist AI death cult.\"\n\nI'm sure you've heard that before. How do you respond?\n\n**Eliezer:** That if you literally know nothing about nuclear weapons or artificial intelligence, except that somebody has claimed of both of them that they'll destroy the world, then sure, you can't tell the difference. As far as you can tell, nuclear weapons were claimed to destroy the world, and then they didn't destroy the world, and then somebody claimed that about AI.\n\nSo, you know, Laplace's rule of induction: at most a 1/3 probability that AI will destroy the world, if nuclear weapons and AI are the only case.\n\nYou can bring in so many more cases than that. Why, people should have known in the first place that nuclear weapons wouldn't destroy the world! Because their next door neighbor once said that the sky was falling, and that didn't happen; and if their next door weapon was \\[??\\], how could the people saying that nuclear weapons would destroy the world be right?\n\nAnd basically, as long as people are trying to run off of models of human psychology, to derive empirical information about the world, they're stuck. They're in a trap they can never get out of. They’re going to always be trying to psychoanalyze the people talking about nuclear weapons or whatever. And the only way you can actually get better information is by understanding how nuclear weapons work, understanding what the international equilibrium with nuclear weapons looks like. And the international equilibrium, by the way, is that nobody profits from setting off small numbers of nuclear weapons, especially given that they know that large numbers of nukes would follow. And, you know, that's why they haven't been used yet. There was nobody who made a buck by starting a nuclear war. The nuclear war was clear, the nuclear war was legible. People knew what would happen if they fired off all the nukes.\n\nThe analogy I sometimes try to use with artificial intelligence is, “Well, suppose that instead you could make nuclear weapons out of a billion pounds of laundry detergent. And they spit out gold until you make one that's too large, whereupon it ignites the atmosphere and kills everyone. *And* you can't calculate exactly how large is too large. *And* the international situation is that the private research labs spitting out gold don't want to hear about igniting the atmosphere.” And that's the technical difference. You need to be able to tell whether or not that is true as a scientific claim about how reality, the universe, the environment, artificial intelligence, actually works. What actually happens when the giant inscrutable matrices go past a certain point of capability? It's a falsifiable hypothesis.\n\nYou know, if it *fails* to be falsified, then everyone is dead, but that doesn't actually change the basic dynamic here, which is, you can't figure out how the world works by psychoanalyzing the people talking about it.\n\n**David:** One line of questioning that has come up inside of the Bankless Nation Discord is the idea that we need to train AI with data, lots of data. And where are we getting that data? Well, humans are producing that data. And when humans produce that data, by nature of the fact that it was produced by humans, that data has our human values embedded in it somehow, some way, just by the aggregate nature of all the data in the world, which was created by humans that have certain values. And then AI is trained on that data that has all the human values embedded in it. And so there's actually no way to create an AI that isn't trained on data that is created by humans, and that data has human values in it.\n\nIs there anything to this line of reasoning about a potential glimmer of hope here?\n\n**Eliezer:** There's a distant glimmer of hope, which is that an AI that is trained on tons of human data in this way probably understands some things about humans. And because of that, there's a branch of research hope within alignment, which is something that like, “Well, this AI, to be able to predict humans, needs to be able to predict the thought processes that humans are using to make their decisions. So can we thereby point to human values inside of the knowledge that the AI has?”\n\nAnd this is, like, very nontrivial, because the simplest theory that you use to predict what humans decide next, does not have what you might term “valid morality under reflection” as a clearly labeled primitive chunk inside it that is directly controlling the humans, and which you need to understand on a scientific level to understand the humans.\n\nThe humans are full of hopes and fears and thoughts and desires. And somewhere in all of that is what we call “morality”, but it's not a clear, distinct chunk, where an alien scientist examining humans and trying to figure out just purely on an empirical level “how do these humans work?” would need to point to one particular chunk of the human brain and say, like, \"Ahh, that circuit there, the morality circuit!\"\n\nSo it's not easy to point to inside the AI's understanding. There is not currently any obvious way to actually promote that chunk of the AI's understanding to then be in control of the AI's planning process. As it must be complicatedly pointed to, because it's not just a simple empirical chunk for explaining the world.\n\nAnd basically, I don't think that is actually going to be the route you should try to go down. You should try to go down something much simpler than that. The problem is not that we are going to fail to convey some *complicated subtlety* of human value. The problem is that we do not know how to align an AI on a task like “put two identical strawberries on a plate” without destroying the world.\n\n(Where by \"put two identical strawberries on the plate\", the concept is that's invoking enough power that it's not safe AI that can build two strawberries identical down to the cellular level. Like, that's a powerful AI. Aligning it isn't simple. If it's powerful enough to do that, it's also powerful enough to destroy the world, etc.)\n\n**David:** There's like a number of other lines of logic I could try to go down, but I think I would start to feel like I'm in the bargaining phase of death. Where it's like “Well, what about this? What about that?”\n\nBut maybe to summate all of the arguments, is to say something along the lines of like, \"Eliezer, how much room do you give for the long tail of black swan events? But these black swan events are actually us finding a solution for this thing.\" So, like, a reverse black swan event where we actually don't know how we solve this AI alignment problem. But really, it's just a bet on human ingenuity. And AI hasn't taken over the world *yet*. But there's space between now and then, and human ingenuity will be able to fill that gap, especially when the time comes?\n\nLike, how much room do you leave for the long tail of just, like, \"Oh, we'll discover a solution that we can't really see today\"?\n\n**Eliezer:** I mean, on the one hand, that hope is all that's left, and all that I'm pursuing. And on the other hand, in the process of actually pursuing that hope I do feel like I've gotten some feedback indicating that this hope is not necessarily very large.\n\nYou know, when you've got stage four cancer, is there still hope that your body will just rally and suddenly fight off the cancer? Yes, but it's not what usually happens. And I've seen people come in and try to direct their ingenuity at the alignment problem and most of them all invent the *same* small handful of bad solutions. And it's harder than usual to direct human ingenuity at this.\n\nA lot of them are just, like — you know, with capabilities ideas, you run out and try them and they mostly don't work. And some of them do work and you publish the paper, and you get your science \\[??\\], and you get your ego boost, and maybe you get a job offer someplace.\n\nAnd with the alignment stuff you can try to run through the analogous process, but the stuff we need to align is mostly not here yet. You can try to invent the smaller large language models that are public, you can go to work at a place that has access to larger large language models, you can try to do these very crude, very early experiments, and getting the large language models to at least not threaten your users with death —\n\n— *which isn't the same problem at all*. It just kind of looks related.\n\nBut you're at least trying to get AI systems that do what you want them to do, and not do other stuff; and that is, at the very core, a similar problem.\n\nBut the AI systems are not very powerful, they're not running into all sorts of problems that you can predict will crop up later. And people just, kind of — like, mostly people short out. They do pretend work on the problem. They're desperate to help, they got a grant, they now need to show the people who made the grant that they've made progress. They, you know, paper mill stuff.\n\nSo the human ingenuity is not functioning well right now. You cannot be like, \"Ah yes, this present field full of human ingenuity, which is working great, and coming up with lots of great ideas, and building up its strength, will continue at this pace and make it to the finish line in time!”\n\nThe capability stuff is *storming on* ahead. The human ingenuity that's being directed at that is much larger, but also it's got a much easier task in front of it.\n\nThe question is not \"Can human ingenuity ever do this at all?\" It's \"Can human ingenuity *finish* doing this before OpenAI blows up the world?\"\n\n**Ryan: **Well, Eliezer, if we can't trust in human ingenuity, is there any possibility that we can trust in AI ingenuity? And here's what I mean by this, and perhaps you'll throw a dart in this as being hopelessly naive.\n\nBut is there the possibility we could ask a reasonably intelligent, maybe almost superintelligent AI, how we might fix the AI alignment problem? And for it to give us an answer? Or is that really not how superintelligent AIs work?\n\n**Eliezer:** I mean, if you literally build a superintelligence and for some reason it was motivated to answer you, then sure, it could answer you.\n\nLike, if Omega comes along from a distant supercluster and offers to pay the local superintelligence lots and lots of money (or, like, mass or whatever) to give you a correct answer, then sure, it knows the correct answer; it can give you the correct answers.\n\nIf it *wants* to do that, you must have *already* solved the alignment problem. This reduces the problem of solving alignment to the problem of solving alignment. No progress has been made here.\n\nAnd, like, working on alignment is actually one of the most difficult things you could possibly try to align.\n\nLike, if I had the health and was trying to die with more dignity by building a system and aligning it as best I could figure out how to align it, I would be targeting something on the order of “build two strawberries and put them on a plate”. But instead of building two identical strawberries and putting them on a plate, you — don't actually do this, this is not the best thing you should do —\n\n— but if for example you could safely align “turning all the GPUs into Rubik's cubes”, then that *would* prevent the world from being destroyed two weeks later by your next follow-up competitor.\n\nAnd that's *much easier* to align an AI on than trying to get the AI to solve alignment for you. You could be trying to build something that would *just* think about nanotech, just think about the science problems, the physics problems, the chemistry problems, the synthesis pathways. \n\n(The open-air operation to find all the GPUs and turn them into Rubik's cubes would be harder to align, and that's why you shouldn't actually try to do that.)\n\nMy point here is: whereas \\[with\\] alignment, you've got to think about AI technology and computers and humans and intelligent adversaries, and distant superintelligences who might be trying to exploit your AI's imagination of those distant superintelligences, and ridiculous weird problems that would take so long to explain.\n\nAnd it just covers this enormous amount of territory, where you’ve got to understand how humans work, you've got to understand how adversarial humans might try to exploit and break an AI system — because if you're trying to build an aligned AI that's going to run out and operate in the real world, it would have to be resilient to those things.\n\nAnd they're just hoping that the AI is going to do their homework for them! But it's a chicken and egg scenario. And if you could actually get an AI to help you with something, you would not try to get it to help you with something as weird and not-really-all-that-effable as alignment. You would try to get it to help with something much simpler that could prevent the next AGI down the line from destroying the world.\n\nLike nanotechnology. There's a whole bunch of advanced analysis that's been done of it, and the *kind of thinking* that you have to do about it is so much more straightforward and so much less fraught than trying to, you know... And how do you even tell if it's lying about alignment?\n\nIt's hard to tell whether *I'm* telling you the truth about all this alignment stuff, right? Whereas if I talk about the tensile strength of sapphire, this is easier to check through the lens of logic.\n\n**David:** Eliezer, I think one of the reasons why perhaps this episode impacted Ryan – this was an analysis from a Bankless Nation community member — that this episode impacted Ryan a little bit more than it impacted me is because Ryan's got kids, and I don't. And so I'm curious, like, what do you think — like, looking 10, 20, 30 years in the future, where you see this future as inevitable, do you think it's futile to project out a future for the human race beyond, like, 30 years or so?\n\n**Eliezer**: Timelines are very hard to project. 30 years does strike me as unlikely at this point. But, you know, timing is famously much harder to forecast than saying that things can be done at all. You know, you got your people saying it will be 50 years out two years before it happens, and you got your people saying it'll be two years out 50 years before it happens. And, yeah, it's... Even if I knew *exactly* how the technology would be built, and *exactly* who was going to build it, I *still* wouldn't be able to tell you how long the project would take because of project management chaos.\n\nNow, since I don't know exactly the technology used, and I don't know exactly who's going to build it, and the project may not even have started yet, how can I possibly figure out how long it's going to take?\n\n**Ryan:** Eliezer, you've been quite generous with your time to the crypto community, and we just want to thank you. I think you've really opened a lot of eyes. This isn't going to be our last AI podcast at Bankless, certainly. I think the crypto community is going to dive down the rabbit hole after this episode. So thank you for giving us the 400-level introduction into it.\n\nAs I said to David, I feel like we waded straight into the deep end of the pool here. But that's probably the best way to address the subject matter. I'm wondering as we kind of close this out, if you could leave us — it is part of the human spirit to keep and to maintain slivers of hope here or there. Or as maybe someone you work with put it – to *fight the fight*, even if the hope is gone.\n\n100 years in the future, if humanity is still alive and functioning, if a superintelligent AI has not taken over, but we live in coexistence with something of that caliber — imagine if that's the case, 100 years from now. How did it happen?\n\nIs there some possibility, some sort of narrow pathway by which we can navigate this? And if this were 100 years from now the case, how could you imagine it would have happened?\n\n**Eliezer:** For one thing, I predict that if there's a glorious transhumanist future (as it is sometimes conventionally known) at the end of this, I don't predict it was there by getting like, “coexistence” with superintelligence. That's, like, some kind of weird, inappropriate analogy based off of humans and cows or something.\n\nI predict alignment was solved. I predict that if the humans are alive at all, that the superintelligences are being quite nice to them.\n\nI have basic moral questions about whether it's ethical for humans to have human children, if having transhuman children is an option instead. Like, these humans running around? Are they, like, the current humans who wanted eternal youth but, like, not the brain upgrades? Because I do see the case for letting an existing person choose \"No, I just want eternal youth and no brain upgrades, thank you.\" But then if you're deliberately having the equivalent of a very crippled child when you could just as easily have a not crippled child.\n\nLike, should humans in their present form be around together? Are we, like, kind of too sad in some ways? I have friends, to be clear, who disagree with me so much about this point. (*laughs*) But yeah, I'd say that the happy future looks like beings of light having lots of fun in a nicely connected computing fabric powered by the Sun, if we haven't taken the sun apart yet. Maybe there's enough real sentiment in people that you just, like, clear all the humans off the Earth and leave the entire place as a park. And even, like, maintain the Sun, so that the Earth is still a park even after the Sun would have ordinarily swollen up or dimmed down.\n\nYeah, like... That was always the things to be fought for. That was always the point, from the perspective of everyone who's been in this for a long time. Maybe not literally everyone, but like, the whole old crew.\n\n**Ryan:** That is a good way to end it: with some hope. Eliezer, thanks for joining the crypto community on this collectibles call and for this follow-up Q&A. We really appreciate it.\n\n**michaelwong.eth: **Yes, thank you, Eliezer.\n\n**Eliezer:** Thanks for having me.",
      "plaintextDescription": "Eliezer gave a very frank overview of his take on AI two weeks ago on the cryptocurrency show Bankless: \n\n\nI've posted a transcript of the show and a follow-up Q&A below.\n\nAndrea_Miotti, remember, and vonk previously posted transcripts to LessWrong (1,2 — see the comments there for discussion), but they contained many important errors, so I've corrected the transcripts below. (They also weren't available on the EA Forum.)\n\n----------------------------------------\n\n\nIntro\nEliezer Yudkowsky: [clip] I think that we are hearing the last winds start to blow, the fabric of reality start to fray. This thing alone cannot end the world, but I think that probably some of the vast quantities of money being blindly and helplessly piled into here are going to end up actually accomplishing something.\n\n \n\nRyan Sean Adams: Welcome to Bankless, where we explore the frontier of internet money and internet finance. This is how to get started, how to get better, how to front run the opportunity. This is Ryan Sean Adams. I'm here with David Hoffman, and we're here to help you become more bankless.\n\nOkay, guys, we wanted to do an episode on AI at Bankless, but I feel like David...\n\nDavid: Got what we asked for.\n\nRyan: We accidentally waded into the deep end of the pool here. And I think before we get into this episode, it probably warrants a few comments. I'm going to say a few things I'd like to hear from you too. But one thing I want to tell the listener is, don't listen to this episode if you're not ready for an existential crisis. Okay? I'm kind of serious about this. I'm leaving this episode shaken. And I don't say that lightly. In fact, David, I think you and I will have some things to discuss in the debrief as far as how this impacted you. But this was an impactful one. It sort of hit me during the recording, and I didn't know fully how to react. I honestly am coming out of this episode wanting to refute some of the claims made in this episode by our guest, Eliezer Yudkowsky, who ",
      "wordCount": 22530
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "svuBpoSduzhYjFPrA",
    "title": "Elements of Rationalist Discourse",
    "slug": "elements-of-rationalist-discourse",
    "url": null,
    "baseScore": 226,
    "voteCount": 97,
    "viewCount": null,
    "commentCount": 49,
    "createdAt": null,
    "postedAt": "2023-02-12T07:58:42.479Z",
    "contents": {
      "markdown": "I liked Duncan Sabien's [Basics of Rationalist Discourse](https://www.lesswrong.com/posts/XPv4sYrKnPzeJASuk/basics-of-rationalist-discourse-1), but it felt somewhat different from what my brain thinks of as \"*the* basics of rationalist discourse\". So I decided to write down my own version (which overlaps some with Duncan's).\n\nProbably this new version also won't match \"*the* basics\" as other people perceive them. People may not even agree that these are all good ideas! Partly I'm posting these just out of curiosity about what the delta is between my perspective on rationalist discourse and y'alls perspectives.\n\nThe basics of rationalist discourse, as I understand them:\n\n1. **Truth-Seeking. **Try to contribute to a social environment that encourages [belief](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re) [accuracy](https://www.lesswrong.com/posts/X3HpE8tMXz4m4w6Rz/the-simple-truth) and good [epistemic](https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality-1) [processes](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs/p/HcCpvYLoSFP4iAqSz). Try not to “[win](https://www.lesswrong.com/posts/9f5EXt8KNNxTAihtZ/a-rational-argument)” [arguments](https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/C8nEXTcjZb9oauTCW) using [symmetric](https://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/) weapons (tools that work similarly well whether you're right or wrong). Indeed, try not to treat [arguments](https://www.lesswrong.com/s/3ELrPerFTSo75WnrH/p/PeSzc9JTBxhaYRp9b) like [soldiers](https://www.lesswrong.com/posts/yFJ7vCjefBxnTchmG/outline-of-galef-s-scout-mindset) at all.\n\n2. **Non-Violence:** Argument gets [counter-argument](https://slatestarcodex.com/2013/12/29/the-spirit-of-the-first-amendment/). Argument does not get bullet. Argument does not get doxxing, death threats, or coercion.[^84y8cwc1yaa]\n\n3. **Non-Deception.** Never try to steer your conversation partners (or onlookers) toward having falser models. Where possible, avoid saying stuff that you expect to lower the net belief accuracy of the average reader; or failing that, at least flag that you're worried about this happening.\n\nAs a corollary:\n\n3.1. [**Meta-Honesty**](https://www.lesswrong.com/posts/xdwbX9pFEr7Pomaxv/meta-honesty-firming-up-honesty-around-its-edge-cases)**.** Make it easy for others to tell how honest, literal, PR-y, etc. you are (in general, or in particular contexts). This can include everything from \"prominently publicly discussing the sorts of situations in which you'd lie\" to \"tweaking your image/persona/tone/etc. to make it likelier that people will have the right [priors](https://www.lesswrong.com/tag/priors) about your honesty\".\n\n4. **Localizability.** Give people a social affordance to [decouple](https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms) / evaluate the [local](https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization) validity of claims. Decoupling is not *required*, and indeed context is often important and extremely worth talking about! But it should almost always be *OK* to locally address a specific point or subpoint, without necessarily weighing in on the larger context or suggesting you’ll engage further.\n\n5\\. **Alternative-Minding.** Consider [alternative](https://www.lesswrong.com/s/FrqfoG3LJeCZs96Ym/p/erGipespbbzdG5zYb) hypotheses, and ask yourself what [Bayesian](https://www.lesswrong.com/s/oFePMp9rKftEeZDDr/p/QkX2bAkwG2EpGvNug) [evidence](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK) you have that you're not in those alternative worlds. This mostly involves asking what models [retrodict](https://twitter.com/robbensinger/status/1524473598118834176).\n\nCultivate the skills of original [seeing](https://www.lesswrong.com/s/pmHZDpak4NeRLLLCw) and of seeing from new vantage points.\n\nAs a special case, try to understand and evaluate the alternative hypotheses that other people are advocating. Paraphrase stuff back to people to see if you [understood](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/sSqoEw9eRP2kPKLCz), and see if they think you pass their [Ideological Turing Test](https://www.econlib.org/archives/2011/06/the_ideological.html) on the relevant ideas.\n\nBe a fair bit more willing to consider nonstandard beliefs, frames/lenses, and methodologies, compared to (e.g.) the average academic. Keep in mind that inferential gaps can be [large](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK/p/HLqWn5LASfhhArZ7w), most life-experience is hard to transmit in a [small](https://www.lesswrong.com/posts/AJ9dX59QXokZb35fk/when-not-to-use-probabilities) number of [words](https://www.lesswrong.com/posts/4gDbqL3Tods8kHDqs/limits-to-legibility) (or in words at all), and converging on the truth can require a long process of cultivating the right mental motions, doing exercises, gathering and interpreting new data, etc.\n\nMake it a habit to explicitly distinguish \"what this person literally said\" from \"what I think this person means\". Make it a habit to explicitly distinguish \"what I think this person means\" from \"what I infer about this person as a result\".\n\n6. **Reality-Minding.** Keep your eye on the ball, [hug](https://www.lesswrong.com/posts/2jp98zdLo898qExrr/hug-the-query) the query, and don’t lose sight of [object-level](https://www.lesswrong.com/posts/Js34Ez9nrDeJCTYQL/politics-is-way-too-meta) [reality](https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing).\n\nMake it a habit to flag when you notice ways to test an assertion. Make it a habit to actually test claims, when the [value-of-information](https://www.lesswrong.com/tag/value-of-information) is high enough.\n\nReward scholarship, inquiry, [betting](https://marginalrevolution.com/marginalrevolution/2012/11/a-bet-is-a-tax-on-bullshit.html), pre-registered predictions, and sticking your neck out, especially where this is time-consuming, effortful, or socially risky.\n\n7. **Reducibility.** Err on the side of using simple, [concrete](https://www.lesswrong.com/posts/Lz64L3yJEtYGkzMzu/rationality-and-the-english-language), literal, and [precise](https://www.lesswrong.com/posts/yDfxTj9TKYsYiWH5o/the-virtue-of-narrowness) language. Make it a habit to [taboo](https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words) your words, do [reductionism](https://www.lesswrong.com/s/p3TndjYbdYaiWwm9x), explain what you mean, [define](https://www.lesswrong.com/posts/895quRDaK6gR2rM82/diseased-thinking-dissolving-questions-about-disease) your [terms](https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb), etc.\n\nAs a corollary, applying precision and naturalism your own cognition:\n\n7.1. **Probabilism. **Try to [quantify](https://www.lesswrong.com/s/zpCiuR4T343j9WkcK) your [uncertainty](https://www.lesswrong.com/s/FrqfoG3LJeCZs96Ym) to some degree.\n\n8. **Purpose-Minding.** Try not to lose [purpose](https://www.lesswrong.com/posts/sP2Hg6uPwpfp3jZJN/lost-purposes) (unless you're deliberately creating a sandbox for a more free-form and undirected stream of consciousness, based on some meta-purpose or impulse or hunch you want to follow).\n\nAsk yourself why you're having a conversation, and whether you want to do something [differently](https://www.lesswrong.com/posts/ZHWiCM4QmX8WwYajH/goal-factoring-1). Ask others what their goals are. Keep the [Void](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality) in view.\n\nAs a corollary:\n\n8.1. **Cruxiness.** Insofar as you have a sense of what the topic/goal of the conversation is, focus on [cruxes](https://www.lesswrong.com/tag/double-crux), or (if your goal shifts) consider explicitly flagging that you're tangenting or switching to a new conversational topic/goal.[^1f6sfa3ew9v]\n\n9. **Goodwill.** Reward others' good epistemic conduct (e.g., updating) more than most people naturally do. Err on the side of carrots over sticks, forgiveness over punishment, and civility over incivility, unless someone has explicitly set aside a weirder or more rough-and-tumble space.[^myb62zkh2d]\n\n10. **Experience-Owning. **Err on the side of explicitly [owning](https://medium.com/authenticrelating/owning-your-experience-21104b4a62b6) your experiences, mental states, beliefs, and impressions. Flag your inferences as inferences, and beware the [Mind Projection Fallacy](https://www.lesswrong.com/posts/ZTRiSNmeGQK8AkdN2/mind-projection-fallacy) and [Typical Mind Fallacy](https://www.lesswrong.com/posts/baTWMegR42PAsH9qJ/generalizing-from-one-example).\n\nAs a corollary:\n\n10.1. **Valence-Owning. **Err on the side of explicitly owning your [shoulds](https://www.lesswrong.com/posts/zqwWicCLNBSA5Ssmn/by-which-it-may-be-judged) and [desires](https://www.lesswrong.com/s/9bvAELWc8y2gYjRav). Err on the side of stating your wants and beliefs (and why you want or believe them) instead of (or in addition to) saying what you think people [ought](https://mindingourway.com/guilt/) to do.\n\nTry to phrase things in ways that make space for disagreement, and try to avoid socially pressuring people into doing things. Instead, as a strong default, approach people with an attitude of informing and [empowering](https://www.tumblr.com/theunitofcaring/177842591031/what-are-your-favorite-virtues) them to do what *they* want.\n\nFavor language with fewer and milder connotations, and make your arguments explicitly where possible, rather than relying excessively on the connotations, feel, [fnords](https://slatestarcodex.com/2014/05/24/nydwracus-fnords/), or vibes of your words.\n\n* * *\n\nA longer, less jargony version of this post is available [on the EA Forum](https://forum.effectivealtruism.org/posts/JnijsXwYDCJDwRcuc/elements-of-rationalist-discourse).\n\n[^84y8cwc1yaa]: Counter-arguments aren't the only OK response to an argument. You can choose not to reply. You can even ban someone because they keep making off-topic arguments, as long as you do this in a non-deceptive way. But some responses to arguments are explicitly off the table. \n\n[^1f6sfa3ew9v]: Note that \"the topic/goal of the conversation\" is an abstraction. \"Goals\" don't exist in a vacuum. You have goals (though these may not be perfectly stable, coherent, etc.), and other individuals have goals too. Conversations can be mutually beneficial when some of my goals are the same as some of yours, or when we have disjoint goals but some actions are useful for my goals as well as yours.Be wary of abstractions and unargued premises in this very list! Try to taboo these prescriptions and claims, paraphrase them back, figure out why I might be saying all this stuff, and explicitly ask yourself whether these norms serve your goals too.Part of why I've phrased this list as a bunch of noun phrases (\"purpose-minding\", etc.) rather than verb phrases (\"mind your purpose\", etc.) is that I suspect conversations will go better (on the dimension of goodwill and cheer) if people make a habit of saying \"hm, I think you violated the principle of experience-owning there\" or \"hm, your comment isn't doing the experience-owning thing as much as I'd have liked\", as opposed to \"own your experience!!\".But another part of why I used nouns is that commands aren't experience-owning, and can make it harder for people to mind their purposes. I do have imperatives in the post (mostly because the prose flowed better that way), but I want to encourage people to engage with the ideas and consider whether they make sense, rather than just blindly obey them. So I want people to come into this post engaging with these first as ideas to consider, rather than as commands to obey. \n\n[^myb62zkh2d]: Note that this doesn't require assuming everyone you talk to is honest or has good intentions.It does have some overlap with the rule of thumb \"as a very strong but defeasible default, carry on object-level discourse as if you were role-playing being on the same side as the people who disagree with you\".",
      "plaintextDescription": "I liked Duncan Sabien's Basics of Rationalist Discourse, but it felt somewhat different from what my brain thinks of as \"the basics of rationalist discourse\". So I decided to write down my own version (which overlaps some with Duncan's).\n\nProbably this new version also won't match \"the basics\" as other people perceive them. People may not even agree that these are all good ideas! Partly I'm posting these just out of curiosity about what the delta is between my perspective on rationalist discourse and y'alls perspectives.\n\nThe basics of rationalist discourse, as I understand them:\n\n \n\n1. Truth-Seeking. Try to contribute to a social environment that encourages belief accuracy and good epistemic processes. Try not to “win” arguments using symmetric weapons (tools that work similarly well whether you're right or wrong). Indeed, try not to treat arguments like soldiers at all.\n\n \n\n2. Non-Violence: Argument gets counter-argument. Argument does not get bullet. Argument does not get doxxing, death threats, or coercion.[1]\n\n \n\n3. Non-Deception. Never try to steer your conversation partners (or onlookers) toward having falser models. Where possible, avoid saying stuff that you expect to lower the net belief accuracy of the average reader; or failing that, at least flag that you're worried about this happening.\n\nAs a corollary:\n\n3.1. Meta-Honesty. Make it easy for others to tell how honest, literal, PR-y, etc. you are (in general, or in particular contexts). This can include everything from \"prominently publicly discussing the sorts of situations in which you'd lie\" to \"tweaking your image/persona/tone/etc. to make it likelier that people will have the right priors about your honesty\".\n\n \n\n4. Localizability. Give people a social affordance to decouple / evaluate the local validity of claims. Decoupling is not required, and indeed context is often important and extremely worth talking about! But it should almost always be OK to locally address a specific point or subpoint, with",
      "wordCount": 939
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tuwwLQT4wqk25ndxk",
    "title": "Thoughts on AGI organizations and capabilities work",
    "slug": "thoughts-on-agi-organizations-and-capabilities-work",
    "url": null,
    "baseScore": 102,
    "voteCount": 44,
    "viewCount": null,
    "commentCount": 17,
    "createdAt": null,
    "postedAt": "2022-12-07T19:46:04.004Z",
    "contents": {
      "markdown": "(*Note: This essay was largely written by Rob, based on notes from Nate. It’s formatted as Rob-paraphrasing-Nate because (a) Nate didn’t have time to rephrase everything into his own words, and (b) most of the impetus for this post came from Eliezer wanting MIRI to* [*praise a recent OpenAI post*](https://www.lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1) *and Rob wanting to share more MIRI-thoughts about the space of AGI organizations, so it felt a bit less like a Nate-post than usual.*)\n\n* * *\n\nNate and I have been happy about the AGI conversation seeming more honest and “real” recently. To contribute to that, I’ve collected some general Nate-thoughts in this post, even though they’re relatively informal and disorganized.\n\nAGI development is a critically important topic, and the world should obviously be able to hash out such topics in conversation. (Even though it can feel weird or intimidating, and even though there’s inevitably some social weirdness in sometimes saying negative things about people you like and sometimes collaborate with.) My hope is that we'll be able to make faster and better progress if we move the conversational norms further toward candor and substantive discussion of disagreements, as opposed to saying everything behind a veil of collegial obscurity.\n\nCapabilities work is currently a bad idea\n-----------------------------------------\n\nNate’s top-level view is that ideally, Earth should take a break on doing work that might move us closer to AGI, until we understand alignment better.\n\nThat move isn’t available to us, but individual researchers and organizations who choose not to burn the timeline are helping the world, *even if other researchers and orgs don't reciprocate*. You can unilaterally lengthen timelines, and give humanity more chances of success, by choosing not to personally shorten them.\n\nNate thinks capabilities work is currently a bad idea for a few reasons:\n\n*   He doesn’t buy that current capabilities work is a likely path to ultimately solving alignment.\n*   Insofar as current capabilities work does seem helpful for alignment, it strikes him as helping with parallelizable research goals, whereas our bottleneck is serial research goals. (See [A note about differential technological development](https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development).)\n*   Nate doesn’t buy that we *need *more capabilities progress before we can start finding a better path.\n\nThis is *not* to say that capabilities work is never useful for alignment, or that alignment progress is never bottlenecked on capabilities progress. As an extreme example, having a working AGI on hand tomorrow would indeed make it easier to run experiments that teach us things about alignment! But in a world where we build AGI tomorrow, we're dead, because we won't have time to get a firm understanding of alignment before AGI technology proliferates and someone accidentally destroys the world.[^6s6cxct97ko] Capabilities progress can be useful in various ways, while still being harmful on net.\n\n(Also, to be clear: AGI capabilities are obviously an essential part of humanity's long-term path to good outcomes, and it's important to develop them at some point — the sooner the better, once we're [confident](https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/BSee6LXg4adtrndwy) this will have good outcomes — and [it would be catastrophically bad to delay realizing them *forever*](https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/HoQ5Rp7Gs6rebusNP)*.*)\n\nOn Nate’s view, the field should do experiments with ML systems, not just abstract theory. But if he were magically in charge of the world's collective ML efforts, he would put a pause on further capabilities work until we've had more time to orient to the problem, consider the option space, and think our way to *some* sort of plan-that-will-actually-probably-work. It’s not as though we’re hurting for ML systems to study today, and our understanding already lags far behind today’s systems' capabilities.[^y6bcybdnyfr]\n\n*Publishing* capabilities advances is even more obviously bad\n-------------------------------------------------------------\n\nFor researchers who aren't willing to hit the pause button, an even more obvious (and cheaper) option is to avoid publishing any capabilities research (including results of the form \"it turns out that X can be done, though we won't say how we did it\").\n\nInformation can leak out over time, so \"do the work but don't publish about it\" still shortens AGI timelines in expectation. However, it can potentially shorten them a lot less.\n\nIn an ideal world, the field would currently be doing ~zero publishing of capabilities research — and marginal action to publish less is beneficial even if the rest of the world continues publishing.\n\nThoughts on the landscape of AGI organizations\n----------------------------------------------\n\nWith those background points in hand:\n\nNate was asked [earlier this year](https://twitter.com/orellanin/status/1543442309374689281) whether he agrees with Eliezer's negative[ takes](https://twitter.com/ESYudkowsky/status/1446562946717421568) on OpenAI. There's also been a good amount of recent discussion of OpenAI on[ LessWrong](https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai).\n\nNate tells me that his headline view of OpenAI is mostly the same as his view of other AGI organizations, so he feels a little odd singling out OpenAI. That said, here are his notes on OpenAI anyway:\n\n*   On Nate’s model, the effect of OpenAI is almost entirely dominated by its capabilities work (and sharing of its work), and this effect is robustly negative. (This is true for DeepMind, FAIR, and Google Brain too.)\n*   Nate thinks that DeepMind, OpenAI, Anthropic, FAIR, Google Brain, etc. should hit the pause button on capabilities work (or failing that, at least halt publishing). (And he thinks any one actor can unilaterally do good in the process, even if others aren't reciprocating.)\n*   On Nate’s model, OpenAI isn't close to operational adequacy in the sense of the [Six Dimensions of Operational Adequacy](https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects) write-up — which is another good reason to hold off on doing capabilities research. But this is again a property OpenAI shares with DeepMind, Anthropic, etc.\n\nInsofar as Nate or I think OpenAI is doing the wrong thing, we’re happy to criticize it.[^3bch7zrei8l] But, while this doesn't change the fact that we view OpenAI's effects as harmful on net currently, Nate does want to acknowledge that OpenAI seems to him to be doing *better* than some other orgs on a number of fronts:\n\n*   Nate liked a lot of things about the [OpenAI Charter](https://openai.com/charter/). (As did Eliezer, though compared to Eliezer, Nate saw the Charter as a more important positive sign about OpenAI's internal culture.)\n*   Nate would suspect that OpenAI is much better than Google Brain and FAIR (and comparable with DeepMind, and maybe a bit behind Anthropic? it's hard to judge these things from the outside) on some important adequacy dimensions, like research closure and operational security. (Though Nate worries that, e.g., he may hear more about efforts in these directions made by OpenAI than about DeepMind just by virtue of spending more time in the Bay.) \n*   Nate is also happy that Sam Altman and others at OpenAI talk to EAs/rationalists and try to resolve disagreements, and he’s happy that OpenAI has had people like Holden and Helen on their board at various points.\n*   Also, obviously, OpenAI (along with DeepMind and Anthropic) has put in a much clearer AGI alignment effort than Google, FAIR, etc. (Albeit Nate thinks the absolute amount of \"real\" alignment work is still small.)\n*   Most recently, Nate and Eliezer [both think it’s great](https://www.lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1) that OpenAI released a blog post that states their plan going forward, and we want to encourage DeepMind and Anthropic to do the same.[^2d3at0qsbw]\n\nComparatively, Nate thinks of OpenAI as being about on par with DeepMind, maybe a bit behind Anthropic (who publish less), and better than most of the other big names, in terms of attempts to take not-killing-everyone seriously. But again, Nate and I think that the overall effect of OpenAI (and DeepMind and FAIR and etc.) is bad, because we think it's dominated by \"shortens AGI timelines\". And we’re a little leery of playing “who's better on \\[x\\] dimension” when everyone seems to be on the floor of the [logistic success curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/).\n\nWe don't want \"here are a bunch of ways OpenAI is doing unusually well for its reference class\" to be treated as encouragement for those organizations to stay in the pool, or encouragement for others to join them in the pool. Outperforming DeepMind, FAIR, and Google on one or two dimensions is a weakly positive sign about the future, but on my model and Nate’s, it doesn't come close to outweighing the costs of \"adding another capabilities org to the world\".\n\n[^6s6cxct97ko]: Nate simultaneously endorses these four claims:1.  More capabilities would make it possible to learn some new things about alignment.2.  We can't do all the alignment work pre-AGI. Some trial-and-error and experience with working AGI systems will be required.3.  It can't all be trial-and-error, and it can't all be improvised post-AGI. Among other things, this is because:3.1.  Some errors kill you, and you need insight into which errors those are, and how to avoid them, in advance.3.2.  We’re likely to have at most a few years to upend the gameboard once AGI arrives. Figuring everything out under that level of time pressure seems unrealistic; we need to be going into the AGI regime with a solid background understanding, so that empirical work in the endgame looks more like \"nailing down a dozen loose ends and making moderate tweaks to a detailed plan\" rather than \"inventing an alignment field from scratch\".3.3.  AGI is likely to coincide with a sharp left turn, which makes it harder (and more dangerous) to rely on past empirical generalizations, especially ones that aren't backed by deep insight into AGI cognition.3.4.  Other points raised in AGI Ruin: A List of Lethalities.4.  If we end up able to do alignment, it will probably be because we figured out at least one major thing that we don't currently know, that isn't a part of the current default path toward advancing SotA or trying to build AGI ASAP with mainstream-ish techniques, and isn't dependent on such progress. \n\n[^y6bcybdnyfr]: And, again, small individual “don’t burn the timeline” actions all contribute to incrementally increasing the time humanity has to get its act together and figure this stuff out. You don’t actually need coordination in order to have a positive effect in this way.And, to reiterate: I say \"pause\" rather than \"never build AGI at all\" because MIRI leadership thinks that humanity never building AGI would mean the loss of nearly all of the future's value. If this were a live option, it would be an unacceptably bad one. \n\n[^3bch7zrei8l]: Nate tells me that his current thoughts on OpenAI are probably a bit less pessimistic than Eliezer's. As a rule, Nate thinks of himself as generally less socially cynical than Eliezer on a bunch of fronts, though not less-cynical enough to disagree with the basic conclusions.Nate tells me that he agrees with Eliezer that the original version of OpenAI (\"an AGI in every household\", the associated social drama, etc.) was a pretty negative shock in the wake of the camaraderie of the 2015 Puerto Rico conference.At this point, of course, the founding of OpenAI is a sunk cost. So Nate mostly prefers to assess OpenAI's current state and future options.Currently, Nate thinks that OpenAI is trying harder than most on some important safety fronts — though none of this reaches the standards of \"adequate project\" and we're still totally going to die if they meet great success along their current path.Since I’ve listed various positives about OpenAI here, I'll note some examples of recent-ish developments that made Nate less happy about OpenAI: his sense that OpenAI was less interested in Paul Christiano's research, Evan Hubinger's research, etc. than he thought they should have been, when Paul was at OpenAI; Dario's decision to leave OpenAI; and OpenAI focusing on the “use AI to solve AI alignment” approach (as opposed to other possible strategies), as endorsed by e.g. Jan Leike, the head of OpenAI’s safety team after Paul's departure. \n\n[^2d3at0qsbw]: If a plan doesn't make sense, the research community can then notice this and apply corrective arguments, causing the plan to change. As indeed happened when Elon and Sam stated their more-obviously-bad plan for OpenAI at the organization's inception.It would have been better to state their plan first and start an organization later, so rounds of critical feedback and updating could occur before you lock in decisions about hiring, org structure, name, culture, etc.But at least it happened at all; if OpenAI had just said \"yeah, we're gonna do alignment research!\" and left it there, the outcome probably would have been far worse.Also, if organizations release obviously bad plans but are then unresponsive to counter-arguments, researchers can go work at the orgs with better plans and avoid the orgs with worse plans. This encourages groups to compete to have the seemingly-sanest plan, which strikes me as a better equilibrium than the current one.",
      "plaintextDescription": "(Note: This essay was largely written by Rob, based on notes from Nate. It’s formatted as Rob-paraphrasing-Nate because (a) Nate didn’t have time to rephrase everything into his own words, and (b) most of the impetus for this post came from Eliezer wanting MIRI to praise a recent OpenAI post and Rob wanting to share more MIRI-thoughts about the space of AGI organizations, so it felt a bit less like a Nate-post than usual.)\n\n----------------------------------------\n\nNate and I have been happy about the AGI conversation seeming more honest and “real” recently. To contribute to that, I’ve collected some general Nate-thoughts in this post, even though they’re relatively informal and disorganized.\n\nAGI development is a critically important topic, and the world should obviously be able to hash out such topics in conversation. (Even though it can feel weird or intimidating, and even though there’s inevitably some social weirdness in sometimes saying negative things about people you like and sometimes collaborate with.) My hope is that we'll be able to make faster and better progress if we move the conversational norms further toward candor and substantive discussion of disagreements, as opposed to saying everything behind a veil of collegial obscurity.\n\n \n\n\nCapabilities work is currently a bad idea\nNate’s top-level view is that ideally, Earth should take a break on doing work that might move us closer to AGI, until we understand alignment better.\n\nThat move isn’t available to us, but individual researchers and organizations who choose not to burn the timeline are helping the world, even if other researchers and orgs don't reciprocate. You can unilaterally lengthen timelines, and give humanity more chances of success, by choosing not to personally shorten them.\n\nNate thinks capabilities work is currently a bad idea for a few reasons:\n\n * He doesn’t buy that current capabilities work is a likely path to ultimately solving alignment.\n * Insofar as current capabilities work do",
      "wordCount": 1359
    },
    "tags": [
      {
        "_id": "txkDg4aLmiRq8wsSu",
        "name": "Organizational Culture & Design",
        "slug": "organizational-culture-and-design"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tD9zEiHfkvakpnNam",
    "title": "A challenge for AGI organizations, and a challenge for readers",
    "slug": "a-challenge-for-agi-organizations-and-a-challenge-for-1",
    "url": null,
    "baseScore": 302,
    "voteCount": 148,
    "viewCount": null,
    "commentCount": 33,
    "createdAt": null,
    "postedAt": "2022-12-01T23:11:44.279Z",
    "contents": {
      "markdown": "*(Note: This post is a write-up by Rob of a point Eliezer wanted to broadcast. Nate helped with the editing, and endorses the post’s main points.)*\n\nEliezer Yudkowsky and Nate Soares (my co-workers) want to broadcast strong support for OpenAI’s recent decision to release a blog post (\"[Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research/)\") that states their current plan as an organization.\n\nAlthough Eliezer and Nate disagree with OpenAI's proposed approach — a variant of \"use relatively unaligned AI to align AI\" — they view it as very important that OpenAI *has a plan* and has said what it is.\n\nWe want to challenge Anthropic and DeepMind, the other major AGI organizations with a stated concern for existential risk, to do the same: come up with a plan (possibly a branching one, if there are crucial uncertainties you expect to resolve later), write it up in some form, and publicly announce that plan (with sensitive parts fuzzed out) as the organization's current alignment plan.\n\nCurrently, Eliezer’s impression is that neither Anthropic nor DeepMind has a secret plan that's better than OpenAI's, nor a secret plan that's worse than OpenAI's. His impression is that they don't have a plan at all.[^jajubgo6rle]\n\nHaving a plan is critically important for an AGI project, not because anyone should expect everything to play out as planned, but because plans force the project to concretely state their crucial assumptions in one place. This provides an opportunity to notice and address inconsistencies, and to notice updates to the plan (and fully propagate those updates to downstream beliefs, strategies, and policies) as new information comes in.\n\nIt's also healthy for the field to be able to debate plans and think about the big picture, and for orgs to be in some sense \"competing\" to have the most sane and reasonable plan.\n\nWe acknowledge that there are reasons organizations might want to be *abstract* about some steps in their plans — e.g., to avoid immunizing people to good-but-weird ideas, in a public document where it’s hard to fully explain and justify a chain of reasoning; or to avoid sharing capabilities insights, if parts of your plan depend on your inside-view model of how AGI works.\n\nWe’d be happy to see plans that fuzz out some details, but are still much more concrete than (e.g.) “figure out how to build AGI and expect this to go well because we'll be particularly conscientious about safety once we have an AGI in front of us\".\n\nEliezer also hereby gives a challenge to the reader: Eliezer and Nate are thinking about writing up their thoughts at some point about OpenAI's plan of using AI to aid AI alignment. We want you to write up your own unanchored thoughts on the OpenAI plan first, focusing on the most important and decision-relevant factors, with the intent of rendering our posting on this topic superfluous.\n\nOur hope is that challenges like this will test how superfluous we are, and also move the world toward a state where we’re more superfluous / there’s more redundancy in the field when it comes to generating ideas and critiques that would be lethal for the world to never notice.[^2l6fx16h4xj][^nzkw454v5t]  \n \n\n[^jajubgo6rle]: We didn't run a draft of this post by DM or Anthropic (or OpenAI), so this information may be mistaken or out-of-date. My hope is that we’re completely wrong!Nate’s personal guess is that the situation at DM and Anthropic may be less “yep, we have no plan yet”, and more “various individuals have different plans or pieces-of-plans, but the organization itself hasn’t agreed on a plan and there’s a lot of disagreement about what the best approach is”.In which case Nate expects it to be very useful to pick a plan now (possibly with some conditional paths in it), and make it a priority to hash out and document core strategic disagreements now rather than later. \n\n[^2l6fx16h4xj]: Nate adds: “This is a chance to show that you totally would have seen the issues yourselves, and thereby deprive MIRI folk of the annoying ‘y'all'd be dead if not for MIRI folk constantly pointing out additional flaws in your plans’ card!” \n\n[^nzkw454v5t]: Eliezer adds:  \"For this reason, please note explicitly if you're saying things that you heard from a MIRI person at a gathering, or the like.\"",
      "plaintextDescription": "(Note: This post is a write-up by Rob of a point Eliezer wanted to broadcast. Nate helped with the editing, and endorses the post’s main points.)\n\n \n\nEliezer Yudkowsky and Nate Soares (my co-workers) want to broadcast strong support for OpenAI’s recent decision to release a blog post (\"Our approach to alignment research\") that states their current plan as an organization.\n\nAlthough Eliezer and Nate disagree with OpenAI's proposed approach — a variant of \"use relatively unaligned AI to align AI\" — they view it as very important that OpenAI has a plan and has said what it is.\n\nWe want to challenge Anthropic and DeepMind, the other major AGI organizations with a stated concern for existential risk, to do the same: come up with a plan (possibly a branching one, if there are crucial uncertainties you expect to resolve later), write it up in some form, and publicly announce that plan (with sensitive parts fuzzed out) as the organization's current alignment plan.\n\nCurrently, Eliezer’s impression is that neither Anthropic nor DeepMind has a secret plan that's better than OpenAI's, nor a secret plan that's worse than OpenAI's. His impression is that they don't have a plan at all.[1]\n\nHaving a plan is critically important for an AGI project, not because anyone should expect everything to play out as planned, but because plans force the project to concretely state their crucial assumptions in one place. This provides an opportunity to notice and address inconsistencies, and to notice updates to the plan (and fully propagate those updates to downstream beliefs, strategies, and policies) as new information comes in.\n\nIt's also healthy for the field to be able to debate plans and think about the big picture, and for orgs to be in some sense \"competing\" to have the most sane and reasonable plan.\n\nWe acknowledge that there are reasons organizations might want to be abstract about some steps in their plans — e.g., to avoid immunizing people to good-but-weird ideas, in a public docum",
      "wordCount": 527
    },
    "tags": [
      {
        "_id": "KoXbd2HmbdRfqLngk",
        "name": "Planning & Decision-Making",
        "slug": "planning-and-decision-making"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "nKtyrL5u4Y5kmMWT5",
        "name": "DeepMind",
        "slug": "deepmind"
      },
      {
        "_id": "H4n4rzs33JfEgkf8b",
        "name": "OpenAI",
        "slug": "openai"
      },
      {
        "_id": "aHay2tebonHAYKtac",
        "name": "Anthropic (org)",
        "slug": "anthropic-org"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gJtbrjtPzsuRx8tNK",
    "title": "A common failure for foxes",
    "slug": "a-common-failure-for-foxes",
    "url": null,
    "baseScore": 47,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2022-10-14T22:50:59.614Z",
    "contents": {
      "markdown": "A common failure mode for people who pride themselves in being [foxes](https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox) (as opposed to hedgehogs):\n\nPaying more attention to easily-evaluated claims that don't matter much, at the expense of hard-to-evaluate claims that matter a lot.\n\nE.g., maybe there's an RCT that isn't very relevant, but is pretty easily interpreted and is conclusive evidence for some claim. At the same time, maybe there's an informal argument that matters a lot more, but it takes some work to know how much to update on it, and it probably won't be iron-clad evidence regardless.\n\nI think people who think of themselves as being \"foxes\" often spend too much time thinking about the RCT and not enough time thinking about the informal argument, for a few reasons:\n\n1\\. **A desire for cognitive closure, confidence, and a feeling of \"knowing things\"** — of having authoritative Facts on hand rather than mere Opinions.\n\nA proper Bayesian cares about [VOI](https://www.lesswrong.com/tag/value-of-information), and assigns probabilities rather than having separate mental buckets for Facts vs. Opinions. If activity A updates you from 50% to 95% confidence in hypothesis H1, and activity B updates you from 50% to 60% confidence in hypothesis H2, then your assessment of whether to do more A-like activities or more B-like activities going forward should normally depend a lot on how useful it is to know about H1 versus H2.\n\nBut real-world humans (even if they think of themselves as aspiring Bayesians) are often uncomfortable with uncertainty. We prefer sharp thresholds, capital-k Knowledge, and a feeling of having solid ground to rest on.\n\n2\\. **Hyperbolic discounting of intellectual progress.**\n\nWith unambiguous data, you get a fast sense of progress. With fuzzy arguments, you might end up confident after thinking about it a while, or after reading another nine arguments; but it's a long process, with uncertain rewards.\n\n**3\\. Social** [**modesty**](https://equilibriabook.com/toc) **and a desire to look un-arrogant.**\n\nIt can feel socially low-risk and pleasantly virtuous to be able to say \"Oh, I'm not claiming to have good judgment or to be great at reasoning or anything; I'm just deferring to the obvious clear-cut data, and outside of that, I'm totally uncertain.\"\n\nCollecting isolated facts increases the pool of authoritative claims you can make, while protecting you from having to stick your neck out and have an Opinion on something that will be harder to convince others of, or one that rests on an implicit claim about your judgment.\n\nBut in fact it often is better to make small or uncertain updates about extremely important questions, than to collect lots of high-confidence trivia. It keeps your eye on the ball, where you can keep building up confidence over time; and it helps build reasoning skill.\n\nHigh-confidence trivia also often poses a risk: either consciously or unconsciously, you can end up updating about the More Important Questions you really care about, because you're spending all your time thinking about trivia.\n\nEven if you verbally acknowledge that updating from the superficially-related RCT to the question-that-actually-matters would be a non sequitur, there's still a temptation to substitute the one question for the other. Because it's still the Important Question that you actually care about.",
      "plaintextDescription": "A common failure mode for people who pride themselves in being foxes (as opposed to hedgehogs):\n\nPaying more attention to easily-evaluated claims that don't matter much, at the expense of hard-to-evaluate claims that matter a lot.\n\nE.g., maybe there's an RCT that isn't very relevant, but is pretty easily interpreted and is conclusive evidence for some claim. At the same time, maybe there's an informal argument that matters a lot more, but it takes some work to know how much to update on it, and it probably won't be iron-clad evidence regardless.\n\nI think people who think of themselves as being \"foxes\" often spend too much time thinking about the RCT and not enough time thinking about the informal argument, for a few reasons:\n\n \n\n1. A desire for cognitive closure, confidence, and a feeling of \"knowing things\" — of having authoritative Facts on hand rather than mere Opinions.\n\nA proper Bayesian cares about VOI, and assigns probabilities rather than having separate mental buckets for Facts vs. Opinions. If activity A updates you from 50% to 95% confidence in hypothesis H1, and activity B updates you from 50% to 60% confidence in hypothesis H2, then your assessment of whether to do more A-like activities or more B-like activities going forward should normally depend a lot on how useful it is to know about H1 versus H2.\n\nBut real-world humans (even if they think of themselves as aspiring Bayesians) are often uncomfortable with uncertainty. We prefer sharp thresholds, capital-k Knowledge, and a feeling of having solid ground to rest on.\n\n \n\n2. Hyperbolic discounting of intellectual progress.\n\nWith unambiguous data, you get a fast sense of progress. With fuzzy arguments, you might end up confident after thinking about it a while, or after reading another nine arguments; but it's a long process, with uncertain rewards.\n\n \n\n3. Social modesty and a desire to look un-arrogant.\n\nIt can feel socially low-risk and pleasantly virtuous to be able to say \"Oh, I'm not claiming to hav",
      "wordCount": 524
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MdZyLnLHuaHrCskjy",
    "title": "ITT-passing and civility are good; \"charity\" is bad; steelmanning is niche",
    "slug": "itt-passing-and-civility-are-good-charity-is-bad",
    "url": null,
    "baseScore": 163,
    "voteCount": 79,
    "viewCount": null,
    "commentCount": 36,
    "createdAt": null,
    "postedAt": "2022-07-05T00:15:36.308Z",
    "contents": {
      "markdown": "*This post has been recorded as part of the LessWrong Curated Podcast, and can be listened to on* [*Spotify*](https://open.spotify.com/episode/7kbiGrZFQmMnc7o9o1MwIy)*,* [*Apple Podcasts*](https://podcasts.apple.com/us/podcast/itt-passing-and-civility-are-good-charity-is-bad/id1630783021?i=1000571025793)*, and* [*Libsyn*](https://sites.libsyn.com/421877/itt-passing-and-civility-are-good-charity-is-bad-steelmanning-is-niche-by-rob-bensinger)*.*\n\n* * *\n\nI often object to claims like \"charity/steelmanning is an argumentative virtue\". This post collects a few things I and others have said on this topic over the last few years.\n\nMy current view is:\n\n*   [**Steelmanning**](https://themerelyreal.wordpress.com/2012/12/07/steelmanning/) (\"the art of addressing the best form of the other person’s argument, even if it’s not the one they presented\") is a useful niche skill, but I don't think it should be a standard thing you bring out in most arguments, even if it's an argument with someone you strongly disagree with.\n*   Instead, arguments should mostly be organized around things like:\n    *   Object-level learning and truth-seeking, with the conversation as a convenient excuse to improve your own model of something you're curious about.\n    *   Trying to pass each other's [**Ideological Turing Test**](https://www.econlib.org/archives/2011/06/the_ideological.html) (ITT), or some generalization thereof. The ability to pass ITTs is the ability \"to state opposing views as clearly and persuasively as their proponents\".\n        *   The version of \"ITT\" I care about is one where you understand the *substance* of someone's view well enough to be able to correctly describe their beliefs and reasoning; I don't care about whether you can imitate their speech patterns, jargon, etc.\n    *   Trying to identify and resolve [**cruxes**](https://www.lesswrong.com/posts/BmyoYkr7u2oas4ukn/keep-your-beliefs-cruxy): things that would make one or the other of you (or both) change your mind about the topic under discussion.\n*   **Argumentative charity** is a complete mess of a concept⁠—people use it to mean a wide variety of things, and many of those things are actively bad, or liable to cause severe epistemic distortion and miscommunication.\n*   Some version of **civility** and/or **friendliness** and/or **a spirit of camaraderie and goodwill** seems like a useful ingredient in many discussions. I'm not sure how best to achieve this in ways that are emotionally honest (\"pretending to be cheerful and warm when you don't feel that way\" sounds like the wrong move to me), or how to achieve this without steering away from candor, openness, \"realness\", etc.\n\nI've [said](https://twitter.com/robbensinger/status/1542637242568454145) that I think people should be \"nicer and also ruder\". [And](https://twitter.com/robbensinger/status/1521323264777224192):\n\n> The sweet spot for EA PR is something like: 'friendly, nuanced, patient, and totally unapologetic about being a fire hose of inflammatory hot takes'. 🙂\n\nI have an intuition that those are pieces of the puzzle, along with (certain aspects or interpretations of) NVC tech, circling tech, introspection tech, etc. But I'm not sure how to hit the right balance in general.\n\nI do feel very confident that \"steelmanning\" and \"charity\" *aren't* the right tech for achieving this goal. (Because \"charity\" is a bad meme, and \"steelmanning\" is a lot more niche than that.)\n\nThings other people have said\n-----------------------------\n\nOzy Brennan wrote [Against Steelmanning](https://thingofthings.wordpress.com/2016/08/09/against-steelmanning/comment-page-1/) in 2016, and Eliezer Yudkowsky [commented](https://www.facebook.com/yudkowsky/posts/10154462816619228):\n\n> Be it clear: Steelmanning is not a tool of understanding and communication. The communication tool is the Ideological Turing Test. \"Steelmanning\" is what you do to avoid the equivalent of dismissing AGI after reading a media argument. It usually indicates that you think you're talking to somebody as hapless as the media.\n\n> The exception to this rule is when you communicate, \"Well, on my assumptions, the plausible thing that sounds most like this is...\" which is a cooperative way of communicating to the person what your own assumptions are and what you think are the strong and weak points of what you think might be the argument.\n\n> Mostly, you should be trying to pass the Ideological Turing Test if speaking to someone you respect, and offering \"My steelman might be...?\" only to communicate your own premises and assumptions. Or maybe, if you actually believe the steelman, say, \"I disagree with your reason for thinking X, but I'll grant you X because I believe this other argument Y. Is that good enough to move on?\" Be ready to accept \"No, the exact argument for X is important to my later conclusions\" as an answer.\n\n> \"Let me try to imagine a smarter version of this stupid position\" is when you've been exposed to the Deepak Chopra version of quantum mechanics, and you don't know if it's the real version, or what a smart person might really think is the issue. It's what you do when you don't want to be that easily manipulated sucker who can be pushed into believing X by a flawed argument for not-X that you can congratulate yourself for being skeptically smarter than. It's not what you do in a respectful conversation.\n\nIn 2017, Holden Karnofsky [wrote](https://forum.effectivealtruism.org/posts/gTaDDJFDzqe7jnTWG/some-thoughts-on-public-discourse):\n\n> *   **I try to avoid straw-manning,** [**steel-manning**](https://wiki.lesswrong.com/wiki/Steel_man)**, and nitpicking.** I strive for an accurate understanding of the most important premises behind someone's most important decisions, and address those. (As a side note, I find it very unsatisfying to engage with \"steel-man\" versions of my arguments, which rarely resemble my actual views.)\n\nAnd Eliezer wrote, in a private Facebook thread:\n\n> Reminder: Eliezer and Holden are both on record as saying that \"steelmanning\" people is bad and you should stop doing it.\n\n> As Holden says, if you're trying to understand someone or you have any credence at all that they have a good argument, focus on passing their Ideological Turing Test. \"Steelmanning\" usually ends up as weakmanning by comparison. If they don't in fact have a good argument, it's falsehood to pretend they do. If you want to try to make a genuine effort to think up better arguments yourself because they might exist, don't drag the other person into it.\n\nThings I've said\n----------------\n\nIn [2018](https://www.facebook.com/robbensinger/posts/pfbid0VYARo9u5xfoRL4QyUqtrEuHDrCBwL3qAUPuw7wdBEqDHm4FNNqAPY9fwtSptWnGal), I wrote:\n\n> When someone makes a mistake or has a wrong belief, you shouldn't \"steelman\" that belief by replacing it with a different one; it makes it harder to notice mistakes and update from them, and it also makes it harder to understand people's real beliefs and actions.\n> \n> \"What belief does this person have?\" is a particular factual question. Steelmanning, like \"charity\", is sort of about unfocusing your eyes and tricking yourself into treating the factual question as though it were a game: you want to find a fairness-preserving allocation of points to all players, where more credible views warrant more points. Some people like that act of unfocusing because it's fun to brainstorm new arguments; or they think it's a useful trick for reducing social conflict or resistance to new ideas. But it's dangerous to frame that unfocusing as \"steelmanning\" or \"charity\" rather than explicitly flagging \"I want to change the topic to this other thing your statement happened to remind me of\".\n\nIn [2019](https://www.facebook.com/julia.galef/posts/pfbid02xTRsU33tfKMuB7tdAvinN3JxEGGq1ZxbdYPFnbv3d8PWJjctVD7q5gJYnW3LGVW1l), I said:\n\n> Charity seems more useful for rhetoric/persuasion/diplomacy; steel-manning seems more useful for brainstorming; both seem dangerous insofar as they obscure the original meaning and make it harder to pass someone's Ideological Turing Test.\n> \n> \"Charity\" seems like the more dangerous meme to me because it encourages more fuzziness about whether you're flesh-manning \\[i.e., just trying to accurately model\\] vs. steel-manning the argument, and because it has more moral overtones. It's more epistemically dangerous to filter your answers to factual questions by criteria other than truth, than to decide to propose a change of topic.\n> \n> \\[...\\] I endorse \"non-uncharitableness\" -- trying to combat biases toward having an inaccurately negative view of your political enemies and so on.\n> \n> I worry that removing the double negative makes it seem like charity is an epistemic end in its own right, rather than an attempt to combat a bias. I also worry that the word \"charity\" makes it tempting to tie non-uncharitableness to niceness/friendliness, which makes it more effortful to think about and optimize those goals separately.\n> \n> Most of my worries about charity and steelmanning go away if they're discussed with the framings 'non-uncharitableness and niceness are two separate goals' and 'good steelmanning and good fleshmanning are two separate goals', respectively.\n> \n> E.g., actively focus on examples of:\n> \n> *   being epistemically charitable in ways that aren't nice, friendly, or diplomatic.\n> *   being nice and prosocial in ways that require interpreting the person as saying something less plausible.\n> *   trying to better pass someone's Ideological Turing Test by focusing on less plausible claims and arguments.\n> *   coming up with steelmen that explicitly assert the falsehood of the claim they're the steelman of.\n> \n> I also think that the equivocation in \"charity\" is doing some conversational work.\n> \n> E.g.: Depending on context and phrasing, saying that you're optimizing for friendliness can make you seem manipulative or inauthentic, or it can seem like a boast or a backhanded attack (\"I was trying to be nice when I said it that way\" / \"*I'm* trying to be friendly\".) Framing a diplomatic goal as though it were epistemic can mitigate that problem.\n> \n> Similarly, if you're in an intellectual or academic environment and you want to criticize someone for being a jerk, \"you're being uncharitable\" is likely to get less pushback, not only because it's relatively dry but because criticisms of tone are generally more controversial among intellectuals than criticisms of content.\n> \n> \"You're being uncharitable\" is also a common accusation in a motte-and-bailey context. Any argument can be quickly dismissed if it makes your conclusion sound absurd, because the arguer must just be violating the principle of charity. It may not even be necessary to think of an alternative, stronger version of the claim under attack, if you're having an argument over twitter and can safely toss out the \"That sounds awfully uncharitable\" line and then disappear in the mist.\n> \n> ... Hm, this comment ended up going in a more negative direction than I was intending. The concerns above are important, but the thing I originally intended to say was that it's not an accident \"charity\" is equivocal, and there's some risk in disambiguating it without recognizing the conversational purposes the ambiguity was serving, contra my earlier insistence on burning the whole thing down. It may be helping make a lot of social interactions smoother, helping giving people more cover to drop false views with minimal embarrassment (by saying they really meant the more-charitable interpretation all along), etc.\n\n(I now feel more confident that, no, \"charity\" is just a bad meme. Ditch it and replace it with something new.)\n\nFrom [2021](https://twitter.com/robbensinger/status/1362526945645371402):\n\n> The problem isn't 'charity is a good conversational norm, but these people are doing it wrong'; the problem is that charity is a bad conversational norm. If nothing else, it's bad because it equivocates between 'be friendly' norms and 'have accurate beliefs about others' norms.\n> \n> Good norms:\n> \n> *   Keep discussions civil and chill.\n> *   Be wary of biases to strawman others.\n> *   Try to pass others' ITT.\n> *   Use steelmen to help you think outside the box.\n> \n> Bad norms:\n> \n> *   Treat the above norms as identical.\n> *   Try to delude yourself about how good others' arguments are.\n\nFrom [2022](https://twitter.com/robbensinger/status/1517651963705393152):\n\n> I think the term 'charity' is genuinely ambiguous about whether you're trying to find the person's true view, vs. trying to steel-man, vs. some combination. Different people at different times do all of those things and call it argumentative 'charity'.\n> \n> This if anything strikes me as even worse than saying 'I'm steel-manning', because at least steel-manning is transparent about what it's doing, even if people tend to underestimate the hazards of doing it.",
      "plaintextDescription": "This post has been recorded as part of the LessWrong Curated Podcast, and can be listened to on Spotify, Apple Podcasts, and Libsyn.\n\n----------------------------------------\n\nI often object to claims like \"charity/steelmanning is an argumentative virtue\". This post collects a few things I and others have said on this topic over the last few years.\n\nMy current view is:\n\n * Steelmanning (\"the art of addressing the best form of the other person’s argument, even if it’s not the one they presented\") is a useful niche skill, but I don't think it should be a standard thing you bring out in most arguments, even if it's an argument with someone you strongly disagree with.\n * Instead, arguments should mostly be organized around things like:\n   * Object-level learning and truth-seeking, with the conversation as a convenient excuse to improve your own model of something you're curious about.\n   * Trying to pass each other's Ideological Turing Test (ITT), or some generalization thereof. The ability to pass ITTs is the ability \"to state opposing views as clearly and persuasively as their proponents\".\n     * The version of \"ITT\" I care about is one where you understand the substance of someone's view well enough to be able to correctly describe their beliefs and reasoning; I don't care about whether you can imitate their speech patterns, jargon, etc.\n   * Trying to identify and resolve cruxes: things that would make one or the other of you (or both) change your mind about the topic under discussion.\n * Argumentative charity is a complete mess of a concept⁠—people use it to mean a wide variety of things, and many of those things are actively bad, or liable to cause severe epistemic distortion and miscommunication.\n * Some version of civility and/or friendliness and/or a spirit of camaraderie and goodwill seems like a useful ingredient in many discussions. I'm not sure how best to achieve this in ways that are emotionally honest (\"pretending to be cheerful and warm when you don't f",
      "wordCount": 1915
    },
    "tags": [
      {
        "_id": "RE6h98Ziwcfh4EP9T",
        "name": "Steelmanning",
        "slug": "steelmanning"
      },
      {
        "_id": "A4kr45wS7fBW5PBpf",
        "name": "Ideological Turing Tests",
        "slug": "ideological-turing-tests"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Rkxj7TFxhbm59AKJh",
    "title": "The inordinately slow spread of good AGI conversations in ML",
    "slug": "the-inordinately-slow-spread-of-good-agi-conversations-in-ml",
    "url": null,
    "baseScore": 173,
    "voteCount": 95,
    "viewCount": null,
    "commentCount": 62,
    "createdAt": null,
    "postedAt": "2022-06-21T16:09:57.859Z",
    "contents": {
      "markdown": "Spencer Greenberg wrote [on Twitter](https://twitter.com/SpencrGreenberg/status/1536760598486196225):\n\n> Recently @KerryLVaughan has been critiquing groups trying to build AGI, saying that by being aware of risks but still trying to make it, they’re recklessly putting the world in danger. I’m interested to hear your thought/reactions to what Kerry says and the fact he’s saying it.\n\nMichael Page replied:\n\n> I'm pro the conversation. That said, I think the premise -- that folks are aware of the risks -- is wrong.\n> \n> \\[...\\]\n> \n> Honestly, I think the case for the risks hasn't been that clearly laid out. The conversation among EA-types typically takes that as a starting point for their analysis. The burden for the we're-all-going-to-die-if-we-build-x argument is -- and I think correctly so -- quite high.\n\nOliver Habryka then replied:\n\n> I find myself skeptical of this.\n> \n> \\[...\\]\n> \n> Like, my sense is that it's just really hard to convince someone that their job is net-negative. \"It is difficult to get a man to understand something when his salary depends on his not understanding it\" And this barrier is very hard to overcome with just better argumentation.\n\nMy [reply](https://twitter.com/robbensinger/status/1537708636989816832):\n\nI disagree with \"the case for the risks hasn't been that clearly laid out\". I think there's a giant, almost overwhelming pile of intro resources at this point, any one of which is more than sufficient, written in all manner of style, for all manner of audience.[^45y83giorpd]\n\n(I do think it's possible to create a much better intro resource than any that exist today, but 'we can do much better' is compatible with 'it's shocking that the existing material hasn't already finished the job'.)\n\nI also disagree with \"The burden for the we're-all-going-to-die-if-we-build-x argument is -- and I think correctly so -- quite high.\"\n\nIf you're building a machine, you should have an at least somewhat *lower* burden of proof for more serious risks. It's your responsibility to check your own work to some degree, and not impose lots of micromorts on everyone else through negligence.[^vdchyaipp9h]\n\nBut I don't think the latter point matters much, since the 'AGI is dangerous' argument easily meets higher burdens of proof as well.\n\nI do think a lot of people haven't heard the argument in any detail, and the main focus should be on trying to signal-boost the arguments and facilitate conversations, rather than assuming that everyone has heard the basics.\n\nA lot of the field is very smart people who are stuck in circa-1995 levels of discourse about AGI.\n\nI think 'my salary depends on not understanding it' is only a small part of the story. ML people could in principle talk way more about AGI, and understand the problem way better, without coming anywhere close to quitting their job. The level of discourse is by and large *too low* for 'I might have to leave my job' to be the very next obstacle on the path.\n\nAlso, many ML people have other awesome job options, have goals in the field other than pure salary maximization, etc.\n\nMore of the story: Info about AGI propagates too slowly through the field, because when one ML person updates, they usually don't loudly share their update with all their peers. This is because:\n\n1\\. AGI sounds weird, and they don't want to sound like a weird outsider.\n\n2\\. Their *peers* and the *community as a whole* might perceive this information as an attack on the field, an attempt to lower its status, etc.\n\n3\\. Tech forecasting, differential technological development, long-term steering, [exploratory](https://en.wikipedia.org/wiki/Exploratory_engineering) [engineering](https://intelligence.org/files/ExploratoryEngineeringAI.pdf), 'not doing certain research because of its long-term social impact', prosocial research closure, etc. are very novel and foreign to most scientists.\n\nEAs exert effort to try to dig up precedents like [Asilomar](https://intelligence.org/files/TheAsilomarConference.pdf) partly *because* Asilomar is so unusual compared to the norms and practices of the vast majority of science. Scientists generally don't think in these terms at all, especially in *advance* of any major disasters their field causes.\n\nAnd the scientists who do find any of this intuitive often feel vaguely nervous, alone, and adrift when they talk about it. On a gut level, they see that they have no institutional home and no super-widely-shared 'this is a virtuous and respectable way to do science' narrative.\n\nNormal [science](https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy) is not Bayesian, is not agentic, is not 'a place where you're supposed to do arbitrary things just because you heard an argument that makes sense'. Normal science is a specific collection of scripts, customs, and established protocols.\n\nIn trying to move the field toward 'doing the thing that just makes sense', even though it's about a weird topic (AGI), and even though the prescribed response is also weird (closure, differential tech development, etc.), and even though the arguments in support are weird (where's the experimental data??), we're inherently fighting our way upstream, against the current.\n\nSuccess is possible, but way, way more [dakka](https://thezvi.wordpress.com/2017/12/02/more-dakka/) is needed, and IMO it's easy to understand why we haven't succeeded more.\n\nThis is also part of why I've increasingly updated toward a strategy of \"let's all be way too blunt and candid about our AGI-related thoughts\".\n\nThe core problem we face isn't 'people informedly disagree', 'there's a values conflict', 'we haven't written up the arguments', 'nobody has seen the arguments', or even 'self-deception' or 'self-serving bias'.\n\nThe core problem we face is 'not enough information is transmitting fast enough, because people feel nervous about whether their private thoughts are in the Overton window'.\n\nWe need to throw a brick through the Overton window. Both by adopting a very general policy of candidly stating what's in our head, and by propagating the arguments and info a lot further than we have in the past. If you want to normalize weird stuff fast, you have to be weird.\n\nCf. [*Inadequate Equilibria*](https://www.lesswrong.com/posts/PRAyQaiMWg2La7XQy/moloch-s-toolbox-2-2):\n\n> What broke the silence about artificial general intelligence (AGI) in 2014 wasn’t Stephen Hawking writing a careful, well-considered [essay](http://www.huffingtonpost.com/stephen-hawking/artificial-intelligence_b_5174265.html) about how this was a real issue. The silence only broke when Elon Musk [tweeted](http://www.theverge.com/2014/8/3/5965099/elon-musk-compares-artificial-intelligence-to-nukes) about Nick Bostrom’s *Superintelligence*, and then made an off-the-cuff remark about how AGI was “[summoning the demon](http://www.independent.co.uk/life-style/gadgets-and-tech/news/tesla-boss-elon-musk-warns-artificial-intelligence-development-is-summoning-the-demon-9819760.html).”\n> \n> Why did that heave a rock through the Overton window, when Stephen Hawking couldn’t? Because Stephen Hawking *sounded like* he was trying hard to appear sober and serious, which signals that this is a subject you have to be careful not to gaffe about. And then Elon Musk was like, “*Whoa, look at that apocalypse over there!!*” After which there was the equivalent of journalists trying to pile on, shouting, “A gaffe! A gaffe! A… gaffe?” and finding out that, in light of recent news stories about AI and in light of Elon Musk’s good reputation, people weren’t backing them up on that gaffe thing.\n> \n> Similarly, to heave a rock through the Overton window on the War on Drugs, what you need is not state propositions (although those do help) or articles in *The Economist*. What you need is for some “serious” politician to say, “This is dumb,” and for the journalists to pile on shouting, “A gaffe! A gaffe… a gaffe?” But it’s a grave personal risk for a politician to test whether the public atmosphere has changed enough, and even if it worked, they’d capture very little of the human benefit for themselves.\n\n* * *\n\nSimone Sturniolo [commented](https://twitter.com/SturnioloSimone/status/1537738980288040960) on \"AGI sounds weird, and they don't want to sound like a weird outsider.\":\n\n> I think this is really the main thing. It sounds too sci-fi a worry. The \"sensible, rational\" viewpoint is that AI will never be that smart because haha, they get funny word wrong (never mind that they've grown to a point that would have looked like sorcery 30 years ago).\n\nTo which I reply: That's an example of a more-normal view that exists in society-at-large, but it's also a view that makes AI research sound lame. (In addition to being harder to say with a straight face if you've been working in ML for long at all.)\n\nThere's an important tension in ML between \"play up AI so my work sounds important and impactful (and because it's in fact true)\", and \"downplay AI in order to sound serious and respectable\".\n\nThis is a genuine tension, with no way out. There legit isn't any way to speak accurately and concretely about the future of AI without sounding like a sci-fi weirdo. So the field ends up tangled in ever-deeper knots [motivatedly](https://www.lesswrong.com/posts/L32LHWzy9FzSDazEg/motivated-stopping-and-motivated-continuation) searching for some third option that doesn't exist.\n\nCurrently popular strategies include:\n\n1\\. Quietism and directing your attention elsewhere.\n\n2\\. Derailing all conversations about the future of AI to talk about [semantics](https://twitter.com/robbensinger/status/1515479401034489856) (\"'AGI' is a wrong label\").\n\n3\\. Only talking about AI's long-term impact in extremely vague terms, and motivatedly focusing on normal goals like \"cure cancer\" since that's a normal-sounding thing doctors are already trying to do.\n\n(Avoid any weird specifics about how you might go about curing cancer, and avoid weird specifics about the social effects of automating medicine, curing all disease, etc. Concreteness is the enemy.)\n\n4\\. Say that AI's huge impacts will happen someday, in the indefinite future. But that's a \"someday\" problem, not a \"soon\" problem.\n\n(Don't, of course, give specific years or talk about probability distributions over future tech developments, future milestones you expect to see 30 years before AGI, cruxes, etc. That's a weird thing for a scientist to do.)\n\n5\\. Say that AI's impacts will happen gradually, over many years. Sure, they'll ratchet up to being a big thing, but it's not like any crazy developments will happen *overnight*; this isn't science fiction, after all.\n\n(Somehow \"does this happen in sci-fi?\" feels to people like a relevant source of info about the future.)\n\nWhen Paul Christiano talks about soft takeoff, he has in mind a scenario like 'we'll have some years of slow ratcheting to do some preparation, but things will accelerate faster and faster and be extremely crazy and fast in the endgame'.\n\nBut what people outside EA usually have in mind by soft takeoff is:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e465fae85914f3d06d7e0fb82af570642159777f2bf8f605.png)\n\nI think the Paul scenario is one where things start going crazy in the next few decades, and go more and more crazy, and are apocalyptically crazy in thirty years or so?\n\nBut what many ML people seemingly want to believe (or want to talk as though they believe) is a *Jetsons* world.\n\nA world where we gradually ratchet up to \"human-level AI\" over the next 50–250 years, and then we spend another 50–250 years slowly ratcheting up to crazy superhuman systems.\n\nThe clearest place I've seen this perspective explicitly argued for is in [Rodney Brooks' writing](https://www.technologyreview.com/2017/10/06/241837/the-seven-deadly-sins-of-ai-predictions/). But the much more common position isn't to explicitly argue for this view, or even to explicitly state it. It jut sort of lurks in the background, like an obvious sane-and-moderate Default. Even though it *immediately and obviously falls apart as a scenario* as soon as you start poking at it and actually discussing the details.\n\n6\\. Just say it's not your job to think or talk about the future. You're a scientist! Scientists don't think about the future. They just do their research.\n\n7\\. More strongly, you can say that it's irresponsible speculation to even broach the subject! What a silly thing to discuss!\n\nNote that the argument here usually isn't \"AGI is clearly at least 100 years away for reasons X, Y, and Z; therefore it's irresponsible speculation to discuss this until we're, like, 80 years into the future.\" Rather, *even giving arguments for why AGI is 100+ years away* is assumed at the outset to be irresponsible speculation. There isn't a cost-benefit analysis being given here for why this is low-importance; there's just a miasma of unrespectability.\n\n[^45y83giorpd]: Some of my favorite informal ones to link to: Russell 2014; Urban 2015 (w/ Muehlhauser 2015); Yudkowsky 2016a; Yudkowsky 2017; Piper 2018; Soares 2022Some of my favorite less-informal ones: Bostrom 2014a; Bostrom 2014b; Yudkowsky 2016b; Soares 2017; Hubinger et al. 2019; Ngo 2020; Cotra 2021; Yudkowsky 2022; Arbital's listingOther good ones include: Omohundro 2008; Yudkowsky 2008; Yudkowsky 2011; Muehlhauser 2013; Yudkowsky 2013; Armstrong 2014; Dewey 2014; Krakovna 2015; Open Philanthropy 2015; Russell 2015; Soares 2015a; Soares 2015b; Steinhardt 2015; Alexander 2016; Amodei et al. 2016; Open Philanthropy 2016; Taylor et. al 2016; Taylor 2017; Wiblin 2017; Yudkowsky 2017; Garrabrant and Demski 2018; Harris and Yudkowsky 2018; Christiano 2019a; Christiano 2019b; Piper 2019; Russell 2019; Shlegeris 2020; Carlsmith 2021; Dewey 2021; Miles 2021; Turner 2021; Steinhardt 2022 \n\n[^vdchyaipp9h]: Or, I should say, a lower \"burden of inquiry\".You should (at least somewhat more readily) take the claim seriously and investigate it in this case. But you shouldn't require less evidence to believe anything — that would just be biasing yourself, unless you're already biased and are trying to debias yourself. (In which case this strikes me as a bad debiasing tool.)See also the idea of \"conservative futurism\" versus \"conservative engineering\" in Creating Friendly AI 1.0:The conservative assumption according to futurism is not necessarily the “conservative” assumption in Friendly AI. Often, the two are diametric opposites. When building a toll bridge, the conservative revenue assumption is that half as many people will drive through as expected. The conservative engineering assumption is that ten times as many people as expected will drive over, and that most of them will be driving fifteen-ton trucks.Given a choice between discussing a human-dependent traffic-control AI and discussing an AI with independent strong nanotechnology, we should be biased towards assuming the more powerful and independent AI. An AI that remains Friendly when armed with strong nanotechnology is likely to be Friendly if placed in charge of traffic control, but perhaps not the other way around. (A minivan can drive over a bridge designed for armor-plated tanks, but not vice-versa.The core argument for hard takeoff, 'AI can achieve strong nanotech', and \"get it right the first time\" is that they're true, not that they're \"conservative\". But it's of course also true that a sane world that thought hard takeoff were \"merely\" 20% likely, would not immediately give up and write off human survival in those worlds. Your plan doesn't need to survive one-in-a-million possibilities, but it should survive one-in-five ones!",
      "plaintextDescription": "Spencer Greenberg wrote on Twitter:\n\n> Recently @KerryLVaughan has been critiquing groups trying to build AGI, saying that by being aware of risks but still trying to make it, they’re recklessly putting the world in danger. I’m interested to hear your thought/reactions to what Kerry says and the fact he’s saying it.\n\nMichael Page replied:\n\n> I'm pro the conversation. That said, I think the premise -- that folks are aware of the risks -- is wrong.\n> \n> [...]\n> \n> Honestly, I think the case for the risks hasn't been that clearly laid out. The conversation among EA-types typically takes that as a starting point for their analysis. The burden for the we're-all-going-to-die-if-we-build-x argument is -- and I think correctly so -- quite high.\n\nOliver Habryka then replied:\n\n> I find myself skeptical of this.\n> \n> [...]\n> \n> Like, my sense is that it's just really hard to convince someone that their job is net-negative. \"It is difficult to get a man to understand something when his salary depends on his not understanding it\" And this barrier is very hard to overcome with just better argumentation.\n\nMy reply:\n\nI disagree with \"the case for the risks hasn't been that clearly laid out\". I think there's a giant, almost overwhelming pile of intro resources at this point, any one of which is more than sufficient, written in all manner of style, for all manner of audience.[1]\n\n(I do think it's possible to create a much better intro resource than any that exist today, but 'we can do much better' is compatible with 'it's shocking that the existing material hasn't already finished the job'.)\n\nI also disagree with \"The burden for the we're-all-going-to-die-if-we-build-x argument is -- and I think correctly so -- quite high.\"\n\nIf you're building a machine, you should have an at least somewhat lower burden of proof for more serious risks. It's your responsibility to check your own work to some degree, and not impose lots of micromorts on everyone else through negligence.[2]\n\nBut I don't",
      "wordCount": 2386
    },
    "tags": [
      {
        "_id": "6zBEfFYJxhSEcchbR",
        "name": "AI Alignment Fieldbuilding",
        "slug": "ai-alignment-fieldbuilding"
      },
      {
        "_id": "fpEBgFE7fgpxTm9BF",
        "name": "Machine Learning  (ML)",
        "slug": "machine-learning-ml"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "aaeMECbcBzC3cweGK",
    "title": "On saving one's world",
    "slug": "on-saving-one-s-world",
    "url": null,
    "baseScore": 192,
    "voteCount": 97,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2022-05-17T19:53:58.192Z",
    "contents": {
      "markdown": "If the world is likeliest to be saved by sober scholarship, then let us be sober scholars in the face of danger.\n\nIf the world is likeliest to be saved by playful intellectual exploration, then let us be playful in the face of danger.\n\nStrategic, certainly; aware of our situation, of course; but let us not throw away the one mental mode that can actually save us, if that's in fact our situation.\n\nIf the world is likeliest to be saved by honest, trustworthy, and high-integrity groups, who by virtue of their trustworthiness can much more effectively collaborate and much more quickly share updates; then let us be trustworthy. What is the path to good outcomes otherwise?\n\nCFAR has a notion of \"flailing\". Alone on a desert island, if you injure yourself, you're likelier to think fast about how to solve the problem. Whereas injuring yourself around friends, you're more likely to \"flail\": lean into things that demonstrate your pain/trouble to others.\n\nTo my eye, a lot of proposals that we set aside sober scholarship, or playful intellectual exploration, or ethical integrity, look like flailing. I don't see an argument that this setting-aside actually chains forward into good outcomes; it seems performative to me, like hoping that if our reaction \"feels extreme\" enough, some authority somewhere will take notice and come to the rescue.\n\nWho is that authority?\n\nIf you have a coherent model of this, we can talk about it and figure out if that's really the best strategy for eliciting their aid.\n\nBut if no one comes to mind, consider the possibility that you're executing a social instinct that's adaptive to threats like tigers and broken legs, but maladaptive to threats like Unfriendly AI.\n\nIf you feel scared about something, I generally think it's good to be honest about that fact and discuss it soberly, rather than hiding it. I don't think this is incompatible with rigorous scholarship or intellectual play.\n\nBut I would clearly distinguish \"being honest about your world-models and feelings, because honesty is legitimately a good idea\" from \"making it your main strategy to do whatever action sequence feels emotionally resonant with the problem\".\n\nAn \"extreme\" key doesn't necessarily open an \"extreme\" lock. A dire-sounding key doesn't necessarily open a dire-feeling lock. A fearful or angry key doesn't necessarily open a lock that makes you want to express fear or anger.\n\nRather, the lock's exact physical properties determine which exact key (or set of keys) opens it, and we need to investigate the physical world in order to find the right key.",
      "plaintextDescription": "If the world is likeliest to be saved by sober scholarship, then let us be sober scholars in the face of danger.\n\nIf the world is likeliest to be saved by playful intellectual exploration, then let us be playful in the face of danger.\n\nStrategic, certainly; aware of our situation, of course; but let us not throw away the one mental mode that can actually save us, if that's in fact our situation.\n\nIf the world is likeliest to be saved by honest, trustworthy, and high-integrity groups, who by virtue of their trustworthiness can much more effectively collaborate and much more quickly share updates; then let us be trustworthy. What is the path to good outcomes otherwise?\n\nCFAR has a notion of \"flailing\". Alone on a desert island, if you injure yourself, you're likelier to think fast about how to solve the problem. Whereas injuring yourself around friends, you're more likely to \"flail\": lean into things that demonstrate your pain/trouble to others.\n\nTo my eye, a lot of proposals that we set aside sober scholarship, or playful intellectual exploration, or ethical integrity, look like flailing. I don't see an argument that this setting-aside actually chains forward into good outcomes; it seems performative to me, like hoping that if our reaction \"feels extreme\" enough, some authority somewhere will take notice and come to the rescue.\n\nWho is that authority?\n\nIf you have a coherent model of this, we can talk about it and figure out if that's really the best strategy for eliciting their aid.\n\nBut if no one comes to mind, consider the possibility that you're executing a social instinct that's adaptive to threats like tigers and broken legs, but maladaptive to threats like Unfriendly AI.\n\nIf you feel scared about something, I generally think it's good to be honest about that fact and discuss it soberly, rather than hiding it. I don't think this is incompatible with rigorous scholarship or intellectual play.\n\nBut I would clearly distinguish \"being honest about your world-models",
      "wordCount": 415
    },
    "tags": [
      {
        "_id": "oNcqyaWPXNGTTRPHm",
        "name": "Existential risk",
        "slug": "existential-risk"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "34Gkqus9vusXRevR8",
    "title": "Late 2021 MIRI Conversations: AMA / Discussion",
    "slug": "late-2021-miri-conversations-ama-discussion",
    "url": null,
    "baseScore": 119,
    "voteCount": 42,
    "viewCount": null,
    "commentCount": 199,
    "createdAt": null,
    "postedAt": "2022-02-28T20:03:05.318Z",
    "contents": {
      "markdown": "With the release of [Rohin Shah and Eliezer Yudkowsky's conversation](https://www.lesswrong.com/posts/tcCxPLBrEXdxN5HCQ/shah-and-yudkowsky-on-alignment-plans), the **Late 2021 MIRI Conversations** sequence is now complete.\n\nThis post is intended as a generalized comment section for discussing the whole sequence, now that it's finished. Feel free to:\n\n*   raise any topics that seem relevant\n*   signal-boost particular excerpts or comments that deserve more attention\n*   direct questions to participants\n\nIn particular, Eliezer Yudkowsky, Richard Ngo, Paul Christiano, Nate Soares, and Rohin Shah expressed active interest in receiving follow-up questions here. The Schelling time when they're likeliest to be answering questions is **Wednesday March 2**, though they may participate on other days too.",
      "plaintextDescription": "With the release of Rohin Shah and Eliezer Yudkowsky's conversation, the Late 2021 MIRI Conversations sequence is now complete.\n\nThis post is intended as a generalized comment section for discussing the whole sequence, now that it's finished. Feel free to:\n\n * raise any topics that seem relevant\n * signal-boost particular excerpts or comments that deserve more attention\n * direct questions to participants\n\nIn particular, Eliezer Yudkowsky, Richard Ngo, Paul Christiano, Nate Soares, and Rohin Shah expressed active interest in receiving follow-up questions here. The Schelling time when they're likeliest to be answering questions is Wednesday March 2, though they may participate on other days too.",
      "wordCount": 106
    },
    "tags": [
      {
        "_id": "YgizoZqa7LEb3LEJn",
        "name": "AMA",
        "slug": "ama"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "b7Euvy3RCKT7cppDk",
    "title": "Animal welfare EA and personal dietary options",
    "slug": "animal-welfare-ea-and-personal-dietary-options",
    "url": null,
    "baseScore": 38,
    "voteCount": 28,
    "viewCount": null,
    "commentCount": 32,
    "createdAt": null,
    "postedAt": "2022-01-05T18:53:02.157Z",
    "contents": {
      "markdown": "When I imagine an animal welfare EA group, I imagine views breaking down something like:\n\n*   50%: If factory farmed animals are moral patients, it's more likely that they have net-negative lives (i.e., it would better for them not to exist, than to live such terrible lives).\n*   50%: If factory farmed animals are moral patients, it's more likely that they have net-positive lives (i.e., their lives may be terrible, but they aren't so lacking in value that preventing the life altogether is a net improvement).\n\nThis seems like a super hard question, and not one that changes the importance of working to promote animal welfare, so naively (absent some argument for a more informative prior) it should have a 50/50 split within animal welfare circles.\n\n(Possibly more effort should go into the net-positive view within EA because it's more neglected by animal welfare activists, who tend to be veg*ns; but the space as a whole is so neglected that I suspect this shouldn't be a large factor.)\n\nWithin the \"net-negative\" camp, in my unanchored \"what would I naively expect?\" hypothetical, I then imagine dietary preferences breaking down something like:\n\n*   10%: **Approximate veg*nism** or **approximate reducetarianism**. (\"Approximate\" to allow for carve-outs like bivalves and especially-moral animal products. The group generally strongly encourages all members to have at least one carve-out, because bivalves in particular are such a clear case and dietary purity ethics is a risky attractor to avoid.)\n*   10%: Handshake-itarianism.\n*   20%: Boycott-itarianism.\n*   60%: **Anything goes**. A normal meat-eating diet, optimized only for health and convenience. This is the standard animal welfare EA diet, because EA is generally about optimizing your positive impact on the world, not about purifying your personal actions of any possible negative impact.\n\nThe number would be much higher than 60% on strictly utilitarian grounds, but humans aren't strict utilitarians and it makes sense for people working hard on improving animal lives to develop strong feelings about their own personal relationship to factory farming, or to want to self-signal their commitment in some fashion.\n\nWithin the \"net-positive\" camp, I imagine:\n\n*   10%: **Sentience-maximizing diets**. If you think animals in factory farms have net-positive lives, then it makes sense to want to increase the number of animals (by eating the most meat-heavy healthy diet possible) while also working to improve their welfare.\n*   10%: Handshake-itarianism.\n*   20%: Boycott-itarianism.\n*   60%: Anything goes.\n\n**Handshake-itarianism** observes that the ~veg\\*ns and the sentience-maximizers are sort of offsetting each other's efforts, and that it can make more sense for Bob the ~Veg\\*n and Alice the Sentience-Maximizer to pair off and each agree to eat a \"compromise\" diet (e.g., both eat meat but only on the weekend). This has a few nice consequences:\n\n\\- Both diets are likely to be healthier, because they'll be more nutritionally diverse/balanced. This makes it likelier that both EAs will be more productive and have better lives.\n\n\\- The outcome can be closer to an optimal trade between Alice and Bob's values, because it's less likely to be constrained by either individual's personal circumstances or limitations.\n\nMaybe Alice has an easier time sticking to her ideal diet than Bob does, but Bob is more confident in his view than Alice is; in that case, a handshake diet can produce an outcome that's closer to a compromise between Alice and Bob's ideal diets, because less-extreme diets are easier to maintain and because the handshake agreement has more adjustable parameters. (Like, Alice is lactose-intolerant so Bob drinks more milk on Alice's behalf.)\n\n\\- Trades like this are likely to have positive social and psychological effects. If Alice and Bob are actively trying to undo each other's efforts, then they may feel (at least slightly) less cooperation and goodwill about collaborating on other animal welfare projects, even if they don't reflectively endorse that attitude. A joint effort to produce a good compromise outcome is likely to feel better.\n\nThe main downside of handshake-itarianism is that, like reducetarianism, it's complicated — 'never eat meat' is a simple heuristic, whereas 'only eat meat on the weekend' is easier to mess up.\n\nSome people will have an easier time going ~veg*an than reducetarian, e.g., because they find it less stressful to pick a clear black line and stick to it. Others will find handshake-itarianism or reducetarianism easier to stick to, because the rules can be stipulated in ways that provide more leeway ('I can break my diet at the party tonight, as long as I make a note to buy an extra hamburger tomorrow').\n\n**Boycott-itarianism** is the most popular EA dietary restriction (in this visualization), and has a lot of the nice features of the above options. Animal welfare EAs disagree about whether factory-farmed animals have net-positive lives, but they agree that it's good to *improve* those lives, and they agree about a lot of interventions that would improve those lives. So a robustly good dietary intervention is one that:\n\n\\- Chooses some concrete threshold (e.g., 'I'll only eat chickens if they're cage-free'), and loudly sticks to it. This gives meat producers an economic incentive to switch to the more ethical option.\n\n\\- Makes the threshold relatively easy to achieve, so the incentive is stronger. A specific action that can be done today is better than a long list, a fuzzy heuristic, or a thing that's economically / technologically unviable today.\n\n\\- Make the threshold simple and obvious, so it's easier for lots of people to coordinate around the same threshold(s) (thereby making the incentive stronger).",
      "plaintextDescription": "When I imagine an animal welfare EA group, I imagine views breaking down something like:\n\n * 50%: If factory farmed animals are moral patients, it's more likely that they have net-negative lives (i.e., it would better for them not to exist, than to live such terrible lives).\n * 50%: If factory farmed animals are moral patients, it's more likely that they have net-positive lives (i.e., their lives may be terrible, but they aren't so lacking in value that preventing the life altogether is a net improvement).\n\nThis seems like a super hard question, and not one that changes the importance of working to promote animal welfare, so naively (absent some argument for a more informative prior) it should have a 50/50 split within animal welfare circles.\n\n(Possibly more effort should go into the net-positive view within EA because it's more neglected by animal welfare activists, who tend to be veg*ns; but the space as a whole is so neglected that I suspect this shouldn't be a large factor.)\n\nWithin the \"net-negative\" camp, in my unanchored \"what would I naively expect?\" hypothetical, I then imagine dietary preferences breaking down something like:\n\n * 10%: Approximate veg*nism or approximate reducetarianism. (\"Approximate\" to allow for carve-outs like bivalves and especially-moral animal products. The group generally strongly encourages all members to have at least one carve-out, because bivalves in particular are such a clear case and dietary purity ethics is a risky attractor to avoid.)\n * 10%: Handshake-itarianism.\n * 20%: Boycott-itarianism.\n * 60%: Anything goes. A normal meat-eating diet, optimized only for health and convenience. This is the standard animal welfare EA diet, because EA is generally about optimizing your positive impact on the world, not about purifying your personal actions of any possible negative impact.\n\nThe number would be much higher than 60% on strictly utilitarian grounds, but humans aren't strict utilitarians and it makes sense for people working ",
      "wordCount": 911
    },
    "tags": [
      {
        "_id": "Q9ASuEEoJWxT3RLMT",
        "name": "Animal Ethics",
        "slug": "animal-ethics"
      },
      {
        "_id": "827JKe7YNjAegR468",
        "name": "Effective altruism",
        "slug": "effective-altruism"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vT4tsttHgYJBoKi4n",
    "title": "Some abstract, non-technical reasons to be non-maximally-pessimistic about AI alignment",
    "slug": "some-abstract-non-technical-reasons-to-be-non-maximally",
    "url": null,
    "baseScore": 70,
    "voteCount": 38,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2021-12-12T02:08:08.798Z",
    "contents": {
      "markdown": "I basically agree with Eliezer’s picture of things in the [AGI interventions post](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions).\n\nBut I’ve seen some readers rounding off Eliezer’s ‘the situation looks very dire’-ish statements to ‘the situation is hopeless’, and ‘solving alignment still looks to me like our best shot at a good future, but so far we’ve made very little progress, we aren’t anywhere near on track to solve the problem, and it isn’t clear what the best path forward is’-ish statements to ‘let’s give up on alignment’.\n\nIt’s hard to give a *technical* argument for ‘alignment isn’t doomed’, because I don’t know how to do alignment (and, to the best of my knowledge in December 2021, no one else does either). But I can give some of the more abstract reasons I think that.\n\nI feel sort of wary of sharing a ‘reasons to be less pessimistic’ list, because it’s blatantly filtered evidence, it makes it easy to overcorrect, etc. In my experience, people tend to be way too eager to classify problems as either ‘easy’ or ‘impossible’; just adding more evidence may cause people to bounce back and forth between the two rather than planting a flag in the middle ground.\n\nI did write a version of 'reasons not to be maximally pessimistic' for a few friends in 2018. I’m warily fine with sharing that below, with the caveats ‘holy shit is this ever filtered evidence!’ and ‘these are my own not-MIRI-vetted personal thoughts’. And 'this is a casual thing I jotted down for friends in 2018'.\n\nToday, I would add some points (e.g., 'AGI may be surprisingly far off; timelines are hard to predict'), and I'd remove others (e.g., 'Nate and Eliezer feel pretty good about MIRI's current research'). Also, since the list is both qualitative and one-sided, it doesn’t reflect the fact that I’m quantitatively a bit more pessimistic now than I was in 2018.\n\nLo:\n\n* * *\n\n\\[...S\\]ome of the main reasons I'm not extremely pessimistic about artificial general intelligence outcomes.\n\n(Warning: one-sided lists of considerations can obviously be epistemically bad. I mostly mean to correct for the fact that I see a lot of rationalists who strike me as overly pessimistic about AGI outcomes. Also, I don't try to argue for most of these points in any detail; I'm just trying to share my own views for others' stack.)\n\n1\\. AGI alignment is just a technical problem, and humanity actually has a remarkably good record when it comes to solving technical problems. It's historically common for crazy-seeming goals to fall to engineering ingenuity, even in the face of seemingly insuperable obstacles.\n\nSome of the underlying causes for this are 'it's hard to predict what clever ideas are hiding in the parts of your map that aren't filled in yet', and 'it's hard to prove a universal negation'. A universal negation is what you need in order to say that there's *no* clever engineering solution; whereas even if you've had ten thousand failed attempts, a single existence proof — a single solution to the problem — renders those failures totally irrelevant.\n\n2\\. We don't know very much yet about the alignment problem. This isn't a reason for optimism, but it's a reason not to have confident pessimism, because no confident view can be justified by a state of uncertainty. We just have to learn more and do it the hard way and see how things go.\n\nA blank map can feel like 'it's hopeless' for various reasons, even when you don't actually have enough Bayesian evidence to assert a technical problem is hopeless. For example: you think really hard about the problem and can't come up with a solution, which to some extent feels like there just isn't a solution. And: people aren't very good at knowing which parts of their map are blank, so it may feel like there aren't more things to learn even where there are. And: to the extent there are more things to learn, these can represent not only answers to questions you've posed, but answers to questions you never thought to pose; and can represent not only more information relevant to your current angle of attack on the problem, but information that can only be seen as relevant once you've undergone a perspective shift, ditched an implicit assumption, etc. This is to a large extent the normal way intellectual progress has worked historically, but hindsight bias makes this hard to notice and fully appreciate.\n\nOr as Eliezer put it in his [critique](https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal) of Paul Christiano's approach to alignment on LW:\n\n> I restate that these objections seem *to me* to collectively sum up to “This is fundamentally just not a way you can get an aligned powerful AGI unless you already have an aligned superintelligence”, rather than “Some further insights are required for this to work in practice.” But who knows what further insights may really bring? Movement in thoughtspace consists of better understanding, not cleverer tools.\n\nEliezer is not a modest guy. This is not false humility or politeness. This is a statement about what technical progress looks like when you have to live through it and predict it in the future, as opposed to what it looks like with the benefit of hindsight: it looks like paradigm shifts and things going right in really weird and unexpected ways (that make perfect sense and look perfectly obvious in hindsight). If we want to avoid recapitulating the historical errors of people who thought a thing was impossible (or centuries away, etc.) because they didn't know how to do it yet, then we have to either have a flatter prior about how hard alignment is, or make sure to ground our confidence in very solid inside-view domain knowledge.\n\n3\\. If you can get a few very specific things right, you can leverage AGI capabilities to bootstrap your way to getting everything else right, including solving various harder forms of the alignment problem. By the very nature of the AGI problem, you don't have to do everything by human ingenuity; you just have to get this one thing firmly right. Neglecting this bootstrapping effect makes it easy to overestimate the expected difficulty of the problem.\n\n4\\. AGI alignment isn't the kind of problem that requires massive coordination or a global mindset shift or anything like that. It's more like the moon landing or the Manhattan Project, in that it's a concrete goal that a specific project at a certain time or place can pull off all on its own, regardless of how silly the rest of the world is acting at the time.\n\nCoordination can obviously make this task a lot easier. In general, the more coordination you have, the easier the technical challenge becomes; and the more technical progress you make, the lower a level of coordination and resource advantage you need. But at its core, the alignment problem is about building a machine with certain properties, and a team can just *do* that even if the world-at-large that they're operating in is badly broken.\n\n5\\. Sufficiently well-informed and rational actors have extremely good incentives here. The source of the 'AI developers are racing to the brink' problem is bias and information asymmetry, not any fundamental conflict of interest.\n\n6\\. Clear and rigorous thinking is helpful for AGI capabilities, and it's also helpful for understanding the nature and severity of AGI risk. This doesn't mean that there's a strong correlation today between the people who are best at capabilities and the people who are thinking most seriously about safety; but it does mean that there's a force pushing in the direction of a correlation like that becoming more strong over time (e.g., as conversations happen and the smartest people acquire more information, think about things more, and thereby get closer to truth).\n\n7\\. Major governments aren't currently leaders in AI research, and there are reasons to think this is unlikely to change in the future. (This is positive from my perspective because I think state actors can make a lot of aspects of the problem more difficult and complicated.)\n\n8\\. Deference to domain experts. Nate, Eliezer, Benya, and other researchers at MIRI think it's doable, and these are some of the folks I think are most reliably correct and well-calibrated about tricky questions like these. They're also the kind of people I think really would drop this line of research if the probability of success seemed too low to them, or if some other approach seemed more promising.\n\n9\\. This one's hard to communicate, but: some kind of gestalt impression gathered from seeing how MIRI people approach the problem in near mode, and how they break the problem down into concrete smaller subproblems.\n\nI don't think this is a strong reason to expect success, but I do think there's some kind of mindset switch that occurs when you are living and breathing nitty-gritty details related to alignment work, deployment strategy, etc., and when you see various relatively-concrete paths to success discussed in a serious and disciplined way.\n\nI think a big part of what I'm gesturing at here is a more near-mode model of AGI itself: thinking of AGI as software whose properties we determine, where we can do literally anything we want with it (if we can figure out how to represent the thing as lines of code). A lot of people go too far with this and conclude the alignment problem is trivial because it's 'just software'; but I think there's a sane version of this perspective that's helpful for estimating the difficulty of the problem.\n\n10\\. Talking in broad generalities, MIRI tends to think that you need a relatively principled approach to AGI in order to have a shot at alignment. But drilling down on the concrete details, it's still the case that it can be totally fine in real life to use clever hacks rather than deep principled approaches, as long as the clever hacks work. (Which they sometimes do, even in robust code.)\n\nThe key thing from the MIRI perspective isn't 'you never use cheats or work-arounds to make the problem easier on yourself', but rather 'it's not cheats and work-arounds all the way down; the high-level cleverness is grounded in a deep understanding of what the system is fundamentally doing'.\n\n11\\. Relatedly, I have various causes for optimism that are more specific to MIRI's particular research approach; e.g., thinking it's easier to solve various conceptual problems because of inside-view propositions about the problems.\n\n12\\. The problems MIRI is working on have been severely neglected by researchers in the past, so it's not like they're the kind of problem humanity has tried its hand at and found to be highly difficult. Some of the problems have accrued a mythology of being formidably difficult or even impossible, in spite of no one having really tried them before.\n\n(A surprisingly large number of the problems MIRI has actually already solved are problems that various researchers in the field have told us are impossible for anyone to solve even in principle, which indicates that a lot of misunderstandings of things like reflective reasoning are really commonplace.)\n\n13\\. People haven't tried very hard to find non-MIRI-ish approaches that might work.\n\n14\\. Humanity sometimes builds robust and secure software. If the alignment problem is similar to other cases of robustness, then it's a hard problem, but not so hard that large teams of highly motivated and rigorous teams (think NASA) can't solve them.\n\n15\\. Indeed, there are already dedicated communities specializing in methodologically similar areas like computer security, and if they took some ownership of the alignment problem, things could suddenly start to look a lot sunnier.\n\n16\\. More generally, there are various non-AI communities who make me more optimistic than AI researchers on various dimensions, and to the extent I'm uncertain about the role those communities will play in AGI in the future, I'm more uncertain about AGI outcomes.\n\n17\\. \\[redacted\\]\n\n18\\. \\[redacted\\]",
      "plaintextDescription": "I basically agree with Eliezer’s picture of things in the AGI interventions post.\n\nBut I’ve seen some readers rounding off Eliezer’s ‘the situation looks very dire’-ish statements to ‘the situation is hopeless’, and ‘solving alignment still looks to me like our best shot at a good future, but so far we’ve made very little progress, we aren’t anywhere near on track to solve the problem, and it isn’t clear what the best path forward is’-ish statements to ‘let’s give up on alignment’.\n\nIt’s hard to give a technical argument for ‘alignment isn’t doomed’, because I don’t know how to do alignment (and, to the best of my knowledge in December 2021, no one else does either). But I can give some of the more abstract reasons I think that.\n\nI feel sort of wary of sharing a ‘reasons to be less pessimistic’ list, because it’s blatantly filtered evidence, it makes it easy to overcorrect, etc. In my experience, people tend to be way too eager to classify problems as either ‘easy’ or ‘impossible’; just adding more evidence may cause people to bounce back and forth between the two rather than planting a flag in the middle ground.\n\nI did write a version of 'reasons not to be maximally pessimistic' for a few friends in 2018. I’m warily fine with sharing that below, with the caveats ‘holy shit is this ever filtered evidence!’ and ‘these are my own not-MIRI-vetted personal thoughts’. And 'this is a casual thing I jotted down for friends in 2018'.\n\nToday, I would add some points (e.g., 'AGI may be surprisingly far off; timelines are hard to predict'), and I'd remove others (e.g., 'Nate and Eliezer feel pretty good about MIRI's current research'). Also, since the list is both qualitative and one-sided, it doesn’t reflect the fact that I’m quantitatively a bit more pessimistic now than I was in 2018.\n\nLo:\n\n----------------------------------------\n\n[...S]ome of the main reasons I'm not extremely pessimistic about artificial general intelligence outcomes.\n\n(Warning: one-sided lists of consid",
      "wordCount": 1952
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nPauymrHwpoNr6ipx",
    "title": "Conversation on technology forecasting and gradualism",
    "slug": "conversation-on-technology-forecasting-and-gradualism",
    "url": null,
    "baseScore": 108,
    "voteCount": 33,
    "viewCount": null,
    "commentCount": 30,
    "createdAt": null,
    "postedAt": "2021-12-09T21:23:21.187Z",
    "contents": {
      "markdown": "This post is a transcript of a multi-day discussion between Paul Christiano, Richard Ngo, Eliezer Yudkowsky, Rob Bensinger, Holden Karnofsky, Rohin Shah, Carl Shulman, Nate Soares, and Jaan Tallinn, following up on the Yudkowsky/Christiano debate in [1](https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds), [2](https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-shulman-and-yudkowsky-on-ai-progress), [3](https://www.lesswrong.com/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress), and [4](https://www.lesswrong.com/posts/fS7Zdj2e2xMqE6qja/more-christiano-cotra-and-yudkowsky-on-ai-progress).\n\nColor key:\n\n<table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Chat by Paul, Richard, and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid hsl(0, 0%, 0%)\">&nbsp;Other chat&nbsp;</td></tr></tbody></table>\n\n12\\. Follow-ups to the Christiano/Yudkowsky conversation\n========================================================\n\n12.1. Bensinger and Shah on prototypes and technological forecasting\n--------------------------------------------------------------------\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][16:22]&nbsp;</strong> <strong>(Sep. 23)</strong>&nbsp;</p><p>Quoth Paul:</p><blockquote><p>seems like you have to make the wright flyer much better before it's important, and that it becomes more like an industry as that happens, and that this is intimately related to why so few people were working on it</p></blockquote><p>Is this basically saying 'the Wright brothers didn't personally capture much value by inventing heavier-than-air flying machines, and this was foreseeable, which is why there wasn't a huge industry effort already underway to try to build such machines as fast as possible.' ?</p><p>My maybe-wrong model of Eliezer says here 'the Wright brothers knew a (Thielian) secret', while my maybe-wrong model of Paul instead says:</p><ul><li>They didn't know a secret -- it was obvious to tons of people that you could do something sorta like what the Wright brothers did and thereby invent airplanes; the Wright brothers just had unusual non-monetary goals that made them passionate to do a thing most people didn't care about.</li><li>Or maybe it's better to say: they knew some specific secrets about physics/engineering, but only because other people <i>correctly</i> saw 'there are secrets to be found here, but they're stamp-collecting secrets of little economic value to me, so I won't bother to learn the secrets'. ~Everyone knows where the treasure is located, and ~everyone knows the treasure won't make you rich.</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][17:24]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>My model of Paul says there could be a secret, but only because the industry was tiny and the invention was nearly worthless directly.</p><figure class=\"table\"><table><tbody><tr><td>[Cotra: ➕]</td></tr></tbody></table></figure></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][17:53]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>I mean, I think they knew a bit of stuff, but it generally takes a lot of stuff to make something valuable, and the more people have been looking around in an area the more confident you can be that it's going to take a lot of stuff to do much better, and it starts to look like an extremely strong regularity for big industries like ML or semiconductors</p><p>it's pretty rare to find small ideas that don't take a bunch of work to have big impacts</p><p>I don't know exactly what a thielian secret is (haven't read the reference and just have a vibe)</p><p>straightening it out a bit, I have 2 beliefs that combine disjunctively: (i) generally it takes a lot of work to do stuff, as a strong empirical fact about technology, (ii) generally if the returns are bigger there are more people working on it, as a slightly-less-strong fact about sociology</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][18:09]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>secrets = important undiscovered information (or information that's been discovered but isn't widely known), that you can use to get an edge in something. <a href=\"https://www.lesswrong.com/posts/ReB7yoF22GuerNfhH/thiel-on-secrets-and-indefiniteness\">https://www.lesswrong.com/posts/ReB7yoF22GuerNfhH/thiel-on-secrets-and-indefiniteness</a></p><p>There seems to be a Paul/Eliezer disagreement about how common these are in general. And maybe a disagreement about how much more efficiently humanity discovers and propagates secrets as you scale up the secret's value?</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][18:35]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>Many times it has taken much work to do stuff; there's further key assertions here about \"It takes $100 billion\" and \"Multiple parties will invest $10B first\" and \"$10B gets you a lot of benefit first because scaling is smooth and without really large thresholds\".</p><p>Eliezer is like \"ah, yes, sometimes it takes 20 or even 200 people to do stuff, but core researchers often don't scale well past 50, and there aren't always predecessors that could do a bunch of the same stuff\" even though Eliezer agrees with \"it often takes a lot of work to do stuff\". More premises are needed for the conclusion, that one alone does not distinguish Eliezer and Paul by enough.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][20:03]</strong> &nbsp;<strong>(Sep. 23)</strong>&nbsp;</p><p>My guess is that everyone agrees with claims 1, 2, and 3 here (please let me know if I'm wrong!):</p><p>1. The history of humanity looks less like <strong>Long Series of Cheat Codes World</strong>, and more like <strong>Well-Designed Game World</strong>.</p><p>In Long Series of Cheat Codes World, human history looks like this, over and over: Some guy found a cheat code that totally outclasses everyone else and makes him God or Emperor, until everyone else starts using the cheat code too (if the Emperor allows it). After which things are maybe normal for another 50 years, until a new Cheat Code arises that makes its first adopters invincible gods relative to the previous tech generation, and then the cycle repeats.</p><p>In Well-Designed Game World, you can sometimes eke out a small advantage, and the balance isn't <i>perfect</i>, but it's pretty good and the leveling-up tends to be gradual. A level 100 character totally outclasses a level 1 character, and some level transitions are a bigger deal than others, but there's no level that makes you a god relative to the people one level below you.</p><p>2. General intelligence took over the world once. Someone who updated on that fact but otherwise hasn't thought much about the topic should not consider it 'bonkers' that machine general intelligence could take over the world too, even though they should still consider it 'bonkers' that eg a coffee startup could take over the world.</p><p>(Because beverages have never taken over the world before, whereas general intelligence has; and because our inside-view models of coffee and of general intelligence make it a lot harder to imagine plausible mechanisms by which coffee could make someone emperor, kill all humans, etc., compared to general intelligence.)</p><p>(In the game analogy, the situation is a bit like 'I've never found a crazy cheat code or exploit in this game, but I haven't ruled out that there is one, and I heard of a character once who did a lot of crazy stuff that's at least <i>suggestive</i> that she might have had a cheat code.')</p><p>3. AGI is arising in a world where agents with science and civilization already exist, whereas humans didn't arise in such a world. This is one reason to think AGI might not take over the world, but it's <i>not</i> a strong enough consideration on its own to make the scenario 'bonkers' (because AGIs are likely to differ from humans in many respects, and it wouldn't obviously be bonkers if the first AGIs turned out to be qualitatively way smarter, cheaper to run, etc.).</p><p>---</p><p>If folks agree with the above, then I'm confused about how one updates from the above epistemic state to 'bonkers'.</p><p>It was to a large extent physics facts that determined how easy it was to understand the feasibility of nukes without (say) decades of very niche specialized study. Likewise, it was physics facts that determined you need rare materials, many scientists, and a large engineering+infrastructure project to build a nuke. In a world where the <i>physics</i> of nukes resulted in it being some PhD's quiet 'nobody thinks this will work' project like Andrew Wiles secretly working on a proof of Fermat's Last Theorem for seven years, that would have <i>happened</i>.</p><p>If an alien came to me in 1800 and told me that totally new physics would let future humans build city-destroying superbombs, then I don't see why I should have considered it bonkers that it might be lone mad scientists rather than nations who built the first superbomb. The 'lone mad scientist' scenario sounds more conjunctive to me (assumes the mad scientist knows something that isn't widely known, AND has the ability to act on that knowledge without tons of resources), so I guess it should have gotten less probability, but maybe not dramatically less?</p><p>'Mad scientist builds city-destroying weapon in basement' sounds wild to me, but I feel like almost all of the actual unlikeliness comes from the 'city-destroying weapons exist at all' part, and then the other parts only moderately lower the probability.</p><p>Likewise, I feel like the prima-facie craziness of basement AGI mostly comes from 'generally intelligence is a crazy thing, it's wild that anything could be that high-impact', and a much smaller amount comes from 'it's wild that something important could happen in some person's basement'.</p><p>---</p><p>It <i>does</i> structurally make sense to me that Paul might know things I don't about GPT-3 and/or humans that make it obvious to him that we roughly know the roadmap to AGI and it's this.</p><p>If the entire 'it's bonkers that some niche part of ML could crack open AGI in 2026 and reveal that GPT-3 (and the mainstream-in-2026 stuff) was on a very different part of the tech tree' view is coming from a detailed inside-view model of intelligence like this, then that immediately ends my confusion about the argument structure.</p><p>I don't understand why you think you have the roadmap, and given a high-confidence roadmap I'm guessing I'd still put more probability than you on someone finding a very different, shorter path that works too. But the <i>argument structure</i> \"roadmap therefore bonkers\" makes sense to me.</p><p>If there are meant to be <i>other</i> arguments against 'high-impact AGI via niche ideas/techniques' that are strong enough to make it bonkers, then I remain confused about the argument structure and how it can carry that much weight.</p><p>I can imagine an inside-view model of human cognition, GPT-3 cognition, etc. that tells you 'AGI coming from nowhere in 3 years is bonkers'; I can't imagine an ML-is-a-reasonably-efficient-market argument that does the same, because even a perfectly efficient market isn't <i>omniscient</i> and can still be surprised by undiscovered physics facts that tell you 'nukes are relatively easy to build' and 'the fastest path to nukes is relatively hard to figure out'.</p><p>(Caveat: I'm using the 'basement nukes' and 'Fermat's last theorem' analogy because it helps clarify the principles involved, not because I think AGI will be that extreme on the spectrum.)</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure><p>Oh, I also wouldn't be confused by a view like \"I think it's 25% likely we'll see a more Eliezer-ish world. But it sounds like Eliezer is, like, 90% confident that will happen, and <i>that level of confidence</i> (and/or the weak reasoning he's provided for that confidence) seems bonkers to me.\"</p><p>The thing I'd be confused by is e.g. \"ML is efficient-ish, therefore <i>the out-of-the-blue-AGI scenario itself</i> is bonkers and gets, like, 5% probability.\"</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][1:58]</strong> &nbsp;<strong>(Sep. 24)</strong>&nbsp;</p><p>(I'm unclear on whether this is acceptable for this channel, please let me know if not)</p><blockquote><p>I can't imagine an ML-is-a-reasonably-efficient-market argument that does the same, because even a perfectly efficient market isn't omniscient and can still be surprised by undiscovered physics facts</p></blockquote><p>I think this seems right as a first pass.</p><p>Suppose we then make the empirical observation that in tons and tons of other fields, it is extremely rare that people discover new facts that lead to immediate impact. (Set aside for now whether or not that's true; assume that it is.) Two ways you could react to this:</p><p>1. Different fields are different fields. It's not like there's a common generative process that outputs a distribution of facts and how hard they are to find that is common across fields. Since there's no common generative process, facts about field X shouldn't be expected to transfer to make predictions about field Y.</p><p>2. There's some latent reason, that we don't currently know, that makes it so that it is rare for newly discovered facts to lead to immediate impact.</p><p>It seems like you're saying that (2) is not a reasonable reaction (i.e. \"not a valid argument structure\"), and I don't know why. There are lots of things we don't know, is it really so bad to posit one more?</p><p>(Once we agree on the argument structure, we should then talk about e.g. reasons why such a latent reason can't exist, or possible guesses as to what the latent reason is, etc, but fundamentally I feel generally okay with starting out with \"there's probably some reason for this empirical observation, and absent additional information, I should expect that reason to continue to hold\".)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][3:15]</strong> &nbsp;<strong>(Sep. 24)</strong>&nbsp;</p><p>I think 2 is a valid argument structure, but I didn't mention it because I'd be surprised if it had enough evidential weight (in this case) to produce an 'update to bonkers'. I'd love to hear more about this if anyone thinks I'm under-weighting this factor. (Or any others I left out!)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][23:57]</strong> &nbsp;<strong>(Sep. 25)</strong>&nbsp;</p><p>Idk if it gets all the way to \"bonkers\", but (2) seems pretty strong to me, and is how I would interpret Paul-style arguments on timelines/takeoff if I were taking on what-I-believe-to-be your framework</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][11:06]</strong> &nbsp;<strong>(Sep. 25)</strong>&nbsp;</p><p>Well, I'd love to hear more about that!</p><p>Another way of getting at my intuition: I feel like a view that assigns very small probability to 'suddenly vastly superhuman AI, because something that high-impact hasn't happened before'</p><p>(which still seems weird to me, because physics doesn't know what 'impact' is and I don't see what physical mechanism could forbid it that strongly and generally, short of simulation hypotheses)</p><p>... would also assign very small probability in 1800 to 'given an alien prediction that totally new physics will let us build superbombs at least powerful enough to level cities, the superbomb in question will ignite the atmosphere or otherwise destroy the Earth'.</p><p>But this seems flatly wrong to me -- if you buy that the bomb works by a totally different mechanism (and exploits a different physics regime) than eg gunpowder, then the output of the bomb is a <i>physics</i> question, and I don't see how we can concentrate our probability mass much without probing the relevant physics. The history of boat and building sizes is a negligible input to 'given a totally new kind of bomb that suddenly lets us (at least) destroy cities, what is the total destructive power of the bomb?'.</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure><p>(Obviously the bomb <i>didn't</i> destroy the Earth, and I wouldn't be surprised if there's some Bayesian evidence or method-for-picking-a-prior that could have validly helped you suspect as much in 1800? But it would be a suspicion, not a confident claim.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][1:45]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><blockquote><p>would also assign very small probability in 1800 to 'given an alien prediction that totally new physics will let us build superbombs at least powerful enough to level cities, the superbomb in question will ignite the atmosphere or otherwise destroy the Earth'</p></blockquote><p>(As phrased you also have to take into account the question of whether humans would deploy the resulting superbomb, but I'll ignore that effect for now.)</p><p>I think this isn't exactly right. The \"totally new physics\" part seems important to update on.</p><p>Let's suppose that, in the reference class we built of boat and building sizes, empirically nukes were the 1 technology out of 20 that had property X. (Maybe X is something like \"discontinuous jump in things humans care about\" or \"immediate large impact on the world\" or so on.) Then, I think in 1800 you assign ~5% to 'the first superbomb at least powerful enough to level cities will ignite the atmosphere or otherwise destroy the Earth'.</p><p>Once you know more details about how the bomb works, you should be able to update away from 5%. Specifically, \"entirely new physics\" is an important detail that causes you to update away from 5%. I wouldn't go as far as you in throwing out reference classes entirely at that point -- there can still be unknown latent factors that apply at the level of physics -- but I agree reference classes look harder to use in this case.</p><p>With AI, I start from ~5% and then I don't really see any particular detail for AI that I think I should strongly update on. My impression is that Eliezer thinks that \"general intelligence\" is a qualitatively different sort of thing than that-which-neural-nets-are-doing, and maybe that's what's analogous to \"entirely new physics\". I'm pretty unconvinced of this, but something in this genre feels quite crux-y for me.</p><p>Actually, I think I've lost the point of this analogy. What's the claim for AI that's analogous to</p><blockquote><p>'given an alien prediction that totally new physics will let us build superbombs at least powerful enough to level cities, the superbomb in question will ignite the atmosphere or otherwise destroy the Earth'</p></blockquote><p>?</p><p>Like, it seems like this is saying \"We figure out how to build a new technology that does X. What's the chance it has side effect Y?\" Where X and Y are basically unrelated.</p><p>I was previously interpreting the argument as \"if we know there's a new superbomb based on totally new physics, and we know that the first such superbomb is at least capable of leveling cities, what's the probability it would have enough destructive force to also destroy the world\", but upon rereading that doesn't actually seem to be what you were gesturing at.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][3:08]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><p>I'm basically responding to this thing Ajeya wrote:</p><blockquote><p>I think Paul's view would say:</p><ul><li>Things certainly happen for the first time</li><li>When they do, they happen at small scale in shitty prototypes, like the Wright Flyer or GPT-1 or AlphaGo or the Atari bots or whatever</li><li>When they're making a big impact on the world, it's after a lot of investment and research, like commercial aircrafts in the decades after Kitty Hawk or like the investments people are in the middle of making now with AI that can assist with coding</li></ul></blockquote><p>To which my reply is: I agree that the first AGI systems will be shitty compared to <i>later</i> AGI systems. But Ajeya's Paul-argument seems to additionally require that AGI systems be relatively unimpressive at cognition compared to preceding AI systems that weren't AGI.</p><p>If this is because of some general law that things are shitty / low-impact when they \"happen for the first time\", then I don't understand what physical mechanism could produce such a general law that holds with such force.</p><p>As I see it, physics 'doesn't care' about human conceptions of impactfulness, and will instead produce AGI prototypes, aircraft prototypes, and nuke prototypes that have as much impact as is implied by the detailed case-specific workings of general intelligence, flight, and nuclear chain reactions respectively.</p><p>We could frame the analogy as:</p><ul><li>'If there's a year where AI goes from being unable to do competitive par-human reasoning in the hard sciences, to being able to do such reasoning, we should estimate the impact of the first such systems by drawing on our beliefs about par-human scientific reasoning itself.'</li><li>Likewise: 'If there's a year where explosives go from being unable to destroy cities to being able to destroy cities, we should estimate the impact of the first such explosives by drawing on our beliefs about how (current or future) physics might allow a city to be destroyed, and what other effects or side-effects such a process might have. We should spend little or no time thinking about the impactfulness of the first steam engine or the first telescope.'</li></ul></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][3:14]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><p>Seems like your argument is something like \"when there's a zero-to-one transition, then you have to make predictions based on reasoning about the technology itself\". I think in that case I'd say this thing from above:</p><blockquote><p>My impression is that Eliezer thinks that \"general intelligence\" is a qualitatively different sort of thing than that-which-neural-nets-are-doing, and maybe that's what's analogous to \"entirely new physics\". I'm pretty unconvinced of this, but something in this genre feels quite crux-y for me.</p></blockquote><p>(Like, you wouldn't a priori expect anything special to happen once conventional bombs become big enough to demolish a football stadium for the first time. It's because nukes are based on \"totally new physics\" that you might expect unprecedented new impacts from nukes. What's the analogous thing for AGI? Why isn't AGI just regular AI but scaled up in a way that's pretty continuous?)</p><p>I'm curious if you'd change your mind if you were convinced that AGI is just regular AI scaled up, with no qualitatively new methods -- I expect you wouldn't but idk why</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][4:03]</strong> &nbsp;<strong>(Sep. 27)</strong>&nbsp;</p><p>In my own head, the way I think of 'AGI' is basically: \"Something happened that allows humans to do biochemistry, materials science, particle physics, etc., even though none of those things were present in our environment of evolutionary adaptedness. Eventually, AI will similarly be able to generalize to biochemistry, materials science, particle physics, etc. We can call that kind of AI 'AGI'.\"</p><p>There might be facts I'm unaware of that justify conclusions like 'AGI is mostly just a bigger version of current ML systems like GPT-3', and there might be facts that justify conclusions like 'AGI will be preceded by a long chain of predecessors, each slightly less general and slightly less capable than its successor'.</p><p>But if so, I'm assuming those will be facts about CS, human cognition, etc., not at all a list of a hundred facts like 'the first steam engine didn't take over the world', 'the first telescope didn't take over the world'.... Because the physics of brains doesn't care about those things, and because in discussing brains we're already in 'things that have been known to take over the world' territory.</p><p>(I think that paying much attention <i>at all</i> to the technology-wide base rate for 'does this allow you to take over the world?', once you already know you're doing something like 'inventing a new human', doesn't really make sense at all? It sounds to me like going to a bookstore and then repeatedly worrying 'What if they don't have the book I'm looking for? Most stores don't sell books at all, so this one might not have the one I want.' If you know it's a <i>book</i> store, then you shouldn't be thinking at that level of generality at all; the base rate just goes out the window.)</p><figure class=\"table\"><table><tbody><tr><td>[Yudkowsky: +1]</td></tr></tbody></table></figure><p>My way of thinking about AGI is pretty different from saying AGI follows 'totally new mystery physics' -- I'm explicitly anchoring to a known phenomenon, humans.</p><p>The analogous thing for nukes might be 'we're going to build a bomb that uses processes kind of like the ones found in the Sun in order to produce enough energy to destroy (at least) a city'.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][0:44]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><blockquote><p>The analogous thing for nukes might be 'we're going to build a bomb that uses processes kind of like the ones found in the Sun in order to produce enough energy to destroy (at least) a city'.</p></blockquote><p>(And I assume the contentious claim is \"that bomb would then ignite the atmosphere, destroy the world, or otherwise have hugely more impact than just destroying a city\".)</p><p>In 1800, we say \"well, we'll probably just make existing fires / bombs bigger and bigger until they can destroy a city, so we shouldn't expect anything particularly novel or crazy to happen\", and assign (say) 5% to the claim.</p><p>There is a wrinkle: you said it was processes like the ones found in the Sun. Idk what the state of knowledge was like in 1800, but maybe they knew that the Sun couldn't be a conventional fire. If so, then they could update to a higher probability.</p><p>(You could also infer that since someone bothered to mention \"processes like the ones found in the Sun\", those processes must be ones we don't know yet, which also allows you to make that update. I'm going to ignore that effect, but I'll note that this is one way in which the phrasing of the claim is incorrectly pushing you in the direction of \"assign higher probability\", and I think a similar thing happens for AI when saying \"processes like those in the human brain\".)</p><p>With AI I don't see why the human brain is a different kind of thing than (say) convnets. So I feel more inclined to just take the starting prior of 5%.</p><p>Presumably you think that assigning 5% to the nukes claim in 1800 was incorrect, even if that perspective doesn't know that the Sun is not just a very big conventional fire. I'm not sure why this is. According to me this is just the natural thing to do because things are usually continuous and so in the absence of detailed knowledge that's what your prior should be. (If I had to justify this, I'd point to facts about bridges and buildings and materials science and so on.)</p><blockquote><p>there might be facts that justify conclusions like 'AGI will be preceded by a long chain of slightly-less-general, slightly-less-capable successors'.</p></blockquote><p>The frame of \"justify[ing] conclusions\" seems to ask for more confidence than I expect to get. Rather I feel like I'm setting an initial prior that could then be changed radically by engaging with details of the technology. And then I'm further saying that I don't see any particular details that should cause me to update away significantly (but they could arise in the future).</p><p>For example, suppose I have a random sentence generator, and I take the first well-formed claim it spits out. (I'm using a random sentence generator so that we don't update on the process by which the claim was generated.) This claim turns out to be \"Alice has a fake skeleton hidden inside her home\". Let's say we know nothing about Alice except that she is a real person somewhere in the US who has a home. You can still assign &lt; 10% probability to the claim, and take 10:1 bets with people who don't know any additional details about Alice. Nonetheless, as you learn more about Alice, you could update towards higher probability, e.g. if you learn that she loves Halloween, that's a modest update; if you learn she runs a haunted house at Halloween every year, that's a large update; if you go to her house and see the fake skeleton you can update to ~100%. That's the sort of situation I feel like we're in with AI.</p><p>If you asked me what facts justify the conclusion that Alice probably doesn't have a fake skeleton hidden inside her house, I could only point to reference classes, and all the other people I've met who don't have such skeletons. This is not engaging with the details of Alice's situation, and I could similarly say \"if I wanted to know about Alice, surely I should spend most of my time learning about Alice, rather than looking at what Bob and Carol did\". Nonetheless, it is still correct to assign &lt; 10% to the claim.</p><p>It really does seem to come down to -- why is human-level intelligence such a special turning point that should receive special treatment? Just as you wouldn't give special treatment to \"the first time bridges were longer than 10m\", it doesn't seem obvious that there's anything all that special at the point where AIs reach human-level intelligence (at least for the topics we're discussing; there are obvious reasons that's an important point when talking about the economic impact of AI)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Tallinn][7:04]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>FWIW, my current 1-paragraph compression of the debate positions is something like:</p><p><strong>catastrophists</strong>: when evolution was gradually improving hominid brains, suddenly something clicked - it stumbled upon the core of general reasoning - and hominids went from banana classifiers to spaceship builders. hence we should expect a similar (but much sharper, given the process speeds) discontinuity with AI.</p><p><strong>gradualists</strong>: no, there was no discontinuity with hominids per se; human brains merely reached a threshold that enabled cultural accumulation (and in a meaningul sense it was <i>culture</i> that built those spaceships). similarly, we should not expect sudden discontinuities with AI per se, just an accelerating (and possibly unfavorable to humans) cultural changes as human contributions will be automated away.</p><p>—</p><p>one possible crux to explore is “how thick is culture”: is it something that AGI will quickly decouple from (dropping directly to physics-based ontology instead) OR will culture remain AGI’s main environment/ontology for at least a decade.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][11:18]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><blockquote><p>FWIW, my current 1-paragraph compression of the debate positions is something like:</p><p><strong>catastrophists</strong>: when evolution was gradually improving hominid brains, suddenly something clicked - it stumbled upon the core of general reasoning - and hominids went from banana classifiers to spaceship builders. hence we should expect a similar (but much sharper, given the process speeds) discontinuity with AI.</p><p><strong>gradualists</strong>: no, there was no discontinuity with hominids per se; human brains merely reached a threshold that enabled cultural accumulation (and in a meaningul sense it was <i>culture</i> that built those spaceships). similarly, we should not expect sudden discontinuities with AI per se, just an accelerating (and possibly unfavorable to humans) cultural changes as human contributions will be automated away.</p><p>—</p><p>one possible crux to explore is “how thick is culture”: is it something that AGI will quickly decouple from (dropping directly to physics-based ontology instead) OR will culture remain AGI’s main environment/ontology for at least a decade.</p></blockquote><p>Clarification: in the sentence \"just an accelerating (and possibly unfavorable to humans) cultural changes as human contributions will be automated away\", what work is \"cultural changes\" doing? Could we just say \"changes\" (including economic, cultural, etc) instead?</p><blockquote><p>In my own head, the way I think of 'AGI' is basically: \"Something happened that allows humans to do biochemistry, materials science, particle physics, etc., even though none of those things were present in our environment of evolutionary adaptedness. Eventually, AI will similarly be able to generalize to biochemistry, materials science, particle physics, etc. We can call that kind of AI 'AGI'.\"</p><p>There might be facts I'm unaware of that justify conclusions like 'AGI is mostly just a bigger version of current ML systems like GPT-3', and there might be facts that justify conclusions like 'AGI will be preceded by a long chain of predecessors, each slightly less general and slightly less capable than its successor'.</p><p>But if so, I'm assuming those will be facts about CS, human cognition, etc., not at all a list of a hundred facts like 'the first steam engine didn't take over the world', 'the first telescope didn't take over the world'.... Because the physics of brains doesn't care about those things, and because in discussing brains we're already in 'things that have been known to take over the world' territory.</p><p>(I think that paying much attention <i>at all</i> to the technology-wide base rate for 'does this allow you to take over the world?', once you already know you're doing something like 'inventing a new human', doesn't really make sense at all? It sounds to me like going to a bookstore and then repeatedly worrying 'What if they don't have the book I'm looking for? Most stores don't sell books at all, so this one might not have the one I want.' If you know it's a <i>book</i> store, then you shouldn't be thinking at that level of generality at all; the base rate just goes out the window.)</p></blockquote><p>I'm broadly sympathetic to the idea that claims about AI cognition should be weighted more highly than claims about historical examples. But I think you're underrating historical examples. There are at least three ways those examples can be informative - by telling us about:</p><p>1. Domain similarities</p><p>2. Human effort and insight</p><p>3. Human predictive biases</p><p>You're mainly arguing against 1, by saying that there are facts about physics, and facts about intelligence, and they're not very related to each other. This argument is fairly compelling to me (although it still seems plausible that there are deep similarities which we don't understand yet - e.g. the laws of statistics, which apply to many different domains).</p><p>But historical examples can also tell us about #2 - for instance, by giving evidence that great leaps of insight are rare, and so if there exists a path to AGI which doesn't require great leaps of insight, that path is more likely than one which does.</p><p>And they can also tell us about #3 - for instance, by giving evidence that we usually overestimate the differences between old and new technologies, and so therefore those same biases might be relevant to our expectations about AGI.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Bensinger][12:31]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>In the 'alien warns about nukes' example, my intuition is that 'great leaps of insight are rare' and 'a random person is likely to overestimate the importance of the first steam engines and telescopes' tell me practically nothing, compared to what even a small amount of high-uncertainty physics reasoning tells me.</p><p>The 'great leap of insight' part tells me ~nothing because even if there's an easy low-insight path to nukes and a hard high-insight path, I don't thereby know the explosive yield of a bomb on either path (either absolutely or relatively); it depends on how nukes work.</p><p>Likewise, I don't think 'a random person is likely to overestimate the first steam engine' really helps with estimating the power of nuclear explosions. I could <i>imagine</i> a world where this bias exists and is so powerful and inescapable it ends up being a big weight on the scales, but I don't think we live in that world?</p><p>I'm not even sure that a random person <i>would</i> overestimate the importance of prototypes in general. Probably, I guess? But my intuition is still that you're better off in 1800 focusing on physics calculations rather than the tug-of-war 'maybe X is cognitively biasing me in <i>this</i> way, no wait maybe Y is cognitively biasing me in this other way, no wait...'</p><p>Our situation might not be analogous to the 1800-nukes scenario (e.g., maybe we know by observation that current ML systems are basically scaled-down humans). But if it <i>is</i> analogous, then I think the history-of-technology argument is not very useful here.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Tallinn][13:00]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>re “cultural changes”: yeah, sorry, i meant “culture” in very general “substrate of human society” sense. “cultural changes” would then include things like changes in power structures and division of labour, but <i>not</i> things like “diamondoid bacteria killing all humans in 1 second” (that would be a change in humans, not in the culture)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shah][13:09]</strong> &nbsp;<strong>(Sep. 28)</strong>&nbsp;</p><p>I want to note that I agree with your (Rob's) latest response, but I continue to think most of the action is in whether AGI involves something analogous to \"totally new physics\", where I would guess \"no\" (and would do so particularly strongly for shorter timelines).</p><p>(And I would still point to historical examples for \"many new technologies don't involve something analogous to 'totally new physics'\", and I'll note that Richard's #2 about human effort and insight still applies)</p></td></tr></tbody></table>\n\n12.2. Yudkowsky on Steve Jobs and gradualism\n--------------------------------------------\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][15:26]</strong>&nbsp;<strong> (Sep. 28)</strong>&nbsp;</p><p>So recently I was talking with various people about the question of why, for example, Steve Jobs could not find somebody else with UI taste 90% as good as his own, to take over Apple, even while being able to pay infinite money. A successful founder I was talking to was like, \"Yep, I sure would pay $100 million to hire somebody who could do 80% of what I can do, in fact, people have earned more than that for doing less.\"</p><p>I wondered if OpenPhil was an exception to this rule, and people with more contact with OpenPhil seemed to think that OpenPhil did not have 80% of a Holden Karnofsky (besides Holden).</p><p>And of course, what sparked this whole thought process in me, was that I'd staked all the effort I put into the Less Wrong sequences, into the belief that if I'd managed to bring myself into existence, then there ought to be lots of young near-Eliezers in Earth's personspace including some with more math talent or physical stamina not so unusually low, who could be started down the path to being Eliezer by being given a much larger dose of concentrated hints than I got, starting off the compounding cascade of skill formations that I saw as having been responsible for producing me, \"on purpose instead of by accident\".</p><p>I see my gambit as having largely failed, just like the successful founder couldn't pay $100 million to find somebody 80% similar in capabilities to himself, and just like Steve Jobs could not find anyone to take over Apple for presumably much larger amounts of money and status and power. Nick Beckstead had some interesting stories about various ways that Steve Jobs had tried to locate successors (which I wasn't even aware of).</p><p>I see a plausible generalization as being a \"Sparse World Hypothesis\": The shadow of an Earth with eight billion people, projected into some dimensions, is much sparser than plausible arguments might lead you to believe. Interesting people have few neighbors, even when their properties are collapsed and projected onto lower-dimensional tests of output production. The process of forming an interesting person passes through enough 0-1 critical thresholds that all have to be passed simultaneously in order to start a process of gaining compound interest in various skills, that they then cannot find other people who are 80% as good as what they <i>do</i> (never mind being 80% similar to them as people).</p><p>I would expect human beings to start out much denser in a space of origins than AI projects, and for the thresholds and compounding cascades of our mental lives to be much less sharp than chimpanzee-human gaps.</p><p>Gradualism about humans sure sounds totally reasonable! It is in fact much more plausible-sounding a priori than the corresponding proposition about AI projects! I staked years of my own life on the incredibly reasoning-sounding theory that if one actual Eliezer existed then there should be lots of neighbors near myself that I could catalyze into existence by removing some of the accidental steps from the process that had accidentally produced me.</p><p>But it didn't work in real life because plausible-sounding gradualist arguments just... plain don't work in real life even though they sure sound plausible. I spent a lot of time arguing with Robin Hanson, who was more gradualist than I was, and was taken by surprise when reality itself was much less gradualist than I was.</p><p>My model has Paul or Carl coming back with some story about how, why, no, it is totally reasonable that Steve Jobs couldn't find a human who was 90% as good at a problem class as Steve Jobs to take over Apple for billions of dollars despite looking, and, why, no, this is not at all a falsified retroprediction of the same gradualist reasoning that says a leading AI project should be inside a dense space of AI projects that projects onto a dense space of capabilities such that it has near neighbors.</p><p>If so, I was not able to use this hypothetical model of <i>selective</i> gradualist reasoning to deduce in advance that replacements for myself would be sparse in the same sort of space and I'd end up unable to replace myself.</p><p>I do not really believe that, without benefits of hindsight, the advance predictions of gradualism would differ between the two cases.</p><p>I think if you don't peek at the answer book in advance, the same sort of person who finds it totally reasonable to expect successful AI projects to have close lesser earlier neighbors, would also find it totally reasonable to think that Steve Jobs definitely ought to be able to find somebody 90% as good to take over his job - and should actually be able to find somebody <i>much</i> better because Jobs gets to run a wider search and offer more incentive than when Jobs was wandering into early involvement in Apple.</p><p>It's completely reasonable-sounding! Totally plausible to a human ear! Reality disagrees. Jobs tried to find a successor, couldn't, and now the largest company in the world by market cap seems no longer capable of sending the iPhones back to the designers and asking them to do something important differently.</p><p>This is part of the story for why I put gradualism into a mental class of \"arguments that sound plausible and just fail in real life to be binding on reality; reality says 'so what' and goes off to do something else\".</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][17:46] &nbsp;(Sep. 28)</strong>&nbsp;</p><p>It feels to me like a common pattern is: I say that ML in particular, and most technologies in general, seem to improve quite gradually on metrics that people care about or track. You say that some kind of \"gradualism\" worldview predicts a bunch of other stuff (some claim about markets or about steve jobs or whatever that feels closely related on your view but not mine). But it feels to me like there are just a ton of technologies, and a ton of AI benchmarks, and those are just <i>much</i> more analogous to \"future AI progress.\" I know that to you this feels like reference class tennis, but I think I legitimately don't understand what kind of approach to forecasting you are using that lets you just make (what I see as) the obvious boring prediction about all of the non-AGI technologies.</p><p>Perhaps you are saying that symmetrically you don't understand what approach to forecasting I'm using, that would lead me to predict that technologies improve gradually yet people vary greatly in their abilities. To me it feels like the simplest thing in the world: I expect future technological progress in domain X to be like past progress in domain X, and future technological progress to be like past technological progress, and future market moves to be like past market moves, and future elections to be like past elections.</p><p>And it seems like you <i>must</i> be doing something that ends up making almost the same predictions as that almost all the time, which is why you don't get incredibly surprised every single year by continuing boring and unsurprising progress in batteries or solar panels or robots or ML or computers or microscopes or whatever. Like it's fine if you say \"Yes, those areas have trend breaks sometimes\" but there are <i>so many</i> boring years that you must somehow be doing something like having the baseline \"this year is probably going to be boring.\"</p><p>Such that intuitively it feels to me like the disagreement between us <i>must</i> be in the part where AGI feels to me like it is similar to AI-to-date and feels to you like it is very different and better compared to evolution of life or humans.</p><p>It has to be the kind of argument that you can make about progress-of-AI-on-metrics-people-care-about, but <i>not</i> progress-of-other-technologies-on-metrics-people-care-about, otherwise it seems like you are getting hammered every boring year for every boring technology.</p><p>I'm glad we have the disagreement on record where I expect ML progress to continue to get less jumpy as the field grows, and maybe the thing to do is just poke more at that since it is definitely a place where I gut level expect to win bayes points and so could legitimately change my mind on the \"which kinds of epistemic practices work better?\" question. But it feels like it's not the main action, the main action has got to be about you thinking that there is a really impactful change somewhere between {modern AI, lower animals} and {AGI, humans} that doesn't look like ongoing progress in AI.</p><p>I think \"would GPT-3 + 5 person-years of engineering effort foom?\" feels closer to core to me.</p><p>(That said, the way AI could be different need not feel like \"progress is lumpier,\" could totally be more like \"Progress is always kind of lumpy, which Paul calls 'pretty smooth' and Eliezer calls 'pretty lumpy' and doesn't lead to any disagreements; but Eliezer thinks AGI is different in that kind-of-lumpy progress leads to fast takeoff, while Paul thinks it just leads to kind-of-lumpy increases in the metrics people care about or track.\")</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][7:46] &nbsp;(Sep. 29)</strong>&nbsp;</p><blockquote><p>I think \"would GPT-3 + 5 person-years of engineering effort foom?\" feels closer to core to me.</p></blockquote><p>I truly and legitimately cannot tell which side of this you think we should respectively be on. My guess is you're against GPT-3 fooming because it's too low-effort and a short timeline, even though I'm the one who thinks GPT-3 isn't on a smooth continuum with AGI??</p><p>With that said, the rest of this feels on-target to me; I sure do feel like {natural selection, humans, AGI} form an obvious set with each other, though even there the internal differences are too vast and the data too scarce for legit outside viewing.</p><blockquote><p>I truly and legitimately cannot tell which side of this you think we should respectively be on. My guess is you're against GPT-3 fooming because it's too low-effort and a short timeline, even though I'm the one who thinks GPT-3 isn't on a smooth continuum with AGI??</p></blockquote><p>I mean I obviously think you can foom starting from an empty Python file with 5 person-years of effort if you've got the Textbook From The Future; you wouldn't use the GPT code or model for anything in that, the Textbook says to throw it out and start over.</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Christiano][9:45] &nbsp;(Sep. 29)</strong>&nbsp;</p><p>I think GPT-3 will foom given very little engineering effort, it will just be much slower than the human foom</p><p>and then that timeline will get faster and faster over time</p><p>it's also fair to say that it wouldn't foom because the computers would break before it figured out how to repair them (and it would run out of metal before it figured out how to mine it, etc.), depending on exactly how you define \"foom,\" but the point is that \"you can repair the computers faster than they break\" happens much before you can outrun human civilization</p><p>so the relevant threshold you cross is the one where you are outrunning civilization</p><p>(and my best guess about human evolution is pretty similar, it looks like humans are smart enough to foom over a few hundred thousand years, and that we were the ones to foom because that is also roughly how long it was taking evolution to meaningfully improve our cognition---if we foomed slower it would have instead been a smarter successor who overtook us, if we foomed faster it would have instead been a dumber predecessor, though this is <i>much</i> less of a sure-thing than the AI case because natural selection is not trying to make something that fooms)</p><p>and regarding {natural selection, humans, AGI} the main question is why modern AI and homo erectus (or even chimps) aren't in the set</p><p>it feels like the core disagreement is that I mostly see a difference in degree between the various animals, and between modern AI and future AI, a difference that is likely to be covered by gradual improvements that are pretty analogous to contemporary improvements, and so as the AI community making contemporary improvements grows I get more and more confident that TAI will be a giant industry rather than an innovation</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Ngo][5:45]</strong>&nbsp;<strong> (Oct. 1)</strong>&nbsp;</p><p>Do you have a source on Jobs having looked hard for a successor who wasn't Tim Cook?</p><p>Also, I don't have strong opinions about how well Apple is doing now, so I default to looking at the share price, which seems very healthy.</p><p>(Although I note in advance that this doesn't feel like a particularly important point, roughly for the same reason that Paul mentioned: gradualism about Steve Jobs doesn't seem like a central example of the type of gradualism that informs beliefs about AI development.)</p></td></tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][10:40]</strong> &nbsp;<strong>(Oct. 1)</strong>&nbsp;</p><p>My source is literally \"my memory of stuff that Nick Beckstead just said to me in person\", maybe he can say more if we invite him.</p><p>I'm not quite sure what to do with the notion that \"gradualism about Steve Jobs\" is somehow less to be expected than gradualism about AGI projects. Humans are GIs. They are <i>extremely</i> similar to each other design-wise. There are a <i>lot</i> of humans, billions of them, many many many more humans than I expect AGI projects. Despite this the leading edge of human-GIs is sparse enough in the capability space that there is no 90%-of-Steve-Jobs that Jobs can locate, and there is no 90%-of-von-Neumann known to 20th century history. If we are not to take any evidence about this to A-GIs, then I do not understand the rules you're using to apply gradualism to some domains but not others.</p><p>And to be explicit, a skeptic who doesn't find these divisions intuitive, might well ask, \"Is gradualism perhaps isomorphic to 'The coin always comes up heads on Heady occasions', where 'Heady' occasions are determined by an obscure intuitive method going through some complicated nonverbalizable steps one of which is unfortunately 'check whether the coin actually came up heads'?\"</p><p>(As for my own theory, it's always been that AGIs are mostly like AGIs and not very much like humans or the airplane-manufacturing industry, and I do not, on my own account of things, appeal much to supposed outside viewing or base rates.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Shulman][11:11]</strong> &nbsp;<strong>(Oct. 2)</strong>&nbsp;</p><p>I think the way to apply it is to use observable data (drawn widely) and math.</p><p>Steve Jobs does look like a (high) draw (selected for its height, in the sparsest tail of the CEO distribution) out of the economic and psychometric literature (using the same kind of approach I use in other areas like estimating effects of introducing slightly superhuman abilities on science, the genetics of height, or wealth distributions). You have roughly normal or log-normal distributions on some measures of ability (with fatter tails when there are some big factors present, e.g. super-tall people are enriched for normal common variants for height but are more frequent than a Gaussian estimated from the middle range because of some weird disease/hormonal large effects). And we have lots of empirical data about the thickness and gaps there. Then you have a couple effects that can make returns in wealth/output created larger.</p><p>You get amplification from winner-take-all markets, IT, and scale that let higher ability add value to more places. This is the same effect that lets top modern musicians make so much money. Better CEOs get allocated to bigger companies because multiplicative management decisions are worth more in big companies. Software engineering becomes more valuable as the market for software grows.</p><p>Wealth effects are amplified by multiplicative growth (noise in a given period multiplies wealth for the rest of the series, and systematic biases from abilities can grow exponentially or superexponentially over a lifetime), and there are some versions of that in gaining expensive-to-acquire human capital (like fame for Hollywood actors, or experience using incredibly expensive machinery or companies).</p><p>And we can read off the distributions of income, wealth, market share, lead time in innovations, scientometrics, etc.</p><p>That sort of data lead you to expect cutting edge tech to be months to a few years ahead of followers, winner-take-all tech markets to a few leading firms and often a clearly dominant one (but not driving an expectation of being able to safely rest on laurels for years while others innovate without a moat like network effects). That's one of my longstanding arguments with Robin Hanson, that his model has more even capabilities and market share for AGI/WBE than typically observed (he says that AGI software will have to be more diverse requiring more specialized companies, to contribute so much GDP).</p><p>It is tough to sample for extreme values on multiple traits at once, superexponentially tough as you go out or have more criteria. CEOs of big companies are smarter than average, taller than average, have better social skills on average, but you can't find people who are near the top on several of those.</p><p><a href=\"https://www.hbs.edu/ris/Publication%20Files/16-044_9c05278e-9d11-4315-a744-de008edf4d80.pdf\">https://www.hbs.edu/ris/Publication%20Files/16-044_9c05278e-9d11-4315-a744-de008edf4d80.pdf</a></p><p>Correlations between the things help, but it's tough. E.g. if you have thousands of people in a class on a measure of cognitive skill, and you select on only partially correlated matters of personality, interest, motivation, prior experience, etc, the math says it gets thin and you'll find different combos (and today we see more representation of different profiles of abilities, including rare and valuable ones, in this community)</p><p>I think the bigger update for me from trying to expand high-quality save the world efforts has been on the funny personality traits/habits of mind that need to be selected and their scarcity.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Karnofsky][11:30]</strong> &nbsp;<strong>(Oct. 2)</strong>&nbsp;</p><p>A cpl comments, without commitment to respond to responses:</p><p>1. Something in the zone of \"context / experience / obsession\" seems important for explaining the Steve Jobs type thing. It seems to me that people who enter an area early tend to maintain an edge even over more talented people who enter later - examples are not just founder/CEO types but also early employees of some companies who are more experienced with higher-level stuff (and often know the history of how they got there) better than later-entering people.</p><p>2. I'm not sure if I am just rephrasing something Carl or Paul has said, but something that bugs me a lot about the Rob/Eliezer arguments is that I feel like if I accept &gt;5% probability for the kind of jump they're talking about, I don't have a great understanding of how I avoid giving &gt;5% to a kajillion other claims from various startups that they're about to revolutionize their industry, in ways that seem inside-view plausible and seem to equally \"depend on facts about some physical domain rather than facts about reference classes.\"</p><p>The thing that actually most comes to mind here is Thiel - he has been a phenomenal investor financially, but he has also invested by now in a lot of \"atoms\" startups with big stories about what they might do, and I don't think any have come close to reaching those visions (though they have sometimes made $ by doing something orders of magnitude less exciting).</p><p>If a big crux here is \"whether Thielian secrets exist\" this track record could be significant.</p><p>I think I might update if I had a cleaner sense of how I could take on this kind of \"Well, if it is just a fact about physics that I have no idea about, it can't be that unlikely\" view without then betting on a lot of other inside-view-plausible breakthroughs that haven't happened. Right now all I can say to imitate this lens is \"General intelligence is 'different'\"</p><p>I don't feel the same way about \"AI might take over the world\" - I feel like I have good reasons this applies to AI and not a bunch of other stuff</p></td></tr></tbody></table>\n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%);border:1pt solid #000000;vertical-align:top\"><p><strong>[Soares][11:11]</strong> &nbsp;<strong>(Oct. 2)</strong>&nbsp;</p><p>Ok, a few notes from me (feel free to ignore):</p><p>1. It seems to me like the convo here is half attempting-to-crux and half attempting-to-distill-out-a-bet. I'm interested in focusing explicitly on cruxing for the time being, for whatever that's worth. (It seems to me like y'all're already trending in that direction.)</p><p>2. It seems to me that one big revealed difference between the Eliezerverse and the Paulverse is something like:</p><ul><li>In the Paulverse, we already have basically all the fundamental insights we need for AGI, and now it's just a matter of painstaking scaling.</li><li>In the Eliezerverse, there are large insights yet missing (and once they're found we have plenty of reason to expect things to go quickly).</li></ul><p>For instance, in Eliezerverse they say \"The Wright flyer didn't need to have historical precedents, it was allowed to just start flying. Similarly, the AI systems of tomorrow are allowed to just start GIing without historical precedent.\", and in the Paulverse they say \"The analog of the Wright flyer has already happened, it was Alexnet, we are now in the phase analogous to the slow grinding transition from human flight to commercially viable human flight.\"</p><p>(This seems to me like basically what Ajeya articulated <a href=\"https://www.lesswrong.com/posts/fS7Zdj2e2xMqE6qja/more-christiano-cotra-and-yudkowsky-on-ai-progress\">upthread</a>.)</p><p>3. It seems to me that another revealed intuition-difference is in the difficulty that people have operating each other's models. This is evidenced by, eg, Eliezer/Rob saying things like \"I don't know how to operate the gradualness model without making a bunch of bad predictions about Steve Jobs\", and Paul/Holden responding with things like \"I don't know how to operate the secrets-exist model without making a bunch of bad predictions about material startups\".</p><p>I'm not sure whether this is a shallower or deeper disagreement than (2). I'd be interested in further attempts to dig into the questions of how to operate the models, in hopes that the disagreement looks interestingly different once both parties can at least operate the other model.</p><figure class=\"table\"><table><tbody><tr><td>[Tallinn: ➕]</td></tr></tbody></table></figure></td></tr></tbody></table>",
      "plaintextDescription": "This post is a transcript of a multi-day discussion between Paul Christiano, Richard Ngo, Eliezer Yudkowsky, Rob Bensinger, Holden Karnofsky, Rohin Shah, Carl Shulman, Nate Soares, and Jaan Tallinn, following up on the Yudkowsky/Christiano debate in 1, 2, 3, and 4.\n\n \n\nColor key:\n\n Chat by Paul, Richard, and Eliezer  Other chat \n\n \n\n\n12. Follow-ups to the Christiano/Yudkowsky conversation\n \n\n\n12.1. Bensinger and Shah on prototypes and technological forecasting\n \n\n[Bensinger][16:22]  (Sep. 23) \n\nQuoth Paul:\n\n> seems like you have to make the wright flyer much better before it's important, and that it becomes more like an industry as that happens, and that this is intimately related to why so few people were working on it\n\nIs this basically saying 'the Wright brothers didn't personally capture much value by inventing heavier-than-air flying machines, and this was foreseeable, which is why there wasn't a huge industry effort already underway to try to build such machines as fast as possible.' ?\n\nMy maybe-wrong model of Eliezer says here 'the Wright brothers knew a (Thielian) secret', while my maybe-wrong model of Paul instead says:\n\n * They didn't know a secret -- it was obvious to tons of people that you could do something sorta like what the Wright brothers did and thereby invent airplanes; the Wright brothers just had unusual non-monetary goals that made them passionate to do a thing most people didn't care about.\n * Or maybe it's better to say: they knew some specific secrets about physics/engineering, but only because other people correctly saw 'there are secrets to be found here, but they're stamp-collecting secrets of little economic value to me, so I won't bother to learn the secrets'. ~Everyone knows where the treasure is located, and ~everyone knows the treasure won't make you rich.\n\n[Yudkowsky][17:24]  (Sep. 23) \n\nMy model of Paul says there could be a secret, but only because the industry was tiny and the invention was nearly worthless directly.\n\n[Cotra: ➕]",
      "wordCount": 9280
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "sPpZRaxpNNJjw55eu",
        "name": "Progress Studies",
        "slug": "progress-studies"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b0",
        "name": "Technological Forecasting",
        "slug": "technological-forecasting"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Bgt236xrd5j2uAnQJ",
    "title": "Leaving Orbit",
    "slug": "leaving-orbit",
    "url": null,
    "baseScore": 50,
    "voteCount": 34,
    "viewCount": null,
    "commentCount": 17,
    "createdAt": null,
    "postedAt": "2021-12-06T21:48:41.371Z",
    "contents": {
      "markdown": "Duncan says:\n\n> At CFAR workshops, we often installed this as an explicit local norm: \"If at any point you want to convey 'hey, I'm bowing out of this conversation now, but it's not intended to be any sort of reflection on you or the topic, I'm not making a statement, I'm just doing what's good for me and that's all,' you just say '**Leaving orbit!**' and people will wave to you and keep on talking.\"\n\nThis strikes me as good social tech to try out on LessWrong (linking to this page to define the term).\n\nBasic rationale:\n\n1.  It's important to be able to randomly exit conversations. Otherwise, people won't add as much useful stuff to conversations in the first place (lest they be trapped).\n2.  Sometimes people want to leave a message to indicate they're exiting. But it can be tricky to phrase such messages without implying one or more of: \n    *   \"I think my last message is the last word in this conversation, and has resolved all outstanding issues.\"\n    *   \"I think you have raised no important or worth-responding-to points since my last message.\"\n    *   \"I think that you as a person are unworthy of my time.\"\n    *   \"Your last messages convinced me; I agree with you.\"\n    *   \"Your last messages didn't convince me; I disagree.\"\n    *   \"I have no idea how to object-level-respond to your messages.\"\n3.  (Maybe some of these are things you *want* to communicate in some contexts, but it's good to not *have* to communicate them!)\n4.  So having a very generic placeholder thing to say here is useful. \"(Leaving orbit.)\" seems friendly and generic enough to suit the bill.\n\n* * *\n\nCaveat: I think it's also 100% fine to exit conversations without leaving a message at all.\n\nI generally think society has far too many \"do this or you're bad\" norms, especially [automatic norms](https://www.overcomingbias.com/2017/12/10-implications-of-automatic-norms.html). A bad outcome would be if \"leaving orbit\" (or similar) came to be seen as an *expectation* or *requirement*, rather than as an *option*. If this idea catches on *too* much, I suggest deliberately exiting more conversations without leaving any message, so as to keep this option from ossifying into an expectation. 🙂",
      "plaintextDescription": "Duncan says:\n\n> At CFAR workshops, we often installed this as an explicit local norm: \"If at any point you want to convey 'hey, I'm bowing out of this conversation now, but it's not intended to be any sort of reflection on you or the topic, I'm not making a statement, I'm just doing what's good for me and that's all,' you just say 'Leaving orbit!' and people will wave to you and keep on talking.\"\n\nThis strikes me as good social tech to try out on LessWrong (linking to this page to define the term).\n\nBasic rationale:\n\n 1. It's important to be able to randomly exit conversations. Otherwise, people won't add as much useful stuff to conversations in the first place (lest they be trapped).\n 2. Sometimes people want to leave a message to indicate they're exiting. But it can be tricky to phrase such messages without implying one or more of: \n    * \"I think my last message is the last word in this conversation, and has resolved all outstanding issues.\"\n    * \"I think you have raised no important or worth-responding-to points since my last message.\"\n    * \"I think that you as a person are unworthy of my time.\"\n    * \"Your last messages convinced me; I agree with you.\"\n    * \"Your last messages didn't convince me; I disagree.\"\n    * \"I have no idea how to object-level-respond to your messages.\"\n 3. (Maybe some of these are things you want to communicate in some contexts, but it's good to not have to communicate them!)\n 4. So having a very generic placeholder thing to say here is useful. \"(Leaving orbit.)\" seems friendly and generic enough to suit the bill.\n\n----------------------------------------\n\nCaveat: I think it's also 100% fine to exit conversations without leaving a message at all.\n\nI generally think society has far too many \"do this or you're bad\" norms, especially automatic norms. A bad outcome would be if \"leaving orbit\" (or similar) came to be seen as an expectation or requirement, rather than as an option. If this idea catches on too much, I suggest deliberately e",
      "wordCount": 390
    },
    "tags": [
      {
        "_id": "p8nXWqwPH7mPSZf6p",
        "name": "Terminology / Jargon (meta)",
        "slug": "terminology-jargon-meta"
      },
      {
        "_id": "AADZcNS24mmSfPp2w",
        "name": "Communication Cultures",
        "slug": "communication-cultures"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CpvyhFy9WvCNsifkY",
    "title": "Discussion with Eliezer Yudkowsky on AGI interventions",
    "slug": "discussion-with-eliezer-yudkowsky-on-agi-interventions",
    "url": null,
    "baseScore": 328,
    "voteCount": 156,
    "viewCount": null,
    "commentCount": 253,
    "createdAt": null,
    "postedAt": "2021-11-11T03:01:11.208Z",
    "contents": {
      "markdown": "The  following is a partially redacted and lightly edited transcript of a chat conversation about AGI between Eliezer Yudkowsky and a set of invitees in early September 2021. By default, all other participants are anonymized as \"Anonymous\".\n\nI think this Nate Soares quote (excerpted from Nate's [response to a report by Joe Carlsmith](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential)) is a useful context-setting preface regarding timelines, which weren't discussed as much in the transcript:\n\n> \\[...\\] My odds \\[of AGI by the year 2070\\] are around 85%\\[...\\]\n> \n> I can list a handful of things that drive my probability of AGI-in-the-next-49-years above 80%:\n> \n> 1\\. 50 years ago was 1970. The gap between AI systems then and AI systems now seems pretty plausibly greater than the remaining gap, even before accounting the recent dramatic increase in the rate of progress, and potential future increases in rate-of-progress as it starts to feel within-grasp.\n> \n> 2\\. I observe that, 15 years ago, everyone was saying AGI is far off because of what it couldn't do -- basic image recognition, go, starcraft, winograd schemas, programmer assistance. But basically all that has fallen. The gap between us and AGI is made mostly of intangibles. (Computer Programming That Is Actually Good? Theorem proving? Sure, but on my model, \"good\" versions of those are a hair's breadth away from full AGI already. And the fact that I need to clarify that \"bad\" versions don't count, speaks to my point that the only barriers people can name right now are intangibles.) That's a very uncomfortable place to be!\n> \n> 3\\. When I look at the history of invention, and the various anecdotes about the Wright brothers and Enrico Fermi, I get an impression that, when a technology is pretty close, the world looks a lot like how our world looks.\n> \n> *   Of course, the trick is that when a technology is a little far, the world might also look pretty similar!\n> *   Though when a technology is **very** far, the world **does** look different -- it looks like experts pointing to specific technical hurdles. We exited that regime a few years ago.\n> \n> 4\\. Summarizing the above two points, I suspect that I'm in more-or-less the \"penultimate epistemic state\" on AGI timelines: I don't know of a project that seems like they're right on the brink; that would put me in the \"final epistemic state\" of thinking AGI is imminent. But I'm in the second-to-last epistemic state, where I wouldn't feel all that shocked to learn that some group has reached the brink. Maybe I won't get that call for 10 years! Or 20! But it could also be 2, and I wouldn't get to be indignant with reality. I wouldn't get to say \"but all the following things should have happened first, before I made that observation\". I have made those observations.\n> \n> 5\\. It seems to me that the Cotra-style compute-based model provides pretty conservative estimates. For one thing, I don't expect to need human-level compute to get human-level intelligence, and for another I think there's a decent chance that insight and innovation have a big role to play, especially on 50 year timescales.\n> \n> 6\\. There has been a lot of AI progress recently. When I tried to adjust my beliefs so that I was **positively** surprised by AI progress just about as often as I was **negatively** surprised by AI progress, I ended up expecting a bunch of rapid progress. \\[...\\]\n\n**Further preface by Eliezer:** \n\nIn some sections here, I sound gloomy about the probability that coordination between AGI groups succeeds in saving the world.  Andrew Critch reminds me to point out that gloominess like this can be a self-fulfilling prophecy - if people think successful coordination is impossible, they won’t try to coordinate.  I therefore remark in retrospective advance that it seems to me like at least some of the top AGI people, say at Deepmind and Anthropic, are the sorts who I think would rather coordinate than destroy the world; my gloominess is about what happens when the technology has propagated further than that.  But even then, anybody who would *rather* coordinate and *not* destroy the world shouldn’t rule out hooking up with Demis, or whoever else is in front if that person also seems to prefer not to completely destroy the world.  (Don’t be too picky here.)  Even if the technology proliferates and the world ends a year later when other non-coordinating parties jump in, it’s still better to take the route where the world ends one year later instead of immediately.  Maybe the horse will sing.\n\n* * *\n\n **Eliezer Yudkowsky** \n\nHi and welcome. Points to keep in mind: \n\n\\- I'm doing this because I would like to learn whichever *actual* thoughts this target group may have, and perhaps respond to those; that's part of the point of anonymity. If you speak an anonymous thought, please have that be your actual thought that you are thinking yourself, not something where you're thinking \"well, somebody else might think that...\" or \"I wonder what Eliezer's response would be to...\"\n\n\\- Eliezer's responses are uncloaked by default. Everyone else's responses are anonymous (not pseudonymous) and neither I nor MIRI will know which potential invitee sent them.\n\n\\- Please do not reshare or pass on the link you used to get here.\n\n\\- I do intend that parts of this conversation may be saved and published at MIRI's discretion, though not with any mention of who the anonymous speakers could possibly have been.\n\n**Eliezer Yudkowsky** \n\n(Thank you to Ben Weinstein-Raun for building [chathamroom.com](https://www.chathamroom.com/), and for quickly adding some features to it at my request.)\n\n**Eliezer Yudkowsky** \n\nIt is now 2PM; this room is now open for questions.\n\n**Anonymous** \n\nHow long will it be open for?\n\n**Eliezer Yudkowsky** \n\nIn principle, I could always stop by a couple of days later and answer any unanswered questions, but my basic theory had been \"until I got tired\".\n\n* * *\n\n**Anonymous** \n\nAt a high level one thing I want to ask about is research directions and prioritization. For example, if you were dictator for what researchers here (or within our influence) were working on, how would you reallocate them?\n\n**Eliezer Yudkowsky** \n\nThe first reply that came to mind is \"I don't know.\" I consider the present gameboard to look incredibly grim, and I don't actually see a way out through hard work alone. We can hope there's a miracle that violates some aspect of my background model, and we can try to prepare for that unknown miracle; preparing for an unknown miracle probably looks like \"Trying to die with more dignity on the mainline\" (because if you can die with more dignity on the mainline, you are better positioned to take advantage of a miracle if it occurs).\n\n**Anonymous** \n\nI'm curious if the grim outlook is currently mainly due to technical difficulties or social/coordination difficulties. (Both avenues might have solutions, but maybe one seems more recalcitrant than the other?)\n\n**Eliezer Yudkowsky** \n\nTechnical difficulties. Even if the social situation were vastly improved, on my read of things, everybody still dies because there is nothing that a handful of socially coordinated projects can do, or even a handful of major governments who aren't willing to start nuclear wars over things, to prevent somebody else from building AGI and killing everyone 3 months or 2 years later. There's no obvious winnable position into which to play the board.\n\n**Anonymous** \n\njust to clarify, that sounds like a large scale coordination difficulty to me (i.e., we - as all of humanity - can't coordinate to not build that AGI).\n\n**Eliezer Yudkowsky** \n\nI wasn't really considering the counterfactual where humanity had a collective telepathic hivemind? I mean, I've written fiction about a world coordinated enough that they managed to shut down all progress in their computing industry and only manufacture powerful computers in a single worldwide hidden base, but Earth was never going to go down that route. Relative to remotely plausible levels of future coordination, we have a technical problem.\n\n**Anonymous **\n\nCurious about why building an AGI aligned to its users' interests isn't a thing a handful of coordinated projects could do that would effectively prevent the catastrophe. The two obvious options are: it's too hard to build it vs it wouldn't stop the other group anyway. For \"it wouldn't stop them\", two lines of reply are nobody actually wants an unaligned AGI (they just don't foresee the consequences and are pursuing the benefits from automated intelligence, so can be defused by providing the latter) (maybe not entirely true: omnicidal maniacs), and an aligned AGI could help in stopping them. Is your take more on the \"too hard to build\" side?\n\n**Eliezer Yudkowsky ** \n\nBecause it's too technically hard to align some cognitive process that is powerful enough, and operating in a sufficiently dangerous domain, to stop the next group from building an unaligned AGI in 3 months or 2 years. Like, they can't coordinate to build an AGI that builds a nanosystem because it is too technically hard to align their AGI technology in the 2 years before the world ends.\n\n**Anonymous ** \n\nSummarizing the threat model here (correct if wrong): The nearest competitor for building an AGI is at most N (<2) years behind, and building an aligned AGI, even when starting with the ability to build an unaligned AGI, takes longer than N years. So at some point some competitor who doesn't care about safety builds the unaligned AGI. How does \"nobody actually wants an unaligned AGI\" fail here? It takes >N years to get everyone to realise that they have that preference and that it's incompatible with their actions?\n\n**Eliezer Yudkowsky ** \n\nMany of the current actors seem like they'd be really gung-ho to build an \"unaligned\" AGI because they think it'd be super neat, or they think it'd be super profitable, and they don't expect it to destroy the world. So if this happens in anything like the current world - and I neither expect vast improvements, nor have very long timelines - then we'd see Deepmind get it first; and, if the code was not *immediately* stolen and rerun with higher bounds on the for loops, by China or France or whoever, somebody else would get it in another year; if that somebody else was Anthropic, I could maybe see them also not amping up their AGI; but then in 2 years it starts to go to Facebook AI Research and home hobbyists and intelligence agencies stealing copies of the code from other intelligence agencies and I don't see how the world fails to end past that point.\n\n**Anonymous ** \n\nWhat does trying to die with more dignity on the mainline look like? There's a real question of prioritisation here between solving the alignment problem (and various approaches within that), and preventing or slowing down the next competitor. I'd personally love more direction on where to focus my efforts (obviously you can only say things generic to the group).\n\n**Eliezer Yudkowsky ** \n\nI don't know how to effectively prevent or slow down the \"next competitor\" for more than a couple of years even in plausible-best-case scenarios. Maybe some of the natsec people can be grownups in the room and explain why \"stealing AGI code and running it\" is as bad as \"full nuclear launch\" to their foreign counterparts in a realistic way. Maybe more current AGI groups can be persuaded to go closed; or, if more than one has an AGI, to coordinate with each other and not rush into an arms race. I'm not sure I believe these things can be done in real life, but it seems understandable to me how I'd go about trying - though, please do talk with me a lot more before trying anything like this, because it's easy for me to see how attempts could backfire, it's not clear to me that we should be inviting more attention from natsec folks at all. None of that saves us without technical alignment progress. But what are other people supposed to do about researching alignment when I'm not sure what to try there myself?\n\n  \n \n\n**Anonymous ** \n\nthanks! on researching alignment, you might have better meta ideas (how to do research generally) even if you're also stuck on object level. and you might know/foresee dead ends that others don't.\n\n**Eliezer Yudkowsky ** \n\nI definitely foresee a whole lot of dead ends that others don't, yes.\n\n**Anonymous ** \n\nDoes pushing for a lot of public fear about this kind of research, that makes all projects hard, seem hopeless?\n\n**Eliezer Yudkowsky ** \n\nWhat does it buy us? 3 months of delay at the cost of a tremendous amount of goodwill? 2 years of delay? What's that delay for, if we all die at the end? Even if we then got a technical miracle, would it end up impossible to run a project that could make use of an alignment miracle, because everybody was afraid of that project? Wouldn't that fear tend to be channeled into \"ah, yes, it must be a government project, they're the good guys\" and then the government is much more hopeless and much harder to improve upon than Deepmind?\n\n**Anonymous ** \n\nI imagine lack of public support for genetic manipulation of humans has slowed that research by more than three months\n\n**Anonymous ** \n\n'would it end up impossible to run a project that could make use of an alignment miracle, because everybody was afraid of that project?'\n\n...like, maybe, but not with near 100% chance?\n\n**Eliezer Yudkowsky ** \n\nI don't want to sound like I'm dismissing the whole strategy, but it sounds a *lot* like the kind of thing that backfires because you did not get *exactly* the public reaction you wanted, and the public reaction you actually got was bad; and it doesn't sound like that whole strategy actually has a visualized victorious endgame, which makes it hard to work out what the exact strategy should be; it seems more like the kind of thing that falls under the syllogism \"something must be done, this is something, therefore this must be done\" than like a plan that ends with humane life victorious.\n\nRegarding genetic manipulation of humans, I think the public started out very unfavorable to that, had a reaction that was not at all exact or channeled, does not allow for any 'good' forms of human genetic manipulation regardless of circumstances, driving the science into other countries - it is not a case in point of the intelligentsia being able to successfully cunningly manipulate the fear of the masses to some supposed good end, to put it mildly, so I'd be worried about deriving that generalization from it. The reaction may more be that the fear of the public is a big powerful uncontrollable thing that doesn't move in the smart direction - maybe the public fear of AI gets channeled by opportunistic government officials into \"and that's why We must have Our AGI first so it will be Good and we can Win\". That seems to me much more like a thing that would happen in real life than \"and then we managed to manipulate public panic down exactly the direction we wanted to fit into our clever master scheme\", especially when we don't actually *have* the clever master scheme it fits into.\n\n* * *\n\n**Eliezer Yudkowsky ** \n\nI have a few stupid ideas I could try to investigate in ML, but that would require the ability to run significant-sized closed ML projects full of trustworthy people, which is a capability that doesn't seem to presently exist. Plausibly, this capability would be required in any world that got some positive model violation (\"miracle\") to take advantage of, so I would want to build that capability today. I am not sure how to go about doing that either.\n\n**Anonymous ** \n\nif there's a chance this group can do something to gain this capability I'd be interested in checking it out. I'd want to know more about what \"closed\"and \"trustworthy\" mean for this (and \"significant-size\" I guess too). E.g., which ones does Anthropic fail?\n\n**Eliezer Yudkowsky ** \n\nWhat I'd like to exist is a setup where I can work with people that I or somebody else has vetted as seeming okay-trustworthy, on ML projects that aren't going to be published. Anthropic looks like it's a package deal. If Anthropic were set up to let me work with 5 particular people at Anthropic on a project boxed away from the rest of the organization, that would potentially be a step towards trying such things. It's also not clear to me that Anthropic has either the time to work with me, or the interest in doing things in AI that aren't \"stack more layers\" or close kin to that.\n\n**Anonymous ** \n\nThat setup doesn't sound impossible to me -- at DeepMind or OpenAI or a new org specifically set up for it (or could be MIRI) -- the bottlenecks are access to trustworthy ML-knowledgeable people (but finding 5 in our social network doesn't seem impossible?) and access to compute (can be solved with more money - not too hard?). I don't think DM and OpenAI are publishing everything - the \"not going to be published\" part doesn't seem like a big barrier to me. Is infosec a major bottleneck (i.e., who's potentially stealing the code/data)?\n\n**Anonymous ** \n\nDo you think Redwood Research could be a place for this?\n\n**Eliezer Yudkowsky ** \n\nMaybe! I haven't ruled RR out yet. But they also haven't yet done (to my own knowledge) anything demonstrating the same kind of AI-development capabilities as even GPT-3, let alone AlphaFold 2.\n\n**Eliezer Yudkowsky ** \n\nI would potentially be super interested in working with Deepminders if Deepmind set up some internal partition for \"Okay, accomplished Deepmind researchers who'd rather not destroy the world are allowed to form subpartitions of this partition and have their work not be published outside the subpartition let alone Deepmind in general, though maybe you have to report on it to Demis only or something.\" I'd be more skeptical/worried about working with OpenAI-minus-Anthropic because the notion of \"open AI\" continues to sound to me like \"what is the worst possible strategy for making the game board as unplayable as possible while demonizing everybody who tries a strategy that could possibly lead to the survival of humane intelligence\", and now a lot of the people who knew about that part have left OpenAI for elsewhere. But, sure, if they changed their name to \"ClosedAI\" and fired everyone who believed in the original OpenAI mission, I would update about that.\n\n**Eliezer Yudkowsky ** \n\nContext that is potentially missing here and should be included: I wish that Deepmind had more internal closed research, and internally siloed research, as part of a larger wish I have about the AI field, independently of what projects I'd want to work on myself.\n\nThe present situation can be seen as one in which a common resource, the remaining timeline until AGI shows up, is incentivized to be burned by AI researchers because they have to come up with neat publications and publish them (which burns the remaining timeline) in order to earn status and higher salaries. The more they publish along the spectrum that goes {quiet internal result -> announced and demonstrated result -> paper describing how to get the announced result -> code for the result -> model for the result}, the more timeline gets burned, and the greater the internal and external prestige accruing to the researcher.\n\nIt's futile to wish for everybody to act uniformly against their incentives.  But I think it would be a step forward if the relative incentive to burn the commons could be *reduced*; or to put it another way, the more researchers have the *option* to not burn the timeline commons, without them getting fired or passed up for promotion, the more that unusually intelligent researchers might perhaps decide not to do that. So I wish in general that AI research groups in general, but also Deepmind in particular, would have affordances for researchers who go looking for interesting things to not publish any resulting discoveries, at all, and still be able to earn internal points for them. I wish they had the *option* to do that. I wish people were *allowed* to not destroy the world - and still get high salaries and promotion opportunities and the ability to get corporate and ops support for playing with interesting toys; if destroying the world is prerequisite for having nice things, nearly everyone is going to contribute to destroying the world, because, like, they're not going to just *not* have nice things, that is not human nature for almost all humans.\n\nWhen I visualize how the end of the world plays out, I think it involves an AGI system which has the ability to be cranked up by adding more computing resources to it; and I think there is an extended period where the system is not aligned enough that you can crank it up that far, without everyone dying. And it seems *extremely* likely that if factions on the level of, say, Facebook AI Research, start being able to deploy systems like that, then death is very automatic. If the Chinese, Russian, and French intelligence services all manage to steal a copy of the code, and China and Russia sensibly decide not to run it, and France gives it to three French corporations which I hear the French intelligence service sometimes does, then again, everybody dies. If the builders are sufficiently worried about that scenario that they push too fast too early, in fear of an arms race developing very soon if they wait, again, everybody dies.\n\nAt present we're very much waiting on a miracle for alignment to be possible at all, even if the AGI-builder successfully prevents proliferation and has 2 years in which to work. But if we get that miracle at all, it's not going to be an instant miracle.  There’ll be some minimum time-expense to do whatever work is required. So any time I visualize anybody trying to even start a successful trajectory of this kind, they need to be able to get a lot of work done, without the intermediate steps of AGI work being published, or demoed at all, let alone having models released.  Because if you wait until the last months when it is really really obvious that the system is going to scale to AGI, in order to start closing things, almost all the prerequisites will already be out there. Then it will only take 3 more months of work for somebody else to build AGI, and then somebody else, and then somebody else; and even if the first 3 factions manage not to crank up the dial to lethal levels, the 4th party will go for it; and the world ends by default on full automatic.\n\nIf ideas are theoretically internal to \"just the company\", but the company has 150 people who all know, plus everybody with the \"sysadmin\" title having access to the code and models, then I imagine - perhaps I am mistaken - that those ideas would (a) inevitably leak outside due to some of those 150 people having cheerful conversations over a beer with outsiders present, and (b) be copied outright by people of questionable allegiances once all hell started to visibly break loose. As with anywhere that handles really sensitive data, the concept of \"need to know\" has to be a thing, or else everyone (and not just in that company) ends up knowing.\n\nSo, even if I got run over by a truck tomorrow, I would still very much wish that in the world that survived me, Deepmind would have lots of penalty-free affordance internally for people to not publish things, and to work in internal partitions that didn't spread their ideas to all the rest of Deepmind.  Like, *actual* social and corporate support for that, not just a theoretical option you'd have to burn lots of social capital and weirdness points to opt into, and then get passed up for promotion forever after.\n\n**Anonymous ** \n\nWhat's RR?\n\n**Anonymous ** \n\nIt's a new alignment org, run by Nate Thomas and ~co-run by Buck Shlegeris and Bill Zito, with maybe 4-6 other technical folks so far. My take: the premise is to create an org with ML expertise and general just-do-it competence that's trying to do all the alignment experiments that something like Paul+Ajeya+Eliezer all think are obviously valuable and wish someone would do. They expect to have a website etc in a few days; the org is a couple months old in its current form.\n\n* * *\n\n**Anonymous ** \n\nHow likely really is hard takeoff? Clearly, we are touching the edges of AGI with GPT and the like. But I'm not feeling this will that easily be leveraged into very quick recursive self improvement.\n\n**Eliezer Yudkowsky ** \n\nCompared to the position I was arguing in the Foom Debate with Robin, reality has proved way to the further Eliezer side of Eliezer along the Eliezer-Robin spectrum. It's been very unpleasantly surprising to me how little architectural complexity is required to start producing generalizing systems, and how fast those systems scale using More Compute. The flip side of this is that I can imagine a system being scaled up to interesting human+ levels, without \"recursive self-improvement\" or other of the old tricks that I thought would be necessary, and argued to Robin would make fast capability gain possible. You could have fast capability gain well before anything like a FOOM started. Which in turn makes it more plausible to me that we could hang out at interesting not-superintelligent levels of AGI capability for a while before a FOOM started. It's not clear that this helps anything, but it does seem more plausible.\n\n**Anonymous ** \n\nI agree reality has not been hugging the Robin kind of scenario this far.\n\n**Anonymous ** \n\nGoing past human level doesn't necessarily mean going \"foom\".\n\n**Eliezer Yudkowsky ** \n\nI do think that if you get an AGI significantly past human intelligence in all respects, it would obviously tend to FOOM. I mean, I suspect that Eliezer fooms if you give an Eliezer the ability to backup, branch, and edit himself.\n\n**Anonymous ** \n\nIt doesn't seem to me that an AGI significantly past human intelligence necessarily tends to FOOM.\n\n**Eliezer Yudkowsky ** \n\nI think in principle we could have, for example, an AGI that was just a superintelligent engineer of proteins, and of nanosystems built by nanosystems that were built by proteins, and which was corrigible enough not to want to improve itself further; and this AGI would also be dumber than a human when it came to eg psychological manipulation, because we would have asked it not to think much about that subject. I'm doubtful that you can have an AGI that's significantly above human intelligence in *all* respects, without it having the capability-if-it-wanted-to of looking over its own code and seeing lots of potential improvements.\n\n**Anonymous ** \n\nAlright, this makes sense to me, but I don't expect an AGI to *want* to manipulate humans that easily (unless designed to). Maybe a bit.\n\n**Eliezer Yudkowsky ** \n\nManipulating humans is a convergent instrumental strategy if you've accurately modeled (even at quite low resolution) what humans are and what they do in the larger scheme of things.\n\n**Anonymous ** \n\nYes, but human manipulation is also the kind of thing you need to guard against with even mildly powerful systems. Strong impulses to manipulate humans, should be vetted out.\n\n**Eliezer Yudkowsky ** \n\nI think that, by default, if you trained a young AGI to expect that 2+2=5 in some special contexts, and then scaled it up without further retraining, a generally superhuman version of that AGI would be very likely to 'realize' in some sense that SS0+SS0=SSSS0 was a consequence of the Peano axioms. There's a natural/convergent/coherent output of deep underlying algorithms that generate competence in some of the original domains; when those algorithms are implicitly scaled up, they seem likely to generalize better than whatever patch on those algorithms said '2 + 2 = 5'.\n\nIn the same way, suppose that you take weak domains where the AGI can't fool you, and apply some gradient descent to get the AGI to stop outputting actions of a type that humans can detect and label as 'manipulative'.  And then you scale up that AGI to a superhuman domain.  I predict that deep algorithms within the AGI will go through consequentialist dances, and model humans, and output human-manipulating actions that can't be detected as manipulative by the humans, in a way that seems likely to bypass whatever earlier patch was imbued by gradient descent, because I doubt that earlier patch will generalize as well as the deep algorithms. Then you don't get to retrain in the superintelligent domain after labeling as bad an output that killed you and doing a gradient descent update on that, because the bad output killed you. (This is an attempted very fast gloss on what makes alignment difficult *in the first place*.)\n\n**Anonymous ** \n\n\\[i appreciate this gloss - thanks\\]\n\n**Anonymous ** \n\n\"deep algorithms within it will go through consequentialist dances, and model humans, and output human-manipulating actions that can't be detected as manipulative by the humans\"\n\nThis is true if it is rewarding to manipulate humans. If the humans are on the outlook for this kind of thing, it doesn't seem that easy to me.\n\nGoing through these \"consequentialist dances\" to me appears to presume that mistakes that should be apparent haven't been solved at simpler levels. It seems highly unlikely to me that you would have a system that appears to follow human requests and human values, and it would suddenly switch at some powerful level. I think there will be signs beforehand. Of course, if the humans are not paying attention, they might miss it. But, say, in the current milieu, I find it plausible that they will pay enough attention.\n\n\"because I doubt that earlier patch will generalize as well as the deep algorithms\"\n\nThat would depend on how \"deep\" your earlier patch was. Yes, if you're just doing surface patches to apparent problems, this might happen. But it seems to me that useful and intelligent systems will require deep patches (or deep designs from the start) in order to be apparently useful to humans at solving complex problems enough. This is not to say that they would be perfect. But it seems quite plausible to me that they would in most cases prevent the worst outcomes.\n\n**Eliezer Yudkowsky ** \n\n\"If you've got a general consequence-modeling-and-searching algorithm, it seeks out ways to manipulate humans, even if there are no past instances of a random-action-generator producing manipulative behaviors that succeeded and got reinforced by gradient descent over the random-action-generator. It invents the strategy de novo by imagining the results, even if there's no instances in memory of a strategy like that having been tried before.\" Agree or disagree?\n\n**Anonymous ** \n\nCreating strategies de novo would of course be expected of an AGI.\n\n> \"If you've got a general consequence-modeling-and-searching algorithm, it seeks out ways to manipulate humans, even if there are no past instances of a random-action-generator producing manipulative behaviors that succeeded and got reinforced by gradient descent over the random-action-generator. It invents the strategy de novo by imagining the results, even if there's no instances in memory of a strategy like that having been tried before.\" Agree or disagree?\n\nI think, if the AI will \"seek out ways to manipulate humans\", will depend on what kind of goals the AI has been designed to pursue.\n\nManipulating humans is definitely an instrumentally useful kind of method for an AI, for a lot of goals. But it's also counter to a lot of the things humans would direct the AI to do -- at least at a \"high level\". \"Manipulation\", such as marketing, for lower level goals, can be very congruent with higher level goals. An AI could clearly be good at manipulating humans, while not manipulating its creators or the directives of its creators.\n\nIf you are asking me to agree that the AI will generally seek out ways to manipulate the high-level goals, then I will say \"no\". Because it seems to me that faults of this kind in the AI design is likely to be caught by the designers earlier. (This isn't to say that this kind of fault couldn't happen.) It seems to me that manipulation of high-level goals will be one of the most apparent kind of faults of this kind of system.\n\n**Anonymous ** \n\nRE: \"I'm doubtful that you can have an AGI that's significantly above human intelligence in *all* respects, without it having the capability-if-it-wanted-to of looking over its own code and seeing lots of potential improvements.\"\n\nIt seems plausible (though unlikely) to me that this would be true in practice for the AGI we build -- but also that the potential improvements it sees would be pretty marginal. This is coming from the same intuition that current learning algorithms might already be approximately optimal.\n\n**Eliezer Yudkowsky ** \n\n> If you are asking me to agree that the AI will generally seek out ways to manipulate the high-level goals, then I will say \"no\". Because it seems to me that faults of this kind in the AI design is likely to be caught by the designers earlier.\n\nI expect that when people are trying to stomp out convergent instrumental strategies by training at a safe dumb level of intelligence, this will not be effective at preventing convergent instrumental strategies at smart levels of intelligence; also note that at very smart levels of intelligence, \"hide what you are doing\" is also a convergent instrumental strategy of that substrategy.\n\nI don't know however if I should be explaining at this point why \"manipulate humans\" is convergent, why \"conceal that you are manipulating humans\" is convergent, why you have to train in safe regimes in order to get safety in dangerous regimes (because if you try to \"train\" at a sufficiently unsafe level, the output of the unaligned system deceives you into labeling it incorrectly and/or kills you before you can label the outputs), or why attempts to teach corrigibility in safe regimes are unlikely to generalize well to higher levels of intelligence and unsafe regimes (qualitatively new thought processes, things being way out of training distribution, and, the hardest part to explain, corrigibility being \"anti-natural\" in a certain sense that makes it incredibly hard to, eg, exhibit any coherent planning behavior (\"consistent utility function\") which corresponds to being willing to let somebody else shut you off, without incentivizing you to actively manipulate them to shut you off).\n\n* * *\n\n**Anonymous ** \n\nMy (unfinished) idea for buying time is to focus on applying AI to well-specified problems, where constraints can come primarily from the action space and additionally from process-level feedback (i.e., human feedback providers understand why actions are good before endorsing them, and reject anything weird even if it seems to work on some outcomes-based metric). This is basically a form of boxing, with application-specific boxes. I know it doesn't scale to superintelligence but I think it can potentially give us time to study and understand proto AGIs before they kill us. I'd be interested to hear devastating critiques of this that imply it isn't even worth fleshing out more and trying to pursue, if they exist.\n\n**Anonymous ** \n\n(I think it's also similar to CAIS in case that's helpful.)\n\n**Eliezer Yudkowsky ** \n\nThere's lots of things we can do which don't solve the problem and involve us poking around with AIs having fun, while we wait for a miracle to pop out of nowhere. There's lots of things we can do with AIs which are weak enough to not be able to fool us and to not have cognitive access to any dangerous outputs, like automatically generating pictures of cats.  The trouble is that nothing we can do with an AI like that (where \"human feedback providers understand why actions are good before endorsing them\") is powerful enough to save the world.\n\n**Eliezer Yudkowsky ** \n\nIn other words, if you have an aligned AGI that builds complete mature nanosystems for you, that *is* enough force to save the world; but that AGI needs to have been aligned by some method other than \"humans inspect those outputs and vet them and their consequences as safe/aligned\", because humans cannot accurately and unfoolably vet the consequences of DNA sequences for proteins, or of long bitstreams sent to protein-built nanofactories.\n\n**Anonymous ** \n\nWhen you mention nanosystems, how much is this just a hypothetical superpower vs. something you actually expect to be achievable with AGI/superintelligence? If expected to be achievable, why?\n\n**Eliezer Yudkowsky ** \n\nThe case for nanosystems being possible, if anything, seems even more slam-dunk than the already extremely slam-dunk case for superintelligence, because we can set lower bounds on the power of nanosystems using far more specific and concrete calculations. See eg the first chapters of Drexler's Nanosystems, which are the first step mandatory reading for anyone who would otherwise doubt that there's plenty of room above biology and that it is possible to have artifacts the size of bacteria with much higher power densities. I have this marked down as \"known lower bound\" not \"speculative high value\", and since Nanosystems has been out since 1992 and subjected to attemptedly-skeptical scrutiny, without anything I found remotely persuasive turning up, I do not have a strong expectation that any new counterarguments will materialize.\n\nIf, after reading Nanosystems, you still don't think that a superintelligence can get to and past the Nanosystems level, I'm not quite sure what to say to you, since the models of superintelligences are much less concrete than the models of molecular nanotechnology.\n\nI'm on record as early as 2008 as saying that I expected superintelligences to crack protein folding, some people disputed that and were all like \"But how do you know that's solvable?\" and then AlphaFold 2 came along and cracked the protein folding problem they'd been skeptical about, far below the level of superintelligence.\n\nI can try to explain how I was mysteriously able to forecast this truth at a high level of confidence - not the exact level where it became possible, to be sure, but that superintelligence would be sufficient - despite this skepticism; I suppose I could point to prior hints, like even human brains being able to contribute suggestions to searches for good protein configurations; I could talk about how if evolutionary biology made proteins evolvable then there must be a lot of regularity in the folding space, and that this kind of regularity tends to be exploitable.\n\nBut of course, it's also, in a certain sense, very *obvious* that a superintelligence could crack protein folding, just like it was obvious years before *Nanosystems* that molecular nanomachines would in fact be possible and have much higher power densities than biology. I could say, \"Because proteins are held together by van der Waals forces that are much weaker than covalent bonds,\" to point to a reason how you could realize that after just reading *Engines of Creation* and before *Nanosystems* existed, by way of explaining how one could possibly guess the result of the calculation in advance of building up the whole detailed model. But in reality, precisely because the possibility of molecular nanotechnology was already obvious to any sensible person just from reading *Engines of Creation*, the sort of person who wasn't convinced by *Engines of Creation* wasn't convinced by *Nanosystems* either, because they'd already demonstrated immunity to sensible arguments; an example of the general phenomenon I've elsewhere termed the Law of Continued Failure.\n\nSimilarly, the sort of person who was like \"But how do you know superintelligences will be able to build nanotech?\" in 2008, will probably not be persuaded by the demonstration of AlphaFold 2, because it was already clear to anyone sensible in 2008, and so anyone who can't see sensible points in 2008 probably also can't see them after they become even clearer. There are some people on the margins of sensibility who fall through and change state, but mostly people are not on the exact margins of sanity like that.\n\n**Anonymous ** \n\n\"If, after reading Nanosystems, you still don't think that a superintelligence can get to and past the Nanosystems level, I'm not quite sure what to say to you, since the models of superintelligences are much less concrete than the models of molecular nanotechnology.\"\n\nI'm not sure if this is directed at *me* or the [https://en.wikipedia.org/wiki/Generic_you](https://en.wikipedia.org/wiki/Generic_you), but I'm only expressing curiosity on this point, not skepticism :)\n\n* * *\n\n**Anonymous ** \n\nsome form of \"scalable oversight\" is the naive extension of the initial boxing thing proposed above that claims to be the required alignment method -- basically, make the humans vetting the outputs smarter by providing them AI support for all well-specified (level-below)-vettable tasks.\n\n**Eliezer Yudkowsky ** \n\nI haven't seen any plausible story, in any particular system design being proposed by the people who use terms about \"scalable oversight\", about how human-overseeable thoughts or human-inspected underlying systems, compound into very powerful human-non-overseeable outputs that are trustworthy. Fundamentally, the whole problem here is, \"You're allowed to look at floating-point numbers and Python code, but how do you get from there to trustworthy nanosystem designs?\" So saying \"Well, we'll look at some thoughts we can understand, and then from out of a much bigger system will come a trustworthy output\" doesn't answer the hard core at the center of the question. Saying that the humans will have AI support doesn't answer it either.\n\n**Anonymous ** \n\nthe kind of useful thing humans (assisted-humans) might be able to vet is reasoning/arguments/proofs/explanations. without having to generate neither the trustworthy nanosystem design nor the reasons it is trustworthy, we could still check them.\n\n**Eliezer Yudkowsky ** \n\nIf you have an untrustworthy general superintelligence generating English strings meant to be \"reasoning/arguments/proofs/explanations\" about eg a nanosystem design, then I would not only expect the superintelligence to be able to fool humans in the sense of arguing for things that were not true in a way that fooled the humans, I'd expect the superintelligence to be able to covertly directly hack the humans in ways that I wouldn't understand even after having been told what happened. So you must have some prior belief about the superintelligence being aligned before you dared to look at the arguments. How did you get that prior belief?\n\n**Anonymous ** \n\nI think I'm not starting with a general superintelligence here to get the trustworthy nanodesigns. I'm trying to build the trustworthy nanosystems \"the hard way\", i.e., if we did it without ever building AIs, and then speed that up using AI for automation of things we know how to vet (including recursively). Is a crux here that you think nanosystem design requires superintelligence?\n\n(tangent: I think this approach works even if you accidentally built a more-general or more-intelligent than necessary foundation model as long as you're only using it in boxes it can't outsmart. The better-specified the tasks you automate are, the easier it is to secure the boxes.)\n\n**Eliezer Yudkowsky ** \n\nI think that China ends the world using code they stole from Deepmind that did things the easy way, and that happens 50 years of natural R&D time before you can do the equivalent of \"strapping mechanical aids to a horse instead of building a car from scratch\".\n\nI also think that the speedup step in \"iterated amplification and distillation\" will introduce places where the fast distilled outputs of slow sequences are not true to the original slow sequences, because gradient descent is not perfect and won't be perfect and it's not clear we'll get any paradigm besides gradient descent for doing a step like that.\n\n* * *\n\n**Anonymous ** \n\nHow do you feel about the safety community as a whole and the growth we've seen over the past few years?\n\n**Eliezer Yudkowsky ** \n\nVery grim. I think that almost everybody is bouncing off the real hard problems at the center and doing work that is predictably not going to be useful at the superintelligent level, nor does it teach me anything I could not have said in advance of the paper being written. People like to do projects that they know will succeed and will result in a publishable paper, and that rules out all real research at step 1 of the social process.\n\nPaul Christiano is trying to have real foundational ideas, and they're all wrong, but he's one of the few people trying to have foundational ideas at all; if we had another 10 of him, something might go right.\n\nChris Olah is going to get far too little done far too late. We're going to be facing down an unalignable AGI and the current state of transparency is going to be \"well look at this interesting visualized pattern in the attention of the key-value matrices in layer 47\" when what we need to know is \"okay but was the AGI plotting to kill us or not”. But Chris Olah is still trying to do work that is on a pathway to anything important at all, which makes him exceptional in the field.\n\nStuart Armstrong did some good work on further formalizing the shutdown problem, an example case in point of why corrigibility is hard, which so far as I know is still resisting all attempts at solution.\n\nVarious people who work or worked for MIRI came up with some actually-useful notions here and there, like Jessica Taylor's expected utility quantilization.\n\nAnd then there is, so far as I can tell, a vast desert full of work that seems to me to be mostly fake or pointless or predictable.\n\nIt is very, very clear that at present rates of progress, adding that level of alignment capability as grown over the next N years, to the AGI capability that arrives after N years, results in everybody dying very quickly. Throwing more money at this problem does not obviously help because it just produces more low-quality work.\n\n**Anonymous ** \n\n\"doing work that is predictably not going to be really useful at the superintelligent level, nor does it teach me anything I could not have said in advance of the paper being written\"\n\nI think you're underestimating the value of solving small problems. Big problems are solved by solving many small problems. (I do agree that many academic papers do not represent much progress, however.)\n\n**Eliezer Yudkowsky ** \n\nBy default, I suspect you have longer timelines and a smaller estimate of total alignment difficulty, not that I put less value than you on the incremental power of solving small problems over decades. I think we're going to be staring down the gun of a completely inscrutable model that would kill us all if turned up further, with no idea how to read what goes on inside its head, and no way to train it on humanly scrutable and safe and humanly-labelable domains in a way that seems like it would align the superintelligent version, while standing on top of a whole bunch of papers about \"small problems\" that never got past “small problems”.\n\n**Anonymous ** \n\n\"I think we're going to be staring down the gun of a completely inscrutable model that would kill us all if turned up further, with no idea how to read what goes on inside its head, and no way to train it on humanly scrutable and safe and humanly-labelable domains in a way that seems like it would align the superintelligent version\"\n\nThis scenario seems possible to me, but not very plausible. GPT is not going to \"kill us all\" if turned up further. No amount of computing power (at least before AGI) would cause it to. I think this is apparent, without knowing exactly what's going on inside GPT. This isn't to say that there aren't AI systems that wouldn't. But *what kind of system would*? (A GPT combined with sensory capabilities at the level of Tesla's self-driving AI? That still seems too limited.)\n\n**Eliezer Yudkowsky ** \n\nAlpha Zero scales with more computing power, I think AlphaFold 2 scales with more computing power, Mu Zero scales with more computing power. Precisely because GPT-3 doesn't scale, I'd expect an AGI to look more like Mu Zero and particularly with respect to the fact that it has some way of scaling.\n\n* * *\n\n**Steve Omohundro ** \n\nEliezer, thanks for doing this! I just now read through the discussion and found it valuable. I agree with most of your specific points but I seem to be much more optimistic than you about a positive outcome. I'd like to try to understand why that is. I see mathematical proof as the most powerful tool for constraining intelligent systems and I see a pretty clear safe progression using that for the technical side (the social side probably will require additional strategies). Here are some of my intuitions underlying that approach, I wonder if you could identify any that you disagree with. I'm fine with your using my name (Steve Omohundro) in any discussion of these.\n\n1) Nobody powerful wants to create unsafe AI but they do want to take advantage of AI capabilities.\n\n2) None of the concrete well-specified valuable AI capabilities require unsafe behavior\n\n3) Current simple logical systems are capable of formalizing every relevant system involved (eg. MetaMath [http://us.metamath.org/index.html](http://us.metamath.org/index.html) currently formalizes roughly an undergraduate math degree and includes everything needed for modeling the laws of physics, computer hardware, computer languages, formal systems, machine learning algorithms, etc.)\n\n4) Mathematical proof is cheap to mechanically check (eg. MetaMath has a 500 line Python verifier which can rapidly check all of its 38K theorems)\n\n5) GPT-F is a fairly early-stage transformer-based theorem prover and can already prove 56% of the MetaMath theorems. Similar systems are likely to soon be able to rapidly prove all simple true theorems (eg. that human mathematicians can prove in a day).\n\n6) We can define provable limits on the behavior of AI systems that we are confident prevent dangerous behavior and yet still enable a wide range of useful behavior. \n\n7) We can build automated checkers for these provable safe-AI limits. \n\n8) We can build (and eventually mandate) powerful AI hardware that first verifies proven safety constraints before executing AI software \n\n9) For example, AI smart compilation of programs can be formalized and doesn't require unsafe operations \n\n10) For example, AI design of proteins to implement desired functions can be formalized and doesn't require unsafe operations \n\n11) For example, AI design of nanosystems to achieve desired functions can be formalized and doesn't require unsafe operations.\n\n12) For example, the behavior of designed nanosystems can be similarly constrained to only proven safe behaviors\n\n13) And so on through the litany of early stage valuable uses for advanced AI.\n\n14) I don't see any fundamental obstructions to any of these. Getting social acceptance and deployment is another issue! \n\nBest, Steve\n\n**Eliezer Yudkowsky ** \n\nSteve, are you visualizing AGI that gets developed 70 years from now under absolutely different paradigms than modern ML? I don't see being able to take anything remotely like, say, Mu Zero, and being able to prove any theorem about it which implies anything like corrigibility or the system not internally trying to harm humans. Anything in which enormous inscrutable floating-point vectors is a key component, seems like something where it would be very hard to prove any theorems about the treatment of those enormous inscrutable vectors that would correspond in the outside world to the AI not killing everybody.\n\nEven if we somehow managed to get structures far more legible than giant vectors of floats, using some AI paradigm very different from the current one, it still seems like huge key pillars of the system would rely on non-fully-formal reasoning; even if the AI has something that you can point to as a utility function and even if that utility function's representation is made out of programmer-meaningful elements instead of giant vectors of floats, we'd still be relying on much shakier reasoning at the point where we claimed that this utility function meant something in an intuitive human-desired sense, say. And if that utility function is learned from a dataset and decoded only afterwards by the operators, that sounds even scarier. And if instead you're learning a giant inscrutable vector of floats from a dataset, gulp. \n\nYou seem to be visualizing that we prove a theorem and then get a theorem-like level of assurance that the system is safe. What kind of theorem? What the heck would it say? \n\nI agree that it seems plausible that the good cognitive operations we want do not *in principle* require performing bad cognitive operations; the trouble, from my perspective, is that generalizing structures that do lots of good cognitive operations will automatically produce bad cognitive operations, especially when we dump more compute into them; \"you can't bring the coffee if you're dead\".\n\nSo it takes a more complicated system and some feat of insight I don't presently possess, to \"just\" do the good cognitions, instead of doing all the cognitions that result from decompressing the thing that compressed the cognitions in the dataset - even if that original dataset only contained cognitions that looked good to us, even if that dataset actually *was* just correctly labeled data about safe actions inside a slightly dangerous domain. Humans do a lot of stuff besides maximizing inclusive genetic fitness, optimizing purely on outcomes labeled by a simple loss function doesn’t get you an internal optimizer that pursues only that loss function, etc.\n\n**Anonymous ** \n\nSteve's intuitions sound to me like they're pointing at the \"well-specified problems\" idea from an earlier thread. Essentially, only use AI in domains where unsafe actions are impossible by construction. Is this too strong a restatement of your intuitions Steve?\n\n**Steve Omohundro ** \n\nThanks for your perspective! Those sound more like social concerns than technical ones, though. I totally agree that today's AI culture is very \"sloppy\" and that the currently popular representations, learning algorithms, data sources, etc. aren't oriented around precise formal specification or provably guaranteed constraints. I'd love any thoughts about ways to help shift that culture toward precise and safe approaches! Technically there is no problem getting provable constraints on floating point computations, etc. The work often goes under the label \"Interval Computation\". It's not even very expensive, typically just a factor of 2 worse than \"sloppy\" computations. For some reason those approaches have tended to be more popular in Europe than in the US. Here are a couple lists of references: [http://www.cs.utep.edu/interval-comp/](http://www.cs.utep.edu/interval-comp/) [https://www.mat.univie.ac.at/~neum/interval.html](https://www.mat.univie.ac.at/~neum/interval.html)\n\nI see today's dominant AI approach of mapping everything to large networks ReLU units running on hardware designed for dense matrix multiplication, trained with gradient descent on big noisy data sets as a very temporary state of affairs. I fully agree that it would be uncontrolled and dangerous scaled up in its current form! But it's really terrible in every aspect except that it makes it easy for machine learning practitioners to quickly slap something together which will actually sort of work sometimes. With all the work on AutoML, NAS, and the formal methods advances I'm hoping we leave this \"sloppy\" paradigm pretty quickly. Today's neural networks are terribly inefficient for inference: most weights are irrelevant for most inputs and yet current methods do computational work on each. I developed many algorithms and data structures to avoid that waste years ago (eg. \"bumptrees\" [https://steveomohundro.com/scientific-contributions/)](https://steveomohundro.com/scientific-contributions/))\n\nThey're also pretty terrible for learning since most weights don't need to be updated for most training examples and yet they are. Google and others are using Mixture-of-Experts to avoid some of that cost: [https://arxiv.org/abs/1701.06538](https://arxiv.org/abs/1701.06538)\n\nMatrix multiply is a pretty inefficient primitive and alternatives are being explored: [https://arxiv.org/abs/2106.10860](https://arxiv.org/abs/2106.10860)\n\nToday's reinforcement learning is slow and uncontrolled, etc. All this ridiculous computational and learning waste could be eliminated with precise formal approaches which measure and optimize it precisely. I'm hopeful that that improvement in computational and learning performance may drive the shift to better controlled representations.\n\nI see theorem proving as hugely valuable for safety in that we can easily precisely specify many important tasks and get guarantees about the behavior of the system. I'm hopeful that we will also be able to apply them to the full AGI story and encode human values, etc., but I don't think we want to bank on that at this stage. Hence, I proposed the \"Safe-AI Scaffolding Strategy\" where we never deploy a system without proven constraints on its behavior that give us high confidence of safety. We start extra conservative and disallow behavior that might eventually be determined to be safe. At every stage we maintain very high confidence of safety. Fast, automated theorem checking enables us to build computational and robotic infrastructure which only executes software with such proofs.\n\nAnd, yes, I'm totally with you on needing to avoid the \"basic AI drives\"! I think we have to start in a phase where AI systems are not allowed to run rampant as uncontrolled optimizing agents! It's easy to see how to constrain limited programs (eg. theorem provers, program compilers or protein designers) to stay on particular hardware and only communicate externally in precisely constrained ways. It's similarly easy to define constrained robot behaviors (eg. for self-driving cars, etc.) The dicey area is that unconstrained agentic edge. I think we want to stay well away from that until we're very sure we know what we're doing! My optimism stems from the belief that many of the socially important things we need AI for won't require anything near that unconstrained edge. But it's tempered by the need to get the safe infrastructure into place before dangerous AIs are created.\n\n**Anonymous ** \n\nAs far as I know, all the work on \"verifying floating-point computations\" currently is way too low-level -- the specifications that are proved about the computations don't say anything about what the computations mean or are about, beyond the very local execution of some algorithm. Execution of algorithms in the real world can have very far-reaching effects that aren't modelled by their specifications.\n\n**Eliezer Yudkowsky ** \n\nYeah, what they said. How do you get from proving things about error bounds on matrix multiplications of inscrutable floating-point numbers, to saying anything about what a mind is trying to do, or not trying to do, in the external world?\n\n**Steve Omohundro ** \n\nUltimately we need to constrain behavior. You might want to ensure your robot butler won't leave the premises. To do that using formal methods, you need to have a semantic representation of the location of the robot, your premise's spatial extent, etc. It's pretty easy to formally represent that kind of physical information (it's just a more careful version of what engineers do anyway). You also have a formal model of the computational hardware and software and the program running the system.\n\nFor finite systems, any true property has a proof which can be mechanically checked but the size of that proof might be large and it might be hard to find. So we need to use encodings and properties which mesh well with the safety semantics we care about.\n\nFormal proofs of properties of programs has progressed to where a bunch of cryptographic, compilation, and other systems can be specified and formalized. Why it's taken this long, I have no idea. The creator of any system has an argument as to why its behavior does what they think it will and why it won't do bad or dangerous things. The formalization of those arguments should be one direct short step.\n\nExperience with formalizing mathematician's informal arguments suggest that the formal proofs are maybe 5 times longer than the informal argument. Systems with learning and statistical inference add more challenges but nothing that seems in-principal all that difficult. I'm still not completely sure how to constrain the use of language, however. I see inside of Facebook all sorts of problems due to inability to constrain language systems (eg. they just had a huge issue where a system labeled a video with a racist term). The interface between natural language semantics and formal semantics and how we deal with that for safety is something I've been thinking a lot about recently.\n\n**Steve Omohundro ** \n\nHere's a nice 3 hour long tutorial about \"probabilistic circuits\" which is a representation of probability distributions, learning, Bayesian inference, etc. which has much better properties than most of the standard representations used in statistics, machine learning, neural nets, etc.: [https://www.youtube.com/watch?v=2RAG5-L9R70](https://www.youtube.com/watch?v=2RAG5-L9R70) It looks especially amenable to interpretability, formal specification, and proofs of properties.\n\n**Eliezer Yudkowsky ** \n\nYou're preaching to the choir there, but even if we were working with more strongly typed epistemic representations that had been inferred by some unexpected innovation of machine learning, automatic inference of those representations would lead them to be uncommented and not well-matched with human compressions of reality, nor would they match exactly against reality, which would make it very hard for any theorem about \"we are optimizing against this huge uncommented machine-learned epistemic representation, to steer outcomes inside this huge machine-learned goal specification\" to guarantee safety in outside reality; especially in the face of how corrigibility is unnatural and runs counter to convergence and indeed coherence; especially if we're trying to train on domains where unaligned cognition is safe, and generalize to regimes in which unaligned cognition is not safe. Even in this case, we are not nearly out of the woods, because what we can prove has a great type-gap with that which we want to ensure is true. You can't handwave the problem of crossing that gap even if it's a solvable problem.\n\nAnd that whole scenario would require some major total shift in ML paradigms.\n\nRight now the epistemic representations are giant inscrutable vectors of floating-point numbers, and so are all the other subsystems and representations, more or less.\n\nProve whatever you like about that Tensorflow problem; it will make no difference to whether the AI kills you. The properties that can be proven just aren't related to safety, no matter how many times you prove an error bound on the floating-point multiplications. It wasn't floating-point error that was going to kill you in the first place.",
      "plaintextDescription": "The following is a partially redacted and lightly edited transcript of a chat conversation about AGI between Eliezer Yudkowsky and a set of invitees in early September 2021. By default, all other participants are anonymized as \"Anonymous\".\n\nI think this Nate Soares quote (excerpted from Nate's response to a report by Joe Carlsmith) is a useful context-setting preface regarding timelines, which weren't discussed as much in the transcript:\n\n> [...] My odds [of AGI by the year 2070] are around 85%[...]\n> \n> I can list a handful of things that drive my probability of AGI-in-the-next-49-years above 80%:\n> \n> 1. 50 years ago was 1970. The gap between AI systems then and AI systems now seems pretty plausibly greater than the remaining gap, even before accounting the recent dramatic increase in the rate of progress, and potential future increases in rate-of-progress as it starts to feel within-grasp.\n> \n> 2. I observe that, 15 years ago, everyone was saying AGI is far off because of what it couldn't do -- basic image recognition, go, starcraft, winograd schemas, programmer assistance. But basically all that has fallen. The gap between us and AGI is made mostly of intangibles. (Computer Programming That Is Actually Good? Theorem proving? Sure, but on my model, \"good\" versions of those are a hair's breadth away from full AGI already. And the fact that I need to clarify that \"bad\" versions don't count, speaks to my point that the only barriers people can name right now are intangibles.) That's a very uncomfortable place to be!\n> \n> 3. When I look at the history of invention, and the various anecdotes about the Wright brothers and Enrico Fermi, I get an impression that, when a technology is pretty close, the world looks a lot like how our world looks.\n> \n>  * Of course, the trick is that when a technology is a little far, the world might also look pretty similar!\n>  * Though when a technology is very far, the world does look different -- it looks like experts pointing to specif",
      "wordCount": 10094
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "oNcqyaWPXNGTTRPHm",
        "name": "Existential risk",
        "slug": "existential-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "A8AyYHFNPJAYJ2Q2D",
    "title": "Excerpts from Veyne's \"Did the Greeks Believe in Their Myths?\"",
    "slug": "excerpts-from-veyne-s-did-the-greeks-believe-in-their-myths",
    "url": null,
    "baseScore": 24,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2021-11-08T20:23:25.271Z",
    "contents": {
      "markdown": "\"Did the Greeks Believe in Their Myths?\" is a long 1983 essay by Paul Veyne, a French archaeologist/historian. The following post (written by me in 2011) consists of excerpts of the essay that I found particularly interesting when I read it.\n\nTopics Veyne discusses include:\n\n*   Can a politician really believe all the things she says?\n*   Can contradictory views, e.g., of alternative medicine, coexist in the same mind? If so, how?\n*   Do people 'believe' in their religion in the same way that they believe in the existence of laundry detergent?\n*   Can someone fabricate completely new scriptures in good faith?\n\n* * *\n\n![](https://scontent-sjc3-1.xx.fbcdn.net/v/t31.18172-8/240491_10150636764605447_3689036_o.jpg?_nc_cat=105&ccb=1-5&_nc_sid=abc084&_nc_ohc=ABdmW47cTSMAX8q_zcA&_nc_ht=scontent-sjc3-1.xx&oh=7c1480adff9ee42b4024713380085c07&oe=61AF7785)\n\n1\\. When historical truth was tradition and vulgate\n---------------------------------------------------\n\nLet us return to Estienne Pasquier, whose *Reserches de la France* appeared in 1560. Before publishing it, G. Huppert tells us, Pasquier circulated his manuscript among his friends. Their most frequent reproach concerned Pasquier's habit of giving too many references to the sources he cited. This procedure, they told him, cast a \"scholastic pall\" (\"umbre des escholes\") on the book and was unbecoming in a work of history. Was it truly necessary each time to confirm his \"words by some ancient author\"? If it was a matter of lending his account authority and credibility, time alone would see to that. After all, the works of the Ancients were not encumbered by citations, and their authority had been affirmed with time. Pasquier should let time alone sanction his book!\n\nThese startling lines show us the gulf that divides our conception of history from the one that was held by ancient historians and was still current among Pasquier's contemporaries. For them, as for the ancient Greeks, historical truth was a vulgate authenticated by consensus over the ages. This consensus sanctioned the truth as it sanctioned the reputation of those writers held to be classical or even, I imagine, the tradition of the Church. Far from having to establish the truth by means of references, Pasquier should have waited to be recognized as an authentic text himself. By putting his notes at the bottom of the page, by furnishing proofs as the jurists do, he indiscreetly sought to force the consensus of posterity concerning his work. \\[...\\]\n\n\\[W\\]e believe that the ancient historians' respect for the tradition transmitted by their predecessors can be explained by the fact that for them history is born, not out of controversy—as it is with us—but from inquiry (and that is precisely the meaning of the Greek word *historia*). When one inquires (whether as traveler, geographer, ethnographer, or reporter), one can only say, \"Here is what I found, here is what I was told by generally reliable sources.\" It would be futile to include the list of informants. Who would check them? One bases one's estimation of a journalist not on his respect for his sources but on an internal critique or a detail where he has been caught in a blatant error or lapse into partiality. \\[...\\] A reporter adds nothing to his credibility by including his informant's identity. We judge his value on internal criteria. We need only read him to know whether he is intelligent, impartial, or precise and whether he has a broad cultural background \\[...\\]\n\nNow, at the university the historian no longer writes for the common reader, as journalists or \"writers\" do, but instead writes for other historians, his colleagues. This was not the case for ancient historians. Thus the latter have an apparently lax attitude toward scientific rigor that we find shocking or surprising. \\[...\\]\n\nThe aim for objectivity delimited the historian's role: before the age of controversy, before the time of Nietzsche and Max Weber, facts existed. The historian had neither to interpret (since facts existed) nor prove (because facts are not the stakes of a controversy). He had only to report the facts, either as a \"reporter\" or a compiler. For that he did not require vertiginous intellectual gifts. He needed only three virtues, which any good journalist possesses: diligence, competence, and impartiality.\n\n![](https://scontent-sjc3-1.xx.fbcdn.net/v/t31.18172-8/243987_10150636767155447_4291441_o.jpg?_nc_cat=111&ccb=1-5&_nc_sid=abc084&_nc_ohc=f7UuGhlo1sYAX-DdquS&_nc_ht=scontent-sjc3-1.xx&oh=15972951a6f560f70862ccdca1be6aa0&oe=61B09B4C)\n\n2\\. The plurality and analogy of true worlds\n--------------------------------------------\n\nVarro's Stoic distinction of three categories of gods is still fundamental: the gods of the city, to whom men made cult; the gods of the poets, that is, those of mythology; and the gods of the philosophers.\n\n\\[...\\]\n\nThese legendary worlds were accepted as true in the sense that they were not doubted, but they were not accepted the way that everyday reality is. For the faithful, the lives of the martyrs were filled with marvels situated in an ageless past, defined only in that it was earlier, outside of, and different from the present. It was the \"time of the pagans.\" The same was true of the Greek myths. They took place \"earlier,\" during the heroic generations, when the gods still took part in human affairs. Mythological space and time were secretly different from our own. A Greek put the gods \"in heaven,\" but he would have been astounded to see them in the sky. He would have been no less astounded if someone, using time in its literal sense, told him that Hephaestus had just remarried or that Athena had aged a great deal lately. Then he would have realized that in his own eyes mythic time had only a vague analogy with daily temporality; he would also have thought that a kind of lethargy had always kept him from recognizing this difference. The analogy between these temporal worlds disguises their hidden plurality. It is not self-evident that humanity has a past, known or unknown. One does not perceive the limit of the centuries, held in memory, any more than one perceives the line bounding the visual field. One does not see the obscure centuries stretching beyond this horizon. One simply stops seeing, and that is all. The heroic generations are found on the other side of this temporal horizon in another world.\n\n\\[...\\]\n\nPindar magnifies his victor's glory by exalting this other, higher world, where glory itself is greater. \\[...\\] It is precisely because the mythical world is definitively other, inaccessible, different, and remarkable that the problem of its authenticity is suspended, and Pindar's listeners float between wonderment and credulity. \\[...\\]\n\nThere is a problem, then, that we cannot avoid: Did the Greeks believe in these tales? More specifically, did they distinguish between what they held as authentic—the historicity of the Trojan War or the existence of Agamemnon and Zeus—and the obvious inventions of the poet, who desired to please his audience? Did they listen with the same ear to the geographical lists and catalogues of ships and to the tale, worthy of Boccaccio, of the amorous adventures of Aphrodite and Ares caught in bed by her husband? If they really believed in myth, did they at least know how to distinguish fable from fiction? But, precisely, it would be necessary to know whether literature or religion are more fictitious than history or physics, and vice versa. Let us say that a work of art is accepted as true in its way, even when it passes for fiction. For truth is a homonym that should be used only in the plural. There are only different programs of truth, and Fustel de Coulanges is neither more nor less true than Homer, even if differently so. Only, it is of truth as it is with Being, according to Aristotle: it is homonymical and analogical, for all truths seem analogous among themselves, so that Racine seems to us to have portrayed the truth of the human heart. \\[...\\]\n\nAccording to a certain program of truth, that of deductive and quantified physics, Einstein is true in our eyes. But if we believe in the *Iliad*, it is no less true according to its own mythical program. The same can be said for *Alice in Wonderland*. For, even if we consider *Alice* and the plays of Racine as fiction, while we are reading them we believe; we weep at the theater. The world of *Alice* and its fairy-tale program is offered to us as a realm as plausible and true as our own—as real in relation to itself, so to speak. We have shifted the sphere of truth, but we are still within the true or its analogy. This is why realism in literature is at once a fake (it is not reality), a useless exertion (the fairy world would seem no less real), and the most extreme sophistication (to fabricate the real with our real; how baroque!). Far from being opposed to the truth, fiction is only its by-product. All we need to do is open the *Iliad* and we enter the story, as they say, and lose our bearings. The only subtlety is that later on we do not believe. There are societies where, once the book is closed, the reader goes on believing; there are others where he does not. \\[...\\]\n\nMyth is information. There are informed people who have alighted, not on a revelation, but simply on some vague information they have chanced upon. If they are poets, it will be the Muses, their appointed informants, who will tell them what is known and said. For all that, myth is not a revelation from above, nor is it arcane knowledge. The Muse only repeats to them what is known—which, like a natural resource, is available to all who seek it.\n\nMyth is not a specific mode of thought. It is nothing more than knowledge obtained through information, which is applied to realms that for us would pertain to argument or experiment. As Oswald Ducrot writes in *Dire et ne pas dire*, information is an illocution that can be completed only if the receiver recognizes the speaker's competence and honesty beforehand, so that, from the very outset, a piece of information is situated beyond the alternative between truth and falsehood. To see this mode of knowledge function, we need only read the admirable Father Huc's account of how he converted the Tibetans a century and a half ago:\n\n> We had adopted a completely historical mode of instruction, taking care to exclude anything that suggested argument and the split of contention; proper names and very precise dates made much more of an impression on them than the most logical reasoning. When they knew the names Jesus, Jerusalem, and Pontius Pilate and the date 4000 years after Creation, they no longer doubted the mystery of the Redemption and the preaching of the Gospel. Furthermore, we never noticed that mysteries or miracles gave them the slightest difficulty. We are convinced that it is through teaching and not the method of argument that one can work efficaciously toward the conversion of the infidel.\n\nSimilarly, in Greece there existed a domain, the supernatural, where everything was to be learned from people who knew. It was composed of events, not abstract truths against which the listener could oppose his own reason. The facts were specific: heroes' names and patronyms were always indicated, and the location of the action was equally precise (Pelion, Cithaeron, Titaresius . . . place names have a music in Greek mythology). This state of affairs may have lasted more than a thousand years. It did not change because the Greeks discovered reason or invented democracy but because the map of the field of knowledge was turned upside down by the creation of new powers of affirmation (historical investigation and speculative physics) that competed with myth and, unlike it, expressly offered the alternative between true and false. \\[...\\]\n\n\\[G\\]enealogical literature, in which Pausanias found a historiography, in reality tells of *aitiai*, origins, the establishment of the order of the world. The implicit idea (still found in book 5 of Lucretius) is that our world is finished, formed and complete (as my child said to me with some amazement, while watching masons at work, \"Papa, so all the houses haven't been built yet?\"). By definition, this establishment occurred before the dawn of history, in the mythical time of the hero. \n\n![](https://scontent-sjc3-1.xx.fbcdn.net/v/t31.18172-8/240564_10150637378490447_6756865_o.jpg?_nc_cat=110&ccb=1-5&_nc_sid=abc084&_nc_ohc=IrzKbri0Z40AX8dCwB6&_nc_ht=scontent-sjc3-1.xx&oh=a0a0762cd45a6a4145bc43b16839bb36&oe=61ADA599)\n\n3\\. The social distribution of knowledge and the modalities of belief\n---------------------------------------------------------------------\n\nThere is no such thing as a sense of the real. Furthermore, there is no reason—quite the contrary—for representing what is past or foreign as analogous to what is current or near. The content of myth was situated in a noble and platonic temporality, as foreign to individual experience and individual interests as are government proclamations or esoteric theories learned at school and accepted at face value. In other words, myth was information obtained from someone else. This was the primary attitude of the Greeks toward myth; in this modality of belief they were depending on someone else's word. Two effects can be noted. First, there is a sort of lethargic indifference, or at least hesitation, about truth and fiction. And this dependence ends up leading to rebellion: people wish to judge things for themselves, according to their own experience. It is precisely this principle of current things that will cause the Greeks to measure the marvelous against everyday reality and pass on to other modalities of belief.\n\nThe most widespread modality of belief occurs when one trusts the word of another. \\[...\\] Westerners, at least those among us who are not bacteriologists, believe in germs and increase the sanitary precautions we take for the same reason that the Azande believe in witches and multiply their magical precautions against them: their belief is based on trust. For Pindar's or Homer's contemporaries, truth was defined either as it related to daily experience or in terms of the speaker's character: whether it was loyal or treacherous.\n\nGiven its dissymmetry, belief in someone else's word could in fact support individual enterprises that opposed their truth to the general error or ignorance. This is the case with Hesiod's speculative theogony, which is not a revelation given by the gods. Hesiod received this knowledge from the muses—that is, from his own reflections. By pondering all that had been said about the gods and the world, he understood things and was able to establish a true and complete list of genealogies. First were Chaos and Earth, as well as Love; Chaos begat Night, Earth gave birth to Heaven and Oceanus. \\[...\\] Many of these genealogies are allegories, and one has the impression that Hesiod takes his god-concepts more seriously than he takes the Olympians. But how does he know so many names and so many details? How does it come to pass that all these old cosmogonies are veritable novels? Because of the dissymmetry that characterizes knowledge based on faith in another. Hesiod knows that we will take him at his word, and he treats himself as he will be treated: he is the first to believe everything that enters his head.\n\nAlong with these more or less esoteric speculations, truth based on belief had another type of hero: the solver of riddles. \\[...\\] For example, here is how Greek tradition depicted the beginnings of philosophy. Thales was the first to find the key to all things: \"Everything is water.\" Was he teaching the unity of the world? Was he on the track that would lead to monism, to the problems of Being and the unity of nature? In fact, if we believe tradition, his thesis was neither metaphysical nor ontological but, instead, allegorical and . . . chemical. \\[...\\] It is not an explanation but a key, and a key must be simple. Monism? Not even that. It is not monism that leads us to speak of the \"key\" to an enigma in the singular. Now, a key is not an explanation. While an explanation accounts for a phenomenon, a key makes us forget the riddle. It erases and replaces it in the same way that a clear sentence eclipses an earlier, more confused, and obscure formulation. As Greek philosophical tradition presents him, Thales does not account for the world in its diversity. He gives us its true meaning, \"water,\" and this answer replaces the enigmatic confusion, which is immediately forgotten. For one forgets the text of a riddle; the solution is the whole point.\n\nAn explanation is something that is sought and proved. The key of a riddle is guessed and, once guessed, it operates instantaneously. There is not even the possibility of an argument. The veil falls away, and our eyes are opened. \\[...\\] We can get a glimpse of it in the work of Freud. It is amazing that the strangeness of his work startles us so little: these tracts, unfurling the map of the depths of the psyche, without a shed of proof or argument; without examples, even for purposes of clarification; without the slightest clinical illustration; without any means of seeing where Freud found all that or how he knows it. From observing his patients? Or, more likely, from observing himself? It is not surprising that this archaic work has been carried on in a form of knowledge that is no less archaic: commentary. What else can be done but comment when the key to the enigma has been found?\n\n![](https://scontent-sjc3-1.xx.fbcdn.net/v/t31.18172-8/256919_10150637365435447_3149008_o.jpg?_nc_cat=110&ccb=1-5&_nc_sid=abc084&_nc_ohc=vUwzQziR6sUAX9dt0-C&_nc_ht=scontent-sjc3-1.xx&oh=11cb7dc364c39f16e563aadd97dc07e2&oe=61B0D8DA)\n\n7\\. Myth and rhetorical truth\n-----------------------------\n\n\\[T\\]here were serious history books, and there were also many of them that were not so serious; but the most important thing is that no external sign differentiated the first from the second. The public was reduced to judging them on an individual basis. As we see, nonprofessionalization had harmful effects. \\[... T\\]he blending of the best and worst misled minds, ruined the readers' moral nature, and fostered a sly skepticism. It thus was necessary for the historians of the day to tactfully manage all the inclinations of a rather mixed audience. When Livy or Cicero in *De re publica* write that Rome is enough of a big city for people to respect the tales with which she adorned her origins, they are not bluffing their readers with ideological stories—quite the contrary. As good reporter-historians, they disdainfully allow each of their readers to choose his preferred version of the facts. Nevertheless, they reveal that on their part they do not believe a word of these tales.\n\nWe see how far ancient artlessness was removed from ideological dictatorship or edifying pretenses. The function created its organ, the \"stock languages\" of etiology or rhetoric, but no political or religious authority contributed its weight. Compared to the Christian or Marxist centuries, Antiquity often has a Voltairean air. Two soothsayers cannot meet without smirking at each other, writes Cicero. I feel I am becoming a god, said a dying emperor.\n\nThis poses a general problem. \\[... T\\]he Greeks believe and do not believe in their myths. They believe in them, but they use them and cease believing at the point where their interest in believing ends. It should be added in their defense that their bad faith resided in their belief rather than in their ulterior motives. \\[...\\]\n\nIt would be better to admit that no knowledge is disinterested and that truths and interests are two different terms for the same thing; for practice thinks what it does. It was desirable to make a distinction between truth and interest only in order to explain the limitations of the former; it was thought that truth was bounded by the influence of interests. This is to forget that interests themselves are limited (in every age they fall within historical limits; they are arbitrary in their fierce interestedness) and that they have the same boundaries as the corresponding truths. They are inscribed within the horizons that the accidents of history assign to different programs.\n\nIf this were not the case, it would seem paradoxical that interests can be the victims of their own ideology. If one were to forget that practices and interests are limited and *rare*, one would take Athenian and Hitlerian imperialism for two examples of an eternal Imperialism, and then Hitlerian racism would be nothing more than an ideological blanket—a motley one, to be sure, but what does it matter? Since the only function of racism is to justify totalitarianism or fascism, the Hitlerian version would be only a superstition or a sham. Then one would note with astonishment that Hitler, because of his racism, sometimes compromised the success of his totalitarian imperialism. The truth is less complicated. Hitler confined himself to putting his racist ideas, which were what interested him, into practice. Jackel and Trevor-Roper have shown that his true war aim was the extermination of the Jews and the extension of Germanic colonization throughout the Slavic states. For him Russians, Jews, and Bolsheviks amounted to the same thing, for he did not think that his persecution of the first two would compromise his victory over the latter . . . Just because one is \"interested\" does not mean that one is rational; even class interests are the products of chance.\n\nSince interests and truths do not arise from \"reality\" or a powerful infrastructure but are jointly limited by the programs of chance, it would be giving them too much credit to think that the eventual contradiction between them is disturbing. Contradictory truths do not reside in the same mind—only different programs, each of which encloses different truths and interests, even if these truths have the same name. I know a doctor who is a passionate homeopath but who nonetheless has the wisdom to prescribe antibiotics in serious cases; he reserves homeopathy for mild or hopeless situations. His good faith is whole. I attest to it. On the one hand, he wants to take pleasure in unorthodox medicines, and, on the other, he is of the opinion that the interest of both doctor and patient is that the patient recovers. These two programs neither contradict each other nor have anything in common, and the apparent contradiction emerges only by taking the corresponding truths literally, which demand that one be a homeopath or not. But truths are not sprinkled like stars on the celestial sphere; they are the point of light that appears at the end of a telescope of a program, and so two different truths obviously correspond to two different programs, even if they go by the same name.\n\nThis is not without interest in the history of beliefs. We do not suffer when our mind, apparently contradicting itself, secretly changes programs of truth and interest, as it unceasingly does. This is not ideology; it is our most habitual way of being. A Roman who manipulates the state religions according to his political ends can be of as good faith as my friend the homeopath. If he is acting in bad faith, it will be because he does not believe in one of his two programs while he is using it; it will not be because he believes in two contradictory truths. Besides, bad faith is not always found where we think it is. Our Roman could be sincerely pious. If he affects a religious scruple that he scarcely believes in in order to call off an election in which the people are likely to make a poor choice, this does not prove that he does not believe in his gods; it proves only that he does not believe in the state religion and holds it to be a useful imposture invented by men. Even more likely, he will think that all the values must be defended together, religion or fatherland, and that a reason is never a bad one when it supports a good cause.\n\nOur daily life is composed of a great number of different programs, and the impression of quotidian mediocrity is precisely the result of this plurality, which in some states of neurotic scrupulosity is sensed as hypocrisy. We move endlessly from one program to another the way we change channels on the radio, but we do it without realizing it. Religion is only one of these programs, and it rarely acts within the others.\n\nAs Paul Pruyser says in his *Dynamic Psychology of Religion*, religion occupies only the slightest part of a religious man's thoughts during the day, but the same could be said of a sports fan, militant, or poet. It occupies a narrow band, but it does so genuinely and intensely. \\[...\\] Religion, politics, and poetry may well be the most important things in this world or any other; nevertheless, in practice they occupy only a narrow band of our existence, and they tolerate contradiction all the more easily since it generally passes unnoticed. This does not mean that these beliefs are any less sincere and intense. The metaphysical importance or individual sincerity of a truth is not measured by its wavelength. \\[...\\]\n\nThe different truths are all true in our eyes, but we do not think about them with the same part of our head. In a passage in *Das Heilige*, Rudolf Otto analyzes the fear of ghosts. To be exact, if we thought about ghosts with the same mind that makes us think about physical facts, we would not be afraid of them, or at least not in the same way. We would be afraid as we would be of a revolver or of a vicious dog, while the fear of ghosts is the fear of the intrusion of a different world. For my part, I hold ghosts to be simple fictions but perceive their truth nonetheless. I am almost neurotically afraid of them, and the months I spent sorting through the papers of a dead friend were an extended nightmare. At the very moment I type these pages I feel the hairs stand up on the back of my neck. Nothing would reassure me more than to learn that ghosts \"really\" exist. Then they would be a phenomenon like any other, which could be studied with the right instruments, a camera or a Geiger counter. This is why science fiction, far from frightening me, delightfully reassures me.\n\n\\[...\\]\n\nThe faithful did not consider their all-powerful master to be an ordinary man, and the official hyperbole that made of this mortal a god was true in spirit. It corresponded to their filial devotion. Swept on by the linguistic tide, they experienced this feeling of dependence all the more strongly. However, the absence of votive offerings proves that they did not take the hyperbole literally.\n\n\\[... T\\]hese truths are only the clothing of forces; they are practices, not the light that guides them. When men depend on an all-powerful man, they experience him as a man and see him from a valet's perspective as a mere mortal; but they also experience him as their master and therefore also see him as a god. The plurality of truths, an affront to logic, is the normal consequence of the plurality of forces. The thinking reed is humbly proud to oppose his weak and pure truth to brute forces, yet all the while this truth is one of these forces. Thought belongs to the infinitely pluralized monism of the will to power. \\[...\\] And because thought is a force, it is not separated from practice the way the soul is from the body. It is a part of it. Marx spoke of ideology to emphasize that thought was action and not pure understanding; but as a materialist of the old school he attached the soul to the body instead of not distinguishing the one from the other and handling practice as a unit.\n\n![](https://scontent-sjc3-1.xx.fbcdn.net/v/t1.18169-9/250467_10150637473490447_7381207_n.jpg?_nc_cat=106&ccb=1-5&_nc_sid=abc084&_nc_ohc=BVqVgTaTdFAAX88o1CU&_nc_ht=scontent-sjc3-1.xx&oh=be65a44fce2be6e7cec90b43332ca451&oe=61AED33A)\n\n9\\. Forger's truth, philologist's truth\n---------------------------------------\n\n\\[T\\]hese imaginary lists \\[...\\] had duped so many people, beginning with their own creators. This historiography of sincere forgers is so strange that it is worth considering for a moment. \\[...\\] How does one decide that a king was called Ampyx? Why this name instead of a million others? A program of truth existed in which it was accepted that someone, Hesiod or someone else, told the truth when he reeled off the names that passed through his mind or spouted the most unbridled Swedenborgian fantasies. For such people psychological imagination is a source of veracity.\n\nThis attitude, normal in the founder of a religion, is not incomprehensible in a historian, either. Historians are merely prophets in reverse, and they flesh out and animate their *post eventum* predictions with imaginative flourishes. This is called \"historical retrodiction\" or \"synthesis,\" and this imaginative faculty furnishes three-fourths of any page of history, with documents providing the rest. There is more. History is also a novel containing deeds and proper names, and we have seen that, while reading, we believe that what we read is true. Only afterward do we call it fiction, and even then we must belong to a society in which the idea of fiction obtains.\n\nWhy shouldn't a historian invent the names of his heroes? A novelist does. Neither one invents in the strict sense of the word. They discover in their imagination a name they had never thought of before. The mythographer who made up the list of the kings of Arcadia thereby discovered in himself a foreign reality that he had not deliberately put there and that had not been there beforehand. He was in the state of mind in which a novelist finds himself when his characters \"get away from him.\" \\[...\\]\n\nThe *Iliad* passed largely for history, but, since readers expected entertainment, the poet could add his own inventions without disturbing them. On the other hand, readers of Castor, the inventive historian of the long line of legendary kings of Argos, consulted him for information, and instead of floating in pleasure—which is neither true nor false—they believed it all. But that is precisely the issue. The very boundary between information and entertainment is dictated by convention, and societies other than ours have practiced agreeable sciences. \\[...\\] When the parent of a student, clever and well read, asked his son's grammarian sticky questions concerning \"the name of Anchises' nurse or Anchemolus' stepmother,\" as Juvenal puts it, he cares little about the historicity of the two. Even we moderns may take pleasure in history as a detective story\\[.\\]",
      "plaintextDescription": "\"Did the Greeks Believe in Their Myths?\" is a long 1983 essay by Paul Veyne, a French archaeologist/historian. The following post (written by me in 2011) consists of excerpts of the essay that I found particularly interesting when I read it.\n\nTopics Veyne discusses include:\n\n * Can a politician really believe all the things she says?\n * Can contradictory views, e.g., of alternative medicine, coexist in the same mind? If so, how?\n * Do people 'believe' in their religion in the same way that they believe in the existence of laundry detergent?\n * Can someone fabricate completely new scriptures in good faith?\n\n----------------------------------------\n\n \n\n\n1. When historical truth was tradition and vulgate\n \n\nLet us return to Estienne Pasquier, whose Reserches de la France appeared in 1560. Before publishing it, G. Huppert tells us, Pasquier circulated his manuscript among his friends. Their most frequent reproach concerned Pasquier's habit of giving too many references to the sources he cited. This procedure, they told him, cast a \"scholastic pall\" (\"umbre des escholes\") on the book and was unbecoming in a work of history. Was it truly necessary each time to confirm his \"words by some ancient author\"? If it was a matter of lending his account authority and credibility, time alone would see to that. After all, the works of the Ancients were not encumbered by citations, and their authority had been affirmed with time. Pasquier should let time alone sanction his book!\n\nThese startling lines show us the gulf that divides our conception of history from the one that was held by ancient historians and was still current among Pasquier's contemporaries. For them, as for the ancient Greeks, historical truth was a vulgate authenticated by consensus over the ages. This consensus sanctioned the truth as it sanctioned the reputation of those writers held to be classical or even, I imagine, the tradition of the Church. Far from having to establish the truth by means of references, Pas",
      "wordCount": 4930
    },
    "tags": [
      {
        "_id": "x5TtBDjRg9egvg9gm",
        "name": "Cultural knowledge",
        "slug": "cultural-knowledge"
      },
      {
        "_id": "XYHzLjwYiqpeqaf4c",
        "name": "Dark Arts",
        "slug": "dark-arts"
      },
      {
        "_id": "tPfh8oYyG8Wgkkg8X",
        "name": "Journalism",
        "slug": "journalism"
      },
      {
        "_id": "NSMKfa8emSbGNXRKD",
        "name": "Religion",
        "slug": "religion"
      },
      {
        "_id": "YTCrHWYHAsAD74EHo",
        "name": "Self-Deception",
        "slug": "self-deception"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wfTCAxjPhmFs7qLyD",
    "title": "Transcript for Geoff Anders and Anna Salamon's Oct. 23 conversation",
    "slug": "transcript-for-geoff-anders-and-anna-salamon-s-oct-23",
    "url": null,
    "baseScore": 83,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 97,
    "createdAt": null,
    "postedAt": "2021-11-08T02:19:04.189Z",
    "contents": {
      "markdown": "Geoff Anders of Leverage and Anna Salamon of CFAR had a conversation on [Geoff's Twitch channel](https://www.twitch.tv/geoffanders) on October 23, triggered by Zoe Curzi's [post](https://www.lesswrong.com/posts/XPwEptSSFRCnfHqFk/zoe-curzi-s-experience-with-leverage-research) about having horrible experiences at Leverage.\n\nTechnical issues meant that the Twitch video was mostly lost, and the audio only resurfaced a few days ago, thanks to Lulie.\n\nI thought the conversation was pretty important, so I'm (a) using this post the signal-boost the audio, for people who missed it; and (b) posting a transcript here, along with the (previously unavailable, and important for making sense of the audio) last two hours of chat log.\n\n[**You can find the full audio here**](https://www.dropbox.com/s/p0nah8ulohypexe/Geoff%20Anders%20-%20Twitch%20-%20Leverage%20History%201%20with%20Anna%20Salamon.m4a?dl=0), and video of the first few minutes [**here**](https://www.dropbox.com/s/pt3q5xejglsgrcr/1st%20Twitch%20Stream%20-%20Leverage%20History%20-%20Beginning.mp4?dl=0#). The full audio discusses a bunch of stuff about Leverage's history; the only part of the stream transcribed below is the Anna/Geoff conversation, which starts about two hours in.\n\nThe chat messages I have aren't timestamped, and sometimes consist of side-conversations that don't directly connect with what Geoff and Anna are discussing. So I've inserted blocks of chat log at what seemed like *roughly* the right parts of the conversation; but let me know if any are in the wrong place for comprehension, and I'll move them elsewhere.\n\n* * *\n\n1\\. (1:57:57) Early EA history and EA/Leverage interactions\n-----------------------------------------------------------\n\n\\[...\\]  \n  \nAnna Salamon:  \nAll right, let's talk.  \n  \nGeoff Anders:  \nAll right, all right. Yeah, let's get to it.  \n  \nAnna Salamon:  \nI hear that the rationality and Leverage communities have some sort of history!  \n  \nGeoff Anders:  \nThat's right. Okay, so basically, Anna, as I mentioned to you when we chatted about this, I wanted to talk about in general Leverage history, and then there's a long history with the rationality community, and right now, it seems like there's... I'd like to understand what happened with the relations, basically.\n\nLike, earlier on, things were substantially more congenial. More communication. There were various types of joint events. Ben, who suggested he would be called by first name, organized the first solstice for the rationality community. He did that while working at Leverage. That was totally fine. I volunteered at the CFAR workshops. Ben also did that. Then, EA Summit 2013, I thought was sort of a big deal. We had you guys sort of do a half-workshop during that. I thought that was great. Ben reminded me that we actually held a book launch for MIRI somehow. I remember the event very vaguely. Yeah, but it was at Luke Nosek's house, and I don't know. Maybe it was a joint... I don't remember, but there was a bunch of stuff.  \n  \nLet's see. All right, yeah, so I just switched my audio. Ali, let me know if that's better or worse.\n\nBut anyway, so there was that, and then I feel like we just had lots of interests in common and so forth, but then somehow, things didn't end up working out as excellently as possible. So I was interested in just chatting about what happened, getting some common context on the history, maybe trying to...\n\n<table><tbody><tr><td>sowgone: 'Our Final Invention', maybe</td></tr></tbody></table>\n\nGeoff Anders:  \nOh, yeah. Maybe *Our Final*... Yes! Yes! I think that was the book. At least, that sounds pretty likely.\n\nYeah, so basically looking a bit at the past, and also trying to understand where things are presently, and *maybe* figuring out whether there's anything constructive to be done. Like, in some circumstances it seems like trust is so low that I'd love to know what could make things better, basically.\n\nSo anyway, I just talked for a bit. Yeah, so let's talk about it.  \n  \nAnna Salamon:  \nYeah. Are we still trying to end at 1:00, in 20 minutes?\n\nGeoff Anders:  \nI mean, I'm willing to go on longer. I know you had a thing, but you know-  \n \n\n<table><tbody><tr><td>benitopace: I'd be down for a 60 minute conversation, starting now.</td></tr></tbody></table>\n\n  \nAnna Salamon:  \nOh, I don't have a thing.  \n  \nGeoff Anders:  \nOh, okay. Yeah, so-  \n  \nAnna Salamon:  \nYeah.  \n  \nGeoff Anders:  \nApologies to all the viewers and so forth, but yeah, let's go to 1:30.\n\n<table><tbody><tr><td>benitopace: Sg!</td></tr></tbody></table>\n\nAnna Salamon:  \nGreat. So yeah, I don't know. I guess one thing I want to know...\n\nLook, I'm going to try to tell history. I'm going to get it wrong. There's 50 other people out there who are going to have 50 other histories. Maybe we can get them all out there via mine and Geoff's to start with, or something.\n\nBut, so my perception is - I want to talk about that 2013 summit, where we all... like, 50 people or something stuffed into a giant Leverage house for like, I forget, several days.  \n  \nGeoff Anders:  \nYeah, I think it was 60-  \n  \nAnna Salamon:  \nA week?  \n  \nGeoff Anders:  \n... I think, and it was a week, yeah. It was about seven days long, I think.  \n  \nAnna Salamon:  \nAnd these people were CFAR, and Leverage, and MIRI, and a bunch of EA-ers from overseas?  \n  \nGeoff Anders:  \nYeah, we-  \n  \nAnna Salamon:  \nWho else was there?\n\n<table><tbody><tr><td><p>anna_salamon: Who here was there, at the 2013 stuff into leverage house EA retreat?</p><p>anna_salamon: Anyone but me and Geoff?</p><p>larissa24joy: <a href=\"https://www.youtube.com/user/nnevvinn/videos\">https://www.youtube.com/user/nnevvinn/videos</a></p><p>fiddlemath: Anja's here with me, and was there</p><p>larissa24joy: Videos from it above</p></td></tr></tbody></table>\n\n  \nGeoff Anders:  \nWell, we tried to invite basically a selection of everybody. You know, Holden Karnofsky talked there. I think we beamed in Jaan and Singer for talks. Thiel talked, but then we had-  \n  \nAnna Salamon:  \nOh yeah, in person.  \n  \nGeoff Anders:  \nYeah, yeah. Then, we had-  \n  \nAnna Salamon:  \nFor several hours. I'm remembering as you're saying the things.  \n  \nGeoff Anders:  \nYeah, yeah. And then we... I remember when we designed it, we tried to basically balance it to have people who were interested in the different parts of EA. I have some spreadsheet somewhere, but we tried to... We flew in a bunch of people from the UK, maybe a couple other places, but something like that. And obviously not everybody invited came, but yeah, we had... I feel like we had a mix. Like, for people who are interested, the-  \n  \nAnna Salamon:  \nWas Toby Ord there?  \n  \nGeoff Anders:  \nNo. I don't think-  \n  \nAnna Salamon:  \nI feel like CEA was somehow proto-there. Maybe I'm making that up. Okay. Great. \\[inaudible\\] says I made that up. I probably did.  \n  \nGeoff Anders:  \nI think some CEA-affiliated people were there. I don't think the main people came, though they were invited.\n\nThere's a video online that, yeah, if you go to YouTube and I guess search 2013 EA Summit-  \n  \nAnna Salamon:  \nAnyway, great, great, great. I am in a hurry.  \n  \nGeoff Anders:  \nYeah, go ahead.  \n  \nAnna Salamon:  \nEven though we have until 1:30.\n\nSo the thing I want to say about that - which I'm not sure how many people would agree with me - but my own narrative is something like that there were a bunch of different people working on and having strands that were kind of related, and we were trying abstractly to think well with each other, because we all knew it was good to be cooperative or something, but we didn't have that much actual flesh on our cooperation.\n\nAnd then my narrative is that Geoff or Leverage or somebody was like, \"You know what would help? I think it would help if we all spent a week stuck in a house together.\"  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nAnd it did, and then we emerged all friends somehow. Like, not exactly friends, but all actually feeling like there was a thing that was in common, that we might call the EA thing, and that it was worth talking to each other. And a bunch of people got the AI risk thing, who came in skeptical about the AI risk thing. And I don't know, I was very self-centered about the AI risk thing. I'm not sure how many...\n\n<table><tbody><tr><td>sowgone: I was at 2014, not 2013</td></tr></tbody></table>\n\nAnna Salamon:  \nNo, no, no, it wasn't... Oh, you were at 2014. Yeah, yeah. There was a previous one. The 2014 one was much larger, and more disparate. I liked it too, but it was like we were... Sorry, somebody in the chat was mentioning 2014.  \n  \nGeoff Anders:  \nWell, we did-  \n  \nAnna Salamon:  \nIt was-  \n  \nGeoff Anders:  \nIf I could just jump in with one fact.  \n  \nAnna Salamon:  \nOkay.  \n  \nGeoff Anders:  \nWe did both the week-long thing at a retreat place, and then we did a two-day event, like the Summit, so the retreat was like-  \n  \nAnna Salamon:  \nIn 2014.  \n  \nGeoff Anders:  \nYeah, so we did both in 2014.  \n  \nAnna Salamon:  \nYeah, but even the week-long thing to me seemed, in some ways, less good than the week-long thing in 2013, because it was spread out instead of being in one house.  \n  \nGeoff Anders:  \nI agree. Yeah.  \n  \nAnna Salamon:  \nWhich was maybe necessary because there were more people or whatever.\n\nBut anyway, from my perception, there was a lot of camaraderie in the early days, even like 2008, 2007, up through 2013, but the camaraderie sort of congealed and became more of a thing across more of the people that week in the house. And there were always tones of skepticism, but the tones of skepticism were maybe at a local low point right after that week from my perception.\n\nAnd then we have gradually, not just Leverage and the rationality community - whatever that is, sorry CFAR, LessWrong, I don't know, MIRI -  \n  \nGeoff Anders:  \nI... think there's a community.  \n  \nAnna Salamon:  \nWhat? There is a community, but it's a community that's... Yeah, there's a community, but there's also several different partial centers of the community or something.  \n  \nGeoff Anders:  \nAgreed, yeah.  \n  \nAnna Salamon:  \nSo I wasn't quite sure which one I was trying to talk about.\n\nAnyway, so Leverage and various strands of the rationality community were pretty harmonious there in my perception, and were a little less so before, and much less so gradually after.\n\nBut also like, I don't know - CFAR and CEA, for example. In my head, there was more of the congealment there. Like, we were all friends back then.\n\nAnd now we're all... maybe friends, certainly trying to be harmonious - many of us, maybe not with Leverage, maybe Leverage is our official \"figure out whether we're supposed to ostracize this month\" target, we can talk about that...  \n  \nBut I guess I just wanted to situate the puzzle of how rationality and Leverage drifted apart, with a puzzle of: I think that in general, most of the organizations think that most of the other organizations' work isn't that good, and usually do try to be cordial and harmonious, but I think that things somehow started out much more \"everybody believing in cooperation with each other\", and are much less that way now, even though there's quite a bit of cordialness still.\n\nAnd I guess I want to situate the splitting of Leverage with CFAR or whatever, in that larger context where I think most things split from most things. And I'm kind of curious about what was up with the larger context.  \n  \nGeoff Anders:  \nYeah. I mean, I agree with this broad picture. I wouldn't put the start of EA to the EA Summit, though I think people could argue about that. I think EA started before. But apart from that, I agree.\n\nI think there is something like most of the groups have become more disparate. It's really hard to figure out how to say this without sounding biased in my own favor, and also while there's lots of concerns, and a bunch of sort of justifiable things, it's really hard to say things like this, but in 2012 and 2013 and 2014, we were working hard to try to get the different groups to be able to work together.\n\nLike, 2013 we did the Summit. 2012, we did THINK, the high impact network. Where we wanted to have CFAR, The Life You Can Save, 80,000 Hours, and Giving What We Can and Leverage all collaborate. And that didn't end up working. But then the EA Summit did.\n\nYeah. And then, I feel like the cordial relations and the \"more on the same team\" thing extended through 2014.\n\nFor my own story, just to add a part to it, I think that I stopped wanting to... like, stopped engaging as much with CFAR after CFAR stopped doing as much research? Like, CFAR was doing more research early on, and then the research diminished over the course of time. That was something that mattered for me, personally.\n\nBut I mean even, I think in 2018 we did a Paradigm workshop specifically for CFAR people to try to, like, something. And I talked to the people about, \"Is there room for some sort of alliance or working together?\" And that didn't go anywhere, but who knows?  \n  \nAnna Salamon:  \nHuh. I don't remember this. Was I invited?  \n  \nGeoff Anders:  \nNo, no, this was the-  \n  \nAnna Salamon:  \nIs it specifically for CFAR people...?  \n  \nGeoff Anders:  \nI think at the time you were... I haven't always tracked when you're in charge of CFAR. Like, there's at some times...  \n  \nAnna Salamon:  \nBut I've been working at CFAR continually.  \n  \nGeoff Anders:  \nYeah. So-  \n  \nAnna Salamon:  \nI wasn't ED, but I was totally working there.  \n  \nGeoff Anders:  \nOkay. Yeah. So I-  \n  \nAnna Salamon:  \nI've been in a leadership role.  \n  \nGeoff Anders:  \nOkay. Yeah. So this was a reach out to Eli Tyre et al., I think.  \n  \nAnna Salamon:  \nOkay. Yeah, yeah.\n\nThis is not very charitable of me at all, but I think I had a perception that you were trying specifically to recruit our junior staff and that you weren't that interested in talking to people like - by, recruit, sorry, I don't mean to pull them away from CFAR, but I do mean to sort of ideologically recruit them - but that you weren't that interested in me because you were less likely to get that much of my mind or soul or something. Confirm or deny?  \n  \nGeoff Anders:  \nYeah, no, it's a good pointed question.\n\nHow do we draw a distinction between \"get you ideologically\" and \"have constructive conversations, wherein both sides update\"?  \n  \nAnna Salamon:  \nUm. I don't know. I do think there's... Well, what do you think there... I think these are quite different. I think they're both great things, depending on context.  \n  \nGeoff Anders:  \nSure.  \n  \nAnna Salamon:  \nEspecially the second one.  \n  \nGeoff Anders:  \nYeah, yeah.  \n \n\n<table><tbody><tr><td><p>anna_salamon: Anja, would love to hear if your perceptions match/differ, what you remember</p><p>fiddlemath: Anja: matches up so far</p><p>fiddlemath: Anja is carrying the baby, but -- she has a pet theory that a lot of the friction / tension traces back to Leverage overstepping boundaries with their recruiting practices.</p><p>AgileCaveman: The vibe here is two people talking about a failed relationship, except it's two large orgs.</p><p>habrykas: I can confirm memories of tension that was associated with Leverage's recruiting practices</p><p>habrykas: From the 2014-2016 era</p></td></tr></tbody></table>\n\n  \nAnna Salamon:  \nI often learned from conversations with you, though. I remember going over to your house a bunch of times with Nate, it seemed good. You seemed less interested.  \n  \nGeoff Anders:  \nWhat year was that?  \n  \nAnna Salamon:  \nThat was, I'm going to make this up, my made-up year is 2016 or 2017.  \n  \nGeoff Anders:  \nYeah, I-  \n  \nAnna Salamon:  \nRemember?  \n  \nGeoff Anders:  \nI was talking - \\[crosstalk\\] - I was talking to somebody who jogged my memory on that. And I do think that was less important to me at the time. Like, the...  \n  \nAnna Salamon:  \nOkay, so my yeah jerk story as to why we drifted apart - I can tell jerk stories that emphasize me as the bad guy, I'm going to emphasize you as the bad guy -  \n  \nGeoff Anders:  \nWell, y'know...  \n  \nAnna Salamon:  \n\\- but you're welcome to turnabout or whatever.  \n  \nGeoff Anders:  \nYep.  \n  \nAnna Salamon:  \nMy jerk story is, you were busy trying to build an empire for reasons, that you thought would be able to build toward good things, or whatever. And we didn't look like good ways for you to do that. Partly because we related differently to power than you do, or power, closure, how much of your stuff to keep inside in what way. And partly because, as you say, we were not doing that much of the kind of psychological research that you guys were trying to do, so we weren't particularly project-aligned in that way. And I think it's cool that you guys were trying to do that kind of thing. And I think in CFAR, a lot of us were curious as to what your stuff was and whether it was cool. So I think that was one of the things.\n\nAnyway, I think you were trying to build a thing with power where you could control it, and I think alternate perspectives were not that interesting to you on a visceral level for that reason. And this is part of why we drifted apart.  \n  \nGeoff Anders:  \nOkay.  \n  \nAnna Salamon:  \nAnd I think I looked more like a... kind-of-like-resource-kind-of-like-obstacle, whereas some of the junior staff looked more like people you could recruit.  \n  \nGeoff Anders:  \nYeah. But then, you said \"recruit\". So, if I could ask some questions about this. So, you said \"recruit, but not hire\".  \n  \nAnna Salamon:  \nWell, maybe hire, but \"hire\" isn't the important part.  \n  \nGeoff Anders:  \nOkay. Right. So then I guess my question is something like: I do end up forming views on who I'll be able to work with, more and less and so forth. And then, I mean... there's, like, a nefarious-themed-ness to the thing you're saying. But how does the...  \n  \nAnna Salamon:  \nShould I try to actually spell out what the thing is that I think is bad?  \n  \nGeoff Anders:  \nYes! \\[crosstalk\\] That would be great. Because I can tell the same story without the nefarious theme. Yeah.  \n  \nAnna Salamon:  \nYeah. Sorry for accidentally having it in the connotation.  \n  \nGeoff Anders:  \nOh, it's okay.\n\n2\\. (2:13:22) Narrative addiction and EA leaders being unable to talk to each other\n-----------------------------------------------------------------------------------\n\nAnna Salamon:  \nOkay. I'll take a couple steps back, explain a bit of worldview in which I think it contexts why a thing I think might be bad might have been occurring. And then say the thing, sorry.\n\nSo, I guess I've been playing around a lot lately, in the last six months or something, with the concept of *narrative addiction*. Where in my head... I don't know, I'm gonna talk about stuff I don't know anything about, nobody should believe me if you think it's false. If you guys could just figure out which parts are which, that'll be great.\n\nI tend to think of, for example, anorexia. Which I know nothing about, I'm just going to terribly say bad things about it as a metaphor while violating all rules. I tend to think of anorexia as being something maybe sort of like narrative addiction.\n\nLike, it's addictive, but it's not addictive the way heroin is addictive. It's not physical, but there's a sense of control or something. And you want to be in control. There's a narrative, or something, and somehow something will get better in the narrative. In extreme cases, people cling to it even at the expense of life. That's probably horribly wrong for anorexia, and I apologize to everybody for using it as a metaphor.\n\nBut the thing I actually want to talk about is I think, for me, I spent a whole bunch of the last five years or something in what I would call narrative addiction.\n\n  \nGeoff Anders:  \nHm!\n\n  \nAnna Salamon:  \nYeah, sorry. Accuse me of a thing and then I'll go accuse you of the thing. And then you can tell me that it's false about you.\n\nWhere according to me, there was a thing that I wanted, which was something like \"to make the world a lot better.\" And a story that I had in which I was doing a thing like that. And then various things would happen that were kind of uncomfortable, or didn't feel good, or whatever. And I would sort of retreat to perching on this narrative and to doing things that would locally reinforce the narrative for me.\n\nAnd I think basically this was pretty costly and not very good for my happiness or my effects in the world, or my ability to have actual conversations where I was listening to the other party.\n\nI think a thing that happens to a lot of people in positions of relative power and influence in a social graph - similar to the one that I was in, and similar also to the one that I believe you were in - is somehow only doing the things that reinforce the narrative, and having much more trouble than other people have hearing perspectives that aren't their own.\n\nSo, like, in terms of this fragmentation of the initial EA community, it seems... Like, I had an interesting conversation with some people several years ago where they were like, \"Yeah, I feel like if we stuck...\" I can't remember who they actually named, but something like, \"I feel like if you stuck, for example, Anna and Geoff, or Geoff and Eliezer, or Eliezer and Toby Ord, or whatever, in a room, it would actually be like relatively...\" I don't know about Toby. Maybe Toby's better. \"... It would be relatively difficult for most of these pairs of people to have a real conversation. But if you take random people from the organizations, they do fine. What's up with this?\"  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nWe were like, \"Yeah, that's kind of interesting.\"\n\n<table><tbody><tr><td><p>spiracularity: I like the way this is going, this feels both interesting and I think it hits something valuable.</p><p>larissa24joy: +1</p><p>AgileCaveman: oh yeah super valuable,</p><p>larissa24joy: something like the narrative addiction concept rhymes with some of my EA experience so this is an interesting and useful concept (thanks Anna)</p><p>fiddlemath: Geoff, for later: Some of the narrative addiction effect may come from organizations growing larger and more of their members navigating reality via narratives and social cues</p></td></tr></tbody></table>\n\n  \nGeoff Anders:  \nI definitely just affirm - I definitely think there's a leader's thing. In different fights and so forth that happened at different times, my observation was, the leaders were more opposed. The people were like, \"Can we get along?\"  \n  \nAnna Salamon:  \nAnd they could, and the leaders couldn't; and the leaders also couldn't *talk*, which is more the interesting thing. Not just couldn't get along.  \n  \nGeoff Anders:  \nYeah, yeah.  \n  \nAnna Salamon:  \nAnyway, so I believe you... So, look, the negative thing that may or may not have been the case, is that you wanted people who were \"small\" enough that you could - and I think these people were great and were on great growth trajectories, but nevertheless, to say terrible things - you wanted people who were \"small\" enough that you could intellectually dominate them and they would follow your thing, rather than being willing to loosen your grip on narrative addiction or something enough to be able to hear alternate perspectives. That would be the-\n\n3\\. (2:17:14) Early Geoff cooperativeness\n-----------------------------------------\n\nGeoff Anders:  \nOkay. I definitely think that there's at least... Like, I definitely think I didn't appreciate really at all the work-life distinction and the value of doing things that weren't for the mission and so forth. So I think that's definitely there. But if I could ask a question about the thing, what was the narrative that-  \n  \nAnna Salamon:  \nMine?  \n  \nGeoff Anders:  \nNo, no, for-  \n  \nAnna Salamon:  \nYours?  \n  \nGeoff Anders:  \nYeah, because from my perspective... I mean, you may recall this and this may be another source of tension. Initially, I wanted to just merge the organizations, right? I tried to be hired as research director at MIRI. I don't know if you recall this, but-  \n  \nAnna Salamon:  \nI remember you tried to get hired at MIRI, at the very beginning, yeah.  \n  \nGeoff Anders:  \nYep. Yep. But that was like, Leverage was super small then. And I'm like, well, we have some differences, but there's a... There's the inside-Leverage thing, where there's like the different projects working in parallel and helping each other. I also thought about that with regard to external organizations. I talked to Will MacAskill in... 2012, I guess. Was it 2012? I think 2012. And I was like, \"We should maybe merge Leverage and Giving What We Can.\" And he was like, \"Yeah, good idea,\" or something like that. And then it didn't happen. And I don't know if he was... I'm not sure what his attitude was on that.\n\nBut, like, even take something like Giving What We Can, which is focused on global poverty, or was focused on global poverty. I always had the perception that it just would be valuable for the other projects to build up a really big Giving What We Can thing. I mean, if you have somebody build up Giving What We Can in a very large, substantial way, that just will in fact pick up more people who are interested in animals and building EA and AI and Leverage's style of research, and et cetera. So there's a thing that I'd like you to try to square with-  \n  \nAnna Salamon:  \n\\[crosstalk\\] Rationality is the common interest of many causes. Yeah, we had this in the beginning, then it became rational-  \n  \nGeoff Anders:  \nRight. But, so square that with the idea of needing people to be in my thing.\n\nWhoops, sorry. Hold on, my phone is beeping-  \n  \nAnna Salamon:  \nWell, okay. So my story about you and me, and many people, is that we started out not having as much of the narrative addiction and ended up more in the narrative addiction. So I agree that at the beginning you didn't seem that much like you were doing this, compared to my stories about you later on.  \n  \nGeoff Anders:  \nOkay.  \n  \nAnna Salamon:  \nI have the same story about myself and several other people.  \n \n\n4\\. (2:20:00) Possible causes for EAs becoming more narrative-addicted\n----------------------------------------------------------------------\n\nGeoff Anders:  \nDo you have any idea what would have caused us to become more narratively addicted? Because I find this to be really an interesting hypothesis. I think there's some ways that I may not have had as much perspective on things, especially near the end before dissolving Leverage. And so I'm really interested in this hypothesis, but I haven't been able to... It's really hard to see how you lose perspective.  \n  \nAnna Salamon:  \nYeah, it is hard to see how you lose perspective. You almost... Yeah, you need to look from inside and from outside at once, or something.  \n  \nGeoff Anders:  \nYeah. But maybe you have leads on... or, what...  \n  \nAnna Salamon:  \nYeah. I mean, I don't know. One story - this is a very, very generic story, I think it would also be nice to try to track this one with Leverage and CFAR specifically.  \n  \nGeoff Anders:  \nSure. Yeah.  \n  \nAnna Salamon:  \nBut sort of as a case study in this larger thing, almost. In addition to being a thing in itself.\n\nBut, yeah, I have a story that's something like... Many, many actions run partly on promissory notes. (Borrowing a lot of this, by the way, from Bryce Hidysmith, so if anybody likes these ideas, I think they should give some credit to him.)\n\nMany, many actions run partly on a promissory note. I make myself tea, and this is because I can visualize the tea which is located in the future, and it's calling to me, and I believe that if I put my cup in the microwave I'm going to successfully get some tea, and it's going to be great. Right?\n\nBut there's the question of how much of the thing is a promissory note that hasn't been cashed yet? Like, what the ratio is of \"expectation of the future\" compared to \"realized stuff right now\". And what ratios those have in pulling our actions.  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nThe EA movement was very, very high in \"we're going to have a whole bunch of glorious stuff in the future, it's going to be amazing\", compared to how much we had actually done so far in terms of lives saved, or whatever.  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nOne story I have is that we got together, and initially we all hoped to do the best thing. And the best thing was basically, we were all going to cooperate and it was going to be great. And then various bits of data came in, and the data was not super coherent with the best thing that we were hoping for. And so we retreated farther and farther and farther into smaller and smaller scope of what we were taking in kind of narrative addictions. And less and less willingness and ability to listen and bridge. But I mean, that's an abstract story and then we can tell a more mundane story.\n\n<table><tbody><tr><td><p>habrykas: Huh, at the time, it felt like EA was almost only focused on doing things right now</p><p>larissa24joy: From my own experience of something that might be like \"Narrative addiction,\" I feel like I lost some perspective out of fear/responsibility. I felt hugely responsible for EA (more than makes sense on reflection) and I feel like that led me to tunnel vision focus on EA and trying to make that go well.</p><p>habrykas: Like, the movement was primarily about global poverty things and high-standards of evidence</p></td></tr></tbody></table>\n\n5\\. (2:22:34) Conflict causing group insularity\n-----------------------------------------------\n\nGeoff Anders:  \nNo, but that's interesting, because I think the... I've been tracking a different aspect that maybe fits with the narrative... I want to think about the narrative addiction part. I definitely think there's truth to that, so that seems like a good part, and I want to think about it.\n\nThe thing that I've been tracking is the ways that groups become more insular as a result of conflict. Where, so, really simple examples like: Leverage had a lot of information on its website back in 2011. And then we got a bunch of critique, and we weren't getting a lot of positive engagement. And so then we eventually, by 2015 I think we're down to a splash page.\n\nBut then there's something similar. So right now I think one of the potentially really negative, I would imagine unintended, side-effects of the current thing happening right now, is when there's fighting it sort of scorches the earth between organizations. Where it's something like... Yeah, go ahead.  \n  \nAnna Salamon:  \nI guess... sorry. In my opinion... No, no, I had a thing to say, but maybe I should hear the end of your sentence.\n\n<table><tbody><tr><td><p>spiracularity: Someone also appears to have petitioned to get the old Leverage website taken down from Internet Archive, too.</p><p>spiracularity: ...why did you guys feel like you needed to do that, anyway?</p><p>habrykas: My memory of the reasoning (when I asked people about this many years ago) was something like: \"People keep digging up a bunch of stuff I said and keep taking it out of context. Like, that one time when I said 'Connection Theory is a complete theory of psychology', by which I meant 'Connection Theory is trying to be a bold stick-out-your-neck type theory that aims to be complete, but look, it obviously doesn't explain everything, but this is the thing it's aiming for'\"</p><p>habrykas: \"And people kept accusing us of saying wrong things in annoying ways that seemed pretty confused, and it seemed like overall when people looked at the archives, they got more confused than enlightened\"</p><p>habrykas: But someone should feel free to correct me</p><p>larissa24joy: Something like Habryka's account seems right to me but that's based on similar conversations with people about their recollection, not from any knowledge at the time</p><p>larissa24joy: (so people around at the time should totally fill in gaps instead)</p></td></tr></tbody></table>\n\n  \nGeoff Anders:  \nI mean, I think that there are... Basically, I think there's lots of forces that produce insularity. And I think something like continued unresolved-but-still-real conflict is one of them. I'm not sure that that's true, but it's like, if you have like... Let me just add, it'll just take a second. You've got red team and blue team on different sides, right? And each has a negative thing about the other that's not being expressed, but it's being held on to. And I'm not saying it's not - it could both be real. It's not meant to not be real. But I think if that happens, then it's really natural for the things to sort of curl in and not... Yeah, basically become more insular. But yeah, go ahead. You were going to say a thing.\n\n6\\. (2:24:51) Anna on narrative businesses, narrative pyramid schemes, and disagreements\n----------------------------------------------------------------------------------------\n\nAnna Salamon:  \nYeah. I was going to say, I think there's different kinds of... Sorry. A different word that I want to introduce besides narrative addiction is \"narrative pyramid schemes\".  \n  \nGeoff Anders:  \nOkay.  \n  \nAnna Salamon:  \nWhere you get a bunch of people buying into a narrative or something, and this is how you're able to... Sort of a group-level version of something like narrative addiction. But you get a bunch of people buying into a narrative. Because they bought into the narrative, they'll do different things depending on what you say. So they'll work for CFAR, they'll work for Leverage, they'll go take a bunch of actions in the world that you want.  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nBut they'll do it because they're expecting this narrative to pay off in some fashion. And the thing... yeah, I could just say \"narrative business\" or something rather than \"narrative pyramid scheme\". The thing that makes it a narrative pyramid scheme is if it only works insofar as the narrative is able to get other people to buy into the narrative, is able to get other people to buy into narrative, and in this way-  \n  \nGeoff Anders:  \nSure. Well, yeah. I mean-  \n  \nAnna Salamon:  \n\\- whatever, the movement can grow, even though the movement doesn't really have an actual business model that produces value in the sort of... Anyway, you could agree or disagree with that, but I want to connect it back to what you were just saying.  \n  \nGeoff Anders:  \nSure.  \n  \nAnna Salamon:  \nAnd then you can reply. It seems to me that there's lots of different ways that groups A and B could be in conflict. One way we could be in conflict is by, I don't know... well, maybe there aren't. Maybe we just have different research agendas and we're just trying to do our different research agendas. And we just think each other's research agendas is going to be dumb. And so we're most \\[inaudible\\] to do the research agendas. And in the end it'll go to the glory of group A, I mean of group B. Okay, that's a kind of conflict. It's not a very active kind, because we're not really disputing what to do with a resource. But anyway.  \n  \nGeoff Anders:  \nI do think that can be an actually real thing. Different research projects are based on different assumptions about the world. And I think you can-  \n  \nAnna Salamon:  \nYou can have pretty important disagreements that way. I'm not sure whether they're conflicts, exactly.  \n  \nGeoff Anders:  \nThere's going to be some reason why they're not being expressed or being discussed fruitfully, right?  \n  \nAnna Salamon:  \nWell, you could have... in particular, I could imagine a set of people who have really interesting intellectual disagreements while disagreeing about which research are going to be fruitful or whatever, and it's just really fun. I don't know if that's a conflict, it's like conflict...  \n  \nGeoff Anders:  \nOh yeah, no, no. Yeah. I agree, you can have it without the conflicty part. Yeah, basically.  \n  \nAnna Salamon:  \nBut the interesting thing to me is that if you have two groups that are each pulling on their own people via narrative, and if the narrative is in some important sense false - I don't mean it's 100% false, but it's one of those things that could be destroyed by the truth, in some sense...  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nMaybe by pointed, carefully selected truths designed to destroy it, or whatever. But it's not robustly \"just go notice this stuff\". It's, like, a narrative.  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nThen conflict between the two groups I think tends to create dead zones of non-conversation. I don't know, because... sorry, maybe you could just actually flesh this out into a model that I'm not articulating yet.  \n  \n \n\n7\\. (2:27:51) Geoff on narratives, morale, and the epistemic sweet spot\n-----------------------------------------------------------------------\n\nGeoff Anders:  \nNo, that's super interesting. I want to add a piece, though, that I think relates to maybe disagreement, but I think is important, and also I think relates to the rationality/Leverage divide.\n\nI think there's a bunch of dangers associated with narrative. The narrative addiction thing, I think, is real. I want to think about it and understand it better, et cetera, but I've noticed some things that are similar - the thing I've been thinking of is in terms of \"fantasies\" basically, and people getting trapped in them. That's my own language.\n\nBut there's a way in which narratives are really, really valuable. Like, narratives can help people to maintain morale in circumstances that are really bad and really hard. And then I think this is one of the flash points between Leverage and rationality, where I think of it as a very careful navigation. If narratives are misleading and wrong, people become epistemically less good; but I also think that if your morale gets too low, you also become epistemically less good.\n\nAnd so I think the epistemic sweet spot is - the best thing is \"you know how to do things and you can accomplish them, and you can see all the truths and everything works\". But then, as a person's ability to accomplish things, or the quality of their position goes down, then I think frequently narratives are useful, and sometimes necessary, to maintain morale. And that if you graphed narrative versus epistemics, it's not \"epistemics are the best when narratives are the least\". I think that there's frequently - like I said, morale. Morale sometimes helps people to look for paths for hope. You can have a person who just doesn't think about how things could be better. So I think narrative is dangerous, but also useful. And so it has to be used correctly, roughly.\n\n<table><tbody><tr><td><p>spiracularity: You know what else is often surprisingly adaptive to negative environments, then becomes maladaptive but hard to deconstruct once you're in other circumstances? A lot of trauma responses!</p><p>329zwydswr: what's morale, other than a belief that a plan will work?</p><p>spiracularity: (The tone is not intended to be as bad as it sounds, though. I just think they're both STICKY.)</p><p>WaistcoatDave: @spiracularity exactly. and as everyone is human, they come into these situations with preexisting identities'e</p><p>329zwydswr: that seems to conflict with the thing about morale</p><p>WaistcoatDave: identities, experiences and often trauma</p><p>larissa24joy: Can you elaborate @spiracularity ? (I feel like there might be an important thing here but I don't quite grokk it yet)</p></td></tr></tbody></table>\n\n  \n  \n8\\. (2:30:08) Anna on trying to block out things that would weaken the narrative, and on external criticism of Leverage\n-----------------------------------------------------------------------------------------------------------------------------\n\nAnna Salamon:  \nYeah, so I guess the thing I would say is narratives... are great. Like, even something like \"I'm going to put the tea in the microwave and then it'll be warm and I can add a tea bag and then I'll have tea\" sort of a narrative. It's a carefully selected small portion of a thing that I can use as a storyline. Larger-scale narratives are great too...\n\nNarrative *addiction* I would like to distinguish from narratives, and to say that it has to do with the specific motion whereby I'm trying to block out things that would otherwise weaken the narrative.\n\nAnd I am sure there are local benefits that can be obtained by blocking out things that will weaken the narrative. But my current position is that we just shouldn't do that. Like, it *can* help locally, but it creates something like technical debt. Let's not do it.  \n  \nGeoff Anders:  \nI roughly agree. I roughly agree. I mean, I think there can be cases, but roughly I agree with that.  \n  \nAnna Salamon:  \nOkay, I... don't believe you that you agree.\n\nUm. Or, sorry, I'm sorry, I believe - I'm sure you tried to tell the truth and so on. But I would like to bring up things that I think you also think, that I think are in tension with what you just said, and see what you say about it. I mean...  \n  \nGeoff Anders:  \nGreat. Yeah, sure. Yeah, yeah. Great.  \n  \nAnna Salamon:  \nSo, I think... So, look, I - mm. It's hard to say all the things in all the orders at once. I'm going to say a different thing and then I'll \\[inaudible\\], sorry.\n\nSo, once upon a time I heard from a couple junior staff members at CFAR that you were saying bad things to them about me and CFAR.  \n  \nGeoff Anders:  \n\\[I\\] believe it.  \n  \nAnna Salamon:  \nI forget. They weren't particularly false things. So that I don't accidentally \\[inaudible\\]-  \n  \nGeoff Anders:  \nOkay.  \n  \nAnna Salamon:  \nWhatever. Personal view is that it was probably partly silly that I was upset about this. It was not my view at the time; we can talk about it. But anyway, the reason I was upset about it is basically because I did not hold the attitude... I don't know. Sorry, I could say...\n\nI think it was partly that it didn't reach *me* very naturally. I think that was more of a legitimate objection. But, whatever, we can talk about it, I probably am getting the facts of the situation wrong while accidentally maligning you on Twitch TV.  \n  \nGeoff Anders:  \nI'm sorry if I... did an incorrect thing there. So I'll just let you know that-  \n  \nAnna Salamon:  \nWell, let me-  \n  \nGeoff Anders:  \nGo ahead. Go ahead. Go ahead.  \n  \nAnna Salamon:  \nThanks. I appreciate it. I should skip you for the story. Once upon a time there were various past contexts where I would be upset by people seeming to me to weaken CFAR's narratives in its staff. I currently think that that was a mistake on my part. And that it violated this heuristic that I currently think we should have, where you don't use narratives to try to block stuff out.\n\nI think there were also past situations where you, Geoff, were upset at people saying bad things about Leverage in ways that made it harder for your staff members to have, as you would put it, morale. And I'm not sure how that's consistent with the thing you just said about how you basically agree that one shouldn't use narratives to try to block things out.  \n  \nGeoff Anders:  \nWell, do you have cases in mind? That'd be helpful.  \n  \nAnna Salamon:  \nI have only generic cases, but I bet you can fill in the specifics.  \n  \nGeoff Anders:  \nYeah. Try a generic case.  \n  \nAnna Salamon:  \nI think that you're all, like... I don't know, the straw Geoff in my head is all like, \"Man, the rationale of the community was mean to us, because they kept saying bad things about Leverage in ways that made it harder for our staff members to have morale. I wish they would stop it. They kept saying it specifically to staff members and so on, and I don't like it. They shouldn't-\"  \n  \nGeoff Anders:  \nI mean, I *do* think that that's true. I do think that Leverage - this is one thing that I think is a really unappreciated aspect of this whole situation. It's like, a *whole* bunch of Leverage staff just got *tons* of negative stuff from people. And it was, like, as soon as you joined.  \n  \nAnna Salamon:  \nI know, I know. I would be afraid... just to put it out there, I would be afraid to say positive true things about Leverage because then people would get mad at me.  \n  \nGeoff Anders:  \nRight. So I-  \n  \nAnna Salamon:  \nAnd then I would say fewer positive true things about Leverage than I thought, even though... Like, whatever, I've thought various true bad things and various true good things. Well, of course I think my things were true. I thought various good and bad things. But like-  \n  \nGeoff Anders:  \nBut it's super nice to hear you say that. I mean, wow. Okay. All right. Well, good. I feel like we're having a good conversation, at least from my side. Hopefully this will turn out good for you also. Okay. I...\n\n<table><tbody><tr><td><p>AgileCaveman: it was somewhat surprising that some really vocal critics of leverage later applied to join. There was certainly an under-current of envy</p><p>habrykas: @AgileCaveman : Huh, do you have an example?</p><p>habrykas: None come to mind for me, but I am of course not omniscient</p><p>z1zek100: I think there are cases the other way. People wanted to work at Leerage, didn't for whatever reason and then were very upset.</p><p>AgileCaveman: yes, i have a couple people in mind, but i'd rather not name for privacy sake</p><p>habrykas: Yeah, seems sensible</p><p>AgileCaveman: z1zek100 yes, true as well</p><p>spiracularity: To be fair, you're taking Rationalists who love 'ferretting out weird rare elusive dangerous truths,\" and then do stuff like taking down your website. Some of them being envious and then joining doesn't surprise me. Some of them assuming you had skeletons doesn't surprise me, but I agree they could be pretty brutal sometimes.</p></td></tr></tbody></table>\n\n9\\. (2:34:24) More on early Geoff cooperativeness\n-------------------------------------------------\n\nGeoff Anders:  \nDefinitely one thing that was a practice, a part of Leverage culture, at least as I understood it - I'm going to say a pro-Leverage thing, but I'm going to say then a negative thing after that. The pro-Leverage thing is: As I understood it and as I experienced it, we had a practice of trying quite hard to understand the value of different projects as part of assessing potential collaboration. And so it was really important to us, something like \"how much original research was happening at CFAR\". And the more original research, the more it would be like, \"Okay, good,\" because that's something we could conceivably learn from. And also, at least in my... It's just better to have groups that are... If they're research groups doing more research, then that's good.  \n  \nI think that that's the positive thing. I mean, there is just a lot of \"try to say the positives and negatives\", and then a way that this may have... One of the things that's been coming to my attention as I've been thinking about it, is the way that something like culture is not always transmitted. Like, some of the things about narratives, some of the things that have been said I was really surprised to hear that people were using narratives in that way. But it's quite possible that people were using the relevant negatives \\[*sic*\\] \\- something like, trying to paint an overly negative story. But, I mean... Yeah, I don't want to, again, be overly pro my own thing because there's obvious biases and so forth. But it's like, if, I mean... yeah, it's... anyway.  \n  \nAnna Salamon:  \nMaybe best to just display your own thing with obvious biases mixed in, and then anyone can complain about them.  \n  \nGeoff Anders:  \nYeah. I mean, okay. Because invited, and in the name of transparency:\n\nI think we tried hard to cooperate with the other groups. I think we reached out. I think we ran events. I think we did, like, a whole bunch of things. We, like... see, look, now I'm going back into the \"We did many things wrong,\" which I do think is true, but... I think that it just would be nice to not be excluded or cut out of the histories, or et cetera. So it's like, you look at tellings of... Look at the EA Global Wikipedia page and see what you think about whether or not we're all, like, being included and so forth.\n\nAnd so my big update from 2012 was: I had done a whole bunch of reaching out and trying to cause collaboration, and people just weren't interested.\n\nAnd so, I thought about it like: I took a hard lesson. Like, the hard lesson was \"the other people don't want to collaborate.\"\n\nAnd then I had all these theories - like, one of the theories was that it had to do with planning horizons, where it seemed like people were very focused on what would be useful for their organization in the short run. And I know I get critiqued to death for having, like, really long-term plans with all of these parts. One of the things that happens if you have that is you just see ways that other people can fit in.\n\nAnd so it's like, huh, if there's like... I mean, this is part of the EA Summit \"all the groups sort of aid each other in various ways\" thing. And THINK was meant to be a collaboration. So from my - maybe this is biased, but if score it, I just give us *great* marks on trying, and I don't give the other groups great marks on trying.\n\nAnd so, yeah, people can be mad about that; it does seem... Yeah. Okay. Let me just say one other thing. I think this is important.\n\nI think there's definitely a way in which I and other people at Leverage have been beaten down by the constant narrative sort-of opposition.\n\n<table><tbody><tr><td><p>fiddlemath: Anja says: Geoff -- I think the people in the Rationality community who say negative things about Leverage, are/were in an arms race with an intention on Leverage's part to withhold details that could be negatively construed.</p><p>spiracularity: @fiddlemath EXACTLY.</p><p>fiddlemath: yep</p><p>spiracularity: Add an extra layer of \"you guys were not optimizing for outside legibility\", so the straight words didn't really convey things very well.</p></td></tr></tbody></table>\n\nGeoff Anders:  \nAnd part of why I wanted to have a Twitch channel was just to stop doing that. And this is pointing to an area where I think I-  \n  \nAnna Salamon:  \nSorry, stop doing which?  \n  \nGeoff Anders:  \nTo stop, like... The thing I want to say is something like, \"I think we valued collaboration and so forth.\" Whereas, the thing that I *should* somehow be able to say was: We tried to collaborate. We offered help. If other people had wanted to collaborate, it *would* have happened. That it counterfactually, causally depends on the other people's not wanting to collaborate. If you, Anna, had come to me and said, \"Geoff, let's figure out how to do a joint psychology research program and have a bunch of workshops,\" I would have said \"Yes,\" or I would have said \"Yes, if,\" and then worked out some complex thing.\n\nI'm sorry that that's coming across as, like, directive or determinative, or I don't know - the way I'm saying it. But I'm, like, pushing through-  \n  \nAnna Salamon:  \nI appreciate that you're putting your narrative out there in a clearer fashion. It seems nice. It seems good for conversation, to me.  \n  \nGeoff Anders:  \nOkay. Well, it's - yeah. I'm just expecting to get critiqued to death on this because-  \n  \nAnna Salamon:  \nWell, I mean, I think that is one of the things that happens if you say something clear - is people can respond. And that seems good too.  \n  \nGeoff Anders:  \nIt is notable, also: I'm usually *okay* being critiqued to death, or whatever, but, like - Okay. Useful. Useful. There's my narrative. I want updates. I... anyway. Yeah, you know. Okay, go ahead, say something.\n\n<table><tbody><tr><td><p>LuliePlays: One thing I'm finding odd about this conversation about narratives is that there were accusations of demon seances -- I'm curious what @anna_salamon thinks about that stuff, or if it seems irrelevant to CFAR's relationship with Leverage</p><p>329zwydswr: hm. sounds like geoff is describe a sort of progressive escalation of narrative warfare. like a feedback loop of perceiving a slight narrative tilt -&gt; tilt right back, to correct the scales -&gt; other people push back -&gt; repeat</p><p>Turbowandray: +1</p><p>habrykas: My current model is we are talking about Leverage from 2013-2017</p><p>habrykas: Which did not involve any demon seances</p><p>Turbowandray: (to clearer position/etc)</p><p>habrykas: By the time demon seances were happening, I think relationships between CFAR and Leverage had mostly come to a stop</p><p>spiracularity: Maybe your narrative was protecting you from feeling critiqued to death, but had costs elsewhere? (speculative, though)</p><p>DaystarEld: \"<i>One thing I'm finding odd about this conversation about narratives is that there were accusations of demon seances -- I'm curious what @anna_salamon thinks about that stuff, or if it seems irrelevant to CFAR's relationship with Leverage</i>\" +1</p><p>z1zek100: +1</p></td></tr></tbody></table>\n\n  \n  \nAnna Salamon:  \nYeah... Um, sorry, somehow I'm now wanting to read the sentence that has my name in it from the chat.  \n  \nGeoff Anders:  \nYeah, it's really hard to navigate when people are saying interesting things.  \n  \nAnna Salamon:  \nOh, yeah. They want us to talk about different stuff from what we're talking about, but I think I should reply to you first. So I'm going to do that, and then we can go there, or not.\n\nYeah, so... I agree... Like, my perception... I agree that I remember a bunch of concrete things from the early years, like 2012 through 2015 or something - 2014, I don't know - in which Leverage was doing a bunch of things that I would call trying to collaborate. Especially things in the vicinity of \"trying to collaborate on concrete projects that were meant to bring resources to both groups\", or whatever.  \n  \nGeoff Anders:  \nRight. Yeah. Yeah.  \n  \nAnna Salamon:  \nI recall Leverage as being extraordinarily, by local standards, open to and initiative-y in proposing things of this sort.  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nAnd I believe you, that you would have been receptive to such initiative from other groups \\[inaudible\\].  \n  \nGeoff Anders:  \nReally?! Wow! Really? Okay.  \n  \nAnna Salamon:  \nMy brain really thinks I should be adding some sort of \"but\" afterward, and I keep looking to see what the content of the \"but\" is, and my brain's like, \"*But*... I don't like the rearrangement of egos that happens when I say that sentence!\" And I'm like, \"Great, brain. Can you give me something with more content?\" And my brain's like, \"I think there's probably something to say. Didn't Leverage...\"\n\nAnd now I'm going to try it, because there's sort of \"epistemic status: kind-of-like rationalization\".  \n  \n \n\n10\\. (2:41:45) \"Stealing donors\", Leverage's vibe, keeping weird things at arm's length, and \"writing off\" on philosophical grounds\n-----------------------------------------------------------------------------------------------------------------------------------\n\nAnna Salamon:  \nThere *was* that incident at the beginning where everybody was very upset because Leverage was \"trying to steal our donors\" or something - like, going down some sort of donor list.  \n  \nGeoff Anders:  \nYeah-  \n  \nAnna Salamon:  \nWhat was that? I don't remember it.  \n  \nGeoff Anders:  \nYeah, yeah. So it's-  \n  \nAnna Salamon:  \nAnja or somebody brought it upthread, is why I remembered it *at all*.  \n  \nGeoff Anders:  \nYes.  \n  \nAnna Salamon:  \nBut once she did, I was like, \"Yeah, yeah, yeah! That's how we knew Leverage was bad and out to get us!\"\n\nSo, I actually don't think... Sorry. You can reply to that in a second. I'm just going to keep babbling for a second, though.  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nActually I think there's something different, though. I think Leverage came - and by \"Leverage\" in the beginning, I mean you, Geoff - came in with a different pattern of something that I think a lot of people had an immune response to.  \n  \nGeoff Anders:  \nYeah. I agree.  \n  \nAnna Salamon:  \nAnd a different pattern of something - it was partly the particular way that you weren't into a particular kind of materialism. The \"pole through the head\" thing - I can say this more slowly for people to follow it. It was partly that you... I don't know, you told me a story when we first...\n\nSorry. I guess I'm inclined to just say all the things, but I-  \n  \n \n\n<table><tbody><tr><td><p>WaistcoatDave: I think there's a toxic narrative that critiquing is entirely fact based and therefore no time or focus should be put on HOW the points are raised. It's clear the consequences of that has put Leverage staff on edge about entering conversations</p><p>habrykas: Content I remember: \"Leverage reached out to a bunch of people on the top MIRI donor list, and some people were upset about that.\"</p><p>habrykas: I think the people who were upset about that were kind of wrong. But I thought they were right for a while</p><p>sowgone: Cartesians vs Dennettheads</p><p>WaistcoatDave: and ultimately open conversations in the face of that fear and where that's supported by the community, is how things are moved forward healthily.</p></td></tr></tbody></table>\n\n  \n  \nGeoff Anders:  \nI have interjections, but you know, it seems good. The thing I, you know-  \n  \nAnna Salamon:  \nYou told me a story when we first met, I can't even remember how it went, but it led me to think that you sometimes were, like, sort of more towards manic than most people. It was something about high school. You can fill in the details if you want, or ask me to, or we can skip it.  \n  \nGeoff Anders:  \nDefinitely energetic.  \n  \nAnna Salamon:  \nIt wasn't - It wasn't particularly \\[inaudible\\].   \n  \nGeoff Anders:  \nPeople have accused me of being hypomanic at times.  \n  \nAnna Salamon:  \nYeahh... I was like, \"Okay, this guy, I feel like he's...\" Anyway, whatever.\n\nThat, plus the materialism... I think you came in with a thing that a bunch of us - not so much me, although I went along with it-  \n  \nGeoff Anders:  \nCan I?  \n  \nAnna Salamon:  \nBut a bunch of us wanted- Mm-hm?  \n  \nGeoff Anders:  \nWell, so, interestingly - I mean, we're getting to a bunch of things. So, my experience was that *literally within thirty seconds of our meeting*, you had written me off on philosophical grounds because I... I visited SingInst, I got off the plane, I came out, I got picked up by you, and we started talking.\n\nAnd maybe it wasn't thirty seconds. Maybe it was sixty seconds. But literally the first thing that happened was that I was subjected to, in my perspective, a philosophical litmus test. I failed. And you were like, \"Okay, not that one.\"\n\nAnd so that... You know. What do you think about that?  \n  \nAnna Salamon:  \nWhat do I think about that? I feel-  \n  \nGeoff Anders:  \nDo you remember it?  \n  \nAnna Salamon:  \nI don't remember it, but I believe you.  \n  \nGeoff Anders:  \nOkay.  \n  \nAnna Salamon:  \nUm. Yeah. If you want to fill in more details, it might help me remember it. But probably \\[inaudible\\] gotten you from the airport.  \n  \nGeoff Anders:  \nNo, no. It's- No, I don't remember.  \n  \nAnna Salamon:  \nAre you sure it was me, for that matter?  \n  \nGeoff Anders:  \nYeah, absolutely. Yes.  \n  \nAnna Salamon:  \nGreat. So, I don't feel skepticism about it. So I do have feelings about it, despite not remembering it.  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nI remember later on, in what I assume was that same visit, in my head, we were sitting.\n\nBut what year was this? In my head, we were sitting in the house that I was living in in 2011, 2012 in Berkeley.  \n  \nGeoff Anders:  \nI do remember talking in that house a couple times. This was 2011. Yeah, this was in March 2011.\n\n<table><tbody><tr><td>sowgone: I think I (Rob Bensinger) wrote you off immediately on philosophical grounds. (basically 'Geoff is Cartesian + Geoff is' confident')</td></tr></tbody></table>\n\n  \n  \nAnna Salamon:  \nYeah, but Rob Bensinger was not who got you from the airport, even though he's saying that he remembers something similar.  \n  \nGeoff Anders:  \nNo, I don't remember Rob Bensinger being at SingInst at the time.  \n  \nAnna Salamon:  \nRob wasn't even around yet.  \n  \nGeoff Anders:  \nNo, this was this right before Luke Muehlhauser came in as the ED of SingInst, which turned into MIRI.\n\n<table><tbody><tr><td><p>habrykas: I think I docked Geoff a lot of points on philosophical grounds</p><p>DaystarEld: \"<i>By the time demon seances were happening, I think relationships between CFAR and Leverage had mostly come to a stop</i>\" - Ah, this makes sense, but is the point of the conversation then to try to clear the air about Leverage, or to try to help people understand why it diverged from the rest of the community? (I only joined the call in the last 30 minutes)</p><p>habrykas: I still engaged with Leverage a lot though, so definitely not \"writing off\"</p><p>spiracularity: On demons: Look, I think Leverage's internal phrasing around it is toxic BS that is kinda bad for some people. I personally have never had anyone flip-their-shit when I explain social contagion, so feel free to reach out for phrasing-for-normals help if you want.</p><p>habrykas: But I think I also stand by that docking of points</p><p>sowgone: I wasn't involved then, I'm just saying I am a MIRI person who wrote you off later</p><p>habrykas: Oh, that's my docking of points</p><p>habrykas: Yeah, that's what I said too</p><p>spiracularity: (I also think it's bad and panic-inducing in ways that aren't true; analogies with Roko warranted.)</p></td></tr></tbody></table>\n\n  \n  \nAnna Salamon:  \nYeah. Sorry, I'm just going to randomly respond to parts of the chat without having read \\[inaudible\\]-  \n  \nGeoff Anders:  \nFine, fine, all right, well.  \n  \nAnna Salamon:  \nOllie Habryka is all like, \"But I think I stand by that docking of points.\" I haven't even read his context, so I'm just responding. Personally, I stand by docking of points or something? Maybe?\n\nWhat I don't stand by is a \"having written off\". And the reason I don't stand by the \"having written off\" is to my mind, it resembles this thing I'm calling narrative addiction.  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nSo, like, to respond to Geoff's question from a minute ago: Geoff, you were all like, \"How do you feel about it now?\" How I *feel* about it now is I try to turn my mental eyes toward this memory, which is somehow located in my chest - maybe this is too much phenomenological detail-  \n  \nGeoff Anders:  \nYep-  \n  \nAnna Salamon:  \nAnd I feel a cringe. I feel a mental cringe, or something, as I look. And I think the reason for it is that to my mind, there was an element of something like cowardice mixed into my response.  \n  \nGeoff Anders:  \nReally?  \n  \nAnna Salamon:  \nWhere it wasn't just my own authentic response to you seeming to me to be philosophically wrong about a thing. Sorry, and I don't remember the event that you're talking about-  \n  \nGeoff Anders:  \nNo, no, that's fine. That's fine.  \n  \nAnna Salamon:  \nSo I'm thinking of other events that are sort of in the same reference class. But to my mind, it was partly that I didn't want the group to reject me. And that this was sort of part-  \n  \nGeoff Anders:  \nYou mean, *your* group?  \n  \nAnna Salamon:  \n*My* group. I didn't want my group to reject *me*.  \n  \nGeoff Anders:  \nOkay.  \n  \nAnna Salamon:  \nAnd so I was participating in having more of a \"that's weird and I need to keep it at arm's length\" reaction than I would have had in my own person. As opposed to a \"That's interesting! What's with a smart person like you seeming to believe something dumb! Let's have a conversation!\"  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nWhich is an entirely different reaction that doesn't have the dissociating arm's-length this-guy's-a-weirdo component.  \n  \nGeoff Anders:  \nOkay. That's-  \n  \nAnna Salamon:  \nLike, when you say \"written off\", it's more like that, I think \\[inaudible\\]-  \n  \nGeoff Anders:  \nYeah. No, that's super interesting because, I mean... Some of the people in the chats are talking about docking points or writing people off on philosophical grounds. I think there's something like it. I think it can make sense to dock people points. Maybe in some cases writing off? It's, like, a bit extreme, but I could... But there's something like... There's, like, a broader... I'm sort of responding to people in the chat also, but it's like... There's just this large corpus of philosophical works and texts from the history... You know, people are going to be writing me off for dualism. Note, I'm not even a dualist. But it's something like, I think it's usually not correct to see something like a point-sized view or something like that, and then write something off.  \n  \nI also think there can be trends, but it's something like... it would be good to crystallize what exactly is wrong. Like, if you could make a strong argument from... It's like when people say, \"Oh, well, so-and-so believes in God, therefore they're bad intellectually or something.\" And you're like, \"What about Newton?\" And they're like, I don't know, something.\n\nIt's like, if you can draw a strong link between \"believes X\" and \"has various other features\" then, like, sure. But I think that empirically that's frequently not the case.\n\nAnd even if there are general trends, you can recognize when there's something different happening with a person and look into it more. So. Yeah.\n\nBut I appreciate the thing you said. I mean, I, yeah. It's... Why did the group need to write off, like whatever, I mean, we got...  \n  \nAnna Salamon:  \nYeah-  \n  \nGeoff Anders:  \nRobby Beninger said \"Cartesian\", so, like, Cartesianism is really bad. And like, I know dualism was a really big deal. Like... *why*?\n\nI mean, it is cool sort of in a way to have communities differ on the basis of really basic philosophical propositions, because, like, at least aesthetically you're like, \"Oh, you know, there's deep philosophical commitments.\" But I feel like we should be able to go meta and notice them and be like, \"Okay, deep, different philosophical commitments-\"  \n  \nAnna Salamon:  \nWell-  \n  \nGeoff Anders:  \nYeah, go ahead.\n\n11\\. (2:49:42) The value of looking at historical details, and narrative addiction collapse\n-------------------------------------------------------------------------------------------\n\n<table><tbody><tr><td><p>fiddlemath: Anja notes a significant incident at the EA summit, 2014, where Geoff and Anna got into a ... pretty heated? ... argument about dualism. Seemed very emotional to both parties, given the degree of abstraction of the object-level conversation.</p><p>habrykas: Yeah, that was a big deal</p><p>habrykas: It caused me to have nightmares, and was a big component of me distancing myself from Leverage</p><p>habrykas: (Nightmares because I was interning at Leverage at the time, and it made me feel very alienated from my environment)</p><p>habrykas: (And felt like some kind of common ground was pulled out from under me)</p><p>DaystarEld: \"<i>(Nightmares because I was interning at Leverage at the time, and it made me feel very alienated from my environment)</i>\" - /hug</p><p>LuliePlays: \"<i>On demons: Look, I think Leverage's internal phrasing around it is toxic BS that is kinda bad for some people. I personally have never had anyone flip-their-shit when I explain social contagion, so feel free to reach out for phrasing-for-normals help if you want.</i>\" - yeah I love Leverage's psych tech from what I've experienced of it (workshops), it just seems like it would be odd if those weird-sounding things were not explicitly addressed + I'm curious how Anna views the surrounding context of that post</p><p>z1zek100: A bunch of people were obsessed with that conversation for whatever reason. Will McAskill repeatedly cited it as his most definitive critique of Leverage for example.</p><p>z1zek100: I was there and I really never followed the strength of the response.</p><p>habrykas: I think that's kind of fair. I do think the conversation exposed some really big philosophical holes.</p><p>z1zek100: (This is Kerry btw)</p><p>larissa24joy: I'd find it really helpful to understand the emotional charge that seems to be associated with that (dualism) conversation for people?</p><p>habrykas: I think naturalistic reductionism is a really major foundation for my epistemology, and I do think it implicitly supports a lot of other common assumptions, and whenever I did into a bunch of my beliefs and plans, I do think a lot of them are pretty based on naturalistic reductionism</p><p>LuliePlays: and I'm finding myself wondering why they're talking about the past stuff (which I find interesting) when there's questions about the recent stuff and what CFAR's relationship to Leverage is now</p><p>fiddlemath: Anja says she'd like to try to explain...</p><p>habrykas: And I think it's pretty sensible. I do indeed think that an epistemology without naturalistic reductionism feels a lot less grounded, and I feel a lot more scared of having a group of people who \"take ideas seriously\" without being grounded in that way</p><p>larissa24joy: I'd love to hear from Anja on that if she didn't mind and we had time?</p><p>sowgone: I think Cartesianism is kind of a hard-to-escape trap with lots of implications, like a dangerous meme that affects a lot of other stuff. I don't think it's morally bad to be Cartesian (you got trapped!), but I think there are so few Cartesians + it's so bad, that I tend to write people off. Like a smart very devout Christian</p><p>AgileCaveman: imo, people tending to write each other off is a common pattern. I feel like i have been written off because of my theories of social contagion. I also wrote people off because of a rejection of biological substrate</p><p>habrykas: Agree that people trying and tending to write each other off is a common and kind of bad pattern</p><p>fiddlemath: The way I would have known not to reveal myself socially as a cartesian/dualist has to do with the tone and volume of Eliezer's writing on the topic</p><p>sowgone: (BTW as a *separate* thing, I did the 'Leverage is low-status so I want to avoid them', which I don't endorse and is bad.)</p><p>fiddlemath: Plus I remember something in the sequences about epistemic spotchecks being good in particular on philosophical topics</p><p>zwydswr: \"<i>I think naturalistic reductionism is a really major foundation for my epistemology, and I do think it implicitly supports a lot of other common assumptions, and whenever I did into a bunch of my beliefs and plans, I do think a lot of them are pretty based on naturalistic reductionism</i>\" - how is it a foundation for your epistemology?</p><p>z1zek100: @habrykas: That makes sense to me. I think I had a reaction that was something like \"that seems wrong to me, but I don't understand why Geoff thinks it and there's probably an interesting story there at least\" which involved not being very threatened about it in some way. I guess I generally expected people to have that reaction, but it could be that I don't have the same emotional valence on this topic or something.</p></td></tr></tbody></table>\n\nAnna Salamon:  \nYeah. I would also... Sorry, Larissa and the chat, yeah, I'm also pretty interested if anyone on the chat wants to explain about the emotional charge that it had for you personally - or, sorry, not for you personally but for the person in the chat who might explain.\n\nYeah, people are wondering about the current stuff. Lulie, I think I'm talking about the past stuff partly because Geoff mentioned in pre-call wanting to focus on the past stuff and partly because it's genuinely easier.  \n  \nGeoff Anders:  \nOh, we also, the thing is...  \n  \nAnna Salamon:  \nRight?  \n  \nGeoff Anders:  \nWe also - we're at, like - we're at the 1:30 mark. I mean, so we did say we'd go sort of this long. I'm willing to go a little bit longer, but maybe we should shift gears and be like, \"What's the way forward?\" Because - is there a way forward, you know...  \n  \nAnna Salamon:  \nI... I'm... yeah. I think we're all hurtling forward in time. So there's clearly some ways forward-  \n  \nGeoff Anders:  \nLove it.  \n  \nAnna Salamon:  \nI don't know toward where. I mean, personally, I'm really into history and conversation and sharing all the details. And I think once we do that, we'll find a real way forward to somewhere good. And if we don't do that, I don't trust our ability to reason or get anywhere.\n\nI also think sharing all the details is the opposite of narrative addictions. Or, like, it'll help dismantle them. And I think it's in fact compatible with morale or with the good parts of morale, in the long run. I may be wrong.\n\nI could be wrong, but I don't think I'm wrong.  \n  \nGeoff Anders:  \nI think I agree with like 80% or 90% of that or something. It's like, I think there are some things... It's not like everything needs to be hashed out. I think there are problems other than ones addressed by the history. But I do think history is really undervalued, to put it that way. And I've gotten value from looking into some of the things. Yeah, I...  \n  \nAnna Salamon:  \nI mean, I think we should-  \n  \nGeoff Anders:  \nGo ahead.  \n  \nAnna Salamon:  \nYeah. I guess I'm of the hopeful-to-my-mind opinion that many of the things I've been calling \"narrative addictions\" and \"narrative pyramid schemes\" and so on are sort of on the point of collapse, or have already begun to collapse. And that maybe we're heading into a place where we can have real conversations across people, again.  \n  \nGeoff Anders:  \nI'd love that. I think that'd be great.  \n  \nAnna Salamon:  \nTime based more in a grounded thing and less in a \"I'll support your crazy scheme if you support mine\" kind of deal that I don't think it's how we build, in the real world for the actual situation.\n\n12\\. (2:52:23) Geoff wants out of the rationality community; PR and associations; and disruptive narratives\n-----------------------------------------------------------------------------------------------------------\n\nGeoff Anders:  \nYeah... Something like that seems generally good. But I do want to say something like, there's a way that I feel like I'm... You know, I sort of want to say, like, \"How do I get out of the rationality community?\"\n\nLike, I've never considered myself a member. I don't read LessWrong. People are like, \"Did you see the whatever?\" and the answer is like, \"No, no I didn't.\" And I'm running an organization and we're going to do all sorts of other stuff. And so it's not that I don't see value. There is potential value. But I... it's like the whole thing is sort of a nightmare in a certain way.\n\nLike, how many people inside the rationality community - like, sum up, do an integral or something, on their attitudes towards me; and it's like, that doesn't give them a right to actually interact... Like, the fact that someone's mad at me doesn't by itself cause it to make sense for me to engage.\n\nI do think some of the things are my fault. Absolutely. I - theories about various things... Like, sounds like I said some bad stuff about you in the past in a way that you didn't find toward. Like, I...  \n  \nAnna Salamon:  \nWell, but sorry. I was mentioning that partly as an example of ways that I used to be doing a thing that I think was not that great.  \n  \nGeoff Anders:  \nWell-  \n  \nAnna Salamon:  \nI had mixed feelings about it.  \n  \nGeoff Anders:  \nNo, but I guess what I mean to say is, like, you have a claim or a stake. And then maybe the rationality community has some stake.\n\nLike, definitely one thing I've read a little bit about was, seemed like people were doing a memory consolidation sort of event. Like, I think that's great. I would love the rationality community to do more of X, whatever that is.\n\nAnd then I feel like it makes total sense for *you* to want to go into all the details, because you're much more closely tied to the rationality community. You're part of it, a leader, et cetera. I don't know if you would go for leader, but \"leader\", definitely.\n\nHow do I set up my relationship properly so that, like... I don't want people thinking that I'm going to, like... Like, there was one comment somebody directed me to - actually, I think, was it Spiracular? I think Spiracular's here in the chat.\n\nAnyway, there was some comment that was like, the person's like, \"I'm going to make a stand. There's all this fear, and somebody needs to stand up. And so it might as well be me.\" And I'm like, \"Well, that's good.\" You know, I like that. But it's really different than my reality.\n\nAnd so I don't really know how to navigate this sort of circumstance. Do you have any thoughts?\n\n<table><tbody><tr><td><p>spiracularity: If you critique the rationalists, you are one of them. :P</p><p>spiracularity: It's how we roll.</p><p>sowgone: BTW I'd love to have a LW thread/post where we can argue about Cartesian (esp. 'trust first-person stuff a lot') stuff</p><p>AgileCaveman: you used rationality to critique rationality, it's inside of you now. A classic blunder</p><p>sowgone: ('write off' for me doesn't mean 'war' / 'don't talk', it means they're a friend I don't expect to do amazing things)</p><p>benitopace: \"<i>you used rationality to critique rationality, it's inside of you now. A classic blunder</i>\" - hahahaha</p><p>sowgone: :o</p><p>z1zek100: \"<i>you used rationality to critique rationality, it's inside of you now. A classic blunder</i>\" - NotLikeThis</p></td></tr></tbody></table>\n\n  \n  \nAnna Salamon:  \nSorry. So I think you just said, \"How do I get the rationalist to be bored by me, or to otherwise leave me alone?\"  \n  \nGeoff Anders:  \nIt's not \"be bored by me\". It's like - I think Spiracularity just said, \"If you critique the rationalists, you're one of them\"! Yeah. That's a viewpoint. Not one I endorse...  \n  \nAnna Salamon:  \nYeah, to my mind you seem very obviously part of this broader community. Partly because, yes, you critique. Partly because you recruit from. Partly because-  \n  \nGeoff Anders:  \nWell, \"critique or recruit from\" - like, but when? Like, what years? Like, not Leverage more recently, right?  \n  \nAnna Salamon:  \nYeahh.  \n  \nGeoff Anders:  \nAnd like- Yeah.  \n  \nAnna Salamon:  \nI don't have a good solution for you here.  \n \n\n<table><tbody><tr><td><p>DaystarEld: To me \"Why is dualism bad?\" can be answered pretty easily with something like \"It can reliably lead to people doing things like demonic seances.\" I'm not trying to be fighty by saying that, it just seems like a pretty straightforward relationship in my experience/observations of distinguishing philosophies that are more or less likely to go down dangerous paths</p><p>DaystarEld: (I get that \"demons\" can just be a language handle on psychology stuff, but what was described is much more than just a semantic handle)</p><p>z1zek100: \"<i>To me 'Why is dualism bad?' can be answered pretty easily with something like 'It can reliably lead to people doing things like demonic seances.' I'm not trying to be fighty by saying that, it just seems like a pretty straightforward relationship in my experience/observations of distinguishing philosophies that are more or less likely to go down dangerous paths</i>\" - That kind of implies that once you believe in dualism all bets are off or something and I don't quite see why that ought to be the case.</p><p>habrykas: \"<i>I think naturalistic reductionism is a really major foundation for my epistemology, and I do think it implicitly supports a lot of other common assumptions, and whenever I did into a bunch of my beliefs and plans, I do think a lot of them are pretty based on naturalistic reductionism</i>\" - My guess is this would take a bit longer to explain. Short answer is something like \"It's what allows me to have a shared foundation for all the different methods of knowledge gathering that I use, from emotions to scientific experiments to expert deference, and is something like the foundation of how I sanity-check different parts of my map against each other\"</p><p>329zwydswr: how was it different than a semantic handle and how is that related to dualism?</p><p>AgileCaveman: i am SUPER interested in learning more about demonic seanses. I am not at all turned of by the label \"demons.\" I am interested in \"did they work?\"</p><p>habrykas: Also, like, a bunch of your staff was like working at CEA until 2017</p><p>larissa24joy: 2 of the 4 staff (me and Kerry) were at CEA until very early 2019.</p><p>spiracularity: I was making a pun off of the fact that we use the term for both the philosophy stance, and the community, though. (But also the community kinda loves its best critics.)</p><p>habrykas: So at the very least you shouldn't be surprised to be understood as part of the community if you are literally sharing staff with one of the central community organizations</p><p>habrykas: Sorry, meant 2019</p><p>spiracularity: Like, the fact that these are different didn't escape me.</p><p>spiracularity: (end explaining the joke)</p><p>larissa24joy: So if you're ever involved in EA or Rationality you can never leave?</p></td></tr></tbody></table>\n\n  \nGeoff Anders:  \nOkay. Well, I beseech the powers that be inside of rationality - Anna Salamon, Oliver Habryka, others - Robby Bensinger, et cetera, Ben Pace: I'd like some sort of reasonable exit strategy. I think you guys don't, like... I just get *crazy* messages where it's like, people don't want me anywhere around *and* then they don't want me to leave. That's not cool. That's, you know-  \n  \nAnna Salamon:  \nThe same people, or different people?  \n  \nGeoff Anders:  \nI, you know, I'm not even in it enough to know. Okay. And so-  \n  \nAnna Salamon:  \nWell, sorry, did you get a message from some person Bob who's both saying \"stay around\" and \"get out\", or do you get a message from Bob saying \"stay around\" and Carol saying \"get out\" or whatever?  \n  \nGeoff Anders:  \nI think that... I mean, I'm loathe to point to particular individuals...  \n  \nAnna Salamon  \nYou don't have to name them.  \n  \nGeoff Anders:  \n... But yeah, I think I'm getting mixed messages from at least one person! And it's something like... There should just be - I mean, this gets to like a thing that I think is a bigger issue with the rationality community. Like, there's all this stuff about Leverage managing its PR - like, PR matters in the actual world. Like, we want to do projects with people. They don't want to - you know, it diminishes the likelihood of their wanting to be associated with you if-  \n  \nAnna Salamon:  \nYeah, I... have an important disagreement with you here that would take-\n\nGeoff Anders:  \nNo, I agree it would take a while, but something thing like: How do we... I think there's some way that people want, like... I want something a bit more concrete. I want somebody to figure out what I'm supposed to do, and I want to have something I can interact with.\n\n<table><tbody><tr><td><p>sowgone: I'd love to see a Geoff-reply to Anna's <a href=\"https://www.lesswrong.com/posts/SWxnP5LZeJzuT3ccd/pr-is-corrosive-reputation-is-not\">https://www.lesswrong.com/posts/SWxnP5LZeJzuT3ccd/pr-is-corrosive-reputation-is-not</a></p><p>benitopace: \"<i>I'd love to see a Geoff-reply to Anna's </i><a href=\"https://www.lesswrong.com/posts/SWxnP5LZeJzuT3ccd/pr-is-corrosive-reputation-is-not&quot;\"><i>https://www.lesswrong.com/posts/SWxnP5LZeJzuT3ccd/pr-is-corrosive-reputation-is-not</i>\"</a> - +1</p><p>habrykas: As long as you have power over people, they will want you to be held accountable somehow, and want you to play by their norms. And you've had large amounts of power over people for many years.</p><p>z1zek100: Over Rationalists specifically?</p><p>habrykas: And EAs</p><p>z1zek100: That could be true, but it would be a bit surprising</p><p>spiracularity: \"<i>As long as you have power over people, they will want you to be held accountable somehow, and want you to play by their norms. And you've had large amounts of power over people for many years.</i>\" - +1</p><p>habrykas: I mean, Leverage shared staff with CEA for like 4 years, and was intimately involved with almost all EA Global conferences.</p><p>habrykas: So why would it be surprising?</p></td></tr></tbody></table>\n\n  \nGeoff Anders:  \nLook, if it's a mob, if there's no way to stop it, right, like, people, I would appreciate a little bit of clarity on that. We could have you and Habryka and other people just get together and issue an official formal statement that says the rationality community is going to continue doing this until they happen to personally feel satisfied. And then I can let my people I work with know that I've got this rival/enemy that is planning to constantly interact with me and there's really not a way out. And then I can at least be transparent to them about that.  \n  \nAnna Salamon:  \nAh. Yeah, sorry, I feel like there's so many threads and-  \n  \nGeoff Anders:  \nYeah, no, I mean it's - and I don't mean to say that I wouldn't put in time to addressing problems or concerns and so forth. But there's some people for whom the rationality community, like, *is* the world, and, like, that's the thing they care about. And I think that's fine. I think that's, like - people have communities. Communities seem fine.  \n  \nAnna Salamon:  \nYeah, sorry, you're adding more and more to your thing. \\[crosstalk\\]  \n  \nGeoff Anders:  \nBut - Okay, cool. So it's too hard, alright, alright. Well, it seems like there's - so, we didn't manage-  \n  \nAnna Salamon:  \nOh, I'm going to try to reply anyway, I was just \\[crosstalk\\]-  \n  \nGeoff Anders:  \nGo ahead. Sure, alright.  \n  \nAnna Salamon:  \n-how it's too many threads to reply to you and you were like, \"Here's some more threads!\" And I'm like, \"Okay, great.\"  \n  \nGeoff Anders:  \nAlright.  \n  \nAnna Salamon:  \n\"Here my new preface again.\" \"Here's some more threads!\" And I'm like \"Gaahhh!\"  \n  \nGeoff Anders:  \nI'm happy to let you say a thing.  \n  \nAnna Salamon:  \nI don't know. It seems to me like there's a bunch of different things going on. So, personally I feel like Zoe writing that public essay is sort of an invitation for me to take an interest in what happened to that part of Leverage, and an invitation to a bunch of other people.  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nAn invitation from Zoe. Like, otherwise I'd be like, \"Oh, well, that's not exactly my business.\" Like, I don't know whether I would or wouldn't be that way; but I feel like somebody who was in Leverage if 2018, 2019, sort of invited me and a bunch of other onlookers in, in my opinion.  \n  \nGeoff Anders:  \nI agree with that. I accept that.  \n  \nAnna Salamon:  \nSo that's part of it. Then there's a different part that is something like: Everybody has the right to free speech and free thought. Particularly LessWrongers. And I'm not going to be trying to rein them in, and probably neither is anybody. And if they do, I'm probably going to oppose them. And that might get in the way of your PR, as you put it. And I *actually* feel bad about that, but I don't think it's my problem. Or, like, I don't plan to take any interest in it. (Like, I'm *interested*. I get to watch!)  \n  \nGeoff Anders:  \nNo, I think that's fine. The-  \n  \nAnna Salamon:  \nAnd then there's a different thing, which is... Okay, here's my most controversial belief on this topic. Mostly not vis-à-vis you.\n\nSo let's talk about Eliezer for a second. Eliezer has huge communities of people who hang out complaining about him all the time. Not really, that exaggerates slightly - but, like, Sneer Club and so on.  \n  \nGeoff Anders:  \nI was thinking about this. Yeah. How does Eliezer get out if he wants to?  \n  \nAnna Salamon:  \nHe can't! Sorry, he *could*, but he *won't* be able to. Here's my story about why it's happening to Eliezer.  \n  \nGeoff Anders:  \nOkay.  \n  \nAnna Salamon:  \nAs I said, this is my own - I don't know, I haven't put this take out there before. I don't know if it's right.\n\nLike, it seems to me that Eliezer... Let's talk about narratives. Lots and lots of people have personal narratives that are something like, \"I'm going to be a school teacher and then I'll help a bunch of kids and that's good. And that makes me a good person and means I did something useful with my life.\" Or whatever. People have all kinds of, like, little local narratives.\n\nEliezer put these sequences out there, and they're just, like, a *massive* vortex that disrupts a *huge* number of people's local narratives.  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nAnd then some of those people hang out and get really upset.  \n  \nGeoff Anders:  \nYes.  \n  \nAnna Salamon:  \nAnd this is the sense in which... And there's a co-dynamic here... I don't know. Look, I'm probably going to go to various kinds of Said Something Wrong Out Loud On The Internet In Front Of Fifty People Hell for this remark. But, like, it seems to me that Eliezer's narratives... I, sorry, I love and admire Eliezer more than almost - like, he's one of my favorite people in the whole world. But *also*, I imagine when I read the sequences that they've got kind of an edge, and that it's not just a presentation of facts, that there's sort of a reinforcement of his own narrative in there; I could be wrong about this.  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nAnd I think the reinforcement of his own narrative is sort of part of why it erodes other people's narratives so much.  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nAnd it's part of why a bunch of them then try to get vengeance.  \n  \nGeoff Anders:  \nYeah. Yeah, yeah. Totally agree.  \n  \nAnna Salamon:  \nI think there's something a little like that about you and the rationality community that's happening on a somewhat smaller scale. And this is part of what's up with it. Although I think the thing about \"Did Leverage actually harm some people, and were we invited into that room to try to help sort it out and make sure that there's, like-\"  \n  \nGeoff Anders:  \nYeah, yeah. Right.  \n  \nAnna Salamon:  \n\"-restoration of things that those people might want or need,\" is a different and pretty legitimate point.\n\nSo those are the three things that I have.  \n  \nGeoff Anders:  \nOkay. Yeah. Well, so, I agree with the harms and being brought in-  \n  \nAnna Salamon:  \nOh wait, sorry.  \n  \nGeoff Anders:  \nOh, go ahead.  \n  \nAnna Salamon:  \nThere's a fourth thing I really want to say. I'm sorry. Is that okay?  \n  \nGeoff Anders:  \nYeah.  \n  \nAnna Salamon:  \nThe fourth thing is: Rationality was *never* trying to be an organization in the way that Leverage is trying to be an organization. At least, I don't think so. Like, I think if I had been like, \"Hey, Geoff, your Leverage is messing up my life. How do I get out?\" You would have tried to be the responsible person at the front who could have had an answer for me.  \n  \nGeoff Anders:  \nRight.  \n  \nAnna Salamon:  \nRationality is really not trying to be the kind of thing that can have a responsible person at the front who has an answer for people.  \n \n\n<table><tbody><tr><td><p>DaystarEld: \"<i>how was it different than a semantic handle and how is that related to dualism?</i>\" - From my understanding of Zoe's post... if people treat semantic handles for psychological phenomenon as real enough to be affected by using crystals, it starts to leave the realm of semantic handles and enter the realm of irrationality</p><p>sowgone: motte: 'Geoff, stop doing abusive actions X, Y, Z.' bailey: 'Geoff, stop being weird. And given that you've ever been weird, stop trying to improve the world.' ?</p><p>329zwydswr: \"<i>It's what allows me to have a shared foundation for all the different methods of knowledge gathering that I use</i>\" - ah ok this makes sense. hard to discuss here, but i want to note that this i think is actually compatible with some kinds of dualism. eg saying \"beliefs aren't material things\" (or maybe easier to agree on would be, \"algorithms aren't material things\") is compatible with \"everything that happens, happens in accord with physical law\"</p><p>habrykas: \"<i>ah ok this makes sense. hard to discuss here, but i want to note that this i think is actually compatible with some kinds of dualism. eg saying 'beliefs aren't material things' (or maybe easier to agree on would be, 'algorithms aren't material things') is compatible with 'everything that happens, happens in accord with physical law'</i>\" - Yeah, agree, though with many many caveats. I have a tiny steelman in my mind that's trying to map between Tegmark 4, Platonic Realism and Cartesianism. Though I've never gotten it to actually fully typecheck and make work.</p><p>329zwydswr: \"<i>as real enough to be affected by using crystals.</i>\" - well... this is clearly a mistake, but does this really argue against they (the psychological phenomena) being really real?</p><p>329zwydswr: like, is talking about \"having beliefs\" a form of dualism? seems fairly dualistic, in that beliefs aren't material things. doesn't have to be irrational / nonsensical / crazy</p><p>DaystarEld: \"<i>well... this is clearly a mistake, but does this really argue against they (the psychological phenomena) being really real?</i>\" - I think there's a certain type of thinking that *leads* to those mistakes, and we should know that by now</p><p>329zwydswr: \"<i>Though I've never gotten it to actually fully typecheck and make work.</i>\" - this problematic seems to be something in the air, there's maybe an idea here who's time is coming.</p><p>AgileCaveman: One thing that's somewhat frustrating about the question of \"did leverage harm people\" is the misunderstanding of \"harm base rates\". I have a couple friends severly harmed by rationality or working at normie tech companies. there is a high baseline of people being harmed in the world and the question of whether Leverage is above this baseline is harder than \"did it harm people\"</p><p>habrykas: Agree with this. I think establishing baselines is super important.</p><p>329zwydswr: \"<i>a certain type of thinking agreed</i>\" - i'm curious wat it is, and it doesn't seem like dualism fits, but maybe i'm missing the point.</p><p>DaystarEld: &nbsp;\"<i>One thing that's somewhat frustrating about the question of \"did leverage harm people\" is the misunderstanding of 'harm base rates'. I have a couple friends severly harmed by rationality or working at normie tech companies. there is a high baseline of people being harmed in the world and the question of whether Leverage is above this baseline is harder than 'did it harm peopl'e\"</i> - Agree with baselines too. But one of the reasons (for example, sorry to use the C word) cults are bad is that they can do targeted harm in excess to baseline organizations to a few people beyond what a randomly selected control group might</p><p>DaystarEld: Even as most people have positive or neutral experiences</p></td></tr></tbody></table>\n\n  \nGeoff Anders:  \nOkay. I've got some thoughts. We still have viewers, so we're still going.\n\nSo, first I should say, I do think that there is a legitimate public interest in the things that Zoe said. I think that those be pursued. I think the people that want to pursue them are doing the right thing, or doing an acceptable thing. And so I'm not trying to push back on that at all.\n\nOn the \"Eliezer created the narrative and is still reinforcing it, and then the people are stuck inside it and it erodes their narratives thing,\" if there's a thing that's similar that I'm doing, I want to find it and stop. I hereby offer that I will spend time doing that. And I would love to have one or more interlocutors who could help to try to crystallize what it is, so as to make this process more effective. I don't know if Eliezer believes that something like that's happening. I find it plausible that something like this is happening, even though the interactions with me are obviously less thoroughgoing than with Eliezer from the rationality community or from rationalists.\n\nSo, but I hereby offer to try to resolve that. And you know, I'm not sure that it's there, but if it's... I'll look, I'll do this myself. I don't need other - and I would like help from relevant other people who maybe have a bit more insight into how people are thinking about it, because one of the things that's harder for me is I don't in fact track the hive-mind of LessWrong, if it is a hive mind, or the non-hive-mind of many, many individuals who... something, like, I don't know.\n\nBut, yeah, so that sounds good. And then - you said a fourth thing that seemed super important. What was the fourth thing?  \n  \nAnna Salamon:  \nNot trying to be that kind of thing, that Leverage I think was trying to be, where there's somebody at the front who can take calls.  \n  \nGeoff Anders:  \nAh, so here's-  \n  \nAnna Salamon:  \nCFAR maybe is, LessWrong maybe is, but, like, the rationality community, no.  \n  \nGeoff Anders:  \nWell, but this is where I think we have a disagreement, but one that's not like a deep philosophical disagreement about Cartesianism or something like that. It's a disagreement about social technology.\n\nI think that the correct form for communities involves community leaders and involves walls of particular types. I think that that's actually from the nature of a community. I think communities - I'm not sure about this, but I think communities come together over shared strengths and shared weaknesses. The community offers an opportunity for the people to, like, collect and grow the shared strengths, and to help protect the people from the shared weaknesses.\n\nAnd then I think that that's - like, the reason you need walls is because there is the shared weakness and the walls help to protect the community. And then, in terms of leaders, I think that communities, because there are certain types of shared weaknesses and blind spots, they're really subject to predators.  \n  \nAnd so I think something like \"Communities need to have leaders,\" and the challenge of the leader is to do their best to try to handle the predators.\n\nBut this is a claim about the nature of communities. And as a plug for my Twitch stream, I'm going to be talking about coordination research, which includes looking at different forms of social technologies, including communities. So I'm going to be exploring this.\n\nBut I think that there's actually, like, a positive claim here, not a normative claim, not a philosophical-whatever claim, there's a claim about a particular social form and its correct thing. And so maybe there's a fruitful line of discussion to have there.  \n  \nAnna Salamon:  \nYeah. I'm pretty into this conversation.  \n  \nGeoff Anders:  \nGreat. All right. Well, I feel like we had a really natural breaking point. All right. It took a little bit longer, and I apologize to everybody for the technical difficulties. Those will get better.\n\nOkay. I think this was fine, right? All right. Good. Okay! Awesome. Thanks, everybody.\n\nI will see... Maybe people will be less interested in my philosophical whatever. I don't know how we're gonna do better. This was good. Anyway, whatever. \\[crosstalk\\] I'll send out a tweet. All right. I'll-  \n  \nAnna Salamon:  \nI appreciate you chatting.  \n  \nGeoff Anders:  \nGreat. All right. I'll see you later-  \n  \nAnna Salamon:  \nAnd all the people in the text.\n\n<table><tbody><tr><td><p>z1zek100: This seemed good</p><p>sowgone: this conversation was amazing, thank you</p><p>z1zek100: Love you both</p><p>329zwydswr: thanks</p><p>habrykas: I appreciate this conversation happening.</p><p>larissa24joy: Thank you for doing this Anna and Geoff!</p><p>LuliePlays: Thanks Geoff and everyone!</p><p>Turbowandray: bye</p><p>habrykas: Sorry for not being able to fix the audio things</p><p>linacalabria: +1</p><p>Turbowandray: ty!</p><p>WaistcoatDave: see you a</p><p>z1zek100: &lt;3</p><p>DaystarEld: Thanks everyone</p><p>WaistcoatDave: see you all</p></td></tr></tbody></table>\n\n  \n  \nGeoff Anders:  \nAll right-  \n  \nAnna Salamon:  \nBye.  \n  \nGeoff Anders:  \nBye everybody.",
      "plaintextDescription": "Geoff Anders of Leverage and Anna Salamon of CFAR had a conversation on Geoff's Twitch channel on October 23, triggered by Zoe Curzi's post about having horrible experiences at Leverage.\n\nTechnical issues meant that the Twitch video was mostly lost, and the audio only resurfaced a few days ago, thanks to Lulie.\n\nI thought the conversation was pretty important, so I'm (a) using this post the signal-boost the audio, for people who missed it; and (b) posting a transcript here, along with the (previously unavailable, and important for making sense of the audio) last two hours of chat log.\n\nYou can find the full audio here, and video of the first few minutes here. The full audio discusses a bunch of stuff about Leverage's history; the only part of the stream transcribed below is the Anna/Geoff conversation, which starts about two hours in.\n\nThe chat messages I have aren't timestamped, and sometimes consist of side-conversations that don't directly connect with what Geoff and Anna are discussing. So I've inserted blocks of chat log at what seemed like roughly the right parts of the conversation; but let me know if any are in the wrong place for comprehension, and I'll move them elsewhere.\n\n----------------------------------------\n\n\n1. (1:57:57) Early EA history and EA/Leverage interactions\n[...]\n\nAnna Salamon:\nAll right, let's talk.\n\nGeoff Anders:\nAll right, all right. Yeah, let's get to it.\n\nAnna Salamon:\nI hear that the rationality and Leverage communities have some sort of history!\n\nGeoff Anders:\nThat's right. Okay, so basically, Anna, as I mentioned to you when we chatted about this, I wanted to talk about in general Leverage history, and then there's a long history with the rationality community, and right now, it seems like there's... I'd like to understand what happened with the relations, basically.\n\nLike, earlier on, things were substantially more congenial. More communication. There were various types of joint events. Ben, who suggested he would be called by fir",
      "wordCount": 17527
    },
    "tags": [
      {
        "_id": "sYbszETv5rKst6gxD",
        "name": "Leverage Research",
        "slug": "leverage-research"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "JTekFwz3rWjsirBbr",
    "title": "2020 PhilPapers Survey Results",
    "slug": "2020-philpapers-survey-results",
    "url": null,
    "baseScore": 58,
    "voteCount": 28,
    "viewCount": null,
    "commentCount": 50,
    "createdAt": null,
    "postedAt": "2021-11-02T05:00:13.859Z",
    "contents": {
      "markdown": "In 2009, David Bourget and David Chalmers ran the PhilPapers Survey ([results](https://philpapers.org/surveys/results.pl?affil=Target+faculty&areas0=0&areas_max=1&grain=medium), [paper](https://philpapers.org/archive/BOUWDP.pdf)), sending questions to \"all regular faculty members\" at top \"Ph.D.-granting \\[philosophy\\] departments in English-speaking countries\" plus ten other philosophy departments deemed to have \"strength in analytic philosophy comparable to the other 89 departments\".\n\nBourget and Chalmers now have a new PhilPapers Survey out, run in 2020 ([results](https://survey2020.philpeople.org/survey/results/all), [paper](https://philarchive.org/archive/BOUPOP-3)). I'll use this post to pick out some findings I found interesting, and say opinionated stuff about them. Keep in mind that I'm focusing on topics and results that dovetail with things I'm curious about (e.g., 'why do academic decision theorists and LW decision theorists disagree so much?'), not giving a neutral overview of the whole 100-question survey.\n\nThe new survey's target population consists of:\n\n> (1) in Australia, Canada, Ireland, New Zealand, the UK, and the US: all regular faculty members (tenuretrack or permanent) in BA-granting philosophy departments with four or more members (according to the PhilPeople database); and (2) in all other countries: English-publishing philosophers in BA-granting philosophy departments with four or more English-publishing faculty members.\n\nIn order to make comparisons to the 2009 results, the 2020 survey also looked at a \"2009-comparable departments\" list selected using similar criteria to the 2009 survey:\n\n> It should be noted that the “2009-comparable department” group differs systematically from the broader target population in a number of respects. Demographically, it includes a higher proportion of UK-based philosophers and analytic-tradition philosophers than the target population. Philosophically, it includes a lower proportion of theists, along with many other differences evident in comparing 2020 results in table 1 (all departments) to table 9 (2009-comparable departments).\n\nBased on this description, I expect the \"2009-comparable departments\" in the 2020 survey to be more elite, influential, and reasonable than the 2020 \"target group\", so I mostly focus on 2009-comparable departments below. In the tables below, if the row doesn't say \"*Target*\" (i.e., target group), the population is \"2009-comparable departments\".\n\nNote that in the 2020 survey (unlike 2009), respondents could endorse multiple answers.\n\n1\\. Decision theory\n-------------------\n\n[Newcomb's problem](https://survey2020.philpeople.org/survey/results/4886): The following groups (with *n* noting their size, and skipping people who skipped the question or said they weren't sufficiently familiar with it) endorsed the following options in the 2020 survey:\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i></td><td><strong>one box</strong>&nbsp;</td><td><strong>two boxes</strong></td><td><strong>diff</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1071</td><td>31%</td><td>39%</td><td>8%</td></tr><tr><td>Decision theorists (<i>Target</i>)</td><td>48</td><td>21%</td><td>73%</td><td>58%</td></tr><tr><td>Philosophers</td><td>470</td><td>28%</td><td>43%</td><td>15%</td></tr><tr><td>Decision theorists</td><td>22</td><td>23%</td><td>73%</td><td>50%</td></tr></tbody></table>\n\n5% of decision theorists said they \"accept a combination of views\", and 9% said they were \"agnostic/undecided\".\n\nI think decision theorists are astonishingly wrong here, so I was curious to see if other philosophy fields did better.\n\nI looked at *every* field where enough surveyed people gave their views on Newcomb's problem. Here they are in order of 'how much more likely are they to two-box than to one-box':\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i></td><td><strong>one box</strong>&nbsp;</td><td><strong>two boxes</strong></td><td><strong>diff</strong></td></tr><tr><td>Philosophers of gender, race, and sexuality</td><td>13</td><td>23%</td><td>54%</td><td>31%</td></tr><tr><td>20th-century-philosophy specialists</td><td>23</td><td>13%</td><td>43%</td><td>30%</td></tr><tr><td>Social and political philosophers</td><td>62</td><td>19%</td><td>48%</td><td>29%</td></tr><tr><td>Philosophers of law</td><td>14</td><td>14%</td><td>43%</td><td>29%</td></tr><tr><td>Phil. of computing and information (<i>Target</i>)</td><td>18</td><td>22%</td><td>50%</td><td>28%</td></tr><tr><td>Philosophers of social science</td><td>24</td><td>33%</td><td>58%</td><td>25%</td></tr><tr><td>Philosophers of biology</td><td>20</td><td>30%</td><td>55%</td><td>25%</td></tr><tr><td>General philosophers of science</td><td>53</td><td>26%</td><td>51%</td><td>25%</td></tr><tr><td>Philosophers of language</td><td>100</td><td>25%</td><td>47%</td><td>22%</td></tr><tr><td>Philosophers of mind</td><td>96</td><td>25%</td><td>45%</td><td>20%</td></tr><tr><td>Philosophers of computing and information</td><td>5</td><td>20%</td><td>40%</td><td>20%</td></tr><tr><td>19th-century-philosophy specialists</td><td>10</td><td>10%</td><td>30%</td><td>20%</td></tr><tr><td>Philosophers of action</td><td>32</td><td>28%</td><td>47%</td><td>19%</td></tr><tr><td>Logic and philosophy of logic</td><td>58</td><td>26%</td><td>43%</td><td>17%</td></tr><tr><td>Philosophers of physical science</td><td>25</td><td>28%</td><td>44%</td><td>16%</td></tr><tr><td>Epistemologists</td><td>129</td><td>32%</td><td>47%</td><td>15%</td></tr><tr><td>Meta-ethicists</td><td>72</td><td>29%</td><td>43%</td><td>14%</td></tr><tr><td>Metaphysicians</td><td>123</td><td>32%</td><td>44%</td><td>12%</td></tr><tr><td>Normative ethicists</td><td>102</td><td>32%</td><td>42%</td><td>10%</td></tr><tr><td>Philosophers of religion</td><td>15</td><td>33%</td><td>40%</td><td>7%</td></tr><tr><td>Philosophers of mathematics</td><td>19</td><td>32%</td><td>37%</td><td>5%</td></tr><tr><td>17th/18th-century-philosophy specialists</td><td>39</td><td>31%</td><td>36%</td><td>5%</td></tr><tr><td>Metaphilosophers</td><td>23</td><td>35%</td><td>39%</td><td>4%</td></tr><tr><td>Applied ethicists</td><td>50</td><td>34%</td><td>38%</td><td>4%</td></tr><tr><td>Philosophers of cognitive science</td><td>50</td><td>32%</td><td>36%</td><td>4%</td></tr><tr><td>Aestheticians</td><td>14</td><td>29%</td><td>21%</td><td>-8%</td></tr><tr><td>Greek and Roman philosophy specialists</td><td>17</td><td>41%</td><td>29%</td><td>-12%</td></tr></tbody></table>\n\n(Note that many of these groups are small-*n*. Since philosophers of computing and information were an especially small and weird group, and I expect LWers to be extra interested in this group, I also looked at the target-group version for this field.)\n\n*Every* field did much better than decision theory (by the \"getting more utility in Newcomb's problem\" metric). However, the only fields that favored one-boxing over two-boxing was ancient Greek and Roman philosophy, and aesthetics.\n\nAfter those two fields, the best fields were philosophy of cognitive science, applied ethics, metaphilosophy, philosophy of mathematics, and 17th/18th century philosophy (only 4-5% more likely to two-box than one-box), followed by philosophy of religion, normative ethics, and metaphysics.\n\nMy quick post-hoc, low-confidence guess about why these fields did relatively well is (hiding behind a spoiler tag so others can make their own unanchored guesses):\n\n My inclination is to model the aestheticians, historians of philosophy, philosophers of religion, and applied ethicists as 'in-between' analytic philosophers and the general public ([who one-box more often than they two-box, unlike analytic philosophers](https://www.theguardian.com/science/alexs-adventures-in-numberland/2016/nov/30/newcombs-problem-which-side-won-the-guardians-philosophy-poll)). I think of specialists in those fields as relatively normal people, who have had less exposure to analytic-philosophy culture and ideas and whose views therefore tend to more closely resemble the views of some person on the street.\n\nThis would also explain why the \"2009-comparable departments\", who I expected to be more elite and analytic-philosophy-ish, did so much worse than the \"target group\" here.\n\nI would have guessed, however, that philosophers of gender/race/sexuality would also have done relatively well on Newcomb's problem, if 'analytic-philosophy-ness' were the driving factor.\n\nI'm pretty confused about this, though the small *n* for some of these populations means that a lot of this could be pretty random. (E.g., network effects: a single just-for-fun faculty email thread about Newcomb's problem could convince a bunch of  philosophers of sexuality that two-boxing is great. Then this would show up in the survey because very few philosophers of sexuality have ever even heard of Newcomb's problem, and the ones who haven't heard of it aren't included.)\n\nAt the same time, my inclination is to treat philosophers of cognitive science, mathematics, normative ethics, metaphysics, and metaphilosophy as 'heavily embedded in analytic philosophy land, but smart enough (/ healthy enough as a field) to see through the bad arguments for two-boxing to some extent'.\n\nThere's also a question of why cognitive science would help philosophers do better on Newcomb's problem, when computer science doesn't. I wonder if the *kinds of debates* that are popular in computer science are the sort that attract people with bad epistemics? ('Wow, the Chinese room argument is amazing, I want to work in this field!') I really have no idea, and wouldn't have predicted this in advance.\n\nNormative ethics also surprises me here. And both of my explanations for 'why did field X do well?' are post-hoc, and based on my prior sense that some of these fields are much smarter and more reasonable than others.\n\nIt's very plausible that there's *some* difference between the factors that make aestheticians one-box more, and the factors that make philosophers of cognitive science one-box more. To be confident in my particular explanations, however, we'd want to run various tests and look at various other comparisons between the groups.\n\nThe fields that did the worst after decision theory were philosophy of gender/race/sexuality, 20th-century philosophy, philosophy of language, philosophy of law, political philosophy, and philosophy of biology, of social science, and of science-in-general.\n\nA separate question is whether academic decision theory has gotten better since the 2009 survey. Eyeballing the (small-*n*) numbers, the answer is that it seems to have gotten worse: two-boxing became even more popular (in 2009-comparable departments), and one-boxing even less popular:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1882ba7d812315b9e7dd498ae3a98ee4375b0542c5010a4.png)\n\n*n*=31 for the 2009 side of the comparison, *n*=22 for the 2020 side. The numbers above are different from the ones I originally presented because Bourget and Chalmers include \"skip\" and \"insufficiently familiar\" answers, and exclude responses that chose multiple options, in order to make the methodology more closely match that of the 2009 survey.\n\n2\\. (Non-animal) ethics\n-----------------------\n\nRegarding \"[Meta-ethics](https://survey2020.philpeople.org/survey/results/4866): moral realism or moral anti-realism?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>moral realism</strong> &nbsp;</td><td><strong>moral anti-realism</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1719</td><td>62%</td><td>26%</td></tr><tr><td>Philosophers</td><td>630</td><td>62%</td><td>27%</td></tr><tr><td>Applied ethicists</td><td>64</td><td>67%</td><td>23%</td></tr><tr><td>Normative ethicists</td><td>132</td><td>74%</td><td>16%</td></tr><tr><td>Meta-ethicists</td><td>94</td><td>68%</td><td>22%</td></tr></tbody></table>\n\nRegarding \"[Moral judgment](https://survey2020.philpeople.org/survey/results/4882): non-cognitivism or cognitivism?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>cognitivism</strong> &nbsp;</td><td><strong>non-cognitivism</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1636</td><td>69%</td><td>21%</td></tr><tr><td>Philosophers</td><td>594</td><td>70%</td><td>20%</td></tr><tr><td>Applied ethicists</td><td>62</td><td>76%</td><td>23%</td></tr><tr><td>Normative ethicists</td><td>132</td><td>82%</td><td>14%</td></tr><tr><td>Meta-ethicists</td><td>93</td><td>76%</td><td>16%</td></tr></tbody></table>\n\nRegarding \"[Morality](https://survey2020.philpeople.org/survey/results/5078): expressivism, naturalist realism, constructivism, error theory, or non-naturalism?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp;</strong></td><td><strong>non-nat</strong></td><td><strong>nat realism</strong></td><td><strong>construct</strong></td><td><strong>express</strong></td><td><strong>error</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1024</td><td>27%</td><td>32%</td><td>21%</td><td>11%</td><td>5%</td></tr><tr><td>Philosophers</td><td>386</td><td>25%</td><td>33%</td><td>19%</td><td>10%</td><td>5%</td></tr><tr><td>Applied ethicists</td><td>40</td><td>20%</td><td>35%</td><td>38%</td><td>5%</td><td>0%</td></tr><tr><td>Normative ethicists</td><td>90</td><td>34%</td><td>36%</td><td>21%</td><td>8%</td><td>1%</td></tr><tr><td>Meta-ethicists</td><td>68</td><td>37%</td><td>29%</td><td>18%</td><td>13%</td><td>7%</td></tr></tbody></table>\n\nRegarding \"[Normative ethics](https://survey2020.philpeople.org/survey/results/4890): virtue ethics, consequentialism, or deontology?\" (putting in parentheses the percentage that *only* chose the option in question):\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td>deontology&nbsp;</td><td>consequentialism</td><td>virtue ethics</td><td>combination</td></tr><tr><td>All (<i>Target</i>)</td><td>1741</td><td>32% (20%)</td><td>31% (21%)</td><td>37% (25%)</td><td>16%</td></tr><tr><td>All</td><td>631</td><td>37% (23%)</td><td>32% (22%)</td><td>31% (19%)</td><td>17%</td></tr><tr><td>Applied...</td><td>63</td><td>56% (33%)</td><td>38% (22%)</td><td>33% (8%)</td><td>27%</td></tr><tr><td>Normative...</td><td>132</td><td>46% (27%)</td><td>32% (20%)</td><td>36% (17%)</td><td>25%</td></tr><tr><td>Meta...</td><td>94</td><td>44% (28%)</td><td>32% (22%)</td><td>24% (13%)</td><td>19%</td></tr></tbody></table>\n\nExcluding responses that endorsed multiple options, we can see that normative ethicists have moved away from deontology and towards virtue ethics since 2009, though deontology is still the most popular:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/513147020cd5833a75c302dcddf299d0269f1c2ff0bf190b.png)\n\n30 normative-ethicist respondents also wrote in \"pluralism\" or \"pluralist\" in the 2020 survey.\n\nRegarding \"[Trolley problem](https://survey2020.philpeople.org/survey/results/4922) (five straight ahead, one on side track, turn requires switching, what ought one do?): don't switch or switch?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>switch</strong> &nbsp;</td><td><strong>don't switch</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1736</td><td>63%</td><td>13%</td></tr><tr><td>Philosophers</td><td>635</td><td>68%</td><td>12%</td></tr><tr><td>Applied ethicists</td><td>63</td><td>71%</td><td>13%</td></tr><tr><td>Normative ethicists</td><td>132</td><td>70%</td><td>13%</td></tr><tr><td>Meta-ethicists</td><td>94</td><td>70%</td><td>13%</td></tr></tbody></table>\n\nRegarding \"[Footbridge](https://survey2020.philpeople.org/survey/results/4946) (pushing man off bridge will save five on track below, what ought one do?): push or don't push?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>push</strong> &nbsp;</td><td><strong>don't push</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1740</td><td>22%</td><td>56%</td></tr><tr><td>Philosophers</td><td>636</td><td>23%</td><td>58%</td></tr><tr><td>Applied ethicists</td><td>63</td><td>24%</td><td>63%</td></tr><tr><td>Normative ethicists</td><td>132</td><td>17%</td><td>70%</td></tr><tr><td>Meta-ethicists</td><td>94</td><td>18%</td><td>66%</td></tr></tbody></table>\n\nRegarding \"[Human genetic engineering](https://survey2020.philpeople.org/survey/results/5046): permissible or impermissible?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>permissible</strong></td><td><strong>impermissible</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1059</td><td>62%</td><td>19%</td></tr><tr><td>Philosophers</td><td>379</td><td>68%</td><td>13%</td></tr><tr><td>Applied ethicists</td><td>39</td><td>85%</td><td>8%</td></tr><tr><td>Normative ethicists</td><td>83</td><td>69%</td><td>12%</td></tr><tr><td>Meta-ethicists</td><td>59</td><td>68%</td><td>12%</td></tr></tbody></table>\n\nRegarding \"[Well-being](https://survey2020.philpeople.org/survey/results/5206): hedonism/experientialism, desire satisfaction, or objective list?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>hedonism</strong></td><td><strong>desire satisfaction</strong></td><td><strong>objective list</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>967</td><td>10%</td><td>19%</td><td>53%</td></tr><tr><td>Philosophers</td><td>348</td><td>9%</td><td>19%</td><td>54%</td></tr><tr><td>Applied ethicists</td><td>43</td><td>16%</td><td>26%</td><td>56%</td></tr><tr><td>Normative ethicists</td><td>90</td><td>11%</td><td>21%</td><td>63%</td></tr><tr><td>Meta-ethicists</td><td>62</td><td>11%</td><td>24%</td><td>55%</td></tr></tbody></table>\n\n[Moral internalism](https://plato.stanford.edu/entries/moral-motivation/#IntVExt) \"holds that a person cannot sincerely make a moral judgment without being *motivated* at least to some degree to abide by her judgment\". Regarding \"[Moral motivation](https://survey2020.philpeople.org/survey/results/4878): externalism or internalism?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp;&nbsp;</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>internalism</strong> &nbsp;</td><td><strong>externalism</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1429</td><td>41%</td><td>39%</td></tr><tr><td>Philosophers</td><td>528</td><td>38%</td><td>42%</td></tr><tr><td>Applied ethicists</td><td>57</td><td>53%</td><td>37%</td></tr><tr><td>Normative ethicists</td><td>128</td><td>34%</td><td>51%</td></tr><tr><td>Meta-ethicists</td><td>92</td><td>35%</td><td>47%</td></tr></tbody></table>\n\nOne of the largest changes in philosophers' views since the 2009 survey is that philosophers have somewhat shifted toward externalism. In 2009, internalism was 5% more popular than externalism; now externalism is 3% more popular than internalism.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/592070a869c6d4209ba58f3fe05c1fa621c189137689387a.png)\n\n(Again, the 2009-2020 comparisons give different numbers for 2020 in order to make the two surveys' methodologies more similar.)\n\n3\\. Minds and animal ethics\n---------------------------\n\nRegarding \"[Hard problem of consciousness](https://survey2020.philpeople.org/survey/results/5042) (is there one?): no or yes?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong></td><td><strong>yes</strong></td><td><strong>no</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1468</td><td>62%</td><td>30%</td></tr><tr><td>Philosophers of computing and information (<i>Target</i>)</td><td>18</td><td>44%</td><td>50%</td></tr><tr><td>Philosophers</td><td>366</td><td>58%</td><td>33%</td></tr><tr><td>Philosophers of cognitive science</td><td>40</td><td>48%</td><td>50%</td></tr><tr><td>Philosophers of mind</td><td>91</td><td>62%</td><td>34%</td></tr></tbody></table>\n\nRegarding \"[Mind](https://survey2020.philpeople.org/survey/results/4874): non-physicalism or physicalism?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong></td><td><strong>physicalism</strong></td><td><strong>non-physicalism</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1733</td><td>52%</td><td>32%</td></tr><tr><td>Phil of computing and information (<i>Target</i>)</td><td>25</td><td>64%</td><td>24%</td></tr><tr><td>Philosophers</td><td>630</td><td>59%</td><td>27%</td></tr><tr><td>Philosophers of cognitive science</td><td>73</td><td>78%</td><td>12%</td></tr><tr><td>Philosophers of computing and information</td><td>5</td><td>60%</td><td>20%</td></tr><tr><td>Philosophers of mind</td><td>135</td><td>60%</td><td>26%</td></tr></tbody></table>\n\nRegarding \"[Consciousness](https://survey2020.philpeople.org/survey/results/5010): identity theory, panpsychism, eliminativism, dualism, or functionalism?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i></td><td>dualism</td><td>elim</td><td>function</td><td>identity</td><td>panpsychism</td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1020</td><td>22%</td><td>5%</td><td>33%</td><td>13%</td><td>8%</td></tr><tr><td>Phil of computing (<i>Target</i>)</td><td>16</td><td>25%</td><td>19%</td><td>31%</td><td>0%</td><td>0%</td></tr><tr><td>Philosophers</td><td>362</td><td>22%</td><td>4%</td><td>33%</td><td>14%</td><td>7%</td></tr><tr><td>Phil of cognitive science</td><td>43</td><td>14%</td><td>12%</td><td>40%</td><td>19%</td><td>2%</td></tr><tr><td>Philosophers of mind</td><td>95</td><td>24%</td><td>2%</td><td>38%</td><td>12%</td><td>10%</td></tr></tbody></table>\n\nRegarding \"[Zombies](https://survey2020.philpeople.org/survey/results/4930): conceivable but not metaphysically possible, metaphysically possible, or inconceivable?\" (also noting \"agnostic/undecided\" results):\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i></td><td><strong>inconceivable</strong></td><td><strong>conceivable</strong><br><strong>+impossible</strong></td><td><strong>possible</strong></td><td><strong>agnostic</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1610</td><td>16%</td><td>37%</td><td>24%</td><td>11%</td></tr><tr><td>Phil of computing (<i>Target</i>)</td><td>24</td><td>29%</td><td>25%</td><td>17%</td><td>8%</td></tr><tr><td>Philosophers</td><td>582</td><td>15%</td><td>42%</td><td>22%</td><td>11%</td></tr><tr><td>Phil of cognitive science</td><td>72</td><td>24%</td><td>50%</td><td>11%</td><td>6%</td></tr><tr><td>Phil of computing</td><td>5</td><td>20%</td><td>40%</td><td>20%</td><td>0%</td></tr><tr><td>Philosophers of mind</td><td>132</td><td>20%</td><td>51%</td><td>17%</td><td>3%</td></tr></tbody></table>\n\nMy understanding is that the \"psychological view\" of personal identity more or less says 'you're software', the \"biological view\" says 'you're hardware', and the \"further-fact view\" says 'you're a supernatural soul'. Regarding \"[Personal identity](https://survey2020.philpeople.org/survey/results/4898): further-fact view, psychological view, or biological view?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp;</strong></td><td><strong>biological</strong></td><td><strong>psychological</strong></td><td><strong>further-fact</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1615</td><td>19%</td><td>44%</td><td>15%</td></tr><tr><td>Phil of computing... (<i>Target</i>)</td><td>23</td><td>22%</td><td>70%</td><td>4%</td></tr><tr><td>Philosophers</td><td>598</td><td>20%</td><td>44%</td><td>13%</td></tr><tr><td>Philosophers of cognitive science</td><td>69</td><td>26%</td><td>55%</td><td>6%</td></tr><tr><td>Philosophers of computing...</td><td>5</td><td>40%</td><td>60%</td><td>20%</td></tr><tr><td>Philosophers of mind</td><td>130</td><td>26%</td><td>47%</td><td>12%</td></tr></tbody></table>\n\nComparing this to some other philosophy subfields, as a gauge of their health:\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp;</strong></td><td><strong>biological</strong></td><td><strong>psychological</strong></td><td><strong>further-fact</strong></td></tr><tr><td>Decision theorists</td><td>18</td><td>17%</td><td>67%</td><td>17%</td></tr><tr><td>Epistemologists</td><td>144</td><td>22%</td><td>35%</td><td>17%</td></tr><tr><td>General philosophers of science</td><td>58</td><td>29%</td><td>55%</td><td>7%</td></tr><tr><td>Metaphysicians</td><td>150</td><td>23%</td><td>39%</td><td>15%</td></tr><tr><td>Normative ethicists</td><td>127</td><td>14%</td><td>46%</td><td>15%</td></tr><tr><td>Philosophers of language</td><td>120</td><td>18%</td><td>38%</td><td>17%</td></tr><tr><td>Philosophers of mathematics</td><td>18</td><td>17%</td><td>61%</td><td>6%</td></tr><tr><td>Philosophers of religion</td><td>25</td><td>24%</td><td>16%</td><td>44%</td></tr></tbody></table>\n\nDecision theorists come out of this looking pretty great (I claim). This is particularly interesting to me, because some people diagnose the 'academic decision theorist vs. LW decision theorist' disagreement as coming down to 'do you identify with your algorithm or with your physical body?'.\n\nThe above is some evidence that *either* this diagnosis is wrong, *or* academic decision theorists haven't fully followed their psychological view of personal identity to its logical conclusions.\n\nRegarding \"[Mind uploading](https://survey2020.philpeople.org/survey/results/5094) (brain replaced by digital emulation): survival or death?\" (adding answers for \"the question is too unclear to answer\" and \"there is no fact of the matter\"):\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp;</strong></td><td><strong>survival</strong></td><td><strong>death</strong></td><td><strong>Q too unclear</strong></td><td><strong>no fact</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1016</td><td>27%</td><td>54%</td><td>5%</td><td>4%</td></tr><tr><td>Phil of computing... (<i>Target</i>)</td><td>19</td><td>47%</td><td>42%</td><td>11%</td><td>0%</td></tr><tr><td>Philosophers</td><td>369</td><td>28%</td><td>54%</td><td>5%</td><td>4%</td></tr><tr><td>Decision theorists</td><td>12</td><td>42%</td><td>8%</td><td>8%</td><td>17%</td></tr><tr><td>Philosophers of cognitive science</td><td>37</td><td>35%</td><td>51%</td><td>5%</td><td>3%</td></tr><tr><td>Philosophers of mind</td><td>91</td><td>34%</td><td>52%</td><td>4%</td><td>2%</td></tr></tbody></table>\n\nFrom my perspective, decision theorists do great on this question — very few endorse \"death\", and a lot endorse \"there is no fact of the matter\" (which, along with \"survival\", strike me as good indirect signs of clear thinking given that this is a kind-of-terminological question and, depending on terminology, \"death\" is *at best* a *technically-true-but-misleading* answer).\n\nAlso, a respectable 25% of decision theorists say \"agnostic/undecided\", which is almost always something I give philosophers points for — no one's an expert on everything, a lot of these questions are confusing, and recognizing the limits of your own understanding is a very positive sign.\n\nRegarding \"[Chinese room](https://survey2020.philpeople.org/survey/results/5002): doesn't understand or understands?\" (adding \"the question is too unclear to answer\" responses):\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp;</strong></td><td><strong>understands</strong></td><td><strong>doesn't</strong></td><td><strong>Q too unclear</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1031</td><td>18%</td><td>67%</td><td>6%</td></tr><tr><td>Phil of computing... (<i>Target</i>)</td><td>18</td><td>22%</td><td>56%</td><td>17%</td></tr><tr><td>Philosophers</td><td>381</td><td>18%</td><td>66%</td><td>6%</td></tr><tr><td>Philosophers of cognitive science</td><td>44</td><td>34%</td><td>50%</td><td>7%</td></tr><tr><td>Philosophers of mind</td><td>91</td><td>15%</td><td>70%</td><td>8%</td></tr></tbody></table>\n\nRegarding \"Other minds (for which groups are some members conscious?)\" (looking only at the \"2009-comparable departments\", except for philosophy of computing and information because there aren't viewable results for that subgroup):\n\n(Options: adult humans; cats; fish; flies; worms; plants; particles; newborn babies; current AI systems; future AI systems.)\n\n(Respondent groups: philosophers; applied ethicists; decision theorists; meta-ethicists; metaphysicians; normative ethicists; philosophy of biology; philosophers of cognitive science; philosophers of computing and information; philosophers of mathematics; philosophers of mind.)\n\n<table><tbody><tr><td>&nbsp;</td><td><i><strong>n</strong></i></td><td><strong>adult h</strong></td><td><strong>cat</strong></td><td><strong>fish</strong></td><td><strong>fly</strong></td><td><strong>worm</strong></td><td><strong>plant</strong></td><td><strong>partic</strong></td><td><strong>baby h</strong></td><td><strong>AI</strong></td><td><strong>AI fut</strong></td></tr><tr><td>Phil</td><td>404</td><td>97%</td><td>93%</td><td>68%</td><td>36%</td><td>24%</td><td>7%</td><td>2%</td><td>89%</td><td>2%</td><td>43%</td></tr><tr><td>App</td><td>35</td><td>100%</td><td>94%</td><td>77%</td><td>31%</td><td>17%</td><td>3%</td><td>0%</td><td>89%</td><td>0%</td><td>63%</td></tr><tr><td>Dec</td><td>14</td><td>86%</td><td>86%</td><td>71%</td><td>36%</td><td>21%</td><td>0%</td><td>0%</td><td>64%</td><td>0%</td><td>57%</td></tr><tr><td>MtE</td><td>64</td><td>98%</td><td>92%</td><td>72%</td><td>38%</td><td>23%</td><td>6%</td><td>2%</td><td>91%</td><td>0%</td><td>47%</td></tr><tr><td>MtP</td><td>108</td><td>98%</td><td>97%</td><td>71%</td><td>42%</td><td>32%</td><td>7%</td><td>3%</td><td>95%</td><td>4%</td><td>49%</td></tr><tr><td>Nor</td><td>85</td><td>99%</td><td>94%</td><td>73%</td><td>35%</td><td>22%</td><td>5%</td><td>0%</td><td>88%</td><td>4%</td><td>41%</td></tr><tr><td>Bio</td><td>13</td><td>100%</td><td>85%</td><td>62%</td><td>38%</td><td>15%</td><td>8%</td><td>0%</td><td>69%</td><td>8%</td><td>54%</td></tr><tr><td>Cog</td><td>44</td><td>98%</td><td>98%</td><td>73%</td><td>39%</td><td>25%</td><td>11%</td><td>2%</td><td>95%</td><td>2%</td><td>50%</td></tr><tr><td>Com</td><td>19</td><td>100%</td><td>89%</td><td>68%</td><td>32%</td><td>26%</td><td>5%</td><td>0%</td><td>89%</td><td>11%</td><td>58%</td></tr><tr><td>Mat</td><td>14</td><td>100%</td><td>93%</td><td>86%</td><td>57%</td><td>43%</td><td>7%</td><td>0%</td><td>93%</td><td>7%</td><td>50%</td></tr><tr><td>Min</td><td>92</td><td>99%</td><td>97%</td><td>79%</td><td>45%</td><td>30%</td><td>10%</td><td>4%</td><td>96%</td><td>0%</td><td>43%</td></tr></tbody></table>\n\nI am confused, delighted, and a little frightened that an equal (and not-super-large) number of decision theorists think adult humans and cats are conscious. (Though as always, small *n*.)\n\nAlso impressed that they gave a low probability to newborn humans being conscious — it seems hard to be confident about the answer to this, and being willing to entertain 'well, maybe not' seems like a strong sign of epistemic humility beating out motivated reasoning.\n\nAlso, 11% of philosophers of cognitive science think *PLANTS* are conscious??? Friendship with philosophers of cognitive science ended, decision theorists new best friend.\n\nRegarding \"[Eating animals and animal products](https://survey2020.philpeople.org/survey/results/4938) (is it permissible to eat animals and/or animal products in ordinary circumstances?): vegetarianism (no and yes), veganism (no and no), or omnivorism (yes and yes)?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>omnivorism</strong></td><td><strong>vegetarianism</strong></td><td><strong>veganism</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1764</td><td>48%</td><td>26%</td><td>18%</td></tr><tr><td>Philosophers</td><td>643</td><td>46%</td><td>27%</td><td>21%</td></tr><tr><td>Applied ethicists</td><td>64</td><td>33%</td><td>23%</td><td>41%</td></tr><tr><td>Normative ethicists</td><td>131</td><td>40%</td><td>27%</td><td>31%</td></tr><tr><td>Meta-ethicists</td><td>94</td><td>41%</td><td>23%</td><td>24%</td></tr><tr><td>Philosophers of mind</td><td>134</td><td>48%</td><td>31%</td><td>12%</td></tr><tr><td>Philosophers of cognitive science</td><td>73</td><td>48%</td><td>29%</td><td>14%</td></tr></tbody></table>\n\n4\\. Metaphysics, philosophy of physics, and anthropics\n------------------------------------------------------\n\nRegarding \"[Sleeping beauty](https://survey2020.philpeople.org/survey/results/5170) (woken once if heads, woken twice if tails, credence in heads on waking?): one-half or one-third?\" (including the answers \"this question is too unclear to answer,\" \"accept an alternative view,\" \"there is no fact of the matter,\" and \"agnostic/undecided\"):\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i></td><td><strong>1/3</strong></td><td><strong>1/2</strong></td><td><strong>unclear</strong></td><td><strong>alt</strong></td><td><strong>no fact</strong></td><td><strong>agnostic</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>429</td><td>28%</td><td>19%</td><td>8%</td><td>1%</td><td>3%</td><td>40%</td></tr><tr><td>Philosophers</td><td>191</td><td>27%</td><td>20%</td><td>4%</td><td>2%</td><td>5%</td><td>42%</td></tr><tr><td>Decision theorists</td><td>13</td><td>54%</td><td>8%</td><td>0%</td><td>15%</td><td>0%</td><td>23%</td></tr><tr><td>Epistemologists</td><td>70</td><td>33%</td><td>20%</td><td>3%</td><td>3%</td><td>1%</td><td>40%</td></tr><tr><td>Logicians and phil of logic</td><td>28</td><td>36%</td><td>14%</td><td>4%</td><td>7%</td><td>4%</td><td>36%</td></tr><tr><td>Phil of cognitive science</td><td>18</td><td>28%</td><td>22%</td><td>6%</td><td>0%</td><td>0%</td><td>44%</td></tr><tr><td>Philosophers of mathematics</td><td>6</td><td>50%</td><td>17%</td><td>0%</td><td>0%</td><td>0%</td><td>33%</td></tr></tbody></table>\n\nRegarding \"[Cosmological fine-tuning](https://survey2020.philpeople.org/survey/results/5018) (what explains it?): no fine-tuning, brute fact, design, or multiverse?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i></td><td><strong>design</strong></td><td><strong>multiverse</strong></td><td><strong>brute fact</strong></td><td><strong>no fine-tuning</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>807</td><td>17%</td><td>15%</td><td>32%</td><td>22%</td></tr><tr><td>Philosophers</td><td>289</td><td>14%</td><td>16%</td><td>35%</td><td>22%</td></tr><tr><td>Decision theorists</td><td>13</td><td>8%</td><td>23%</td><td>23%</td><td>38%</td></tr><tr><td>General phil of science</td><td>33</td><td>9%</td><td>21%</td><td>48%</td><td>24%</td></tr><tr><td>Metaphysicians</td><td>93</td><td>28%</td><td>18%</td><td>32%</td><td>13%</td></tr><tr><td>Phil of cognitive science...</td><td>32</td><td>6%</td><td>19%</td><td>50%</td><td>13%</td></tr><tr><td>Phil of physical science</td><td>16</td><td>13%</td><td>25%</td><td>38%</td><td>6%</td></tr></tbody></table>\n\nRegarding \"[Quantum mechanics](https://survey2020.philpeople.org/survey/results/5150): epistemic, hidden-variables, many-worlds, or collapse?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong> &nbsp; &nbsp;</i></td><td><strong>collapse</strong></td><td><strong>hidden-var</strong></td><td><strong>many-worlds</strong></td><td><strong>epistemic</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>556</td><td>17%</td><td>22%</td><td>19%</td><td>13%</td></tr><tr><td>Philosophers</td><td>197</td><td>13%</td><td>29%</td><td>24%</td><td>8%</td></tr><tr><td>Decision theorists</td><td>8</td><td>38%</td><td>13%</td><td>63%</td><td>13%</td></tr><tr><td>General phil of science</td><td>26</td><td>31%</td><td>23%</td><td>27%</td><td>4%</td></tr><tr><td>Metaphysicians</td><td>78</td><td>12%</td><td>35%</td><td>31%</td><td>5%</td></tr><tr><td>Phil of cognitive science...</td><td>16</td><td>6%</td><td>31%</td><td>31%</td><td>19%</td></tr><tr><td>Phil of physical science</td><td>15</td><td>13%</td><td>33%</td><td>33%</td><td>0%</td></tr></tbody></table>\n\nFrom [SEP](https://plato.stanford.edu/entries/causation-metaphysics/):\n\n> What is the metaphysical basis for causal connection? That is, what is the difference between causally related and causally unrelated sequences?\n> \n> The question of connection occupies the bulk of the vast literature on causation. \\[...\\] Fortunately, the details of these many and various accounts may be postponed here, as they tend to be variations on two basic themes. In practice, the nomological, statistical, counterfactual, and agential accounts tend to converge in the indeterministic case. All understand connection in terms of *probability*: causing is making more likely. The change, energy, process, and transference accounts converge in treating connection in terms of *process*: causing is physical producing. Thus a large part of the controversy over connection may, in practice, be reduced to the question of whether connection is a matter of probability or process (Section 2.1).\n\nRegarding \"[Causation](https://survey2020.philpeople.org/survey/results/4998): process/production, primitive, counterfactual/difference-making, or nonexistent?\": \n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n &nbsp; &nbsp; &nbsp;&nbsp;</strong></i></td><td><strong>counterfactual</strong></td><td><strong>process</strong></td><td><strong>primitive</strong></td><td><strong>non</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>892</td><td>37%</td><td>23%</td><td>21%</td><td>4%</td></tr><tr><td>Philosophers</td><td>342</td><td>39%</td><td>22%</td><td>21%</td><td>3%</td></tr><tr><td>Decision theorists</td><td>14</td><td>71%</td><td>7%</td><td>7%</td><td>7%</td></tr><tr><td>Metaphysicians</td><td>103</td><td>32%</td><td>28%</td><td>28%</td><td>5%</td></tr><tr><td>Philosophers of cognitive science</td><td>38</td><td>58%</td><td>21%</td><td>13%</td><td>5%</td></tr><tr><td>Philosophers of physical science</td><td>16</td><td>63%</td><td>19%</td><td>0%</td><td>6%</td></tr></tbody></table>\n\nRegarding [Foundations of mathematics](https://survey2020.philpeople.org/survey/results/5030): constructivism/intuitionism, structuralism, set-theoretic, logicism, or formalism?:\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n&nbsp;</strong></i></td><td>constructiv</td><td>formal</td><td>logicism</td><td>structural</td><td>set</td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>600</td><td>15%</td><td>6%</td><td>12%</td><td>21%</td><td>15%</td></tr><tr><td>Philosophers</td><td>229</td><td>12%</td><td>4%</td><td>12%</td><td>24%</td><td>17%</td></tr><tr><td>Philosophers of mathematics</td><td>15</td><td>13%</td><td>7%</td><td>7%</td><td>40%</td><td>33%</td></tr></tbody></table>\n\n5\\. Superstition\n----------------\n\nRegarding \"[God](https://survey2020.philpeople.org/survey/results/4842): atheism or theism?\" (with subfields ordered by percentage that answered \"theism\"):\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</td><td><strong>theism</strong> &nbsp; &nbsp;&nbsp;</td></tr><tr><td>Philosophy (<i>Target</i>)</td><td>1770</td><td>19%</td></tr><tr><td>Philosophy</td><td>645</td><td>13%</td></tr><tr><td>Philosophy of religion</td><td>27</td><td>74%</td></tr><tr><td>Medieval and Renaissance philosophy</td><td>10</td><td>60%</td></tr><tr><td>Philosophy of action</td><td>42</td><td>21%</td></tr><tr><td>17th/18th century philosophy</td><td>64</td><td>20%</td></tr><tr><td>20th century philosophy</td><td>36</td><td>19%</td></tr><tr><td>Metaphysics</td><td>153</td><td>16%</td></tr><tr><td>Normative ethics</td><td>132</td><td>16%</td></tr><tr><td>19th century philosophy</td><td>22</td><td>14%</td></tr><tr><td>Asian philosophy</td><td>7</td><td>14%</td></tr><tr><td>Decision theory</td><td>22</td><td>14%</td></tr><tr><td>Philosophy of mind</td><td>135</td><td>13%</td></tr><tr><td>Aesthetics</td><td>30</td><td>13%</td></tr><tr><td>Ancient Greek and Roman philosophy</td><td>31</td><td>13%</td></tr><tr><td>Applied ethics</td><td>64</td><td>13%</td></tr><tr><td>Epistemology</td><td>153</td><td>13%</td></tr><tr><td>Logic and philosophy of logic</td><td>70</td><td>13%</td></tr><tr><td>Philosophy of language</td><td>128</td><td>12%</td></tr><tr><td>Philosophy of social science</td><td>27</td><td>11%</td></tr><tr><td>Meta-ethics</td><td>94</td><td>10%</td></tr><tr><td>Philosophy of law</td><td>21</td><td>10%</td></tr><tr><td>Philosophy of mathematics</td><td>20</td><td>10%</td></tr><tr><td>Social and political philosophy</td><td>100</td><td>10%</td></tr><tr><td>Philosophy of gender, race, and sexuality</td><td>23</td><td>9%</td></tr><tr><td>Philosophy of biology</td><td>24</td><td>8%</td></tr><tr><td>Philosophy of physical science</td><td>26</td><td>8%</td></tr><tr><td>General philosophy of science</td><td>65</td><td>6%</td></tr><tr><td>Philosophy of cognitive science</td><td>73</td><td>5%</td></tr><tr><td>Philosophy of computing and information</td><td>5</td><td>0%</td></tr><tr><td>Philosophy of the Americas</td><td>5</td><td>0%</td></tr><tr><td>Continental philosophy</td><td>15</td><td>0%</td></tr><tr><td>Philosophy of computing and information (<i>Target</i>)</td><td>25</td><td>0%</td></tr><tr><td>Metaphilosophy</td><td>29</td><td>0%</td></tr></tbody></table>\n\nThis question is 'philosophy in easy mode', so seems like a decent proxy for field health / competence (though the anti-religiosity of Marxism is a confounding factor in my eyes, for fields where Marx is influential).\n\nThe \"A-theory of time\" says that there is a unique objectively real \"present\", corresponding to \"which time seems to me to be *right now*\", that is universal and observer-independent, contrary to special relativity. The \"B-theory of time\" says that there is no such objective, universal \"present\".\n\nThis provides another good \"reasonableness / basic science literacy\" litmus test, so I'll order the subfields (where enough people in the field answered at all) by how much more they endorse B-theory over A-theory. Regarding \"[Time](https://survey2020.philpeople.org/survey/results/4918): B-theory or A-theory?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>A-theory</strong></td><td><strong>B-theory</strong></td><td><strong>diff</strong></td></tr><tr><td>Philosophy (<i>Target</i>)</td><td>1123</td><td>27%</td><td>38%</td><td>11%</td></tr><tr><td>Philosophy</td><td>449</td><td>22%</td><td>44%</td><td>22%</td></tr><tr><td>19th century philosophy</td><td>13</td><td>31%</td><td>8%</td><td>-23%</td></tr><tr><td>Philosophy of religion</td><td>22</td><td>45%</td><td>27%</td><td>-18%</td></tr><tr><td>Medieval and Renaissance philosophy</td><td>9</td><td>33%</td><td>22%</td><td>-11%</td></tr><tr><td>Philosophy of law</td><td>10</td><td>30%</td><td>20%</td><td>-10%</td></tr><tr><td>Aesthetics</td><td>18</td><td>22%</td><td>17%</td><td>-5%</td></tr><tr><td>Philosophy of social science</td><td>17</td><td>24%</td><td>24%</td><td>0%</td></tr><tr><td>Social and political philosophy</td><td>42</td><td>29%</td><td>33%</td><td>4%</td></tr><tr><td>Philosophy of gender, race, and sexuality</td><td>12</td><td>25%</td><td>33%</td><td>8%</td></tr><tr><td>Ancient Greek and Roman philosophy</td><td>21</td><td>24%</td><td>33%</td><td>9%</td></tr><tr><td>Philosophy of action</td><td>31</td><td>29%</td><td>39%</td><td>10%</td></tr><tr><td>20th century philosophy</td><td>22</td><td>23%</td><td>36%</td><td>13%</td></tr><tr><td>Normative ethics</td><td>78</td><td>31%</td><td>44%</td><td>13%</td></tr><tr><td>Philosophy of cognitive science</td><td>49</td><td>18%</td><td>31%</td><td>13%</td></tr><tr><td>Meta-ethics</td><td>60</td><td>27%</td><td>43%</td><td>16%</td></tr><tr><td>Philosophy of mathematics</td><td>17</td><td>12%</td><td>29%</td><td>17%</td></tr><tr><td>Asian philosophy</td><td>5</td><td>20%</td><td>40%</td><td>20%</td></tr><tr><td>Applied ethics</td><td>29</td><td>31%</td><td>52%</td><td>21%</td></tr><tr><td>Epistemology</td><td>113</td><td>21%</td><td>42%</td><td>21%</td></tr><tr><td>Philosophy of mind</td><td>111</td><td>20%</td><td>41%</td><td>21%</td></tr><tr><td>17th/18th century philosophy</td><td>44</td><td>14%</td><td>36%</td><td>22%</td></tr><tr><td>Metaphilosophy</td><td>23</td><td>17%</td><td>39%</td><td>22%</td></tr><tr><td>Metaphysics</td><td>144</td><td>28%</td><td>51%</td><td>23%</td></tr><tr><td>Logic and philosophy of logic</td><td>61</td><td>28%</td><td>52%</td><td>24%</td></tr><tr><td>Phil of computing and information (<i>Target</i>)</td><td>20</td><td>25%</td><td>50%</td><td>25%</td></tr><tr><td>Philosophy of language</td><td>107</td><td>22%</td><td>54%</td><td>32%</td></tr><tr><td>General philosophy of science</td><td>48</td><td>19%</td><td>56%</td><td>37%</td></tr><tr><td>Philosophy of biology</td><td>16</td><td>19%</td><td>63%</td><td>44%</td></tr><tr><td>Decision theory</td><td>16</td><td>13%</td><td>63%</td><td>50%</td></tr><tr><td>Philosophy of physical science</td><td>26</td><td>12%</td><td>62%</td><td>50%</td></tr></tbody></table>\n\nDecision theorists doing especially well here is surprising to me! Especially since they didn't excel on theism; if they'd hit both out of the park, from my perspective that would have been a straightforward update to \"wow, decision theorists are really exceptionally reasonable as analytic philosophers go, even if they're getting Newcomb's problem in particular wrong\".\n\nAs is, this still strikes me as a reason to be more optimistic that we might be able to converge with working decision theorists in the future. (Or perhaps more so, a reason to be *relatively* optimistic about persuading decision theorists vs. people working in most other philosophy areas.)\n\n(*Added*: OK, after writing this I saw decision theorists do great on the 'personal identity' and 'mind uploading' questions, and am feeling much more confident that productive dialogue is possible. I've added those two questions earlier in this post.)\n\n(*Added added*: OK, decision theorists are also unusually great on \"which things are conscious?\" and they apparently love MWI. How have we not converged more???)\n\n6\\. Identity politics topics\n----------------------------\n\nRegarding \"[Race](https://survey2020.philpeople.org/survey/results/4966): social, unreal, or biological?\":\n\n<table><tbody><tr><td><strong>Group</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>biological</strong></td><td><strong>social</strong></td><td><strong>unreal</strong></td></tr><tr><td>Philosophy (<i>Target</i>)</td><td>1649</td><td>19%</td><td>63%</td><td>15%</td></tr><tr><td>Philosophy</td><td>591</td><td>21%</td><td>67%</td><td>12%</td></tr></tbody></table>\n\n(Note that many respondents said 'yes' to multiple options.)\n\n7\\. Metaphilosophy\n------------------\n\nRegarding \"[Philosophical progress](https://survey2020.philpeople.org/survey/results/4958) (is there any?): a little, a lot, or none?\":\n\n<table><tbody><tr><td><strong>Group &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>none</strong></td><td><strong>a little</strong></td><td><strong>a lot</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1775</td><td>4%</td><td>47%</td><td>42%</td></tr><tr><td>Philosophers</td><td>645</td><td>3%</td><td>45%</td><td>46%</td></tr></tbody></table>\n\nRegarding \"[Philosophical knowledge](https://survey2020.philpeople.org/survey/results/5114) (is there any?): a little, none, or a lot?\":\n\n<table><tbody><tr><td><strong>Group &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><i><strong>n</strong></i><strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</strong></td><td><strong>none</strong></td><td><strong>a little</strong></td><td><strong>a lot</strong></td></tr><tr><td>Philosophers (<i>Target</i>)</td><td>1110</td><td>4%</td><td>33%</td><td>56%</td></tr><tr><td>Philosophers</td><td>397</td><td>4%</td><td>30%</td><td>58%</td></tr></tbody></table>\n\nAnother interesting result is \"[Philosophical methods](https://survey2020.philpeople.org/survey/results/4962) (which methods are the most useful/important?)\", which finds (looking at analogous-to-2009 departments):\n\n*   66% of philosophers think \"**conceptual analysis**\" is especially important, 14% disagree.\n*   60% say \"**empirical philosophy**\", 12% disagree.\n*   59% say \"**formal philosophy**\", 10% disagree.\n*   51% say \"**intuition-based philosophy**\", 27% disagree.\n*   44% say \"**linguistic philosophy**\", 23% disagree.\n*   39% say \"**conceptual engineering**\", 23% disagree.\n*   29% say \"**experimental philosophy**\", 39% disagree.\n\n8\\. How have philosophers' views changed since 2009?\n----------------------------------------------------\n\nBourget and Chalmers' paper has a table for the largest changes in philosophers' views since 2009:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/32ab696e960d5bd17741d6714f0fdf1fdc517ecbf2daa1e5.png)\n\nAs noted earlier in this post, one of the larger shifts in philosophers' views was a move away from moral internalism and toward externalism.\n\nOn 'which do you endorse, classical logic or non-classical?' (a strange question, but maybe this is something like 'what kind of logic is reality's source code written in?'), non-classical logic is roughly as unpopular as ever, but fewer now endorse classical logic, and more give answers like \"insufficiently familiar with the issue\" and \"the question is too unclear to answer\":\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/81a30d5d16043623ec9f657ce881b52effa12f79e94cde42.png)\n\n[Epistemic contextualism](https://plato.stanford.edu/entries/contextualism-epistemology/) says that the accuracy of your claim that someone \"knows\" something depends partly on contextual features — e.g., the standards for \"knowledge\" can rise \"as the stakes rise or the skeptical doubts become more serious\".\n\nHere, it was the less popular view (invariantism) that lost favor; and the view that lost favor again lost it via an increase in 'other' answers (especially  \"insufficiently familiar with the issue\" and \"agnostic/undecided\") more so than increased favor for its rival view (contextualism):\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fd5cf7dace1729a651d27f5ced5b4f709fbddeecfcbfd420.png)\n\nHumeanism (a misnomer, since Hume himself wasn't a Humean, though his skeptical arguments helped inspire the Humeans) say that \"laws of nature\" aren't fundamentally different from other observed regularities, they're just patterns that humans have given a fancy high-falutin name to; whereas anti-Humeans think there's something deeper about laws of nature, that they in some sense 'necessitate' things to go one way rather than another.\n\n(Maybe Humeans = 'laws of nature are program outputs like any other', non-Humeans = 'laws of nature are part of reality's source code'?)\n\nOnce again, one view lost favor (the more popular view, non-Humeanism), but the other didn't gain favor; instead, more people endorsed \"insufficiently familiar with the issue\", and \"agnostic/undecided\", etc.:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5b60d53c6c6a1108619cb59affe40f3b9e658880f5c457d7.png)\n\nPhilosophers in 2020 are more likely to say that \"yes\", humans have *a priori* knowledge of some things (already very much the dominant view):\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d98e081a0112a426bdeebd884a9c6b0fe3963fa756388e67.png)\n\n'Aesthetic value is objective' was favored over 'subjective' (by 3%) in 2009; now 'subjective' is favored over 'objective' (by 4%). \"Agnostic/undecided\" also gained ground.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/9a70200fc8e3cd285c4c861d40b15cf72189bcab5c796594.png)\n\nPhilosophers mostly endorsed \"switch\" in the trolley dilemma, and still do; but \"don't switch\" gained a bit of ground, and \"insufficiently familiar with the issue\" lost ground.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f5be101b87c10074c526e173494c7567a47ab7527162a0a8.png)\n\n[Moral realism](https://survey2020.philpeople.org/survey/results/4866) also became a bit more popular (was endorsed by 56% of philosophers, now 60%), as did [compatibilism about free will](https://survey2020.philpeople.org/survey/results/4838) (was 59% compatibilism, 14% libertarianism, 12% no free will; now 62%, 13%. and 10%).\n\nThe paper also looked at the individual respondents who answered the survey in both 2009 and 2020. Individuals tended to update away from switching in the trolley dilemma, away from consequentialism, and toward virtue ethics and non-cognitivism. They also updated toward Platonism about abstract objects, and away from 'no free will'.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5123ed4b7683ffe62a62ed5d5f611a7b254d235afb9385e0.png)\n\nThese are all comparisons across 2009-target-population philosophers in general, however. In most (though not all) cases, I'm more interested in the views of subfields specialized in investigating and debating a topic, and how the subfield's view changes over time. Hence my earlier sections largely focused on particular fields of philosophy.",
      "plaintextDescription": "In 2009, David Bourget and David Chalmers ran the PhilPapers Survey (results, paper), sending questions to \"all regular faculty members\" at top \"Ph.D.-granting [philosophy] departments in English-speaking countries\" plus ten other philosophy departments deemed to have \"strength in analytic philosophy comparable to the other 89 departments\".\n\nBourget and Chalmers now have a new PhilPapers Survey out, run in 2020 (results, paper). I'll use this post to pick out some findings I found interesting, and say opinionated stuff about them. Keep in mind that I'm focusing on topics and results that dovetail with things I'm curious about (e.g., 'why do academic decision theorists and LW decision theorists disagree so much?'), not giving a neutral overview of the whole 100-question survey.\n\nThe new survey's target population consists of:\n\n> (1) in Australia, Canada, Ireland, New Zealand, the UK, and the US: all regular faculty members (tenuretrack or permanent) in BA-granting philosophy departments with four or more members (according to the PhilPeople database); and (2) in all other countries: English-publishing philosophers in BA-granting philosophy departments with four or more English-publishing faculty members.\n\nIn order to make comparisons to the 2009 results, the 2020 survey also looked at a \"2009-comparable departments\" list selected using similar criteria to the 2009 survey:\n\n> It should be noted that the “2009-comparable department” group differs systematically from the broader target population in a number of respects. Demographically, it includes a higher proportion of UK-based philosophers and analytic-tradition philosophers than the target population. Philosophically, it includes a lower proportion of theists, along with many other differences evident in comparing 2020 results in table 1 (all departments) to table 9 (2009-comparable departments).\n\nBased on this description, I expect the \"2009-comparable departments\" in the 2020 survey to be more elite, influential,",
      "wordCount": 3496
    },
    "tags": [
      {
        "_id": "kCrvmjZhsAZANnZL6",
        "name": "Philosophy",
        "slug": "philosophy-1"
      },
      {
        "_id": "kJrjorSx3hXa7q7CJ",
        "name": "Surveys",
        "slug": "surveys"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "F3aESx4JWWEDAFEiH",
    "title": "Nate Soares on the Ultimate Newcomb's Problem",
    "slug": "nate-soares-on-the-ultimate-newcomb-s-problem",
    "url": null,
    "baseScore": 57,
    "voteCount": 25,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2021-10-31T19:42:01.353Z",
    "contents": {
      "markdown": "Nate Soares's distilled exposition of the [Ultimate Newcomb's Problem](https://www.lesswrong.com/posts/RAh4fekdiRhZxb2Kw/the-ultimate-newcomb-s-problem), plus a quick analysis of how different decision theories perform, copied over from a recent email exchange (with Nate-revisions):\n\n* * *\n\n> You might be interested in Eliezer's \"Ultimate Newcomb's Problem\", which is a rare decision problem where EDT and CDT agree, but disagree with FDT. In this variant, the small box contains $1k, and the big box is transparent and contains a number X whose primality you are unsure of. Omega will pay you $1M if the number in the big box is prime, and put a prime number in that box iff they predicted you will take only the big box. Meanwhile, a third actor Omicron chooses a number at random each day, and will pay you $2M if their randomly-selected number is composite, and today they happen to have selected the number X.\n> \n> The causal decision theorist takes both boxes, reasoning that all they can control is whether they get an extra $1k, so they might as well. The evidential decision theorist takes both boxes, reasoning that this makes X composite, which pays more than making X prime by taking one box (the extra $1k being inconsequential). The functional decision theorist takes one box, reasoning that on days that they're going to get paid by Omicron the number in the big box and the number chosen by Omicron will not coincide, but recognizing that their decision about whether or not to one-box has no effect on the probability that Omicron pays them.\n> \n> As for who performs better, for clarity assume that Omega makes the number in the big box coincide with the number chosen by Omicron whenever possible, and write \\\\(p\\\\) for the probability that Omicron chooses a composite number. Then CDT will always see a composite number (and it will match Omicron's in the \\\\(p\\\\) fraction of the time when Omicron's is also composite); EDT will see a number with the same primality as Omicron's number (that matches in the \\\\(p\\\\) fraction of cases where Omicron's number is composite, and differs in the (\\\\(1 - p\\\\)) fraction of cases where Omicron's number is prime); FDT will always see a prime number (that matches Omicron's in the (\\\\(1 - p\\\\)) fraction where Omicron's is prime). The payouts, then, will be \\\\((\\$2000000 * p) + \\$1000\\\\) for CDT; \\\\((\\$2000000 * p) + (\\$1000 * p) + (\\$1000000 * (1-p))\\\\) for EDT; and \\\\((\\$2000000 * p) + \\$1000000\\\\) for FDT; a clear victory in terms of expected utility for FDT. (Exercise: a similar ranking holds when Omega only sometimes matches Omicron's number conditional on that being possible.)",
      "plaintextDescription": "Nate Soares's distilled exposition of the Ultimate Newcomb's Problem, plus a quick analysis of how different decision theories perform, copied over from a recent email exchange (with Nate-revisions):\n\n----------------------------------------\n\n> You might be interested in Eliezer's \"Ultimate Newcomb's Problem\", which is a rare decision problem where EDT and CDT agree, but disagree with FDT. In this variant, the small box contains $1k, and the big box is transparent and contains a number X whose primality you are unsure of. Omega will pay you $1M if the number in the big box is prime, and put a prime number in that box iff they predicted you will take only the big box. Meanwhile, a third actor Omicron chooses a number at random each day, and will pay you $2M if their randomly-selected number is composite, and today they happen to have selected the number X.\n> \n> The causal decision theorist takes both boxes, reasoning that all they can control is whether they get an extra $1k, so they might as well. The evidential decision theorist takes both boxes, reasoning that this makes X composite, which pays more than making X prime by taking one box (the extra $1k being inconsequential). The functional decision theorist takes one box, reasoning that on days that they're going to get paid by Omicron the number in the big box and the number chosen by Omicron will not coincide, but recognizing that their decision about whether or not to one-box has no effect on the probability that Omicron pays them.\n> \n> As for who performs better, for clarity assume that Omega makes the number in the big box coincide with the number chosen by Omicron whenever possible, and write p for the probability that Omicron chooses a composite number. Then CDT will always see a composite number (and it will match Omicron's in the p fraction of the time when Omicron's is also composite); EDT will see a number with the same primality as Omicron's number (that matches in the p fraction of cases where Omicron",
      "wordCount": 426
    },
    "tags": [
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      },
      {
        "_id": "WnnwytvzpKqJ3YChY",
        "name": "Newcomb's Problem",
        "slug": "newcomb-s-problem"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "TkahaFu3kb6NhZRue",
    "title": "Quick general thoughts on suffering and consciousness",
    "slug": "quick-general-thoughts-on-suffering-and-consciousness",
    "url": null,
    "baseScore": 38,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 50,
    "createdAt": null,
    "postedAt": "2021-10-30T18:05:59.612Z",
    "contents": {
      "markdown": "Below, I've collected some of my thoughts on consciousness. Topics covered (in the post and/or the comments below) include:\n\n*   To what extent did subjective pain evolve as a social signal?\n*   Why did consciousness evolve? What function(s) did it serve?\n*   What would the evolutionary precursors of 'full' consciousness look like?\n*   What sorts of human values are more or less likely to extend to unconscious things?\n*   Is consciousness more like a machine (where there's a sharp cutoff between 'the machine works' or 'the machine doesn't'), or is it more like a basic physical property like mass (where there's a continuum from very small fundamental things that have small amounts of the property, all the way up to big macroscopic objects that have way more of the property)?\n*   How should illusionism (the view that in an important sense we *aren't* conscious, but non-phenomenally 'appear' to be conscious) change our answers to the questions above?\n\n* * *\n\n1\\. Pain signaling\n------------------\n\n*In September 2019, I wrote on my* [*LW shortform*](https://www.lesswrong.com/posts/HXyGXq9YmKdjqPseW/rob-b-s-shortform-feed?commentId=CJZNw35YNsFEiXgvA)*:*\n\nRolf Degen, summarizing part of Barbara Finlay's \"[The neuroscience of vision and pain](https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2019.0292)\":\n\n> Humans may have evolved to experience far greater pain, malaise and suffering than the rest of the animal kingdom, due to their intense sociality giving them a reasonable chance of receiving help.\n\nFrom the paper:\n\n> Several years ago, we proposed the idea that pain, and sickness behaviour had become systematically increased in humans compared with our primate relatives, because human intense sociality allowed that we could ask for help and have a reasonable chance of receiving it. We called this hypothesis ‘the pain of altruism’ \\[68\\]. This idea derives from, but is a substantive extension of Wall’s account of the placebo response \\[43\\]. Starting from human childbirth as an example (but applying the idea to all kinds of trauma and illness), we hypothesized that labour pains are more painful in humans so that we might get help, an ‘obligatory midwifery’ which most other primates avoid and which improves survival in human childbirth substantially (\\[67\\]; see also \\[69\\]). Additionally, labour pains do not arise from tissue damage, but rather predict possible tissue damage and a considerable chance of death. Pain and the duration of recovery after trauma are extended, because humans may expect to be provisioned and protected during such periods. The vigour and duration of immune responses after infection, with attendant malaise, are also increased. Noisy expression of pain and malaise, coupled with an unusual responsivity to such requests, was thought to be an adaptation.\n\n> We noted that similar effects might have been established in domesticated animals and pets, and addressed issues of ‘honest signalling’ that this kind of petition for help raised. No implication that no other primate ever supplied or asked for help from any other was intended, nor any claim that animals do not feel pain. Rather, animals would experience pain to the degree it was functional, to escape trauma and minimize movement after trauma, insofar as possible.\n\nFinlay's original article on the topic: \"[The pain of altruism](https://www.ncbi.nlm.nih.gov/pubmed/25200380)\".\n\n\\[Epistemic status: Thinking out loud\\]\n\nIf the evolutionary logic here is right, I'd naively also expect non-human animals to suffer more to the extent they're (a) more social, and (b) better at communicating specific, achievable needs and desires.\n\nThere are reasons the logic might not generalize, though. Humans have fine-grained language that lets us express very complicated propositions about our internal states. That puts a lot of pressure on individual humans to have a totally ironclad, consistent \"story\" they can express to others. I'd expect there to be a lot more evolutionary pressure to *actually* experience suffering, since a human will be better at spotting holes in the narratives of a human who fakes it (compared to, e.g., a bonobo trying to detect whether another bonobo is really in that much pain).\n\nIt seems like there should be an arms race across many social species to give increasingly costly signals of distress, up until the costs outweigh the amount of help they can hope to get. But if you don't have the language to actually express concrete propositions like \"Bob took care of me the last time I got sick, six months ago, and he can attest that I had a hard time walking that time too\", then those costly signals might be mostly or entirely things like \"shriek louder in response to percept X\", rather than things like \"internally represent a hard-to-endure pain-state so I can more convincingly stick to a verbal narrative going forward about how hard-to-endure this was\".\n\n* * *\n\n2\\. To what extent is suffering conditional or complex?\n-------------------------------------------------------\n\n*In July 2020, I wrote on my* [*shortform*](https://www.lesswrong.com/posts/HXyGXq9YmKdjqPseW/rob-b-s-shortform-feed?commentId=CJZNw35YNsFEiXgvA)*:*\n\n\\[Epistemic status: Piecemeal wild speculation; not the kind of reasoning you should gamble the future on.\\]\n\nSome things that make me think suffering (or 'pain-style suffering' specifically) might be surprisingly neurologically conditional and/or complex, and therefore more likely to be rare in non-human animals (and in subsystems of human brains, in AGI subsystems that aren't highly optimized to function as high-fidelity models of humans, etc.):\n\n1\\. **Degen and Finlay's social account of suffering above.**\n\n2\\. **Which things we suffer from seems to depend heavily on mental narratives and mindset.** See, e.g., Julia Galef's [Reflections on Pain, from the Burn Unit](https://web.archive.org/web/20170507113747/http://measureofdoubt.com/2011/09/27/reflections-on-pain-from-the-burn-unit/).\n\nPain management is one of the main things hypnosis appears to be useful for. Ability to cognitively regulate suffering is also one of the main claims of meditators, and seems related to [existential psychotherapy's claim](https://www.amazon.com/Mans-Search-Meaning-Viktor-Frankl/dp/080701429X) that narratives are more important for well-being than material circumstances.\n\nEven if suffering isn't highly social (*pace* Degen and Finlay), its dependence on higher cognition suggests that it is much more complex and conditional than it might appear on initial introspection, which on its own reduces the probability of its showing up elsewhere: complex things are relatively unlikely *a priori*, are especially hard to evolve, and demand especially strong selection pressure if they're to evolve and if they're to be maintained.\n\n(Note that suffering *introspectively feels* relatively basic, simple, and out of our control, even though it's not. Note also that *what things introspectively feel like* is itself under selection pressure. If suffering *felt* complicated, derived, and dependent on our choices, then the whole suite of social thoughts and emotions related to deception and manipulation would be much more salient, both to sufferers and to people trying to evaluate others' displays of suffering. This would muddle and complicate attempts by sufferers to consistently socially signal that their distress is important and real.)\n\n3\\. **When humans experience large sudden neurological changes and are able to remember and report on them, their later reports generally suggest positive states more often than negative ones.** This seems true of near-death experiences and drug states, though the case of drugs is obviously filtered: the more pleasant and/or reinforcing drugs will generally be the ones that get used more.\n\nSometimes people report remembering that a state change was scary or disorienting. But they rarely report feeling *agonizing pain*, and they often either endorse having had the experience (with the benefit of hindsight), or report having enjoyed it at the time, or both.\n\nThis suggests that humans' capacity for suffering (especially more 'pain-like' suffering, as opposed to fear or anxiety) may be fragile and complex. Many different ways of disrupting brain function seem to prevent suffering, suggesting suffering is the more difficult and conjunctive state for a brain to get itself into; you need more of the brain's machinery to be in working order in order to pull it off.\n\n4\\. **Similarly, I frequently hear about dreams that are scary or disorienting, but I don't think I've ever heard of someone recalling having experienced severe pain from a dream**, even when they remember dreaming that they were being physically damaged.\n\nThis may be for reasons of selection: if dreams were more unpleasant, people would be less inclined to go to sleep and their health would suffer. But it's interesting that *scary* dreams are nonetheless common. This again seems to point toward 'states that are further from the typical human state are much more likely to be capable of things like fear or distress, than to be capable of suffering-laden physical agony.'\n\n* * *\n\n3\\. Consciousness and suffering\n-------------------------------\n\n*Eliezer recently* [*criticized*](https://twitter.com/ESYudkowsky/status/1453544516393205765) *\"people who worry that chickens are sentient and suffering\" but \"don't also worry that GPT-3 is sentient and maybe suffering\". (He thinks chickens and GPT-3 are both non-sentient.)*\n\n*Jemist responded on LessWrong, and Nate Soares wrote a* [*reply*](https://www.lesswrong.com/posts/KFbGbTEtHiJnXw5sk/i-really-don-t-understand-eliezer-yudkowsky-s-position-on?commentId=fCPjzSFPE8v3XhXr7#comments) *to Jemist that I like:*\n\n> Instrumental status: off-the-cuff reply, out of a wish that more people in this community understood what the sequences have to say about how to do philosophy correctly (according to me).\n> \n> *\\> EY's position seems to be that self-modelling is both necessary and sufficient for consciousness.*\n> \n> That is not how it seems to me. My read of his position is more like: \"Don't start by asking 'what is consciousness' or 'what are qualia'; start by asking 'what are the cognitive causes of people talking about consciousness and qualia', because while abstractions like 'consciousness' and 'qualia' might turn out to be labels for our own confusions, the words people emit about them are physical observations that won't disappear. Once one has figured out what is going on, they can plausibly rescue the notions of 'qualia' and 'consciousness', though their concepts might look fundamentally different, just as a physicist's concept of 'heat' may differ from that of a layperson. Having done this exercise at least in part, I (Nate's model of Eliezer) assert that consciousness/qualia can be more-or-less rescued, and that there is a long list of things an algorithm has to do to 'be conscious' / 'have qualia' in the rescued sense. The mirror test seems to me like a decent proxy for at least one item on that list (and the presence of one might correlate with a handful of others, especially among animals with similar architectures to ours).\"\n> \n> \\> *An ordering of consciousness as reported by humans might be:*\n> \n> \\> *Asleep Human < Awake Human < Human on Psychedelics/Zen Meditation*\n> \n> \\> *I don't know if EY agrees with this.*\n> \n> My model of Eliezer says \"Insofar as humans do report this, it's a fine observation to write down in your list of 'stuff people say about consciousness', which your completed theory of consciousness should explain. However, it would be an error to take this as much evidence about 'consciousness', because it would be an error to act like 'consciousness' is a coherent concept when one is so confused about it that they cannot describe the cognitive antecedents of human insistence that there's an ineffable redness to red.\"\n> \n> \\> *But what surprises me the most about EY's position is his confidence in it.*\n> \n> My model of Eliezer says \"The type of knowledge I claim to have, is knowledge of (at least many components of) a cognitive algorithm that looks to me like it codes for consciousness, in the sense that if you were to execute it then it would claim to have qualia for transparent reasons and for the same reasons that humans do, and to be correct about that claim in the same way that we are. From this epistemic vantage point, I can indeed see clearly that consciousness is not much intertwined with predictive processing, nor with the \"binding problem\", etc. I have not named the long list of components that I have compiled, and you, who lack such a list, may well not be able to tell what consciousness is or isn't intertwined with. However, you can still perhaps understand what it would feel like to believe you can see (at least a good part of) such an algorithm, and perhaps this will help you understand my confidence. Many things look a lot more certain, and a lot less confusing, once you begin to see how to program them.\"\n\n*Some conversations I had on Twitter and Facebook, forking off of Eliezer's tweet (somewhat arbitrarily ordered, and with ~4 small edits to my tweets):*\n\n**Bernardo Subercaseux:** I don't understand this take at all. It's clear to me that the best hypothesis for why chickens have reactions to physical damage that are consistent with our models of expressions of suffering, as also do babies, is bc they can suffer in a similar way that I do. Otoh, best hypothesis for why GPT-3 would say \"don't kill me\" is simply because it's a statistically likely response for a human to have said in a similar context. I think claiming that animals require a certain level of intelligence to experience pain is unfalsifiable...\n\n**Rob Bensinger:** Many organisms with very simple nervous systems, or with no nervous systems at all, change their behavior in response to bodily damage -- albeit not in the specific ways that chickens do. So there must be some more specific behavior you have in mind here.\n\nAs for GPT-3: if you trained an AI to *perfectly* imitate all human behaviors, then plausibly it would contain suffering subsystems. This is because real humans suffer, and a good way to predict a system (including a human brain) is to build a detailed emulation of it.\n\nGPT-3 isn't a perfect emulator of a human (and I don't think it's sentient), but there's certainly a nontrivial question of how we can know it's not sentient, and how sophisticated a human-imitator could get before we'd start wanting to assign non-tiny probability to sentience.\n\n**Bernardo Subercaseux: **I don't think it's possible to perfectly imitate all human behavior for anything non-human, in the same fashion as we cannot perfectly imitate all chicken behaviors, of plant behaviors... I think being embodied as a full X is a requisite to perfectly imitate the behavior of X\n\n**Rob Bensinger: **If an emulated human brain won't act human-like unless it sees trees and grass, outputs motor actions like walking (with the associated stable sensory feedback), etc., then you can place the emulated brain in a virtual environment and get your predictions about humans that way.\n\n**Bernardo Subercaseux: **My worry is that this converges to the \"virtual environment\" having to be exactly the real world with real trees and real grass and a real brain made of the same as ours and connected to as many things as ours is connected...\n\n**Rob Bensinger:** Physics is local, so you don't have to simulate the entire universe to accurately represent part of it.\n\nE.g., suppose you want to emulate how a human would respond to being slipped notes inside a locked white room. You might have to simulate the room in some sensory detail, but you wouldn't need to simulate anything outside the room in any detail. You can just decide what you want the note to say, and then simulate a realistic-feeling, realistic-looking note coming into existence in the white room's mail chute.\n\n**Bernardo Subercaseux:** \n\n> \\[Physics is local, so you don't have to simulate the entire universe to accurately represent part of it.\n> \n> E.g., suppose you want to emulate how a human would respond to being slipped notes inside a locked white room. You might have to simulate the room in some sensory detail...\\]\n\ni) not sure about the first considering quantum, but you might know more than I do.\n\nii) but I'm not saying the entire universe, just an actual human body.\n\nIn any case, I still think that a definite answer relies on understanding the physical processes of consciousness, and yet it seems to me that no AI at the moment is close to pose a serious challenge in terms of whether it has the ability to suffer. This in opposition to animals like pigs or chicken...\n\n**Rob Bensinger:** QM allows for some nonlocal-looking phenomena in a sense, but it still has a speed-of-light limit.\n\nI don't understand what you mean by 'actual human body' or 'embodied'. What specific properties of human bodies are important for human cognition, and expensive to simulate?\n\nI think this is a reasonable POV: 'Humans are related to chickens, so maybe chickens have minds sort of like a human's and suffer in the situations humans would suffer in. GPT-3 isn't related to us, so we should worry less about GPT-3, though both cases are worth worrying about.'\n\nI don't think 'There's no reason to worry whatsoever about whether GPT-3 suffers, whereas there are major reasons to worry animals might suffer' is a reasonable POV, because I haven't seen a high-confidence model of consciousness grounding that level of confidence in all of that.\n\n**Bernardo Subercaseux:** doesn't your tweet hold the same if you replace \"GPT-3\" by \"old Casio calculator\", or \"rock\"?\n\n**Rob Bensinger:** I'm modeling consciousness as 'a complicated cognitive something-we-don't-understand, which is connected enough to human verbal reporting that we can verbally report on it in great detail'.\n\nGPT-3 and chickens have a huge number of (substantially non-overlapping) cognitive skills, very unlike a calculator or a rock. GPT-3 is more human-like in some (but not all) respects. Chickens, unlike GPT-3, are related to humans. I think these facts collectively imply uncertainty about whether chickens and/or GPT-3 are conscious, accompanied by basically no uncertainty about whether rocks or calculators are conscious.\n\n( Also, I agree that I was being imprecise in my earlier statement, so thanks for calling me out on that. 🙂 )\n\n**Bernardo Subercaseux:** the relevance of verbal reporting is an entire conversation on its own IMO haha! Thanks for the thought-provoking conversation :) I think we agree on the core, and your comments made me appreciate the complexity of the question at hand!\n\n**Eli Tyre:** I mean, one pretty straightforward thing to say:\n\nIF chickens are sentient, then the chickens in factory farms are DEFINITELY in a lot of pain.\n\nIF GPT-3 is sentient, I have no strong reason to think that it is or isn't in pain.\n\n**Rob Bensinger:** Chickens in factory farms definitely undergo a lot of bodily damage, illness, etc. If there are sentient processes in those chickens' brains, then it seems like further arguments are needed to establish that the damage is registered by the sentient processes.\n\nThen another argument for 'the damage is registered as suffering', and another (if we want to establish that such lives are net-negative) for 'the overall suffering outweighs everything else'. This seems to require a model of what sentience is / how it works / what it's for.\n\nIt might be that the explanation for all this is simple -- that you get all this for free by positing a simple mechanism. So I'm not decomposing this to argue the prior must be low. I'm just pointing at what has to be established at all, and that it isn't a freebie.\n\n**Eli Tyre:** We have lots of intimate experience of how, for humans, damage and nociception leads to pain experience.\n\nAnd the mappings make straightforward evolutionary sense. Once you're over the hump of positing conscious experience at all, it makes sense that damage is experienced as negative conscious experience.\n\nConditioning on chickens being conscious at all, it seems like the prior is that their \\[conscious\\] experience of \\[nociception\\] follows basically the same pattern as a human's.\n\nIt would be really surprising to me if humans were conscious and chickens were conscious, but humans were conscious of pain, while chickens weren't?!?\n\nThat would seem to imply that conscious experience of pain is adaptive for humans but not for chickens?\n\nLike, assuming that consciousness is that old on the phylogenitc tree, why is conscious experience of pain a separate thing that comes later?\n\nI would expect pain to be one of the first things that organisms evolved to be conscious of.\n\n**Rob Bensinger:** I think this is a plausible argument, and I'd probably bet in that direction. Not with much confidence, though, 'cause it depends a lot on what the function of 'consciousness' is over and above things like 'detecting damage to the body' (which clearly doesn't entail 'conscious').\n\nMy objection was to \"IF chickens are sentient, then the chickens in factory farms are DEFINITELY in a lot of pain.\"\n\nI have no objection to 'humans and chickens are related, so we can make a plausible guess that if they're conscious, they suffer in situations where we'd suffer.'\n\nExample: maybe consciousness evolved as a cog in some weird specific complicated function like 'remembering the smell of your kin' or, heck, 'regulating body temperature'. Then later developed things like globalness / binding / verbal reportability, etc.\n\nMy sense is there's a crux here like\n\nEli: 'Conscious' is a pretty simple, all-or-nothing thing that works the same everywhere. If a species is conscious, then we can get a good first-approximation picture by imagining that we're inside that organism's skull, piloting its body.\n\nMe: 'Conscious' is incredibly complicated and weird. We have no idea how to build it. It seems like a huge mechanism hooked up to tons of things in human brains. Simpler versions of it might have a totally different function, be missing big parts, and work completely differently.\n\nI might still bet in the same *direction* as you, because I know so little about *which ways* chicken consciousness would differ from my consciousness, so I'm forced to not make many big directional updates away from human anchors. But I expect way more unrecognizable-weirdness.\n\nMore specifically, re \"why is conscious experience of pain a separate thing that comes later\": [https://www.lesswrong.com/posts/HXyGXq9YmKdjqPseW/rob-b-s-shortform-feed?commentId=mZw9Jaxa3c3xrTSCY#mZw9Jaxa3c3xrTSCY](https://www.lesswrong.com/posts/HXyGXq9YmKdjqPseW/rob-b-s-shortform-feed?commentId=mZw9Jaxa3c3xrTSCY#mZw9Jaxa3c3xrTSCY) \\[section 2 above\\] provides some reasons to think pain-suffering is relatively conditional, complex, social, high-level, etc. in humans.\n\nAnd noticing + learning from body damage seems like a very simple function that we already understand how to build. If a poorly-understood thing like consc is going to show up in surprising places, it would probably be more associated with functions that are less straightforward.\n\nE.g., it would be shocking if consc evolved to help organisms notice body damage, or learn to avoid such damage.\n\nIt would be less shocking if a weird esoteric aspect of intelligence eg in 'aggregating neural signals in a specific efficiency-improving way' caused consciousness.\n\nBut in that case we should be less confident, assuming chickens are conscious, that their consciousness is 'hooked up' to trivial-to-implement stuff like 'learning to avoid bodily damage at all'.\n\n**silencenbetween:** \n\n> This seems to require a model of what sentience is.\n\nI think I basically agree with you, that there is a further question of pain=suffering? And that that would ideally be established.\n\nBut I feel unsure of this claim. Like, I have guesses about consciousness and I have priors and intuitions about it, and that leads me to feeling fairly confident that chickens experience pain.\n\nBut to my mind, we can never unequivocally establish consciousness, it’s always going to be a little bit of guesswork. There’s always a further question of first hand experience.\n\nAnd in that world, models of consciousness refine our hunches of it and give us a better shared understanding, but they never conclusively tell us anything.\n\nI think this is a stronger claim than just using Bayesian reasoning. Like, I don’t think you can have absolute certainty about anything…\n\nbut I also think consciousness inhabits a more precarious place. I’m just making the hard problem arguments, I guess, but I think they’re legit.\n\nI don’t think the hard problem implies that you are in complete uncertainty about consciousness. I do think it implies something trickier about it relative to other phenomena.\n\nWhich to me implies that a model of the type you’re imagining wouldn’t conclusively solve the problem anymore than models of the sort “we’re close to each other evolutionarily” do.\n\nI think models of it can help refine our guesses about it, give us clues, I just don’t see any particular model being the final arbiter of what counts as conscious.\n\nAnd in that world I want to put more weight on the types of arguments that Eli is making. So, I guess my claim is that these lines of evidence should be about as compelling as other arguments.\n\n**Rob Bensinger:** I think we'll have a fully satisfying solution to the hard problem someday, though I'm pretty sure it will have to route through illusionism -- not all parts of the phenomenon can be saved, even though that sounds paradoxical or crazy.\n\nIf we *can't* solve the problem, though (the phil literature calls this view 'mysterianism'), then I don't think that's a good reason to be *more* confident about which organisms are conscious, or to put more weight on our gut hunches.\n\nI endorse the claim that the hard problem is legit (and hard), btw, and that it makes consciousness trickier to think about in some ways.\n\n**David Manheim:** \n\n> **\\[**It might be that the explanation for all this is simple -- that you get all this for free by positing a simple mechanism. So I'm not decomposing this to argue the prior must be low. I'm just pointing at what has to be established at all, and that it isn't a freebie.\\]\n\nAgreed - but my strong belief on how any sentience / qualia would need to work is that it would be beneficial to evolutionary fitness, meaning pain would need to be experienced as a (fairly strong) negative for it to exist.\n\nClearly, the same argument doesn't apply to GPT-3.\n\n**Rob Bensinger:** I'm not sure what you mean by 'pain', so I'm not sure what scenarios you're denying (or why).\n\nAre you denying the scenario: part of an organism's brain is conscious (but not tracking or learning from bodily damage), and another (unconscious) part is tracking and learning from bodily damage?\n\nAre you denying the scenario: a brain's conscious states are changing in response to bodily damage, in ways that help the organism better avoid bodily damage, *and* none of these conscious changes feel 'suffering-ish' / 'unpleasant' to the organism?\n\nI'm not asserting that these are all definitely plausible, or even possible -- I don't understand where consciousness comes from, so I don't know which of these things are independent. But you seem to be saying that some of these things aren't independent, and I'm not sure why.\n\n**David Manheim:** Clearly something was lost here - I'm saying that the claim that there is a disconnect between conscious sensation and tracking bodily damage is a difficult one to believe. And if there is any such connection, the reason that physical damage is negative is that it's beneficial.\n\n**Rob Bensinger:** I don't find it difficult to believe that a brain could have a conscious part doing something specific (eg, storing what places look like), and a separate unconscious part doing things like 'learning which things cause bodily damage and tweaking behavior to avoid those'.\n\nI also don't find it difficult to believe that a conscious part of a brain could 'learn which things cause bodily damage and tweak behavior to avoid those' without experiencing anything as 'bad' per se.\n\nEg, imagine building a robot that has a conscious subsystem. The subsystem's job is to help the robot avoid bodily damage, but the subsystem experiences this as being like a video game -- it's fun to rack up 'the robot's arm isn't bleeding' points.\n\n**David Manheim:** That is possible for a \\[robot\\]. But in a chicken, you're positing a design with separable features and subsystems that are conceptually distinct. Biology doesn't work that way - it's spaghetti towers the whole way down.\n\n**Rob Bensinger:** What if my model of consciousness says 'consciousness is an energy-intensive addition to a brain, and the more stuff you want to be conscious, the more expensive it is'? Then evolution will tend to make a minimum of the brain conscious -- whatever is needed for some function.\n\n> people can be conscious about almost any signal that reaches their brain, largely depending on what they have been trained to pay attention to\n\nThis seems wrong to me -- what do you mean by 'almost any signal'? (Is there a paper you have in mind operationalizing this?)\n\n**David Manheim:** I mean that people can choose / train to be conscious of their heartbeat, or pay attention to certain facial muscles, etc, even though most people are not aware of them. (And obviously small children need to be trained to pay attention to many different bodily signals.)\n\n> \\[Clearly something was lost here - I'm saying that the claim that there is a disconnect between conscious sensation and tracking bodily damage is a difficult one to believe. And if there is any such connection, the reason that physical damage is negative is that it's beneficial.\\]\n\n(I see - yeah I phrased my tweet poorly.)\n\nI meant that pain would need to be experienced as a negative for consciousness to exist - otherwise it seems implausible that it would have evolved.\n\n**Rob Bensinger:** I felt like there were a lot of unstated premises here, so I wanted to hear what premises you were building in (eg, your concept of what 'pain' is).\n\nBut even if we grant everything, I think the only conclusion is \"pain is less positive than non-pain\", not \"pain is negative\".\n\n**David Manheim:** Yeah, I grant that the only conclusion I lead to is relative preference, not absolute value.\n\nBut for humans, I'm unsure that there is a coherent idea of valence distinct from our experienced range of sensation. Someone who's never missed a meal finds skipping lunch painful.\n\n**Rob Bensinger:** ? I think the idea of absolute valence is totally coherent for humans. There's such a thing as hedonic treadmills, but the idea of 'not experiencing a hedonic treadmill' isn't incoherent.\n\n**David Manheim:** Being unique only up to to linear transformations implies that utilities don't have a coherent notion of (psychological) valence, since you can always add some number to shift it. That's not a hedonic treadmill, it's about how experienced value is relative to other things.\n\n**Rob Bensinger:** 'There's no coherent idea of valence distinct from our experienced range of sensation' seems to imply that there's no difference between 'different degrees of horrible torture' and 'different degrees of bliss', as long as the organism is constrained to one range or the other.\n\nSeems very false!\n\n**David Manheim:** It's removed from my personal experience, but I don't think you're right. If you read Knut Hamsun's \"Hunger\", it really does seem clear that even in the midst of objectively painful experiences, people find happiness in slightly less pain.\n\nOn the other hand, all of us experience what is, historically, an unimaginably wonderful life. Of course, it's my inside-view / typical mind assumption, but we experience a range of experienced misery and bliss that seems very much comparable to what writers discuss in the past.\n\n**Rob Bensinger:** This seems like 'hedonic treadmill often happens in humans' evidence, which is wildly insufficient even for establishing 'humans will perceive different hedonic ranges as 100% equivalent', much less 'this is true for all possible minds' or 'this is true for all sentient animals'.\n\n\"even in the midst of objectively painful experiences, people find happiness in slightly less pain\" isn't even the right claim. You want 'people in objectively painful experiences can't comprehend the idea that their experience is worse, seem just as cheerful as anyone else, etc'\n\n**David Manheim:** \n\n> \\[This seems like 'hedonic treadmill often happens in humans' evidence, which is wildly insufficient even for establishing 'humans will perceive different hedonic ranges as 100% equivalent', much less 'this is true for all possible minds' or 'this is true for all sentient animals'.\\]\n\nAgreed - I'm not claiming it's universal, just that it seems at least typical for humans.\n\n> \\[\"even in the midst of objectively painful experiences, people find happiness in slightly less pain\" isn't even the right claim. You want 'people in objectively painful experiences can't comprehend the idea that their experience is worse, seem just as cheerful as anyone else, etc'\\]\n\nFlip it, and it seems trivially true - 'people in objectively wonderful experiences can't comprehend the idea that their experience is better, seem just as likely to be sad or upset as anyone else, etc'\n\n**Rob Bensinger:** I don't think that's true. E.g., I think people suffering from chronic pain acclimate a fair bit, but nowhere near completely. Their whole life just sucks a fair bit more; chronic pain isn't a happiness- or welfare-preserving transformation.\n\nMaybe people believe their experiences are a lot like other people's, but that wouldn't establish that humans (with the same variance of experience) really do have similar-utility lives. Even if you're right about your own experience, you can be wrong about the other person's in the comparison you're making.\n\n**David Manheim: ** \n\nAgreed - But I'm also unsure that there is any in-principle way to unambiguously resolve any claims about how good/bad relative experiences are, so I'm not sure how to move forward about discussing this.\n\n> \\[I also don't find it difficult to believe that a conscious part of a brain could 'learn which things cause bodily damage and tweak behavior to avoid those' without experiencing anything as 'bad' per se.\\]\n\nThat involves positing two separate systems which evidently don't interact that happen to occupy the same  substrate. I don't see how that's plausible in an evolved system.\n\n**Rob Bensinger:** Presumably you don't think in full generality 'if a conscious system X interacts a bunch with another system Y, then Y must also be conscious'. So what kind of interaction makes consciousness 'slosh over'?\n\nI'd claim that there are complicated systems in my own brain that have tons of causal connections to the rest of my brain, but that I have zero conscious awareness of.\n\n(Heck, I wouldn't be shocked if some of those systems are suffering right now, independent of 'my' experience.)\n\n**Jacy Anthis: ** \n\n> \\[But in that case we should be less confident, assuming chickens are conscious, that their consciousness is 'hooked up' to trivial-to-implement stuff like 'learning to avoid bodily damage at all'.\\]\n\nI appreciate you sharing this, Rob. FWIW you and Eliezer seem confused about consciousness in a very typical way, No True Scotsmanning each operationalization that comes up with vague gestures at ineffable qualia. But once you've dismissed everything, nothing meaningful is left.\n\n**Rob Bensinger:** I mean, my low-confidence best guess about where consciousness comes from is that it evolved in response to language. I'm not saying that it's impossible to operationalize 'consciousness'. But I do want to hear decompositions before I hear confident claims 'X is conscious'.\n\n**Jacy Anthis:** At least we agree on that front! I would extend that for 'X is not conscious', and I think other eliminativists like Brian Tomasik would agree that this is a huge problem in the discourse.\n\n**Rob Bensinger:** Yep, I agree regarding 'X is not conscious'.\n\n(Maybe I think it's fine for laypeople to be confident-by-default that rocks, electrons, etc are unconscious? As long as they aren't so confident they could never update, if a good panpsychism argument arose.)\n\n**Sam Rosen:** It's awfully suspicious that:\n\n*   Pigs in pain look and sound like what we would look and sound like if we were in pain.\n*   Pigs have a similar brain to us with similar brain structures.\n*   The parts of the brain that light up in pig's brain when they are in pain are the same as the parts of the brain that light up in our brains when we are in pain. (I think this is true, but could totally be wrong.)\n*   Pain plausibly evolved as a mechanism to deter receiving physical damage which pigs need just as much as humans.\n*   Pain feels primal and simple—something a pig could understand. It's not like counterfactual reasoning, abstraction, or complicated emotions like sonder.\n\nIt just strikes me as plausible that pigs can feel lust, thirst, pain and hunger—and humans merely evolved to learn how to talk about those things. It strikes me as less plausible that pigs unconsciously have mechanisms that control \"lust\", \"thirst,\" \"pain,\" and \"hunger\" and humans became the first species on Earth that made all those unconscious functional mechanisms conscious.\n\n(Like why did humans only make *those* unconscious processes conscious? Why didn't humans, when emerging into consciousness, become conscious of our heart regulation and immune system and bones growing?)\n\nIt's easier to evolve language and intelligence than it is to evolve language and intelligence PLUS a way of integrating and organizing lots of unconscious systems into a consciousness producing system where the attendant qualia of each subsystem incentivizes the correct functional response.\n\n**Rob Bensinger:** Why do you think pigs evolved qualia, rather than evolving to do those things without qualia? Like, why does evolution like qualia?\n\n**Sam Rosen:** I don't know if evolution likes qualia. It might be happy to do things unconsciously. But thinking non-human animals aren't conscious of pain or thirst or hunger or lust means adding a big step from apes to humans. Evolution prefers smooth gradients to big steps.\n\nMy last paragraph of my original comment is important for my argument.\n\n**Rob Bensinger:** My leading guess about where consciousness comes from is that it evolved in response to language.\n\nOnce you can report fine-grained beliefs about your internal state (including your past actions, how they cohere with your present actions, how this coherence is virtuous rather than villainous, how your current state and future plans are all the expressions of a single Person with a consistent character, etc.), there's suddenly a ton of evolutionary pressure for you to internally represent a 'global you state' to yourself, and for you to organize your brain's visible outputs to all cohere with the 'global you state' narrative you share with others; where almost zero such pressure exists before language.\n\nLike, a monkey that emits different screams when it's angry, hungry, in pain, etc. can freely be a Machiavellian reasoner: it needs to scream in ways that at least somewhat track whether it's *really* hungry (or in pain, etc.), or others will rapidly learn to distrust its signals and refuse to give aid. But this is a very low-bandwidth communication channel, and the monkey is free to have basically any internal state (incoherent, unreflective, unsympathetic-to-others, etc.) as long as it ends up producing cries in ways that others will take sufficiently seriously. (But not maximally seriously, since *never* defecting/lying is surely not going to be the equilibrium here, at least for things like 'I'm hungry' signals.)\n\nThe game really does change radically when you're no longer emitting an occasional scream, but are actually *constructing sentences that tell stories about your entire goddamn brain, history, future behavior, etc.*\n\n**Nell Watson:** So, in your argument, would it follow then that feral human children or profoundly autistic human beings cannot feel pain, because they lack language to codify their conscious experience?\n\n**Rob Bensinger:** Eliezer might say that? Since [he does think human babies aren't conscious](https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/), with very high confidence.\n\nBut my argument is evolutionary, not developmental. Evolution selected for consciousness once we had language (on my account), but that doesn't mean consciousness has to depend on language developmentally.",
      "plaintextDescription": "Below, I've collected some of my thoughts on consciousness. Topics covered (in the post and/or the comments below) include:\n\n * To what extent did subjective pain evolve as a social signal?\n * Why did consciousness evolve? What function(s) did it serve?\n * What would the evolutionary precursors of 'full' consciousness look like?\n * What sorts of human values are more or less likely to extend to unconscious things?\n * Is consciousness more like a machine (where there's a sharp cutoff between 'the machine works' or 'the machine doesn't'), or is it more like a basic physical property like mass (where there's a continuum from very small fundamental things that have small amounts of the property, all the way up to big macroscopic objects that have way more of the property)?\n * How should illusionism (the view that in an important sense we aren't conscious, but non-phenomenally 'appear' to be conscious) change our answers to the questions above?\n\n----------------------------------------\n\n\n1. Pain signaling\nIn September 2019, I wrote on my LW shortform:\n\n \n\nRolf Degen, summarizing part of Barbara Finlay's \"The neuroscience of vision and pain\":\n\n> Humans may have evolved to experience far greater pain, malaise and suffering than the rest of the animal kingdom, due to their intense sociality giving them a reasonable chance of receiving help.\n\nFrom the paper:\n\n> Several years ago, we proposed the idea that pain, and sickness behaviour had become systematically increased in humans compared with our primate relatives, because human intense sociality allowed that we could ask for help and have a reasonable chance of receiving it. We called this hypothesis ‘the pain of altruism’ [68]. This idea derives from, but is a substantive extension of Wall’s account of the placebo response [43]. Starting from human childbirth as an example (but applying the idea to all kinds of trauma and illness), we hypothesized that labour pains are more painful in humans so that we might get help, an ‘",
      "wordCount": 6339
    },
    "tags": [
      {
        "_id": "Q9ASuEEoJWxT3RLMT",
        "name": "Animal Ethics",
        "slug": "animal-ethics"
      },
      {
        "_id": "XSryTypw5Hszpa4TS",
        "name": "Consciousness",
        "slug": "consciousness"
      },
      {
        "_id": "LaDu5bKDpe8LxaR7C",
        "name": "Suffering",
        "slug": "suffering"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FBbHEjkZzdupcjkna",
    "title": "MIRI/OP exchange about decision theory",
    "slug": "miri-op-exchange-about-decision-theory-1",
    "url": null,
    "baseScore": 58,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2021-08-25T22:44:10.389Z",
    "contents": {
      "markdown": "Open Philanthropy's Joe Carlsmith and Nick Beckstead had a short conversation about [decision theory](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/zcPLNNw4wgBX5k8kQ) a few weeks ago with MIRI's Abram Demski and Scott Garrabrant (and me) and LW's Ben Pace. I'm copying it here because I thought others might find it useful.\n\nTerminology notes:\n\n*   CDT is [**causal decision theory**](https://plato.stanford.edu/entries/decision-causal/), the dominant theory among working decision theorists. CDT says to choose the action with the best causal consequences.\n*   EDT is **evidential decision theory**, CDT's traditional rival. EDT says to choose the action such that things go best *conditional* on your choosing that action.\n*   TDT is [**timeless decision theory**](http://intelligence.org/files/TDT.pdf), a theory proposed by Eliezer Yudkowsky in 2010. TDT was superseded by FDT/UDT because TDT fails on dilemmas like [counterfactual mugging](https://www.lesswrong.com/tag/counterfactual-mugging), refusing to pay the mugger.\n*   UDT is [**updateless decision theory**](https://www.lesswrong.com/tag/updateless-decision-theory), a theory proposed by Wei Dai in 2009. UDT in effect asks what action \"you would have pre-committed to without the benefit of any observations you have made about the universe\", and chooses that action.\n*   FDT is [**functional decision theory**](https://arxiv.org/abs/1710.05060), an umbrella term introduced by Yudkowsky and Nate Soares in 2017 to refer to UDT-ish approaches to decision theory.\n\n* * *\n\n**Carlsmith:** Anyone have an example of a case where FDT and updateless EDT give different verdicts?\n\n**Beckstead: ** Is smoking lesion an example?\n\nI haven't thought about how updateless EDT handles that differently from EDT.\n\n**Demski:** FDT is supposed to be an overarching framework for decision theories \"in the MIRI style\", whereas updateless EDT is a specific decision theory.\n\nIn particular, FDT may or may not be updateless.\n\nUpdateful FDT is basically TDT.\n\nNow, I generally claim it's harder to find examples where EDT differs from causal counterfactuals than people realize; eg, EDT and CDT [do the same thing on smoking lesion](https://www.alignmentforum.org/s/fgHSwxFitysGKHH56). So be aware that you're not going to get the \"standard view\" from me.\n\nHowever, TDT gets some problems wrong which UDT gets right, eg, counterfactual mugging.\n\nUpdateless FDT would not get this wrong, though; it appears to be all about the updatelessness.\n\nTo get EDT-type and CDT-type DTs to really differ, we have to go to [Troll Bridge]( https://www.lesswrong.com/posts/hpAbfXtqYC2BrpeiC/troll-bridge-5). EDT fails, FDT variants will often succeed.\n\n**Garrabrant:** I feel like it is not hard to come up with examples where updateful EDT and CDT differ ([XOR blackmail](https://intelligence.org/files/DeathInDamascus.pdf)), and for the updateless question, I think the field is small enough that whatever Abram says is the “standard view.”\n\nI think that to get EDT and CDT to differ updatelessly, you need to cheat and have the agent have some weird non-Bayesian epistemics (Bayesians get the tickle defense), so it is hard to construct formal examples.\n\nUnfortunately, all agents have weird non-Bayesian epistemics, so that doesn’t mean we get to just skip the question.\n\nMy “standard view” position is that EDT is obviously philosophically correct the way that Bayesianism is obviously philosophically correct; CDT is an uglier thing that gets the same answer in ideal conditions; but then [embeddedness](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh) gives you non-ideal conditions everywhere, and CDT is closer to being structured in a way that can handle getting the right answer in spite of having weird epistemics.\n\nMy non-standard-view answer is that hopefully the successors to the [Cartesian Frame](https://www.lesswrong.com/s/2A7rrZ4ySx6R8mfoT) ontology / [Factored Set](https://www.lesswrong.com/s/kxs3eeEti9ouwWFzr) ontology will make this question go away.\n\n**Bensinger:** Terminology/history side-note: Abram's right that \"FDT is supposed to be an overarching framework for decision theories 'in the MIRI style'\", but I don't think it's meant to be so overarching as to include TDT. I think the original intended meaning was basically 'FDT = UDT-ish approaches to decision theory'.\n\nFrom the comments on [](https://www.lesswrong.com/posts/2THFt7BChfCgwYDeA/let-s-discuss-functional-decision-theory:) [Let’s Discuss Functional Decision Theory](https://www.lesswrong.com/posts/2THFt7BChfCgwYDeA/let-s-discuss-functional-decision-theory):\n\n> My model is that 'FDT' is used in the paper instead of 'UDT' because:\n> \n> *   The name 'UDT' seemed less likely to catch on.\n> *   The term 'UDT' (and 'modifier+UDT') had come to refer to a bunch of very different things over the years. 'UDT 1.1' is a lot less ambiguous, since people are less likely to think that you're talking about an umbrella category encompassing all the 'modifier+UDT' terms; but it's a bit of a mouthful.\n> *   I've heard someone describe 'UDT' as \"FDT + a theory of anthropics\" -- i.e., it builds in the core idea of what we're calling \"FDT\" (\"choose by imagining that your (fixed) decision function takes on different logical outputs\"), plus a view to the effect that decisions+[probutilities](https://www.lesswrong.com/posts/TeYro2ntqHNyQFx8r/policy-approval) are what matter, and subjective expectations don't make sense. Having a name for the FDT part of the view seems useful for evaluating the subclaims separately.\n> \n> The FDT paper introduces the FDT/UDT concept in more CDT-ish terms (for ease of exposition), so I think some people have also started using 'FDT' to mean something like 'variants of UDT that are more CDT-ish', which is confusing given that FDT was originally meant to refer to the superset/family of UDT-ish views. Maybe that suggests that researchers feel more of a need for new narrow terms to fill gaps, since it's less often necessary in the trenches to crisply refer to the superset.\n> \n> \\[...\\]\n> \n> Nate says: \"The main datapoint that Rob left out: one reason we don't call it UDT (or cite Wei Dai much) is that Wei Dai doesn't endorse FDT's focus on causal-graph-style counterpossible reasoning; IIRC he's holding out for an approach to counterpossible reasoning that falls out of evidential-style conditioning on a logically uncertain distribution. (FWIW I tried to make the formalization we chose in the paper general enough to technically include that possibility, though Wei and I disagree here and that's definitely not where the paper put its emphasis. I don't want to put words in Wei Dai's mouth, but IIRC, this is also a reason Wei Dai declined to be listed as a co-author.)\"\n\nFootnote: a philosopher might say 'FDT is an overarching framework for *approaches to decision theory* in the MIRI style'; and they might be happy calling FDT \"a decision theory\", in the same sense that 'CDT' and 'EDT' are deemed decision theories even though they've been interpreted and operationalized in dozens of different ways by philosophers.\n\n(The FDT paper calls FDT a 'decision theory' rather than 'a family of decision theories' because it's written for mainstream philosophers.)\n\nAs a matter of terminology, I think MIRI-cluster people are more likely to (e.g.) see 10 distinct decision algorithms and group them in ~8 distinct 'decision theories' where a philosopher might group them into ~2 distinct 'decision theories'. 🤷‍♀️\n\n**Carlsmith: ** Thanks for the comments, all. My hazy understanding had been something like: updateful CDT and updateful EDT are both focused on evaluating actions, but CDT evaluates them using counterfactuals/do-operators or some such, whereas EDT evaluates them using conditionals.\n\nThe difference that updatelessness makes is that you instead evaluate overall policies (mappings from inputs to outputs) relative to some prior, and act on that even after you’ve “learned more.” The CDT version of this, I thought, would do something like counterfactual/do-operator type reasoning about what sort of policy to have — and this sounded a lot like FDT to me, so I’ve been basically rounding FDT off to “updateless CDT.” The EDT version, I imagined, would do something like conditional reasoning about what sort of policy to have. Thus, the whiteboard diagram below.\n\n![](https://lh5.googleusercontent.com/m3r6bi1UwTSIu9gyb7up-2PEN1Mt1goO-At0eMoqzlJ_DzpUlZ0LcM58SU31-PbM5xLMucow9Sk4fVGkd5s9DmJty7IDrGVuHlmPk5jkKfVRctlQYGXsbdov1X8MSBLoTF-6BYe5=s0)\n\nOn this framework, I’m a bit confused by the idea that FDT is a neutral over-arching term for MIRI-style decision theory, which can be updateless or not. For example, my impression from the paper was that FDT was supposed to be updateless in the sense of e.g. paying up in counterfactual mugging. and my sense was that FDT was taking a stand on the “counterfactuals vs. conditionals” at least to some extent, insofar as it was using counterfactuals/do-operators on causal graphs. But sounds like I’m missing some of the relevant distinctions here, and/or just mis-remembering what the paper was committed to (this is just me speaking from impressions skimming through the less-wrong-ish literature on this stuff).\n\n**Garrabrant:** I think that there is this (obvious to LessWrongers, because it is deeply entangled with the entire LessWrong philosophy) ontology in which “I am an algorithm” rather than “I am a physical object.” I think that most decision theorists haven’t really considered this ontology. I mostly view FDT (the paper) as a not-fully-formal attempt to bridge that inferential difference and argue for identifying with your algorithm.\n\nI view it as a 2x2x2 cube for (algorithm vs physical), (CDT vs EDT), (updateless vs updateful).\n\nAnd FDT is mostly about the first axis, because that is the one people are being stupid about. I think that the general MIRI-LW consensus is that the third axis should go on the updateless side, although there is also some possibility of preferring to build tools that are not updateless/do not identify with their algorithm (for the purpose of enslaving them 🙂).\n\n**Pace:  **😆\n\n**Garrabrant:  **However, the CDT vs EDT axis is more controversial, and maybe the actual answer looks more like “the question doesn’t really make sense once you identify with your algorithm correctly”.\n\nOne view I partially hold is that updateless-algorithm EDT is correct for an ideal reasoner, but all three axes represent tools to get approximately the right answer in spite of not being an ideal reasoner. \n\nWhere naively pretending you are an ideal reasoner leads to catastrophe.\n\nAnd this does not mean they are just hacks. Not being an ideal reasoner is part of being an embedded agent.\n\n(Anyway, I think the paper may or may not make some choices about the other axes, but the heart of FDT is about the algorithm question.)\n\n**Pace:** Is 'ideal embedded reasoner' a wrong concept?\n\n**Garrabrant:** It depends on your standards for “ideal.” I doubt we will get anywhere near as ideal as “Bayesianism/Solomonoff induction.”\n\n**Carlsmith:** OK, so is this getting closer? EDT vs. CDT: evaluate X using conditionals vs. counterfactuals/do-operators/some causation-like thing.\n\nAlgorithm vs. Physical: X is your algorithm vs. X is something else. (I would’ve thought: the action? In which case, would this reduce to something like the policy vs. action? Not sure if evaluating policies on an “I am a physical object” view ends up different from treating yourself as an algorithm.)\n\nUpdateful vs. updateless: evaluate X using all your information, vs. relative to some prior.\n\n**Bensinger:** Agreed the FDT paper was mainly about the algorithm axis. I think the intent behind the paper was to make FDT = 'yes algorithm, yes updateless, agnostic about counterfactuals-vs-conditionals', but because the paper's goal was \"begin to slowly bridge the inferential gap between mainstream philosophers and MIRI people\" it brushed a lot of the interesting details under the rug.\n\nAnd we thought it would be easier to explain LW-style decision theory using a counterfactual-ish version of FDT than using a conditional-ish version of FDT, and I think the paper failed to make it clear that we wanted to allow there to be conditional-ish FDTs.\n\n**Demski:  **\n\n> Algorithm vs. Physical: X is your algorithm vs. X is something else. (I would’ve thought: the action?\n\nI don't agree with this part, quite. You can't change your whole algorithm, so it doesn't really make sense to evaluate different possible algorithms. You do, however, have control over your policy (which, at least roughly, is \"the input-to-output mapping implemented by your algorithm\").\n\nAlgorithm-vs-physical isn't quite \"evaluate possible policies\", however. I think a central insight is \"think as if you have control over all your instances, rather than just one instance\". This is commonly associated with EDT (because a causal decision theorist can \"interpret\" EDT as mistakenly thinking it has control over everything correlated with it, including other instances -- the you-are-your-algorithm insight says this is actually good, if perhaps not quite the right reasoning to use). So it's possible for someone to heavily endorse the \"you are your algorithm\" insight without grokking the \"act as if you control your policy\" idea.\n\n(Such a person might get Newcomb right but be confused about how to handle Transparent Newcomb, a state which I think was common on LW at one point??)\n\n(Transparent Newcomb is super confusing if you don't get the action/policy distinction, because Omega is actually choosing based on your policy -- what you *would* do *if* you saw a full box. But that's not easy to see if you're used to thinking about actions, to the point where you can easily think Transparent Newcomb isn't well-defined, or have other confusions about it.)\n\nBut I think TDT did have *both* the \"you control your instances\" insight *and* the \"you control your policy\" insight, just *not* coupled with the actual updatelessness part!\n\nSo if we really wanted to, we could make a 3x2x2, with the \"alg vs physical\" split into:\n\n-\\> I am my one instance (physical); evaluate actions (of one instance; average over cases if there is anthropic uncertainty)\n\n-\\> I am my instances: evaluate action (for all instances; no worries about anthropics)\n\n-\\> I am my policy: evaluate policies\n\n> is 'ideal embedded reasoner' a wrong concept?\n\nA [logical inductor](https://intelligence.org/2016/09/12/new-paper-logical-induction/) is ideal in the specific sense that it has some theoretical guarantees which we could call \"rationality guarantees\", and embedded in the sense that with a large (but finite) computer you could actually run it. I think this scales: we can generally talk about bounded rationality notions which can actually apply to actual algorithms, and these rationality conditions are \"realizable ideals\" in some sense.\n\nThe true \"ideal\" rationality should be capable of giving sound advice to embedded agents. But the pre-existing concepts of rationality are very far from this. So yeah, I think \"Scott's Paradox\" (\"updateless EDT is right for ideal agents, but pretending you're an ideal agent can be catastrophic, so in some cases you should not follow the advice of ideal DT\") is one which should dissolve as we get better concepts.\n\n> Updateful vs. updateless: evaluate X using all your information, vs. relative to some prior\n\nAnd this is a basic problem with UDT: which prior? How much information should we use / not use?\n\nDo realistic embedded agents even \"have a prior\"?\n\nOr just, like, a sequence of semi-coherent belief states?\n\n> On this framework, I’m a bit confused by the idea that FDT is a neutral over-arching term for MIRI-style decision theory, which can be updateless or not. For example, my impression from the paper was that FDT was supposed to be updateless in the sense of e.g. paying up in counterfactual mugging.\n\nI currently see it as a \"reasonable view\" (it's been my view at times) that updatelessness is doomed, so we have to find other ways to achieve eg paying up in counterfactual mugging. It still points to something about \"MIRI-style DT\" to say \"we want to pay up in counterfactual mugging\", even if one does not endorse updatelessness as a principle.\n\nSo I see all axes *except* the \"algorithm\" axis as \"live debates\" -- basically anyone who has thought about it very much seems to agree that you control \"the policy of agents who sufficiently resemble you\" (rather than something more myopic like \"your individual action\"), but there are reasonable disagreements to be had about updatelessness and counterfactuals.\n\n**Beckstead:** One thing I find confusing here is how to think about the notion of \"sufficiently resemble.\" E.g., how would I in principle estimate how many more votes go to my favored presidential candidate in a presidential election (beyond the standard answer of \"1\")?\n\n(I have appreciated this discussion of the 3 x 2 x 2 matrix. I had previously been thinking of it in the 2 x 2 terms of CDT vs. EDT and updateless/updateful.)\n\n**Demski:  **\n\n> One thing I find confusing here is how to think about the notion of \"sufficiently resemble.\" E.g., how would I in principle estimate how many more votes go to my favored presidential candidate in a presidential election (beyond the standard answer of \"1\")?\n\nMy own answer would be the EDT answer: how much does your decision correlate with theirs? Modulated by ad-hoc updatelessness: how much does that correlation change if we forget \"some\" relevant information? (It usually increases a lot.)\n\nFor voting in particular, *if* these esoteric DT considerations would change my answer, then they usually *wouldn't*, actually (because if the DT is important enough in my computation, then I'm part of a very small reference class of voters, and so, should mostly act like it's just my one vote anyway).\n\nBut I think this line of reasoning might actually underestimate the effect for subtle reasons I won't get into (related to [Agent Simulates Predictor](https://www.lesswrong.com/posts/q9DbfYfFzkotno9hG/example-decision-theory-problem-agent-simulates-predictor)).\n\n**Beckstead:** Cool, thanks.\n\n**Carlsmith:** For folks who think that CDT and EDT basically end up equivalent in practice, does that mean updateful EDT two-boxes in non-transparent newcomb, and you need to appeal to updatelessness to get one-boxing?\n\nAnyone have a case that differentiates between (a) updateful, but evaluates policies, and (b) updateless?\n\n**Demski:** EDT might one-box at first due to simplicity priors making it believe its actions are correlated with similar things (but then again, it might not; depends on the prior), but eventually it'll learn the same thing as CDT.\n\nNow, that doesn't mean it'll two-box. If Omega is a perfect predictor (or more precisely, if it knows the agent's action better than the agent itself), EDT will learn so, and one-box. And CDT will do the same (under some potentially contentious assumptions about CDT learning empirically) because no experiments will be able to show that there's no causal relationship.\n\nOn the other hand, if Omega is imperfect (more precisely, if its predictions are worse than or equal to the agent's own), EDT will learn to two-box like CDT, because its knowledge about its own action \"screens off\" the probabilistic relationship.\n\n> Anyone have a case that differentiates between (a) updateful, but evaluates policies, and (b) updateless?\n\nCounterfactual mugging!",
      "plaintextDescription": "Open Philanthropy's Joe Carlsmith and Nick Beckstead had a short conversation about decision theory a few weeks ago with MIRI's Abram Demski and Scott Garrabrant (and me) and LW's Ben Pace. I'm copying it here because I thought others might find it useful.\n\nTerminology notes:\n\n * CDT is causal decision theory, the dominant theory among working decision theorists. CDT says to choose the action with the best causal consequences.\n * EDT is evidential decision theory, CDT's traditional rival. EDT says to choose the action such that things go best conditional on your choosing that action.\n * TDT is timeless decision theory, a theory proposed by Eliezer Yudkowsky in 2010. TDT was superseded by FDT/UDT because TDT fails on dilemmas like counterfactual mugging, refusing to pay the mugger.\n * UDT is updateless decision theory, a theory proposed by Wei Dai in 2009. UDT in effect asks what action \"you would have pre-committed to without the benefit of any observations you have made about the universe\", and chooses that action.\n * FDT is functional decision theory, an umbrella term introduced by Yudkowsky and Nate Soares in 2017 to refer to UDT-ish approaches to decision theory.\n\n----------------------------------------\n\nCarlsmith:  Anyone have an example of a case where FDT and updateless EDT give different verdicts?\n\nBeckstead:  Is smoking lesion an example?\n\nI haven't thought about how updateless EDT handles that differently from EDT.\n\nDemski:  FDT is supposed to be an overarching framework for decision theories \"in the MIRI style\", whereas updateless EDT is a specific decision theory.\n\nIn particular, FDT may or may not be updateless.\n\nUpdateful FDT is basically TDT.\n\nNow, I generally claim it's harder to find examples where EDT differs from causal counterfactuals than people realize; eg, EDT and CDT do the same thing on smoking lesion. So be aware that you're not going to get the \"standard view\" from me.\n\nHowever, TDT gets some problems wrong which UDT gets right, eg, count",
      "wordCount": 2898
    },
    "tags": [
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      },
      {
        "_id": "fM6pmeSEncbzxoGpr",
        "name": "Functional Decision Theory",
        "slug": "functional-decision-theory"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb28e",
        "name": "Causal Decision Theory",
        "slug": "causal-decision-theory"
      },
      {
        "_id": "MP8NqPNATMqPrij4n",
        "name": "Embedded Agency",
        "slug": "embedded-agency"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb28f",
        "name": "Evidential Decision Theory",
        "slug": "evidential-decision-theory"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "rbeFZr5pqCTCfKiW4",
    "title": "COVID/Delta advice I'm currently giving to friends",
    "slug": "covid-delta-advice-i-m-currently-giving-to-friends",
    "url": null,
    "baseScore": 61,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 31,
    "createdAt": null,
    "postedAt": "2021-08-24T03:46:09.053Z",
    "contents": {
      "markdown": "\\[Epistemic status: US-centric. gathering various impressions even though I haven't deeply investigated this topic and am not an expert, so it's all in one place and others can better use or critique the claims.\\]\n\n### **1\\. Figure things out for yourself**\n\nFigure out what risk level you're comfortable with, and use [microCOVID.org](https://www.microcovid.org/) to get a sense of what policy makes sense for you personally.\n\nConsider sharing your Fermi estimates with others (e.g., make a small Facebook group or chat for the purpose, or a LW thread if you're happy to do it publicly). This is a way to compare notes, get feedback, and share info without there needing to be a single big Policy Proposal For Everyone blog post people are passing around.\n\n### **2\\. Be much more wary of COVID when hospitals are full**\n\nKeep an eye on confirmed COVID-19 cases, hospitalizations, and deaths in your area, and put effort into avoiding catching or transmitting COVID-19 if it looks like hospitals in your area will be overloaded 2-4 weeks from now.\n\n### **3\\. Consider mostly not worrying about it**\n\nFor young healthy vaccinated people in places with relatively good health care and relatively high vaccination rates (e.g., the US), if it doesn't look like your local hospitals will be overloaded in the next few weeks, then I think COVID-19 is mostly not worth worrying about right now.\n\n(Likely exceptions: you're spending a lot of face-to-face time with an immunocompromised friend; you're visiting your seventy-five-year-old parents for the holidays in two weeks; etc.)\n\nI emphasize this point mostly because my friends are in left/liberal spaces, where I think social forces encourage people to voice their \"worry more\" thoughts and keep quiet about their \"worry less\" thoughts.\n\nThis makes it extra valuable to speak up about \"worry less\" when you think it's true. (Though, again, all of this is too individual-specific for a public blog post to be able to say much about the exactly correct level of \"worry\".)\n\nPeople in my social circle have mostly been taking large precautions throughout the pandemic. I see rationalist friends worrying a lot about whether it's OK to host get-togethers without rapid-testing all attendees (which I'd tentatively guess is not worth the trouble for the vast majority of gatherings). I see non-rationalist friends posting about how sad they are to not get to see friends again until things 'go back to normal'.\n\nWhat I don't see are rough quantitative arguments for why the benefits are worth the costs here, especially ones that take into account the importance of social distancing's emotional costs (cf. Scott Alexander's [Things I Learned Writing The Lockdown Post](https://astralcodexten.substack.com/p/things-i-learned-writing-the-lockdown)).\n\nI don't see explanations of what 'going back to normal' looks like, and why we should expect that to happen at all (cf. Alyssa Vance on [\"we should distance until X\"](https://www.facebook.com/alyssamvance/posts/10226542292453240)).\n\nDaniel Filan tells me that he initially locked down in response to COVID, but when he took the time to do a quick Fermi on the risks of COVID (around August 2020), he was surprised to find how costly his precautions were compared to their benefit:\n\n> \\[... M\\]y estimate \\[in late 2020\\] was that I should pay up to tens of cents to avoid a [uCOVID](https://www.microcovid.org/).\n> \n> I wasn't really modelling externalities well, and I'm still not totally sure how to do that right.\n> \n> \\[...\\] TBC, the Fermi was something like \"look up p(death | covid) given my age and sex, then estimate cost of other side effects as equal to cost of death, then add the cost of being normally sick for one week\".\n> \n> \\[T\\]hat being said, paying $100k to definitely not get covid seems pretty pricey.\n> \n> My current guess: the cost per uCOVID \\[in late 2020\\] came out to under 10c, and I rounded up for caution or something.\n\nSee also Connor Flexman's [Delta Strain: Fact Dump and Some Policy Takeaways](https://www.lesswrong.com/posts/9xd5ArGud8897q5on/delta-strain-fact-dump-and-some-policy-takeaways), which (with a bunch of caveats and uncertainty) estimates that given Delta's ubiquity, a healthy vaccinated thirty-year-old who would otherwise live a full life now loses something like the equivalent of 1 hour of life for every 1,000–5,000 microCOVIDs they get.\n\nI think the vaccines are remarkably effective (both vs. infection and vs. [severe-symptoms-given-infection](https://khub.net/web/phe-national/public-library/-/document_library/v2WsRK3ZlEig/view_file/479607329), which correlates with long COVID and [death](https://www.jefftk.com/p/delta-deaths-and-vaccinations) risks), and COVID wasn't a large risk for young healthy people in the first place (though Delta is a bigger risk than Alpha, ignoring effects of vaccination).\n\n(*Added Aug. 24*: I think COVID risk for young vaccinated people isn't that different from the risk you face from other widespread viruses. In particular, my impression is that many (if not all?) viruses cause long-term symptoms similar to long COVID. Though at least in unvaccinated people, it seems that COVID causes long-term issues more often than the typical virus.\n\nI know some people whose conclusion from this is \"I'll try to avoid *all* viruses\", including people who were taking such precautions pre-COVID. For healthy individuals, I don't have a strong view on whether that's more or less reasonable than mostly-ignoring this risk. But I *am* suspicious of the view that *only and exactly* COVID is worth worrying about.)\n\n### **4\\. If you do worry about it...**\n\n... I would especially prioritize doing things [outdoors](https://twitter.com/robbensinger/status/1280886236799012866).\n\nI also think in-person friend groups, events, and group houses should consider restructuring to cater to people with this or that risk tolerance. The typical especially at-risk person, IMO, shouldn't be trying to get a huge community of people to all match their needs. Nor should they be bunkering down to \"wait out COVID\" and avoiding in-person socializing until then (unless they *really* don't want or need in-person socializing). Rather, I'd propose planning around the assumption that things will stay at least this risky for years to come, and build connections with other people who want to keep their risk low on that timescale.\n\n(*Added Aug. 24*: The older you are, the higher your COVID risk generally is. Mayo Clinic says other [major risk factors](https://www.mayoclinic.org/diseases-conditions/coronavirus/in-depth/coronavirus-who-is-at-risk/art-20483301) are: obesity, diabetes, heart disease (including hypertension), lung problems, cancer, sickle cell anemia, weakened immune system, chronic kidney or liver disease, and Down syndrome.)\n\n### **5\\. Try to get triple-vaccinated**\n\nJ&J seems to be much less effective than Pfizer/Moderna, so people who received J&J should especially prioritize getting two mRNA shots ASAP. But mRNA recipients should probably also get a third shot if their second shot was 4-6 months ago.\n\nWill Eden re 'should people get a third shot, and if they had Pfizer/Moderna the first time should they try to have J&J this time around?':\n\n> The mechanisms for cell entry are a bit different, but they all end up using RNA to encode the S protein on the cell surface, so the nature of the immunity is basically the exact same mechanism. J&J is also going to immunize you against the vector virus, making it harder to receive future AAV vaccines/gene editing in the future, so personally I would stick to the mRNA vaccines only.\n> \n> From the Israeli data it seems like a third shot is probably most effective around 5 months later for little or no lapse in coverage? I would recommend it personally and have to others already. Doing third shots is in trials, as is mixing different vaccines, both appear to work well.\n\nIn response to Zvi Mowshowitz's [criticism of the Israeli data](https://www.lesswrong.com/s/rencyawwfr4rfwt5C/p/JiSwpipNX2avGEbcJ), Will adds:\n\n> I do think there’s a reasonable point about the denominator problem, and how outbreaks began in more heavily vaccinated areas thus skewing the results downwards. But he also admits this means there are enough vaccine breakthroughs to cause a pandemic!\n> \n> On the flip side, the discrepancy between protection against symptomatic COVID vs hospitalization/death I don’t agree with him on. I think a straightforward view of the data, especially the effectiveness of vaccine vs month of administration released by Israel, suggests that you need high circulating antibodies to prevent the infection from taking root at all, but prevention of severe disease is more reliant on cellular immunity and your body mounting a quick response after exposure, which is the benefit provided by vaccination. (On the other hand, this is why I want mucosal immunity - an optimal vaccine would even prevent infection + spread, and clearly the current vaccines don’t.)\n\n[Moderna seems to be better than Pfizer](https://www.reuters.com/business/healthcare-pharmaceuticals/moderna-may-be-superior-pfizer-against-delta-breakthrough-odds-rise-with-time-2021-08-09/), perhaps because it uses a higher dose.\n\nCNBC [reports](https://www.cnbc.com/2021/08/18/covid-booster-shots-us-to-begin-wide-distribution-of-third-vaccine-doses-next-month.html):\n\n> The United States will begin widely distributing Covid-19 booster shots next month as new data shows that vaccine protection wanes over time\\[...\\] U.S. agencies are preparing to offer booster shots to all eligible Americans beginning the week of Sept. 20, starting eight months after their second dose of Pfizer or Moderna’s vaccines.\n\nBut this seems too slow to me, given the next section. (My family and peers would nearly all have to wait till Nov/Dec/Jan.) So I still suggest trying to get a third shot sooner.\n\n### **6\\. Right now is the** ***safe*** **time**\n\nCOVID seems to be quite seasonal -- it spreads a lot more in the winter. To think about the coming months, I looked at COVID rates where I'm living:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/40706c2024a10c98a66eae5f728ff0bb63cc3732e8f1c29f.png)\n\nBased on this, I expect this coming Nov/Dec/Jan/Feb to be way worse again around SF. Except the baseline is much higher now, so I expect the elevated case rate to likewise be much higher.\n\nI think most people who track these things are looking at the current COVID rates, seeing they're a lot higher than in the past, and concluding that we should be buckling down temporarily right now.\n\nExcept that the rise is because Delta took over, and Delta is the new normal going forward. So if you're planning to throw some parties before March 2022, now is very likely the best time to do it. If you're planning to \"buckle down temporarily\" and \"temporarily\" means \"a few months\" rather than \"a few years\", you may want to reconsider whether things will actually look any better five or ten months from now.\n\nEyeballing NYC, it looks like things may be bad in some parts of the US even in May?\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5997206643cc3442f32ab05ac68f2cf3ad5b5df1c22ef3a4.png)\n\nBeyond seasonality, other things that may make things worse in the future include:\n\n*   Fading vaccine efficacy, if that's a thing. Even if you personally get more shots, others may not.\n    *   In general, my tentative default guess (if something like the status quo continues) is that people's willingness to get more COVID booster shots will drop off a lot over the coming months and years. (Because it will feel more like an annoying regular chore than like a miracle cure.)\n*   New COVID strains.\n*   Regulatory hurdles inhibiting and delaying vaccine roll-out.\n\nI'm not especially worried about long COVID, because I expect long COVID frequency and severity to track symptom severity pretty closely -- if vaccines protect against severe symptoms, they'll tend to reduce long COVID risk too.\n\nEven in the best-case scenario, there's a delay between \"new COVID strain evolves\" and \"new vaccine targeted at that strain rolls out,\" and a further delay before we get data on long COVID in people infected by that strain (either for vaccinated or unvaccinated people), which makes it harder to be confident about the risks.\n\nOr perhaps not. The future could hold a lot of things. But in my own planning, I'll be acting like most of the probability mass is on 'things stay about the same for a long time, or get worse'. I mostly am planning around the expectation that this is life going forward, and I'm not going to shut down my life indefinitely -- but I can plan around taking more risks during warmer months, and fewer risks during colder ones.\n\n### **7\\. Small exposures are better than large exposures, and** ***maybe*** **better than no exposure?**\n\nThis is a probably a relatively good time to get infected if the trend \"COVID evolves to be more dangerous and high-viral-load\" continues. Getting infected with a lower viral load is better, and may confer a lot of immunity against future variants.\n\nBy the same logic, if you expect to get infected with Delta regardless in the next few months (as a large portion of the US presumably will), it's better if your exposures tend to be in low-risk settings where you'll get a small initial viral load. (And it's better to get sick when you *know* hospitals won't be overloaded.)\n\n### **8\\. Be aware of possible signs of COVID**\n\n(*Added Aug. 24.*)\n\nAccording to [CBS](https://www.cbsnews.com/news/delta-variant-symptoms-covid-19/), an August CDC study found that the most common symptoms of Delta are still \"cough, headache, sore throat, myalgia \\[muscle pain\\], and fever\". Oddly, the COVID Symptom Study instead found that the most common Delta symptoms were runny nose, headache, sneezing, sore throat, and loss of smell.\n\nBut: don't assume people are COVID-free just because they're showing no symptoms. COVID is often asymptomatic (or the symptoms are hard to notice), and people with COVID transmit the illness a lot before they start showing symptoms.\n\nIf you might have COVID, self-isolate and try to get tested. PCR tests are the most reliable, but they have to be sent to a lab and normally take days to give back a result. Also, Connor Flexman [suspects](https://www.lesswrong.com/posts/9xd5ArGud8897q5on/delta-strain-fact-dump-and-some-policy-takeaways) that PCR tests may still have a ~40% false negative rate, which is much higher than usually advertised. Antigen tests have something like a ~50% false negative rate, but can be done fully at home and give much faster results (within a few minutes). LAMP tests like [Lucira](https://checkit.lucirahealth.com/) are also fast, and are in-between PCR and antigen tests in efficacy, according to Connor. (Flagging that these are very off-the-cuff numbers with minimal due diligence done; I'll revise them if the situation becomes clearer.)\n\nConsider taking zinc lozenges soon after COVID exposure or symptom onset. (They should taste bad if they're working.)\n\n### **9\\. Be prepared for if you get pretty sick**\n\nThings to buy now for if you get sick: Pedialyte or gatorade powder, acetaminophen or ibuprofen, oral thermometers, and a finger pulse oximeter.\n\nMaybe also: over-the-counter inhalers, a humidifier, mucinex/guaifenesin, pseudoephedrin.\n\n**10\\. Take care of yourself if you get sick**\n\n(*section in progress*)",
      "plaintextDescription": "[Epistemic status: US-centric. gathering various impressions even though I haven't deeply investigated this topic and am not an expert, so it's all in one place and others can better use or critique the claims.]\n\n \n\n\n1. Figure things out for yourself\nFigure out what risk level you're comfortable with, and use microCOVID.org to get a sense of what policy makes sense for you personally.\n\nConsider sharing your Fermi estimates with others (e.g., make a small Facebook group or chat for the purpose, or a LW thread if you're happy to do it publicly). This is a way to compare notes, get feedback, and share info without there needing to be a single big Policy Proposal For Everyone blog post people are passing around.\n\n \n\n\n2. Be much more wary of COVID when hospitals are full\nKeep an eye on confirmed COVID-19 cases, hospitalizations, and deaths in your area, and put effort into avoiding catching or transmitting COVID-19 if it looks like hospitals in your area will be overloaded 2-4 weeks from now.\n\n \n\n\n3. Consider mostly not worrying about it\nFor young healthy vaccinated people in places with relatively good health care and relatively high vaccination rates (e.g., the US), if it doesn't look like your local hospitals will be overloaded in the next few weeks, then I think COVID-19 is mostly not worth worrying about right now.\n\n(Likely exceptions: you're spending a lot of face-to-face time with an immunocompromised friend; you're visiting your seventy-five-year-old parents for the holidays in two weeks; etc.)\n\n \n\nI emphasize this point mostly because my friends are in left/liberal spaces, where I think social forces encourage people to voice their \"worry more\" thoughts and keep quiet about their \"worry less\" thoughts.\n\nThis makes it extra valuable to speak up about \"worry less\" when you think it's true. (Though, again, all of this is too individual-specific for a public blog post to be able to say much about the exactly correct level of \"worry\".)\n\nPeople in my social circle hav",
      "wordCount": 2300
    },
    "tags": [
      {
        "_id": "tNsqhzTibgGJKPEWB",
        "name": "Covid-19",
        "slug": "covid-19"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "yFJ7vCjefBxnTchmG",
    "title": "Outline of Galef's \"Scout Mindset\"",
    "slug": "outline-of-galef-s-scout-mindset",
    "url": null,
    "baseScore": 164,
    "voteCount": 84,
    "viewCount": null,
    "commentCount": 17,
    "createdAt": null,
    "postedAt": "2021-08-10T00:16:59.050Z",
    "contents": {
      "markdown": "Julia Galef's [*The Scout Mindset*](https://www.amazon.com/Scout-Mindset-People-Things-Clearly/dp/B07RP27XJP/) is superb.\n\nFor effective altruists, I think (based on the topic and execution) it's straightforwardly the #1 book you should use when you want to recruit new people to EA. It doesn't actually talk much about EA, but I think starting people on this book will result in an EA that's thriving more and doing more good five years from now, compared to the future EA that would exist if the top go-to resource were more obvious choices like *The Precipice*, *Doing Good Better*, the EA Handbook, etc.\n\nFor rationalists, I think the best intro resource is still *HPMoR* or *R:AZ*, but I think *Scout Mindset* is a great supplement to those, and probably a better starting point for people who prefer Julia's writing style over Eliezer's.\n\nI've made an outline of the book below, for my own reference and for others who have read it. If you don't mind spoilers, you can also use this to help decide whether the book's worth reading for you, though my summary skips a lot and doesn't do justice to Julia's arguments.\n\n### Introduction\n\n*   Scout mindset is \"the motivation to see things as they are, not as you wish they were\".\n*   We aren't perfect scouts, but we can improve. \"My approach has three prongs\":  \n      \n    1.  *Realize that truth isn't in conflict with your other goals.* People tend to overestimate how useful self-deception is for things like personal happiness and motivation, starting a company, being an activist, etc.\n    2.  *Learn tools that make it easier to see clearly.* Use various kinds of thought experiments and probabilistic reasoning, and rethink how you go about listening to the \"other side\" of an issue.\n    3.  *Appreciate the emotional rewards of scout mindset.* \"It's empowering to be able to resist the temptation to self-deceive, and to know that you can face reality even when it's unpleasant. There's an equanimity that results from understanding risk and coming to terms with the odds you're facing. And there's a refreshing lightness in the feeling of being free to explore ideas and follow the evidence wherever it leads\". Looking at lots of real-world examples of people who have exemplified scout mindset can make these positives more salient.\n\n### PART I: The Case for Scout Mindset\n\n***Chapter 1.*** **Two Types of Thinking**\n\n*   *\"Can I believe it?\" vs. \"must I believe it?\"* In directionally motivated reasoning, often shortened to \"motivated reasoning\", we disproportionately put our effort into finding evidence/reasons that support what we wish were true.\n*   *Reasoning as defensive combat.* Motivated reasoning, a.k.a. soldier mindset, \"doesn't *feel* like motivated reasoning from the inside\". But it's extremely common, as shown by how often we describe our reasoning in militaristic terms.\n*   *\"Is it true?\"* An alternative to (directionally) motivated reasoning is accuracy motivated reasoning, i.e., scout mindset.\n*   *Your mindset can make or break your judgment.* This stuff matters in real life, in almost every domain. Nobody is purely a scout or purely a soldier, but it's possible to become more scout-like.\n\n***Chapter 2.*** **What the Soldier is Protecting**\n\n*   \"\\[I\\]f scout mindset is so great, why isn't everyone already using it all the time?\" Three emotional reasons:\n    1.  *Comfort: avoiding unpleasant emotions.* This even includes comforting pessimism: \"there's no hope, so you might as well not worry about it.\"\n    2.  *Self-esteem: feeling good about ourselves.* Again, this can include ego-protecting negativity and avoiding \"'getting my hopes up'\".\n    3.  *Morale: motivating ourselves to do hard things.*\n*   And three social reasons:\n    1.  *Persuasion: convincing ourselves so we can convince others.*\n    2.  *Image: choosing beliefs that make us look good.* \"Psychologists call it impression management, and evolutionary psychologists call it signaling: When considering a claim, we implicitly ask ourselves, 'What kind of person would believe a claim like this, and is that how I want others to see me?'\"\n    3.  *Belonging: fitting in to your social groups.*\n*   \"We use motivated reasoning not because we don't know any better, but because we're trying to protect things that are vitally important to us\". So it's no surprise that e.g. 'training people in critical thinking' don't help change people's thinking. But while \"soldier mindset is often our default strategy for getting what we want\", it's not generally the best strategy available.\n\n***Chapter 3.*** **Why Truth is More Valuable Than We Realize**\n\n*   *We make unconscious trade-offs.* \"\\[T\\]he whole point of self-deception is that it's occurring beneath our conscious awareness. \\[...\\] So it's left up to our unconscious minds to choose, on a case-by-case basis, which goals to prioritize.\" Sometimes it chooses to be more soldier-like, sometimes more scout-like.\n*   *Are we rationally irrational?* I.e., are we good at \"unconsciously choosing *just enough* epistemic irrationality to achieve our \\[instrumental\\] social and emotional goals, without impairing our judgment too much\"? No. \"There are several major biases in our decision-making \\[... that\\] cause us to *overvalue* soldier mindset\":\n    1.  *We overvalue the immediate rewards of soldier mindset.* Present bias: we prefer small rewards now over large rewards later.\n    2.  *We underestimate the value of building scout habits.* Cognitive skills are abstract (and, again, have most of their benefits in the future), so they're harder to notice and care about.\n    3.  *We underestimate the ripple effects of self-deception.* These ripple effects are \"delayed and unpredictable\", which is \"exactly the kind of cost we tend to neglect\".\n    4.  *We overestimate social costs.*\n*   *An accurate map is more useful now.* Humans have more options now than we did tens of thousands of years ago, and more ability to improve our circumstances. \"So if our instincts undervalue truth, that's not surprising—our instincts evolved in a different world, one better suited to the soldier.\"\n\n### PART II: Developing Self-Awareness\n\n***Chapter 4.*** **Signs of a Scout**\n\n*   \"A key factor preventing us from being in scout mindset more frequently is our conviction that we're already in it.\" Examples of \"things that make us feel like scouts even when we're not\":\n    1.  *Feeling objective doesn't make you a scout.*\n    2.  *Being smart and knowledgeable doesn't make you a scout.* On ideologically charged questions, learning more tends to make people more polarized; and even scientists studying cognitive biases have a track record of exhibiting soldier mindset.\n*   *Actually practicing scout mindset makes you a scout.* \"The test of scout mindset isn't whether you see yourself as the kind of person who \\[changes your mind in response to evidence, is fair-minded, etc. ...\\] It's whether you can point to concrete cases in which you did, in fact, do those things. \\[...\\] The only real sign of a scout is whether you act like one.\" Behavioral cues to look for:\n    1.  *Do you tell other people when you realize they were right?*\n    2.  *How do you react to personal criticism?* \"Are there examples of criticism you've acted upon? Have you rewarded a critic (for example, by promoting him)? Do you go out of your way to make it easier for other people to criticize you?\"\n    3.  *Do you ever prove yourself wrong?*\n    4.  *Do you take precautions to avoid fooling yourself?* E.g., \"Do you avoid biasing the information you get?\" and \"\\[D\\]o you decide ahead of time what will count as a success and what will count as a failure, so you're not tempted to move the goalposts later?\"\n    5.  *Do you have any good critics?* \"Can you name people who are critical of your beliefs, profession, or life choices who you consider thoughtful, even if you believe they're wrong? Or can you at least name reasons why someone might disagree with you that you would consider reasonable\\[...\\]?\"\n    6.  \"But the biggest sign of scout mindset may be this: Can you point to occasions in which you were in soldier mindset? \\[... M\\]otivated reasoning is our natural state,\" so if you never notice yourself doing it, the likeliest explanation is that you're not self-aware about it.\n\n***Chapter 5.*** **Noticing Bias**\n\n*   \"One of the essential tools in a magician's tool kit is a form of manipulation called forcing.\" The magician asks you to choose between two cards. \"If you point to the card on the left, he says, 'Okay, that one's yours.' If you point to the card on the right, he says, 'Okay, we'll remove that one.' \\[...\\] If you could see both of those possible scenarios at once, the trick would be obvious. But because you end up in only one of those worlds, you never realize.\"\n*   \"Forcing is what your brain is doing to get away with motivated reasoning while still making you feel like you're being objective.\" The Democratic voter doesn't notice that they're going easier on a Democratic politician than they would on a Republican, because the question \"How would I act if this politician were a Republican?\" isn't salient to them, or they're tricking themselves into thinking they'd apply the same standard.\n*   *A thought experiment is a peek into the counterfactual world.* \"You can't detect motivated reasoning in yourself just by scrutinizing your reasoning and concluding that it makes sense. You have to compare your reasoning to the way you *would have* reasoned in a counterfactual world, a world in which your motivations were different—would you judge that politician's actions differently if he was in the opposite party? \\[...\\] Would you consider that study's methodology sound if its conclusion supported your side? \\[...\\] Try to *actually imagine* the counterfactual scenario. \\[... D\\]on't simply formulate a verbal question for yourself. Conjure up the counterfactual world, place yourself in it, and observe your reaction.\" Five types of thought experiment:\n    1.  *The double standard test.* Am I judging one person/group by a standard I wouldn't apply to another person/group?\n    2.  *The outsider test.* \"Imagine someone else stepped into your shoes—what do you expect they would do in your situation?\" Or imagine that *you're* an outsider who just magically teleported into your body.\n    3.  *The conformity test.* \"If other people no longer held this view, would you still hold it?\"\n    4.  *The selective skeptic test.* \"Imagine this evidence supported the other side. How credible would you find it then?\"\n    5.  *The status quo bias test.* \"Imagine your current situation was no longer the status quo. Would you then actively choose it?\"\n*   Thought experiments on their own \"can't tell you what's true or fair or what decision you should make.\" But they allow you to catch your brain \"in the act of motivated reasoning,\" and take that into account as you work to figure out what's true.\n*   Beyond the specific thought experiments, the core skill of this chapter is \"a kind of self-awareness, a sense that your judgments are *contingent*—that what seems true or reasonable or fair or desirable can change when you mentally vary some features of the question that should have been irrelevant.\"\n\n***Chapter 6.*** **How Sure Are You?**\n\n*   *We like feeling certain.* \"Your strength as a scout is in your ability \\[...\\] to think in shades of gray instead of black and white. To distinguish the feeling of '95% sure' from '75% sure' from '55% sure'.\"\n*   *Quantifying your uncertainty*. For scouts, probabilities are predictions of how likely they are to be right. The goal is to be calibrated in the probabilities you assign.\n*   *A bet can reveal how sure you* really *are.* \"Evolutionary psychologist Robert Kurzban has an analogy\\[...\\] In a company, there's a board of directors whose role is to make the crucial decisions for the company—how to spend its budget, which risks to take, when to change strategies, and so on. Then there's a press secretary whose role it is to give statements\\[...\\] The press secretary makes *claims*; the board makes *bets*. \\[...\\] A bet is any decision in which you stand to gain or lose something of value, based on the outcome.\"\n*   *The equivalent bet test.* By comparing different bets and seeing when you prefer taking one vs. the other, or when they feel about the same, you can translate your feeling \"does X sound like a good bet?\" into probabilities.\n*   The core skill of this chapter is \"being able to tell the difference between the feeling of *making a claim* and the feeling of *actually trying to guess what's true*.\"\n\n### PART III: Thriving Without Illusions\n\n***Chapter 7.*** **Coping with Reality**\n\n*   *Keeping despair at bay.* Motivated reasoning is especially tempting in emergencies; but it's also especially dangerous in emergencies. In dire situations, it's essential to be able to keep despair at bay without distorting your map of reality. E.g., you can count your blessings, come to terms with your situation, or remind yourself that you're doing the best you can.\n*   *Honest vs. self-deceptive ways of coping.* Honest ways of coping with painful or difficult circumstances include:\n    1.  *Make a plan.* \"It's striking how much the urge to conclude 'That's not true' diminishes once you feel like you have a concrete plan for what you would do if the thing *were* true.\"\n    2.  *Notice silver linings.* \"You're recognizing a silver lining to the cloud, not trying to convince yourself the whole cloud is silver. But in many cases, that's all you need\".\n    3.  *Focus on a different goal.*\n    4.  *Things could be worse.*\n*   *Does research show that self-deceived people are happier?* No, the research quality is terrible.\n\n***Chapter 8.*** **Motivation Without Self-Deception**\n\n*   Using self-deception to motivate yourself is bad, because:\n    1.  *An accurate picture of your odds helps you choose between goals.*\n    2.  *An accurate picture of the odds helps you adapt your plan over time.*\n    3.  *An accurate picture of the odds helps you decide how much to stake on success.*\n*   *Bets worth taking.* \"\\[S\\]couts aren't motivated by the thought, 'This is going to succeed.' They're motivated by the thought, 'This is a bet worth taking.'\" Which bets are worth taking is a matter of their expected value.\n*   *Accepting variance gives you equanimity.* Expecting to always succeed is unrealistic, and will lead to unnecessary disappointments. \"Instead of being elated when your bets pay off, and crushed when they don't,\" try to get a realistic picture of the variance in bets and focus on ensuring your bets have high expected value.\n*   *Coming to terms with the risk.*\n\n***Chapter 9.*** **Influence Without Overconfidence**\n\n*   *Two types of confidence.* Epistemic confidence is \"how sure you are about what's true,\" while social confidence is self-assurance: \"Are you at ease in social situations? Do you act like you deserve to be there, like you're secure in yourself and your role in the group? Do you speak as if you're worth listening to?\" Influencing people requires social confidence, which people *conflate* with epistemic confidence.\n*   *People judge you on social confidence, not epistemic confidence.* Various studies show that judgments of competence are mediated by perceived social (rather than epistemic) confidence.\n*   *Two kinds of uncertainty.* People trust you less if you seem uncertain due to ignorance or inexperience, but not if you seem uncertain due to reality being messy and unpredictable. Three ways to communicate uncertainty without looking inexperienced or incompetent:\n    1.  *Show that uncertainty is justified.*\n    2.  *Give informed estimates.* \"Even if reality is messy and it's impossible to know the right answer with confidence, you can at least be confident in your analysis.\"\n    3.  *Have a plan.*\n*   *You don't need to promise success to be inspiring.* \"You can paint a picture of the world you're trying to create, or why your mission is important, or how your product has helped people, without claiming you're guaranteed to succeed. There are lots of ways to get people excited that don't require you to lie to others or to yourself.\"\n*   \"That's the overarching theme of these last three chapters: whatever your goal, there's probably a way to get it that doesn't require you to believe false things.\"\n\n### PART IV: Changing Your Mind\n\n***Chapter 10.*** **How to Be Wrong**\n\n*   *Change your mind a little at a time.* Superforecasters constantly revise their views in small ways.\n*   *Recognizing you were wrong makes you better at being right.* Most people, when they learn they were wrong, give excuses like \"I Was Almost Right\". Superforecasters instead \"reevaluate their process, asking, 'What does this teach me about how to make better forecasts?'\"\n*   *Learning domain-general lessons.* Even if your error is in a domain that seems unimportant to you, noticing your errors can teach domain-general lessons \"about how the world works, or how your own brain works, and about the kinds of biases that tend to influence your judgment.\" Or they can help you fully internalize a lesson you previously only believed in the abstract.\n*   *\"Admitting a mistake\" vs. \"updating\".* Being factually wrong about something doesn't necessarily mean you screwed up. Learning new information should usually be thought of in matter-of-fact terms, as an opportunity to update your beliefs—not as something humbling or embarrassing.\n*   *If you're not changing your mind, you're doing something wrong.* By default, you should be learning more over time, and changing your strategy accordingly.\n\n***Chapter 11.*** **Lean in to Confusion**\n\n*   Usually, \"we react to observations that conflict with our worldview by explaining them away. \\[...\\] We couldn't function in the world if we were constantly questioning our perception of reality. But especially when motivated reasoning is in play, we take it too far, shoehorning conflicting evidence into a narrative\" well past the point where it makes sense. \"This chapter is about how to resist the urge to dismiss details that don't fit your theories, and instead, allow yourself to be confused and intrigued by them, to see them as puzzles to be solved\".\n*   \"You don't know in advance\" what surprising and confusing observations will teach you. \"All too often, we assume the only two possibilities are 'I'm right' or 'The other guy is right'\\[...\\] But in many cases, there's an unknown unknown, a hidden 'option C,' that enriches our picture of the world in a way we wouldn't have been able to anticipate.\"\n*   *Anomalies pile up and cause a paradigm shift.* \"Acknowledge anomalies, even if you don't yet know how to explain them, and even if the old paradigm still seems correct overall. Maybe they'll add up to nothing in particular. Maybe they just mean that reality is messy. But maybe they're laying the groundwork for a big change of view.\"\n*   *Be willing to stay confused.*\n\n***Chapter 12.*** **Escape Your Echo Chamber**\n\n*   *How* not *to learn from disagreement.* Listening to the \"other side\" usually makes people *more* polarized. \"By default, we end up listening to people who initiate disagreements with us, as well as the public figures and media outlets who are the most popular representatives of the other side.\" But people who initiate disagreements tend to be unusually disagreeable, and popular representatives of an ideology are often \"ones who do things like cheering for their side and mocking or caricaturing the other side—i.e., you\". To learn from disagreement:\n    1.  *Listen to people you find reasonable.*\n    2.  *Listen to people you share intellectual common ground with.*\n    3.  *Listen to people who share your goals.*\n*   *The problem with a \"team of rivals.\"* \"Dissent isn't all that useful from people you don't respect or from people who don't even share enough common ground with you to agree that you're supposed to be on the same team.\"\n*   *It's harder than you think.* \"We assume that if both people are basically reasonable and arguing in good faith, then getting to the bottom of a disagreement should be straightforward\\[...\\] When things don't play out that way \\[...\\] everyone gets frustrated and concludes the others must be irrational.\" But even under ideal conditions, learning from disagreements is still hard, e.g., because:\n    1.  *We misunderstand each other's views.*\n    2.  *Bad arguments inoculate us against good arguments.*\n    3.  *Our beliefs are interdependent—changing one requires changing others.*\n\n### PART V: Rethinking Identity\n\n***Chapter 13.*** **How Beliefs Become Identities**\n\n*   *What it means for something to be part of your identity.* Criticizing part of someone's identity tends to spark passionate, combative, and defensive reactions.  Two things that tend to turn a belief into an identity:\n    1.  *Feeling embattled.*\n    2.  *Feeling proud.*\n*   *Signs a belief might be an identity.*\n    1.  *Using the phrase \"I believe\".*\n    2.  *Getting annoyed when an ideology is criticized.*\n    3.  *Defiant language.*\n    4.  *A righteous tone.*\n    5.  *Gatekeeping.*\n    6.  *Schadenfreude.*\n    7.  *Epithets.*\n    8.  *Having to defend your view.*\n*   \"Identifying with a belief makes you feel like you have to be ready to defend it, which motivates you to focus your attention on collecting evidence in its favor. Identity makes you reflexively reject arguments that feel like attacks on you or on the status of your group. \\[...\\] And when a belief is part of your identity it becomes far harder to change your mind\\[.\\]\"\n\n***Chapter 14.*** **Hold Your Identity Lightly**\n\n*   *What it means to hold your identity lightly.* Rather than trying to have no identities, you should try to \"keep those identities from colonizing your thoughts and values. \\[...\\] Holding your identity lightly means thinking of it in a matter-of-fact way, rather than as a central source of pride and meaning in your life. It's a description, not a flag to be waved proudly.\"\n*   *Could you pass an ideological Turing test?* Passing means explaining an ideology \"*as a believer would*, convincingly enough that other people couldn't tell the difference between you and a genuine believer\". The ideological Turing test tests your knowledge of the other side's beliefs, but \"it also serves as an emotional test: Do you hold your identity lightly enough to be able to avoid caricaturing your ideological opponents?\"\n*   *A strongly held identity prevents you from persuading others.*\n*   *Understanding the other side makes it possible to change minds.*\n*   *Is holding your identity lightly compatible with activism?* Activists usually \"face trade-offs between identity and impact,\" and holding your identity lightly can make it easier to focus on the highest-impact options.\n\n***Chapter 15.*** **A Scout Identity**\n\n*   *Flipping the script on identity.* Identifying as a truth-seeker can make you a better scout.\n*   *Identity makes hard things rewarding.* When you act like a scout, you can take pride and satisfaction in living up to your values. This short-term reward helps patch our bias for short-term rewards, which normally favors soldier mindset.\n*   *Your communities shape your identity.* \"\\[I\\]n the medium-to-long term, one of the biggest things you can do to change your thinking is to change the people you surround yourself with.\"\n*   *You can choose what kind of people you attract.* You can't please everyone, so \"you might as well aim to please the kind of people you'd most like to have around you, people who you respect and who motivate you to be a better version of yourself\".\n*   *You can choose your communities online.*\n*   *You can choose your role models.*",
      "plaintextDescription": "Julia Galef's The Scout Mindset is superb.\n\nFor effective altruists, I think (based on the topic and execution) it's straightforwardly the #1 book you should use when you want to recruit new people to EA. It doesn't actually talk much about EA, but I think starting people on this book will result in an EA that's thriving more and doing more good five years from now, compared to the future EA that would exist if the top go-to resource were more obvious choices like The Precipice, Doing Good Better, the EA Handbook, etc.\n\nFor rationalists, I think the best intro resource is still HPMoR or R:AZ, but I think Scout Mindset is a great supplement to those, and probably a better starting point for people who prefer Julia's writing style over Eliezer's.\n\nI've made an outline of the book below, for my own reference and for others who have read it. If you don't mind spoilers, you can also use this to help decide whether the book's worth reading for you, though my summary skips a lot and doesn't do justice to Julia's arguments.\n\n \n\n\nIntroduction\n * Scout mindset is \"the motivation to see things as they are, not as you wish they were\".\n * We aren't perfect scouts, but we can improve. \"My approach has three prongs\":\n   \n   \n   1. Realize that truth isn't in conflict with your other goals. People tend to overestimate how useful self-deception is for things like personal happiness and motivation, starting a company, being an activist, etc.\n   2. Learn tools that make it easier to see clearly. Use various kinds of thought experiments and probabilistic reasoning, and rethink how you go about listening to the \"other side\" of an issue.\n   3. Appreciate the emotional rewards of scout mindset. \"It's empowering to be able to resist the temptation to self-deceive, and to know that you can face reality even when it's unpleasant. There's an equanimity that results from understanding risk and coming to terms with the odds you're facing. And there's a refreshing lightness in the feeling of bei",
      "wordCount": 4067
    },
    "tags": [
      {
        "_id": "4Kcm4etxAJjmeDkHP",
        "name": "Book Reviews / Media Reviews",
        "slug": "book-reviews-media-reviews"
      },
      {
        "_id": "8SfkJYYMe75MwjHzN",
        "name": "Summaries",
        "slug": "summaries"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Wap8sSDoiigrJibHA",
    "title": "Garrabrant and Shah on human modeling in AGI",
    "slug": "garrabrant-and-shah-on-human-modeling-in-agi",
    "url": null,
    "baseScore": 60,
    "voteCount": 24,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2021-08-04T04:35:11.225Z",
    "contents": {
      "markdown": "This is an edited transcript of a conversation between Scott Garrabrant (MIRI) and Rohin Shah (DeepMind) about whether researchers should focus more on approaches to AI alignment that don’t require highly capable AI systems to do much human modeling. CFAR’s Eli Tyre facilitated the conversation.\n\nTo recap, and define some terms:\n\n*   The [alignment problem](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ) is the problem of figuring out \"how to develop sufficiently advanced machine intelligences such that running them produces good outcomes in the real world\" ([outcome alignment](https://arbital.com/p/ai_alignment/)) or the problem of building powerful AI systems that are trying to do what their operators want them to do ([intent alignment](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)).\n*   In 2016, Hadfield-Mennell, Dragan, Abbeel, and Russell proposed that we think of the alignment problem in terms of “[Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137)” (CIRL), a framework where the AI system is initially uncertain of its reward function, and interacts over time with a human (who knows the reward function) in order to learn it.\n*   In 2016-2017, Christiano proposed “[Iterated Distillation and Amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)” (IDA), an approach to alignment that involves [iteratively training AI systems](https://openai.com/blog/amplifying-ai-training/) to learn from human experts assisted by AI helpers. In 2018, Irving, Christiano, and Amodei proposed [AI safety via debate](https://openai.com/blog/debate/), an approach based on similar principles.\n*   In early 2019, Scott Garrabrant and DeepMind’s Ramana Kumar argued in “[Thoughts on Human Models](https://www.lesswrong.com/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models)” that we should be “cautious about AGI designs that use human models” and should “put more effort into developing approaches that work well in the absence of human models”.\n*   In early February 2021, Scott and Rohin [talked more about human modeling](https://www.lesswrong.com/posts/kdGSTBj3NA2Go3XaE/2019-review-voting-results?commentId=p54HiB7gmvRSqPTge#comments) and decided to have the real-time conversation below.\n\nYou can find a recording of the Feb. 28 discussion below (sans Q&A) [**here**](https://www.youtube.com/watch?v=tSQWMNQBtAg).\n\n1\\. IDA, CIRL, and incentives\n-----------------------------\n\n**Eli:  **I guess I want to first check what our goal is here. There was some stuff that happened online. Where are we according to you guys?\n\n**Scott:  **I think Rohin spoke last and I think I have to reload everything. I'm coming at it fresh right now. \n\n**Eli:  **Yeah. So we'll start by going from “What are the positions that each of you were thinking about?” But I guess I'm most curious about: from each of your perspectives, what would a win of this conversation be like? What could happen that you would go away and be like, \"Yes, this was a success.\"\n\n**Rohin:  **I don't know. I generally feel like I want to have more models of what AI safety should look like, and that's usually what I'm trying to get out of conversations with other people. I also don't feel like we have even narrowed down on what the disagreement is yet. So, first goal would be to figure that out.\n\n**Eli:  **Definitely. Scott?\n\n**Scott:**  Yeah. So, I have two things. One is that I feel frustrated with the relationship between the AI safety community and the human models question. I mean, I feel like I haven't done a good job arguing about it or something. And so on one side, I'm trying to learn what the other position is because I'm stuck there.\n\nThe other thing is that I feel especially interested in this conversation now because I feel like some of my views on this have recently shifted. \n\nMy worry about human-modeling systems has been that modeling humans is in some sense \"close\" to behaviors like deception. I still see things that way, but now I have a clearer idea of what the relevant kind of “closeness” is: human-modeling is close to unacceptable behavior on some measure of closeness that involves your ability to observe the system’s internals and figure out whether the system is doing the intended thing or doing something importantly different.\n\nI feel like this was an update to my view on the thing relative to how I thought about it a couple of years ago, and I want similar updates.\n\n**Eli:  **Awesome. As I'm thinking about it, my role is to try and make this conversation go well, according to your goals.\n\nI want to encourage both of you to take full responsibility for making the conversation go well as well. Don't have the toxic respect of, \"Ah, someone else is taking care of it.\" If something seems interesting to you, definitely say so. If something seems boring to you, say so. If there's a tack that seems particularly promising, definitely go for that. If I recommend something you think is stupid, you should say, \"That's a stupid thing.\" I will not be offended. That is, in fact, helpful for me. Sound good?\n\nAll right. Scott, do you want to start with your broad claim?\n\n**Scott:  **Yeah. So, I claim that it is plausible that the problem of aligning a super capable system that is working in some domain like physics is significantly easier than aligning a super capable system that's working in contexts that involve modeling humans. Either contexts involving modeling humans because they're working on inherently social tasks, or contexts involving modeling humans because part of our safety method involves modeling humans.\n\nAnd so I claim with moderate probability that it's significantly easier to align systems that don’t do any human-modeling. I also claim, with small but non-negligible probability, that one can save the world using systems that don’t model humans. Which is not the direction I want the conversation to really go, because I think that's not really a crux, because I don't expect my probability on “non-human-modeling systems can be used to save the world” to get so low that I conclude researchers shouldn’t think a lot about how to align and use such systems.\n\nAnd I also have this observation: most AI safety plans feel either adjacent to a Paul Christiano IDA-type paradigm or adjacent to a Stuart Russell CIRL-type paradigm.\n\nI think that two of the main places that AI safety has pushed the field have been towards human models, either because of IDA/debate-type reasons or because we want to not specify the goal, we want it to be in the human, we want the AI to be trying to do \"what we want” in quotation marks, which requires a pointer to modeling us.\n\nAnd I think that plausibly, these avenues are mistakes. And all of our attention is going towards them.\n\n**Rohin:  **Well, I can just respond to that maybe. \n\nSo I think I am with you on the claim that there are tasks which incentivize more human-modeling. And I wouldn't argue this with confidence or anything like that, but I'm also with you that plausibly, we should not build AI systems that do those tasks because it's close to manipulation of humans.\n\nThis seems pretty distinct from the algorithms that we use to build the AI systems. You can use iterated amplification to build a super capable physics AI system that doesn't know that much about humans and isn't trying to manipulate them.\n\n**Scott:  **Yeah. So you're saying that the IDA paradigm is sufficiently divorced from the dangerous parts of human modeling. And you're not currently making the same claim about the CIRL paradigm?\n\n**Rohin:  **It depends on how broadly you interpret the CIRL paradigm. The version where you need to use a CIRL setup to infer all of human values or something, and then deploy that agent to optimize the universe—I'm not making the claim for that paradigm. I don't like that paradigm. To my knowledge, I don't think Stuart likes that paradigm. So, yeah, I'm not making claims about that paradigm.\n\nI think in the world where we use CIRL-style systems to do fairly bounded, narrow tasks—maybe narrow is not the word I want to use here—but you use that sort of system for a specific task (which, again, might only involve reasoning about physics), I would probably make the same claim there, yes. I have thought less about it.\n\nI might also say I'm not claiming that there is literally no incentive to model humans in any of these cases. My claim is more like, \"No matter what you do, if you take a very broad definition of “incentive,” there will always be an incentive to model humans because you need some information about human preferences for the AI system to do a thing that you like.\"\n\n**Scott:  **I think that it's not for me about an incentive to model humans and more just about, is there modeling of humans at all? \n\nWell, no, so I'm confused again, because... Wait, you're saying there's not an incentive to model humans in IDA?\n\n**Rohin:  **It depends on the meaning of the word “incentive.”\n\n**Scott:  **Yeah. I feel like the word “incentive” is distracting or something. I think the point is that there is modeling of humans.\n\n(*reconsidering*) There might not be modeling of humans in IDA. There's a sense in which, in doing it right, maybe there's not... I don't know. I don't think that's where the disagreement is, though, is it?\n\n**Eli:  **Can we back up for a second and check what's the deal with modeling humans? Presumably there's something at least potentially bad about that. There's a reason why we care about it. Yes? \n\n**Rohin:  **Yeah. I think the reason that I'm going with, which I feel fairly confident that Scott agrees with me on, at least as the main reason, is that if your AI system is modeling humans, then it is “easy” or “close” for it to be manipulating humans and doing something that we don't want that we can't actually detect ahead of time, and that therefore causes bad outcomes. \n\n**Scott:  **Yeah. I want to be especially clear about the metric of closeness being about our ability to oversee/pre-oversee a system in the way that we set up the incentives or something.\n\nThe closeness between modeling humans and manipulating humans is not in the probability that some system that’s doing one spontaneously changes to doing the other. (Even though I think that they *are* close in that metric.) It's more in the ability to be able to distinguish between the two behaviors. \n\nAnd I think that there's a sense in which my model is very pessimistic about oversight, such that maybe if we really try for the next 20 years or something, we can distinguish between “model thinking superintelligent thoughts about physics” and “model thinking about humans.” And we have no hope of actually being able to distinguish between “model that's trying to manipulate the human” and “model that's trying to do IDA-type stuff (or whatever) the legitimate way.”\n\n**Rohin:  **Right. And I'm definitely a lot more optimistic about oversight than you, but I still agree directionally that it's harder to oversee a model when you're trying to get it to do things that are very close to manipulation. So this feels not that cruxy or something.\n\n**Scott:  **Yeah, not that cruxy... I don't know, I feel like I want to hear more about what Rohin thinks or something instead of responding to him. Yeah, not that that cruxy, but what's the cruxy part, then? I feel like I'm already granting the thing about incentives and I'm not even talking about whether you have incentives to model humans. I'm assuming that there's systems that model humans, there's systems that don't model humans. And it's a *lot* easier to oversee the ones that don't.  \n \n\n2\\. Mutual information with humans\n----------------------------------\n\n**Rohin:**  I think my main claim is: the determining factor about whether a system is modeling humans or not—or, let's say there is an *amount* of modeling the system does. I want to talk about spectrums because I feel like you say too many wrong things if you think in binary terms in this particular case. \n\n**Scott:  **All right.\n\n**Rohin:**  So, there's an amount of modeling humans. Let's call it a scale—\n\n**Scott:**  We can have an entire *one* dimension instead of zero! (*laughs*) \n\n**Rohin:**  Yes, exactly. The art of choosing the right number of dimensions. (*laughs*) It's an important art.\n\nSo, we'll have it on a scale from 0 to 10. I think my main claim is that the primary determinant of where you are on the spectrum is what you are trying to get your AI system to do, and not the source of the feedback by which you train the system. And IDA is the latter, not the former.\n\n**Scott:**  Yeah, so I think that this is why I was distinguishing between the two kinds of “closeness.” I think that the probability of spontaneously manipulating humans is stronger when there are humans in the task than when there are just humans in the IDA/CIRL way of pointing at the task or something like that. But I think that the distance in terms of ability to oversee is not large...\n\n**Rohin:**  Yeah, I think I am curious why.\n\n**Scott:**  Yeah. Do I think that? (*pauses*) \n\nHm. Yeah. I might be responding to a fake claim right now, I'm not sure. But in IDA, you aren't keeping some sort of structure of [HCH](https://ai-alignment.com/strong-hch-bedb0dc08d4e). You're not *trying* to do this, because they're trying to do oversight, but you're distilling systems and you're allowing them to find what gets the right answer. And then you have these structures that get the right answer on questions about what humans say, and *maybe* rich enough questions that contain a lot of needing to understand what's going on with the human. (I claim that yes, many domains are rich enough to require modeling the human; but maybe that's false, I don't know.) \n\nAnd basically I'm imagining a black box. Not entirely black box—it's a gray box, and we can look at some features of it—but it just has mutual information with a bunch of stuff that humans do. And I almost feel like, I don't think this is the way that transparency will actually work, but I think just the question “is there mutual information between the humans and the models?” is the extent to which I expect to be able to do the transparency or something.\n\nMaybe I'm claiming that in IDA, there's *not* going to be mutual information with complex models of humans.\n\n**Rohin:**  Yep. That seems right. Or more accurately, I would say it depends on the task that you're asking IDA to do primarily. \n\n**Scott:**  Yeah, well, it depends on the task and it depends on the IDA. It depends on what instructions you give to the humans.\n\n**Rohin:**  Yes.\n\n**Scott:**  If you imagined that there's some core of how to make decisions or something, and humans have access to this core, and this core is not specific to humans, but humans don't have access to it in such a way that they can write it down in code, then you would imagine a world in which I would be incentivized to do IDA and I would be able to do so safely (kind of) because I'm not actually putting mutual information with humans in the system. I'm just asking the humans to follow their “how to make decisions” gut that doesn't actually relate to the humans. \n\n**Rohin:**  Yes. That seems right.\n\nWhat do I think about that? Do I actually think humans have a core?... \n\n**Scott:**  I almost wasn't even trying to make that claim. I was just trying to say that there exists a plausible world where this might be the case. I'm uncertain about whether humans have a core like that. \n\nBut I do think that in IDA, you're doing more than just asking the humans to use their “core.” Even if humans had a core of how to solve differential equations that they couldn't write down in code and then wanted to use IDA to solve differential equations via only asking humans to use their core of differential equations.\n\nIf this were the case… Yeah, I think the IDA is asking more of the human than that. Because regardless of the task, the IDA is asking the human to also do some oversight. \n\n**Rohin:**  … Yes. \n\n**Scott:**  And I think that the “oversight” part is capturing the richness of being human even if the task manages to dodge that. \n\nAnd the analog of oversight in debates is the debates. I think it's more clear in debates because it's debate, but I think there's the oversight part and that's going to transfer over.\n\n**Rohin:**  That seems true. One way I might rephrase that point is that if you have a results-based training system, if your feedback to the agent is based on results, the results can be pretty independent of humans. But if you have a feedback system based not just on results, but also on the process by which you got the results—for whatever reason, it just happens to be an empirical fact about the world that there's no nice, correct, human-independent core of how to provide feedback on process—then it will necessarily contain a bunch of information about humans the agent will then pick up on.\n\n**Scott:**  Yeah.\n\nYeah, I think I believe this claim. I'm not sure. I think that you said back a claim that not only is what I said, but also I temporarily endorse, which is stronger. (*laughs*) \n\n**Rohin:**  Cool. (*laughs*) Yeah. It does seem plausible. It seems really rough to not be giving feedback on the process, too. \n\nI will note that it is totally possible to do IDA, debate, and CIRL without process-level feedback. You just tell your humans to only write the results, or you just replace the human with an automated reward function that only evaluates the results if you can do that. \n\n**Scott:**  I mean... \n\n**Rohin:**  I agree, that's sort of losing the point of those systems in the first place. Well, maybe not CIRL, but at least IDA and debate. \n\n**Scott:**  Yeah. I feel like I can imagine \"Ah, do something like IDA without the process-level feedback.\" I wouldn't even want to call debate “debate” without the process-level feedback.\n\n**Rohin:**  Yeah. At that point it's like two-player zero-sum optimization. It's like AlphaZero or something.\n\n**Scott:**  Yeah. I conjecture that the various people who are very excited about each of these paradigms would be unexcited about the version that does not have process-level feedback—\n\n**Rohin:**  Yes. I certainly agree with that. \n\nI also would be pretty unexcited about them without the process-level feedback. Yeah, that makes sense. I think I would provisionally buy that for at least physics-style AI systems. \n\n**Scott:**  What do you mean? \n\n**Rohin:**  If your task is like, \"Do good physics,\" or something. I agree that the process-level feedback will, like, 10x the amount of human information you have. (I made up the number 10.)\n\nWhereas if it was something else, like if it was sales or marketing, I'd be like, \"Yeah, the process-level feedback makes effectively no difference. It increases the information by, like, 1%.\" \n\n**Scott:**  Yeah. I think it makes very little difference in the mutual information. I think that it still feels like it makes some difference in some notion of closeness-to-manipulation to me.\n\nSo I'm in a position where if it's the case that one could do superhuman STEM work without humans, I want to know this fact. Even if I don't know what to do with it, that feels like a question that I want to know the answer to, because it seems plausibly worthwhile. \n\n**Rohin:**  Well, I feel like the answer is almost certainly yes, right? AlphaFold is an example of it. \n\n**Scott:**  No, I think that one could make the claim... All right, fine, I'll propose an example then. One could make the claim that IDA is a capability enhancement. IDA is like, \"Let's take the humans' information about how to break down and solve problems and get it into the AI via the IDA process.\" \n\n**Rohin:**  Yep. I agree you could think of it that way as well. \n\n**Scott:**  And so one could imagine being able to answer physics questions via IDA and not knowing how to make an AI system that is capable of answering the same physics questions without IDA. \n\n**Rohin:**  Oh. Sure. That seems true. \n\n**Scott:**  So at least in principle, it seems like… \n\n**Rohin:**  Yeah. \n\n**Eli:  **There's a further claim I hear you making, Scott—and correct me if this is wrong—which is, “We want to explore the possibility of solving physics problems without something like IDA, because that version of how to solve physics problems may be safer.”\n\n**Scott:**  Right. Yeah, I have some sort of conjunction of “maybe we can make an AGI system that just solves physics problems” and also “maybe it's safer to do so.” And also “maybe AI that could only solve physics problems is sufficient to save the world.” And even though I conjuncted three things there, it's still probable enough to deserve a lot of attention. \n\n**Rohin:**  Yeah. I agree. I would probably bet against “we can train an AI system to do good STEM reasoning in general.” I'm certainly on board that we can do it some of the time. As I've mentioned, AlphaFold is an example of that. But it does feel like, yeah, it just seems really rough to have to go through an evolution-like process or something to learn general reasoning rather than just learning it from humans. Seems so much easier to do the latter that that's probably going to be the best approach in most cases. \n\n**Scott:**  Yeah. I guess I feel like some sort of automated working with human feedback—I feel like we can be inspired by humans when trying to figure out how to figure out some stuff about how to make decisions or something. And I'm not too worried about mutual information with humans leaking in from the fact that we were inspired by humans. We can use the fact that we know some facts about how to do decision-making or something, to not have to just do a big evolution. \n\n**Rohin:**  Yeah. I think I agree that you could definitely do a bit better that way. What am I trying to say here? I think I'm like, “For every additional bit of good decision-making you specify, you get correspondingly better speed-ups or something.” And IDA is sort of the extreme case where you get lots and lots of bits from the humans. \n\n**Scott:**  Oh! Yeah... I don't think that the way that humans make decisions is that much more useful as a thing to draw inspiration from than, like, how humans think ideal decision-making should be. \n\n**Rohin:**  Sure. Yeah, that's fair. \n\n**Scott:**  Yeah. It’s not obvious to me that you have that much to learn from humans relative to the other things in your toolbox. \n\n**Rohin:**  That's fair. I think I do disagree, but I wouldn't bet confidently one way or the other.  \n \n\n3\\. The default trajectory\n--------------------------\n\n**Eli:  **I guess I want to back up a little bit and just take stock of where we are in this thread of the conversation. Scott made a conjunctive claim of three things. One is that it's possible to train an AI system to do STEM work without needing to have human models in the mix. Two, this might be sufficient for saving the world or otherwise doing pretty powerful stuff. And three, this might be substantially safer than doing it IDA-style or similar. I guess I just want to check, Rohin. Do you agree or disagree with each of those points? \n\n**Rohin:**  I probably disagree *somewhat* with all of them, but the upstream disagreement is how optimistic versus pessimistic Scott and I are about AI safety in general. If you're optimistic the way I am, then you're much more into trying not to change the trajectory of AI too much, and instead make the existing trajectory better. \n\n**Scott:**  Okay. (*laughs*) I don't know. I feel like I want to say this, because it's ironic or something. I feel like the existing framework of AI before AI safety got involved was “just try to solve problems,” and then AI safety's like, \"You know what we need in our ‘trying to solve problems’? We need a lot of really meta human analysis.\" (*laughs*)\n\nIt does feel to me like the default path if nobody had ever thought about AI safety looks closer to the thing that I'm advocating for in this discussion. \n\n**Rohin:**  I do disagree with that. \n\n**Scott:**  Yeah. For the default thing, you do need to be able to say, \"Hey, let's do some transparency and let's watch it, and let's make sure it's not doing the human-modeling stuff.\" \n\n**Rohin:**  I also think people just would have started using human feedback. \n\n**Scott:**  Okay. Yeah, that might be true. Yeah. \n\n**Rohin:**  It's just such an easy way to do things, relative to having to write down the reward function. \n\n**Scott:**  Yeah, I think you're right about that.\n\n**Rohin:**  Yeah. But I think part of it is that Scott's like, \"Man, all the things that are not this sort of three-conjunct approach are pretty doomed, therefore it's worth taking a plan that might be unlikely to work because that's the only thing that can actually make a substantial difference.\" Whereas I'm like, \"Man, this plan is unlikely to work. Let's go with a likely one that can cut the risk in half.\" \n\n**Scott:**  Yeah. I mean, I basically don't think I have any kind of plan that I think is likely to work… But I'm also not in the business of making plans. I want other people to do that. (*laughs*)\n\n**Rohin:**  I should maybe respond to Eli's original question. I think I am more pessimistic than Scott on each of the three claims, but not by much.\n\n**Eli:  **But the main difference is you're like, \"But it seems like there's this alternative path which seems like it has a pretty good shot.\"\n\n**Rohin:**  I think Scott and I would both agree that the no-human-models STEM AI approach is unlikely to work out. And I’m like, \"There's this *other* path! It's *likely* to work out! Let's do that.\" And Scott's like, \"This is the only thing that has *any* chance of working out. Let's do this.\"\n\n**Eli:  **Yeah. Can you tell if it's a crux, whether or not your optimism about the no-human-models path would affect how optimistic you feel about the human-models path? It’s a very broad question, so it may be kind of unfair.\n\n**Rohin:**  It would seriously depend on why I became less optimistic about what I'm calling the usual path, the IDA path or something.\n\nThere are just a *ton* of beliefs that are correlated that matter. Again, I'm going to state things more confidently than I believe them for the sake of faster and clearer communication. There's a ton of beliefs like, \"Oh man, good reasoning is just sort of necessarily messy and you're not going to get nice, good principles, and so you should be more in the business of incrementally improving safety rather than finding the one way to make a nice, sharp, clear distinction between things.\"\n\nThen other parts are like, \"It sure seems like you can't make a sharp distinction between ‘good reasoning’ and ‘what humans want.’” And this is another reason I'm more optimistic about the first path.\n\nSo I could imagine that I get unconvinced of many of these points, and definitely some of them, if I got unconvinced of those, I would be more optimistic about the path Scott's outlining.  \n  \n \n\n4\\. Two kinds of risk\n---------------------\n\n**Scott:**  Yeah. I want to clarify that I'm not saying, like, \"Solve decision theory and then put the decision theory into AI,\" or something like that, where you're putting the “how to do good reasoning” in directly. I think that I'm imagining that you have to have some sort of system that's learning how to do reasoning. \n\n**Rohin:**  Yep. \n\n**Scott:**  And I'm basically distinguishing between a system that's learning how to do reasoning while being overseen and kept out of the convex hull of human modeling versus… And there are definitely trade-offs here, because you have more of a [daemon](https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction) problem or something if you're like, \"I'm going to learn how to do reasoning,\" as opposed to, \"I'm going to be told how to do reasoning from the humans.\" And so then you have to search over this richer space or something of how to do reasoning, which makes it harder. \n\nI'm largely not thinking about the capability cost there. There's a safety cost associated with running a big evolution to discover how reasoning works, relative to having the human tell you how reasoning works. But there's also a safety cost to having the human tell you how reasoning works. \n\nAnd it's a different kind of safety cost in the two cases. And I'm not even sure that I believe this, but I think that I believe that the safety cost associated with learning how to do reasoning in a STEM domain might be lower than the safety cost associated with having your reasoning directly point to humans via the path of being able to have a thick line between your good and bad behavior.\n\n**Rohin:**  Yeah.\n\n**Scott:**  And I can imagine you saying, like, \"Well, but that doesn't matter because you can't just say, 'Okay, now we're going to run the evolution on trying to figure out how to solve STEM problems,' because you need to actually have the capabilities or something.\"\n\n**Rohin:**  Oh. I think I lost track of what you were saying at the last sentence. Which probably means I failed to understand the previous sentences too.\n\n**Eli:  **I heard Scott to be saying—obviously correct me if I've also misapprehended you, Scott—that there's at least two axes that you can compare things along. One is “What's the safety tax? How much AI risk do you take on for this particular approach?” And this other axis is, “How hard is it to make this approach work?” Which is a capabilities question. And Scott is saying, \"I, Scott, am only thinking about the safety tax question, like which of these is—”\n\n**Scott:**  “Safety tax” is a technical term that you're using wrong. But yeah.\n\n**Rohin:**  If you call it “safety difficulty”...\n\n**Scott:**  Or “risk factor.”\n\n**Eli:  **Okay. Thank you. “I'm only thinking about the risk factor, and not thinking about how much extra difficulty with making it work comes from this approach.” And I imagine, Rohin, that you're like, \"Well, even—”\n\n**Rohin:**  Oh, I just agree with that. I'm happy making that decomposition for now. Scott then made a further claim about the delta between the risk factor from the IDA approach and the STEM AI approach.\n\n**Scott:  **Well, first I made the further claim that the risk from the IDA approach and the risk from the STEM approach are kind of different in kind. It's not like you can just directly compare them; they're different kinds of risks. \n\nAnd so because of that, I'm uncertain about this: But then I further made the claim, at least in this current context, that the IDA approach might have more risk.\n\n**Rohin:**  Yeah. And what *were* the two risks? I think that's the part where I got confused.\n\n**Scott:**  Oh. One of the risks is we have to run an evolution because we're not learning from the humans, and so the problem's harder. And that introduces risks because it's harder for us to understand how it's working, because it's working in an alien way, because we had to do an evolution or something.\n\n**Rohin:**  So, roughly, we didn't give it process-level feedback, and so its process could be wild?\n\n**Scott:**  … Yeahh... I mean, I’m imagining... Yeah. Right.\n\n**Rohin:**  Cool. All right. I understand that one. Well, maybe not... \n\n**Scott:**  Yeah. I think that if you phrase it as \"We didn't give it any process-level feedback,\" I'm like, “Yeah, maybe this one is obviously the larger risk.” I don't know.\n\nBut yeah, the risk associated with \"You have to do an evolution on your system that’s thinking about STEM.\" Yeah, it's something like, \"We had to do an evolution.\" \n\nAnd the risk of the other one is that we made it such that we can't oversee it in the required ways. We can't run the policy, watch it carefully, and ensure it doesn’t reason about humans. \n\n**Rohin:**  Yeah. \n\n**Scott:**  And yeah, I think I'm not even making a claim about which of these is more or less risky. I'm just saying that they're sufficiently different risks, that we want to see if we can mitigate either of them. \n\n**Rohin:**  Yeah. That makes sense.  \n \n\n5\\. Scaling up STEM AI\n----------------------\n\n**Rohin:**  So the thing I wanted to say, and now I'm more confident that it actually makes sense to say, is that it feels to me like the STEM AI approach is lower-risk for a somewhat-smarter-than-human system, but if I imagine scaling up to arbitrarily smarter-than-human systems, I'm way more scared of the STEM AI version. \n\n**Scott:**  Yeah.\n\n**Eli:  **Can you say why, Rohin?\n\n**Rohin:**  According to me, the reason for optimism here is that your STEM AI system isn't really thinking about humans, doesn't know about them. Whatever it's doing, it's not going to successfully execute a treacherous turn, because successfully doing that requires you to model humans. That's at least the case that *I* would make for this being safe.\n\nAnd when we're at *somewhat* superhuman systems, I'm like, \"Yeah, I mostly buy that.\" But when we talk about the limit of infinite intelligence or something, then I'm like, \"But it's not literally true that the STEM AI system has *no* reason to model the humans, to the extent that it has goals (which seems like a reasonable thing to assume).” Or at least a reasonable thing to worry about, maybe not assume.\n\nIt will maybe want more resources. That gives it an incentive to learn about the external world, which includes a bunch of humans. And so it could just start learning about humans, do it very quickly, and then execute a treacherous turn. That seems entirely possible, whereas with the IDA approach, you can hope that we have successfully instilled the notion of, \"Yes, you are really actually trying to—”\n\n**Scott:**  Yeah. I definitely don’t want us to do this forever.\n\n**Rohin:**  Yeah.\n\n**Scott:**  I'm imagining—I don't know, for concreteness, even though this is a silly plan, there's the plan of “turn Jupiter into a supercomputer, invent some scanning tech, run a literal HCH.”\n\n**Rohin:**  Yeah. \n\n**Scott:**  And I don't know, this probably seems like it requires super-superhuman intelligence by your scale. Or maybe not.\n\n**Rohin:**  I don't really have great models for those scales.\n\n**Scott:**  Yeah, I don't either. It's plausible to me that... Yeah, I don't know.\n\n**Rohin:**  Yeah. I'm happy to say, yes, we do not need a system to scale to literally the maximal possible intelligence that is physically possible. And I do not—\n\n**Scott:**  Further, we don't want that, because at some point we need to get the human values into the future, right? (*laughs*)\n\n**Rohin:**  Yes, there is that too.\n\n**Scott:**  At some point we need to... Yeah. There is a sense in which it's punting the problem. \n\n**Rohin:**  Yeah. And to be clear, I am fine with punting the problem. \n\nIt does feel a little worrying though that we... Eh, I don't know. Maybe not. Maybe I should just drop this point.  \n \n\n6\\. Optimism about oversight\n----------------------------\n\n**Scott:**  Yeah. I feel like I want to try to summarize you or something. Yeah, I feel like I don't want to just say this optimism thing that you've said several times, but it's like...\n\nI think that I predict that we're in agreement about, \"Well, the STEM plan and the IDA plan have different and incomparable-in-theory risk profiles.\" And I don't know, I imagine you as being kind of comfortable with the risk profile of IDA. And probably also more comfortable than me on the risk profile of basically every plan.\n\nI think that I'm coming from the perspective, \"Yep, the risk profile in the STEM plan also seems bad, but it seems higher variance to me.\"\n\nIt seems bad, but maybe we can find something adjacent that's less bad, or something like that. And so I suspect there’s some sort of, \"I'm pessimistic, and so I'm looking for things such that I don't know what the risk profiles are, because maybe they'll be better than this option.\"\n\n**Rohin:**  Yeah.\n\n**Scott:**  Yeah. Yeah. So maybe we're mostly disagreeing about the risk profile of the IDA reference class or something—which is a large reference class. It's hard to bring in one thing.\n\n**Rohin:**  I mean, for what it's worth, I started out this conversation not having appreciated the point about process-level feedback requiring more mutual information with humans. I was initially making a stronger claim.\n\n**Scott:**  Okay.\n\n**Eli:  **Woohoo! An update.\n\n**Scott:**  (*laughs*) Yeah. I feel like I want to flag that I am suspicious that neither of us were able to correctly steel-man IDA, because of... I don't know, I'm suspicious that the IDA in Paul's heart doesn't care about whether you build your IDA out of humans versus building it out of some other aliens that can accomplish things, because it's really trying to... I don't know. That's just a flag that I want to put out there that it's plausible to me that both Rohin and I were not able to steel-man IDA correctly.\n\n**Rohin:**  I think I wasn't trying to. Well, okay, I was trying to steel-man it inasmuch as I was like, “Specifically the risks of human manipulation seem not that bad,” which, I agree, Paul might have a better case for than me.\n\n**Scott:**  Yeah.\n\n**Rohin:**  I do think that the overall case for optimism is something like, “Yep, you'll be learning, you'll be getting some mutual info about humans, but the oversight will be good enough that it just is not going to manipulate you.”\n\n**Scott:**  I also suspect that we're not just optimistic versus pessimistic on AI safety. I think that we can zoom in on optimism and pessimism about transparency, and it might be that “optimism versus pessimism on transparency” is more accurate.\n\n**Rohin:**  I think I'm also pretty optimistic about process-level feedback or something—which, you might call it transparency... \n\n**Scott:**  Yeah. It’s almost like when I'm saying “transparency,” I mean “the whole reference class that lets you do oversight” or something. \n\n**Rohin:**  Oh. In that case, yes, that seems right. \n\nSo it includes adversarial training, it includes looking at the concepts that the neural net has learned to see if there's a deception neuron that's firing, it includes looking at counterfactuals of what the agent would have done and seeing whether those would have been bad. All that sort of stuff falls under “transparency” to you? \n\n**Scott:**  When I was saying that sentence, kind of.\n\n**Rohin:**  Sure. Yeah, I can believe that that is true. I think there's also some amount of optimism on my front that the intended generalization is just the one that is usually learned, which would also be a difference that isn't about transparency. \n\n**Scott:**  Yeah.\n\n**Rohin:**  Oh, man. To everyone listening, don't quote me on that. There are a bunch of caveats about that. \n\n**Scott:**  (*laughs*)\n\n**Eli:  **It seems like this is a pretty good place to wrap up, unless we want to dive back in and crux on whether or not we should be pessimistic about transparency and generalization.\n\n**Scott:**  Even if we are to do that, I want to engage with the audience first. \n\n**Eli:  **Yeah, great. So let us open the fishbowl, and people can turn on their cameras and microphones and let us discuss.  \n \n\n7\\. Q&A: IDA and getting useful work from AI\n--------------------------------------------\n\n**Scott:**  Wow, look at all this chat with all this information. \n\n**Rohin:**  That is a lot of chat. \n\nSo, I found one of the chat questions, which is, “How would you build an AI system that did not model humans? Currently, to train the model, we just gather a big data set, like a giant pile of all the videos on YouTube, and then throw it at a neural network. It's hard to know what generalizations the network is even making, and it seems like it would be hard to classify generalizations into a ‘modeling humans’ bucket or not.”\n\n**Scott:**  Yeah. I mean, you can make a Go AI that doesn't model humans. It's plausible that you can make something that can do physics using methods that are all self-play-adjacent or something.\n\n**Rohin:**  In a simulated environment, specifically. \n\n**Scott:**  Right. Yeah.\n\nOr even—I don't know, physics seems hard, but you could make a thing that tries to learn some math and answer math questions, you could do something that looks like self-play with how useful limits are or something in a way that's not... I don't know.\n\n**Rohin:**  Yeah.\n\n**Donald Hobson:**  I think a useful trick here is: anything that you can do in practice with these sorts of methods is easy to brute-force. You can do Go with AlphaGo Zero, and you can easily brute-force Go given unlimited compute. AlphaGo Zero is just a clever— \n\n**Scott:**  Yeah. I think there's some part where the human has to interact with it, but I think I'm imagining that your STEM AI might just be, \"Let's get really good at being able to do certain things that we could be able to brute-force in principle, and then use this to do science or something.”\n\n**Donald Hobson:**  Yeah. Like a general differential equation solver or something. \n\n**Scott:**  Yeah. I don't know how to get from this to being able to get, I don't know, reliable brain scanning technology or something like that. But I don't know, it feels plausible that I could turn processes that are about how to build a quantum computer into processes that I could turn into math questions. I don't know.\n\n**Donald Hobson:**  “Given an arbitrary lump of biochemicals, find a way to get as much information as possible out of it.” Of a specific certain kind of information—not thermal noise, obviously.\n\n**Scott:**  Yeah. Also in terms of the specifications, you could actually imagine just being able to specify, similar to being able to specify Go, the problem of, \"Hey, I have a decent physics simulation and I want to have an algorithm that is Turing-complete or something, and use this to try to make more…” I don't know. Yeah, it's hard. It's super hard.\n\n**Ben Pace:**  The next question in the chat is from Charlie Steiner saying, \"What's with IDA being the default alternative rather than general value learning? Is it because you're responding to imaginary Paul, or is it because you think IDA is particularly likely or particularly good?\"\n\n**Scott:**  I think that *I* was engaging with IDA especially because I think that IDA is among the best plans I see. I think that IDA is especially good, and I think that part of the reason why I think that IDA is especially good is because it seems plausible to me that basically you *can* do it without the core of humanity inside it or something.\n\nI think the version of IDA that you're hoping to be able to do in a way that doesn't have the core of humanity inside the processes that it's doing seems plausible to exist. I like IDA. \n\n**Donald Hobson:**  By that do you mean IDA, like, trained on chess? Say, you take Deep Blue and then train IDA on that? Or do you mean IDA that was originally trained on humans? Because I think the latter is obviously going to have a core of humanity, but IDA trained on Deep Blue—\n\n**Scott:**  Well, no. IDA trained on humans, where the humans are trained to follow an algorithm that's sufficiently reliable.\n\n**Donald Hobson:**  Even if the humans are doing something, there will be side channels. Suppose the humans are solving equations—\n\n**Scott:**  Yeah, but you might be able to have the system watch itself and be able to have those side channels not actually... In theory, there are those side channels, and in the limit, you'd be worried about them, but you might be able to get far enough to be able to get something great without having those side channels actually interfere.\n\n**Donald Hobson:**  Fair enough. \n\n**Ben Pace:**  I'm going to ask a question that I want answered, and there's a good chance the answer is obvious and I missed it, but I think this conversation initially came from, Scott wrote a post called “[Thoughts on Human Models](https://www.lesswrong.com/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models),” where he was— \n\n**Scott:**  Ramana wrote the post. I ranted at Ramana, and then he wrote the post.\n\n**Ben Pace:**  Yes. Where he was like, \"What if we look more into places that didn't rely on human models in a bunch of ways?\" And then later on, Rohin was like, \"Oh, I don't think I agree with that post much at all, and I don't think that's a promising area.\" I currently don't know whether Rohin changed his mind on whether that was a promising area to look at. I'm interested in Rohin's say on that.\n\n**Rohin:**  So A, I'll note that we mostly didn't touch on the things I wrote in those comments because Scott's reasons for caring about this have changed. So I stand by those comments. \n\n**Ben Pace:**  Gotcha. \n\n**Rohin:**  In terms of “Do I think this is a promising area to explore?”: eh, I'm a *little* bit more interested in it, mostly based on “perhaps process level feedback is introducing risks that we don't need.” But definitely my all-things-considered judgment is still, \"Nah I would not be putting resources into this,\" and there are a bunch of other disagreements that Scott and I have that are feeding into that. \n\n**Ben Pace:**  And Scott, what's your current feelings towards... I can't remember, in the post you said \"there's both human models and not human models\", and then you said something like, \"theory versus\"... I can't remember what that dichotomy was.\n\nBut how much is your feeling a feeling of “this is a great area with lots of promise” versus “it's plausible and I would like to see some more work in it,” versus “this is the primary thing I want to be thinking about.”\n\n**Scott:**  So, I think there is a sense in which I am primarily doing science and not engineering. I'm not following the plan of “how do you make systems that are…” I'm a *little* bit presenting a plan, but I'm not primarily thinking about that plan as a plan, as opposed to trying to figure out what's going on.\n\nSo I'm not with my main attention focused on an area such that you can even *say* whether or not there are human models in the plan, because there's not a plan.\n\nI think that I didn't update much during this conversation and I did update earlier. Where the strongest update I've had recently, and partially the reason why I was excited about this, was: The post that was written two years ago basically is just talking about \"here's why human models are dangerous\", and it's not about the thing that I think is the strongest reason, which is the ability to oversee and the ability to be able to implement the strategy “don't let the thing have human models at all.”\n\nIt's easy to separate “thinking about physics” from “thinking about manipulating humans” if you put “think about modeling humans” in the same cluster as “think about manipulating humans.” And It's a lot harder to draw the line if you want to put “think about modeling humans” on the other side, with “thinking about physics.”\n\nAnd I think that this idea just didn't show up in the post. So to the extent that you want me to talk about change from two years ago, I think that the center of my reasoning has changed, or the part that I'm able to articulate has changed.\n\nThere was a thing during this conversation... I think that I actually noticed that I probably don't, even in expectation, think that... I don't know.\n\nThere was a part during this conversation where I backpedaled and was like, \"Okay, I don't know about the expectation of the risk factor of these two incomparable risks. I just want to say: first, they're different in kind, therefore we should analyze both. They're different in kind, therefore if we're looking for a good strategy, we should keep looking at both because we should keep that OR gate in our abilities.\n\n“And two: It seems like there might be some variance things, where we might be able to learn more about the risk factors on this other side, AI that doesn’t use human models. It might turn out that the risks are lower, so we should want to keep attention on both types of approach.”\n\nI think maybe I would have said this before. But I think that I'm not confident about what side is better, and I'm more arguing from “let's try lots of different options because I'm not satisfied with any of them.”\n\n**Rohin:**  For what it's worth, if we don't talk just about \"what are the risks\" and we also include \"what is the plan that involves this story\", then I'm like, \"the IDA one seems many orders of magnitude more likely to work\". “Many” being like, I don’t know, over two.\n\n**Ben Pace:**  It felt like I learned something when Scott explained his key motivation being around the ability to understand what the thing is doing and blacklist or whitelist certain types of cognition, being easier when it's doing no human modeling, as opposed to when it is trying to do human modeling but you also don't want it to be manipulative.\n\n**Rohin:**  I think I got that from the comment thread while we were doing the comment thread, whenever that was.\n\n**Ben Pace:**  The other question I have in chat is: Steve—\n\n**Scott:**  Donald was saying mutual information is too low a bar, and I want to flag that I did not mean that mutual information *is* the metric that should be used to determine whether or not you're modeling humans. But the *type* of thing that I think that we could check for, has some common intuition with trying to check for mutual information.\n\n**Donald Hobson:**  Yeah, *similar* to mutual information. I'd say it was closer to mutual information conditional on a bunch of physics and other background facts about the universe.\n\n**Scott:**  I want to say something like, there's some concept that we haven't invented yet, which is like “logical mutual information,” and we don't actually know what it means yet, but it might be something.\n\n**Ben Pace:**  I'm going to go to the next question in chat, which is Steve Byrnes: “Scott and Rohin both agreed/posited right at the start that maybe we can make AIs that do tasks that do not require modeling humans, but I'm still stuck on that. An AI that can't do all of these things, like interacting with people, is seemingly uncompetitive and seemingly unable to take over the world or help solve all of AI alignment. That said, ‘use it to figure out brain scanning’ was a helpful idea by Scott just now. But I'm not 100% convinced by that example. Is there any other plausible example, path, or story?”\n\n**Scott:**  Yeah, I feel like I can't give a good story in terms of physics as it exists today. But I could imagine being in a world where physics was different such that it would help. And because of that I'm like, “It doesn't seem obviously bad in principle,” or something.\n\nI could imagine a world where basically we could run HCH and HCH would be great, but we can't run HCH because we don't have enough compute power. But also, if you can inverse this one hash you get infinity compute because that's how physics works.\n\n**Group:**  (*laughs*)\n\n**Scott:**  Then your plan is: let's do some computer science until we can invert this one hash and then we unlock the hyper-computers. Then we use the hyper-computers to run a literal HCH as opposed to just an IDA.\n\nAnd that's not the world we live in, but the fact that I can describe that world means that it's an empirical about-the-world fact as opposed to an empirical about-how-intelligence-works fact, or something. So I'm, like, open to the possibility. I don’t know.\n\n**Gurkenglas:**  On what side of the divide would you put reasoning about decision theory? Because thinking about agents in general seems like it might not count as modeling humans, and thinking about decision theory is kind of all that's required in order to think about AI safety, and if you can solve AI safety that's the win condition that you were talking about.\n\nBut also it's kind of hard to be sure that an AI that is thinking about agency is not going to be able to manipulate us, even if it doesn't know that we are humans. It might just launch attacks against whoever is simulating its box. So long as that person is an agent.\n\n**Scott:**  Yeah. I think that thinking about agents in general is already scary, but not as scary. And I'm concerned about the fact that it might be that thinking about agents is just convergent and you're not going to be able to invert the hash that gives you access to the hyper-computer, unless you think about agents.\n\nThat's a particularly pessimistic world, where it's like, you need to have things that are thinking about agency and thinking about things in the context of an agent in order to be able to do anything super powerful.\n\nI'm really uncertain about the convergence of agency. I think there are certain parts of agency that are convergent and there are certain parts of agency that are not, and I'm confused about what the parts of agency even *are*, enough that I kind of just have this one cluster. And I'm like, yeah, it seems like that cluster is kind of convergent. But maybe you can have things that are optimizing “figuring out how to divert cognitive resources” and that's kind of like being an agent, but it's doing so in such a way that's not self-referential or anything, and maybe that's enough? I don’t know.\n\nThere's this question of the convergency of thinking about agents that I think is a big part of a hole in my map. A hole in my prioritization map is, I'm not actually sure what I think about what parts of agency are convergent, and this affects a lot, because if certain parts of agency are convergent then it feels like it really dooms a lot of plans.\n\n**Ben:**  Rohin, is there anything you wanted to add there?\n\n**Rohin:**  I don’t know. I think I broadly agree with Scott in that, if you're going to go down the path of “let's exclude all the types of reasoning that could be used to plan or execute a treacherous turn,” you probably want to put general agency on the outside of that barrier. That seems roughly right if you're going down this path.\n\nAnd on the previous question of: can you do it, can this even be done? I'm super with, I think it was Steve, on these sorts of AI systems probably being uncompetitive. But I sort of have a similar position as Scott: *maybe* there's just a way that you can leverage great knowledge of just science in order to take over the world for example. It seems like that might be possible. I don't know, one way or the other. I would bet against, unless you have some pretty weird scenarios. But I wouldn't bet against at, like, 99% confidence. Or maybe I would bet against it that much. But there I'm getting to “that's probably a bit too high.”\n\nAnd like, Scott is explicitly saying that most of these things are not high-probability things. They're just things that should be investigated. So that didn't seem like an avenue to push down.\n\n**Ben Pace:**  I guess I just disagree. I feel like if you were the first guy to invent nukes, and you invented them in 1800 or something, I feel I could probably tell various stories about having advanced technologies in a bunch of ways helping strategically—not even just weaponry.\n\n**Rohin:**  Yeah, I'd be interested in a story like this. I *don't* feel like you can easily make it. It just seems hard unless you're already a big power in the world.\n\n**Ben Pace:**  Really! Okay. I guess maybe I'll follow up with you on that sometime.\n\n**Joe Collman:**  Doesn't this slightly miss the point though, in that you'd need to be able to take over the world but then also make it safe afterwards, and the “make it safe afterwards” seems to be the tricky part there. The AI safety threat is still there if you through some conventional means—\n\n**Donald Hobson:**  Yeah, and you want to do this without massive collateral damage. If you invent nukes... If I had a magic map, where I pressed it and the city on it blew up or something, I can't see a way of taking over the world with that without massive collateral damage. Actually, I can't really see a way of taking over the world with that *with* massive collateral damage.\n\n**Ben Pace:**  I agree that the straightforward story with nukes sounds fairly unethical. I think there's probably ways of doing it that aren't unethical but are more about just providing sufficient value that you're sort of in charge of how the world goes.\n\n**Joe Collman:**  Do you see those ways then making it safer? Does that solve AI safety or is it just sort of, “I'm in charge but there's still the problem?”\n\n**Scott:**  I liked the proof of concept, even though this is not what I would want to do, between running literal HCH and IDA. I feel like literal HCH is just a whole lot safer, and the only difference between literal HCH and IDA is compute and tech.  \n \n\n8\\. Q&A: HCH’s trustworthiness\n------------------------------\n\n**Joe Collman:**  May I ask what your intuition is about motivation being a problem for HCH? Because to me it seems like we kind of skip this and we just get into what it's computationally capable of doing, and we ignore the fact that if you have a human making the decisions, that they're not going to care about the particular tasks you give them necessarily. They're going to do what they think is best, not what you think is best.\n\n**Scott:**  Yeah, I think that that's a concern, but that's a concern in IDA too.\n\nIf I think that IDA is the default plan to compare things to, then I'm like, “Well, you could instead do HCH, which is just safer—if you can safely get to the point where you can get to HCH.”\n\n**Joe Collman:**  Right, yeah. I suppose my worry around this is more if we're finding some approximation to it and we're not sure we've got it right yet. Whereas I suppose if you're uploading and you're sure that the uploading process is actually—\n\n**Scott:**  Which is not necessarily the thing that you would do with STEM AI, it's just a proof of concept.\n\n**Donald Hobson:**  If you're uploading, why put the HCH structure on it at all? Why not just have a bunch of uploaded humans running around a virtual village working on AI safety? If you've got incredibly powerful nano-computers, can't you just make uploaded copies of a bunch of AI safety researchers and run them for a virtual hundred years, but real time five minutes, doing AI safety research?\n\n**Scott:**  I mean, yeah, that's kind of like the HCH thing. It's different, but I don't know...\n\n**Charlie Steiner:**  I'm curious about your intuitive, maybe not necessarily a probability, but gut feeling on HCH success chances, because I feel like it's quite unlikely to preserve human value.\n\n**Donald Hobson:**  I think a small HCH will probably work and roughly preserve human value if it's a proper HCH, no approximations. But with a big one, you're probably going to get ultra-viral memes that aren't really what you wanted.\n\n**Gurkenglas:**  We have a piece of evidence on the motivation problem for HCH and IDA, namely GPT. When it pretends to write for a human, we can easily make it pretend to be any kind of human that we want by simply specifying it in the prompt. The hard part is making the pretend human capable enough.\n\n**Donald Hobson:**  I'm not sure that it's that easy to pick what kind of human you actually get from the prompt.\n\n**Scott:**  I think that the version of IDA that I have any hope for has humans following sufficiently strict procedures and such that this isn't actually a thing that... I don’t know, I feel like this is a misunderstanding of what IDA is supposed to do.\n\nI think that you're not supposed to get the value out of the individual pieces in IDA. You're supposed to use that to be able to… Like, you have some sort of task, and you're saying, \"Hey, help me figure out how to do this task.\" And the individual humans inside the HCH/IDA system are not like, \"Wait, do I really want to do this task?\". The purpose of the IDA is to have the capabilities come from a human-like method as opposed to just, like, a large evolution, so that you can oversee it and trust where it’s coming from and everything.\n\n**Donald Hobson:**  Doesn't that mean that the purpose is to have the humans provide all the implicit values so obvious no one bothered to mention them. So if you ask IDA to [put two strawberries on the plate](https://intelligence.org/stanford-talk/), it's the humans’ implicit values that do it in a way that doesn't destroy the world.\n\n**Scott:**  I think that it's partially that. I think it's not all that. I think that you only get the basics of human values out of the individual components of the HCH or IDA, and the actual human values are coming from the humans using the IDA system. \n\n*I* think that IDA as intended should work almost as well if you replace all the humans with well-intentioned aliens. \n\n**Ben Pace:**  Wait, can you say that again? That felt important to me. \n\n**Scott:**  I think, and I might be wrong about this, that the true IDA, the steel-man~Scott~ IDA, should work just as well or almost as well if you replaced all of the humans with well-intentioned aliens. \n\n**Donald Hobson:**  Well-intentioned aliens wouldn't understand English. Your IDA is getting its understanding of English from the humans in it. \n\n**Scott:**  No, I think that if I wanted to use IDA, I might use IDA to for example solve some physics problems, and I would do so with humans inside overseeing the process. There's not supposed to be a part of the IDA that’s making sure that human values are being brought into our process of solving these physics problems. The IDA is just there to be able to safely solve physics problems.\n\nAnd so if you replaced it with well-intentioned aliens, you still solve the physics problems. And if you direct your IDA towards a problem like “solve this social problem”, you need information about social dynamics in your system, but I think that you should think of that as coming from a different channel than the core of the breaking-problems-up-and-stuff, and the core of the breaking-problems-up-an-stuff should be thought of as something that could be done just as well by well-intentioned aliens. \n\n**Donald Hobson:**  So you've got humans that are being handed a social problem and told to break it up, that is the thing you're imitating, but the humans are trying to pretend that they know absolutely nothing about how human societies work except for what's on this external piece of paper that you handed them.\n\n**Scott:**  It's not necessarily a paper. You do need some human-interacting-with-human in order to think about social stuff or something... Yeah, I don't know what I'm saying. \n\nIt's more complicated on the social thing than on the physics thing. I think the physics thing should work just as well. I think the physics thing should work just as well with well-intentioned aliens, and the human thing should work just as well if you have an HCH that's built out of humans and well-intentioned aliens and the humans never do any decomposition, they only ask the well-intentioned aliens questions about social facts or something. And the process that's doing decomposition is the well-intended aliens' process of doing composition. \n\nI also might be wrong. \n\n**Ben Pace:**  I like this thread. I also kind of liked it when Joe Collman asked questions that he cared about. If you had more questions you cared about, Joe, I would be interested in you asking those \n\n**Joe Collman:**  I guess, sure. I was just thinking again with the HCH stuff—I would guess, Scott, that you probably think this isn't a practical issue, but I would be worried about the motivational side of HCH in the limit of infinite training of IDA. \n\nAre you thinking that, say, you've trained a thousand iterations of IDA, you're training the thousand-and-first, you've got the human in there. They've got this system that's capable of answering arbitrarily amazing, important questions, and you feed them the question “Do you like bananas?” or something like that, some irrelevant question that's just unimportant. Are we always trusting that the human involved in the training will precisely follow instructions? Are you seeing that as a non-issue, that we can just say, well that's a sort of separate thing— \n\n**Scott:**  I think that you could make your IDA kind of robust to “sometimes the humans are not following the instructions”, if it's happening a small amount of the time. \n\n**Joe Collman:**  But if it's generalizing from that into other cases as well, and then it learns, “OK, you don't follow instructions a small amount of time…”—if it generalizes from that, then in any high-stakes situation it doesn't follow instructions, and if we're dealing with an extremely capable system, it might see every situation as a high-stakes situation because it thinks, “I can answer your question or I can save the world.” Then I'm going to choose to save the world rather than answering your question directly, providing useful information. \n\n**Scott:**  Yeah, I guess if there's places where the humans reliably... I was mostly just saying, if you're collecting data from humans and there's noise in your data from humans, you could have systems that are more robust to that. But if you have it be the case that humans *reliably* don't follow instructions on questions of type X, then the thing that you're training is a thing that reliably doesn't follow instructions on questions of type X. And if you have assumptions about what the decomposition is going to do, you might be wrong based on this, and that seems bad, but... \n\n**Rohin:**  I heard Joe as asking a slightly different question, which is: “In not HCH but IDA, which is importantly—” \n\n**Joe Collman:**  To me it seems to apply to either. In HCH it seems clear to me that this will be a problem, because if you give it any task and the task is not effectively “Give me the most valuable information that you possibly can,” then the morality of the H—assuming you've got an H that wants the best for the world—then if you ask it, “Do you like bananas?”, it's just going to give you the most useful information. \n\n**Rohin:**  But an HCH is made up of humans. It should do whatever humans would do. Humans don't do that. \n\n**Joe Collman:**  No, it's going to do what the HCH tree would do. It's not going to do what the top-level human would do. \n\n**Rohin:**  Sure. \n\n**Joe Collman:**  So the top level human might say, “Yes, I like bananas,” but the tree is going to think, “I have infinite computing power. I can tell you how to solve the world's problems, or I can tell you, ‘Yes, I like bananas.’ Of those two, I'm going to tell you how to solve the world's problems, not answer your question.” \n\n**Rohin:**  Why? \n\n**Joe Collman:**  Because that's what a human would do—this is where it comes back into the IDA situation, where I'm saying, eventually it seems... I would assume probably this isn't going to be a practical problem and I would imagine your answer would be, “Okay, maybe in the limit this…” \n\n**Rohin:**  No, I'm happy with focusing on the limit. Even eventually, why does a human do this? The top-level human. \n\n**Joe Collman:**  The top level human: if it's me and you've asked me a question “Do you like bananas?”, and I'm sitting next to a system that allows me to give you information that will immediately allow you to take action to radically improve the world, then I'm just not going to tell you, “Yes, I like bananas”, when I have the option to tell you something that's going to save lives in the next five minutes or otherwise radically improve the world. \n\nIt seems that if we assume we've got a human that really cares about good things happening in the world, and you ask a trivial question, you're not going to get an answer to that question, it seems to me. \n\n**Rohin:**  Sure, I agree if you have a human who is making decisions that way, then yes, that's the decision that would come out of it. \n\n**Joe Collman:**  The trouble is, isn't that the kind of human that we want in these situations—is one precisely that does make decisions that way? \n\n**Rohin:**  No, I don't think so. We don't want that. We want a human who's trying to do what the user wants them to do. \n\n**Joe Collman:**  Right. The thing is, they have to be applying their own... HCH is basically a means of getting enlightened judgment. So the enlightened judgment has to come from the HCH framework rather than from the human user. \n\n**Rohin:**  I mean, I think if your HCH is telling the user something and the user is like, \"Dammit, I didn't like this\", then your HCH is not aligned with the user and you have failed. \n\n**Joe Collman:**  Yeah, but the HCH is aligned with the enlightenment. I suppose the thing I'm saying is that the enlightened judgment as specified by HCH will not agree with me myself. An HCH of me—it's enlightened judgment is going to do completely different things than the things I would want. So, yes, it's not aligned, but it's more enlightened than me. It's doing things that on “long reflection” I would agree with. \n\n**Rohin:**  I mean, sure, if you want to define terms that way then I would say that HCH is not trying to do the enlightened judgment thing, and if you've chosen a human who does the enlightened judgment thing you've failed. \n\n**Donald Hobson:**  Could we solve this by just never asking HCH trivial questions?\n\n**Joe Collman:**  The thing is, this is only going to be a problem where there's a departure between solving the task that's been assigned versus doing the thing that sort of maximally improves the world. Obviously if those are both the same, if you've asked wonderfully important questions or if you asked the HCH system “What is the most important question I can ask you?” and then you ask that, then it's all fine— \n\n**Scott:**  I think that most of the parts of the HCH will have the correct belief that the way to best help the world is to follow instructions corrigibly or something. \n\n**Joe Collman:**  Is that the case though, in general? \n\n**Scott:**  Well, it's like, if you're part of this big network that's trying to answer some questions, it's— \n\n**Joe Collman:**  So are you thinking from a UDT point of view, if I reasoned such that every part of this network is going to reason in the same way as me, therefore I need the network as a whole to answer the questions that are assigned to it or it's not going to come up with anything useful at all. Therefore I need to answer the question assigned to me, otherwise the system is going to fail. Is that the kind of…? \n\n**Scott:**  Yeah, I don't even think you have to call on UDT in order to get this answer right. I think it's just: I am part of this big process and it's like, \"Hey solve this little chemistry problem,\" and if I start trying to do something other than solve the chemistry problem I'm just going to add noise to the system and make it less effective. \n\n**Joe Collman:**  Right. That makes sense to me if the actual top-level user has somehow limited the output so that it can only respond in terms of solving a chemistry problem. Then I guess I'd go along with you. But if the output is just free text output and I'm within the system and I get the choice to solve the chemistry problem as assigned, or I have the choice to provide the maximally useful information, just generally, then I'm going to provide the maximally useful information, right? \n\n**Scott:**  Yeah, I think I wouldn't build an HCH out of you then. (*laughs*) \n\n**Rohin:**  Yeah, that’s definitely what I’m thinking. \n\n**Scott:**  I think that I don't want to have the world-optimization in the HCH in that way. I want to just... I think that the thing that gives us the best shot is to have a corrigible HCH. And so— \n\n**Joe Collman:**  Right, but my argument is basically that eventually the kind of person you want to put in is not going to be corrigible. With enough enlightenment— \n\n**Rohin:**  I think I continue to be confused about why you assume that we want to put a consequentialist into the HCH. \n\n**Joe Collman:**  I guess I assume that with sufficient reflection, pretty much everyone is a consequentialist. I think the idea that you can find a human who at least is *reliably* going to *stay* not a consequentialist, even after you amplify their reasoning hugely, that seems a very suspicious idea to me. \n\nEssentially, let's say for instance, due to the expansion of the universe we're losing—I calculated this very roughly, but we're losing about two stars per second, in the amount of the universe we can access, something like that. \n\nSo in theory, that means every second we delay in space colonization and the rest of it, is a huge loss in absolute terms. And so, any system that could credibly make the argument, “With this approach we can do this, and we can not lose these stars”—it seems to me that if you're then trading that against, “Oh, you can do this, and you can answer this question, solve this chemistry problem, or you can improve the world in this huge way”... \n\n**Rohin:**  So, if your claim is like, there are two people, Alice and Bob. And if you put Alice inside a giant HCH, and *really* just take the limit all the way to infinity, then that HCH is not going to be aligned with Bob, because Alice is probably going to be a consequentialist. Then yes, sure, that seems probably true. \n\n**Joe Collman:**  I'm saying it's not really going to be aligned with Alice in the sense that it will do what Alice wants. It will do what the HCH of Alice wants, but it won't do what Alice wants. \n\n**Ben Pace:**  Can I move to Donald’s question? \n\n**Rohin:**  Sure. \n\n**Donald Hobson:**  I was just going to say that, I think the whole reason that a giant HCH just answering the question is helpful: Suppose that the top-level question is “solve AI alignment.” And somewhere down from that you get “design better computers.” And somewhere down from that you get “solve this simple chemistry problem to help the \\[inaudible\\] processes or whatever.” \n\nAnd so, all of the other big world-improvement stuff is already being done above you in some other parts of the tree. So, literally the best thing you can do to help the world, if you find yourself down at the bottom of the HCH, is just solving that simple chemistry problem. Because all the other AI stuff is being done by some other copy of you. \n\n**Joe Collman:**  That's plausible, sure. Yeah, if you reason that way. My only thing that I'm claiming quite strongly, is that eventually with suitable amplification, everyone is going to come around to the idea, “I should give the response that is best for the world,” or something like that.\n\nSo, if your chain of reasoning takes you to “solving that chemistry problem is making a contribution that at the top level is best for the world,” then sure, that's plausible. It's hard to see exactly how you reliably get to draw that conclusion, that solving the question you've been asked *is* the best for the world. But yeah.\n\n**Rob Miles:**  Is it necessary for HCH that the humans in it have no idea where in the tree they are? Or could you pass in some contexts that just says, \"Solve this chemistry problem that will improve—\"\n\n**Scott:**  You actually want to send that context in. There's discussion about one of the things that you might do in solving HCH, which is: along with the problems, you pass in an ordinal, that's how much resources of HCH you're allowed. And so you say, like, \"Hey, solve this problem. And you get 10^100^ resources.” And then you're like, \"Here, sub-routine, solve this problem. You get 10^100^ / 2 resources.” And then, once you get down into zero resources, you aren't allowed to make any further calls.\n\nAnd you could imagine working this into the system, or you could imagine just corrigible humans following this instruction, and saying, “Whenever you get input in some amount of resources, you don't spend more than that.”\n\nAnd this resource kind of tells you some information about where you are in the tree. And not having this resource is a bad thing. The purpose of this resource is to make it so that there's a unique fixed point of HCH. And without it, there's not necessarily a unique fixed point. Which is basically to say that, I think that individual parts of an HCH are not intended to be fully thinking about all the different places in the tree, because they're supposed to be thinking locally, because the tree is big and complex. But I think that there's no “Let's try to hide information about where you are in the tree from the components.” \n\n**Rob Miles:**  It feels like that would partly solve Joe's problem. If you're given a simple chemistry problem and 10^100^ resources, then you might be like, \"This is a waste of resources. I'm going to do something smarter.\" But if you're being allocated some resources that seem reasonable for the difficulty of the problem you're being set, then you can assume that you're just part of the tree— \n\n**Scott:**  Well, if you started out with— \n\n**Ben Pace:**  Yeah, Rob is right that in real life, if you give me a trillion dollars to solve a simple chemistry problem, I'll primarily use the resources to do cooler shit. (*laughs*) \n\n**Joe Collman:**  I think maybe the difficulty might be here that if you're given a problem where the amount of resources is about right for the difficulty of solving the problem, but the problem isn't actually important.\n\nSo obviously “Do you like bananas?” is a bad example, because you could set up an IDA training scheme that *learns* how many resources to put into it. And there the top level human could just reply, \"Yes,\" immediately. And so, you don't—\n\n**Scott:**  I'm definitely imagining the resources are, it's reasonable to give more resources than you need. And then you just don't even use them. \n\n**Joe Collman:**  Right, yeah. But I suppose, just to finish off the thing that I was going to say is that yes, it still seems, with my kind of worry, it's difficult if you have a question which has a reasonable amount of resources applied to it, but is trivial, is insignificant. It seems like the question wouldn't get answered then. \n\n**Charlie Steiner:**  I want to be even more pessimistic, because of the unstable gradient problem, or any fixed point problem. In the limit of infinite compute or infinite training, we have at each level some function being applied to the input, and then that generates an output. It seems like you're going to end up outputting the thing that most reliably leads to a cycle that outputs itself, or eventually outputs itself. I don't know. This is similar to what Donald said about super-virulent memes.  \n \n\n9\\. Q&A: Disagreement and mesa-optimizers\n-----------------------------------------\n\n**Ray Arnold:**  So it seems like a lot of stuff was reducing to, “Okay, are we pessimistic or not pessimistic about safe AGI generally being hard?” \n\n**Rohin:**  Partly that, and partly me be plan-oriented, and Scott being science-oriented.\n\n**Scott:**  I mean, I'm trying to be plan-oriented in this conversation, or something. \n\n**Ray Arnold:**  It seems like a lot of these disagreements reduce to this ur-disagreement of, \"Is it going to be hard or easy?\", or a few different axes of how it's going to be hard or easy. And I'm curious, how important is it right now to be prioritizing ability to resolve the really deep, gnarly disagreements, versus just “we have multiple people with different paradigms, and maybe that's fine, and we hope one of them works out”? \n\n**Eli:  **I'll say that I have updated downward on thinking that that's important. \n\n**Ray Arnold:**  (*laughs*) As Mr. Double Crux. \n\n**Eli:  **Yeah. It seems like things that cause concrete research progress are good, and conversations like this one do seem to cause insights that are concrete research progress, but... \n\n**Ray Arnold:**  Resolving the disagreement isn't concrete research progress?\n\n**Eli:  **Yeah. It's like, “What things help with more thinking?” Those things seem good. But I used to think there were big disagreements—I don't know, I still have some probability mass on this—but I used to think there were big disagreements, and there was a lot of value on the table of resolving them.\n\n**Gurkenglas:**  Do you agree that if you can verify whether a system is thinking about agents, that you could also verify whether it has a [mesa-optimizer](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB)? \n\n**Scott:**  I kind of think that systems will have mesa-optimizers. There's a question of whether or not mesa-optimizers will be there explicitly or something, but that kind of doesn't matter. \n\nIt would be nice if we could understand where the base level is. But I think that the place where the base level is, we won't even be able to point a finger at what a “level” is, or something like that. \n\nAnd we're going to have processes that make giant spaghetti code that we don't understand. And that giant spaghetti code is going to be doing optimization. \n\n**Gurkenglas:**  And therefore we won’t be able to tell whether it's thinking about agency. \n\n**Scott:**  Yeah, I don't know. I want to exaggerate a little bit less what I'm saying. Like, I said, \"Ah, maybe if we work at it for 20 years, we can figure out enough transparency to be able to distinguish between the thing that's thinking about physics and the thing that's thinking about humans.\" And maybe that involves something that's like, “I want to understand logical mutual information, and be able to look at the system, and be able to see whether or not I can see humans in it.” I don't know. Maybe we can solve the problem.\n\n**Donald Hobson:**  I think that actually conventional mutual information is good enough for that. After all, a good bit of quantum random noise went into human psychology. \n\n**Scott:**  Mutual information is intractable. You need some sort of, “Can I look at the system…” It's kind of like this— \n\n**Donald Hobson:**  Yeah. Mutual information \\[inaudible\\] Kolmogorov complexity, sure. But computable approximations to that. \n\n**Scott:**  I think that there might be some more information on what computable approximations, as opposed to just saying “computable approximations,” but yeah. \n\n**Charlie Steiner:**  I thought the point you were making was more like “mutual information requires having an underlying distribution.” \n\n**Scott:**  Yeah. That too. I'm conflating that with “the underlying distributions are tractable,” or something. \n\nBut I don't know. There's this thing where you want to make your AI system assign credit scores, but you want it to not be racist. And so, you determine whether or not by looking at the last layer, you can determine the race of the participants. And then you optimize against that in your adversarial network, or something like that.\n\nAnd there's like *that*, but for thinking about humans. And much better than that.\n\n**Gurkenglas:  **Let's say we have a system that we can with our interpretability tools barely tell has some mesa-optimizer at the top level, that's probably doing some further mesa-optimization in there. Couldn't we then make that outer mesa-optimization explicit, part of the architecture, but then train the whole thing anew and repeat? \n\n**Scott:**  I feel like this process might cause infinite regress, but— \n\n**Gurkenglas:**  Surely every layer of mesa-optimization stems from the training process discovering either a better prior or a better architecture. And so we can increase the performance of our architecture. And surely that process converges. Or like, perhaps we just— \n\n**Scott:**  I’m not sure that just breaking it down into layers is going to hold up as you go into the system.\n\nIt's like: for us, maybe we can think about things with meta-optimization. But your learned model might be equivalent to having multiple levels of mesa-optimization, while actually, you can't clearly break it up into different levels.\n\nThe methods that are used by the mesa-optimizers to find mesa-optimizers might be sufficiently different from our methods, that you can't always just... I don't know. \n\n**Charlie Steiner:**  Also Gurkenglas, I'm a little pessimistic about baking a mesa-optimizer into the architecture, and then training it, and then hoping that that resolves the problem of distributional shift. I think that even if you have your training system finding these really good approximations for you, even if you use those approximations within the training distribution and get really good results, it seems like you're still going to get distributional shift. \n\n**Donald Hobson:**  Yeah. I think you might want to just get the mesa-optimizers out of your algorithm, rather than having an algorithm that's full of mesa-optimizers, but you've got some kind of control over them somehow. \n\n**Gurkenglas:**  Do you all think that by your definitions of mesa-optimization, AlphaZero has mesa-optimizers? \n\n**Donald Hobson:**  I don't know. \n\n**Gurkenglas:**  I'm especially asking Scott, because he said that this is going to happen. \n\n**Scott:**  I don't know what I think. I feel not-optimistic about just going deeper and deeper, doing mesa-optimization transparency inception.",
      "plaintextDescription": "This is an edited transcript of a conversation between Scott Garrabrant (MIRI) and Rohin Shah (DeepMind) about whether researchers should focus more on approaches to AI alignment that don’t require highly capable AI systems to do much human modeling. CFAR’s Eli Tyre facilitated the conversation.\n\nTo recap, and define some terms:\n\n * The alignment problem is the problem of figuring out \"how to develop sufficiently advanced machine intelligences such that running them produces good outcomes in the real world\" (outcome alignment) or the problem of building powerful AI systems that are trying to do what their operators want them to do (intent alignment).\n * In 2016, Hadfield-Mennell, Dragan, Abbeel, and Russell proposed that we think of the alignment problem in terms of “Cooperative Inverse Reinforcement Learning” (CIRL), a framework where the AI system is initially uncertain of its reward function, and interacts over time with a human (who knows the reward function) in order to learn it.\n * In 2016-2017, Christiano proposed “Iterated Distillation and Amplification” (IDA), an approach to alignment that involves iteratively training AI systems to learn from human experts assisted by AI helpers. In 2018, Irving, Christiano, and Amodei proposed AI safety via debate, an approach based on similar principles.\n * In early 2019, Scott Garrabrant and DeepMind’s Ramana Kumar argued in “Thoughts on Human Models” that we should be “cautious about AGI designs that use human models” and should “put more effort into developing approaches that work well in the absence of human models”.\n * In early February 2021, Scott and Rohin talked more about human modeling and decided to have the real-time conversation below.\n\nYou can find a recording of the Feb. 28 discussion below (sans Q&A) here.\n\n \n\n\n1. IDA, CIRL, and incentives\nEli:  I guess I want to first check what our goal is here. There was some stuff that happened online. Where are we according to you guys?\n\nScott:  I think Rohin spoke l",
      "wordCount": 14177
    },
    "tags": [
      {
        "_id": "EY623WWCXKTvN3kmj",
        "name": "Factored Cognition",
        "slug": "factored-cognition"
      },
      {
        "_id": "JNs7RFfpex7Fbd38q",
        "name": "Humans consulting HCH",
        "slug": "humans-consulting-hch"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "4mRJmYxNDnn7r2gNu",
        "name": "Iterated Amplification ",
        "slug": "iterated-amplification"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "6t9F5cS3JjtSspbAZ",
    "title": "Finite Factored Sets: LW transcript with running commentary",
    "slug": "finite-factored-sets-lw-transcript-with-running-commentary",
    "url": null,
    "baseScore": 30,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2021-06-27T16:02:06.063Z",
    "contents": {
      "markdown": "This is an edited transcript of Scott Garrabrant's May 23 LessWrong talk on finite factored sets. Video:\n\n Slides: [https://intelligence.org/files/Factored-Set-Slides.pdf](https://intelligence.org/files/Factored-Set-Slides.pdf)\n\nSince there are existing introductions to finite factored sets that are shorter ([link](https://www.lesswrong.com/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets)) or more formal ([link](https://www.lesswrong.com/s/kxs3eeEti9ouwWFzr/p/sZa5LQg6rrWgMR4Jx)), I decided to do things a bit differently with this transcript:\n\n*   I've incorporated Scott's slides into the transcript, and cut out some parts of the transcript that are just reading a slide verbatim.\n*   There was a text chat and Google Doc attendees used to discuss the talk while it was happening; I've inserted excerpts from this conversation into the transcript, in grey boxes.\n*   Scott also paused his talk at various times to answer questions; I've included these Q&A sections in white boxes.\n\nFinite Factored Sets\n====================\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets01.jpg)\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Anonymous α: </strong>Scott: do you have any suspicions that the basis of the mathematics is poorly fit to reality (q.v. Banach-Tarski decomposition)?</p><p><strong>Daniel Filan:</strong> I really liked it when causality came from pictures that I can look at. Does finite factored sets have pictures? One way this could be true is if it were some sort of category. Like, every other way I know of thinking about time (Newtonian mechanics, general relativity) is super geometric.</p><p><strong>Ramana Kumar:</strong> In case others are interested, I’ve been formalising these results in <a href=\"https://hol-theorem-prover.org/\"><u>HOL</u></a> (as I did for Cartesian Frames, which recently was<a href=\"https://github.com/deepmind/cartesian-frames\"><u> posted publicly</u></a>)</p><p><strong>Daniel Filan:</strong> (why) is HOL a good system in which to formalize these things?</p><p><strong>Ramana Kumar: </strong>Mainly: just because it’s possible, and I know the system/logic well. HOL is pretty good for most of math though - see<a href=\"http://www.cas.mcmaster.ca/sqrl/papers/SQRLreport18_rev2.pdf\"><u> this</u></a> for some advocacy.</p><p><strong>Daniel Kilan: </strong>thx, will give that a look!</p><p><strong>Quinn Dougherty: </strong>+1</p></td></tr></tbody></table>\n\nSome Context\n------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets02.jpg)\n\n**Scott:** I'm going to start with some context. For people who are not already familiar with my work: My main motivation is to reduce existential risk. This leads me to want to figure out how to [align](https://intelligence.org/2017/04/12/ensuring/) advanced AI. This leads me to want to become [less confused](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section2) about intelligence and about [agents embedded in their environment](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh). I think there are a lot of hard open problems in trying to figure out how to do this.\n\nThis leads me to do a bunch of weird math and philosophy, and this talk is going to be an example of some of that weird math and philosophy.\n\nFactoring the Talk\n------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets03.jpg)\n\n**Scott:** This talk can be split into \\\\(2 \\times 5 = 10\\\\) parts. For example, this slide is in part 1 (the combinatorics talk) and \\\\(\\color{green}{\\text{Table of Contents}}\\\\).\n\nI recognize that this table of contents has a lot more table and a lot less content than tables of contents normally have. But that's okay, because we'll have another table of contents in 6 more slides.\n\nSet Partitions\n--------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets04.jpg)\n\n**Scott:** All right. Let's do some background math.\n\nA partition of a set \\\\(S\\\\) is a way to view \\\\(S\\\\) as a disjoint union. You have a bunch of sets that are disjoint from each other, and you can union the sets together to get \\\\(S\\\\).\n\nWe'll also be using the [lattice structure](https://en.wikipedia.org/wiki/Partition_of_a_set#Refinement_of_partitions) on partitions:\n\n*   We'll say that \\\\(X\\\\) is *finer* than \\\\(Y\\\\) (and \\\\(Y\\\\) is *coarser* than \\\\(X\\\\)) if for all elements \\\\(s,t \\in S\\\\), if \\\\(S\\\\) and \\\\(T\\\\) are in the same part in \\\\(X\\\\), they have to be in the same part in \\\\(Y\\\\). So if \\\\(X\\\\) is finer than \\\\(Y\\\\), then you can think of \\\\(Y\\\\) as making some distinctions; and \\\\(X\\\\) has to make all of those distinctions, and possibly some more distinctions.\n*   The *common refinement* of two partitions \\\\(X\\\\) and \\\\(Y\\\\) is the coarsest partition that is finer than both \\\\(X\\\\) and \\\\(Y\\\\). So it has to make all of the distinctions that either \\\\(X\\\\) or \\\\(Y\\\\) make, and no more.\n\nThis picture on the right is all of the partitions of a four-element set, and the graph structure is showing the \"finer\" relation, with finer things closer to the bottom.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Glenn Willen: </strong>for anybody who had the same confusion I did at first: The dots on white in the diagram were singleton partitions.</p><p><strong>Daniel Filan:</strong> +1 I was also confused about this</p></td></tr></tbody></table>\n\nSet Factorizations\n------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets05.jpg)\n\n**Scott:** Now for some new stuff.\n\nA *factorization* is a set \\\\(B\\\\) of non-trivial partitions (\"factors\") of some set \\\\(S\\\\), such that for each way of choosing one part from each factor in \\\\(B\\\\), there exists a unique element of \\\\(S\\\\) in the intersection of those parts.\n\nA factorization is basically a multiplicative version of a partition. A factorization of \\\\(S\\\\) is a way to view \\\\(S\\\\) as a product, in the same way that a partition of \\\\(S\\\\) is a way to view \\\\(S\\\\) as a disjoint union.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Anonymous β: </strong>This looks like dirichlet product of species</p><p><strong>Jason Gross:</strong> Is the universal property of a factorization the same as the universal property of a partition in the opposite category?</p><p><strong>Luke Miles: </strong>The intersection of the parts of the factors should have exactly one element right?</p><p><strong>Ben Pace: </strong>That sounds right.</p><p><strong>Glenn Willen:</strong> The intersection of one part from each factor should be one element, if I'm understanding right, yeah.</p><p><strong>Jason Gross: </strong>what does \"nontrivial\" mean in \"nontrivial partition\"?</p><p><strong>Daniel Filan: </strong>trivial partition has only one part</p><p><strong>Rob Bensinger: </strong>'trivial partition' has <i>exactly</i> one part</p><p><strong>Jason Gross:</strong> singleton set has no factorizations, empty set has … exactly one factorization?</p><p><strong>Ramana Kumar: </strong>singleton does have a factorization.&nbsp;<span class=\"math-tex\">\\(\\{\\{x\\}\\}\\)</span>&nbsp;factorizes&nbsp;<span class=\"math-tex\">\\(\\{x\\}\\)</span></p><p><strong>Ramana Kumar: </strong>no wait... that's trivial</p></td></tr></tbody></table>\n\n**Scott:** If we have a factorization \\\\(B=\\{b_0,…,b_n\\}\\\\) of our set \\\\(S\\\\), there will be a bijection between \\\\(S\\\\) and the product \\\\(b_0×⋯×b_n\\\\). This bijection comes from sending an element \\\\(s \\in S\\\\) to the tuple consisting only of parts containing that element, \\\\(([s]_{b_0},…,[s]_{b_n})\\\\).\n\nBecause of this, the size of \\\\(S\\\\) is always going to be equal to the product of the sizes of \\\\(S\\\\)'s factors.\n\nThis bijection is basically restating the definition above. It's saying that for each way of choosing one part from each factor, we're going to have a unique element in the intersection of those parts.\n\nAnother thing we can observe is that any factor in a factorization must always be a partition into parts of equal size, because otherwise you're not going to be able to make this bijection work out.\n\nWe will often be working with a *finite factored set*, which is a pair \\\\((S,B)\\\\), where \\\\(S\\\\) is a finite set and \\\\(B\\\\) is a factorization of \\\\(S\\\\).\n\nThis definition is a little bit loopy. There's this thing where I'm first introducing \\\\(S\\\\), and I'm saying that \\\\(B\\\\) is a collection of partitions of \\\\(S\\\\). But there's another way you can view it in which you first define \\\\(B\\\\), which is just a collection of sets. Then you take the product of all of the sets in \\\\(B\\\\), and this gives you a set \\\\(S\\\\), which is tuples of the product of the elements in \\\\(B\\\\). Then you can identify, with each element of a \\\\(b \\in B\\\\), the subset of the space of tuples that you get by projecting onto that part in the tuple.\n\nSo there's this temptation to define \\\\(S\\\\) first and then define \\\\(B\\\\) from \\\\(S\\\\), and there's also a temptation to do the opposite. This bijection is saying that we can take either lens. \n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>John Wentworth: </strong>One other way to picture a “factored set”: it’s a set with a coordinate system attached. Each point in the set corresponds to a unique coordinate-tuple and vice versa (e.g.&nbsp;<span class=\"math-tex\">\\(x, y, z\\)</span>&nbsp;uniquely specify each point in our usual 3D space).</p><p>To translate this to the “partitions” view: each partition is the set of equivalence classes induced by one coordinate. For instance, the&nbsp;<span class=\"math-tex\">\\(x\\)</span>-coordinate partitions 3D space into planes, each of the form&nbsp;<span class=\"math-tex\">\\(x\\)</span>=&lt;constant&gt;.</p><p>(I asked Scott why he’s using this more-complicated-seeming definition in terms of sets, and he has some good reasons, but I still think it’s easier to start with the coordinate-system view when first seeing this stuff.)</p><p><strong>Ramana Kumar: </strong>nice!</p><p><strong>Ramana Kumar:</strong> Some things that confused me initially about FFS:</p><ul><li>(As Scott says on slide 5) The set&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;isn’t important since you can think of&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;as generated from the sets in&nbsp;<span class=\"math-tex\">\\(B\\)</span>&nbsp;(i.e., the Cartesian product of all the elements of&nbsp;<span class=\"math-tex\">\\(B\\)</span>).</li><li>The partitions in&nbsp;<span class=\"math-tex\">\\(B\\)</span>&nbsp;aren’t themselves important (I have this as a cached belief but can’t reconstruct it). It is perhaps better to think of each partition as a function from&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;(to an arbitrary set, which you could say is the set of part labels). These functions can be thought of as measurements of variables: the value a variable takes in a given observed world.</li></ul><p>Philosophically, what are good ways to think of what the set&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;is? Currently I think of it as a set of possible (complete) worlds, at a certain level of abstraction. (Similar to the set&nbsp;<span class=\"math-tex\">\\(W\\)</span>&nbsp;in a <a href=\"https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames\">Cartesian Frame</a> over&nbsp;<span class=\"math-tex\">\\(W\\)</span>)</p><p><strong>Scott: </strong>In the Cartesian frame analogy&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;is&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>, not&nbsp;<span class=\"math-tex\">\\(W\\)</span>.&nbsp;<span class=\"math-tex\">\\(\\Omega\\)</span>&nbsp;is&nbsp;<span class=\"math-tex\">\\(W\\)</span>&nbsp;(from the <a href=\"https://www.lesswrong.com/posts/6t9F5cS3JjtSspbAZ/finite-factored-sets-lw-transcript-with-running-commentary#Temporal_Inference\">inference slide</a>).</p><p><strong>Anonymous γ:</strong> I don’t understand what a bijection is.</p><p><strong>Quinn Dougherty:</strong> invertible function</p><p><strong>Ben Pace:</strong> Do the multiplication signs basically mean “intersection”?</p><p><strong>Daniel Filan:</strong> No</p><p><strong>Ben Pace</strong>: Daniel, what does it mean here?</p><p><strong>Anonymous δ:</strong>&nbsp;<span class=\"math-tex\">\\(\\times\\)</span>&nbsp;= Cartesian product aka tupling</p><p><strong>Daniel Filan:&nbsp;</strong><span class=\"math-tex\">\\(\\{1,2\\} \\times \\{3,4\\} = \\{(1,3), (1,4), (2,3), (2,4)\\}\\)</span></p><p><strong>Anonymous ε:</strong> I think it corresponds to intersection under the bijection though</p><p><strong>Ramana Kumar:</strong> @[Anonymous ε] I don't think that's right</p><p><strong>Ben Pace: </strong>That’s what I thought but I also thought it was supposed to end up as a single element…</p><p><strong>Diffractor:</strong> The real number line&nbsp;<span class=\"math-tex\">\\(\\times\\)</span>&nbsp;the real number line is the 2d plane, that has a lot more than one point</p><p><strong>Daniel Filan: </strong>Yep. In this setting, one ordered pair corresponds to the thing that's the intersection of all the pair elements.&nbsp;<span class=\"math-tex\">\\(\\{A,B\\} \\times \\{C,D\\} = \\{(A,C), (A,D), (B,C), (B,D)\\}\\)</span>. If&nbsp;<span class=\"math-tex\">\\(A\\)</span>,&nbsp;<span class=\"math-tex\">\\(B\\)</span>,&nbsp;<span class=\"math-tex\">\\(C\\)</span>, and&nbsp;<span class=\"math-tex\">\\(D\\)</span>&nbsp;are themselves sets, you can then go from e.g.&nbsp;<span class=\"math-tex\">\\((A,C)\\)</span>&nbsp;to the intersection of&nbsp;<span class=\"math-tex\">\\(A\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(C\\)</span>. And in the case of a factorization, you can go back from some subset to the tuple of sets that it’s the intersection of.</p><p><strong>Jasper Chapman-Black:</strong> there's a bijection between the set-of-elements and the set-of-tuples-of-factors, is the claim being made</p><p><strong>Leo Gao:</strong> Which operation is the bijection over?</p><p><strong>Diffractor:</strong> Bijection between base set and the cartesian product of the factorizations</p></td></tr></tbody></table>\n\n**Scott:** To me, this is an extremely natural notion. I want to really push this naturalness, so I want to describe how factorization is dual to the definition of a partition.\n\n<table style=\"background-color:rgb(255, 255, 255);border-bottom:1px double rgb(179, 179, 179);border-left:1px double rgb(179, 179, 179);border-right:1px double rgb(179, 179, 179);border-top:1px double rgb(179, 179, 179)\"><tbody><tr><td style=\"border-bottom:1px double rgb(217, 217, 217);border-left:1px double rgb(217, 217, 217);border-right:1px double rgb(217, 217, 217);border-top:1px double rgb(217, 217, 217);padding:0.4em\">A <strong>partition</strong> is a set&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;of&nbsp;<span class=\"math-tex\">\\(\\color{red}{\\text{non-empty}}\\)</span><br><span class=\"math-tex\">\\(\\color{orange}{\\text{subsets}}\\)</span>&nbsp;of&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;such that the obvious<br>function&nbsp;<span class=\"math-tex\">\\(\\color{green}{\\text{from}}\\)</span>&nbsp;the&nbsp;<span class=\"math-tex\">\\(\\color{blue}{\\text{disjoint union}}\\)</span>&nbsp;of<br>the elements of&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;<span class=\"math-tex\">\\(\\color{purple}{\\text{to}}\\)</span>&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;is a bijection.</td><td style=\"border-bottom:1px double rgb(217, 217, 217);border-left:1px double rgb(217, 217, 217);border-right:1px double rgb(217, 217, 217);border-top:1px double rgb(217, 217, 217);padding:0.4em\">A <strong>factorization</strong> is a set&nbsp;<span class=\"math-tex\">\\(B\\)</span>&nbsp;of&nbsp;<span class=\"math-tex\">\\(\\color{red}{\\text{non-trivial}}\\)</span><br><span class=\"math-tex\">\\(\\color{orange}{\\text{partitions}}\\)</span>&nbsp;of&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;such that the obvious<br>function&nbsp;<span class=\"math-tex\">\\(\\color{green}{\\text{to}}\\)</span>&nbsp;the&nbsp;<span class=\"math-tex\">\\(\\color{blue}{\\text{product}}\\)</span>&nbsp;of<br>the elements of&nbsp;<span class=\"math-tex\">\\(B\\)</span>&nbsp;<span class=\"math-tex\">\\(\\color{purple}{\\text{from}}\\)</span>&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;is a bijection.</td></tr></tbody></table>\n\nYou can see here I have two alternative definitions of partition and factorization. \n\nFor partition: We have a bunch of (non-empty) subsets of \\\\(S\\\\); we have functions to \\\\(S\\\\), which are the inclusions; we can add all these functions together to give a function from their disjoint union to \\\\(S\\\\); and we want this function to be a bijection.\n\nSimilarly, a factorization is a set of (non-trivial, meaning not having exactly one element) partitions of \\\\(S\\\\) such that the obvious function *to* the *product* of the elements of \\\\(B\\\\) *from* \\\\(S\\\\) is a bijection. So for each partition in \\\\(B\\\\), there's this projection onto that partition; and we can multiply all of these together to get a function from \\\\(S\\\\) to the product, and we want this function to be a bijection.\n\nYou can see that in these two definitions, I’m pointwise dualizing a bunch of the words to show that there's a duality between partitions and factorizations. You'll notice that there’s also a duality between subsets and partitions. You can view a partition as \"that which is dual to a subset,\" and you can also view it as \"that which is built up out of subsets in a certain way.\" When I say that factorizations are dual to partitions, I'm using the \"that which is built up from subsets in a certain way\" conception of partitions.\n\nI'm first going to give some examples, and then I'll open up the floor for questions about factorizations.\n\nEnumerating Factorizations\n--------------------------\n\n**Scott:** Exercise: What are the factorizations of a four-element set, \\\\(\\{0, 1, 2, 3\\}\\\\)?\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nThe first thing to observe is that given any set with at least one element, we're always going to be able to define at least one factorization, a sort of trivial factorization.\n\n\\\\\\[\\begin{split} \\{   \\{ \\{0\\},\\{1\\},\\{2\\},\\{3\\} \\}   \\} \\end{split} \\begin{split}                 \\underline{ 0     1    2    3  } \\end{split}\\\\\\]\n\nThis is a kind of one-dimensional factorization. We have one factor, which is the discrete partition.\n\nRecall that in the definition of factorization, we want it to be the case that for each way of choosing one part from each factor, we want a unique element of \\\\(S\\\\) in the intersection of those parts. The trivial factorization satisfies that.\n\nWhat if we want a nontrivial factorization? Well, recall that the product of the sizes of the factors always has to equal the size of \\\\(S\\\\). The only way to express \\\\(4\\\\) as a nontrivial product is to express it as a \\\\(2 \\times 2\\\\) product. So we're looking for two partitions/factors that are going to combine to give us \\\\(S\\\\).\n\nAll of our factors also have to break \\\\(S\\\\) into parts of equal size. So we're looking for \\\\(2\\\\) partitions that each break our \\\\(4\\\\)-element set into \\\\(2\\\\) parts of size \\\\(2\\\\). There are exactly three partitions that break a 4-element set into \\\\(2\\\\) parts of size \\\\(2\\\\): \\\\(\\{\\{0, 1\\}, \\{2, 3\\}\\}\\\\), which separates the small numbers from the large numbers; \\\\(\\{\\{0, 2\\}, \\{1, 3\\}\\}\\\\), which separates the even numbers from the odd numbers; and \\\\(\\{\\{0, 3\\}, \\{1, 2\\}\\}\\\\), which separates the inner numbers from the outer numbers.\n\nWe're going to be able to choose any pair of these. So we're going to have three more factorizations:\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets06.jpg)\n\nThe point here is that for each way of choosing, you're going to get a unique element that is the conjunction of those two choices. \"Small or large?\" and \"even or odd?\" jointly pick out a specific element of \\\\(\\{0, 1, 2, 3\\}\\\\).\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Adam Scherlis:</strong> (I get four non-isomorphic factorizations)</p><p><strong>Gurkenglas:</strong> Adam, are you sure three of them aren't isomorphic?</p><p><strong>Adam Scherlis:</strong> yeah, on screen now :)</p><p><strong>Adam Scherlis:</strong> distinguished by which element shares no partition elements with 0</p><p><strong>David Manheim:</strong> Isomorphic over relabeling of items, or non-order dependent given the labels?</p><p><strong>David Manheim:</strong> Right.</p><p><strong>Adam Scherlis:</strong> over relabeling of <i>partitions</i> is what I meant, sorry</p></td></tr></tbody></table>\n\n**Scott:** In general, given an \\\\(n\\\\)-element set, you can ask how many factorizations there are of that set. We have the sequence here.\n\nYou can notice that if \\\\(n\\\\) is prime, there’s only one factorization, which is the trivial one.\n\nA very surprising fact to me was that this sequence did not appear on [OEIS](https://oeis.org/). OEIS is a database that combinatorialists use to share their sequences of integers, and check whether their sequences have been studied by other people and look for connections. This sequence didn't show up, even when I did minor modifications to deal with the degenerate cases.\n\nI just don't get this. To me this basically feels like the multiplicative version of the [Bell numbers](https://oeis.org/A000110). The Bell numbers count how many partitions there are in a set of size \\\\(n\\\\). It's sequence #110 out of over 300,000 on the OEIS. And yet this sequence doesn't show up at all. I don't really have an understanding of why that happened.\n\nOkay. Now I'm going to pause for questions.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Quinn Dougherty: </strong>remind me what oeis is</p><p><strong>Glenn Willen: </strong>online encyclopedia of integer sequences</p><p><strong>Quinn Doughterty: </strong>thanks</p><p><strong>Diffractor: </strong>The giant internet database of number sequences</p><p><strong>Alex Mennen: </strong>I think the 0th element should be 0</p><p><strong>Alex Mennen:&nbsp;</strong>wait, nvm</p><p><strong>Adam Scherlis: </strong>OEIS omission really is surprising, nice find Scott</p><p><strong>Jasper Chapman-Black: </strong>Checked&nbsp;<span class=\"math-tex\">\\((n-1)\\)</span>&nbsp;in OEIS and that was missing too. Huh. That’s super surprising.</p><p>I’ve seen 1680, 5040, 15120 a <i>lot</i> before in random OEIS sequences which are (factorization sizes&nbsp;<span class=\"math-tex\">\\(- \\ 1\\)</span>), I suspect there’s some Crazy Equivalence going on here</p><p><strong>Daniel Filan: </strong>Shouldn't the element corresponding to&nbsp;<span class=\"math-tex\">\\(|S|=1\\)</span>&nbsp;be&nbsp;<span class=\"math-tex\">\\(0\\)</span>?</p><p><strong>Ramana Kumar: </strong>yeah is&nbsp;<span class=\"math-tex\">\\(|\\text{Fact}(S)| = 0\\)</span>&nbsp;for&nbsp;<span class=\"math-tex\">\\(|S| = 1\\)</span>&nbsp;?</p><p><strong>Alex Mennen: </strong>no, there's the empty factorization</p><p><strong>Diffractor: </strong>Actually, it's 1 because there's bullshit going on with the empty set and single-element set</p><p>A whole lot of \"vacuously true\" for those two cases</p><p><strong>Daniel Filan: </strong>aaaah</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Anonymous ζ: </strong>There's a question in the chat about how many factorizations a singleton set has. This table says it has one. What is it?</p><p><strong>Scott:</strong> It's the empty factorization. It has no factors, and for every way—the one way—of choosing one part from every factor in the empty set of factorizations, you'll have a single element in the intersection of those parts.</p><p><strong>Anonymous η: </strong>Partitions were over graphs, right?</p><p><strong>Scott:</strong> No, the graph was just a way to organize all of the partitions of a four-element set. There's no graphs involved here.</p><p><strong>Anonymous η: </strong>Ah, okay, cool.</p></td></tr></tbody></table>\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Anonymous δ: </strong>The graph is a <a href=\"https://en.wikipedia.org/wiki/Hasse_diagram\">hasse diagram</a></p><p><strong>Jason Gross: </strong>Is the number of factorizations related to the number of groups of a given size?</p><p><strong>Quinn Dougherty: </strong>jason ask that question i wanna know the answer</p><p><strong>Daniel Filan: </strong>(asking questions in this context is done via voice, not text)</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Anonymous θ:</strong> Is there a similar sort of thing for factorizations, or not really? Or is it not relevant?</p><p><strong>Scott:</strong> If we were to do something similar to having the notion of \"finer\" in factorizations, I think it would look something like: Given a factorization&nbsp;<span class=\"math-tex\">\\(B\\)</span>&nbsp;of a set&nbsp;<span class=\"math-tex\">\\(S\\)</span>, you can take two of your factors&nbsp;<span class=\"math-tex\">\\(b_0,b_1 \\in B\\)</span>&nbsp;and replace those two factors with a single factor&nbsp;<span class=\"math-tex\">\\(b_0\\vee_S b_1\\)</span>&nbsp;that is the common refinement of those two factors.</p><p>This gives you a factorization with fewer factors, and you can view this as analogous to merging two subsets in a partition.</p><p>So you can have a \"finer\" relation, which says whether you can get from one factorization to another in this way. I'd say that the factorization with more factors is the multiplicatively finer one in that picture.</p><p>I think that you're not going to get the meets and joins. Or maybe you get one of them, but not the other? I’m not sure; I didn't think about that. But you can at least get a partial order on them.</p></td></tr></tbody></table>\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Anonymous ι: </strong>i.e.&nbsp;<span class=\"math-tex\">\\(2 \\times 2 \\times 3\\)</span>&nbsp;is finer than&nbsp;<span class=\"math-tex\">\\(2 \\times 6\\)</span></p><p><strong>Kyle: </strong>Does product mean intersection in the definition of factors? It doesn't seem to mean cartesian product</p><p><strong>Ramana Kumar: </strong>product is cartesian product</p><p><strong>Quinn Dougherty: </strong>isn't the common refinement one of a meet or a join?</p><p><strong>Diffractor:</strong> Common refinement is a supremum in the ordering on partitions where the indiscrete partition is at the bottom and the discrete partition is at the top</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Anonymous 5: </strong>Is there a meaning attached to a particular product? Like, in your example, let's say you pick the&nbsp;<span class=\"math-tex\">\\(\\{\\{0,1\\},\\{0,2\\}\\}\\)</span>&nbsp;product. Is there structure in the resulting output of that multiplication, or are they all equivalent once you do the multiplication?</p><p><strong>Scott:</strong> You could think of these as four different structures that you can put on your four-element set. Three of them are isomorphic, so you can relabel the elements of your set to get the same structure. But I'm thinking of them as different even though they're isomorphic. It depends on what version of \"the same\" you want.</p><p><strong>Ben Pace: </strong>I'll ask Jason Gross' question, which is, \"Is the number of factorizations related to the number of groups of a given size?\"</p><p><strong>Scott: </strong>It's related in that they agree whenever the number is prime. (<i>laughs</i>) I think it's not directly related, although I think that if you drew out the chart, it would look similar. These numbers grow a lot faster. I have proved some things about this chart, like that they all end in 1 except for the fourth one.</p></td></tr></tbody></table>\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Jason Gross: </strong>I don't see how it could be related now; the number of groups of order 8 is 5, where we have 1681 factorizations</p><p><strong>Vanessa Kosoy: </strong>Is there a reason almost all of these numbers are equal to 1 modulo 10?</p><p><strong>Daniel Filan: </strong>+1 Vanessa</p><p><strong>Alex: </strong>I'd be interested in factorizations of 81 (a sudoku grid) and whether it has anything interesting about it</p><p><strong>Alex Mennen: </strong>perhaps the ending in 1 is a special case of something like, for any&nbsp;<span class=\"math-tex\">\\(n\\)</span>, all but finitely many of these are 1 mod&nbsp;<span class=\"math-tex\">\\(n\\)</span></p><p><strong>Daniel Filan: </strong>oh</p><p><strong>David Manheim: </strong>Vanessa: it is a product plus the single trivial factorization?</p><p><strong>Diffractor: </strong>The ending in 1 is because of the partition that's like \"every element is in its own equivalence class\"</p><p><strong>Daniel Filan: </strong>+1 David</p><p><strong>Glenn Willen: </strong>a product, or a sum of products?</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Gurkenglas:</strong> Have you looked at the sequence of isomorphism classes of factorizations?</p><p><strong>Scott:</strong> Yeah, the sequence of isomorphism classes of factorizations is going to be equal to the multiplicative partition function. It's the number of ways to express&nbsp;<span class=\"math-tex\">\\(n\\)</span>&nbsp;as a product. And that's simple enough that it does show up on OEIS; I forget exactly what the sequence is.</p><p>Okay. I'm going to move on to the main talk.</p></td></tr></tbody></table>\n\nThe Main Talk (It's About Time)\n===============================\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets07.jpg)\n\nThe Pearlian Paradigm\n---------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets08.jpg)\n\n**Scott:** We can't talk about time without talking about [Pearlian causality](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs/p/hzuSDMx7pd2uxFc5w). This paradigm allows you to, given a collection of variables and a joint probability distribution over those variables, infer causal/temporal relationships between the variables.\n\n(In this talk, I'm going to be using \"causal\" and \"temporal\" interchangeably, though I think that there might be more to say there philosophically.)\n\nPearl can infer temporal data from statistical data. I'm not going to explain exactly how it works, because I'm not going to be technically dependent on it. This is really great. There's this adage, \"Correlation does not imply causation,” and I think that it's just wrong and Pearl is correctly inferring causation from correlation all over the place using the combinatorial structure of the correlation.\n\nThe way that he does this is by inferring these graphs, like the one on the left. I think this graph is not entirely inferable from a probability distribution on the variables as it is, but some graphs are. You can definitely infer from a probability distribution that came from this graph, using the Pearlian paradigm, whether or not \"the ground is wet\" is before \"the ground is slippery,\" for example.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Jason Gross:</strong> I thought Pearl could only infer lack of causation?</p><p><strong>Matthew Graves:</strong> yes but you can process-of-elimination</p><p><strong>Rick Schwall:</strong> Correlation does not imply causation for only 2 variables.&nbsp; Pearl makes use of more variables.</p><p><strong>Jason Gross:</strong> re process of elimination: you need to assume that you know all the possible events/occurences, right?</p><p><strong>Matthew Graves:</strong> I remember he talks a lot about how he handles the possibility of common causes but it's been a while</p><p><strong>Jason Gross:</strong> I should actually read Pearl sometime...</p><p><strong>Mark Xu:</strong> there are results on how to infer causation from 2 variables if you know that they have non-gaussian independent random errors I think</p></td></tr></tbody></table>\n\n**Scott:** The Pearlian paradigm is great. It's my main starting point. However, I claim that it's cheating: the \"given a collection of variables\" is actually hiding a lot of the work.\n\nNormally people emphasize the joint distribution in saying that Pearl is inferring temporal data from statistical data, but I think that what's actually going on is that Pearl is inferring temporal data from statistical data *together with* *factorization data*.\n\nFor example, let's say all the variables in this picture are binary. The choice that we're working with these 5 variables, as opposed to just working with 32 distinct worlds without labels, is actually doing a lot of the work.\n\nI think this issue is also entangled with a failure to adequately handle abstraction and determinism.\n\nLet's say you're given these 5 variables and a joint probability distribution on the variables. One thing you could do is say, \"I'm going to forget the variables that were given to me, and I'm just going to consider all Bell number of 32 different ways that the world can be.\"\n\nAnd you could try to do Pearlian inference on this set of variables. The set is huge, but I'm working from an ontology where I don't care about things being huge. But you're going to run into a problem, which is that many of these variables that you define are going to be deterministic functions of each other, or partially deterministic functions of each other. Some of your variables/partitions will be coarsenings of other variables/partitions.\n\nIt turns out that the Pearlian paradigm, or the part that I'm trying to copy here, doesn't work well with all of this determinism. So you can't just throw away the variables you were given.\n\nThat's why I think that this cheating I'm accusing the Pearlian paradigm of doing is entangled with not being able to handle abstraction and determinism. By \"abstraction\" I mean I want some of my variables to be able to be coarser or finer copies of each other, which is similar to determinism.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Alex Ray:</strong> The choice of what variables to work with has always been my biggest problem w/ pearl causal inference</p><p>Pearl assumes that you have some “relevance machine” to narrow down to variables, and some separate process to pick from all graphs that involve those variables</p></td></tr></tbody></table>\n\nWe Can Do Better\n----------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets09.jpg)\n\n**Scott:** So we're going to do something new.\n\n*   Where Pearl was given a collection of variables, we're going to take all partitions of a given set.\n*   Where Pearl inferred a directed acyclic graph, we're going to infer a finite factored set.\n*   In the Pearlian world, we could read off of our directed acyclic graph something that we'd want to call time. When we have a path from one node to another in a DAG, that means that the first node is before the second node.  \n      \n    In the factored sets paradigm, we're going to have to define our own way of looking at a finite factored set and telling when some partitions are before or after other partitions.\n*   Pearl draws special attention to when two nodes in your DAG have no common ancestor. This corresponds to orthogonality, or independence. (In this talk, I'm going to use the word \"independence\" when I'm talking about probability, and I'm going to use the word \"orthogonality\" when I'm talking about combinatorial properties.)  \n      \n    We're going to be able to read something analogous off of our finite factored sets.\n*   Pearl has a concept, *d*-separation, which is kind of complicated. And the reason *d*-separation is interesting is that it's equivalent to conditional independence in all probability distributions you could put on your DAG.  \n      \n    We're going to define an analogous concept, *conditional orthogonality*, which we're going to be able to read off of a finite factored set.\n*   In the Pearlian world, we have that *d*-separation gives a compositional graphoid. In our world, we're going to have a compositional semigraphoid satisfied by conditional orthogonality. (There's a fifth graphoid axiom that we're not going to satisfy, but I argue we didn't want to satisfy it in the first place.)\n*   As with *d*-separation in Pearl, we're going to have a result saying that conditional orthogonality exactly corresponds to conditional independence in all probability distributions that you could put on your structure.\n*   We're going to be able to do temporal inference.\n*   And we're going to discuss some applications.\n\nThis is a green slide, so it's a table of contents slide; here are some slide numbers.\n\nWe've already talked about partitions and factorizations. Now, we're going to talk about time and orthogonality.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>David Manheim: </strong>...and I immediately jumped to \"wait, as long as we have a prime number of elements, we can infer things?\" before I realized that the number is the product of the variables, not the number of them.</p><p><strong>Quinn Dougherty: </strong>what's graphoids and semigraphoids</p><p><strong>Glenn Willen: </strong>\"A graphoid is a set of statements of the form, \"X is irrelevant to Y given that we know Z\" where X, Y and Z are sets of variables. The notion of \"irrelevance\" and \"given that we know\" may obtain different interpretations, including probabilistic, relational and correlational, depending on the application.\" -- <a href=\"https://en.wikipedia.org/wiki/Graphoid\"><u>Wikipedia</u></a></p><p><strong>Quinn Dougherty: </strong>thanks</p><p><strong>David Manheim: </strong>(But Scott's new model still only works if the variables are discrete. Extending to continuity is gonna be even harder than for Pearl's causality.)</p><p><strong>Edward Kmett: </strong>i really want the mapping between slide numbers and types to be a finite factorization</p><p><strong>Daniel Filan:</strong> \"What is a semigraphoid\" - <a href=\"https://foxflo.github.io/whatis/semigraphoid.pdf\"><u>https://foxflo.github.io/whatis/semigraphoid.pdf</u></a></p><p>(I haven't read this)</p><p><strong>Ben Pace: </strong>&lt;3 Ed Kmett</p></td></tr></tbody></table>\n\nTime and Orthogonality\n----------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets10.jpg)\n\n**Scott:** Before we define time and orthogonality, I'm going to define something called *history*.\n\nWe're starting with a fixed finite factored set, and that’s our context. \\\\(X\\\\) is going to be a partition of \\\\(S\\\\), and the history of \\\\(X\\\\) is the smallest set of factors \\\\(H \\subseteq B\\\\) such that, if I take an element of \\\\(S\\\\) and hide it from you, and you want to know what part in \\\\(X\\\\) it's in, it suffices for me to tell you what part it's in within each of the factors in \\\\(H\\\\).\n\nSo if I want to determine the \\\\(X\\\\)-value of an element of \\\\(S\\\\), it suffices for me to know the value of all of the factors in \\\\(H\\\\). So what's written here is saying that if two elements agree on all of the factors in \\\\(H\\\\), then they have to agree on \\\\(X\\\\).\n\n(Whenever I have a partition, I can kind of think of it as a variable. So when I say \"the \\\\(X\\\\)-value of an element,\" I mean \"what part it's in within \\\\(X\\\\).\")\n\nThis definition is the central starting point. We're going to define everything from this, so it's worth pausing on.\n\nYou can kind of think of factors as basic properties from which everything can be computed. And I want to know which factors are going into the computation of \\\\(X\\\\), which factors determine the value of \\\\(X\\\\).\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Leo Gao:</strong> My understanding: a factorization lets you break a set&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;into a bunch of different partitions of&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;such that if you pick one part from each partition and get their intersection, you always identify a specific thing; a history is the smallest factorization that uniquely specifies your partition for any way you could look at it.</p><p>I think one useful way to think about this is the shape of a factorization is [partitions, parts, elements-within-part]</p><p><strong>John Wentworth:</strong> Taking the “FFS = set with coordinate system” view, history is the minimum set of coordinates on which some function depends. So e.g. if we have a function on&nbsp;<span class=\"math-tex\">\\(\\mathbb{R}^3\\)</span>&nbsp;which can be expressed in terms of just&nbsp;<span class=\"math-tex\">\\(x, y\\)</span>&nbsp;(but not in terms of&nbsp;<span class=\"math-tex\">\\(x\\)</span>&nbsp;alone or&nbsp;<span class=\"math-tex\">\\(y\\)</span>&nbsp;alone), then its history is&nbsp;<span class=\"math-tex\">\\(x, y\\)</span>.</p><p>When talking about distributions on factored sets, each coordinate is effectively an independent random variable, and everything else is deterministically computed from those independent random variables. In other words, the coordinates are like independent random bits from which other variables are generated. So in that picture, “<span class=\"math-tex\">\\(X\\)</span>&nbsp;before&nbsp;<span class=\"math-tex\">\\(Y\\)</span>” roughly says that the random bits on which&nbsp;<span class=\"math-tex\">\\(Y\\)</span>&nbsp;depends are a superset of the random bits on which&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;depends.</p><p><strong>Diffractor:</strong> history of&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;= \"smallest set of coordinates for the cartesian product where knowing those coordinates alone can tell you which equivalence class you're in\"</p><p><strong>Felix: </strong>is there a unique history for every&nbsp;<span class=\"math-tex\">\\(X\\)</span>?</p><p><strong>Alex Ray:</strong> This is a very nice compact description of&nbsp;<span class=\"math-tex\">\\(X\\)</span></p><p><strong>Diffractor:</strong> Yes, it's unique</p><p><strong>Ramana Kumar:</strong> @Felix yes</p><p>it is unique because it's smallest</p><p><strong>Gustavo Lacerda:</strong> Can we frame it in terms of “sufficiency”?</p><p><strong>Ramana Kumar:</strong> (there is something to prove though - that all sets of factors with this property overlap)</p><p><strong>Alex:</strong> This seems kind of intuitive from the \"coordinate space\" perspective of factorization</p><p><strong>Jason Gross:</strong> Why can't you have two different smallest sets of the same size, neither of which is a subset of the other?</p><p><strong>Daniel Filan:</strong> What's an example where history(<span class=\"math-tex\">\\(X\\)</span>) isn't&nbsp;<span class=\"math-tex\">\\(X\\)</span>?</p><p><strong>Diffractor:</strong> Better to frame it in terms of \"necessity\". To Jason, it just works</p><p><strong>Ramana Kumar:</strong> @Daniel when X is not in B</p><p><strong>David Manheim:</strong> Once we have the finite factored set size, we know at least how many variables x values we have - but we don't know how many variables we have...</p><p><strong>Diffractor:</strong> For Daniel, history(<span class=\"math-tex\">\\(X\\)</span>) is not a partition, it's a set of sets of partitions</p><p><strong>Ramana Kumar: </strong>@[Diffractor] but history(<span class=\"math-tex\">\\(X\\)</span>) can sometimes be&nbsp;<span class=\"math-tex\">\\(\\{X\\}\\)</span></p><p><strong>Luke Miles:</strong> \"The future has more history than the past\"</p><p><strong>Quinn Dougherty:</strong> When Scott says \"partitions are like variables\" does he mean variable in like the data science sense? like a column of tabular data?</p><p><strong>Diffractor:</strong> Which broadly correspond to \"which coordinates of our big hypercube are <i>strictly necessary</i> for knowledge of them to pin down which equivalence class of&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;you're in</p><p><strong>Quinn Dougherty: </strong>What’s the analogy between partition and variable?</p><p><strong>Ramana Kumar: </strong>Given a partition&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;of&nbsp;<span class=\"math-tex\">\\(S\\)</span>, one can think of the different parts as possible values of a variable&nbsp;<span class=\"math-tex\">\\(X\\)</span>. Then given an element&nbsp;<span class=\"math-tex\">\\(s\\)</span>&nbsp;of&nbsp;<span class=\"math-tex\">\\(S\\)</span>, the value of&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;in&nbsp;<span class=\"math-tex\">\\(s\\)</span>&nbsp;is the part to which&nbsp;<span class=\"math-tex\">\\(s\\)</span>&nbsp;belongs in&nbsp;<span class=\"math-tex\">\\(X\\)</span>.</p></td></tr></tbody></table>\n\n**Scott:** Then we're going to define time. We'll say that \\\\(X\\\\) is *weakly before* \\\\(Y\\\\) if the history of \\\\(X\\\\) is a subset of the history of \\\\(Y\\\\). And we'll say that \\\\(X\\\\) is *strictly before* \\\\(Y\\\\) if the history of \\\\(X\\\\) is a strict subset of the history of \\\\(Y\\\\).\n\nOne way you might want to think about this is that the history of \\\\(X\\\\) is kind of like a past light cone. One spacetime point being before another spacetime point means that the past light cone of the first point is a subset of the past light cone of the second point. That gives some intuition as to why one might want to think of a temporal relationship as a subset relationship.\n\nWe're also going to define orthogonality. We’ll say that partitions \\\\(X\\\\) and \\\\(Y\\\\) are *orthogonal* if their histories are disjoint.\n\nI think I actually want to pause for questions here, so let's do that.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Daniel Filan:</strong> ah I think I get it. So like if you have the&nbsp;<span class=\"math-tex\">\\(x\\)</span>-<span class=\"math-tex\">\\(y\\)</span>&nbsp;plane, the history of the half-planes&nbsp;<span class=\"math-tex\">\\(\\{x &gt; 0, x \\leq 0\\}\\)</span>&nbsp;is the set&nbsp;<span class=\"math-tex\">\\(\\{x\\}\\)</span>?</p><p>(where in the second set&nbsp;<span class=\"math-tex\">\\(x\\)</span>&nbsp;is standing for partitioning the coordinates by their&nbsp;<span class=\"math-tex\">\\(x\\)</span>&nbsp;value)</p><p><strong>Anurag Kashyap:</strong> does the history have to be unique for any partition&nbsp;<span class=\"math-tex\">\\(X\\)</span>?</p><p><strong>Adam Scherlis:</strong> I think so, Daniel</p><p><strong>Diffractor:</strong> Yeah, the set&nbsp;<span class=\"math-tex\">\\(\\{\\mathbb{R}\\}\\)</span>, where&nbsp;<span class=\"math-tex\">\\(\\mathbb{R}\\)</span>&nbsp;is taken as a partition of&nbsp;<span class=\"math-tex\">\\(\\mathbb{R}^2\\)</span></p><p><strong>Ramana Kumar:</strong> @Anurag yes</p><p><strong>Daniel Filan:</strong> and&nbsp;<span class=\"math-tex\">\\(\\{x &gt; 0, x \\leq 0\\}\\)</span>&nbsp;is orthogonal to&nbsp;<span class=\"math-tex\">\\(\\{y &gt; 0, y \\leq 0\\}\\)</span>?</p><p><strong>Diffractor:</strong> Daniel, yes, that's correct</p><p><strong>Daniel Filan:</strong> aaah</p><p>so orthogonality means \"the things you have to know to determine these two events don't intersect\". So somehow they come from different facts.</p><p><strong>Diffractor:</strong> Yup, that's correct</p><p>Also, for everyone else, I should warn you that there's a whole lot of sets and sets of sets and sets of sets of sets in the posts.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Anonymous 6: </strong>So the size of the history is measured by the number of elements added up from all the little subsets, not the number of factors, right?</p><p><strong>Scott:</strong> I never talk about the size of the history. When I say \"smallest set of factors,\" I mean smallest in the subset relation.</p><p>This is actually maybe not obvious on the surface of it, but it turns out that sets&nbsp;<span class=\"math-tex\">\\(H\\)</span>&nbsp;with the property that knowing all of the values in&nbsp;<span class=\"math-tex\">\\(H\\)</span>&nbsp;is sufficient to determine the value of&nbsp;<span class=\"math-tex\">\\(X\\)</span>, are actually closed under intersection. And so there's going to be a unique smallest set of factors in the subset ordering. So the history is actually going to be a subset of any set of factors that would be sufficient to determine the value of&nbsp;<span class=\"math-tex\">\\(X\\)</span>.</p><p><strong>Anonymous 7: </strong>Is that equivalent to coarsest?</p><p><strong>Scott:</strong> Given a set of factors, we could take their common refinement, and a smaller subset is going to correspond to a coarser common refinement. So in that sense, you can view it as the coarsest.</p><p>There's this choice: Do we want to say it's a set of factors, or do we want to say it is the common refinement of a set of factors? In the past, I’ve thought of it as the common refinement of a set of factors. That's another good way to think about it.</p><p><strong>Anonymous 8: </strong>Scott, are you going to be drawing a specific parallel between the new formalism and some specific causal graph in the Pearlian sense?</p><p><strong>Scott: </strong>I am not going to, in this talk, formally connect things up to Pearl. There <i>is</i> going to be a way to take a Pearlian DAG, and from it compute a finite factored set—and in that sense, my model class is kind of going to be larger, because I'm not going to be able to go the other way around. But I'm not actually going to present that here.</p></td></tr></tbody></table>\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>David Manheim:</strong> The time history seems intuitive, but I’m not following how we can do this with actual values / histories because I don’t quite follow how factorizations work.</p><p><strong>David Manheim: </strong>I only kind of follow the idea of factorizations. What’s the relationship to graphs?</p><p>(Answered; Scott is deferring this question / doesn’t want to propose the mapping because the intuitive one doesn’t work, and the one that works is complex.)</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Anonymous 9: </strong>In a really simple case, if you have two variables and they can each have three values, then you end up with nine elements in your set. Is that right?</p><p><strong>Scott: </strong>If I have two disconnected independent variables—</p><p><strong>Anonymous 9: </strong>You don't know if they're independent.</p><p><strong>Scott: </strong>Oh, I think I'm going to punt this question. I'm going to be talking about temporal inference later.</p><p><strong>Gurkenglas:</strong> Have you tried defining a notion of morphism between finite factored sets and then defining histories in terms of that?</p><p><strong>Scott:</strong> Yeah. I chose to go with a combinatorialist aesthetic in this talk, and also in most of my writing about this, so that things won’t have that many prerequisites. But I also have some thoughts about how to work with finite factored sets in a category-theory aesthetic too.</p><p>Like, in a category-theory aesthetic, instead of talking about partitions of your set, you want to be talking about functions out of your set. Various things like this. And I do have a proposal for what category I'd want to put on factored sets and a way to view history and orthogonality and such as categoric properties. But I'm not going to do that right now.</p></td></tr></tbody></table>\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>David Manheim:</strong> I appreciate not needing category theory for this talk!</p><p><strong>Glenn Willen:</strong> David: Me too, but I suspect it might be easier to understand with category theory, if I understood category theory, which I don't ;-)</p><p><strong>Adam Scherlis:</strong> I feel like the quantum version of this is probably very pretty...</p><p><strong>David Manheim:</strong> Glenn: Ditto - in the counterfactual world where I knew category theory, I would probably prefer it.</p><p><strong>Daniel Filan:</strong> yeah, I like that category theory has pictures to look at</p><p><strong>Mark Xu:</strong> intuitively,&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;is the set of possible worlds,&nbsp;<span class=\"math-tex\">\\(B\\)</span>&nbsp;is grouping&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;into variables, a partition is a property over worlds that you care about, and the history is the smallest set of variables that uniquely determine the property you care about</p><p><strong>Glenn Willen:</strong> Thanks Mark that is incredibly helpful, I'm going to stare at that for a minute.</p><p><strong>Quinn Dougherty:</strong> Mark that's hugely helpful</p><p><strong>Diffractor:</strong> Yup, Mark is exactly correct</p><p><strong>Kyle:</strong> Agreed. Thanks Mark!</p><p><strong>Mark Xu:</strong> “orthogonal” in this intuitive sense means that the two properties you care about can be computed from disjoint variables, so they would be “independent” or “uncorrelated”</p><p><strong>Adam Scherlis:</strong> (yeah I think the quantum version is a bunch of things people talk about in quantum information -- partition ~ observable, factorization ~ tensor factorization, history ~ support of an observable)</p><p><strong>Glenn Willen:</strong> Do you have a link to the definition of \"support\" in this sense? A quick google didn't turn anything up.</p><p><strong>David Manheim:</strong> Glenn: <a href=\"https://en.wikipedia.org/wiki/Support_(mathematics)\">https://en.wikipedia.org/wiki/Support_(mathematics)</a>&nbsp;</p><p><strong>Glenn Willen:</strong> ah okay</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Anonymous 10:</strong> Any factorization has enough information to give you any partition?</p><p><strong>Scott:</strong> I think what you're asking is: In the definition of factorization, we needed for there to be a unique element for each way of assigning values to all the factors. So you can always make&nbsp;<span class=\"math-tex\">\\(H\\)</span>&nbsp;large enough that specifying the values of&nbsp;<span class=\"math-tex\">\\(H\\)</span>&nbsp;is sufficient to specify the value of&nbsp;<span class=\"math-tex\">\\(X\\)</span>. For example, if you take&nbsp;<span class=\"math-tex\">\\(H=B\\)</span>, that will be enough because of our definition of factorization.</p><p>Also, the definition of factorization is important for the fact that history is well-defined because of this \"smallest set of factors.\" The fact that satisfying this property of being sufficient to compute&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;is closed under intersection, comes from the fact that we're working with a set of factors in the form of factorization.</p><p><strong>Anonymous 10:</strong> So is the history an equivalence class, kind of? Or is it one particular smallest set?</p><p><strong>Scott:</strong> The history is the smallest set of factors. By which I mean: for any set of factors that has the property that if I told you the value of all of the factors in that set, that's sufficient to determine&nbsp;<span class=\"math-tex\">\\(X\\)</span>—for any set of factors that satisfy that property, that set of factors must be a superset of the history. It must at least contain the things in the history.</p><p><strong>Anonymous 10: </strong>There's exactly one history?</p><p><strong>Scott:</strong> There's exactly one history.</p><p>I needed finiteness for the fact that I can do all this. It turns out that this definition would not have been well-defined if I didn't say \"finite,\" or at least \"finite-dimensional,\" because we only get that things are closed under finite intersection. So this is part of where we need finite, although there's some hope of being able to generalize past finite.</p><p><strong>Anonymous 11:</strong> For all the factors that aren't in the history, is the common refinement of that factor and&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;the same as that factor?</p><p><strong>Scott:</strong> If you take all the factors outside of the history, and you common-refine them all together, you'll get a new partition. It's not going to be the case that if you take the common refinement of this partition with&nbsp;<span class=\"math-tex\">\\(X\\)</span>, you get back itself. It <i>is</i> going to be the case that this partition is <i>orthogonal</i> to&nbsp;<span class=\"math-tex\">\\(X\\)</span>.</p><p>So if you take the complement of the history of&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;and you take the common refinement of all of those things, you'll get this new partition that is orthogonal to&nbsp;<span class=\"math-tex\">\\(X\\)</span>, and it's in some sense the finest partition that is orthogonal to&nbsp;<span class=\"math-tex\">\\(X\\)</span>.</p><p><strong>Anonymous θ: </strong>I’ve been thinking over in my head what happens if you've got a six-element set factored into two and three, but I'm kind of getting the sense that it might be fairly trivial with something like that, and in order to really get an idea of what's going on with history, you'd need to go into something with more than one reasonable factorization, like 12 could be—</p><p><strong>Scott:</strong> We might want to have a set that's large so that we can say some nice nontrivial stuff, but at least here, we're working with a single fixed factorization. Having multiple factorizations is useful because you need multiple factorizations in order to have a big rich structure, but we're only going to be working with one of them.</p><p><strong>Anonymous θ: </strong>Sure.</p><p><strong>Scott: </strong>Okay. I think I'm going to move on to an example now.</p></td></tr></tbody></table>\n\nGame of Life\n------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets11.jpg)\n\n**Scott:** Let's let \\\\(S\\\\) be the set of all Game of Life computations starting from a board of some size.\n\nI don't want to have to deal with weird stuff about the boundary, and I also want it to be a finite factored set. The way that I'm dealing with this is I'm looking at this pyramid of only the cells that are computable from the initial board state.\n\nSo you can imagine starting with your \\\\((2n + 1) \\times (2n + 1)\\\\) board, and we're just pushing the computation forward in time. (In the picture, going up is forward in time.) And as we progress through the time steps in Game of Life, our board is getting smaller, because the boundary is shrinking in to say, \"I don't know what this is anymore, because it's dependent on stuff outside the boundary.\" So we have this set of all pyramids of Game of Life computations.\n\nBecause Game of Life is deterministic, we actually have a bijection between the set of all Game of Life computations and the set of all initial board states. And we have an obvious factorization of the set of all initial board states, which is that we have one factor \\\\(b \\in B\\\\) for each cell that is the partition that separates out \"Is that cell alive or dead at time \\\\(0\\\\)?\".\n\nWe can also talk about other partitions. There are a whole bunch of partitions on this huge structure.\n\nI'm going to draw attention to partitions that take a cell and take a time step, and ask the question \"Is this cell alive or dead at this given time step?\", which is going to separate out your space into two sets.\n\nIn this picture on the right, I have three (cell, time) pairs—the red cell, the blue cell, and the green cell—which correspond to partitions. And the grid on the bottom is the initial state, time \\\\(0\\\\).\n\nThe multi-cell squares in the initial board state are the history of the red cell, the blue cell, and the green cell. So the light red squares on the bottom are saying, \"Which cells at time \\\\(0\\\\) do you need to know the value of in order to be able to compute the value of the red cell at time \\\\(1\\\\)?\".\n\nThis, again, pushes the picture that histories are like past light cones. We can see that the history of the red partition is a subset of the history of the blue partition. And so the partition corresponding to “Is that red cell alive or dead?” is before the partition corresponding to “Is that blue cell alive or dead?”.\n\nThe red partition is orthogonal to the green partition. And the blue partition is *not* orthogonal to the green partition, because their histories intersect.\n\nThe blue cell and the green cell are *almost* orthogonal, in that if I conditioned on the values of these two cyan squares at the bottom, then they would become orthogonal. So that's what we're going to talk about next—conditional orthogonality, or \"When are things orthogonal conditional on certain assumptions?\".\n\nConditional Orthogonality\n-------------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets12.jpg)\n\n**Scott:** To define conditional orthogonality, we're first going to define conditional history. The conditional history of \\\\(X\\\\) given \\\\(E\\\\) is the smallest set of factors (again, smallest in the subset ordering) satisfying these two conditions.\n\nThe first condition is basically the same as in history, except we make this assumption that we're in \\\\(E\\\\). So the first condition is saying that if I want to know the value of \\\\(X\\\\), and I'm working under the assumption that my point is in \\\\(E\\\\), it suffices for me to know the value of all of the factors in \\\\(H\\\\).\n\nThe second condition is going to be a little hard to motivate. It's not going to mention \\\\(X\\\\); it's just going to be about \\\\(E\\\\) and \\\\(H\\\\). The second condition says that if I take an element of \\\\(S\\\\) and I want to determine whether or not it's in \\\\(E\\\\), this process of determining whether or not it's in \\\\(E\\\\) can be parallelized:\n\n*   I can look at the factors in \\\\(H\\\\) and ask, \"Is the assignment of values to the factors in \\\\(H\\\\) consistent with my point being in \\\\(E\\\\)?\"\n*   And then I can separately look at the factors in \\\\(B\\setminus H\\\\) and ask, \"Is the value of the factors in \\\\(B\\setminus H\\\\) consistent with my point being in \\\\(E\\\\)?\"\n\nAnd then if both of these questions return \"true,\" then my point has to be in \\\\(E\\\\).\n\nI'll say that without the second condition, conditional history would not even be well-defined. I do not think it should be obvious that it's well-defined anyway! In order for this history to be well-defined, I need to be able to say that there's a smallest set of factors (in the subset ordering) satisfying these conditions.\n\nOne way to see why we need the second condition is: You could imagine that we have a finite factored set that consists of two bits. There's my bit and there's your bit, and they're each either \\\\(0\\\\) or \\\\(1\\\\). And my factorization is the obvious factorization, with one factor for “What is my bit?” and one factor for “What is your bit?”. So then the history of “What is my bit?”, the history of the partition that separates out the different values that my bit can be, is just going to be the one factor that is “What is my bit?”.\n\nBut then imagine conditioning on the assumption that our bits are the same. In that case, knowing my bit is sufficient to know my bit, and knowing your bit is sufficient to know my bit, but the intersection of knowing my bit and knowing your bit is empty.\n\nThis is why we need the second condition. The second condition is saying that the act of conditioning on the fact that the two bits were the same, entangled my bit with your bit; so you're not allowed to then take one and not the other. This makes conditional history well-defined.\n\nFrom conditional history we're going to be able to define *conditional orthogonality*. We say that \\\\(X\\\\) and \\\\(Y\\\\) are *orthogonal given* the subset \\\\(E\\\\) if their conditional histories are disjoint. And we'll say that \\\\(X\\\\) and \\\\(Y\\\\) are orthogonal given a *partition*, \\\\(Z\\\\), if \\\\(X\\\\) and \\\\(Y\\\\) are orthogonal given each individual part in \\\\(Z\\\\).\n\nI think it's pretty hard to motivate this definition. If I go over to categorical language, I think I have a picture that makes it look a little bit nicer than this, or a little bit more motivated. But I'm mostly just going to appeal to this definition's consequences to try to convince you that it's a good definition, as opposed to trying to push that it's really natural.\n\nCompositional Semigraphoid Axioms\n---------------------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets13.jpg)\n\n**Scott:** Here's some consequences. This is just a bunch of properties showing that conditional orthogonality is pretty well-behaved.\n\nThere are five graphoid axioms, of which the first four are the semigraphoid axioms. And then there's the axiom of composition, which is the fifth item on the above list and is the reason we have a \"compositional\" semigraphoid.\n\nThe field that works with Pearlian inference sometimes talks about these axioms. When they talk about it, they talk about it with sets of variables; we're instead working with partitions. So where they took a union of a set of variables, we're instead going to work with a common refinement. But that's all an easy translation.\n\nAll of these properties have a \\\\(Z\\\\) in all of the conditions. You can just ignore the \\\\(Z\\\\) and think of it without the \\\\(Z\\\\) if you want to.\n\nThis first property says that \\\\(X\\\\) is orthogonal to \\\\(Y\\\\) if and only if \\\\(Y\\\\) is orthogonal to \\\\(X\\\\), which hopefully should make sense.\n\nThe second and the fifth are converses that together say that \\\\(X\\\\) is orthogonal to the common refinement of \\\\(Y\\\\) and \\\\(W\\\\), if and only if \\\\(X\\\\) is separately orthogonal to \\\\(Y\\\\) and to \\\\(W\\\\). So they're collectively saying that being orthogonal to a common refinement is equivalent to being orthogonal to each of the pieces.\n\nThat's especially nice, because it's saying that if you want to know whether something is orthogonal, it suffices to break it up however you want and check whether it's orthogonal to each individual piece.\n\n*d*-separation in DAGs also satisfies all these properties.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Jasper Chapman-Black: </strong>I wish the theorems/axioms had Game of Life examples</p><p><strong>Daniel Filan:</strong> oh so if&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(Y\\)</span>&nbsp;are variables, then&nbsp;<span class=\"math-tex\">\\(X \\vee Y\\)</span>&nbsp;is like the variable that's the tuple of&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(Y\\)</span></p><p>which helps in reading this</p><p><strong>Ramana Kumar:</strong> @Daniel yes I think so</p><p><strong>Daniel Filan:</strong> which makes me think it should be written&nbsp;<span class=\"math-tex\">\\(X \\land Y\\)</span>...</p><p><strong>Diffractor:&nbsp;</strong><span class=\"math-tex\">\\(\\vee\\)</span>&nbsp;is always used to denote supremum in posets</p><p><strong>Ramana Kumar:</strong> it's using&nbsp;<span class=\"math-tex\">\\(\\vee\\)</span>&nbsp;because it's a join in the poset of partitions</p><p><strong>Anonymous ε: </strong>I don't understand why people use&nbsp;<span class=\"math-tex\">\\(&lt;\\)</span>&nbsp;in posets/lattices to mean \"is coarser / is less informative than\" rather than the opposite.</p><p>As it is,&nbsp;<span class=\"math-tex\">\\(&lt;\\)</span>&nbsp;points in the opposite direction from the subset symbol, in cases where those both make sense.</p><p>Also&nbsp;<span class=\"math-tex\">\\(&lt;\\)</span>&nbsp;points in the opposite direction from the material implication symbol</p><p><strong>Ramana Kumar: </strong>I think Scott will use the opposite direction (for partition refinement) in his write up.</p><p><strong>Anonymous ε:</strong> Oh, cool!</p><p><strong>Anurag Kashyap: </strong>@[Anonymous ε] I think the notation may have been imported from topology where&nbsp;<span class=\"math-tex\">\\(A⊆B\\)</span>&nbsp;means '<span class=\"math-tex\">\\(B\\)</span>&nbsp;is finer than&nbsp;<span class=\"math-tex\">\\(A\\)</span>' and&nbsp;<span class=\"math-tex\">\\(⊆\\)</span>&nbsp;is a partial ordering again.</p><p><strong>Anonymous ε: </strong>It occurs to me that if&nbsp;<span class=\"math-tex\">\\(t_0\\)</span>&nbsp;happens before&nbsp;<span class=\"math-tex\">\\(t_1\\)</span>, we usually say&nbsp;<span class=\"math-tex\">\\(t_0 &lt; t_1\\)</span>&nbsp;and also&nbsp;<span class=\"math-tex\">\\(t_0\\)</span>&nbsp;is coarser than&nbsp;<span class=\"math-tex\">\\(t_1\\)</span>. So it actually makes sense if we want to talk about time!</p><p><strong>Scott: </strong>I decided to go against the more standard partition refinement&nbsp;direction on purpose, and intend to do that consistently. My main reason is that I want my partitions to be&nbsp;like Pearl's sets of variables, so I want finer things to be larger, because they have more variables, (and more information)</p></td></tr></tbody></table>\n\nThe Fundamental Theorem\n-----------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets14.jpg)\n\n**Scott:** Those were just some nice properties. The main thing I want to push is that conditional orthogonality is really a combinatorial fragment of conditional independence.\n\nWe're going to show that \\\\(X\\\\) is orthogonal to \\\\(Y\\\\) given \\\\(Z\\\\) if and only if \\\\(X\\\\) is independent of \\\\(Y\\\\) given \\\\(Z\\\\) in all probability distributions that you can put on that factored set. (If you divide both sides by the probability of \\\\(P(z)^2\\\\) in the equation, it's more obvious that this is asserting conditional independence.)\n\nAnd so I need to explain what it means to put a probability distribution on a factored set.\n\nA probability distribution on a finite factored set \\\\(F\\\\) is just a probability distribution on \\\\(S\\\\) that factors. What I mean by “factors” is that you can view it as coming from a probability distribution on each of the individual factors in your factorization. I take a probability distribution on \\\\(b\\\\) for each \\\\(b \\in B\\\\), and then I multiply together all those probability distributions and take the product of those measures to get a probability distribution on \\\\(S\\\\).\n\nWe'll call a probability distribution on \\\\(S\\\\) a probability distribution on \\\\(F\\\\) if it can be constructed in this way; or equivalently, if the probability of any given element is equal to the product of the probabilities of each of the parts in a factor that contain that element.\n\nAnd then we get the fundamental theorem of finite factored sets. We can also probably extend this theorem to talk about all probability distributions in general position, or something like that, rather than talking about all probability distributions; but I'm not going to worry about that right now.\n\nThis theorem is going to allow us to infer orthogonality data from probabilistic data. If we're given a bunch of probabilistic data—for example, a big probability distribution on our set—we can use the fundamental theorem to tell us when things should be orthogonal.\n\nNext, we're going to show how to infer temporal data from orthogonality data. And we're going to do that working entirely combinatorially; so this slide is actually the only place I'm going to mention probability. Part of the thing that I like about this is that I kind of push all the probability away and think about these questions just using combinatorial properties.\n\nI think I'm going to take another question break.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Alex Ray:</strong> Does this have a computable partition function?</p><p>(It seems like it should be since this is all combinatorics so far)</p><p><strong>Diffractor:</strong> What do you mean by \"this\" and \"computable partition function\"?</p><p><strong>Alex Ray:</strong> Nevermind these are just all probabilities</p><p><strong>Daniel Filan:</strong> so, if I have a Bayes Net&nbsp;<span class=\"math-tex\">\\(A \\rightarrow B \\rightarrow C\\)</span>, and a probability distribution on it, am I right that that wouldn't be a probability distribution on the related finite factored set?</p><p><strong>Ramana Kumar:</strong> @Daniel, what is the related finite factored set you have in mind?</p><p><strong>David Manheim:</strong> This is what I was trying to ask last time...</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Anonymous 12: </strong>I would like to second Daniel's question, which I also do not understand.</p><p><strong>Daniel Filan: </strong>Can I say it?</p><p><strong>Anonymous 12: </strong>Yes.</p><p><strong>Daniel Filan: </strong>Alright, great. Let's say I have a Bayes net with three variables—let's call them&nbsp;<span class=\"math-tex\">\\(R\\)</span>,&nbsp;<span class=\"math-tex\">\\(S\\)</span>, and&nbsp;<span class=\"math-tex\">\\(T\\)</span>&nbsp;for the nodes. There's an arrow from&nbsp;<span class=\"math-tex\">\\(R\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(S\\)</span>. There's an arrow from&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(T\\)</span>. And in this Bayes net I've got some probability distribution on&nbsp;<span class=\"math-tex\">\\(R\\)</span>, then a distribution on&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;conditioned on&nbsp;<span class=\"math-tex\">\\(R\\)</span>, and then&nbsp;<span class=\"math-tex\">\\(T\\)</span>&nbsp;conditioned on&nbsp;<span class=\"math-tex\">\\(S\\)</span>, and that's how I get my whole distribution.</p><p>And let's say they're all binary variables. In the finite factored sets view, this is a set of eight elements, and my&nbsp;<span class=\"math-tex\">\\(B\\)</span>&nbsp;is a factorization into three different—</p><p><strong>Scott: </strong>I'm going to stop you there. I said that I have a way of converting a Bayes net into a finite factored set. It's not the obvious way.</p><p><strong>Daniel Filan: </strong>But if I foolishly did it the wrong way, which is the way I'm imagining I would do it, then this wouldn't be a probability distribution on that finite factored set. Am I right about that?</p><p><strong>Scott: </strong>Yeah.</p><p><strong>Daniel Filan: </strong>Doesn't that seem weird and crazy?</p><p><strong>Scott: </strong>I think the thing that's going on there is that I feel like I'm living in a world where the concept of factorization doesn't exist yet. And so I used the word \"factorization\" twice in this talk in different ways: once when I said Pearl is assuming factorization data, and once when I said we're going to infer a finite factored set. And \"factorization\" was the right word to use in both of those places, but you should be thinking of it as a different kind of factorization.</p><p>Like, the finite factored set structure is not equivalent to just “What are the variables that Pearl should have used?” or something like that.</p><p>So in this example, I can say that the finite factored structure I'll want is a&nbsp;<span class=\"math-tex\">\\(2 \\times 4 \\times 4\\)</span>&nbsp;factored set. I have one factor for the value of&nbsp;<span class=\"math-tex\">\\(R\\)</span>; I have one factor for a function from the value of&nbsp;<span class=\"math-tex\">\\(R\\)</span>&nbsp;to the value of&nbsp;<span class=\"math-tex\">\\(S\\)</span>; and I have one factor for a function from the value of&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;to the value of&nbsp;<span class=\"math-tex\">\\(T\\)</span>.</p><p>My&nbsp;<span class=\"math-tex\">\\(2 \\times 4 \\times 4\\)</span>&nbsp;factored set is going to give me 32 different possibilities. And then I'm going to project down and hide some of that information from you to get the eight possibilities you're talking about.</p><p>The point of the factored set is that the factors are independent, and so you shouldn't put a factor on&nbsp;<span class=\"math-tex\">\\(S\\)</span>&nbsp;and another factor on&nbsp;<span class=\"math-tex\">\\(R\\)</span>, because they're not independent. You can put a factor on&nbsp;<span class=\"math-tex\">\\(R\\)</span>&nbsp;and a factor on a function from&nbsp;<span class=\"math-tex\">\\(R\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(S\\)</span>.</p><p><strong>Daniel Filan: </strong>Oh. So the factors are supposed to be like the things that were true at the origin of time.</p><p><strong>Scott: </strong>Yeah.</p><p><strong>Daniel Filan: </strong>Oh, okay. Cool, thank you.</p></td></tr></tbody></table>\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Ramana Kumar:</strong> I think there are FFSs that don't have a corresponding Bayes Net. Also, the factors typically don't correspond to the random variables in the Bayes Net when there is a corresponding Bayes Net. (This is kind of what Scott is saying right now.)</p><p><strong>John Wentworth:</strong> The obvious related FFS would be an SEM rather than a Bayes net - i.e. each node has an independent random input, and everything is deterministically computed from those. The random inputs are the variables/partitions.</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Gurkenglas: </strong>The probability distributions are the reasons that the sets have to be finite, yes?</p><p><strong>Scott: </strong>No, the probability distribution is not the issue. I'll say why we want things to be finite.</p><p>Imagine I took an infinite-dimensional factored set that is all infinite bit strings with the obvious factorization, which is that I'll have one factor for each bit in the bit string.</p><p>Consider the partition that is asking whether you have the all-<span class=\"math-tex\">\\(0\\)</span>&nbsp;string, which has two parts. One of them contains the singleton all-<span class=\"math-tex\">\\(0\\)</span>&nbsp;string, and the other contains everything else. Call this&nbsp;<span class=\"math-tex\">\\(X\\)</span>.</p><p>Then let's call&nbsp;<span class=\"math-tex\">\\(Y\\)</span>&nbsp;the partition that asks \"Are you the all-<span class=\"math-tex\">\\(1\\)</span>&nbsp;string?\".</p><p>For all probability distributions that you can put on this infinite factored set, at least one of&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(Y\\)</span>&nbsp;is going to need to have probability zero. In order to make it so that you have a non-zero probability of being all-<span class=\"math-tex\">\\(0\\)</span>, you need each individual bit to be converging to very, very likely&nbsp;<span class=\"math-tex\">\\(0\\)</span>. And then the all-<span class=\"math-tex\">\\(1\\)</span>&nbsp;string has probability zero.</p><p>So one of these two partitions is going to be probability zero in any given distribution on your factored set. Which means that these two partitions are going to be independent of each other in all distributions. And it really feels like these two partitions should not be orthogonal.</p></td></tr></tbody></table>\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><p><strong>Ramana Kumar:</strong> (dimension = cardinality of&nbsp;<span class=\"math-tex\">\\(B\\)</span>)</p><p><strong>Anonymous ι:</strong> Ah. Like the infinite additivity problem?</p><p><strong>Alex Mennen:</strong> you could restrict attention to partitions into clopen sets</p><p>(I mean finite Boolean combinations of factors)</p><p><strong>Diffractor: </strong>I proved that if you consider just hausdorff quotients on compact metric spaces, you can&nbsp; take common refinement of any collection of quotients and it'll still be just as nice topologically</p><p>I was thinking kinda along these lines independently, but more from a topology angle, and I'm pretty dang sure that's the way to go</p></td></tr></tbody></table>\n\n<table><tbody><tr><td><p><strong>Gurkenglas</strong>: I was mentioning that because we would have to generalize your notion of probability distribution. When the set gets too large, then you also have to use something larger than real numbers in order to tell between all these different possible sets.</p><p><strong>Scott:</strong> It's not clear. I am personally very excited about extending factored sets to the infinite case, but I think that it runs into problems that are not just probability.</p><p>For example, you have to do some weird stuff with the definition of history in order to make it all work out. You don't actually want to use history, because it's not going to be well-defined. Just getting the compositional semigraphoid axioms on an arbitrary-dimension factored set is going to be complicated in itself.</p><p>But I'm still optimistic about it. I'm still excited about infinite factored sets, but I think it's going to take a lot more work, and I think that more work is not just going to be because of probability. But I think some of it will be because of probability.</p><p>So you asked the question: Is it because of probability that we have to do finite stuff? And I said no, but my answer is it's not <i>only</i> the probability. There's more than just that that causes problems in the infinite case.</p><p><strong>Anonymous 13:</strong> Would it make sense in this infinite-bitstring case to consider a sequence of finite factored sets, where each one is a superset of the previous factorization and you just add one more bit?</p><p><strong>Scott: </strong>My intuition is that in the infinite-bitstring case the thing that makes sense to do is to instead talk about finite-dimensional flattenings. So you start out with an infinite-dimensional factored set, and then you can merge a bunch of the factors so that you only have finitely many different equivalence classes of your factors, and this is like a flattening of your factored set. And then you work over that space.</p><p>But that's just what I'd do in order to save the other parts. I think that the fundamental theorem is just not going to be saved.</p><p>I think that factored sets are still going to be interesting in the infinite case, and we're going to want to do stuff with them. But independence is just not going to be enough to infer orthogonality in the infinite case. Although the other direction will hold; you'll still have orthogonality imply independence in all distributions.</p><p>My intuition is that I'm excited for arbitrary factored sets, but I suspect that even the fundamental theorem does not work. My guess is that the fundamental theorem can be extended to the finite-dimensional case, but not the infinite-dimensional case. But I'm mostly thinking about and working on the finite case.</p></td></tr></tbody></table>\n\nTemporal Inference\n------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets15.jpg)\n\n**Scott:** We're going to start with a set \\\\(\\Omega\\\\), which is going to be our sample space—although I don't want to carry around any probability intuitions. I want to think about everything combinatorially, and I want the only place probability shows up to be the previous slide. \\\\(\\Omega\\\\) is a finite set, and it’s something like the space of all possible observations we can make.\n\nOne thing you might think that we'd want to do would be to infer a factorization of \\\\(\\Omega\\\\), but that's not what we're going to do. Instead, we're going to find a factored set *model* of \\\\(\\Omega\\\\). We're doing this because I'm thinking of \\\\(\\Omega\\\\) as the observably distinct situations that we can be in, and I want to allow for inferring latent structure.\n\nThere was a question about how to make a factored set out of a three-node Pearlian DAG. There were eight possibilities, but I said, \"We don't want a factorization on an 8-element set; we want a factorization on a 32-element set, and then we want to send that over to an 8-element set with a function.\" In that example, the function \\\\(R \\rightarrow S\\\\) was latent structure.\n\nIn the model \\\\((F,f)\\\\):\n\n*   Not assuming \\\\(f\\\\) is injective is what allows for latent structure.\n*   Not assuming \\\\(f\\\\) is surjective means that it's okay if some of the elements of \\\\(\\Omega\\\\) don't actually get hit. This is useful if we just want to start with some variables and take their product and have that be my sample space. I don't want to assume that every combination of variables is necessarily possible; and that's what not assuming surjectivity is going to do.\n\nGiven a partition of \\\\(\\Omega\\\\), we can send that partition backwards across the function \\\\(f\\\\) to give a partition of \\\\(S\\\\). Which is just saying that if you have all of your parts, and then you take the preimage as functions of the parts, this will give you a partition of your original space.\n\nWe also have an orthogonality database on \\\\(\\Omega\\\\), which is a collection of rules about orthogonality in \\\\(\\Omega\\\\). One place you could think of this database as coming from is a probability distribution. You sample a whole bunch of times, and you get a probability distribution, and whenever you observe that two things are independent you'll put in a rule saying they're orthogonal. And then whenever you observe that things are *not* independent, you put in a rule that says those things should *not* be orthogonal. Which makes sense because of the fundamental theorem.\n\nBut you could also imagine that orthogonality database coming from anywhere else. This is entirely a combinatorial notion.\n\nA \"rule\" means putting a triple of partitions either in the set \\\\(O\\\\) (for \"orthogonal\") or the set \\\\(N\\\\) (for \"not orthogonal\").\n\nSo we have this collection of rules, and we want to infer a factored set model. We're not actually going to be able to infer a full factored set model, because whenever you have one model, you can always just add in more factors and add in more stuff, and then just delete all that data. Instead, we're going to be able to infer properties of factored set models—properties that all factored set models that satisfy our database are going to have to satisfy.\n\nOne of these properties is: we'll say that \\\\(X\\\\) is before \\\\(Y\\\\) according to the database \\\\(D\\\\) if \\\\(f^{-1}\\\\) of \\\\(X\\\\) is before \\\\(f^{-1}\\\\) of \\\\(Y\\\\) for all models that satisfy our database.\n\nThis is what we're going to mean by temporal inference. Now we have a mathematical specification of (though not a computational procedure for), \"Given a bunch of orthogonality rules and non-orthogonality rules, when are those rules sufficient to infer a temporal relationship?\"\n\nI've specified this combinatorial notion of temporal inference. But I haven't shown this notion isn't vacuous. So there are these questions: “Are we ever going to be able to infer time with this?\" \"How is our approach to temporal inference going to be able to compare to Pearlian temporal inference?”\n\nPearlian temporal inference is actually very strong. If you have a big system with a bunch of variables, you're very likely going to be able to infer a lot of temporal relations from it. Can we keep up with that?\n\nTo answer that question, we're going to look at an example.\n\nTwo Binary Variables (Pearl)\n----------------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets16.jpg)\n\n**Scott:** If \\\\(X\\\\) and \\\\(Y\\\\) *are* independent, then we're not going to have a path from \\\\(X\\\\) to \\\\(Y\\\\) or vice versa. If \\\\(X\\\\) and \\\\(Y\\\\) are *not* independent, then maybe there's a path from \\\\(X\\\\) to \\\\(Y\\\\), maybe there's a path from \\\\(Y\\\\) to \\\\(X\\\\), maybe there's some third node that has a path to both of them.\n\nIn either case, we're not going to be able to infer any positive temporal relationships.\n\nTo me, this feels like it's where \"correlation does not imply causation\" comes from. If you just have two variables, you can't really infer causation; Pearl needs a lot more variables in order to have a richer combinatorial structure that will allow his approach to infer time.\n\nBut the natural next question is: \"Is \\\\(X\\\\) independent of \\\\(X  \\text{XOR}  Y\\\\)?\"\n\nIn the Pearlian ontology, we have \\\\(X\\\\) and \\\\(Y\\\\), and those are our variables, and we ask whether they're independent.\n\nIn the factored sets ontology, we take our \\\\(X\\\\) and our \\\\(Y\\\\), and the first thing we do is we take a product of \\\\(X\\\\) and \\\\(Y\\\\), and now we have four different possibilities. And then we forget all our labels, and we consider all possible variables on these four possibilities. And one of those variables is \\\\(X  \\text{XOR}  Y\\\\)—are \\\\(X\\\\) and \\\\(Y\\\\) the same?\n\nAnd if \\\\(X\\\\) *is* independent of \\\\(X  \\text{XOR}  Y\\\\), then the finite factored set paradigm is actually going to be able to conclude that \\\\(X\\\\) is before \\\\(Y\\\\).\n\nSo not only is the paradigm non-vacuous, and not only can it keep up with Pearl and infer things that Pearl can't, but also we can use it to infer time even from two binary variables, which is kind of the simplest possible thing that you could hope to be able to do.\n\nPearl is also going to be able to infer some things that we can't, but as a reminder, Pearl is using extra assumptions. Relative to our ontology, there's extra factorization data that Pearl is using; and I'm not exactly sure where one would get that extra data.\n\nTwo Binary Variables (Factored Sets)\n------------------------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets17.jpg)\n\n**Scott:** Let \\\\(\\Omega\\\\) be the set of all binary strings of length two. We're going to define an orthogonality database which is only going to have two rules.\n\n*   The first rule is that \\\\(X\\\\) (\"What is the first bit?\") is orthogonal to \\\\(Z\\\\) (\"Do the bits match?\"). The \\\\(\\{\\Omega\\}\\\\) here is just saying we're not making any conditions. (You don't have to worry about the complicated conditional orthogonality definitions for this proof; everything is just going to be normal orthogonality, which will make it easier to think through.)\n*   Second, \\\\(Z\\\\) is not orthogonal to itself. This just means that \\\\(Z\\\\) is nondeterministic; both of the parts in \\\\(Z\\\\) are supported by our function \\\\(f\\\\), both parts are possible.\n\nIf we got this orthogonality database from a probability distribution in general position, we would have a lot more rules in our database. We would have a rule for basically every triple of partitions. But inferring time is going to be monotonic with respect to adding partitions: adding more rules just lets you infer more stuff. So we can just work with the smallest database that we're going to need in order to be able to show that \\\\(X\\\\) (\"What is the first bit?\") is before \\\\(Y\\\\) (\"What is the second bit?\").\n\nYou should probably just assume that everything is not orthogonal to itself; but \"\\\\(Z\\\\) is not orthogonal to itself\" is the only statement of that form we're going to need.\n\nSo, theorem: \\\\(X\\\\) is before \\\\(Y\\\\). Which, recall, means that \\\\(f^{-1}(X)\\\\) is before \\\\(f^{-1}(Y)\\\\) for all factored set models.\n\n*Proof.* First, we’re going to prove that \\\\(X\\\\) is weakly before \\\\(Y\\\\):\n\n*   Let \\\\((F, f)\\\\) be a model that satisfies our database, and write \\\\(H_X\\\\) for the history of \\\\(f^{-1}(X)\\\\), etc. Since \\\\(X\\\\) is orthogonal to \\\\(Z\\\\), we're going to have that \\\\(H_X\\\\) and \\\\(H_Z\\\\) are disjoint.\n\nIf the \\\\(f^{-1}\\\\) makes it a little bit harder to see what these things are saying, you can kind of ignore that part. If you just think of \\\\(X\\\\), \\\\(Y\\\\), and \\\\(Z\\\\) as partitions of \\\\(S\\\\), where \\\\(F=(S,B)\\\\), it might be a little bit easier to understand what's going on here.\n\n*   Because \\\\((X,Z,\\{\\Omega\\})\\\\) is in \\\\(O\\\\), \\\\(f^{-1}(X)\\\\) is orthogonal to \\\\(f^{-1}(Z)\\\\), which means the history of \\\\(f^{-1}(X)\\\\) is disjoint from the history of \\\\(f^{-1}(Z)\\\\): \\\\(H_X \\cap H_Z = \\{\\}\\\\).\n*   Because \\\\(Z\\\\) is not orthogonal to itself, the history of \\\\(f^{-1}(Z)\\\\) is not disjoint with itself, i.e., it's not empty.\n*   \\\\(X\\\\) is a coarsening of the common refinement of \\\\(Y\\\\) and \\\\(Z\\\\): \\\\(X\\leq_\\Omega Y\\vee_\\Omega Z\\\\). And since we can compute the value of \\\\(X\\\\) from the values of \\\\(Y\\\\) and \\\\(Z\\\\), we can also compute the value of \\\\(f^{-1}(X)\\\\) from the values of \\\\(f^{-1}(Y)\\\\) and \\\\(f^{-1}(Z)\\\\). Which means that the history of \\\\(f^{-1}(X)\\\\), \\\\(H_X\\\\), has to be a subset of \\\\(H_Y \\cup H_Z\\\\).\n\nTo see why this is, recall that a history is the smallest set of factors I need to be able to compute a partition's value. If I have a set of factors that is sufficient to compute the value of \\\\(Y\\\\) and sufficient to compute the value of \\\\(Z\\\\), that's going to be sufficient to compute the value of \\\\(X\\\\), because \\\\(X\\\\) is a deterministic function of \\\\(Y \\times Z\\\\).\n\n*   We also get this subset relation for any other combination of \\\\(X\\\\), \\\\(Y\\\\), and \\\\(Z\\\\). So \\\\(H_Z \\subseteq H_X \\cup H_Y\\\\), and \\\\(H_Y \\subseteq H_X \\cup H_Z\\\\).\n*   Since \\\\(H_X \\subseteq H_Y \\cup H_Z\\\\), and \\\\(H_X\\\\) is disjoint from \\\\(H_Z\\\\), we get that \\\\(H_X \\subseteq H_Y\\\\)—which is to say, \\\\(f^{-1}(X)\\\\) is (weakly) before \\\\(f^{-1}(Y)\\\\) in all models.\n\nLet's extend this further, and show that \\\\(f^{-1}(X)\\\\) is *strictly* before \\\\(f^{-1}(Y)\\\\).\n\n*   Assume for the purpose of contradiction that \\\\(H_X = H_Y\\\\). Since \\\\(H_Z \\subseteq H_X \\cup H_Y\\\\), we'll then just have that \\\\(H_Z\\\\) is a subset of \\\\(H_X\\\\), because \\\\(H_X \\cup H_Y\\\\) will be \\\\(H_X\\\\).\n*   But \\\\(H_Z\\\\) is disjoint from \\\\(H_X\\\\), which means that \\\\(H_Z\\\\) is just going to have to be empty. But we assumed that \\\\(H_Z\\\\) is nonempty; so this is a contradiction.\n\nThus \\\\(H_X\\\\) is not equal to \\\\(H_Y\\\\). So \\\\(H_X\\\\) is a *strict* subset of \\\\(H_Y\\\\), and \\\\(X\\\\) is before \\\\(Y\\\\). \\\\(\\square\\\\)\n\nProofs in this space largely look like this. We look at a bunch of histories of our variables, we look at various different Boolean combinations of our histories, and we keep track of when they are empty and non-empty; and we use this to eventually infer that some of our histories are subsets of others.\n\nI haven't spent much time on temporal inference, and I haven't really looked into the computational problem of inferring time from orthogonality databases yet. I basically just have two proofs of concept: this one, and another proof of concept that uses something that looks a little bit more like screening off, and uses conditional orthogonality to infer time in a more complicated situation.\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td><strong>Vanessa Kosoy:</strong> Can it happen that some set of rules is consistent with a probability distribution on&nbsp;<span class=\"math-tex\">\\(\\Omega\\)</span>&nbsp;but has no models?</td></tr></tbody></table>\n\nApplications / Future Work / Speculation\n----------------------------------------\n\n![](https://intelligence.org/wp-content/uploads/2021/05/FiniteFactoredSets18.jpg)\n\n**Scott:** I'm going to break some directions I want to go in into three categories. The first category is inference, which has a lot of computational questions. The second category is infinity, which has more mathematical questions. And the third category is [embedded agency](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh), which has more philosophical questions.\n\n### Inference\n\n**Decidability of temporal inference —** I haven't been able to even prove whether the temporal inference method I just described is decidable, because there are infinitely many factored set models that you might have to consider.\n\nI suspect it *is* decidable. Given the size of \\\\(\\Omega\\\\), there is an upper bound on how complicated of an \\\\(S\\\\) you have to consider; and I conjecture that this upper bound is small, and probably computable by some simple function.\n\nThere's then the further question of whether we could do **efficient temporal inference**.\n\n**Conceptual inference** — In our example, we inferred that \\\\(X\\\\) is before \\\\(Y\\\\), and we kind of inferred this structure that what's going on is that the universe is computing \\\\(X\\\\) and \\\\(Z\\\\), and then it's computing \\\\(Y\\\\) from \\\\(X\\\\) together with \\\\(Z\\\\). So \\\\(X\\\\) and \\\\(Z\\\\) look like primitive properties, and \\\\(Y\\\\) looks like a derived property that you might compute from \\\\(X\\\\) and \\\\(Z\\\\). And I think that this \"being a primitive property\" is actually also pointing at something about what kind of concepts you want to use.\n\nImagine that I have a number that's either 0 or 1, and it's also either blue or green. I can define a property called \"bleen,\" which is that the number is a blue 0 \\\\(\\text{XOR}\\\\) a green 1. Now I have these three different properties I can talk about: number, color, and bleen-ness.\n\nAnd we're doing something that looks like inferring that color is primitive, while bleen-ness is derived.\n\nThis system has the potential to be able to distinguish between blue and bleen. I think finite factored sets have the potential to do to \"there's no principled difference between blue and bleen\" what Pearl did to the adage \"correlation does not imply causation\". So that's a thing I'm really excited about maybe being able to do.\n\n**Temporal inference from raw data and fewer ontological assumptions** — We can do a more end-to-end form of temporal inference, one that doesn't have to see where our variables are coming from. \n\nWe can do **temporal inference with deterministic relationships**. \n\n**Time without orthogonality** — You could actually imagine starting with a huge factored set with a large number of dimensions, and then taking some \\\\(\\Omega\\\\) and forgetting a lot of the structure and just taking some weird subset of our big factored set.\n\nIf you do this, it might be the case that there is no orthogonality besides the vacuous orthogonalities. There might be no orthogonality in \\\\(\\Omega\\\\), but you can still make inferences from the probability distribution. The probability distribution will still lie on some algebraic curve that's pointing at the structure. And so even when there's no orthogonality at all, you might still be able to infer temporal relationships just from the probabilities.\n\n**Conditioned factored sets** — Instead of assuming there's a factored set model of \\\\(\\Omega\\\\), you could imagine a partial factored set model where my \\\\(f\\\\) can be a partial function.\n\nThis kind of corresponds to pretending that you had a factored set and you got some samples from \\\\(\\Omega\\\\), but those samples got passed through a filter. And so you can try to do inference on the more general class where maybe your data also went through a filter before you got it.\n\n### Infinity\n\n**Extending definitions to the infinite case** — I talked a little bit about this earlier. I think that the fundamental theorem should be able to be extended to finite-dimensional factored sets, where by \"finite-dimensional\" I mean that \\\\(|B|\\\\) is finite, but \\\\(|S|\\\\) is infinite. But my proof assume finiteness, not just finite dimension.\n\n**Continuous time** — Here's something I'm pretty excited about. Basically, I think that Pearl is not really ready to be continuous. There’s this blog post by Eliezer called [Causal Universes](https://www.lesswrong.com/posts/o5F2p3krzT4JgzqQc/causal-universes) that pointed out that physics kind of looks like a continuous Bayes net. But we didn't have a continuous Bayes net, and we still don't have a continuous Bayes net; and I feel like we're a lot closer to having a continuous Bayes net now.\n\nEverything's still finite, and so we're running into problems there. But I think that once we’ve figured out this infinity stuff, we're going to be able to make things continuous, and I think that for two reasons.\n\n1.  The Pearlian world is really dependent on parents. We really cared about if this node was a parent of this node. In the finite factored sets paradigm, we only care about the analog of ancestor. So we're not really talking about an edge from this node to this node; we're only talking about “Are there paths?”, which is a lot closer to being able to be continuous.\n2.  We have this ability to zoom in and out on our variables. Instead of having fixed variables, we're taking them to be partitions, which allows for this zooming in and out.\n\nI think that because of these two properties, we're a lot closer to being able to have not just continuous probabilities, but continuous time—continuity in the part that in Pearl was the DAG.\n\n**New lens on physics** — Finite factored sets might be able to shine some new light on physics. I don't know; I'm not a physicist.\n\n### Embedded agency\n\nI'm mostly viewing finite factored sets as the successor to [Cartesian frames](https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames). In fact, if you look at the definition of a factored set model, it's almost equivalent; a binary factored set model is basically a Cartesian frame.\n\nSo I want to be able to model things like embedded observations. I want to be able to take a partition that corresponds to an action of an agent and take another partition that corresponds to an observation, and be able to say, \"Ah, the thing that is deciding, this agent, kind of gets to do whatever functions it wants on this other thing.\" And I actually have some preliminary thoughts on how to define observations in a factored set world.\n\nA point I want to make here is that in these embedded agency applications, I'm largely doing something different than I was doing in the temporal inference set. I focused in this talk on being able to do temporal inference because I think it's really concrete to be able to say, \"Hey, we can do something like Pearlian temporal inference, but better and with fewer assumptions.\" \n\nWhen I'm thinking about embedded agency, I'm not really thinking about doing inference. I'm just thinking about being able to *set up* these factored set models where I would otherwise set up a DAG.\n\nOr it doesn't even need to be a DAG. Anywhere we would want to set up a graph that has edges between nodes that correspond to causality, or information flow, or something like that, there's some hope of being able to translate that over to the factored set world and thus be able to have it work better with abstraction.\n\nSo I'm not really thinking about where these things are coming from. I'm more thinking about \"How we can use this as a new way of modeling things that we would otherwise model with graphs?\"\n\nSo:\n\n**Embedded observations** — I have a definition of embedded observation. It looks a lot like the Cartesian frames version, except it's cleaner. \n\n**Counterfactability** — I have a notion of counterfactability that I can get out of factored sets. It's basically taking the position that you can't always take counterfactuals, but some places you can. I'm using the fact that a factored set takes your world and views it as a bunch of tuples to give me \"interventions.\" My \"interventions\" correspond to coming in and tweaking some of the values in the tuples. So the factored set is also giving you a theory of how to do interventions.\n\n**Cartesian Frame successor** — I think this framework is largely capturing a lot of the Cartesian frames content.\n\n**Unraveling causal loops** — This is largely what I talk about in [Saving Time](https://www.lesswrong.com/posts/gEKHX8WKrXGM4roRC/saving-time). It feels like often we have these causal loops, and it feels like a bottleneck to being able to not have causal loops in our picture is to be able to have something that has time and also abstraction in the picture. And that's kind of what we're doing.\n\n**Conditional time** — This is observing that just like we defined conditional orthogonality, we could define conditional time. We just take conditional time to be that the conditional histories are subsets of each other.\n\nI have some hope that this might actually be interesting and useful, especially for the unraveling causal loops thing. I think that conditional time allows you to have multiple different timelines on your structure simultaneously, and have them be concretely playing together. I'm not sure whether it's actually going to end up being useful, but I'm just noticing that there's a very tempting way to define conditional time. \n\n**Logical causality from logical induction** — One of the first things that we did after finding logical induction was to ask, \"Can we extend this to a theory of logical counterfactuals?\" And we said, \"How can we do this? Well, we could take our big probability distribution over logical sentences that we have at any given time. Can we do Pearl to that?”\n\nAnd the answer is, \"No, because there's a whole bunch of determinism everywhere.\"\n\nI think that we still can't just go and do logical causality from something like logical induction. Basically, the finite factored set framework kind of doesn't believe in the uniform distribution on a four-element set. And I have some hopes of being able to fix this, but if you imagine a four-element set and the uniform distribution on it, and you consider the four two-part partitions on your four-element set (like the \\\\(X\\\\), \\\\(Y\\\\), and \\\\(Z\\\\) in our \"two binary variables\" example). If you have the uniform distribution, we have that \\\\(X\\\\) is independent of \\\\(Y\\\\), and \\\\(X\\\\) is independent of \\\\(Z\\\\), but \\\\(X\\\\) is not independent of \\\\(Y∨ Z\\\\).\n\nSo the independence structure of the uniform distribution on a four-element set does not actually satisfy the compositional semigraphoid axioms. It specifically doesn't satisfy the composition axiom.\n\nSimilarly to how I'm excited about the infinite case even though the fundamental theorem will fail, I think that we're not going to be able to just directly use the fundamental theorem, but we could still ask, \"What are some things consistent with this data?\" We might have some extra independence that comes from something other than orthogonality. I could talk more about that, but I think I'll move on now.\n\n**Orthogonality as simplifying assumptions for decisions** — We talked about orthogonality as coming from independence, but I think part of being an agent interacting with the world is that there are some parts of the world out there that I just believe are not going to be affected by my action. And I make this assumption, and I act based on this assumption. And I think this is another way that we can think of orthogonality as coming in.\n\nAnd so I just wanted to point out that orthogonality doesn't need to come from probability. We can think of it as coming from somewhere else. \n\nAnd finally, **conditional orthogonality as an abstraction desideratum** — This is largely the kind of stuff that John Wentworth talks about. Like, an abstraction of \\\\(X\\\\) should have the property that everything that I care about is orthogonal to \\\\(X\\\\), given the abstraction of \\\\(X\\\\). So I only throw away irrelevant data, and I can express \"I only throw away irrelevant data\" as \"everything I care about is orthogonal to the underlying structure, given the abstraction.\"\n\nAll right. Those are my long rants about potential research directions for finite factored sets.\n\nI'm going to say one more thing that's not on this slide: You can think of history as similar to entropy. You can think of my notion of history as a non-quantitative version of entropy.\n\nAnd actually, when proving the semigraphoid axioms, I use some lemmas that look a lot like theorems about entropy, and then I replaced the sums and the expectations with unions.\n\nAnd when you look at time—well, time is the subset relation on history, which is like saying that time is an inequality relation on entropy, and \"later\" corresponds to higher entropy. This is speculation, but this is a whole other ontology that you could put on what history means, etc.\n\nQ&A\n===\n\n\\[*Select questions from the Q&A are included below. The full Q&A transcript is* [*here*](https://docs.google.com/document/d/1sMrsjsv3i657Nqnvvs839Whjw97XUFAGwNL0zNYACes/edit)*.*\\]\n\n<table><tbody><tr><td><p>[...]</p><p><strong>Anonymous 14: </strong>This is a really wide question—feel free to punt it—but could you say anything about your overall theory of change? How you see this work trickling into the real world over time and leading to safer systems?</p><p><strong>Scott: </strong>Man, I don't know. I'm trying to figure out what agency is. (<i>laughs</i>)</p><p><strong>Anonymous 14: </strong>Yeah.</p><p><strong>Scott: </strong>And I really have felt like a major bottleneck has been something about abstraction and time. I now feel like I have a much better ability to play with abstraction, and I think this might help me and might help people who are working on things very adjacent to me.</p><p>That's in the embedded agency column here. I think that for factored sets helping with AI safety, the two main axes are (1) figuring out the stuff related to embedded agency, and (2) the possibility of being able to have a new analog of Pearl.</p><p>I think that conceptual-inference-type stuff is especially useful for transparency. Being able to have systems and see what they're doing feels like something that's passing through conceptual inference, basically. Like, the extent to which we can distinguish between blue and bleen feels like it could help with being able to translate and oversee stuff.</p><p>And when I’ve seen random applications that factored through, I often see places where I'm like, “Oh, yeah, if I understood this thing that I'm kind of saying is about embedded agency, but really is about understanding the world, then I'd be less confused about this random thing that I'm trying to figure out.”</p><p><strong>Vanessa Kosoy:</strong> You were saying before that one of the problems with Pearl's formalism is that it doesn't deal well with deterministic things. And that this is supposed to be one of the advantages of finite factored sets. So I'm confused about how it's going to work, because if I think about this temporal inference and you have something deterministic, then instead of a probability distribution&nbsp;<span class=\"math-tex\">\\(Ω\\)</span>, you just have a point in&nbsp;<span class=\"math-tex\">\\(Ω\\)</span>, and it doesn't seem to give you anything?</p><p><strong>Scott: </strong>Yeah, that's not what I meant by determinism. I meant that I want some variables to be deterministic functions of other variables. I'm not saying the whole system is deterministic; I'm saying some variables are deterministic functions of other variables.</p><p>And, this is a long tangent, but I think largely where one might see a crux between me and Pearl is something like: I'm doing some variable non-realism. In a sense, I’m saying variables are super important; but in another sense, I’m also doing some variable non-realism, where I'm saying that there isn't a reality to the variable beyond its information content.</p><p>So Pearl will have a structure that has all this “Who listens to whom?”, and there's this node corresponding to this variable and an arrow to a node corresponding to that variable.</p><p>And I'm working with, “There isn't reality in the variable beyond its information content, so I wouldn't be able to have two variables that are deterministic copies of each other.” Or I would, and it wouldn't matter or something. My notion of a variable is only its information content.&nbsp;</p><p>So, one thing that might happen if you tried to do temporal inference in Pearl and allowed for determinism is you might be able to say: Well, there's an arrow from&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(Y\\)</span>. But I can just make a copy of&nbsp;<span class=\"math-tex\">\\(X\\)</span>,&nbsp;<span class=\"math-tex\">\\(X^{\\prime}\\)</span>, that's just a deterministic copy of&nbsp;<span class=\"math-tex\">\\(X\\)</span>. And I have an arrow from&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(X^{\\prime}\\)</span>&nbsp;that is just “copy&nbsp;<span class=\"math-tex\">\\(X\\)</span>”. And then&nbsp;<span class=\"math-tex\">\\(X^{\\prime}\\)</span>&nbsp;is not before&nbsp;<span class=\"math-tex\">\\(Y\\)</span>, but&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;is, in the Pearlian world.</p><p>And I could just swap&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(X^{\\prime}\\)</span>, and now&nbsp;<span class=\"math-tex\">\\(X^{\\prime}\\)</span>&nbsp;is before&nbsp;<span class=\"math-tex\">\\(Y\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(X\\)</span>&nbsp;is not.</p><p>And doing stuff like this in Pearl is pointing out, “Well, there's this variable realism and things can be deterministically the same, but have different temporal properties.” And I'm taking seriously the variables just as information content, and that's one way that you can see the generator of this.</p><p><strong>Ben Pace: </strong>When did you do the work?</p><p><strong>Scott: </strong>Largely after I finally got <a href=\"https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames\">Cartesian frames</a> out. I was like, “Okay, now I can do new stuff.” And then I was like, “Okay, the new stuff is better.” (<i>laughs</i>)</p><p>But really, you could view Cartesian frames as building up to this. Largely, I was looking at Cartesian frames, and I was frustrated with how it felt like the whole function from&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(W\\)</span>&nbsp;was not derived enough. I was like, “Oh, let's derive everything and not have basic things.” But I was like, “The function from&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(W\\)</span>—well, what is that really?”</p><p>Well for one, you have a function from&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(W\\)</span>, which is saying that the information in&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>&nbsp;is enough to determine the&nbsp;<span class=\"math-tex\">\\(W\\)</span>.</p><p>But then also I have this&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>, which is not just the set&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>. I'm actually thinking of it as&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>. There's this orthogonality between&nbsp;<span class=\"math-tex\">\\(A\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(E\\)</span>. And I have this extra thing, which is the relationship between&nbsp;<span class=\"math-tex\">\\(A\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(E\\)</span>. And what is that made of?</p><p>There's this combinatorial notion of independence, where every way of combining an element of&nbsp;<span class=\"math-tex\">\\(A\\)</span>&nbsp;with an element of&nbsp;<span class=\"math-tex\">\\(E\\)</span>&nbsp;is possible. But it felt like there was more than that. When I draw the Cartesian frame&nbsp;<span class=\"math-tex\">\\(A \\times E\\)</span>&nbsp;to&nbsp;<span class=\"math-tex\">\\(W\\)</span>, there's this orthogonality assumption, and I'm saying that we can break the world up into&nbsp;<span class=\"math-tex\">\\(A\\)</span>&nbsp;and&nbsp;<span class=\"math-tex\">\\(E\\)</span>—what's going on there? That's largely the stuff that I was thinking about.</p><p>[...]</p><p><strong>Ben Pace: </strong>Which are the bits that you think would be easiest for someone else to write posts about and add on to?</p><p><strong>Scott:</strong> I don't know, just however they're excited about applying it. I don't have a concrete path forward for doing something like that. I have some of these things that I have thoughts on, and I can write up some more on those. But mostly I'm just like: Here's a new way to model things that involve time, that isn't graphs. Go. (<i>laughs</i>)</p><p>Yeah, I guess I could say things that I would be especially excited for somebody who really wanted to get into this to work on, and possibly write papers, or maybe coauthor papers. One thing would be figuring out the infinite case. Another would be just writing up how to translate from Pearl to this.</p><p>I said I have a way to get from a DAG to a finite factored set. DAGs are a larger model class. And then I think that there are theorems of the form, “You can take a DAG and you can build a finite factored set from it, and time in the DAG exactly corresponds to time in the finite factored set.” Where Pearl is able to ask the questions, where I might be able to ask more questions, and <i>d</i>-separation is going to exactly correspond to Cartesian orthogonality.</p><p>Proving all of that stuff that's really in the structure learning world seems good. If somebody who's an expert in structure learning wanted to do that and would actually be good at the task of figuring out how to get all that written up and get the paper such that we can sell this thing to structure learning people and then maybe do some sort of structure learning revolution, that would be cool.</p><p>Infinity is the hard mathematical problem that would take some work. And then there’s connecting it up to structure learning. And then, a programmer that wanted to work on this could try to generate a bunch of theorems about temporal inference. Like, I gave a combinatorial thing where on one side you can search for factored set models that give you counterexamples to temporal properties, and on the other side you can search for proofs that look like the proof that I gave, where you do Boolean combinations of histories and collect properties on those. And I think that there's some hope of being able to do some computational temporal inference. And a programmer that wanted to try to do that, could do that. And that would be an interesting project and path forward.</p><p>And then in the embedded agency world, it's just a whole bunch of taking places where we use DAGs, and seeing what it looks like with factored sets. Especially places where we use DAGs, and wish we had better ability to work with abstraction. I think that's where the fruit is going to be embedded-agency-wise.</p><p>So those are four concrete handles.</p></td></tr></tbody></table>",
      "plaintextDescription": "This is an edited transcript of Scott Garrabrant's May 23 LessWrong talk on finite factored sets. Video:\n\n\n Slides: https://intelligence.org/files/Factored-Set-Slides.pdf\n\nSince there are existing introductions to finite factored sets that are shorter (link) or more formal (link), I decided to do things a bit differently with this transcript:\n\n * I've incorporated Scott's slides into the transcript, and cut out some parts of the transcript that are just reading a slide verbatim.\n * There was a text chat and Google Doc attendees used to discuss the talk while it was happening; I've inserted excerpts from this conversation into the transcript, in grey boxes.\n * Scott also paused his talk at various times to answer questions; I've included these Q&A sections in white boxes.\n\n \n\n\nFinite Factored Sets\nAnonymous α: Scott: do you have any suspicions that the basis of the mathematics is poorly fit to reality (q.v. Banach-Tarski decomposition)?\n\nDaniel Filan: I really liked it when causality came from pictures that I can look at. Does finite factored sets have pictures? One way this could be true is if it were some sort of category. Like, every other way I know of thinking about time (Newtonian mechanics, general relativity) is super geometric.\n\nRamana Kumar: In case others are interested, I’ve been formalising these results in HOL (as I did for Cartesian Frames, which recently was posted publicly)\n\nDaniel Filan: (why) is HOL a good system in which to formalize these things?\n\nRamana Kumar: Mainly: just because it’s possible, and I know the system/logic well. HOL is pretty good for most of math though - see this for some advocacy.\n\nDaniel Kilan: thx, will give that a look!\n\nQuinn Dougherty: +1\n\n \n\n \n\n\nSome Context\nScott: I'm going to start with some context. For people who are not already familiar with my work: My main motivation is to reduce existential risk. This leads me to want to figure out how to align advanced AI. This leads me to want to become less confused about int",
      "wordCount": 15191
    },
    "tags": [
      {
        "_id": "zFrdtg6wCuzWu6mhy",
        "name": "Finite Factored Sets",
        "slug": "finite-factored-sets"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "QvwSr5LsxyDeaPK5s",
    "title": "\"Existential risk from AI\" survey results",
    "slug": "existential-risk-from-ai-survey-results",
    "url": null,
    "baseScore": 66,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2021-06-01T20:02:05.688Z",
    "contents": {
      "markdown": "I sent a two-question survey to ~117 people working on long-term AI risk, asking about the level of existential risk from \"humanity not doing enough technical AI safety research\" and from \"AI systems not doing/optimizing what the people deploying them wanted/intended\".\n\n44 people responded (~38% response rate). In all cases, these represent the views of specific individuals, not an official view of any organization. Since some people's views may have made them more/less likely to respond, I suggest caution in drawing strong conclusions from the results below. Another reason for caution is that respondents added a lot of caveats to their responses (see the [anonymized spreadsheet](https://docs.google.com/spreadsheets/d/1grw0kFSTsZzB93stOpWaOj4CgoTftl9jO50FeHnmlgM/edit)),\\\\(^{1}\\\\) which the aggregate numbers don't capture.\n\nI don’t plan to do any analysis on this data, just share it; anyone who wants to analyze it is of course welcome to.\n\nIf you'd like to make your own predictions before seeing the data, \\\\(\\\\)I made [a separate spoiler-free post for that](https://www.lesswrong.com/posts/3LtDzDJc32TF6bJEY/predict-responses-to-the-existential-risk-from-ai-survey).\n\nMethods\n-------\n\nYou can find a copy of the survey [here](https://docs.google.com/forms/d/13u7164jFHxcE6qWDcsgsLUsMZr72VpEvpxGcJsRp--E/edit). The main questions (including clarifying notes) were:\\\\(^{2}\\\\)\n\n> 1\\. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research?\n> \n> 2\\. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended?\n> \n> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n> \n> Note A: \"Technical AI safety research\" here means good-quality technical research aimed at figuring out how to get highly capable AI systems to produce long-term outcomes that are reliably beneficial.\n> \n> Note B: The intent of question 1 is something like \"How likely is it that our future will be drastically worse than the future of an (otherwise maximally similar) world where we put a huge civilizational effort into technical AI safety?\" (For concreteness, we might imagine that human whole-brain emulation tech lets you gather ten thousand well-managed/coordinated top researchers to collaborate on technical AI safety for 200 subjective years well before the advent of AGI; and somehow this tech doesn't cause any other changes to the world.)\n> \n> The intent of question 1 \\*isn't\\* \"How likely is it that our future will be astronomically worse than the future of a world where God suddenly handed us the Optimal, Inhumanly Perfect Program?\". (Though it's fine if you think the former has the same practical upshot as the latter.)\n> \n> Note C: We're asking both 1 and 2 in case they end up getting very different answers. E.g., someone might give a lower answer to 1 than to 2 if they think there's significant existential risk from AI misalignment even in worlds where humanity put a major civilizational effort (like the thousands-of-emulations scenario) into technical safety research.\n\nI also included optional fields for \"Comments / questions / objections to the framing / etc.\" and \"Your affiliation\", and asked respondents to\n\n> Check all that apply:\n> \n> ☐ I'm doing (or have done) a lot of technical AI safety research.\n> \n> ☐ I'm doing (or have done) a lot of governance research or strategy analysis related to AGI or transformative AI.\n\nI sent the survey out to two groups directly: MIRI's research team, and people who recently left OpenAI (mostly people suggested by Beth Barnes of OpenAI). I sent it to five other groups through org representatives (who I asked to send it to everyone at the org \"who researches long-term AI topics, or who has done a lot of past work on such topics\"): OpenAI, the Future of Humanity Institute (FHI), DeepMind, the Center for Human-Compatible AI (CHAI), and Open Philanthropy.\\\\(^{2,3}\\\\)\n\nThe survey ran for 23 days (May 3–26), though it took time to circulate and some people didn't receive it until May 17.\\\\(^{4}\\\\)\n\nResults\n-------\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/6912afe472dd562bd810527814b28aa98b347e53586e0e5d.png)\n\n[Each point](https://docs.google.com/spreadsheets/d/1grw0kFSTsZzB93stOpWaOj4CgoTftl9jO50FeHnmlgM/edit#gid=1286378694) is a response to Q1 (on the horizontal axis) and Q2 (on the vertical axis). Circles denote (pure) technical safety researchers, squares (pure) strategy researchers; diamonds marked themselves as both, triangles as neither. In four cases, shapes are superimposed because 2–3 respondents gave the same pair of answers to Q1 and Q2.\\\\(^{5}\\\\) One respondent (a \"both\" with no affiliation specified) was left off the chart because they gave interval answers: \\[0.1, 0.5\\] and \\[0.1, 0.9\\].\n\nPurple represents OpenAI, red FHI, green CHAI or UC Berkeley, orange MIRI, blue Open Philanthropy, and black \"no affiliation specified\". No respondents marked DeepMind as their affiliation.\n\nSeparating out the technical safety researchers (left) and the strategy researchers (right):\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/aa2ead7d1547323f08901dd51299d804489cacc015553561.png)\n\nOverall, the mean answer of survey respondents was (~0.3, ~0.4), and the median answer was (0.2, 0.3).\n\n### Background and predictions\n\nI'd been considering running a survey like this for a little while, and decided to pull the trigger after a [conversation on the EA Forum](https://forum.effectivealtruism.org/posts/78NoGoRitPzeT8nga/draft-report-on-existential-risk-from-power-seeking-ai) in which I criticized an analysis that assigned low probability to a class of AI risk scenarios. In the EA Forum conversation, I quoted a prediction of mine (generated in 2017 via discussion with a non-MIRI researcher I trust):\n\n> I think that at least 80% of the AI safety researchers at MIRI, FHI, CHAI, OpenAI, and DeepMind would currently assign a >10% probability to this claim: \"The research community will fail to solve one or more technical AI safety problems, and as a consequence there will be a permanent and drastic reduction in the amount of value in our future.\"\n\nThis is (I think) reasonably close to Q1 in the survey. Looking only at people who identified themselves as MIRI/FHI/CHAI/OpenAI/DM (so, excluding Open Phil and ‘no affiliation specified’) and as technical safety or strategy researchers, 11 / 19 = ~58% gave >10% probability to question 1, which is a far cry from my predicted 80+%. I expect this at least partly reflects shifts in the field since 2017, though I think it also casts some doubt on my original claim (and certainly suggests I should have hedged more in repeating it today). Restricting to technical safety researchers, the number is 10 / 15 = ~67%.\n\nNote that respondents who were following the forum discussion might have been anchored in some way by that discussion, or might have had a social desirability effect from knowing that the survey-writer puts high probability on AI risk. It might also have made a difference that I work at MIRI.\\\\(^{6}\\\\)\n\n### Respondents' comments\n\nA large number of respondents noted that their probability assignments were uncertain or unstable, or noted that they might give a different probability if they spent more time on the question. More specific comments included...\n\n(Caveat: I'm choosing the bins below arbitrarily, and I'm editing out meta statements and uncertainty-flagging statements; see the [spreadsheet](https://docs.google.com/spreadsheets/d/1grw0kFSTsZzB93stOpWaOj4CgoTftl9jO50FeHnmlgM/edit#gid=0) for full answers.)\n\n... from respondents whose highest probability was < 25%:\n\n*   0.1, 0.05: \"\\[...\\] The first is higher than the second because we're thinking a really truly massive research effort -- it seems quite plausible that (a) coordination failures could cause astronomical disvalue relative to coordination successes and (b) with a truly massive research effort, we could fix coordination failures, even when constrained to do it just via technical AI research. I don't really know what probability to assign to this (it could include e.g. nuclear war via MAD dynamics, climate change, production web, robust resistance to authoritarianism, etc and it's hard to assign a probability to all of those things, and that a massive research effort could fix them when constrained to work via technical AI research).\"\n*   0.1, 0.12: \"(2) is a bit higher than (1) because even if we 'solve' the necessary technical problems, people who build AGI might not follow what the technical research says to do\"\n*   0.2, 0.2: \"'Drastically less than it could have been' is confusing, because it could be 'the future is great, far better than the present, but could have been drastically better still' or alternatively 'we destroy the world / the future seems to have very negative utility'. I'm sort of trying to split the difference in my answer. If it was only the latter, my probability would be lower, if the former, it would be higher. Also, a world where there was tremendous effort directed at technical AI safety seems like a world where there would be far more effort devoted to governance/ethics, and I'm not sure how practically to view this as a confounder.\"\n*   0.019, 0.02: \"Low answers to the above should not be taken to reflect a low base rate of P(humans achieve AGI); P(humans achieve AGI) is hovering at around 0.94 for me. \\[...\\]\"\n\n... from respondents whose highest probability was 25–49%:\n\n*   0.25, 0.3: \"\\`1\\` and \\`2\\` seem like they should be close together for me, because in your brain emulation scenario it implies that our civ has a large amount of willingness to sacrifice competitiveness/efficiency of AI systems (by pausing everything and doing this massive up-front research project). This slack seems like it would let us remove the vast majority of AI x-risk.\"\n*   0.15, 0.3: \"I think my answer to (1) changes quite a lot based on whether 'technical AI safety research' is referring only to research that happens before the advent of AGI, separate from the process of actually building it.   \n      \n    In the world where the first AGI system is built over 200 years by 10,000 top technical researchers thinking carefully about how to make it safe, I feel a lot more optimistic about our chances than the world where 10,000 researchers do research for 200 years, then hand some kind of blueprint to the people who are actually building AGI, who may or may not actually follow the blueprint.  \n      \n    I'm interpreting this question as asking about the latter scenario (hand over blueprint). If I interpreted it as the former, my probability would be basically the same as for Q2.\"\n\n... from respondents whose highest probability was 50–74%:\n\n*   0.2, 0.5: \"\\[...\\] Deployment-related work seems really important to me, and I take that to be excluded from technical research, hence the large gap.\"\n*   0.68, 0.63: \"It's a bit ambiguous whether Q1 covers failures to apply technical AI safety research. Given the elaboration, I'm taking 1 to include the probability that some people do enough AI safety research but others don't know/care/apply it correctly. \\[...\\]\"\n*   0.15, 0.5: \"On the answer to 2,  I'm counting stuff like 'Human prefs are so inherently incoherent and path-dependent that there was never any robust win available and the only thing any AI could possibly do is shape human prefs into something arbitrary and satisfy those new preferences, resulting in a world that humans wouldn't like if they had taken a slightly different path to reflection and cognitive enhancement than the one the AI happened to lead them down.' I guess that's not quite a central example of 'the value of the future \\[being\\] drastically less than it could have been'? \\[...\\]\"\n*   0.1, 0.5: \"\\[...\\] I think there is some concern that a lot of ways in which 2 is true might be somewhat vacuous: to get to close-to-optimal futures we might need advanced AI capabilities, and these might only get us there if AI systems broadly optimize what we want. So this includes e.g. scenarios in which we never develop sufficiently advanced AI. Even if we read 'AI systems not doing/optimizing what the people deploying them wanted' as presupposing the existence of AI systems, there may be vacuous ways in which non-advanced AI systems don't do what people want, but the key reason is not their 'misalignment' but simply their lack of sufficiently advanced capabilities.  \n      \n    I think on the most plausible narrow reading of 2, maybe my answer is more like 15%.\"\n*   0.26, 0.66: \"The difference between the numbers is because (i) maybe we will solve alignment but the actor to build TAI will implement the solution poorly or not at all (ii) maybe alignment always comes at the cost of capability and competition will lead to doom (iii) maybe solving alignment is impossible\"\n*   0.5, 0.5: \"\\[...\\] My general thoughts: might be more useful to discuss concrete trajectories AI development could follow, and then concrete problems to solve re safety. Here's one trajectory:  \n      \n    1\\. AI tech gets to the point where it's useful enough to emulate smart humans with little resource costs: AI doesn't look like an 'optimised function for a loss function', rather it behaves like a human who's just very interested in a particular topic (e.g. someone who's obsessed with mathematics, some human interaction, and nothing else).  \n      \n    2\\. Governments use this to replicate many researchers in research fields, resulting in a massive acceleration of science development.  \n      \n    3\\. We develop technology which drastically changes the human condition and scarcity of resources: e.g. completely realistic VR worlds, drugs which make people feel happy doing whatever they're doing while still functioning normally, technology to remove the need to sleep and ageing, depending on what is empirically possible.  \n      \n    Dangers from this trajectory: 1. the initial phase of when AI is powerful, but we haven't reached (3), and so there's a chance individual actors could do bad things. 2. Some may think that certain variations of how (3) ends up may be bad for humanity; for example, if we make a 'happiness drug', it might be unavoidable that everyone will take it, but it might also make humanity content by living as monks.\"\n*   0.5, 0.7: \"For my 0.5 answer to question 1, I'm not imagining an ideal civilizational effort, but am picturing things being 'much better' in the way of alignment research. \\[...\\]\"\n*   0.48, 0.56: \"I’m interpreting 'not doing what the people deploying them wanted/intended' as intended to mean 'doing things that are systematically bad for the overall value of the future', but this is very different from what it literally says. The intentions of people deploying AI systems may very well be at odds with the overall value of the future, so the latter could actually benefit from the AI systems not doing what the people deploying them wanted. If I take this phrase at its literal meaning then my answer to question 2 is more like 0.38.\"\n*   0.73, 0.7: \"2 is less likely than 1 because I put some weight on x-risk from AI issues other than alignment that have technical solutions.\"\n\n... from respondents whose highest probability was > 74%:\n\n*   0.6, 0.8: \"What are the worlds being compared in question 2? One in which AI systems (somehow manage to) do what the people deploying them wanted/intended vs the default? Or <the former> vs one in which humanity makes a serious effort to solve AI alignment? I answered under the first comparison.  \n      \n    My answer to question 1 might increase if I lean more on allowing 'technical' to include philosophy (e.g., decision theory, ethics, metaphysics, content) and social science (e.g., sociology, economics).\"\n*   0.8, 0.7: \"I think that there's a lot riding on how you interpret 'what the people deploying them wanted/intended' in the second question—e.g. does it refer to current values or values after some reflection process?\"\n*   0.98, 0.96: \"I think there's a large chance that the deployers will take an attitude of 'lol we don't care whatevs', but I'm still counting this as 'not doing what they intended' because I expect that the back of their brains will still have expected something other than instant death and empty optimizers producing paperclips. If we don't count this fatality as 'unintended', the answer to question 2 might be more like 0.85.  \n      \n    The lower probability to 2 reflects the remote possibility of deliberately and successfully salvaging a small amount of utility that is still orders of magnitude less than could have been obtained by full alignment, in which possible worlds case 1 would be true and case 2 would be false.\"\n*   0.2, 0.9: \"\\[...\\] I think the basket of things we need a major effort on in order to clear this hurdle is way way broader than technical AI safety research (and inclusive of a massive effort into technical AI safety research), so 1 is a lot lower than 2 because I think 'doing enough technical AI safety research' is necessary but not sufficient \\[...\\]\"\n\nAfter collecting most of the comments above, I realized that people who gave high x-risk probabilities in this survey tended to leave a lot more non-meta comments; I'm not sure why. Maroon below is \"left a non-meta comment\", pink is \"didn't\", red is a {pink, maroon} pair:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a66085fcc312dcb911407dd97579ec2e1944806ce8957dbe.png)\n\n* * *\n\n*Thank you to everyone who participated in the survey or helped distribute it. Additional thanks to Paul Christiano, Evan Hubinger, Rohin Shah, and Carl Shulman for feedback on my question phrasing, though I didn't take all of their suggestions and I'm sure their ideal version of the survey would have looked different.*\n\n* * *\n\nFootnotes\n---------\n\n\\\\(^{1}\\\\) Changes to the spreadsheet: I redacted respondents’ names, standardized their affiliation input, and randomized their order. I interpreted ‘80’ and ‘70’ in one response, and ‘15’ and ‘15’ in another, as percentages.\n\n\\\\(^{2}\\\\) I’d originally intended to only survey technical safety researchers and only send the survey to CHAI/DeepMind/FHI/MIRI/OpenAI, but Rohin Shah suggested adding Open Philanthropy and including strategy and forecasting researchers. I’d also originally intended to only ask question #1, but Carl Shulman proposed that I include something like question #2 as well. I think both recommendations were good ones.\n\n\\\\(^{3}\\\\) The \"117 recipients\" number is quite approximate, because:\n\n*   (a) CHAI's representative gave me a number from memory, which they thought might be off by 1 or 2 people.\n*   (b) FHI's representative erred on the side of sharing it with a relatively large group, of whom he thought \"5-8 people might either self-select into not counting or are people you would not want to count. (Though I would guess that there is decent correlation between the two, which was one reason why I erred on being broad.)\"\n*   (c) Beth from OpenAI shared it in a much larger (85-person) Slack channel; but she told people to only reply if they met my description. OpenAI's contribution to my \"117 recipients\" number is based on Beth's guess about how many people in the channel fit the description.\n\n\\\\(^{4}\\\\) Response rates were ~identical for people who received the survey at different times (ignoring any who might have responded but didn't specify an affiliation):\n\n*   May 3–4: ~30%\n*   May 10: ~29%\n*   May 15–17: ~28%\n\n\\\\(^{5}\\\\) Overlapping answers:\n\n*   (0.2, 0.4) from an FHI technical safety researcher and an affiliation-unspecified technical safety researcher.\n*   (0.25, 0.3) OpenAI tech safety and affiliation-unspecified tech safety.\n*   (0.1, 0.15) CHAI tech safety and FHI general staff.\n*   (0.1, 0.1) FHI tech safety, affiliation-unspecified strategy, FHI general staff.\n\nIn case it's visually unclear, I'll note that in the latter case, there are also two FHI strategy researchers who gave numbers very close to (0.1, 0.1).\n\n\\\\(^{6}\\\\) I checked whether this might have caused MIRI people to respond at a higher rate. 17/117 people I gave the survey to work at MIRI (~15%), whereas 5/27 of respondents who specified an affiliation said they work at MIRI (~19%).",
      "plaintextDescription": "I sent a two-question survey to ~117 people working on long-term AI risk, asking about the level of existential risk from \"humanity not doing enough technical AI safety research\" and from \"AI systems not doing/optimizing what the people deploying them wanted/intended\".\n\n44 people responded (~38% response rate). In all cases, these represent the views of specific individuals, not an official view of any organization. Since some people's views may have made them more/less likely to respond, I suggest caution in drawing strong conclusions from the results below. Another reason for caution is that respondents added a lot of caveats to their responses (see the anonymized spreadsheet),1 which the aggregate numbers don't capture.\n\nI don’t plan to do any analysis on this data, just share it; anyone who wants to analyze it is of course welcome to.\n\nIf you'd like to make your own predictions before seeing the data, I made a separate spoiler-free post for that.\n\n \n\n\nMethods\nYou can find a copy of the survey here. The main questions (including clarifying notes) were:2\n\n> 1. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research?\n> \n> 2. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended?\n> \n> _________________________________________\n> \n> Note A: \"Technical AI safety research\" here means good-quality technical research aimed at figuring out how to get highly capable AI systems to produce long-term outcomes that are reliably beneficial.\n> \n> Note B: The intent of question 1 is something like \"How likely is it that our future will be drastically worse than the future of an (otherwise maximally similar) world where we put a huge civilizational effort into technical AI safety?\" (For concreteness",
      "wordCount": 3268
    },
    "tags": [
      {
        "_id": "kJrjorSx3hXa7q7CJ",
        "name": "Surveys",
        "slug": "surveys"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3LtDzDJc32TF6bJEY",
    "title": "Predict responses to the \"existential risk from AI\" survey",
    "slug": "predict-responses-to-the-existential-risk-from-ai-survey",
    "url": null,
    "baseScore": 44,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2021-05-28T01:32:18.059Z",
    "contents": {
      "markdown": "I sent a short survey to ~117 people working on long-term AI issues, asking about the level of existential risk from AI; 44 responded.\n\nIn ~6 days, I'm going to post the anonymized results. For now, I'm posting the methods section of my post so anyone interested can predict what the results will be.\n\n\\[**Added June 1**: [Results are now up](https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results), though you can still make predictions below before reading the results.\\]\n\nMethods\n-------\n\nYou can find a copy of the survey [here](https://docs.google.com/forms/d/13u7164jFHxcE6qWDcsgsLUsMZr72VpEvpxGcJsRp--E/edit). The main questions (including clarifying notes) were:\n\n> 1\\. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research?\n> \n> 2\\. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended?\n> \n> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n> \n> Note A: \"Technical AI safety research\" here means good-quality technical research aimed at figuring out how to get highly capable AI systems to produce long-term outcomes that are reliably beneficial.\n> \n> Note B: The intent of question 1 is something like \"How likely is it that our future will be drastically worse than the future of an (otherwise maximally similar) world where we put a huge civilizational effort into technical AI safety?\" (For concreteness, we might imagine that human whole-brain emulation tech lets you gather ten thousand well-managed/coordinated top researchers to collaborate on technical AI safety for 200 subjective years well before the advent of AGI; and somehow this tech doesn't cause any other changes to the world.)\n> \n> The intent of question 1 \\*isn't\\* \"How likely is it that our future will be astronomically worse than the future of a world where God suddenly handed us the Optimal, Inhumanly Perfect Program?\". (Though it's fine if you think the former has the same practical upshot as the latter.)\n> \n> Note C: We're asking both 1 and 2 in case they end up getting very different answers. E.g., someone might give a lower answer to 1 than to 2 if they think there's significant existential risk from AI misalignment even in worlds where humanity put a major civilizational effort (like the thousands-of-emulations scenario) into technical safety research.\n\nI also included optional fields for \"Comments / questions / objections to the framing / etc.\" and \"Your affiliation\", and asked respondents to\n\n> Check all that apply:\n> \n> ☐ I'm doing (or have done) a lot of technical AI safety research.\n> \n> ☐ I'm doing (or have done) a lot of governance research or strategy analysis related to AGI or transformative AI.\n\nI sent the survey out to two groups directly: MIRI's research team, and people who recently left OpenAI (mostly people suggested by Beth Barnes of OpenAI). I sent it to five other groups through org representatives (who I asked to send it to everyone at the org \"who researches long-term AI topics, or who has done a lot of past work on such topics\"): OpenAI, the Future of Humanity Institute (FHI), DeepMind, the Center for Human-Compatible AI (CHAI), and Open Philanthropy.\n\nThe survey ran for 23 days (May 3–26), though it took time to circulate and some people didn't receive it until May 17.\n\nResults\n-------\n\n\\[Image redacted\\]\n\nEach point is a response to Q1 (on the horizontal axis) and Q2 (on the vertical axis). Circles denote technical safety researchers, squares strategy researchers; triangles said they were neither, and diamonds said they were both.\n\nPurple represents OpenAI, red FHI, brown DeepMind, green CHAI or UC Berkeley, orange MIRI, blue Open Philanthropy, and black \"no affiliation specified\". (This includes unaffiliated people, as well as people who decided to leave their affiliation out.)\n\n\\[Rest of post redacted\\]\n\n**Added:** I've included some binary predictions below on request, though I don't necessarily think these are the ideal questions to focus on. E.g., I'd expect it to be more useful to draw a rough picture of what you expect the distribution to look like (or, say, what you expect the range of MIRI views is, or the range of strategy researchers' views) \n\nQ1:\n\nElicit Prediction ([forecast.elicit.org/binary/questions/spoTvvw5a](forecast.elicit.org/binary/questions/spoTvvw5a))\n\nElicit Prediction ([forecast.elicit.org/binary/questions/I6jsnozIu](forecast.elicit.org/binary/questions/I6jsnozIu))\n\nElicit Prediction ([forecast.elicit.org/binary/questions/W01xAVOHY](forecast.elicit.org/binary/questions/W01xAVOHY))\n\nQ2:\n\nElicit Prediction ([forecast.elicit.org/binary/questions/olmYp4NLl](forecast.elicit.org/binary/questions/olmYp4NLl))\n\nElicit Prediction ([forecast.elicit.org/binary/questions/OxoKXy-Tu](forecast.elicit.org/binary/questions/OxoKXy-Tu))\n\nElicit Prediction ([forecast.elicit.org/binary/questions/ZJldiMcgq](forecast.elicit.org/binary/questions/ZJldiMcgq))\n\n([*Cross-posted to the Effective Altruism Forum*](https://forum.effectivealtruism.org/posts/iBTon2dRYwcoS9Jyr/predict-responses-to-the-existential-risk-from-ai-survey))",
      "plaintextDescription": "I sent a short survey to ~117 people working on long-term AI issues, asking about the level of existential risk from AI; 44 responded.\n\nIn ~6 days, I'm going to post the anonymized results. For now, I'm posting the methods section of my post so anyone interested can predict what the results will be.\n\n[Added June 1: Results are now up, though you can still make predictions below before reading the results.]\n\n \n\n\nMethods\nYou can find a copy of the survey here. The main questions (including clarifying notes) were:\n\n> 1. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research?\n> \n> 2. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended?\n> \n> _________________________________________\n> \n> Note A: \"Technical AI safety research\" here means good-quality technical research aimed at figuring out how to get highly capable AI systems to produce long-term outcomes that are reliably beneficial.\n> \n> Note B: The intent of question 1 is something like \"How likely is it that our future will be drastically worse than the future of an (otherwise maximally similar) world where we put a huge civilizational effort into technical AI safety?\" (For concreteness, we might imagine that human whole-brain emulation tech lets you gather ten thousand well-managed/coordinated top researchers to collaborate on technical AI safety for 200 subjective years well before the advent of AGI; and somehow this tech doesn't cause any other changes to the world.)\n> \n> The intent of question 1 *isn't* \"How likely is it that our future will be astronomically worse than the future of a world where God suddenly handed us the Optimal, Inhumanly Perfect Program?\". (Though it's fine if you think the former has the same practical up",
      "wordCount": 705
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gRnd32CZnAzmpBaJi",
    "title": "Sabien on \"work-life\" balance",
    "slug": "sabien-on-work-life-balance",
    "url": null,
    "baseScore": 57,
    "voteCount": 34,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2021-05-20T18:33:37.981Z",
    "contents": {
      "markdown": "From Duncan Sabien:\n\n> This morning, a friend of mine referred to his \"work-life balance\" and then grimaced at himself, and noted that he doesn't really like that term.\n> \n> (As far as I can tell, he doesn't like it because his work is important to him, and is part of being alive, and his non-work life isn't some *fundamentally* different kind of thing.)\n> \n> This led me to the mental distinction between one's *directly* valuable life experiences, and one's indirectly/instrumentally valuable ones.\n> \n> Like, there are many fewer layers involved when you, say, snuggle up to someone you love, or pop a delicious food in your mouth, or bounce on a trampoline.  There's a very short, direct path between the action and the reward; it's just straightforwardly close to your values and the things which bring you joy and fulfillment.\n> \n> Whereas if you're good at your work and you think that your job is important, there's an intervening layer or three—I'm doing X because it unblocks Y, and that will lead to Z, and Z is good for the world in ways I care about, and also it earns me $ and I can spend $ on stuff...\n> \n> I think it's less about \"work-life balance\" and more about the ratio of direct vs. indirect value.  \"How many of the things that I'm doing pay off directly, versus how many of them are knocking over dominos that eventually *lead* to the payoffs I'm seeking?\"\n> \n> (And how close is that ratio to one which I will actually find sustainable and enjoyable and healthy.)\n> \n> This, to me, is a distinction that cuts closer to the true joints of reality than the arbitrary categories of \"work\" and \"everything else.\"  As a single easy example, this also applies to the balance, in one's relationships, between straightforwardly valuable interactions with people you enjoy, and meta interactions/maintenance/laying the groundwork for the future.",
      "plaintextDescription": "From Duncan Sabien:\n\n> This morning, a friend of mine referred to his \"work-life balance\" and then grimaced at himself, and noted that he doesn't really like that term.\n> \n> (As far as I can tell, he doesn't like it because his work is important to him, and is part of being alive, and his non-work life isn't some fundamentally different kind of thing.)\n> \n> This led me to the mental distinction between one's directly valuable life experiences, and one's indirectly/instrumentally valuable ones.\n> \n> Like, there are many fewer layers involved when you, say, snuggle up to someone you love, or pop a delicious food in your mouth, or bounce on a trampoline.  There's a very short, direct path between the action and the reward; it's just straightforwardly close to your values and the things which bring you joy and fulfillment.\n> \n> Whereas if you're good at your work and you think that your job is important, there's an intervening layer or three—I'm doing X because it unblocks Y, and that will lead to Z, and Z is good for the world in ways I care about, and also it earns me $ and I can spend $ on stuff...\n> \n> I think it's less about \"work-life balance\" and more about the ratio of direct vs. indirect value.  \"How many of the things that I'm doing pay off directly, versus how many of them are knocking over dominos that eventually lead to the payoffs I'm seeking?\"\n> \n> (And how close is that ratio to one which I will actually find sustainable and enjoyable and healthy.)\n> \n> This, to me, is a distinction that cuts closer to the true joints of reality than the arbitrary categories of \"work\" and \"everything else.\"  As a single easy example, this also applies to the balance, in one's relationships, between straightforwardly valuable interactions with people you enjoy, and meta interactions/maintenance/laying the groundwork for the future.",
      "wordCount": 319
    },
    "tags": [
      {
        "_id": "iP2X4jQNHMWHRNPne",
        "name": "Motivations",
        "slug": "motivations"
      },
      {
        "_id": "udPbn9RthmgTtHMiG",
        "name": "Productivity",
        "slug": "productivity"
      },
      {
        "_id": "YSyvvi4uXvxAARX2D",
        "name": "Slack",
        "slug": "slack"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SgszmZwrDHwG3qurr",
    "title": "MIRI location optimization (and related topics) discussion",
    "slug": "miri-location-optimization-and-related-topics-discussion",
    "url": null,
    "baseScore": 141,
    "voteCount": 71,
    "viewCount": null,
    "commentCount": 163,
    "createdAt": null,
    "postedAt": "2021-05-08T23:12:02.476Z",
    "contents": {
      "markdown": "**Added June 28:** Blake Borgeson [comments](https://www.lesswrong.com/posts/SgszmZwrDHwG3qurr/miri-location-optimization-and-related-topics-discussion?commentId=uqZ3ZWxzeuWQaJLes) below,\n\n> Update: After a lot more discussion and thinking things over, MIRI has decided against relocating away from the Bay Area, unless a new impetus comes along to move some time in the future.\n> \n> Mainly, ongoing/propagating strategic updates are behind this decision. We're more uncertain about our strategy overall following our 2020 strategy update (https://intelligence.org/2020/12/21/2020-updates-and-strategy/#3), so we’d rather not pay a big up-front move cost based on a prediction about what we expect MIRI to look like 2 or 5 years from now. Related to the same strategic updates, over the next 12 months or so we plan to experiment with placing less of an emphasis on staff working from our Berkeley base, and expect some staff to live and work from other places. \n> \n> Separately, in the background, considerations against our recent favorite places became stronger:\n> \n> *   Bellingham is far away from big cities, and it seemed increasingly like MIRI would be more isolated there than we wanted. Also, we became more concerned about low light levels during winter months.\n> *   The area around Peekskill has some of the highest Lyme disease incidence in the US, and it seems possible this is a high enough risk of disability to cause us to want to avoid it. We were still investigating this when we ended our location search.\n> \n> Who am I? I’m a MIRI board member, and was in recent months leading/coordinating this MIRI relocation decision process, which is now complete. \n> \n> Thanks for all the feedback in these comments, in emails, and on Facebook. I (and probably others) read all of it I could find, and it was important input for us.\n\n* * *\n\n**Original post:**\n\nMIRI is moving (with high probability)!\n\nWe haven’t finalized a location yet, but there’s a good chance we’ll make our decision in the next six weeks. I want to solicit:\n\n*   Feedback on our current top location candidates.\n*   Ideas for other places that might fit our criteria.\n\nI’m also interested in a more general location-optimizing discussion. What are your general thoughts on where you’d like to live, and have they changed any since the hub conversations Claire began in [September](https://www.lesswrong.com/posts/FghubkDy6Dp6mnxk7/the-rationalist-community-s-location-problem) and [November](https://www.lesswrong.com/posts/i3FyT3Gvst8pFX7gE/location-discussion-takeaways)? If a new rationality community hub sprang up at any of these locations, would you be tempted to join? Is there a different place you’d prefer (either personally, or for the community)?\n\nAnything from 'statements of personal preferences' to 'models of how the rationality community might make humanity's future much more awesome' is welcome in the comments.\n\nWhat kind of place we're looking for\n------------------------------------\n\nOur priority is to find a place where we think researchers will be able to *think* unusually clearly and well, in line with our [December update](https://intelligence.org/2020/12/21/2020-updates-and-strategy/#3). Recently, we’ve been looking for a campus or proto-campus (one or more buildings, with space and legal ability to build more) that's:\n\n*   far enough away from urban areas to be maximally calm, quiet, and close to nature; and\n*   near enough to urban areas to ensure there are people to see and things to do in driving distance, and to provide access to city conveniences (food delivery, ridesharing, lots of restaurants, etc.).\n\nThese proto-campus-type properties seem to be very rare and hard to find. E.g., we heard good things about Madison, WI and spent time looking for a property in the area, but ended up finding zero candidates currently on the market (that weren't falling apart, etc.).\n\nIf you want to convince MIRI to move to your favorite city, one good route would be to find a property like this for sale and either email it [to Alex Vermeer](mailto:alex@intelligence.org) or PM me (please *don’t* post specific property options you’re recommending publicly). The best places we’ve found have often been at the outskirts of pleasant, walkable 50k-100k population cities or college towns.\n\nThe specific factors we've been looking at fall under three rough, overlapping categories:\n\n1\\. Is this a good place to think?\n\nThis is the biggest factor, and includes...\n\n*   Physical environment: How much do we expect this place to make it easy for researchers to go on peaceful walks to think, talk, etc.? The best options will tend to be secluded, close to nature, and safe-feeling. The worst options will tend to be busy and chaotic, or distracting/jarring for hard cognitive work.\n*   Social environment: What's the local vibe? If we absorb the local vibes, does this make us better or worse at things? Is there strong pressure in the direction of “it's silly/bad/crazy to try to save the world” or “elites aren't dropping the ball”? If the US, the world, or Twitter suddenly got a lot more socially chaotic, would the place we're in provide a feeling of safety, resilience, and nondistraction? (See also 2 and 3 below.)\n*   Usability and scalability: The property has good indoor work and hangout spaces, or spaces that could be easily converted into such in a reasonable timeframe: we want to be able to start doing research here in the near future, without spending 6+ months to get an initial setup in place. The property is also large enough—probably at least 20 acres, and ideally 50+ acres. It's easy to build on the land (and otherwise modify the property), or easy to expand over the years by buying properties within walking distance as they go on the market.\n\n2\\. Is this otherwise a good place for MIRI staff to live?\n\nIf we go with the proto-campus plan, we’re likely to start with most staff living in the city proper (possibly with a second office there). More folks may move to the campus as we build it out or acquire more property.\n\nWe’re likely to be happier if we have friends and colleagues, community spaces, etc. on or near the campus and in the city, though we don't have a settled view on what size or kind of local rationalist community would be ideal.\n\nRegardless of how many MIRI staff are living on campus (and although our biggest constraint is \"can we find a proto-campus for sale here at all?\"), features of the city and area will inevitably matter a lot.\n\n*   (Proto-)campus: Access to city conveniences (Uber, UberEats, etc.). The campus is a nice place to live, and close enough to other places staff can live that commutes don't have to be long.\n*   City: The city is nice to live in, walkable, fun, and safe. Housing is affordable. The area doesn't feel prone to (present or future) political violence, street fighting, riots, instability, etc. The culture is relaxed and LGBTQ-friendly.\n*   Area: Taxes aren't high, and aren’t likely to sharply rise in the future. It seems highly unlikely that the government (at any level) will be very anti-technologist six months or ten years from now. The weather isn’t too extreme. Etc.\n\n3\\. How good is this place socially (and how good could it become)?\n\nThis overlaps with 2, since the things that make a city nice for MIRI staff also affect whether other people will like it.\n\n*   We want to mesh well with people who already live here, so we can make new social connections, benefit from the intellectual exchange with people in the area, and recruit people to do research.\n*   We want our partners, friends, and colleagues to like it here and have strong job options in the area.\n*   We want it to be easy and attractive for friends, colleagues/collaborators on alignment research, and potential hires to visit, and for us to visit them. E.g., places that are close to the Bay and have a major airport will score well here.\n\nOur current top choices\n-----------------------\n\nBellingham and Peekskill\n\nWe’ve spent hundreds of hours searching through long lists of candidates, and have pared those down to 30 relatively promising parts of the US, including two that look quite good as campuses and three other areas we especially like (but haven’t found a property in).\n\nThe two campus options we like (that do the best job of satisfying the criteria I listed above) are near (or on the outskirts of) two cities:\n\n**Bellingham, Washington:** A 90,000-population town near the Canadian border. Located in between Seattle ([80–120 minutes south](https://www.google.com/maps/dir/Bellingham,+WA/Seattle,+WA/@48.4371661,-123.7717911,8z/data=!3m1!4b1!4m13!4m12!1m5!1m1!1s0x5485962ef2458717:0xd57a9ca9cd39e0f0!2m2!1d-122.4786854!2d48.7519112!1m5!1m1!1s0x5490102c93e83355:0x102565466944d59a!2m2!1d-122.3320708!2d47.6062095), depending on traffic) and Vancouver, Canada ([65–165 minutes north](https://www.google.com/maps/dir/Bellingham,+WA/Vancouver,+BC,+Canada/@49.0019076,-123.3699009,9z/data=!3m1!4b1!4m13!4m12!1m5!1m1!1s0x5485962ef2458717:0xd57a9ca9cd39e0f0!2m2!1d-122.4786854!2d48.7519112!1m5!1m1!1s0x548673f143a94fb3:0xbb9196ea9b81f38b!2m2!1d-123.1207375!2d49.2827291), depending on traffic and border crossing delays), with Puget Sound on one side and forests and a lake and distant mountains on the other. It seems enjoyably walkable near [downtown](https://www.youtube.com/watch?v=9Y4qfFTxlWI&list=PL8OWMsW1wx5yRcsgAYCXEpBOdpo61UYZc&index=24) (adjacent neighborhoods: [1](https://www.youtube.com/watch?v=_u0vjpYnl78&list=PL8OWMsW1wx5yRcsgAYCXEpBOdpo61UYZc&index=16), [2](https://www.youtube.com/watch?v=Ks8GXAhUnrU&list=PL8OWMsW1wx5yRcsgAYCXEpBOdpo61UYZc&index=16), [3](https://www.youtube.com/watch?v=fTuKDugTQlY&list=PL8OWMsW1wx5yRcsgAYCXEpBOdpo61UYZc&index=13)), it feels vibrant and youthful, it's pretty good on crime, and considering all those positives and being on the coast, housing is relatively available and affordable.\n\nIt's a bit cloudier than Seattle, which is among the cloudiest parts of the country (something like 35% of the possible sun hours are sunny, compared to 38% for Seattle and 52% for Ann Arbor; this % depends heavily on your source though). It’s especially cloudy and rainy during the winter, though usually the rain and clouds come and go from day to day, and in July and August it’s beautiful almost every day. It’s hardly ever cold and hardly ever hot.\n\nAlso: unlike a lot of places we’ve considered, very little distraction/misery from mosquitoes!\n\n(**Edit**: Removed [CDC mosquito map](https://www.cdc.gov/mosquitoes/mosquito-control/professionals/range.html) because it's only looking at two species of mosquito.)\n\nThe proto-campus we’re looking at is located in a peaceful, wet, quiet forest, full of walking trails. Our current gestalt impression of the city itself is that it’s full of hipsters and hobbyists, plus people who moved to the town for the [beautiful environs](https://www.youtube.com/watch?v=MMabkvaDWBc) and small-town aesthetic.\n\nFolks seem to come here for good food, breweries, nature, and maybe plays to go to, and to attend Bellingham's (unimpressive) medium-sized public university; or they come to escape things they dislike about the rest of the west coast. Hobbies are a very big thing here: sports, hiking, rock climbing, going to the mountains, kayaking on the sound, etc. There aren’t many jobs here except for the service industry—it’s a town of students, hip retirees, and people working remotely in Seattle or California.\n\nBellingham International Airport does *not* have international flights (?!) and is quite small, but does have direct flights to Oakland several times a week.\n\n**Peekskill, New York:** A small 24,000-person town on the Hudson River, [60–120 minutes by car](https://www.google.com/maps/dir/Peekskill,+NY/New+York+City,+NY/@40.9931787,-74.1815526,10z/data=!3m1!4b1!4m13!4m12!1m5!1m1!1s0x89c2b6ea3df98c41:0xe8e288f9b0fab8a1!2m2!1d-73.9204158!2d41.2900939!1m5!1m1!1s0x89c24fa5d33f083b:0xc80b8f06e177fe62!2m2!1d-74.0059728!2d40.7127753) or [65–75 minutes by train](https://www.google.com/maps/dir/Peekskill,+NY/New+York+City,+NY/@41.0016137,-74.2156049,10z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89c2b6ea3df98c41:0xe8e288f9b0fab8a1!2m2!1d-73.9204158!2d41.2900939!1m5!1m1!1s0x89c24fa5d33f083b:0xc80b8f06e177fe62!2m2!1d-74.0059728!2d40.7127753!3e3) from New York City. ([Video tour.](https://www.youtube.com/watch?v=2p_-qOmJ0dE)) Blake Borgeson shares his impressions of this option:\n\n> NYC is our big nearby city, and it seems like roughly the best big city. My picture here at the moment, which I mostly got from Zvi, is that the big way NYC is different from other big cities is it’s more like 10 big cities with 10 different cultures, all of them coexisting in the same place and something between ignoring and welcoming one another. NYC is a place where doing your own thing and bringing your own culture is totally welcome.\n> \n> Zvi seems enthusiastic about NYC culture’s effects on MIRI. (IIRC, he thinks that New Hampshire and Austin are probably the two *best* US locations for MIRI in terms of epistemic effects on us, and I currently mostly buy this claim.)\n> \n> Peekskill is highly diverse and functional-seeming and real-seeming in a way that seems connected to it being near NYC. The folks I’ve talked to who live near Peekskill feel like Vermonters, progressive rural-ish folks, and I like them so far. They treat the land the way I want it treated—like, the nature here is beautiful, we can all agree, and you want to showcase it and preserve it rather than golf course it or English manor it.\n> \n> Besides diversity and coexisting, the culture of this area feels simpler, more like some decades ago. Life is more straightforward. There’s no angst about finding your calling and finding your hobbies. You grow up, you find a job, you find love and have a family, you take care of your family, you make money to provide for them, and if you make enough money for it, congratulations, you can relax in your country estate with your family and enjoy that.\n> \n> Hobby-type things seem not quite to fit in around here. So compared to Bellingham, Peekskill has fewer restaurants that I love (though it has some), and fewer places to sample craft beer, and fewer ways for adults to do things like sports (35 minutes driving from Peekskill to the nearest adult soccer league I could find online, for instance).\n\nSome MIRI staff would live in Peekskill, while others live in more rural houses outside of town. The campus we’re looking at adjoins [big forested hills with trails winding throughout](https://www.youtube.com/watch?v=sw7mEniV0Rs), little used and with space to wander in nature for hours. A couple of miles on these trails takes you to the Appalachian Trail, if you want to go on longer hikes.\n\nThe area has real seasons, with deciduous trees that lose their leaves in the fall. Winter is much colder than in Bellingham, and summer is much hotter. (Bellingham’s weather is something like Berkeley’s with everything shifted 5-10°F colder.)\n\n![](https://lh3.googleusercontent.com/Qhq1MwhAYJ8cm6NE7DSjpAil452YUPkvSa6JyVJFMRb1Dnnw3pKuZp4aegy980Qv2hhTLsgKt43wZBzHF3PRsPM3IFbUVkJ6uLCjeVAva9-NR0gCt4cAHG2TptSBrta-TgewOGy_)\n\n![](https://lh3.googleusercontent.com/qjusM2cNsHwK2GSG4nEvxAugH9yhXp-TMAMYSFaYCSeXS8Kv8lAgEuN6e0pvqlz0l35Xn1yeOV3kkzPNQ70EvnlIAgp1_Znz5k_gN6_IsAz27hfu7cN1ZIzLspdDkZEi4Kx1BjzU)\n\n*Comparisons by* [*weatherspark.com*](https://weatherspark.com/compare/y/991~24825/Comparison-of-the-Average-Weather-in-Bellingham-and-Peekskill)*.*\n\nA big draw of the Peekskill area for us is that we’re currently a lot more interested in being near New York City than being near Seattle/Vancouver. We could imagine starting a chain of rationalist communities along the Hudson Valley train line, allowing the full range from “very rural” to “very urban” life for people with different tastes. The [train itself](https://moovitapp.com/index/en/public_transit-line-HUDSON-NYCNJ-121-427-183787-0) comes hourly, has plenty of seating (during non-COVID times, it seems you might have to stand for half the trip back from Grand Central at certain times of day), and is nice to ride (more like Caltrain, not the BART).\n\nThe campus options we’re looking at in the Peekskill area are also faster and easier to set up, requiring less construction. And there’s a small airport [35–60 minutes](https://www.google.com/maps/dir/Peekskill,+NY/New+York+Stewart+Int'l+Airport+(SWF),+1180+1st+St,+New+Windsor,+NY+12553/@41.4097531,-74.1520625,11z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89c2b6ea3df98c41:0xe8e288f9b0fab8a1!2m2!1d-73.9204158!2d41.2900939!1m5!1m1!1s0x89dd2c1483ed0f17:0x5cd554a560b695dd!2m2!1d-74.1008627!2d41.4984032!3e0) north, typically used to fly to Philadelphia International and then connect to another flight.\n\nOn the other hand, Bellingham is closer to our connections in the Bay Area, and is a much more exciting town than Peekskill in its own right: more restaurants and grocery stores, more climbing gyms, etc. NY state taxes are also higher.\n\nBoth Bellingham and Peekskill are quiet, safe, and relatively affordable (though not *stunningly* affordable—and as with most places, prices are on the rise):\n\n![](https://lh5.googleusercontent.com/-PJ7Egp6Vexa2Ory_Q4zYFMQQRH417qeqsPToaW1TxpSKQ4XPKV2o7TeHVeGdo1jnExSlrtvfz-KKA7_qJkGTk3M3EXdfYmwxo2jck-MBerF1ptsuY3am790Khn6tmMKhYVYTshY)\n\n*Data extracted from Zillow. This is not a \"sale price,\" but a Zillow proprietary \"home value\" number.*\n\nBoth locations are cheaper than the Bay, have fewer local amenities, have worse job markets, and have (what I'd expect most people to consider) worse weather than the Bay. Homes in Bellingham are roughly 0.4x the $/sqft in Berkeley.\n\nSome of the biggest questions we’d love answered about these areas are:\n\n*   Lyme disease: A potentially serious disadvantage of the Peekskill area, especially for outdoorsy or rural-life-enjoying rationalists, is that it’s [tick country](http://www.aldf.com/u-s-maps-and-statistics/). Information like the following could do a lot to influence how good the place looks to us:\n    *   How common is Lyme disease around Peekskill? (According to [this article](https://www.newyorkupstate.com/news/2018/05/half_of_adult_ticks_in_new_york_state_carry_lyme_disease_new_data_shows.html), the Hudson Valley both has the highest number of ticks in New York state and the highest number of Lyme-carrying ticks per tick. Estimating local prevalence of the disease in humans is very important but seems tricky, since many cases go undiagnosed.)\n    *   Lyme disease rates are rising fast in the US—[up 10x](https://www.bayarealyme.org/about-lyme/lyme-disease-facts-statistics/) in the last 30 years. Given that, how much should we expect incidence to increase in areas that are already hard-hit?\n    *   Are there relatively easy, effective, and politically feasible ways to reduce the number of deer ticks in the area? ([Examples.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3072871/))\n    *   How harmful, and common, are long-term symptoms from Lyme disease?\n    *   How easy is it to reduce risk of catching Lyme disease, or risk of long-term symptoms? How effective is it to avoid walks, hikes, picnics, shaded-woodsy-areas, etc.? How effective is it to check for ticks with such-and-such frequency, and such-and-such thoroughness or laziness? (The CDC [claims](https://www.cdc.gov/lyme/transmission/index.html), somewhat vaguely: “In most cases, the tick must be attached for 36 to 48 hours or more before the Lyme disease bacterium can be transmitted.”)\n    *   How costly is it in microLymes to have long hair, a beard, or lots of body hair?\n    *   The CDC claims that [Lyme is usually transmitted by nymphs](https://www.cdc.gov/lyme/transmission/index.html), because they’re tiny and hard to spot during a tick check. How big is this effect, and what are its implications?\n    *   Is there a weird trick that can basically eliminate the risk of tick bites while walking around? E.g., Alan Eaton [proposes](https://patch.com/new-hampshire/concord-nh/daily-tick-checks-must-says-unh-professor) “When outdoors for any length of time, tuck your shirt into your pants and your pants into your socks. Wear tall rubber boots that are too smooth for most ticks to grab onto. Apply insect repellent and/or wear insect \\[repellent\\] clothing.” How effective are these measures?\n    *   How effective is it to monitor for symptoms of Lyme, and administer antibiotics early in response to certain triggers?\n    *   How soon will we have [Lyme vaccines](https://www.pfizer.com/news/press-release/press-release-detail/valneva-and-pfizer-announce-initiation-phase-2-study-lyme) of such-and-such effectiveness? Is there a faster option, like [bringing back LYMErix](https://www.wbur.org/commonhealth/2017/07/21/no-lyme-vaccine-humans-dogs) or just [using the veterinary Lyme vaccine](https://www.motherjones.com/politics/2017/09/dogs-can-be-vaccinated-against-lyme-disease-humans-cant/)? How likely is it that the vaccines (or some other intervention) will drastically reduce the severity or frequency of long-term Lyme symptoms in people already suffering from them?\n*   Uber access in non-plague years:\n    *   Regarding Bellingham, Blake reports: “At the moment there's not much Uber service—maybe 1/4 of the time during normal hours I can find an Uber there. I am pretty confident (85%?) that this is a COVID effect, with 75% of the students not being around, and that at least during school sessions there will be Ubers, but I still need to check this. I'm less confident (60%?) that Ubers will be readily available after COVID when school isn't in session.”\n    *   Regarding Peekskill: “Right now there often aren’t Ubers around. A Peekskill local said there were lots of Ubers before COVID, and especially on Fri/Sat evenings when things are busier. “\n    *   It would be *very* useful to know exactly how easy it is to get ride-shares in these areas during non-COVID times.\n*   How many rationalists might want to live here?\n\nOther candidates\n\nWe also feel some pull to strongly consider the following places more, despite so far not finding properties that fit our campus vision very well there:\n\n*   the parts of **New Hampshire** nearest Boston;\n*   the **Austin, Texas** area; and\n*   **Reno, Nevada.**\n\nI’d currently assign about 50% probability to “we move to the Peekskill or Bellingham area, or somewhere similar, on the strength of properties available there,” and 50% to “we largely give up on the proto-campus idea and move to someplace more like an office building or set of buildings in a city like Reno, not nature-y or lush but still a pleasant, relaxed place for going on walks to think.”\n\nWe’re also still open to suggestions of cities and properties, at least for the next two weeks (and possibly longer). Here are a bunch of other places we’ve seriously considered and in some cases visited:\n\n*   **West Coast:** Bend, OR; Eugene, OR; Issaquah, WA; North Bend, WA\n*   **Rocky Mountain:** Boulder, CO; Fort Collins, CO; Bozeman, MT; Missoula, MT\n*   **Southwest:** —\n*   **Great Plains:** —\n*   **Midwest:** Urbana-Champaign, IL; Bloomington, IN; West Lafayette, IN; Ann Arbor, MI; Madison, WI\n*   **South:** Asheville, NC; Greensboro, NC; Blacksburg, VA\n*   **Mid-Atlantic:** Rochester, NY; the Philadelphia, PA area (e.g., Lambertville, NJ)\n*   **New England:** the Amherst/Springfield, MA area; the outskirts of the Boston metropolitan area (e.g., Norwood, MA); Portland, ME; Keene, NH; Portsmouth, NH; Burlington, VT\n\n![](https://lh6.googleusercontent.com/FkDqGvphOSSXgFtbCOx8bJIB_-qwMJ1t_utBFm3B_U0DG1jW1GyCVF5B4sIRnvHYznoLFEfRiD-OGjjPqn8TGjsbKORAvY9P8db14H8ZrY28rph4Y4plsJPhTsjL4Ri673bO5SMX)\n\n*Berkeley, CA and our top 30 candidates, with the top 5 marked* ❤️*: Bellingham, Reno, Austin, Peekskill, and southern NH.*\n\nAlthough we’ve been focusing heavily on the US in our search, we’re also still interested in *country* suggestions, if you think Canada or some other part of the world scores especially well on metrics like “unlikely to see a future tech backlash or eat-the-rich revolt that makes it decreasingly attractive to live in.” From our perspective, a higher-but-stable tax rate is better than a lower present rate with a lot of future instability and unpredictability.\n\n* * *\n\nAs I said at the start of the post, I want to hear people's thoughts about the locations I mentioned (especially our five favorites), and arguments for other locations. Either 'is this a good place for MIRI?' or 'is this a good place for a rationalist hub?' / 'are there worlds where I'd be happy moving here if a hub *does* spring up?'\n\nI'm also looking for recommendations of specific properties to buy. We've found that criteria like the ones I listed narrow the candidate list a *lot*, so we may end up giving up on the campus dream. (E.g., if you search for parts of the US that are quiet and close to nature, have Uber/Lyft access, are walkable, aren't extremely conservative, aren't too low-population, are permissive about zoning, have low crime rates, don't have super-high elevation, don't have super-high heat, and have a campus-like property on the market, you end up with a very short list.)\n\n* * *\n\n*Thanks to Blake Borgeson and Alex Vermeer for reviewing this post, and for doing most of the legwork to help MIRI think through move tradeoffs and logistical details. Any remaining errors are probably theirs, since they did most of the hard cognitive work and I just wrote a post about it.*",
      "plaintextDescription": "Added June 28: Blake Borgeson comments below,\n\n> Update: After a lot more discussion and thinking things over, MIRI has decided against relocating away from the Bay Area, unless a new impetus comes along to move some time in the future.\n> \n> Mainly, ongoing/propagating strategic updates are behind this decision. We're more uncertain about our strategy overall following our 2020 strategy update (https://intelligence.org/2020/12/21/2020-updates-and-strategy/#3), so we’d rather not pay a big up-front move cost based on a prediction about what we expect MIRI to look like 2 or 5 years from now. Related to the same strategic updates, over the next 12 months or so we plan to experiment with placing less of an emphasis on staff working from our Berkeley base, and expect some staff to live and work from other places. \n> \n> Separately, in the background, considerations against our recent favorite places became stronger:\n> \n>  * Bellingham is far away from big cities, and it seemed increasingly like MIRI would be more isolated there than we wanted. Also, we became more concerned about low light levels during winter months.\n>  * The area around Peekskill has some of the highest Lyme disease incidence in the US, and it seems possible this is a high enough risk of disability to cause us to want to avoid it. We were still investigating this when we ended our location search.\n> \n> Who am I? I’m a MIRI board member, and was in recent months leading/coordinating this MIRI relocation decision process, which is now complete. \n> \n> Thanks for all the feedback in these comments, in emails, and on Facebook. I (and probably others) read all of it I could find, and it was important input for us.\n\n \n\n----------------------------------------\n\nOriginal post:\n\n \n\nMIRI is moving (with high probability)!\n\nWe haven’t finalized a location yet, but there’s a good chance we’ll make our decision in the next six weeks. I want to solicit:\n\n * Feedback on our current top location candidates.\n * Ideas for",
      "wordCount": 3596
    },
    "tags": [
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "Zz3HWyByyKF64Sfns",
        "name": "The SF Bay Area",
        "slug": "the-sf-bay-area"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "rQaDYb5bvfQouNfQW",
    "title": "Scott Alexander 2021 predictions: calibration and updating exercise",
    "slug": "scott-alexander-2021-predictions-calibration-and-updating",
    "url": null,
    "baseScore": 19,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2021-04-29T19:15:39.463Z",
    "contents": {
      "markdown": "Scott Alexander makes yearly predictions on SSC/ACX. This year, I avoided looking at his [blog post](https://astralcodexten.substack.com/p/mantic-monday-predictions-for-2021) so I could make my own unanchored predictions about the same topics, using [a copy of the post with Scott's probabilities removed](https://docs.google.com/document/d/1xiEOvkjvKUnbOA4hWZ18zvgOz6pn3Cz24uXSvS3g5Vk/edit). Then I looked at his and others' numbers, to see how much they update me.\n\nFor each question...\n\n*   Bullet #1 is my initial Apr. 27 guess, based on about 20-120 seconds of googling and/or thinking.\n*   Bullet #2 is my revised prediction after talking to my boyfriend and seeing their unanchored predictions.\n*   Bullet #3 is my revised prediction after seeing Scott's probabilities (**SA**).\n*   Bullet #4 is my revised Apr. 29 prediction after seeing [Zvi Mowshowitz's arguments and probabilities](https://www.lesswrong.com/posts/Cpy23aBzx3hnjKbnt/scott-alexander-2021-predictions-buy-sell-hold) (**ZM**) and [market prices collected and interpreted by SimonM](https://www.lesswrong.com/posts/sAmkpdmfDMCohXiBX/scott-alexander-2021-predictions-market-prices) (**MS**).\n\nMy numbers were influenced a lot by the fact that I expect to round to the nearest 10% (or to 1% / 5% / 95% / 99% at the extremes) to check calibration.\n\nUS/WORLD\n--------\n\n**1\\. Biden approval rating (as per 538) is greater than 50%:** \\[SA 80, ZM 80, MS 61\\]\n\n*   *Initial*: 73%\n*   *Post-Boyfriend*: 73%\n*   *Post-Scott*: 75%\n*   *Final*: 75%\n\n**2\\. Court packing is clearly going to happen (new justices don't have to be appointed by end of year):** \\[SA 5, ZM 5, MS ~4-5+\\]\n\n*   5%\n*   7%\n*   4%\n*   4%\n\n**3\\. Yang is New York mayor:** \\[SA 80, ZM 70, SM 69-77\\]\n\n*   85%\n*   85%\n*   82%\n*   74%\n\n**4\\. Newsom recalled as CA governor:** \\[SA 5, ZM ?, MS ~7\\]\n\n*   4%\n*   6%\n*   4%\n*   10%\n\n**5\\. At least $250 million in damage from BLM protests this year:** \\[SA 30, ZM ≤20\\]\n\n*   45%\n*   45%\n*   30%\n*   19%\n\n**6\\. Significant capital gains tax hike (above 30% for highest bracket):** \\[SA 20, ZM 20\\]\n\n*   40%\n*   40%\n*   20%\n*   23%\n\n**7\\. Trump is allowed back on Twitter:** \\[SA 20, ZM 10\\]\n\n*   7%\n*   8%\n*   13%\n*   12%\n\n**8\\. Tokyo Olympics happen on schedule:** \\[SA 70, ZM 80, MS 75\\]\n\n*   66%\n*   66%\n*   66%\n*   74%\n\n**9\\. Major flare-up (significantly worse than anything in past 5 years) in Russia/Ukraine war:** \\[SA 20, ZM ≤15, MS ~16\\]\n\n*   6%\n*   6%\n*   20%\n*   14%\n\n**10\\. Major flare-up (significantly worse than anything in past 10 years) in Israel/Palestine conflict:** \\[SA 5, ZM 5-~15\\]\n\n*   3%\n*   3%\n*   3%\n*   11%\n\n**11\\. Major flare-up (significantly worse than anything in past 50 years) in China/Taiwan conflict:** \\[SA 5, ZM 3\\]\n\n*   2%\n*   2%\n*   2%\n*   2%\n\n**12\\. Netanyahu is still Israeli PM:** \\[SA 40, ZM 30, MS 22\\]\n\n*   86%\n*   86%\n*   40%\n*   35%\n\n**13\\. Prospera has at least 1000 residents:** \\[SA 30, ZM 30\\]\n\n*   13%\n*   15%\n*   24%\n*   21%\n\nECON/TECH\n---------\n\n**14\\. Gamestop stock price still above $100:** \\[SA 50, ZM 50, MS 60\\]\n\n*   39%\n*   39%\n*   50%\n*   59%\n\n**15\\. Bitcoin above 100K:** \\[SA 40, ZM 25, MS 25\\]\n\n*   20%\n*   20%\n*   36%\n*   18%\n\n**16\\. Ethereum above 5K:** \\[SA 50, ZM 30, MS 11\\]\n\n*   20%\n*   20%\n*   40%\n*   19%\n\n**17\\. Ethereum above 0.05 BTC:** \\[SA 70, ZM 55, MS ~33\\]\n\n*   50%\n*   50%\n*   65%\n*   50%\n\n**18\\. Dow above 35K:** \\[SA 90, ZM<<90, MS 50\\]\n\n*   77%\n*   77%\n*   90%\n*   50%\n\n**19\\. ...above 37.5K:** \\[SA 70, ZM ?, MS 20\\]\n\n*   48%\n*   48%\n*   70%\n*   20%\n\n**20\\. Unemployment above 5%:** \\[SA 40, ZM 50, MS 37\\]\n\n*   74%\n*   74%\n*   55%\n*   55%\n\n**21\\. Google widely allows remote work, no questions asked:** \\[SA 20, ZM 30\\]\n\n*   18%\n*   20%\n*   20%\n*   23%\n\n**22\\. Starship reaches orbit:** \\[SA 60, ZM 60, MS 50\\]\n\n*   42%\n*   45%\n*   60%\n*   57%\n\nCOVID\n-----\n\n**23\\. Fewer than 10K daily average official COVID cases in US in December 2021:** \\[SA 30, ZM 70\\]\n\n*   48%\n*   48%\n*   32%\n*   52%\n\n**24\\. Fewer than 50K daily average COVID cases worldwide in December 2021:** \\[SA 1, ZM 1\\]\n\n*   8%\n*   8%\n*   0.8%\n*   3%\n\n**25\\. Greater than 66% of US population vaccinated against COVID:** \\[SA 50, ZM 60, MS ~77+\\]\n\n*   69%\n*   69%\n*   59%\n*   59%\n\n**26\\. India's official case count is higher than US:** \\[SA 50, ZM ≥80\\]\n\n*   70%\n*   70%\n*   55%\n*   75%\n\n**27\\. Vitamin D is** \\[not\\] **generally recognized (eg NICE, UpToDate) as effective COVID treatment:** \\[SA 70, ZM 50 (*but mistook SA as 30*), MS ~75-85\\]\n\n*   93%\n*   93%\n*   81%\n*   81%\n\n**28\\. Something else not currently used becomes first-line treatment for COVID:** \\[SA 40, ZM 25\\]\n\n*   29%\n*   36%\n*   40%\n*   40%\n\n**29\\. Some new variant not currently known is greater than 25% of cases:** \\[SA 50, ZM 60\\]\n\n*   35%\n*   42%\n*   44%\n*   52%\n\n**30\\. Some new variant where no existing vaccine is more than 50% effective:** \\[SA 40, ZM ≤25\\]\n\n*   8%\n*   10%\n*   27%\n*   17%\n\n**31\\. US approves AstraZeneca vaccine:** \\[SA 20, ZM 20, MS 37\\]\n\n*   68%\n*   65%\n*   20%\n*   20%\n\n**32\\. Most people I see in the local grocery store aren't wearing a mask:** \\[SA 60, ZM 75\\]\n\n*   53%\n*   53%\n*   60%\n*   69%\n\nCOMMUNITY\n---------\n\n**33\\. Major rationalist org leaves Bay Area:** \\[SA 60, ZM ?\\]\n\n*   91%\n*   91%\n*   89%\n*   89%\n\n**34-37. \\[redacted\\]** \\[\\]\n\n**38\\. No new residents at our housing cluster:** \\[SA 40, ZM 35\\]\n\n*   20%\n*   20%\n*   40%\n*   37%\n\n**39\\. No current residents leave our housing cluster:** \\[SA 60, ZM 65\\]\n\n*   48%\n*   48%\n*   60%\n*   56%\n\n**40-52. \\[redacted\\]** \\[\\]\n\n**53\\. At least seven days my house is orange or worse on PurpleAir.com because of fires:** \\[SA 80, ZM 80\\]\n\n*   53%\n*   55%\n*   80%\n*   80%\n\nPERSONAL\n--------\n\n**54-59. \\[redacted\\]** \\[\\]\n\n**60\\. There are no appraisal-related complications to the new house purchase:** \\[SA 50, ZM 60\\]\n\n*   30%\n*   40%\n*   50%\n*   60%\n\n**61\\. I live in the new house:** \\[SA 95, ZM 90\\]\n\n*   82%\n*   82%\n*   93%\n*   92%\n\n**62\\. I live in the top bedroom:** \\[SA 60, ZM 65\\]\n\n*   67%\n*   67%\n*   58%\n*   65%\n\n**63\\. I can hear / get annoyed by neighbor TV noise:** \\[SA 40, ZM 30\\]\n\n*   58%\n*   58%\n*   40%\n*   37%\n\n**64\\. I'm playing in a D&D campaign:** \\[SA 70, ZM 70\\]\n\n*   20%\n*   20%\n*   70%\n*   68%\n\n**65\\. I go on at least one international trip:** \\[SA 60, ZM 60\\]\n\n*   26%\n*   26%\n*   60%\n*   57%\n\n**66\\. I spend at least a month living somewhere other than the Bay:** \\[SA 50, ZM 50\\]\n\n*   17%\n*   17%\n*   50%\n*   50%\n\n**67\\. I continue my current exercise routine (and get through an entire cycle of it) in Q4 2021:** \\[SA 70, ZM 65\\]\n\n*   17%\n*   15%\n*   63%\n*   60%\n\n**68\\. I meditate at least 15 days in Q4 2021:** \\[SA 60, ZM 60\\]\n\n*   15%\n*   15%\n*   60%\n*   59%\n\n**69\\. I take oroxylum at least 5 times in Q4 2021:** \\[SA 40, ZM 40\\]\n\n*   20%\n*   20%\n*   38%\n*   38%\n\n**70\\. I take some substance I haven't discovered yet at least 5 times in Q4 2021 (testing exempted):** \\[SA 30, ZM 25\\]\n\n*   31%\n*   31%\n*   30%\n*   24%\n\n**71\\. I do at least six new biohacking experiments in the next eight months:** \\[SA 40, ZM 40\\]\n\n*   36%\n*   36%\n*   36%\n*   38%\n\n**72\\. \\[redacted\\]** \\[\\]\n\n**73\\. The Twitter account I check most frequently isn't one of the five I check frequently now:** \\[SA 20, ZM 15\\]\n\n*   48%\n*   48%\n*   20%\n*   24%\n\n**74\\. I make/retweet at least 25 tweets between now and 2022:** \\[SA 70, ZM 75\\]\n\n*   60%\n*   60%\n*   70%\n*   70%\n\nWORK\n----\n\n**75\\. Lorien has 100+ patients:** \\[SA 90, ZM 85\\]\n\n*   58%\n*   58%\n*   90%\n*   87%\n\n**76\\. 150+ patients:** \\[SA 20, ZM 25\\]\n\n*   45%\n*   45%\n*   20%\n*   21%\n\n**77\\. 200+ patients:** \\[SA 5, ZM 5-10\\]\n\n*   38%\n*   37%\n*   9%\n*   8%\n\n**78\\. I've written at least ten more Lorien writeups (so total at least 27):** \\[SA 30, ZM 30\\]\n\n*   43%\n*   43%\n*   24%\n*   24%\n\n**79-83. \\[redacted\\]** \\[\\]\n\n**84\\. I have switched medical records systems:** \\[SA 20, ZM 15\\]\n\n*   46%\n*   50%\n*   20%\n*   20%\n\n**85\\. I have changed my pricing scheme:** \\[SA 20, ZM 20\\]\n\n*   28%\n*   34%\n*   20%\n*   20%\n\nBLOG\n----\n\n**86\\. ACX is earning more money than it is right now:** \\[SA 70, ZM 80\\]\n\n*   65%\n*   65%\n*   70%\n*   72%\n\n**87-89. \\[redacted\\]** \\[\\]\n\n**90\\. There is another article primarily about SSC/ACX/me in a major news source:** \\[SA 10, ZM 25\\]\n\n*   75%\n*   71%\n*   23%\n*   23%\n\n**91\\. I subscribe to at least 5 new Substacks (so total of 8):** \\[SA 20, ZM 30\\]\n\n*   13%\n*   18%\n*   20%\n*   20%\n\n**92\\. I've read and reviewed** ***How Asia Works*****:** \\[SA 90, ZM 90\\]\n\n*   50%\n*   53%\n*   88%\n*   88%\n\n**93\\. I've read and reviewed** ***Nixonland*****:** \\[SA 70, ZM 70\\]\n\n*   45%\n*   56%\n*   65%\n*   66%\n\n**94\\. I've read and reviewed** ***Scout Mindset*****:** \\[SA 60, ZM 70\\]\n\n*   59%\n*   60%\n*   60%\n*   63%\n\n**95\\. I've read and reviewed at least two more dictator books:** \\[SA 50, ZM 45\\]\n\n*   42%\n*   42%\n*   42%\n*   42%\n\n**96\\. I've started and am at least 25% of the way through the formal editing process for** ***Unsong*****:** \\[SA 30, ZM 30\\]\n\n*   60%\n*   63%\n*   30%\n*   29%\n\n**97\\.** ***Unsong*** **is published:** \\[SA 10, ZM 10\\]\n\n*   15%\n*   42%\n*   14%\n*   14%\n\n**98\\. I've written at least five chapters of some non-*****Unsong*** **book I hope to publish:** \\[SA 40\\]\n\n*   41%\n*   39%\n*   40%\n*   40%\n\n**99\\. \\[redacted\\] wins the book review contest:** \\[SA 60, ZM 50\\]\n\n*   8%\n*   8%\n*   52%\n*   48%\n\n**100\\. I run an ACX reader survey:** \\[SA 50, ZM 50\\]\n\n*   55%\n*   53%\n*   48%\n*   48%\n\n**101\\. I run a normal ACX survey (must start, but not necessarily finish, before end of year):** \\[SA 90, ZM 90\\]\n\n*   65%\n*   65%\n*   86%\n*   84%\n\n**102\\. By end of year, some other post beats** ***NYT*** **commentary for my most popular post:** \\[SA 10, ZM 10\\]\n\n*   65%\n*   65%\n*   13%\n*   14%\n\n**103\\. I finish and post the culture wars essay I'm working on:** \\[SA 90, ZM 90\\]\n\n*   38%\n*   40%\n*   85%\n*   85%\n\n**104\\. I finish and post the climate change essay I'm working on:** \\[SA 80, ZM 80\\]\n\n*   46%\n*   47%\n*   80%\n*   80%\n\n**105\\. I finish and post the CO~2~ essay I'm working on:** \\[SA 80, ZM 80\\]\n\n*   49%\n*   52%\n*   80%\n*   80%\n\n**106\\. I have a queue of fewer than ten extra posts:** \\[SA 70, ZM 60\\]\n\n*   57%\n*   57%\n*   70%\n*   60%\n\nMETA\n----\n\n**107\\. I double my current amount of money ($1000) on PredictIt:** \\[SA 10, ZM 15\\]\n\n*   49%\n*   49%\n*   10%\n*   22%\n\n**108\\. I post my scores on these predictions before 3/1/22:** \\[SA 70, ZM 75\\]\n\n*   54%\n*   54%\n*   63%\n*   64%",
      "plaintextDescription": "Scott Alexander makes yearly predictions on SSC/ACX. This year, I avoided looking at his blog post so I could make my own unanchored predictions about the same topics, using a copy of the post with Scott's probabilities removed. Then I looked at his and others' numbers, to see how much they update me.\n\nFor each question...\n\n * Bullet #1 is my initial Apr. 27 guess, based on about 20-120 seconds of googling and/or thinking.\n * Bullet #2 is my revised prediction after talking to my boyfriend and seeing their unanchored predictions.\n * Bullet #3 is my revised prediction after seeing Scott's probabilities (SA).\n * Bullet #4 is my revised Apr. 29 prediction after seeing Zvi Mowshowitz's arguments and probabilities (ZM) and market prices collected and interpreted by SimonM (MS).\n\nMy numbers were influenced a lot by the fact that I expect to round to the nearest 10% (or to 1% / 5% / 95% / 99% at the extremes) to check calibration.\n\n \n\n\nUS/WORLD\n1. Biden approval rating (as per 538) is greater than 50%: [SA 80, ZM 80, MS 61]\n\n * Initial: 73%\n * Post-Boyfriend: 73%\n * Post-Scott: 75%\n * Final: 75%\n\n2. Court packing is clearly going to happen (new justices don't have to be appointed by end of year): [SA 5, ZM 5, MS ~4-5+]\n\n * 5%\n * 7%\n * 4%\n * 4%\n\n3. Yang is New York mayor: [SA 80, ZM 70, SM 69-77]\n\n * 85%\n * 85%\n * 82%\n * 74%\n\n4. Newsom recalled as CA governor: [SA 5, ZM ?, MS ~7]\n\n * 4%\n * 6%\n * 4%\n * 10%\n\n5. At least $250 million in damage from BLM protests this year: [SA 30, ZM ≤20]\n\n * 45%\n * 45%\n * 30%\n * 19%\n\n6. Significant capital gains tax hike (above 30% for highest bracket): [SA 20, ZM 20]\n\n * 40%\n * 40%\n * 20%\n * 23%\n\n7. Trump is allowed back on Twitter: [SA 20, ZM 10]\n\n * 7%\n * 8%\n * 13%\n * 12%\n\n8. Tokyo Olympics happen on schedule: [SA 70, ZM 80, MS 75]\n\n * 66%\n * 66%\n * 66%\n * 74%\n\n9. Major flare-up (significantly worse than anything in past 5 years) in Russia/Ukraine war: [SA 20, ZM ≤15, MS ~16]\n\n * 6%\n * 6%\n * 20%\n * 14%\n\n10. Major flare-up (significantly wor",
      "wordCount": 2112
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ReB7yoF22GuerNfhH",
    "title": "Thiel on secrets and indefiniteness",
    "slug": "thiel-on-secrets-and-indefiniteness",
    "url": null,
    "baseScore": 60,
    "voteCount": 24,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2021-04-20T21:59:35.792Z",
    "contents": {
      "markdown": "Some excerpts from Peter Thiel's 2014 book [*Zero to One*](https://www.amazon.com/Zero-One-Notes-Startups-Future/dp/0804139296) that I've repeatedly come back to over the years:\n\n> \\[...\\] Why has so much of our society come to believe that there are no hard secrets left? It might start with geography. There are no blank spaces left on the map anymore. If you grew up in the 18th century, there were still new places to go. After hearing tales of foreign adventure, you could become an explorer yourself. This was probably true up through the 19th and early 20th centuries; after that point photography from *National Geographic* showed every Westerner what even the most exotic, underexplored places on earth look like. Today, explorers are found mostly in history books and children’s tales. Parents don’t expect their kids to become explorers any more than they expect them to become pirates or sultans. Perhaps there are a few dozen uncontacted tribes somewhere deep in the Amazon, and we know there remains one last earthly frontier in the depths of the oceans. But the unknown seems less accessible than ever.\n> \n> Along with the natural fact that physical frontiers have receded, four social trends have conspired to root out belief in secrets. First is incrementalism. From an early age, we are taught that the right way to do things is to proceed one very small step at a time, day by day, grade by grade. If you overachieve and end up learning something that’s not on the test, you won’t receive credit for it. But in exchange for doing exactly what’s asked of you (and for doing it just a bit better than your peers), you’ll get an A. This process extends all the way up through the tenure track, which is why academics usually chase large numbers of trivial publications instead of new frontiers.\n> \n> Second is risk aversion. People are scared of secrets because they are scared of being wrong. By definition, a secret hasn’t been vetted by the mainstream. If your goal is to never make a mistake in your life, you shouldn’t look for secrets. The prospect of being lonely but right—dedicating your life to something that no one else believes in—is already hard. The prospect of being lonely and *wrong* can be unbearable.\n> \n> Third is complacency. Social elites have the most freedom and ability to explore new thinking, but they seem to believe in secrets the least. Why search for a new secret if you can comfortably collect rents on everything that has already been done? Every fall, the deans at top law schools and business schools welcome the incoming class with the same implicit message: \"You got into this elite institution. Your worries are over. You’re set for life.\" But that’s probably the kind of thing that’s true only if you don’t believe it.\n> \n> Fourth is \"flatness.\" As globalization advances, people perceive the world as one homogeneous, highly competitive marketplace: the world is \"flat.\" Given that assumption, anyone who might have had the ambition to look for a secret will first ask himself: if it were possible to discover something new, wouldn’t someone from the faceless global talent pool of smarter and more creative people have found it already? This voice of doubt can dissuade people from even starting to look for secrets in a world that seems too big a place for any individual to contribute something unique.\n> \n> There’s an optimistic way to describe the result of these trends: today, you can’t start a cult. Forty years ago, people were more open to the idea that not all knowledge was widely known. From the Communist Party to the Hare Krishnas, large numbers of people thought they could join some enlightened vanguard that would show them the Way. Very few people take unorthodox ideas seriously today, and the mainstream sees that as a sign of progress. We can be glad that there are fewer crazy cults now, yet that gain has come at great cost: we have given up our sense of wonder at secrets left to be discovered.\n\n> **The World According to Convention**\n> \n> How must you see the world if you don’t believe in secrets? You’d have to believe we’ve already solved all great questions. If today’s conventions are correct, we can afford to be smug and complacent: \"God’s in His heaven, All’s right with the world.\"\n> \n> For example, a world without secrets would enjoy a perfect understanding of justice. Every injustice necessarily involves a moral truth that very few people recognize early on: in a democratic society, a wrongful practice persists only when most people don’t perceive it to be unjust. At first, only a small minority of abolitionists knew that slavery was evil; that view has rightly become conventional, but it was still a secret in the early 19th century. To say that there are no secrets left today would mean that we live in a society with no hidden injustices.\n> \n> In economics, disbelief in secrets leads to faith in efficient markets. But the existence of financial bubbles shows that markets can have extraordinary inefficiencies. (And the more people believe in efficiency, the bigger the bubbles get.) In 1999, nobody wanted to believe that the internet was irrationally overvalued. The same was true of housing in 2005: Fed chairman Alan Greenspan had to acknowledge some \"signs of froth in local markets\" but stated that \"a bubble in home prices for the nation as a whole does not appear likely.\" The market reflected all knowable information and couldn’t be questioned. Then home prices fell across the country, and the financial crisis of 2008 wiped out trillions. The future turned out to hold many secrets that economists could not make vanish simply by ignoring them. \\[...\\]\n\nThiel argues that there are many discoverable (or discovered-but-not-widely-known) truths you can use to get an edge, make plans, and deliberately engineer a better future.\n\nSome general social facts Thiel cites to argue that people in the US are less interested in secrets than they were in, e.g., the 1950s:\n\n*   From ch. 1: We live in an age of globalization, rather than technology innovation—outside of information technology, we've seen a Great Stagnation in new ideas and innovations since the 1970s-80s. We speak as though prosperous nations are \"developed\" as opposed to \"developing,\" and focus on improving the world by spreading ideas that have already worked, rather than by coming up with radically new ideas.\n*   From ch. 2: As a result of the Dot-Com Bubble, even IT has become allergic to developing big new plans and ideas.\n*   From ch. 6: The US in the 1950s had \"definite optimism,\" whereas the present-day US has \"indefinite optimism\". We think things are going to get better, but we're skeptical that we can learn anything that will help us concretely plan or invent the better thing.\n*   From ch. 13: The 2005-2010 cleantech bubble again shows companies relying on incremental improvements, conventional and widely shared knowledge, and vague ungrounded optimism.\n\nIn an indefinite world, according to Thiel...\n\n> Process trumps substance: when people lack concrete plans to carry out, they use formal rules to assemble a portfolio of various options. This describes Americans today. In middle school, we’re encouraged to start hoarding \"extracurricular activities.\" In high school, ambitious students compete even harder to appear omnicompetent. By the time a student gets to college, he’s spent a decade curating a bewilderingly diverse résumé to prepare for a completely unknowable future. Come what may, he’s ready—for nothing in particular. \\[...\\]\n> \n> Instead of working for years to build a new product, indefinite optimists rearrange already-invented ones. Bankers make money by rearranging the capital structures of already existing companies. Lawyers resolve disputes over old things or help other people structure their affairs. And private equity investors and management consultants don’t start new businesses; they squeeze extra efficiency from old ones with incessant procedural optimizations. It’s no surprise that these fields all attract disproportionate numbers of high-achieving Ivy League optionality chasers; what could be a more appropriate reward for two decades of résumé-building than a seemingly elite, process-oriented career that promises to \"keep options open\"? \\[...\\]\n> \n> While a definitely optimistic future would need engineers to design underwater cities and settlements in space, an indefinitely optimistic future calls for more bankers and lawyers. Finance epitomizes indefinite thinking because it’s the only way to make money when you have no idea how to create wealth. If they don’t go to law school, bright college graduates head to Wall Street precisely because they have no real plan for their careers. And once they arrive at Goldman, they find that even inside finance, everything is indefinite. It’s still optimistic—you wouldn’t play in the markets if you expected to lose—but the fundamental tenet is that the market is random; you can’t know anything specific or substantive; diversification becomes supremely important.\n> \n> **Indefinite Finance**\n> \n> The indefiniteness of finance can be bizarre. Think about what happens when successful entrepreneurs sell their company. What do they do with the money? In a financialized world, it unfolds like this:\n> \n> *   The founders don’t know what to do with it, so they give it to a large bank.\n> *   The bankers don’t know what to do with it, so they diversify by spreading it across a portfolio of institutional investors.\n> *   Institutional investors don’t know what to do with their managed capital, so they diversify by amassing a portfolio of stocks.\n> *   Companies try to increase their share price by generating free cash flows. If they do, they issue dividends or buy back shares and the cycle repeats.\n> \n> At no point does anyone in the chain know what to do with money in the real economy. But in an indefinite world, people actually prefer unlimited optionality; money is more valuable than anything you could possibly do with it. Only in a definite future is money a means to an end, not the end itself.\n> \n> **Indefinite Politics**\n> \n> Politicians have always been officially accountable to the public at election time, but today they are attuned to what the public thinks at every moment. Modern polling enables politicians to tailor their image to match preexisting public opinion exactly, so for the most part, they do. Nate Silver’s election predictions are remarkably accurate, but even more remarkable is how big a story they become every four years. We are more fascinated today by statistical predictions of what the country will be thinking in a few weeks’ time than by visionary predictions of what the country will look like 10 or 20 years from now.\n> \n> And it’s not just the electoral process—the very character of government has become indefinite, too. The government used to be able to coordinate complex solutions to problems like atomic weaponry and lunar exploration. But today, after 40 years of indefinite creep, the government mainly just provides insurance; our solutions to big problems are Medicare, Social Security, and a dizzying array of other transfer payment programs. It’s no surprise that entitlement spending has eclipsed discretionary spending every year since 1975. To increase discretionary spending we’d need definite plans to solve specific problems. But according to the indefinite logic of entitlement spending, we can make things better just by sending out more checks. \\[...\\]\n> \n> **Indefinite Philosophy**\n> \n> From Herbert Spencer on the right and Hegel in the center to Marx on the left, the 19th century shared a belief in progress. (Remember Marx and Engels’s encomium to the technological triumphs of capitalism from this page.) These thinkers expected material advances to fundamentally change human life for the better: they were definite optimists.\n> \n> In the late 20th century, indefinite philosophies came to the fore. The two dominant political thinkers, John Rawls and Robert Nozick, are usually seen as stark opposites: on the egalitarian left, Rawls was concerned with questions of fairness and distribution; on the libertarian right, Nozick focused on maximizing individual freedom. They both believed that people could get along with each other peacefully, so unlike the ancients, they were optimistic. But unlike Spencer or Marx, Rawls and Nozick were indefinite optimists: they didn’t have any specific vision of the future. \\[...\\]\n> \n> Today, we exaggerate the differences between left-liberal egalitarianism and libertarian individualism because almost everyone shares their common indefinite attitude. In philosophy, politics, and business, too, arguing over process has become a way to endlessly defer making concrete plans for a better future.\n> \n> **Indefinite Life**\n> \n> Our ancestors sought to understand and extend the human lifespan. In the 16th century, conquistadors searched the jungles of Florida for a Fountain of Youth. Francis Bacon wrote that “the prolongation of life” should be considered its own branch of medicine—and the noblest. In the 1660s, Robert Boyle placed life extension (along with \"the Recovery of Youth\") atop his famous wish list for the future of science. Whether through geographic exploration or laboratory research, the best minds of the Renaissance thought of death as something to defeat. (Some resisters were killed in action: Bacon caught pneumonia and died in 1626 while experimenting to see if he could extend a chicken’s life by freezing it in the snow.)\n> \n> We haven’t yet uncovered the secrets of life, but insurers and statisticians in the 19th century successfully revealed a secret about death that still governs our thinking today: they discovered how to reduce it to a mathematical probability. \"Life tables\" tell us our chances of dying in any given year, something previous generations didn’t know. However, in exchange for better insurance contracts, we seem to have given up the search for secrets about longevity. Systematic knowledge of the current range of human lifespans has made that range seem natural. Today our society is permeated by the twin ideas that death is both inevitable and random.\n> \n> Meanwhile, probabilistic attitudes have come to shape the agenda of biology itself. In 1928, Scottish scientist Alexander Fleming found that a mysterious antibacterial fungus had grown on a petri dish he’d forgotten to cover in his laboratory: he discovered penicillin by accident. Scientists have sought to harness the power of chance ever since. Modern drug discovery aims to amplify Fleming’s serendipitous circumstances a millionfold: pharmaceutical companies search through combinations of molecular compounds at random, hoping to find a hit.\n> \n> But it’s not working as well as it used to. Despite dramatic advances over the past two centuries, in recent decades biotechnology hasn’t met the expectations of investors—or patients. Eroom’s law—that’s Moore’s law backward—observes that the number of new drugs approved per billion dollars spent on R&D has halved every nine years since 1950. \\[...\\]\n> \n> Biotech startups are an extreme example of indefinite thinking. Researchers experiment with things that just might work instead of refining definite theories about how the body’s systems operate. Biologists say they need to work this way because the underlying biology is hard. According to them, IT startups work because we created computers ourselves and designed them to reliably obey our commands. Biotech is difficult because we didn’t design our bodies, and the more we learn about them, the more complex they turn out to be.\n> \n> But today it’s possible to wonder whether the genuine difficulty of biology has become an excuse for biotech startups’ indefinite approach to business in general. Most of the people involved expect some things to work eventually, but few want to commit to a specific company with the level of intensity necessary for success. It starts with the professors who often become part-time consultants instead of full-time employees—even for the biotech startups that begin from their own research. Then everyone else imitates the professors’ indefinite attitude.\n\nOn Thiel's account, people don't believe in secrets, but they do believe in *mysteries* or *things we can't figure out today, though we might know them at some point in the indefinite future*. The indefinite optimism he's criticizing doesn't assume we're omniscient, but it assumes that there are relatively few cheat codes or exploits an individual can discover, especially in the domain of \"altering and predicting the long-term future\".\n\nNew discoveries spontaneously pop out of a slot machine, and then go straight to the textbook or the trash heap; and only the gullible will favor unpopular ideas over popular ones.",
      "plaintextDescription": "Some excerpts from Peter Thiel's 2014 book Zero to One that I've repeatedly come back to over the years:\n\n> [...] Why has so much of our society come to believe that there are no hard secrets left? It might start with geography. There are no blank spaces left on the map anymore. If you grew up in the 18th century, there were still new places to go. After hearing tales of foreign adventure, you could become an explorer yourself. This was probably true up through the 19th and early 20th centuries; after that point photography from National Geographic showed every Westerner what even the most exotic, underexplored places on earth look like. Today, explorers are found mostly in history books and children’s tales. Parents don’t expect their kids to become explorers any more than they expect them to become pirates or sultans. Perhaps there are a few dozen uncontacted tribes somewhere deep in the Amazon, and we know there remains one last earthly frontier in the depths of the oceans. But the unknown seems less accessible than ever.\n> \n> Along with the natural fact that physical frontiers have receded, four social trends have conspired to root out belief in secrets. First is incrementalism. From an early age, we are taught that the right way to do things is to proceed one very small step at a time, day by day, grade by grade. If you overachieve and end up learning something that’s not on the test, you won’t receive credit for it. But in exchange for doing exactly what’s asked of you (and for doing it just a bit better than your peers), you’ll get an A. This process extends all the way up through the tenure track, which is why academics usually chase large numbers of trivial publications instead of new frontiers.\n> \n> Second is risk aversion. People are scared of secrets because they are scared of being wrong. By definition, a secret hasn’t been vetted by the mainstream. If your goal is to never make a mistake in your life, you shouldn’t look for secrets. The prospect of bei",
      "wordCount": 2714
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "GthhzjHDjxrBH5jah",
    "title": "2012 Robin Hanson comment on “Intelligence Explosion: Evidence and Import”",
    "slug": "2012-robin-hanson-comment-on-intelligence-explosion-evidence",
    "url": null,
    "baseScore": 28,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2021-04-02T16:26:51.725Z",
    "contents": {
      "markdown": "The 2013 anthology [*Singularity Hypotheses: A Scientific and Philosophical Assessment*](https://link.springer.com/book/10.1007/978-3-642-32560-1) (edited by Eden, Moor, Soraker, and Steinhart) included short responses to some articles. The response that most stuck with me was Robin Hanson's reply to Luke Muehlhauser and Anna Salamon’s \"[Intelligence Explosion: Evidence and Import](https://intelligence.org/files/IE-EI.pdf).\"\n\nUnfortunately, when I try to [**link**](https://link.springer.com/chapter/10.1007/978-3-642-32560-1_2) to this reply, people don't tend to realize they can scroll down to find it. So I've copied the full thing here.\n\n(Note that I'm not endorsing Robin's argument, just sharing it as a possible puzzle piece for identifying cruxes of disagreement between people with Hansonian versus Yudkowskian intuitions about AGI.)\n\n* * *\n\n> Muehlhauser and Salamon \\[M&S\\] talk as if their concerns are particular to an unprecedented new situation: the imminent prospect of “artificial intelligence” (AI). But in fact their concerns depend little on how artificial will be our descendants, nor on how intelligence they will be. Rather, Muehlhauser and Salamon’s concerns follow from the general fact that accelerating rates of change increase intergenerational conflicts. Let me explain.\n> \n> Here are three very long term historical trends:\n> \n> 1.  Our total power and capacity has consistently increased. Long ago this enabled increasing population, and lately it also enables increasing individual income.\n> 2.  The rate of change in this capacity increase has also increased. This acceleration has been lumpy, concentrated in big transitions: from primates to humans to farmers to industry.\n> 3.  Our values, as expressed in words and deeds, have changed, and changed faster when capacity changed faster. Genes embodied many earlier changes, while culture embodies most today.\n> \n> Increasing rates of change, together with constant or increasing lifespans, generically imply that individual lifetimes now see more change in capacity and in values. This creates more scope for conflict, wherein older generations dislike the values of younger more-powerful generations with whom their lives overlap.\n> \n> As rates of change increase, these differences in capacity and values between overlapping generations increase. For example, Muehlhauser and Salamon fear that their lives might overlap with\n> \n> \\> \"\\[descendants\\] superior to us in manufacturing, harvesting resources, scientific discovery, social charisma, and strategic action, among other capacities. We would not be in a position to negotiate with them, for \\[we\\] could not offer anything of value \\[they\\] could not produce more effectively themselves. … This brings us to the central feature of \\[descendant\\] risk: Unless a \\[descendant\\] is specifically programmed to preserve what \\[we\\] value, it may destroy those valued structures (including \\[us\\]) incidentally.\"\n> \n> The quote actually used the words “humans”, “machines” and “AI”, and Muehlhauser and Salamon spend much of their chapter discussing the timing and likelihood of future AI. But those details are mostly irrelevant to the concerns expressed above. It doesn’t matter much if our descendants are machines or biological meat, or if their increased capacities come from intelligence or raw physical power. What matters is that descendants could have more capacity and differing values.\n> \n> Such intergenerational concerns are ancient, and in response parents have long sought to imprint their values onto their children, with modest success.\n> \n> Muehlhauser and Salamon find this approach completely unsatisfactory. They even seem wary of descendants who are cell-by-cell emulations of prior human brains, “brain-inspired AIs running on human-derived “spaghetti code”, or \\`opaque’ AI designs …produced by evolutionary algorithms.” Why? Because such descendants “may not have a clear \\`slot’ in which to specify desirable goals.”\n> \n> Instead Muehlhauser and Salamon prefer descendants that have “a transparent design with a clearly definable utility function,” and they want the world to slow down its progress in making more capable descendants, so that they can first “solve the problem of how to build \\[descendants\\] with a stable, desirable utility function.”\n> \n> If “political totalitarians” are central powers trying to prevent unwanted political change using thorough and detailed control of social institutions, then “value totalitarians” are central powers trying to prevent unwanted value change using thorough and detailed control of everything value-related. And like political totalitarians willing to sacrifice economic growth to maintain political control, value totalitarians want us to sacrifice capacity growth until they can be assured of total value control.\n> \n> While the basic problem of faster change increasing intergenerational conflict depends little on change being caused by AI, the feasibility of this value totalitarian solution does seem to require AI. In addition, it requires transparent-design AI to be an early and efficient form of AI. Furthermore, either all the teams designing AIs must agree to use good values, or the first successful team must use good values and then stop the progress of all other teams.\n> \n> Personally, I’m skeptical that this approach is even feasible, and if feasible, I’m wary of the concentration of power required to even attempt it. Yes we teach values to kids, but we are also often revolted by extreme brainwashing scenarios, of kids so committed to certain teachings that they can no longer question them. And we are rightly wary of the global control required to prevent any team from creating descendants who lack officially approved values.\n> \n> Even so, I must admit that value totalitarianism deserves to be among the range of responses considered to future intergenerational conflicts.",
      "plaintextDescription": "The 2013 anthology Singularity Hypotheses: A Scientific and Philosophical Assessment (edited by Eden, Moor, Soraker, and Steinhart) included short responses to some articles. The response that most stuck with me was Robin Hanson's reply to Luke Muehlhauser and Anna Salamon’s \"Intelligence Explosion: Evidence and Import.\"\n\nUnfortunately, when I try to link to this reply, people don't tend to realize they can scroll down to find it. So I've copied the full thing here.\n\n(Note that I'm not endorsing Robin's argument, just sharing it as a possible puzzle piece for identifying cruxes of disagreement between people with Hansonian versus Yudkowskian intuitions about AGI.)\n\n----------------------------------------\n\n> Muehlhauser and Salamon [M&S] talk as if their concerns are particular to an unprecedented new situation: the imminent prospect of “artificial intelligence” (AI). But in fact their concerns depend little on how artificial will be our descendants, nor on how intelligence they will be. Rather, Muehlhauser and Salamon’s concerns follow from the general fact that accelerating rates of change increase intergenerational conflicts. Let me explain.\n> \n> Here are three very long term historical trends:\n> \n>  1. Our total power and capacity has consistently increased. Long ago this enabled increasing population, and lately it also enables increasing individual income.\n>  2. The rate of change in this capacity increase has also increased. This acceleration has been lumpy, concentrated in big transitions: from primates to humans to farmers to industry.\n>  3. Our values, as expressed in words and deeds, have changed, and changed faster when capacity changed faster. Genes embodied many earlier changes, while culture embodies most today.\n> \n> Increasing rates of change, together with constant or increasing lifespans, generically imply that individual lifetimes now see more change in capacity and in values. This creates more scope for conflict, wherein older generations dislike",
      "wordCount": 862
    },
    "tags": [
      {
        "_id": "4CQy8rim8PGt4sfCn",
        "name": "Complexity of value",
        "slug": "complexity-of-value"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RrCjPMKi8fpfGZ2cu",
    "title": "Logan Strohl on exercise norms",
    "slug": "logan-strohl-on-exercise-norms",
    "url": null,
    "baseScore": 64,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 30,
    "createdAt": null,
    "postedAt": "2021-03-30T04:28:22.331Z",
    "contents": {
      "markdown": "(cw: exercise, shame)\n\n(also relevant: [Shame Processing](http://agentyduck.blogspot.com/2019/12/shame-processing_29.html))\n\nLogan Strohl said some things about exercise (and shame, and [aspiring to be better](https://www.lesswrong.com/posts/DoLQN5ryZ9XkZjq5h/tsuyoku-naritai-i-want-to-become-stronger)) on Facebook today that I found interesting. Minus some mostly-irrelevant parts:\n\n> \\[...\\] I definitely wasn't trying to kick people who struggle with exercise while they're down.\n> \n> But I guess when I make this claim about exercise out loud, I *am* trying to do something *sort* of similar to that, which is kind of the opposite of saying, \"oh don't worry, exercise doesn't matter anyway\". Exercise *does* matter.\n> \n> \\[...\\]\n> \n> Being difficult is part of the very nature of exercise; you don't get physically stronger or gain physical endurance unless you work hard enough to be uncomfortable, and you don't improve meaningfully in these ways unless you are uncomfortable over and over again frequently and consistently. So I think sentiments like \"it doesn't really matter, only do it if it's easy and you like it\" (which rarely pop up in those words but I think are nevertheless subtly pervasive in nerdier subcultures) are especially detrimental to exercise in particular. And that's why I do sometimes bother to promote exercise explicitly.\n> \n> \\[...\\]\n> \n> I mostly don't think people should be shamed for things, including not exercising. I think shame is a beautiful and powerful psychological process that probably ought to be treated as personal and intimate, much like recountings of first lovemakings. Trying to use it as a public tool to make people act how you want them to seems to break it.\n> \n> What I think is that there are much, much better reactions to recognizing the goodness of something than shutting down and feeling bad about yourself for not instantiating the good thing. Such as, for example, re-considering whether you are allocating your resources correctly. I suspect that one of the reasons people tend to reject \"X is better than Y\" style claims is because they aren't *quite* aware that there *is* a difference between \"someone thinks I'm worse than I could be\" and \"someone is trying to make me feel bad\". Which seems like a great big dumb obstacle to people getting better.\n> \n> I suspect that part of what's going on here in our differing perceptions is... two things.\n> \n> 1\\. in general, nerds struggle more with exercise than non-nerds, nerds get bullied by non-nerds as kids (largely for struggling more with exercise), and then as adults nerds form sub-cultures where they're more protected from the things that hurt them growing up.\n> \n> 2\\. if you're an athlete, it's pretty uncomfortable being in those adult nerd sub-cultures. the sub-cultures have developed strong immune systems against athletics, and athletes are not very welcome.\n> \n> so i find myself, an athletic nerd, right in the middle of this anti-jock immune system, and sometimes i just wanna shout \"look i get that you were hurt but can we please do this less self-deceptively and without deriding athletes???\"\n> \n> like for example i have a friend who does this jovial self-deprecating thing where he talks about my \"dexterity privilege\" whenever i do something that involves coordination or strength, or when he tries to do something like that and doesn't succeed much. it's mostly innocuous, it mostly doesn't bother me.\n> \n> but it's also at least a little annoying and frustrating. because yeah, i almost certainly do have a genetic predisposition to be coordinated and so forth; but also, i started gymnastics training when i was in preschool, and every single year since then i've practiced some combination of gymnastics, soccer, dance, yoga, martial arts, running, weight lifting, swimming, cycling, hiking, and a smattering of more niche activities that require and develop physical skill. it's not like i just woke one morning able to do backflips. i did not roll a natural twenty on \"core stability\". i've worked hard for it my entire life, and that's *important* to me.\n> \n> this one little thing my friend says is really not a big deal, but i do think the mindset it comes from probably *is* kind of a big deal, when that mindset is shared by an entire community. and i think something like that is true of my community.\n\nDuncan Sabien adds a description of Logan's view (which Logan endorses):\n\n> \\[...\\] \"Look, people who don't exercise may be amazing in many ways, but they're also just strictly worse on the exercise-axis, and possibly on related health or willpower or self-control axes, and we shouldn't NOT-notice that specific badness even if we want to make sure we contextualize it along with all the other possible goodnesses.\" \\[...\\]",
      "plaintextDescription": "(cw: exercise, shame)\n\n(also relevant: Shame Processing)\n\nLogan Strohl said some things about exercise (and shame, and aspiring to be better) on Facebook today that I found interesting. Minus some mostly-irrelevant parts:\n\n> [...] I definitely wasn't trying to kick people who struggle with exercise while they're down.\n> \n> But I guess when I make this claim about exercise out loud, I am trying to do something sort of similar to that, which is kind of the opposite of saying, \"oh don't worry, exercise doesn't matter anyway\". Exercise does matter.\n> \n> [...]\n> \n> Being difficult is part of the very nature of exercise; you don't get physically stronger or gain physical endurance unless you work hard enough to be uncomfortable, and you don't improve meaningfully in these ways unless you are uncomfortable over and over again frequently and consistently. So I think sentiments like \"it doesn't really matter, only do it if it's easy and you like it\" (which rarely pop up in those words but I think are nevertheless subtly pervasive in nerdier subcultures) are especially detrimental to exercise in particular. And that's why I do sometimes bother to promote exercise explicitly.\n> \n> [...]\n> \n> I mostly don't think people should be shamed for things, including not exercising. I think shame is a beautiful and powerful psychological process that probably ought to be treated as personal and intimate, much like recountings of first lovemakings. Trying to use it as a public tool to make people act how you want them to seems to break it.\n> \n> What I think is that there are much, much better reactions to recognizing the goodness of something than shutting down and feeling bad about yourself for not instantiating the good thing. Such as, for example, re-considering whether you are allocating your resources correctly. I suspect that one of the reasons people tend to reject \"X is better than Y\" style claims is because they aren't quite aware that there is a difference between \"someone thin",
      "wordCount": 764
    },
    "tags": [
      {
        "_id": "8nAXyYLu8eT72Hwuh",
        "name": "Exercise (Physical)",
        "slug": "exercise-physical"
      },
      {
        "_id": "5ueqn4r7N3WvaLfGy",
        "name": "Guilt & Shame",
        "slug": "guilt-and-shame"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb117",
        "name": "Tsuyoku Naritai",
        "slug": "tsuyoku-naritai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "dT2PxjZanKsenGuuj",
    "title": "Julia Galef and Matt Yglesias on bioethics and \"ethics expertise\"",
    "slug": "julia-galef-and-matt-yglesias-on-bioethics-and-ethics",
    "url": null,
    "baseScore": 45,
    "voteCount": 27,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2021-03-30T03:06:07.323Z",
    "contents": {
      "markdown": "I recently collected excerpts from [33 random bioethics papers from 2014–2020](https://www.lesswrong.com/posts/Wzjjynmp8gMmdX6dt/thirty-three-randomly-selected-bioethics-papers), plus [10 papers from 2000](https://www.lesswrong.com/posts/Wzjjynmp8gMmdX6dt/thirty-three-randomly-selected-bioethics-papers?commentId=zxkr5ymhTCdNxZCAp), in the hope of getting a better picture of what the field is like, how healthy it is, and what its social effects may be.\n\nOne of the conversations that prompted me to look into this was Julia Galef's Feb. 3, 2021 [interview of Matt Yglesias](http://rationallyspeakingpodcast.org/show/episode-251-the-case-for-one-billion-americans-more-matt-ygl.html). The exchange is interesting in its own right, so I've copied it below. The relevant discussion pivots from the Iraq War to COVID-19:\n\n* * *\n\n> **Julia:** Well, not to bring it back to metacognitive updates, but I was wondering whether there's an update in here about trusting expertise.\n> \n> Because with the mistakes that public health communicators have made -- in communicating whether masks work, and how big of a deal COVID was going to be -- there've been a series of debates this year about whether people should just “trust experts,” on a topic that they aren't an expert in. And a lot of people have pointed out that the failure of the public health establishment kind of undercuts that “trust the experts” policy.\n> \n> So do you think there's a similar thing that should be said about the Iraq War? Where the, not politicians, but the “experts” in the intelligence community really fucked up, so should that update us away from “Trust experts in general”?\n> \n> Or is it kind of an exception, for some reason? \n> \n> **Matt:** It's hard for me to know what the alternative to trusting experts is exactly --\n> \n> **Julia**: I mean, it's like having a higher prior on… If you're pretty confident that war is really bad, and should be avoided unless absolutely necessary, then if you have lower trust in the intelligence community, it should mean you should need a really, really compelling case, with strong evidence, before you go with their recommendation.\n> \n> **Matt:** Yeah, I think in both foreign policy and public health that… particularly thinking after the pandemic, which has shook me in a variety of ways, is: I feel like I need to be raising the bar for what is the actual subject matter, that the subject matter expert is an expert in?\n> \n> A lot of what we get is adjacent expertise. So somebody who studies viruses, and maybe knows a lot about the protein structure of viruses, will opine about masks, right? And you’ve got to ask yourself, \"*Do* they have subject matter expertise in this mask thing? Whose expertise do we need here?\"\n> \n> Because one thing that I think clearly came out of the whole masks controversy is that public health experts underrated how easy it would be to get cloth masks in everybody's hands. It didn't occur to them as a solution to the PPE shortages that we could just get everybody a cloth mask.\n> \n> And that's because they're not experts in textile manufacturing. And it's no shame on them for not being experts in textile manufacturing. But they were thinking about, “Will masks give people a false sense of security?” Which is a psychology question. They were thinking about “Can we substitute away from surgical masks?” Which is a textile supply chain question. They were thinking about “Well, what are the antiviral properties of cloth masks?” Which again is a textile question. That's not a public health -- it's obviously relevant to public health, but it's a material science question.\n> \n> They didn't have expertise in those areas, and were in fact just on a par with me, or anybody else, right? But they had the, sometimes, arrogance that comes with believing you're being asked about your area of expertise.\n> \n> And so I'm really trying… it's like, if somebody has published research on the immune response of vaccines, I don't think it's a great idea for me to question them on that, like what do I know? I can read, I can try to do work, but I don't know. They are experts.\n> \n> But what are you an expert in? Because there is so much social psychology, individual psychology, economics, right? This COVID pandemic is such a big problem, that I don't think anybody has actually done deep scholarly work in all the relevant areas. So it's like the hour of the foxes, right? Rather than the hedgehogs. Because it's such an interdisciplinary problem.\n> \n> **Julia:** Right. Yeah. Probably one of the worst offenders, in my opinion, was the debate over vaccine distribution, and who should get a vaccine first.\n> \n> **Matt:** Oh my god…\n> \n> **Julia:** You know where I’m going with this!\n> \n> And there were a number of bioethicists -- or just bioethicist fanboys, I guess -- who got really angry at anyone questioning the draft recommendations for vaccine distribution from the CDC, and said, \"Stay in your lane, you're not an expert.\"\n> \n> We're not even talking here about, like, virus transmissibility. We're just talking about, what is a fair and reasonable way to prioritize different people over each other?\n> \n> And it's horrifying to me that we can have some people who think they're the only ones qualified to have opinions about that. \n> \n> **Matt:** I would like to know more, this is on my to-do list…\n> \n> **Julia:** What do you mean?\n> \n> **Matt:** Like, what is the field of bioethics? I don't understand how that's a purported domain of expertise. Because I've clashed with bioethics Twitter, on both this vaccine distribution thing and on human challenge trials for vaccines.\n> \n> And honestly I say this as a… I was a philosophy major in college. The number of people who have tenure track jobs doing normative ethics in philosophy departments is tiny, because the world does not… It's an interesting subject matter, but it's not... The world needs a lot of engineers. The world doesn't need a lot of \"Experts in normative ethics.\" Because people have their own opinions.\n> \n> And, I don't know, the ethics experts just disagree about the big picture, obvious controversies. The trolley problem, et cetera. I was blown away, on the human challenge trials, that Christine Korsgaard, my former professor -- I think the leading Kantian deontological thinker -- she had her name on the 1Day Sooner challenge trials thing. And then there's these, I don't know who, being like, \"Well, that's not good ethics.\"\n> \n> And I'm like, \"Well, according to whom?\" Right? Obviously in consequentialist terms, it’s good ethics. I happened to know the top expert in Kantian ethics, she thinks that's a good idea. So, who the fuck are you?\n> \n> **Julia:** Even by their own standards of who is allowed to have opinions, they should be taking that seriously.\n> \n> **Matt:** Right. But then I feel weird about it. It's like, okay, she's not a high priest of Kantianism. People are allowed to disagree with her. But then it's like, \"What is this subject area?\"\n> \n> Whereas if somebody wants to tell me they have expertise about spike proteins? You absolutely do.\n> \n> I know, I guess, what a spike protein is, because we've talked about this enough on Twitter. But I've never used an electron microscope. I don't think I could correctly define a virus.\n> \n> There's a lot of stuff that experts know that's relevant to this pandemic, but then like… “I'm an expert in right and wrong”? That doesn't sound like a real thing to me.\n> \n> **Julia:** I agree.",
      "plaintextDescription": "I recently collected excerpts from 33 random bioethics papers from 2014–2020, plus 10 papers from 2000, in the hope of getting a better picture of what the field is like, how healthy it is, and what its social effects may be.\n\nOne of the conversations that prompted me to look into this was Julia Galef's Feb. 3, 2021 interview of Matt Yglesias. The exchange is interesting in its own right, so I've copied it below. The relevant discussion pivots from the Iraq War to COVID-19:\n\n----------------------------------------\n\n> Julia: Well, not to bring it back to metacognitive updates, but I was wondering whether there's an update in here about trusting expertise.\n> \n> Because with the mistakes that public health communicators have made -- in communicating whether masks work, and how big of a deal COVID was going to be -- there've been a series of debates this year about whether people should just “trust experts,” on a topic that they aren't an expert in. And a lot of people have pointed out that the failure of the public health establishment kind of undercuts that “trust the experts” policy.\n> \n> So do you think there's a similar thing that should be said about the Iraq War? Where the, not politicians, but the “experts” in the intelligence community really fucked up, so should that update us away from “Trust experts in general”?\n> \n> Or is it kind of an exception, for some reason? \n> \n> Matt: It's hard for me to know what the alternative to trusting experts is exactly --\n> \n> Julia: I mean, it's like having a higher prior on… If you're pretty confident that war is really bad, and should be avoided unless absolutely necessary, then if you have lower trust in the intelligence community, it should mean you should need a really, really compelling case, with strong evidence, before you go with their recommendation.\n> \n> Matt: Yeah, I think in both foreign policy and public health that… particularly thinking after the pandemic, which has shook me in a variety of ways, is: I feel ",
      "wordCount": 1241
    },
    "tags": [
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "x3zyEPFaJANB2BHmP",
        "name": "Expertise (topic)",
        "slug": "expertise-topic"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Wzjjynmp8gMmdX6dt",
    "title": "Thirty-three randomly selected bioethics papers",
    "slug": "thirty-three-randomly-selected-bioethics-papers",
    "url": null,
    "baseScore": 115,
    "voteCount": 56,
    "viewCount": null,
    "commentCount": 46,
    "createdAt": null,
    "postedAt": "2021-03-22T21:38:08.281Z",
    "contents": {
      "markdown": "Some scholarly fields are very healthy (e.g., physics). Some are fairly unhealthy (e.g., [evolutionary psychology](https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/), sad to say). Some are outright crazy (e.g., philosophy of religion).\n\nHow good or bad is bioethics? How rigorous and truth-tracking is it? How much social benefit or harm does it cause?\n\nLooking at lots of random examples is an under-used tool for making progress on this kind of question. It's fast, it avoids the perils of cherry-picking, and it doesn't require you to trust someone else's high-level summary of the field.\n\nI picked the two highest-impact-factor \"[medical ethics](https://utrgv.libguides.com/c.php?g=483176&p=3304511)\" journals that have the word \"bioethics\" in the title: *The American Journal of Bioethics* and *Bioethics*. Together, these two journals release about 500 reports, book reviews, etc. per year.\n\nI then picked a random article from 2014 through 2020 from each journal, plus extra random articles from the more cited journal (*The American Journal of Bioethics*): five from 2016, 2018, and 2020, and two from 2017 and 2019.\n\nFor each article, I quoted the abstract (if there is one), the first two paragraphs, and the final paragraph. These are provided below, sorted by year.\n\nObviously this breadth-first approach won't provide a full sense of the quality of argumentation in these papers; but it will hopefully provide a picture of what kinds of views tend to get argued for in the field, what those arguments tend to look like, what tends to get taken for granted, what tends to get ignored, and so on.\n\nI expect more accurate conclusions and faster consensus-building about bioethics if conversation is grounded in a common pool of examples (which can then be drilled down on to spot-check a particular paper's arguments, etc.).\n\n**Added**: aphyer summarized the articles in the comment section; I've put aphyer's summaries [**in an editable Google Doc**](https://docs.google.com/document/d/1tmrrTlWLxseSfudS7IWPMEFUWrLoSewFXOjj2ilwFPA/edit) to make it easier to improve on the summaries (and to encourage article-by-article critiques, in the comments below or in the Doc).\n\n* * *\n\n2014\n----\n\n### **1. **[**Against Fetishism About Egalitarianism and in Defense of Cautious Moral Bioenhancement**](https://www.tandfonline.com/doi/full/10.1080/15265161.2014.889248)\n\nIngmar Persson and Julian Savulescu. *The American Journal of Bioethics* 14(4). Open Peer Commentaries.\n\n> We respond to Sparrow’s (2014) criticisms of our support for a project of moral bioenhancement. We distinguish between a confident and cautious proposal, arguing that his objections only apply to a confident proposal, while we endorse a cautious proposal. We argue that the project of moral bioenhancement (1) need not be more compulsory than traditional moral education, which should supplement it; (2) is not dependent on “shonky” sociobiology or biologism that downplays the importance of traditional education and the role of cognitive processes; (3) is not committed to increasing inequality, but if judiciously applied would rather reduce it; and (4) would be desirable even if, as he believes, it leads to a democracy in which a moral elite has more political power. Sparrow fails to appreciate the extent of existing inequalities in moral as well as socioeconomic respects. He displays a fetish about egalitarianism and labors under a prescientific understanding of the relationship between the brain and moral behavior. We also argue that he confuses liberal neutrality with moral relativism, and fails to realize the extent to which all functioning societies have to be based on common core of norms and values.\n> \n> When Robert Sparrow criticizes our work on moral bioenhancement (see Persson and Savulescu 2012), he doesn’t bother to make precise the claims by us to which he objects. It’s important to distinguish two types of proposals about moral bioenhancement, a *confident* one and a *cautious* one. (This is a simplification; there is in fact a whole spectrum of possible views with varying degrees of confidence/cautiousness.) These types of proposals differ with respect to the following three issues.\n> \n> \\[...\\]\n> \n> We believe that judicious moral bioenhancement will reduce the inequality between people with respect to moral motivation, rather than exacerbating it. But let’s play along with Sparrow and hypothesize that it will create a morally enhanced elite and that this will result in an inegalitarian democracy “restricting participation in government to the morally enhanced” (25). He suggests that nonetheless our current commitment to egalitarianism “might provide reasons to resist bringing it about such that an inegalitarian politics should be justified” (42). This is strong stuff. Sparrow seems to saying that even if a morally elitist democracy would pursue the morally right politics—that would bring anthropogenic climate change to a halt, put an effective check on the use of weapons of mass destruction, and so on—we should still prevent it coming into existence, clinging to our current more “egalitarian” democracies that leave us teetering on the brink of disaster. This seems to us to be making a fetish out of current egalitarianism, which—considering the huge socioeconomic inequalities it tolerates both within democratic states and globally—is surely far from perfect.\n\n* * *\n\n### **2. **[**Why Publishing Pseudonymously Can Protect Academic Freedom**](https://onlinelibrary.wiley.com/doi/10.1111/bioe.12091)\n\nFrancesca Minerva. *Bioethics* 28(4). Symposium: 'Anonymised Publishing in Bioethics'.\n\n> ‘The point of philosophy is to start with something so simple as not to seem worth stating, and to end with something so paradoxical that no one will believe it’ Bertrand Russell, *The Philosophy of Logical Atomism*\n> \n> In ‘New Threats to Academic Freedom’ I suggested that new media may represent a threat to academic freedom.\\[1\\] In particular, I argued that academic freedom may be limited *ab ovo* if people fear that they will have to go through media storms when publishing papers that might be perceived as controversial.\n> \n> \\[...\\]\n> \n> I would prefer a world where people were free to ask questions and to attempt to give answers to those questions without being dragged into media frenzies, missing out on job opportunities (as happened to both me and Alberto Giubilini) and being threatened with physical harm. But as we do not live in such an ideal world, the best we can do in order to change the current society into a less violent and intolerant one is to keep asking questions, and to keep seeking answers.\n\n* * *\n\n2015\n----\n\n### **3. **[**A Philosophical Misunderstanding at the Basis of Opposition to Nudging**](https://www.tandfonline.com/doi/full/10.1080/15265161.2015.1074313)\n\nShlomo Cohen. *The American Journal of Bioethics* 15(10). Open Peer Commentaries.\n\n> Ploug and Helm's (2015) insightful article criticizes recent views—mainly those advanced by Saghai (2013) and me (Cohen 2013)—that allow \"nudging\" patients while obtaining their informed consent (IC). Although many of their arguments deserve discussion, this short commentary focuses on one philosophical point that I see as pivotal in determining perspectives in the debate. The authors indeed open their criticism of my approach by reference to this point: \"First and foremost, while tacitly assuming that the value of informed consent is derived from the protection of personal autonomy, Cohen's view fails to provide a coherent and plausible account of personal autonomy.\" Although seemingly trivial and overwhelmingly common, the diagnosis that one's conception of valid IC is determined by one's theoretical conception of personal autonomy is in my view wrong: The specific amount of autonomy often makes little difference.\\[1\\] If we derived the ethics of IC from the concept of autonomy, we would probably conclude with the authors that nudging is incompatible with respecting autonomy, since, conceptually, manipulation and respecting autonomy point in opposing directions. However, the idea that the ethics of IC can be determined by theories of autonomy is wrong. I must leave the more theoretical discussion for another occasion, and restrict myself to showing how this is exemplified in the authors' own account.\n> \n> The requirements for IC entailed by the authors' theoretical account of autonomy are very unspecific: We must provide \"adequate\" information; the patient needs to \"understand\" it; the patient cannot be \"unduly influenced\" while deliberating; these conditions are supposed to enable \"reasonable\" choice. The problem is that all these parameters are terribly underdetermined (\"not unduly influenced\" may be question-begging): Except for hard paternalism or outright deception, they can allow the entire spectrum of views under debate. I too can embrace these formulations—nothing in them rules out nudging. Why don't the authors provide more precise directives? They are not guilty of sloppy work; the deep point is rather that beyond general pronouncements, conceptions of autonomy cannot provide practical ethical guidelines generally or specifically for IC. One central reason for this is that true autonomy is an exceedingly superhuman possibility and therefore cannot reliably prescribe what providing the conditions for humanly possible autonomy requires morally (e.g., whether to avoid nudging entirely)—that must rather be determined by some different method.\n> \n> \\[...\\]\n> \n> In this short commentary I have argued that the guidelines for a valid practice of IC are not predominantly determined by theoretical conceptions of autonomy, that they are rather determined by relevant social conventions, and that those conventions allow room for some types of nudging. Once this philosophical point is clear, the question ceases to be whether nudging is permissible, but which nudging is.\n\n* * *\n\n### **4. **[**Why not Commercial Assistance for Suicide? On the Question of Argumentative Coherence of Endorsing Assisted Suicide**](https://onlinelibrary.wiley.com/doi/10.1111/bioe.12140)\n\nRoland Kripke. *Bioethics* 29(7). Article.\n\n> **Abstract:** Most people who endorse physician‐assisted suicide are against commercially assisted suicide – a suicide assisted by professional non‐medical providers against payment. The article questions if this position – endorsement of physician‐assisted suicide on the one hand and rejection of commercially assisted suicide on the other hand – is a coherent ethical position. To this end the article first discusses some obvious advantages of commercially assisted suicide and then scrutinizes six types of argument about whether they can justify the rejection of commercially assisted suicide while simultaneously endorsing physician‐assisted suicide. The conclusion is that they cannot provide this justification and that the mentioned position is not coherent. People who endorse physician‐assisted suicide have to endorse commercially assisted suicide as well, or they have to revise their endorsement of physician‐assisted suicide.\n> \n> For many years, there has been a fierce controversy regarding the question of the ethical evaluation of assisted suicide. Surprisingly, there is broad agreement on one point, namely the rejection of commercial assistance for suicide. ‘Commercial assistance for suicide’ means that professional non-medical providers assist people in the implementation of their suicidal intents in return for payment. Not only opponents of physician-assisted suicide (PAS) but also most of its proponents think that commercially assisted suicide (CAS) is immoral and should not be permitted – if they discuss this form of assisted dying at all.\\[1\\] For whilst the legitimacy or illegitimacy of PAS is the subject of intense discussion, the question of CAS leads a miserable existence within the international ethical discussion.\n> \n> However, the issue is in no way far-fetched. Firstly, in some countries at least, there is ethical and political discussion on this topic. In Germany, for example, not only have some cases of CAS come to public attention in recent years, but also several legislative initiatives to prohibit CAS were launched.\\[2\\] Secondly, the issue of CAS seems to be reasonable because of the well-known general tendency towards commercialization of various areas of life.\\[3\\] Thirdly, the issue is important due to theoretical reasons, namely as a touchstone for the coherence of the ethical position of the proponents of PAS.\n> \n> \\[...\\]\n> \n> If one does not want a society in which suicide and its support is normal and taken for granted like other services, and if one wants to adhere at the same time to the claim of coherence for their own ethical position, the only possibility is to reject PAS. Those who do not endorse CAS cannot endorse PAS, either.\n\n* * *\n\n2016\n----\n\n### **5. **[**One Exemption Too Many: The Case for Mandated CCHD Screening**](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1128750)\n\nJohn D. Lantos, Julie Caciki, and Jeremy R. Garrett. *The American Journal of Bioethics* 16(1). Guest Editorial.\n\n> For more than 50 years, most industrialized countries have mandated population screening of newborns for medical conditions that meet a few straightforward criteria (Wilson and Jungner [1968](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1128750#)). The condition must be an important health problem. The screening test should be accurate. There must be an effective treatment that is readily available to the people who have been screened. The cost of the program, including screening, diagnosis, and treatment, must be reasonable. When all these conditions are met, then the medical, legal, and ethical consensus has been that it is justifiable to mandate newborn screening. Screening tests that meet these criteria have been endorsed by the Institute of Medicine, the American Society of Human Genetics and the American College of Medical Genetics, an NIH Task Force on Genetic Testing, the American Academy of Pediatrics, and the President's Council on Bioethics (Committee on Assessing Genetic Risks [1994](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1128750#); Botkin et al. [2015](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1128750#); Holtzman 1997; American Academy of Pedriatrics Newborn Screening Task Force 2000; [President's Council on Bioethics 2008](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1128750#)). For all of these organizations, the ethical justification for screening is that it provides direct benefit to the newborn and that avoiding or delaying screening could cause harm to that newborn.\n> \n> In their target article in this issue, Hom and colleagues (2016) raise the question of whether parental religious beliefs should allow exemptions to mandated screening for critical congenital heart disease (CCHD). It is important to be clear that in making this argument, they are not addressing three of the important and thorny controversies that are the usual focus of debates about newborn screening. The three controversial issues are (1) screening to detect conditions for which there is no effective treatment; (2) the use of stored samples from newborn screening for research unrelated to the screening program; and (3) tests for conditions that do not manifest until adulthood and for which there are no beneficial interventions in childhood. All three of those situations are appropriately controversial for traditional newborn screenings. None of them, however, is relevant to the central issue addressed in the target article or to the type of screening that is done for CCHD.\n> \n> \\[...\\]\n> \n> We acknowledge that some CCHDs are not easily correctable and thus that parents can ethically refuse treatment for the most severe CCHDs. But at present, the screening tests do not distinguish the easily correctable ones from those that are not so easy to treat. It is appropriate, then, to mandate newborn screening, without exceptions for religious beliefs, for CCHDs.\n\n* * *\n\n### **6. **[**The Curious Case of the De-ICD: Negotiating the Dynamics of Autonomy and Paternalism in Complex Clinical Relationships**](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1187211)\n\nDaryl Pullman and Kathleen Hodgkinson. *The American Journal of Bioethics* 16(8). Article.\n\n> **Abstract:** This article discusses the response of our ethics consultation service to an exceptional request by a patient to have his implantable cardioverter defibrillator (ICD) removed. Despite assurances that the device had saved his life on at least two occasions, and cautions that without it he would almost certainly suffer a potentially lethal cardiac event within 2 years, the patient would not be swayed. Although the patient was judged to be competent, our protracted consultation process lasted more than 8 months as we consulted, argued with, and otherwise cajoled him to change his mind, all to no avail. Justifying our at times aggressive paternalistic intervention helped us to reflect on the nature of autonomy and the dynamics of the legal, moral, and personal relationships in the clinical decision-making process.\n> \n> Several years ago our ethics consultation service faced a particularly vexing and exceptional case that forced us to reconsider our ethical and professional responsibilities in the face of what appeared to be a legally permissible but ill-advised medical decision. We had to reexamine the relationship between the legal and the moral, and the role of our ethics consultation service in this process. In addition to revisiting the much-discussed question of the relationship between autonomy and justified paternalistic intervention, we had to reflect upon the nature of the relationships between various health care professionals, the patient, and his family. Additionally, we had to assess the roles and responsibilities of members of our ethics consultation group who resisted this patient’s legally permissible but ethically problematic request, and of the cardiologist who eventually facilitated it. Finally, we had to manage the tension between private interests and public goods when the decision in question involved the allocation of scarce resources in a publicly funded health care system.\n> \n> There is a vast and expanding literature on the concept of autonomy and the nature of justified paternalistic intervention. Perspectives vary from those that acknowledge the primacy and centrality of autonomy in biomedical decision making (Gillon 2003; Wolpe 1998) to those that present a somewhat more nuanced perspective (O’Neill 2002; Kukla 2005; Levy 2014). Our intent is not to review this literature in any systematic or robust fashion, but rather to draw upon various perspectives in assessing, critiquing, and in some respects justifying the manner in which we managed this complex and difficult case. Our emphasis is less on analytic precision and more on practical understanding as we continue to rely on these and related concepts in navigating the ambiguities of moral space.\n> \n> \\[...\\]\n> \n> Finally, we comment briefly on the implications of acceding to a patient’s demands when the economic consequences of those choices are borne by a publicly funded health care system. When James’s initial clinical assessment determined he needed an ICD, there was no question about cost; as a resident of a society that has deemed access to basic medical care a positive right, James was entitled to the provision of his basic health care needs. When he subsequently requested that the ICD be removed against all medical advice to the contrary, there was still no mention of the potential financial costs in discussions with James, although the issue did come up in conversation among the health care and clinical ethics teams. In this case the surgeon who offered to remove the ICD said he would not bill the provincial health plan for the usual fee associated with the procedure. But the cost of the removal was insignificant compared to the additional costs of maintaining James after his cardiac event, the subsequent replacement of the ICD, and the eventual heart transplant. What responsibilities should individual patients bear for the consequences of their decisions? Are the considerations in the current case different from those that arise in the context of other “lifestyle choices” where individuals make poor decisions that affect their health, resulting in a greater burden for the health care system? These latter issues are beyond the purview of the current discussion but should not go unmentioned. Talk about money will put a strain on any relationship, and for this reason we often tiptoe around such issues in our personal lives. Nevertheless, when all personal relational resources have been exhausted, as they were eventually in our dealings with James, perhaps the financial consequences of his decision should have been raised. Indeed, at that point the relationship was more contractual than personal in nature, and in the interests of transparency and full disclosure the real financial costs of the transaction he was insisting upon could have been revealed and discussed. But again, such financial considerations and discussions should occur only at the end of the relational encounter, not at the outset. In any case, in our publicly insured health care system talk of costs would serve only as one last (and perhaps desperate) attempt at moral suasion, as there are no coercive financial mechanisms available by which to recoup the costs of an unwarranted and frivolous demand for services. However, such mechanisms might exist in health care systems that rely more heavily on private insurance and where the “ethics of strangers” are often more readily apparent. Thus, while an ethic of intimacy may encourage care and compassion, an ethic of strangers may more readily accommodate individual responsibility. Finding the means by which to assess and nurture the full continuum of relational values and responsibilities is thus an ongoing challenge.\n\n* * *\n\n### **7. **[**A Pilot Evaluation of Portfolios for Quality Attestation of Clinical Ethics Consultants**](https://www.tandfonline.com/doi/full/10.1080/15265161.2015.1134705)\n\nJoseph J. Fins, et al. *The American Journal of Bioethics* 16(3). Target Article.\n\n> **Abstract:** Although clinical ethics consultation is a high-stakes endeavor with an increasing prominence in health care systems, progress in developing standards for quality is challenging. In this article, we describe the results of a pilot project utilizing portfolios as an evaluation tool. We found that this approach is feasible and resulted in a reasonably wide distribution of scores among the 23 submitted portfolios that we evaluated. We discuss limitations and implications of these results, and suggest that this is a significant step on the pathway to an eventual certification process for clinical ethics consultants.\n> \n> In 2013, the American Society for Bioethics and Humanities Quality Attestation Presidential Task Force (ASBH QAPTF) published a proposed two-step model for evaluating and attesting to the quality of clinical ethics (CE) consultants (Kodish and Fins et al. 2013). The first part of this two-step process envisioned the submission of a portfolio that summarizes the education, experience, philosophy, and substance of ethics case consultation to establish a consultant’s eligibility to undergo a subsequent examination.\n> \n> The Quality Attestation Presidential Task Force (QAPTF) was formed to develop a means to assess the performance of CE consultants. The task force functions under the aegis of ASBH, the primary society of bioethicists and scholars in the medical humanities and the organizational home for many individuals who perform clinical ethics consultation (CEC) in the United States. Members of the task force were appointed for their breadth and depth of experience in clinical ethics consultation and the diversity of their professional and educational backgrounds.\n> \n> \\[...\\]\n> \n> Much work remains to be done. The first experience with an ethics consultation portfolio has yielded ways to improve it and leaves undecided what the next steps should be. Future developments will require (1) more resources in time and expertise, (2) attention to governance with regard to the appropriate institutional home for CE consultant attestation/certification, and (3) financial self-sufficiency. Nonetheless, the field has moved further along the path toward a process to demonstrate professional competency of CE consultants and to develop robust quality standards consistent with the expectation for other members of the health care professions. Remaining challenges notwithstanding, this is a positive development for both clinical ethics consultation and for the patients and caregivers it serves.\n\n* * *\n\n### **8. **[**Research Moratoria and Off-Label Use of Ketamine**](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1145285)\n\nAndrea Segal and Dominic Sisti. *The American Journal of Bioethics* 16(4). Open Peer Commentaries.\n\n> We wish to point out an additional consequence of the Catch-22 described by Andreae and colleagues (Andreae et al. 2016). The decades-long research gridlock of controlled drugs has unintentionally resulted in wide-scale off-label use of one such drug, ketamine, in free-standing private psychiatric clinics. The growth of these clinics and burgeoning consumer demand for ketamine raise important ethical and policy issues related to patient safety, consent, and the future of research on controlled drugs.\n> \n> Ketamine was approved in 1970 as a dissociative anesthetic, and by the 1980s researchers began to use ketamine for the treatment of psychiatric disorders. Ketamine’s properties as an NMDA (N-methyl-D-aspartate) receptor antagonist have spurred enthusiasm about a new line of psychiatric treatment for very depressed patients (Sisti, Segal, and Thase 2014). Outpatient clinics have sprung up worldwide purporting to treat serious depression with ketamine. Unfortunately, a market has developed where unscrupulous clinics haphazardly provide ketamine to all comers. In Australia, one clinic was selling ketamine-filled syringes for at-home use, raising serious concerns about patient safety and drug diversion (Worthington 2015).\n> \n> \\[...\\]\n> \n> Off-label commercial use of generic ketamine will accelerate as long as the clinical research on generic ketamine is not incentivized in some way. As this dynamic continues to unfold, we are concerned that desperate, potentially incapacitated patients will be harmed, and that a clinical mishap could set back legitimate research for this highly stigmatized drug. With proper clinical oversight of ketamine administration, and clear systematic assessments of its risks and benefits, some of the ethical concerns about off-label ketamine use can be assuaged. However, unchecked enthusiasm and market forces may take us down “a slippery ketamine slope” (Schatzberg 2014). In addition to pushing for research dollars for ketamine and other controlled substances, we also recommend stricter regulation and oversight of off-label use.\n\n* * *\n\n### **9. **[**Good Ethics Begin With Good Facts**](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1180447)\n\nBirgitta Sujdak Mackiewicz. *The American Journal of Bioethics* 16(7). Case Commentaries.\n\n> Four-year-old James was transferred from an outlying hospital in a critically ill state and has not responded to conventional therapies. For the last day and a half he has been treated with inhalational sevoflurane. James’s condition is not worsening, but he remains critically ill, suffering from a life-threatening condition. If inhalational sevoflurane remains the optimal treatment for James, then there is no clinical justification for transitioning him to VV ECMO given its risks and lack of clinical benefit—in this case there is no indication that any clinical benefit in doing so exists.\n> \n> The case as presented constitutes both a clinical and an organizational ethics issue. The physicians who accepted James for transfer a few days ago are now faced with the decision of whether they can transition James off an effective therapy. The decision is necessitated by organizational decisions about the level of anesthesia staffing on weekends. In short, if the facts of the case hold, the answer is “no.” It is not under ordinary circumstances, citing the case summary, “ethically permissible to transition a patient to a therapy that is working to another therapy, which has increased morbidity and mortality, due to insufficient staffing.” To do so represents a failure of the facility to provide the necessary and reasonably expected resources tacitly promised by the acceptance of James from the outlying hospital. To switch James to a riskier treatment due to institutional inability to provide the appropriate care also constitutes an injustice to James. His initial transfer was based on the understanding that the accepting facility was able to provide the necessary care. James might instead have been refused admission and transferred to a facility able to meet his needs.\n> \n> \\[...\\]\n> \n> The physician must consider whether the current treatment of inhalational sevoflurane is optimal for James even if staffing is maintained. Physicians have an ethical obligation to reassess patient status throughout the course of treatment and to recognize when a patient who is not declining, but is still critically ill, has an insufficient response to the current treatment warranting a new approach. This determination is outside of the scope of the ethicist’s practice; however, it is not unreasonable for an ethicist to inquire as to the clinical factors at play and how the physician arrived at his or her decision. The ethicist may also recommend that physicians consider whether it is appropriate to seek further opinions from other institutions who may have had similar cases. This is not a questioning of the physician’s medical judgment or competency by the ethicist, but rather an interdisciplinary exploration of the case at hand, examining the facts of the case, the literature, and the range of clinically and morally acceptable options from which physicians will make a recommendation.\n\n* * *\n\n### **10\\.** [**The Ethics of Organ Donor Registration Policies: Nudges and Respect for Autonomy**](https://www.tandfonline.com/doi/full/10.1080/15265161.2016.1222007)\n\nDouglas MacKay and Alexandra Robinson. *The American Journal of Bioethics* 16(11). Target Article.\n\n> **Abstract:** Governments must determine the legal procedures by which their residents are registered, or can register, as organ donors. Provided that governments recognize that people have a right to determine what happens to their organs after they die, there are four feasible options to choose from: opt-in, opt-out, mandated active choice, and voluntary active choice. We investigate the ethics of these policies' use of nudges to affect organ donor registration rates. We argue that the use of nudges in this context is morally problematic. It is disrespectful of people's autonomy to take advantage of their cognitive biases since doing so involves bypassing, not engaging, their rational capacities. We conclude that while mandated active choice policies are not problem free—they are coercive, after all—voluntary active choice, opt-in, and opt-out policies are potentially less respectful of people's autonomy since their use of nudges could significantly affect people's decision making.\n> \n> Governments must determine the legal procedures by which their residents are registered, or can register, as organ donors. Provided they recognize that people have a right to determine what happens to their organs after they die, an assumption underlying our analysis below, there are four feasible options to choose from.\\[1\\] *Opt-in* requires people to actively register as organ donors, for example, by signing an organ donor registration card, or checking a box when renewing their driver’s license. Many jurisdictions, including Canada, some U.S. states, the United Kingdom, and Germany, currently register organ donors in this way. *Opt-out* presumes that people are willing to be organ donors and so automatically registers them as such, but provides them with the opportunity to opt out. This policy is employed by many European countries, including Spain, Austria, Belgium, Greece, and France. *Voluntary active choice* (VAC) presents people with the choice of registering as an organ donor or not, but does not require them to make a decision. This policy is employed by the U.S. states of California and Vermont. Finally, *mandated active choice* (MAC) presents people with the choice of registering as an organ donor or not, and requires them to make a decision, for example, by making the renewal of their driver’s license conditional on them stating their donation preference. This policy is employed by the U.S. states of Illinois and New York, as well as New Zealand.\n> \n> Debates regarding the ethics of organ donor registration policies have largely concerned the permissibility of opt-out policies. Proponents of these policies argue that they are likely to yield the highest donor registration rate, but critics object that they are morally deficient since they do not secure the consent of potential donors (Veatch 2000, 167–174; den Hartogh 2011; MacKay 2015). In this article, we set the issue of consent aside, exploring instead a different way in which these policies can be more or less respectful of people’s autonomy. Organ donor registration policies differ not only in terms of whether they secure people’s consent or not, but also in terms of the types of influence they employ to affect people’s decision making. As Richard H. Thaler and Cass R. Sunstein make clear in *Nudge*, one possible reason that organ donor registration rates are so high in jurisdictions employing opt-out is that people exhibit status quo bias, a tendency to stick with the current state of affairs or choose default options (2008, 34– 35). By taking advantage of people’s status quo bias in this way, opt-out policies employ what Thaler and Sunstein refer to as a “nudge,” that is, a way of designing choice situations that “alters people’s behavior in a predictable way without forbidding any options or significantly changing their economic incentives” (2008, 6).\n> \n> \\[...\\]\n> \n> Consider second that people’s registration status need not be fully determinative of the disposition of their organs after death for our analysis to have important implications for the ethics of donor registration policies. Instead, all that needs to be the case is that people’s registration status significantly affects the disposition of their organs after death. That is, even if families continue to have a de facto or de jure veto over the disposition of their relative’s organs, our analysis is important if people’s registration status significantly affects the decisions that families make. Provided that people’s registration status has this effect, then it matters ethically how that registration status is secured, particularly if future studies show that under VAC, opt-in, or opt-out policies, people’s registration status is significantly influenced by their status quo bias. This point is important, since there is a good deal of evidence in the U.S. context showing that a person’s registration as a donor is highly associated with familial decisions to donate (Siminoff and Lawrence 2002; Rodrigue, Cornell, and Howard 2006; Christmas et al. 2008; Traino and Siminoff 2013). The choice of donor registration policies therefore matters morally, and we hope that our article makes an important contribution to the ethical analyses of these policy options, identifying a way in which opt-in, optout, and VAC policies are morally deficient, and MAC policies potentially morally superior.\n\n* * *\n\n### **11\\.** [**A Trust‐Based Pact in Research Biobanks. From Theory to Practice**](https://onlinelibrary.wiley.com/doi/10.1111/bioe.12184)\n\nVirginia Sanchini, et al. *Bioethics* 30(4). Original Article.\n\n> **Abstract:** Traditional Informed Consent is becoming increasingly inadequate, especially in the context of research biobanks. How much information is needed by patients for their consent to be truly informed? How does the quality of the information they receive match up to the quality of the information they ought to receive? How can information be conveyed fairly about future, non‐predictable lines of research? To circumvent these difficulties, some scholars have proposed that current consent guidelines should be reassessed, with trust being used as a guiding principle instead of information. Here, we analyse one of these proposals, based on a Participation Pact, which is already being offered to patients at the Istituto Europeo di Oncologia, a comprehensive cancer hospital in Milan, Italy.\n> \n> Recent studies suggest that it is increasingly difficult to provide patients with accurate information during informed consent procedures.\\[1\\] While this problem affects many areas of medicine, it is particularly urgent in the context of research biobanks. In this latter setting, in addition to other traditional issues – such as patients’ options at stake and their likelihood to develop a ‘therapeutic misconception’\\[2\\] – the kind of information to be delivered renders canonical informed-consent forms potentially ill-suited. Several criticalities have been identified concerning the use of traditional informed consent (henceforth IC) in the domain of research biobanks.\n> \n> The most widely known objection to the use of traditional IC in biobanks relates to the requirement of providing participants with full information about the research projects in which the specimens will be utilized. However, since at the time of collection it is impossible to foresee the future role of tissue samples in research, the provision of full information prior to signing of the consent is unfeasible. Therefore, the very concept of an IC for research biobanks as a means to provide the prospective participant with ‘full information’ seems inconsistent.\\[3\\]\n> \n> \\[...\\]\n> \n> From an applied bioethical viewpoint, one question is paramount. Does the PP work? From the initial data we described, it appears to be so. More than 97% of participants signed the *Pact* in a non-anonymous form, an almost two-fold increase with respect to the traditional IC, arguing for the relevance of trust in mediating the relationship between participants and researchers.\n\n* * *\n\n2017\n----\n\n### **12\\.** [**Responsible Translation of Psychiatric Genetics and Other Neuroscience Developments: In Need of Empirical Bioethics Research**](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1284917)\n\nGabriel Lázaro-Muñoz. *The American Journal of Bioethics* 17(4). Open Peer Commentaries.\n\n> In recent years, billions of dollars have been allocated to large-scale neuroscience projects with the goal of advancing our understanding of neural function, developing neurotechnologies, and, ultimately, improving neuropsychiatric care and prevention. These projects include the U.S. Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative, the European Union’s Human Brain Project (HBP), the U.S. National Institute of Mental Health’s Research Domain Criteria (RDoC) Initiative, and the Psychiatric Genomics Consortium (PGC), among others. Although these initiatives will undoubtedly yield clinical benefits, if we aim to responsibly research and translate the knowledge and technologies they produce, it is essential to empirically examine the potential harms—including opportunity costs—and the ethical or “neuroethics” challenges generated.\n> \n> These initiatives are already yielding benefits. For example, over the past 8 years, the PGC has identified more than 100 genomic loci that are reliably associated with schizophrenia (Schizophrenia Working Group of the PGC 2014). The PGC achieved this by pooling resources and large samples from studies around the world and using an unbiased array-based genetic testing approach. This recent progress is in stark contrast to the years of struggle psychiatric genetics researchers faced trying to replicate findings obtained through candidate gene approach studies with small samples and little collaboration across research laboratories (Farrell et al. 2015; Need and Goldstein 2014). The success of initiatives like the PGC is evidence of how large-scale neuroscience projects can accelerate our understanding of psychiatric illnesses, which are known to be multifactorial and highly complex from a biological standpoint. Findings such as the identification of genomic loci associated with schizophrenia can lead to better risk prediction, resource allocation, prevention, drug targets, diagnosis, and treatment selection.\n> \n> \\[...\\]\n> \n> At the end of the day, as bioethicists, our goal should not be to simply raise questions, but to set well-informed ethical agendas and search for answers to effectively address ethical challenges. These four actions can help us achieve that.\n\n* * *\n\n### **13\\.** [**Interconnectedness and Interdependence: Challenges for Public Health Ethics**](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1353175)\n\nJonathan Beever and Nicolae Morar. *The American Journal of Bioethics* 17(9). Open Peer Commentaries.\n\n> An increasing number of contemporary voices in both bioethics and environmental ethics have grown dissatisfied with the schisms, abysses, and raging torrents that continue to flow between those two domains of ethical inquiry. Thus, Lee’s (2017) call for a public health ethics that serves as a bridge between them is a welcome addition to an expanding literature that highlights in terms of “health” the interconnection between individuals, their communities, and their environments.\n> \n> It is clear to us that public health ethics, in the footsteps of others seeking the same ecological understanding, has begun to articulate values based on “our connectedness with each other, animals, and the environment” (9). This conception comes not only from environmental philosophy (Jamieson 2001), from philosophy of ecology (Shrader-Frechette and McCoy 1993), and from eco-phenomenology (Brown and Toadvine 2003), but also from within bioethics and environmental ethics themselves (Beever and Morar 2016a; Morar and Skorburg 2016; Jennings 2016; Whitehouse 2003). Indeed, Lee’s argument is a revisitation of Van Rensselaer Potter’s own predominantly anthropocentric concerns about “environmental health” as both intend to capture the ways that environmental and nonhuman animal factors influence human health. All agree that there is a growing recognition of the intimate relations among previously siloed epistemic units and ethical domains.\n> \n> \\[...\\]\n> \n> In conclusion, in the absence of a careful distinction between interconnectedness and interdependence, public health ethics reaches too far and too fast so that it can account for the immense complexity of relations for which the challenges demand answers. It is, as of yet, not only epistemically impoverished concerning the nature of relationships, but also normatively impoverished of resources to sufficiently parse out the significance of certain moral contraventions. Yet even if we were to overcome such conceptual issues, focusing on bridges still tends to ignore the water for the sake of holding firm stable shores of inquiry. Perhaps the only way to pay attention to that flowing in between (which is why the bridge exists in the first place, of course) is to move beyond Potter and recognize there are no stable shores (Beever and Morar 2016b). Instead, we might be compelled to call into question the historical silos of applied ethical inquiry. Public health ethics hasn’t evidenced that it is robust enough to do that hard work— yet.\n\n* * *\n\n### **14\\.** [**Expanded Access for Nusinersen in Patients With Spinal Muscular Atropy: Negotiating Limited Data, Limited Alternative Treatments, and Limited Hospital Resources**](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1366199)\n\nBenjamin S. Wilfond, Christian Morales, and Holly A. Taylor. *The American Journal of Bioethics* 17(10). Case Report.\n\n> The issue of resource allocation is not new to bioethics. From the early days of dialysis to current complex solid organ transplant systems, medical advances continue to generate rich questions about the distribution of resources that are limited. Who will qualify? How will we decide? Who will need to wait? Who will die waiting?\n> \n> Spinal muscular atrophy 1 (SMA1) is a rare autosomal recessive disease (approximately 250 cases per year in the United States). The disease causes rapid, progressive decline in muscular strength with preserved intellectual function. Historically, 50% of infants die by age 1 year and the majority of the remaining patients by age 2 years, following the provision of comfort care. Until recently, life extension was limited to symptom management with long-term ventilator and nutrition support.\n> \n> \\[...\\]\n> \n> The Pediatric Neuromuscular Service requests an ethics consultation to assist in determining how to fairly allocate nusinersen to its patients, should they choose to participate in the EAP. Within driving distance of the center there are approximately 100 SMA1 patients, many of whom are already ventilator dependent. These patients are not all being followed by the service, but are not prohibited from applying for EAP therapy. There will likely be 5–10 new cases per year in the center’s catchment area. It is possible that the center will be the only hospital in the state to provide this treatment. Given their current staffing, procedural space, and bed space availability, the center is only able to reasonably guarantee therapy for 2–4 patients per month. The center worries about how to ethically prioritize patients, knowing it will be difficult to accommodate all those who might benefit from treatment. How should the group determine who will be treated and when?\n\n* * *\n\n### **15\\.** [**The Consequences of Vagueness in Consent to Organ Donation**](https://onlinelibrary.wiley.com/doi/10.1111/bioe.12335)\n\nDavid M. Shaw. *Bioethics* 31(6). Original Article.\n\n> **Abstract:** In this article I argue that vagueness concerning consent to post‐mortem organ donation causes considerable harm in several ways. First, the information provided to most people registering as organ donors is very vague in terms of what is actually involved in donation. Second, the vagueness regarding consent to donation increases the distress of families of patients who are potential organ donors, both during and following the discussion about donation. Third, vagueness also increases the chances that the patient's intention to donate will not be fulfilled due to the family's distress. Fourth, the consequent reduction in the number of donated organs leads to avoidable deaths and increased suffering among potential recipients, and distresses them and their families.\n> \n> There are three strategies which could be used to reduce the harmful effects of this vagueness. First, recategorizing the reasons (commonly referred to as ‘overrules’ under the current system) given by families who refuse donation from registered donors would bring greater clarity to donation discussions. Second, people who wish to donate their organs should be encouraged to discuss their wishes in detail with their families, and to consider recording their wishes in other ways. Finally, the consent system for organ donation could be made more detailed, ensuring both that more information is provided to potential donors and that they have more flexibility in how their intentions are indicated; this last strategy, however, could have the disadvantage of discouraging some potential donors from registering.\n> \n> Transplantation of organs from living and deceased organ donors saves and improves tens of thousands of lives globally every year. Despite a substantial increase in the number of living kidney donors in recent years, donation from deceased patients remains the leading source of organs for transplantation in most countries. In this article, organ donation is discussed in the context of the ethical and legal frameworks that apply in the United Kingdom, but most countries operate broadly similar deceased donation systems that are affected by similar issues of vagueness.\n> \n> In England and Northern Ireland, people who wish to donate their organs after they die must consent to any such donation. They can do so by informing relatives of their wishes or by signing up to the organ donor register. In Scotland, the same applies, but the term used is 'authorization' rather than consent. Wales recently switched to a 'deemed consent' system where it will be presumed that a patient wished to donate in the absence of any evidence to the contrary (a so-called opt-out), but here too, those who wish to donate can register their intentions on the UK-wide database.\n> \n> \\[...\\]\n> \n> I have suggested three potential strategies for combating the problem of vagueness. Reclassifying 'overrules' and recognizing the importance of new evidence of refusal and reassessment of best interests will bring clarity to conversations with families. If donors broach the subject of donation with their families vagueness will be reduced, particularly if a personalized organ donation directive is created. Most importantly, preventing vagueness at the point of registration is better than attempting to cure it at the point of donation when the family is upset. Creating a new consent system and providing more detailed information might discourage some people from donating, but it would increase the chances that the wishes of someone who does want to donate will be respected, substantially increase health professionals' and families' confidence in the consent of donors, and consequently reduce the distress currently caused to both families and staff.\n\n* * *\n\n2018\n----\n\n### **16\\.** [**A Model for Communication About Longshot Treatments in the Context of Early Access to Unapproved, Investigational Drugs**](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1401170)\n\nEline M. Bunnik and Nikkie Aarts. *The American Journal of Bioethics* 18(1). Open Peer Commentaries\n\n> When seriously ill patients run out of standard treatment options, they may consider nonstandard treatment options such as expanded access, also known as “compassionate use.” Through expanded access programs, patients are given access to investigational drugs that are still under development and not yet approved for marketing. As the safety and efficacy of unapproved drugs have not been fully established, it is uncertain whether these drugs will offer medical benefit. Especially when the compound is in an early stage of the drug development process, its odds of success may be low or “longshot.” Expanded access raises ethical concerns, notably that seriously ill patients may overestimate the benefits of an investigational drug and underestimate its safety issues, fail to make informed decisions, and become susceptible to false hope and exploitation (Darrow et al. [2015](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1401170#)). The communication model proposed by Weiss and Fiester ([2018](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1401170#)) and its central distinction between low-odds and no-odds treatment can be used to assist seriously ill patients and their treating physicians not only with decision making with regard to initiating expanded access to investigational therapies but also with monitoring and managing their effects. Thus, it may help to overcome some of the ethical problems associated with expanded access.\n> \n> Systems for expanded access differ across countries, but have a set of conditions in common: Patients must be suffering from serious or life-threatening diseases must have exhausted standard treatment options, and must not be eligible for participation in clinical trials (Jarow et al. [2017](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1401170#)). The managing physician must believe that the potential benefits of the drug will outweigh the risks. A regulatory authority such as the Food and Drug Administration (FDA) will need to approve the request (FDA [2017](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1401170#)). In some countries, including the United States, requests must also be evaluated by an institutional review board. Importantly, the pharmaceutical company must be willing to supply the drug, often at no cost. Finally, patients must provide informed consent.\n> \n> \\[...\\]\n> \n> As the—often criticized—Right-to-Try movement in the United States is raising awareness of expanded access and seeking to increase its accessibility (Holbein et al. [2015](https://www.tandfonline.com/doi/full/10.1080/15265161.2017.1401170#)), demand among patients for unapproved drugs around the world is expected to rise. Adequate informed consent processes that explicitly rebut unwarranted therapeutic optimism will be crucial for morally responsible practices of expanded access in the future.\n\n* * *\n\n### **17\\.** [**Enhance Diversity Among Researchers to Promote Participant Trust in Precision Medicine Research**](https://www.tandfonline.com/doi/full/10.1080/15265161.2018.1431323)\n\nDemetrio Sierra-Mercado and Gabriel Lázaro-Muñoz. *The American Journal of Bioethics* 18(4). Open Peer Commentaries.\n\n> Participants in the study reported by Kraft and colleagues (2018) raised an issue that supports how having more diversity among researchers could help promote trust toward precision medicine research and, ultimately, more precise medicine for all. First, participants’ perceptions of trustworthiness in biomedical research were related to their personal or group experiences with racism. Ideally, experience with racism would not play a role in individuals’ willingness to participate in research, and participants would trust researchers who do not look or sound like them as much as they trust those that do. Unfortuantely, however, that does not seem to be the case. As discussed by Kraft and colleagues (2018), participants would feel more comfortable with researchers with whom they can identify, or as a participant described: “I think that people relate to people that look like them. They trust people that look like them more” (10). Researchers who share similar ethnic, racial, or cultural backgrounds, as potential participants, may share similar life experiences, like racism and discrimination. This common history or experiences could help those researchers better understand participants’ concerns and build rapport when interacting with them. Rapport between researchers and potential participants would help build trust toward precision medicine research and likely increase participation rates among minority populations.\n> \n> Not only would increasing diversity among researchers promote participation rates among minority populations, but empirical evidence demonstrates that research is of higher quality when performed by diverse groups (Campbell et al. 2013). However, achieving the goal of more diversity among researchers may be more complex than it seems at first glance. The proportion of researchers from racial and ethnic minority groups such as African Americans, Hispanics/Latinos, and Native Americans in U.S. academic institutions is significantly less than expected based on their share of the U.S. population (National Science Foundation 2012). In order to increase diversity among researchers we must address multiple obstacles at different stages of academic and scientific career development. In the following, we discuss some of these obstacles and potential solutions to help increase the representation of underrepresented minorities (URMs) in the sciences.\n> \n> \\[...\\]\n> \n> A history of research misconduct against minority groups, coupled with research institutions in which potential participants from minority groups do not see themselves or their interests sufficiently represented, can help explain why it is difficult to recruit individuals from minority populations as participants in precision medicine research. How we address the lack of diversity among researchers will be key to promoting trust and participation in precision medicine research among individuals from minority populations and achieving precision medicine for all. Efforts to ensure that URM researchers who are currently in academia are involved in precision medicine research would go a long way. However, the significant disparity between the proportion of URMs in the U.S. population and scientists at research institutions conducting precision medicine research needs to be addressed. Finally, increasing diversity among researchers will enhance precision medicine research not only by increasing the participation rate of minority populations but also because the complex thought processes necessary to conduct quality science are facilitated by having researchers from diverse backgrounds (Antonio et al. 2004).\n\n* * *\n\n### **18\\.** [**Miracles, Scarce Resources, and Fairness**](https://www.tandfonline.com/doi/full/10.1080/15265161.2018.1431716)\n\nSteve Clarke. *The American Journal of Bioethics* 18(5). Open Peer Commentaries.\n\n> Clinical bioethicists (ethicists) work for hospitals, nursing homes, rehabilitation centers, and other health care institutions (Fox, McGee and Caplan 1998). They perform various work roles, including policy development, ethics education, and consulting with patients, surrogate decision makers, families, health care professionals, and administrators about ethical issues (Chidwick et al. 2010). Bibler and colleagues (2018) offer practical suggestions to ethicists who are asked to consult patients, or their surrogate decision makers, when a particular medical intervention or course of care is requested on the grounds that this may enable a miraculous cure to take place. They focus their discussion on Christian patients and surrogates who invoke the possibility of miracles.\n> \n> Bibler and colleagues (2018) argue that when ethicists counsel patients or surrogates who invoke the possibility of miracles, “the overall interests of the patient should (*ceteris paribus*) take priority” (44) in shaping the counsel provided. I do not agree that ethicists should prioritize the overall interests of particular patients when counseling those patients or their surrogate decision makers. Ethicists should be sensitive to the needs of particular patients and should demonstrate respect for the beliefs and values of patients and their surrogates, including belief in the possibility of miraculous cures and the religious values that accompany such beliefs. However, ethicists are not advocates for particular patients. They have a professional duty to help patients and surrogates make, and accept, good all-things-considered ethical decisions. Sometimes such decisions will be ones that go against the overall interests of particular patients. This is especially likely when a proposed course of action, which is the interests of a particular patient, will have harmful consequences for other patients.\n> \n> \\[...\\]\n> \n> It may be, however, that the patient’s family members do not only intend to use the additional time that is being requested to pray, but also plan to undertake specific activities that, they hope, will make it more likely that divine intervention will occur. They may be planning to cast spells, offer sacrifices, recite incantations, and so on. If this is what they intend, then they are not proposing to wait and hope for a miracle. A miracle is usually understood as the consequence of a decision made by God to intervene in the natural world and is not usually understood as being subject to human prediction or control. Someone who acted in a way that, they supposed, would make it more likely for supernatural intervention in the natural world to occur than it would be otherwise, would be attempting to perform magic, rather than praying for a miracle (Clarke 1999). Even though resorting to magic is not uncommon, especially among those who are desperate to see their ill relatives cured, most religious traditions regard attempts to perform magic as incompatible with genuine religious faith (Clarke 2014). If all of this is explained to surrogates who propose to extend life support, in defiance of received medical opinion, then, after discussion with family members, those surrogates may be persuaded to reconsider their proposed course of action.\n\n* * *\n\n### **19\\.** [**Conscience as a Civil and Criminal Defense**](https://www.tandfonline.com/doi/full/10.1080/15265161.2018.1478023)\n\nNadia N. Sawicki. *The American Journal of Bioethics* 18(7). Open Peer Commentaries.\n\n> Prof. Nelson (2018) makes a compelling argument that conscientious objection to abortion (COTA) laws should not immunize health care providers from criminal prosecution when they deny abortions to women with life-threatening pregnancies. Conscience, he argues, should not be a defense to homicide. As a practical matter, however, only a small percentage of COTA laws protect providers from criminal prosecution—less than 15% of states have such protections. Far more concerning is that the majority of states—almost 75%—explicitly immunize providers from civil liability, even when their conduct falls below the standard of care and harms patients. That is, most COTA laws create a “conscience defense” to malpractice. And while state prosecutors may be politically disinclined to take action in such cases, patients have far greater incentive to use whatever legal tools are available to seek redress for their injuries. Thus, those advocating for limits on health care conscience laws would be better served by challenging the many state laws that protect providers from civil liability, rather than the few that protect providers from criminal prosecution.\n> \n> As Prof. Nelson notes, many COTA laws offer “sweeping immunity” from adverse consequences that might result when a provider refuses to participate in abortion for reasons of conscience (Nelson 2018). I am currently conducting a nationwide study of health care conscience laws to evaluate the types of procedural protections they offer, and my research confirms Prof. Nelson’s assessment. While the protections offered vary significantly from state to state, they may include immunity from criminal prosecution, civil liability, administrative sanctions, adverse action by employers or other private entities, loss of government funding, and denial of educational opportunities, among other potential adverse consequences. However, my preliminary research indicates that immunity from civil liability is by far the most common form of procedural protection established by health care conscience laws.\n> \n> \\[...\\]\n> \n> There are good reasons for American law to grant some protections to health care providers whose deeply held conscientious beliefs limit the types of services that they are able to deliver. In many cases, it may be possible to respect a provider’s conscientious convictions without hindering patients’ rights to access high-quality medical care. But in cases of true conflict—such as when a patient seeking emergency treatment is refused care because her treating physician or hospital oppose even lifesaving abortions—the law must strike a balance. While it may not wish to compel a provider to perform a medical service that he considers unconscionable, it can demand that he face the consequences of his choice if his refusal causes patient injury that would otherwise be compensable under civil or criminal law (Sawicki 2018).\n\n* * *\n\n### **20\\.** [**The “Reasonable Subject Standard” as an Alternative to the “Best Interest Standard”**](https://www.tandfonline.com/doi/full/10.1080/15265161.2018.1485763)\n\nJoseph Millum. *The American Journal of Bioethics* 18(8). Open Peer Commentaries.\n\n> Johan Bester makes a compelling case against the use of the harm principle to guide medical decisions on behalf of children (Bester 2018). He notes that parents have positive obligations to their children over and above refraining from harming them. Moreover, the correct decision-making standard for children must take into account the competing duties of parents and clinicians: “The obligation to do what is best for the child should be weighed against competing ethical obligations and practical constraints” (16). Bester claims that a modified best interest standard—“do the best for a child, *all things considered*”—can achieve the requisite balancing. He proposes to operationalize the standard through two questions: (1) “Can a reasonable argument be offered that the decision is best for the child, all things considered?”; (2) “Does the decision expose the child to obvious risk of harm?” If the answers are yes and no respectively, then the decision meets the standard.\n> \n> In this commentary, I argue that Bester’s framework for parental decision making is flawed. I propose an alternative—the “reasonable subject standard”—that can better achieve the goals that Bester endorses.\n> \n> \\[...\\]\n> \n> Bester is right that decisions for children are more complex than the harm principle would imply. He is also right that the standard parents use for proxy decision making must take into account other moral considerations in addition to the child’s interests. I reject only his framework for operationalizing his standard and recommend the reasonable subject standard as a workable alternative that I elaborate and defend elsewhere (Millum 2018, Chapter 6).\n\n* * *\n\n### **21\\.** [**Keep It Simple**](https://www.tandfonline.com/doi/full/10.1080/15265161.2018.1499847)\n\nJohn J. Paris and Brian M. Cummings. *The American Journal of Bioethics* 18(8). Open Peer Commentary.\n\n> The multilayered “tool” proposed by the author of the target article (2018) for resolving disputes between parents and physicians over medical treatment decisions for seriously ill newborns involves not only a close parsing of the “best interest” standard, the “harm principle,” and the “zone of parental discretion,” but a journey through a “think list,” multiple “cues,” and color-coded “assists” for assigning weight “to the concerns queried.” Such a Rube Goldberg formula makes one pine for the simple explanation of “best interests” found in the President’s Commission report, *Deciding to Forego Life-Sustaining* Treatment (President’s Commission for the Study of Ethical Problems in Medicine & Biomedical & Behavoral Research, 1983).\n> \n> The President’s Commission recommended that the decision maker take into account such factors as relief of suffering, the preservation or restoration of functioning, and the quality as well as the extent of life sustained. And because most people have an interest in the wellbeing of their families or close associates, the commission included consideration, from the patient’s perspective, of “the impact of a decision on the patient’s loved ones.” The commission acknowledged there would be cases in which there is no agreement on what the incapacitated patient would select. In such instances it proposed the surrogate have discretion to choose among a range of acceptable choices.\n> \n> \\[...\\]\n> \n> In such a world, one might ask, how do the “toolbox,” “stepladder,” or “multicolored assists” proposed by the author of the target article help understand, let alone resolve, family–physician conflicts on medical treatment? The President’s Commission approach provides a starting point, one that has helped numerous courts and multiple ethics committees resolve such disputes. The three-step AAP Guidelines are even more succinct and readily applicable. Keeping the standards simple is a good starting point.\n\n* * *\n\n### **22\\.** [**Not a matter of parental choice but of social justice obligation: Children are owed measles vaccination**](https://onlinelibrary.wiley.com/doi/10.1111/bioe.12511)\n\nJohan C. Bester. *Bioethics* 32(9). Original Article.\n\n> **Abstract:** This article presents arguments that reframe the discussion on vaccination ethics. The correct starting point for discussions on vaccination ethics is not what society owes parents, but rather what society owes children. Drawing on the justice theory of Powers and Faden, two conclusions are defended by presenting and defending a set of arguments. First, a just society is obligated to protect its children against serious vaccine‐preventable diseases such as measles through adequate levels of vaccination. Second, this obligation of the just society rests on identifiable individuals and institutions: parents, healthcare professionals, government, and vaccine producers have important obligations in this regard. This removes vaccination out of the realm of individual or parental discretion, and situates it in the realm of societal obligation. Children are owed vaccination, society is obligated to provide it. If parents cannot or will not provide it, society ought to respond.\n> \n> I will argue that a just society is obligated to protect its children against serious vaccine‐preventable illnesses such as measles. This societal obligation rests on various identifiable individuals and institutions. These arguments reframe the ongoing discussion on vaccination ethics: they move the primary focus of vaccine ethics and policy away from a dialogue on what is owed parents to focus instead on what is owed to children. This provides grounds for considering vaccination as a matter of a societal moral obligation owed to children rather than a matter of individual or parental choice. These considerations form the moral basis for sound vaccination policy and action.\n> \n> Despite the fact that vaccines are one of the most successful public health and medical interventions of the contemporary world, ethical and policy issues raised by vaccination continue to be a topic of academic and philosophical reflection. This is understandable, given the persistence of vaccine‐preventable disease outbreaks, vaccine hesitancy, vaccine refusals, and various barriers to vaccination in some communities.\\[1\\]\n> \n> \\[...\\]\n> \n> These conclusions reframe the dialogue on measles vaccination, moving it away from a framework of what is owed to parents, and instead placing it in the framework of what is owed to children. Vaccination is a moral obligation owed to children, and not a matter of individual choice or preference. This takes vaccination out of the realm of individual discretion and into the realm of social moral obligation. These considerations have implications for social practices and policy surrounding vaccination.\n\n* * *\n\n2019\n----\n\n### **23\\.** [**Rational Freedom and Six Mistakes of a Bioconservative**](https://www.tandfonline.com/doi/full/10.1080/15265161.2019.1626642)\n\nJulian Savulescu. *The American Journal of Bioethics* 19(7). Editorials.\n\n> “Life’s but a walking shadow, a poor player, that struts and frets his hour upon the stage, and then is heard no more; it is a tale told by an idiot, full of sound and fury, signifying nothing” (*Macbeth*, Act 5, Scene 5).\n> \n> One of the well-worn objections in the enhancement literature is based on inequality. Enhancement will only be available to some, so it will create unjust inequality. This was captured in the popular film *Gattaca.* In the most common form, it is based on concerns about capitalist markets: the rich will buy superior enhancements, exacerbating existing injustice. In the case of genetic enhancement, that injustice will be written into our genes. I have responded elsewhere, arguing that regulation can enable enhancement to promote justice and correct natural inequality (Savulescu [2006](https://www.tandfonline.com/doi/full/10.1080/15265161.2019.1626642#)).\n> \n> \\[...\\]\n> \n> Aging will make us all obsolete. In the end, we will all be dead and forgotten. Time’s passage is a great equalizer. The truly significant enhancement would be immortality. But short of this, we should each just hope for the best chance of the best life.\n\n* * *\n\n### **24\\.** [**The Exploitation of Professional “Guinea Pigs” in the Gig Economy: The Difficult Road From Consent to Justice**](https://www.tandfonline.com/doi/full/10.1080/15265161.2019.1630513)\n\nRoberto Abadie. *The American Journal of Bioethics* 19(9). Open Peer Commentaries.\n\n> Willing to endure pain, discomfort, and, mainly, boredom, professional guinea pigs perceive themselves as independent contractors, placing their bodies at the service of an industry that both exploits and dehumanizes them. With flexible schedules and mobile lifestyles, they constitute the ultimate foot soldiers of global capitalism (Abadie 2010). When I began researching this topic in the early 2000s, Uber, Lift, Airbnb, We Work, and many other forms of the gig economy had yet to arrive. In retrospect, it is clear that guinea pigs were at the forefront of this new form of exploitation; after all, “guineapigging” had been already been their gig for years, raising tough ethical questions. In *Beyond Consent*, Kahn et al. (1998), almost two decades ago, called for a focus on justice while appraising the participation of human subjects in research. Yet the normative analysis of the human subjects participating in clinical trials has been dominated by issues of consent, coercion, and undue inducement, as Millum and Garnett (2019) and Malmqvist (2019) have pointed out in this volume.\n> \n> Both articles make efforts to leave this framework behind, with one making an explicit call to consider social justice aspects and the other introducing the notions of acceptable alternatives, shared goals, and ample benefits. Yet what these authors fail to consider sufficiently is how the feasibility of their proposals is shaped by political and economic forces.\n> \n> \\[...\\]\n> \n> While it is easy to focus on the human subject protections in isolation, one particular phase at a time, or without addressing the sociopolitical context in which they are carried out, a more comprehensive approach might be needed if we are to address social justice issues in clinical trial research (Petryna 2009; Fisher 2008). A social justice approach cannot avoid the racialized, gendered, and class-based forms of inequality that compel mostly poor, desperate people to take risks while others more fortunate can just wait until the drug comes to market. Furthermore, tackling the politics of drug development from a social justice perspective requires asking who is benefiting, who is not, and how benefits could be distributed more equitably. These questions are difficult, but they are harder to avoid in the context of deepening social inequalities, where it is harder to pretend that the clinical trials research enterprise is a shared unmitigated social good. In turn, social inequality raises another challenge for social-justice-based redistributive policies. To succeed, they have to confront powerful economic actors that, thanks to decades of concentrated economic growth, have more access than ever to the economic and political resources to shut down any opposition.\n\n* * *\n\n### **25\\.** [**Caring for Dying Children**](https://www.tandfonline.com/doi/full/10.1080/15265161.2019.1674415)\n\nEdwin N. Forman and Rosalind Ekman Ladd. *The American Journal of Bioethics* 19(12). Open Peer Commentaries.\n\n> Elisabeth Kübler-Ross provided an impetus for physicians and patients to begin thinking in new ways—or to begin thinking at all—about death and dying. Kübler-Ross' work led to expectations about changes and improvements in end-of-life care and consequent increased patient and family satisfaction. Have these expectations been met? Perhaps. But is it still only a myth that a) the development of palliative care has greatly improved the care of dying children and increased parent satisfaction and b) that medical education now prepares pediatricians to communicate effectively and deal with their emotions about death and dying? (Forman and Ladd 2010). Recent literature as well as personal experience indicate that the reality is different from the myths. We will review the literature and suggest some steps that could improve care.\n> \n> There is evidence that often children with life threatening illness suffer unrelieved pain in their final months. Wolfe et al found that children with advanced cancer continue to experience high symptom distress. In the last 12 weeks of life, pain was reported as highly prevalent (Wolfe et al. 2015). Feudtner et al suggest that research is needed to dramatically advance the evidence base for pediatric palliative care, practices, and programs. They identified two challenges in particular: to improve symptom management and quality of life and to improve communication, elicitation of goals of care, and decision-making (Feudtner et al. 2019).\n> \n> \\[...\\]\n> \n> Myths are long-lived and die hard. But we need to do better to honor the legacy of Kübler-Ross.\n\n* * *\n\n### **26\\.** [**Research versus practice: The dilemmas of research ethics in the era of learning health‐care systems**](https://onlinelibrary.wiley.com/doi/10.1111/bioe.12571)\n\nJan Piasecki and Vilius Dranseika. *Bioethics* 33(5). Original Article.\n\n> **Abstract.** In this article we attempt to answer the question of how the ethical and conceptual framework (ECF) for a learning health‐care system (LHS) affects some of the main controversies in research ethics by addressing five key problems of research ethics: (a) What is the difference between practice and research? (b) What is the relationship between research ethics and clinical ethics? (c) What is the ethical relevance of the principle of clinical equipoise? (d) Does participation in research require a higher standard of informed consent than the practice of medicine? and (e) What ethical principle should take precedence in medicine? These questions allow us to construct two opposite idealized positions on the distinction between research and practice: the *integration model* and the *segregation model* of research and practice. We then compare the ECF for an LHS with these two idealized positions. We argue that the ECF for a LHS does not, in fact, solve these problems, but that it is a third, separate position in the relationship between research ethics and clinical ethics. Moreover, we suggest that the ECF for a LHS raises new ethical problems that require additional ethical analysis and justification. Our article contributes to the discussion on the relationship between research ethics and clinical ethics, revealing that although a learning health‐care system may significantly change the landscape of health care, some ethical dilemmas still require resolving on both theoretical and policy‐making levels.\n> \n> There is an ongoing controversy over the distinction between medical research and practice. On the one hand, the proponents of the therapeutic orientation argue that there is no morally significant difference between research and practice. Therefore, researchers have the same moral obligations in a learning health‐care system (LHS) as physicians.\\[1\\]  On the other hand, there are supporters of the research orientation who not only stress the differences between research and practice, but also distinguish between research and clinical moral duties.\\[2\\]  The proponents of the therapeutic orientation have high hopes for the implementation of an LHS that promises to dismantle not only conceptual differences between research and practice but also the regulatory system that differentiates between these two types of activities.\\[3\\]  Nevertheless, one can still ask whether the therapeutic orientation is really in harmony with the LHS. Here we want to reconstruct conceptual and ethical framework of the controversy between therapeutic and research orientations, then show how these two orientations relate to the concept of an LHS. We think that although there seems to be some concurrence of opinions between proponents of the therapeutic orientation and advocates of the LHS, this commonality is only superficial in character.\n> \n> The distinction between research and practice is a focal concept for both clinical and research ethics. Both the ethical and regulatory requirements of an ethics review and informed consent hinge on this very division.\\[4\\]\\]  The debate begins with the definitions given by the *Belmont Report*. According to this influential document, research is an activity that produces generalizable knowledge and practice aims at enhancing the well‐being of an individual patient with a reasonable expectation of success.\\[5\\]  However, by drawing a clear line demarcating research and practice, the National Commission shaped the whole conceptual framework of research ethics. This sharp distinction implies that the concept of therapeutic research should be discarded as it is confusing and conflates research and treatment.\\[6\\]  But this is a controversial resolution, as some argue that in clinical research one tests the therapeutic character of new interventions.\\[7\\]  Thus, the problem of division between research and practice leads to the question of what researcher‐physicians actually do when conducting clinical research and what they should do. This problem of the demarcation between research and practice was recently addressed in the discussion over the LHS and quality improvement studies.\\[8\\]  Some authors argue that the development of medicine is transforming the very nature of medical practice and in consequences the ethical and conceptual lines between research and practice no longer exist. Therefore, one may think that the therapeutic orientation somehow harmonizes with the ethical and conceptual framework (ECF) for an LHS.\\[9\\]  In order to show that this is not the case we construct and compare two competing idealized models of the relationship between research and practice in medicine: a model that segregates (the *segregation model*) and a model that integrates (the *integration model*) scientific research and practice. Elements of both models can be found in current debate over the distinction between research and practice. We analyze the theoretical and ethical assumptions built into these models and show how the ECF for an LHS could change the discussion about the relation between research and practice. We distinguish five key issues that differentiate the segregation model from the integration model and we demonstrate that the ECF for an LHS cannot be treated as a middle position between the two, but instead it creates a separate approach to the issue of research and practice distinction. This new approach can be called the *public health attitude towards research and practice*.\n> \n> \\[...\\]\n> \n> It can be concluded that the ECF for an LHS model of relationship has not provided us with conceptual instruments that would resolve the ethical debate between proponents of the segregation and integration models. The ECF should be considered a third proposal for characterizing the ethical problems of research. Namely, it is a proposal that navigates between research and clinical ethics and heads towards public health ethics. The ECF also does nothing to resolve the problem of a researcher's clinical obligation. Rather, it creates a new source of moral obligation: a health‐care system. Next, the ECF for LHS seems not to resolve the controversy over the concept of clinical equipoise. Rather, the ECF may be interpreted as a skeptical approach to existing therapies, which aims at eliminating ineffective and inefficient treatments from a health‐care system. Finally, we have argued that the ECF for an LHS attributes less significance to the standard of informed consent and that instead, it stresses the principle of beneficence over the principle of respect for autonomy. This makes the ECF for an LHS similar to the integration model, although it is at odds with the integration model's skepticism towards research. Moreover, the ECF for LHS leaves some ethical questions unanswered, such as whether there exists an obligation to participate in biomedical research. Therefore the implementation of an LHS opens a new area for ethical analysis rather than resolving the old problems of research ethics.\n\n* * *\n\n2020\n----\n\n### **27\\.** [**“Unusual Care”: Groupthink and Willful Blindness in the SUPPORT Study**](https://www.tandfonline.com/doi/full/10.1080/15265161.2019.1687787)\n\nGeorge J. Annas and Catherine L. Annas. *The American Journal of Bioethics* 20(1). Open Peer Commentaries.\n\n> The SUPPORT study of extremely premature newborns seems likely to go down as one of the most controversial studies of the 21st century (SUPPORT Study Group 2010). We previously suggested that the researchers in SUPPORT were “legally blind” in failing to understand that the “standard” that defines the content of informed consent is set by law, including the federal regulations, not by what physicians “usually” do or don’t do (Annas and Annas 2013). Macklin and Natanson, also early critics of the SUPPORT study’s failure to disclose the increased risk of death posed by the study, (Macklin et al. 2013) attack the study’s methodology itself in this issue, arguing that even on its own terms SUPPORT was fatally flawed (Macklin and Natanson 2020). Specifically, they argue that one arm of the study (the low oxygen arm) was not followed anywhere and could not be reasonably considered “standard care,” but was rather “unusual” and therefore experimental care (Cortes-Puch et al. 2016; Macklin and Natanson 2020). They also make useful suggestions about how to prevent future mischaracterizations of “usual care.”\n> \n> Another central problem with SUPPORT is denial of death, illustrated by the inability of researchers, IRBs, and supporters of SUPPORT, to acknowledge the fact that the study itself could put the newborn subjects at increased risk of death. This is understandable. It is, and should be, difficult to justify risky research on newborns. Even strong supporters of SUPPORT conclude that if it had been “known before the study began \\[that\\] standard clinical care would not have encompassed the lower oxygen range … it would have been unethical to conduct the study” (Hudson et al. 2013).\n> \n> \\[...\\]\n> \n> It seems reasonable to conclude that SUPPORT will be classified as an ironic, rather than an iconic, study: a study whose goal was to promote evidencebased medicine in the NICU, but whose execution failed to consider contemporary evidence that one of its two arms “differed markedly from usual care” (Cortes-Puch et al. 2016). This “groupthink” result could have been avoided had the human rights and human dignity of the research subjects been taken seriously enough to identify and disclose the risk of death inherent in SUPPORT.\n\n* * *\n\n### **28\\.** [**From Solo Decision Maker to Multi-Stakeholder Process: A Defense and Recommendations**](https://www.tandfonline.com/doi/full/10.1080/15265161.2019.1701745)\n\nDavid Ozar, et al. *The American Journal of Bioethics* 20(2). Open Peer Commentaries.\n\n> Berger (2020) argues effectively that “representativeness is more aptly understood as a variable that is multidimensional and continuous based on relational moral authority,” and also makes some useful suggestions about how taking this observation seriously might require changes in current patterns of practice regarding surrogates. But the essay raises additional important questions about how the Best Interest Standard (BIS) should be used among unrepresented patients and other patients as well because many surrogates besides those who “have no actionable knowledge of a patient’s preferences” find themselves in positions in which they need to determine, with the physician, what is in the patient’s best interests.\n> \n> First of all, consider that much of the author’s argument is based on Pope’s recommendation that, for unrepresented patients, BIS decisions should not be made by any one person solo, but by “a robust and transparent multi-stakeholder process involving other institution-based health care professionals and extrainstitutional parties” (citing Pope 2013). Despite this recommendation being foundational to Berger’s argument, however, Berger does not provide an argument—neither Pope’s nor the author’s own—for the ethical correctness of this method for reaching a BIS judgment about an unrepresented patient, or about any other patient who is incapable of relevant decision making. This omission is internally problematic for the author’s argument since the author’s criticism of permitting “self-identified surrogates” who “have no actionable knowledge of a patient’s preferences” depends on the assumption that the multi-stakeholder process is superior.\n> \n> \\[...\\]\n> \n> It might also be argued that the core values of the medical profession as it has been developed in our society have been sufficiently consistent from physician to physician that they can be depended on to counter the charge of arbitrariness. While that might once have been true, the history of medically related values in our society over the last four decades, and the extent to which they are or aren’t reflected in a particular physician’s own practice-guiding values does not encourage confidence on this matter.\n\n* * *\n\n### **29\\.** [**The Healthcare Ethics Consultant-Certified Program: Fair, Feasible, and Defensible, But Neither Definitive Nor Finished**](https://www.tandfonline.com/doi/full/10.1080/15265161.2020.1718421)\n\nArmand H. Matheny Antommaria, et al. *The American Journal of Bioethics* 20(3). Guest Editorials.\n\n> In June 2018, the Healthcare Ethics Consultant (HCEC) Certification Commission (the Commission) began accepting applications, and since then three candidate cohorts have received the Healthcare Ethics Consultant-Certified (HEC-C) designation. While these individuals have reported favorable experiences, concerns about the HEC-C program—both the certification process and what the designation represents—exist.\n> \n> As members of the Commission, we welcome dialogue about these concerns and wish to enhance understanding of the Commission’s reasoning behind key aspects of the HEC-C program. This program is part of an evolving system of professional structures, for which much work remains. In developing the HEC-C program, we were guided by well-established standards. The HEC-C program is not set in stone, but instead will evolve over time. We hope that transparency will encourage productive dialogue, improving the practice of, and access to, healthcare ethics consultation.\n> \n> \\[...\\]\n> \n> The Commission continuously seeks to improve the HEC-C program. To this end, all comments, suggestions, and criticisms are welcome. The Commission proactively seeks feedback from individuals who pursue the HEC-C designation. Based on this ongoing dialogue and new knowledge, the Commission may change the program, for example, the initial qualifications or the requirements for recertification. The examination itself will certainly change, with new questions written and evaluated. We encourage individuals to participate in developing questions or join the Commission. We hope that these and other improvements in the HEC-C program will be supplemented by the development of other professional structures, and that jointly these developments not only will enhance the value of certification but also will improve the quality of healthcare ethics consultation.\n\n* * *\n\n### **30\\.** [**A Call for Diversity and Inclusivity in the HEC-C Program**](https://www.tandfonline.com/doi/full/10.1080/15265161.2020.1714811)\n\nJulie Aultman and Cynthia Pathmathasan. *The American Journal of Bioethics* 20(3). Open Peer Commentaries.\n\n> As clinical ethics continues to evolve as a part of the nation’s healthcare system, there remains a disconnect between its practice and application across various healthcare organizations. This disconnect has led the American Society for Bioethics and Humanities (ASBH) to call for the standardization of clinical ethicists via the creation of the Healthcare Ethics Consultant-Certified (HEC-C) Program. Although this certification process aims to create a well-developed and equal representation of ethical knowledge, there are associated limitations with the HEC-C exam and eligibility criteria that can be addressed with opportunities for standardized training and additional assessment criteria.\n> \n> From our perspectives as a clinical ethics educator and a dual enrolled 4th year medical and clinical ethics graduate student, we recently examined the benefits and burdens of the HEC-C examination, considering it as a promising opportunity for clinical ethics trainees and experts in the field. Many of these benefits and burdens were eloquently described by Horner et al. (2020) and confirmed our initial assessment of this promising certification program, in addition to a few additional concerns centering on issues of diversity and inclusion. In order for a HEC-C examination and program to be valued among professionals and trainees throughout the United States, it is important to recognize the heterogeneity of clinical ethics consultation services, hospital ethics committees, and community-based ethics consultation programs, and the diversity of providers, patients, and their families who require critical ethical deliberation and person-centered guidance. From alternative systems of ethics training and education to the current HEC-C exam, which is limited in scope and diversity in the 110-question, multiple choice options it provides, there is a need to create a training and certification program that strengthens and assesses foundational ethical concepts and guiding principles, while training clinical ethicists how to deliberate and reflect on complex, diverse ethical situations.\n> \n> \\[...\\]\n> \n> By expanding the HEC-C program to include mock ethics consultation cases (i.e., video simulations) and standardized patient assessments, clinical ethicists could better demonstrate their abilities to empathize and consider patient and others’ values and interests, utilize their own professional experience and ethical knowledge, and be better prepared to address diverse, challenging ethical issues following certification. We have personally found this approach from an educator and a student perspective to be very beneficial for training and assessment, and a mechanism to address issues of diversity and inclusion that often are ignored or misinterpreted in standard multiple-choice questions. We are in support of standardization for clinical ethics consultations, but also argue that supporting resources and more guidance is needed for educators and institutions (beyond the ASBH handbook and accompanying publications) to better prepare future clinical ethicists unless such training can be appropriately provided without significant financial burdens through the HEC-C program.\n\n* * *\n\n### **31\\.** [**To Be Coherently Beneficient, Be Communitarian**](https://www.tandfonline.com/doi/full/10.1080/15265161.2020.1714796)\n\nCharles Foster. *The American Journal of Bioethics* 20(3). Open Peer Commentaries.\n\n> Bester (2020) is right to observe that “beneficence” is not self-defining: it does not determine its own substance. And there is indeed at least apparent tension between (a) the objective functioning/health of the patient and (b) the patient’s view of her own good. If beneficence is to be defined (or at least enacted) as a conversation between (a) and (b), Bester’s suggestion as to how that conversation should be conducted is as good as any and better than most.\n> \n> But Bester’s analysis is tainted by the besetting sin of modern clinical ethics: he sees the patient-clinician relationship as essentially contractual. For him, there are only two people in the consulting room: the patient and the clinician. They are the two contracting parties. And usually (for Bester) there is no difficulty in identifying the patient, and hence her interests. In fact, identifying the patient is tremendously difficult. All human beings change hats, and hence preferences and interests, all the time (Foster 2019). Sometimes the patient will be primarily a mother, steered mainly by her perception of her children’s interests—which might include an interest in not burdening them with the stress and financial cost of her care. Sometimes she will be a wife, for whom the priority is to keep an ailing husband company. Sometimes she will be a tired old woman, desperate for the rest and palliation that presumably comes with death. Sometimes she will be a mentally sprightly woman, looking forward to next spring’s flowers. Sometimes she will be a religious Catholic, determined to stay alive because that’s what she thinks the Church (which is a big part of her) requires. If she is normal she will oscillate queasily between all of these states. This is one reason why informed consent, as commonly conceived, is nonsense. It is also a reason why Bester’s account does not address the human condition as it really is.\n> \n> \\[...\\]\n> \n> So: there is a better way of conceiving and enacting beneficence than that articulated by Bester. It entails seeing the patient as a quintessentially relational entity—*defined* by the network of relationships in which she exists. Seen this way, beneficence always refers to the objective good of the patient and (since it is the same thing) of the people who comprise the relevant network. This sounds tyrannous, and it may indeed lead sometimes (in the interests of the welfare both of the patient and the network members) to autonomy not having the last word. Yet this is how ethicists and lawyers traditionally and uncontroversially talk about resource allocation questions.\n\n* * *\n\n### **32\\.** [**The Case Against Solicitation of Consent for Apnea Testing**](https://www.tandfonline.com/doi/full/10.1080/15265161.2020.1754512)\n\nDhristie Bhagat and Ariane Lewis. *The American Journal of Bioethics* 20(6). Open Peer Commentaries\n\n> Berkowitz and Garrett (2020) provide an excellent overview of the ethical and legal discussion about the need for consent prior to apnea testing (Berkowitz and Garrett 2020). However, we disagree with their conclusions that (1) apnea testing is a medical procedure necessitating consent and (2) the right to refuse treatment is applicable to apnea testing. The brain death exam is unique from other medical testing. Requiring consent for apnea testing would obligate a false equivalence between the test and medical procedures, leading to confusion as well as inappropriate and unjust allocation of resources.\n> \n> The authors defend the need for consent by citing the right to refuse medical treatment (Berkowitz and Garrett 2020). While this is an important concept, it does not pertain to this context, as apnea testing is not medical treatment; rather, apnea testing is an integral component of the evaluation to distinguish life from death, and clarify whether or not a person exists, or has ceased to exist. The intent of apnea testing is not to ameliorate brain injury, nor is there even potential for a therapeutic secondary effect of apnea testing. Furthermore, no matter how broadly one defines “medical treatment,” the term cannot apply if there is no person left to treat.\n> \n> \\[...\\]\n> \n> There are numerous ethical and legal considerations associated with the question of whether consent should be required prior to apnea testing. We thank Berkowitz and Garrett for their contribution to the literature on this topic (Berkowitz and Garrett 2020). We believe that universal legal clarification about both the need for consent and management of dissent is warranted to ensure practice does not vary from patient to patient, hospital to hospital and state to state. It is necessary to keep in mind the fact that apnea testing is neither akin to any other medical procedure nor to treatment. Rather, it is a unique process that clarifies whether or not a person still exists, and that this process has been proven to be low risk when guidelines are followed and prerequisites are met. As such, we disagree with the need for informed consent before apnea testing, and note that this would have profound consequences. \n\n* * *\n\n### **33\\.** [***THE TRUSTED DOCTOR: MEDICAL ETHICS AND PROFESSIONALISM***](https://onlinelibrary.wiley.com/doi/10.1111/bioe.12790)\n\nPhilip Charles Hébert. *Bioethics* 34(9). Book Review.\n\n> A new book by Rosamond Rhodes is an ambitious undertaking, arguing for a new approach to medical ethics. She specifically takes issue with “principlism,” “\\[t\\]he well-entrenched approach that regards medical ethics as an extension of common morality.” Rhodes asserts: “the ethics of medicine is distinct and different from the ethics of everyday life” and “the vast bioethics literature of the past 50-odd years that largely adopts that view, is simply mistaken.”\n> \n> The author claims that: (a) “\\[d\\]octors need a different touchstone for their professional behavior, a theory of medical ethics that provides them with clear and reliable moral guidance,” and (b) “\\[t\\]he most popular approaches to medical ethics were not particularly useful in resolving dilemmas in clinical practice.” Her duty-based analysis, she promises, is intended to provide a better, more reliable, and insightful guide for medical professionals.\n> \n> \\[...\\]\n> \n> Ethical principles and the requirements of common morality, as well as Rhodes’ duty-based perspective, can aid us in arriving at consensual responses to ethical issues in medicine. To paraphrase Raymond Carver, there are many paths to the waterfall.\\[2\\]",
      "plaintextDescription": "Some scholarly fields are very healthy (e.g., physics). Some are fairly unhealthy (e.g., evolutionary psychology, sad to say). Some are outright crazy (e.g., philosophy of religion).\n\nHow good or bad is bioethics? How rigorous and truth-tracking is it? How much social benefit or harm does it cause?\n\nLooking at lots of random examples is an under-used tool for making progress on this kind of question. It's fast, it avoids the perils of cherry-picking, and it doesn't require you to trust someone else's high-level summary of the field.\n\n \n\nI picked the two highest-impact-factor \"medical ethics\" journals that have the word \"bioethics\" in the title: The American Journal of Bioethics and Bioethics. Together, these two journals release about 500 reports, book reviews, etc. per year.\n\nI then picked a random article from 2014 through 2020 from each journal, plus extra random articles from the more cited journal (The American Journal of Bioethics): five from 2016, 2018, and 2020, and two from 2017 and 2019.\n\nFor each article, I quoted the abstract (if there is one), the first two paragraphs, and the final paragraph. These are provided below, sorted by year.\n\nObviously this breadth-first approach won't provide a full sense of the quality of argumentation in these papers; but it will hopefully provide a picture of what kinds of views tend to get argued for in the field, what those arguments tend to look like, what tends to get taken for granted, what tends to get ignored, and so on.\n\nI expect more accurate conclusions and faster consensus-building about bioethics if conversation is grounded in a common pool of examples (which can then be drilled down on to spot-check a particular paper's arguments, etc.).\n\nAdded: aphyer summarized the articles in the comment section; I've put aphyer's summaries in an editable Google Doc to make it easier to improve on the summaries (and to encourage article-by-article critiques, in the comments below or in the Doc).\n\n---------------------------",
      "wordCount": 14890
    },
    "tags": [
      {
        "_id": "ksdiAMKfgSyEeKMo6",
        "name": "Academic Papers",
        "slug": "academic-papers"
      },
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "xHjy88N2uJvGdgzfw",
        "name": "Health / Medicine / Disease",
        "slug": "health-medicine-disease"
      },
      {
        "_id": "kCrvmjZhsAZANnZL6",
        "name": "Philosophy",
        "slug": "philosophy-1"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Js34Ez9nrDeJCTYQL",
    "title": "Politics is way too meta",
    "slug": "politics-is-way-too-meta",
    "url": null,
    "baseScore": 294,
    "voteCount": 154,
    "viewCount": null,
    "commentCount": 46,
    "createdAt": null,
    "postedAt": "2021-03-17T07:04:42.187Z",
    "contents": {
      "markdown": "... i.e., it doesn't spend enough time arguing about object-level things.\n\nThe way I'm using it in this post, \"object-level\" might include these kinds of things:\n\n*   While serving as Secretary of State, did Hillary Clinton send classified information to an insecure email server? How large was the risk that attackers might get the information, and how much harm might that cause?\n*   What are the costs and benefits of various communications protocols (e.g., the legal one during Clinton's tenure, the *de facto* one Clinton followed, or other possibilities), and how should we weight those costs and benefits?\n*   How can we best forecast people's reliability on security issues? Are once-off mistakes like this predictive of future sloppiness? Are there better ways of predicting this?\n\n\"Meta\" might include things like:\n\n*   How much do voters care about Clinton's use of her email server?\n*   How much will reporters cover this story, and how much is their coverage likely to influence voters?\n*   What do various people (with no special information or expertise) *believe* about Clinton's email server, and how might these beliefs change their behavior?\n\nI'll also consider discussions of abstract [authority](https://www.lesswrong.com/posts/ZSWsWyzW2PfvoLPPi/what-is-ra), principle, or symbolism more \"meta\" than concrete policy proposals and questions of fact.\n\n[This](https://twitter.com/mattyglesias/status/1363246281720803328) is too meta:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0393f6c23754d8501db9cb0a29c9096b1ddc1d6d0b711de.png)\n\nMeta stuff is *real*. Elections are real, and matter. Popularity, status, controversy, and Overton windows have real physical effects.\n\nBut it's possible to focus too much on one part of reality and neglect another. If you're driving a car while talking on the phone, the phone and your eyes are both perfectly good information channels; but if you allocate too little attention to the road, you still die.\n\n### **When speaking the demon's name creates the demon**\n\nI claim:\n\n> There are many good ideas that start out discussed by blogs and journal articles for a long time, then get adopted by policymakers.\n> \n> In many of these cases, you could delay adoption by many years by adding more sentences to the blog posts noting the political infeasibility or controversialness of the proposal. Or you could hasten adoption by making the posts *just* focus on analyzing the effects of the policy, without taking a moment to nervously look over their shoulder, without ritually bowing to the Overton window as though it were an authority on immigration law.\n\nI also claim that this obeys the same basic causal dynamics as:\n\n> My friend Azzie posts something that I find cringe. So I decide to loudly and publicly (!) warn Azzie \"hey, the thing you're doing is cringe!\". Because, y'know, I want to *help*.\n\n*Regardless of how \"cringe\" the average reader would have considered the post*, saying it out loud can only help strengthen the perceived level of cringe.\n\nOr, suppose Bel overhears me and it *doesn't* cause her to see the post as more cringe. Still, it might make Bel worry that *Cathy* and other third parties think that the post is cringe. Which is a sufficient worry on its own to greatly change how Bel interacts with the post. Wouldn't want the cringe monster to come after you next!\n\nThis can result in:\n\n> **Non-well-founded gaffes**: statements that are controversial/offensive/impolitic largely or solely because some people think they sound like the kind of thing that would offend, alienate, or be disputed by a hypothetical third party.\n\nOr, worse:\n\n> **Even-less-than-non-well-founded gaffes**: statements that are controversial/offensive/impolitic largely or solely because some people are worried that a hypothetical third party might think that a hypothetical fourth party might be offended, alienated, or unconvinced by the statement.\n\nSee: [Common Knowledge and Miasma](https://medium.com/@ThingMaker/common-knowledge-and-miasma-20d0076f9c8e).\n\nSee: mimesis, herding, [bystander effect](https://www.lesswrong.com/tag/bystander-effect), [conformity instincts](https://www.lesswrong.com/posts/CEGnJBHmkcwPTysb7/lonely-dissent), [coalitional instincts](https://www.edge.org/response-detail/27168), and [The World Forager Elite](https://www.overcomingbias.com/2020/09/the-world-forager-elite.html).\n\nRegardless of how politically unfeasible the policy proposal *would have been*, saying that it's unfeasible will tend to make it more unfeasible*.* Others will pick up on the social cue and be more cautious about promoting the idea; which causes others to imitate *them* and be more cautious, and will cause the idea to spread less. Which makes people [nervously look around for proof](https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence) that this is a [mainstream-enough idea](https://www.lesswrong.com/posts/bzhGBHrGrFfQss4Df/the-feeling-of-breaking-an-overton-window) when they first hear about it.\n\nThis strikes me as one of the main ways that groups can end up stupider than their members, and civilizations can end up [neglecting](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d) low-hanging fruit.\n\n### **Brainstorming and trying things**\n\nObjection: \"Political feasibility matters. There may be traps and self-fulfilling prophecies here; but if we forbid ourselves from talking about it altogether, we'll be ignoring a real and important part of the world, which doesn't sound like the right way to optimize.\"\n\nMy reply: I agree with this.\n\nBut:\n\n*   I think mainstream political discourse is focusing on this way too much, as of 2021.\n*   I think people should be *more cautious* about when and how they bring in political feasibility, especially in the early stages of evaluating and discussing ideas.\n*   I think the blog posts discussing \"would this be a good policy if implemented?\" should mostly be *separate* from the ones discussing \"how politically hard would it be to get this implemented?\".\n\nObjection: \"But if I mention feasibility in my initial blog post on UBI, it will show that I'm a reasonable and practical sort of person, not a crackpot who's [fallen in love](https://www.lesswrong.com/s/M3TJ2fTCzoQq66NBJ/p/XrzQW69HpidzvBxGr) with pie-in-the-sky ideas.\"\n\nMy reply: I can't deny that this is a strategy that can be helpful.\n\nBut there are *other* ways to demonstrate reasonableness that have smaller costs. Even just saying \"I'm not going to talk about political feasibility in this post\" can send an adequate signal: \"Yes, I recognize this is a constraint at all. I'm treating this topic as out-of-scope, not as unimportant.\" Though even saying that much, I worry, can cause readers' thoughts to drift too much away from the road.\n\nPolicies' popularity ([frequently](https://www.overcomingbias.com/2007/05/policy_tugowar.html)) matters. But often the correct response to seeing a good idea that looks plausibly-unfeasible is to go \"oh, this is a good idea\", help bring a huge wave of energy and enthusiasm to bear on the idea, and try your best to get it discussed and implemented, *to find out* how feasible it is.\n\nWe're currently in a world where most good ideas fail. But... failing is mostly fine? See [On Doing the Improbable](https://www.lesswrong.com/posts/st7DiQP23YQSxumCt/on-doing-the-improbable) and [Anxious Underconfidence](https://www.lesswrong.com/posts/o28fkhcZsBhhgfGjx/status-regulation-and-anxious-underconfidence).\n\nNewer, weirder, and/or more ambitious ideas in particular can often seem too \"out there\" to ever get traction. Then a few years later, *everyone's* talking about UBI. That feeling of anxious uncertainty is not actually a crystal ball; better to try things and see what happens.\n\n### **Ungrounded social realities are fragile**\n\nFrom [*Inadequate Equilibria*](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/PRAyQaiMWg2La7XQy):\n\n> \\[...\\] The still greater force locking bad political systems into place is an equilibrium of silence about policies that aren’t “serious.”\n> \n> A journalist thinks that a candidate who talks about ending the War on Drugs isn’t a “serious candidate.” And the newspaper won’t cover that candidate because the newspaper itself wants to look serious… or they think voters won’t be interested because everyone knows that candidate can’t win, or something? Maybe in a US-style system, only contrarians and other people who lack the social skill of getting along with the System are voting for Carol, so Carol is uncool the same way Velcro is uncool and so are all her policies and ideas? I’m not sure exactly what the journalists are thinking subjectively, since I’m not a journalist. But if an existing politician talks about a policy outside of what journalists think is appealing to voters, the journalists think the politician has committed a gaffe, and they write about this sports blunder by the politician, and the actual voters take their cues from that. So no politician talks about things that a journalist believes it would be a blunder for a politician to talk about. The space of what it isn’t a “blunder” for a politician to talk about is conventionally termed the “Overton window.”\n> \n> \\[...\\]  To name a recent example from the United States, \\[this\\] explains how, one year, gay marriage is this taboo topic, and then all of a sudden there’s a huge upswing in everyone being *allowed* to talk about it for the first time and shortly afterwards it’s a done deal.\n> \n> \\[... W\\]e can say, “An increasing number of people over time thought that gay marriage was pretty much okay. But while that group didn’t have a majority, journalists modeled a gay marriage endorsement as a ‘gaffe’ or ‘unelectable’, something they’d write about in the sports-coverage overtone of a blunder by the other team—”\n> \n> \\[...\\] The support level went over a threshold where somebody tested the waters and got away with it, and journalists began to suspect it wasn’t a political blunder to support gay marriage, which let more politicians speak and get away with it, and then the *change of belief about what was inside the Overton window* snowballed.\n\nAnd:\n\n> What broke the silence about artificial general intelligence (AGI) in 2014 wasn’t Stephen Hawking writing a careful, well-considered [essay](http://www.huffingtonpost.com/stephen-hawking/artificial-intelligence_b_5174265.html) about how this was a real issue. The silence only broke when Elon Musk [tweeted](http://www.theverge.com/2014/8/3/5965099/elon-musk-compares-artificial-intelligence-to-nukes) about Nick Bostrom’s *Superintelligence*, and then made an off-the-cuff remark about how AGI was “[summoning the demon](http://www.independent.co.uk/life-style/gadgets-and-tech/news/tesla-boss-elon-musk-warns-artificial-intelligence-development-is-summoning-the-demon-9819760.html).”\n> \n> Why did that heave a rock through the Overton window, when Stephen Hawking couldn’t? Because Stephen Hawking *sounded like* he was trying hard to appear sober and serious, which signals that this is a subject you have to be careful not to gaffe about. And then Elon Musk was like, “*Whoa, look at that apocalypse over there!!*” After which there was the equivalent of journalists trying to pile on, shouting, “A gaffe! A gaffe! A… gaffe?” and finding out that, in light of recent news stories about AI and in light of Elon Musk’s good reputation, people weren’t backing them up on that gaffe thing.\n> \n> Similarly, to heave a rock through the Overton window on the War on Drugs, what you need is not state propositions (although those do help) or articles in *The Economist*. What you need is for some “serious” politician to say, “This is dumb,” and for the journalists to pile on shouting, “A gaffe! A gaffe… a gaffe?” But it’s a grave personal risk for a politician to test whether the public atmosphere has changed enough, and even if it worked, they’d capture very little of the human benefit for themselves.\n\nThe public health response to COVID-19 has *with surprising consistency* been a loop of:\n\n*   Someone like Marc Lipsitch or Alex Tabarrok gives a good argument for X (e.g., \"the expected value of having everyone wear masks is quite high\").\n*   Lots of people spring up to object, but the objections seem all-over-the-place and none of them seem to make sense.\n*   Time passes, and eventually a prestigious institution endorses X.\n*   Everyone who objected to X now starts confabulating arguments in *support* of X instead.\n\nI think this is a pretty socially normal process. But usually it's a *slow* process. Seeing the Overton window and \"social reality\" change with such blinding speed has helped me better grok this dynamic.\n\nAdditionally, something about this process seems to favor faux certainty and faux obliviousness over expected-value-style thinking.\n\n[Zvi Mowshowitz](https://www.lesswrong.com/posts/cR7Zfrc4BtnFes46y/why-i-am-not-in-charge) discusses a hypothetical where Biden does something politically risky, and suffers so much fallout that it cripples his ability to govern. Then Zvi adds:\n\n> Or at least, that’s The Fear.\n> \n> The Fear does a lot of the work. \n> \n> It’s not that such things would actually definitely happen. It’s that there’s some chance they might happen, and thus no one dares find out.\n> \n> \\[... W\\]hen you’re considering changing from policy X to policy Y, you’re being judged against the standard of policy X, so any change is blameworthy. How irresponsible to propose First Doses First.\n> \n> But…\n> \n> A good test is to ask, when right things are done *on the margin,* what happens? When we move in the direction of good policies or correct statements, how does the media react? How does the public react?\n> \n> This does eventually happen on almost every issue.\n> \n> The answer is almost universally that the change is accepted. The few who track such things praise it, and everyone else decides to memory hole that we ever claimed or advocated anything different. We were always at war with Eastasia. Whatever the official policy is becomes the null action, so it becomes the Very Serious Person line, which also means that anyone challenging that is blameworthy for any losses and doesn’t get credit for any improvements.\n> \n> This is a pure ‘[they’ll like us when we win](https://www.youtube.com/watch?v=S4vU1ygmq-s&ab_channel=bt10ant).’ Everyone’s defending the current actions of the powerful in deference to power. Change what power is doing, and they’ll change what they defend. We see it time and again. Social distancing. Xenophobia. Shutdowns. Masks. Better masks. Airborne transmission. Tests. Vaccines. Various prioritization schemes. First Doses First. Schools shutting down. Schools staying open. New strains. The list goes on. \n> \n> There are two strategies. We can do what we’re doing now, and change elite or popular opinion to change policy. Or we can change policy, and in so doing change opinion leaders and opinion. \n\nThis is not to say that attempts to shift (or route around) the Overton window will necessarily have a high success rate.\n\nBut it is to say:\n\n*   Some of the factors influencing this success rate have the character of a self-fulfilling prophecy. And such prophecies can be surprisingly fragile.\n*   The Overton window *distorts thinking* in ways that can *make it harder to estimate* success rates. The current political consensus just feels like 'what's normal', 'what's reasonable', 'what's acceptable', and the fact that this is in many ways a passing fad is hard to appreciate in one's viscera. (At least, it's hard for me; it might be easy for you.)\n\n(I also happen to suspect that some very recent shifts in politics and culture have begun to make Overton windows easier to break and made \"[PR culture](https://www.lesswrong.com/posts/SWxnP5LZeJzuT3ccd/pr-is-corrosive-reputation-is-not)\" less adaptive to the new climate. But this is a separate empirical claim that would probably need its own post-length treatment.)\n\n### **Steering requires looking at the road**\n\nI claim that politics is too meta.\n\n(I would also say that, e.g., effective altruist discourse spends too much time on meta. And a lot of other discourses besides.)\n\nGoing overboard on meta is bad because:\n\n*   It can create or perpetuate harmful social realities that aren't based in any external reality.\n*   It can create (or focus attention on) social realities that actively encourage false beliefs (including false beliefs that make it harder to break the spell).\n*   It's unnecessary. Often the idea that we need to spend a lot of time on meta is *itself* a self-perpetuating myth, and we can solve problems more effectively by just trying to solve the problem.\n*   It makes it harder to innovate.\n*   It makes it harder to experiment and make high-EV bets.\n\nIt's also bad because it's *distracting*.\n\nDistraction is a lot more awful than it sounds. If you go from spending 0% of your time worrying about the hippopotamus in your bathtub to spending 80% of your time worrying about it, your other life-priorities will probably suffer quite a bit even if the hippo doesn’t end up doing any *direct* damage.\n\nAnd it would be quite bad if 15% of the *cognition in the world* were diverted to hippos.\n\n![](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2971159e-bb1e-46b6-9764-fb3f9669263b_528x463.jpeg)\n\nNote the headlines' framings. The *New York Times* is *creating* a social reality by picking which stories to put on its front page, but all the reporting is also *about* social realities: \"How will this information (that we are here reporting on) change what's controversial, what's popular, what's believed, etc., and thereby affect the election?\"\n\nPresidential elections are a time when it's especially *tempting* to go meta, since we're all so curious about *us*, about how we'll vote. But they're also a time when it's an especially *terrible idea* to go meta, because *our models and behavior are unusually important* at this time and *this is the exact time when we most need to be thinking about the object-level tradeoffs* in order to make a higher-quality decision.\n\nImagine trying to steer a ship by polling the crew members, but then covering over all the ship's windows and doors with real-time polling data.\n\nI don't think the reporters and editors here *feel* like they're doing the \"loudly and publicly warn someone that they're being cringe\" thing, or the \"black out the ship's windows with polling data\" thing. But that's what's happening, somehow.\n\nIf the emails are important in their own right, then great! The headlines and articles can be all about their object-level importance. Help voters make a maximally informed decision about the details of this person's security practices and how these translate into likely future behavior.\n\nHeadlines can focus on object-level details and expert attempts to model individuals' rule-following behavior. You can even do head-to-head comparisons of *object-level differences* about the people running for president!\n\nBut the front-page articles really shouldn't be about the *controversy*, the *buzz*, the second-order perceptions and spin and perceptions-of-spin. The buzz is *built* out of newspaper articles, and you want the resultant building to stand on some foundation; you don't want it to be a free-floating thing built on itself.\n\nThe articles shouldn't drool over the tantalizingly influenceable/predictable beliefs and attitudes *of the people reading the articles*. Internal decisions about the topic's importance (and the angle and focus of the coverage) shouldn't rest on *hypothetical perceptions* of the news coverage. \"It's important because it will affect voters' beliefs because we're reporting that it's important because it will affect voters' b...\"\n\nThis is an extreme example, but I'm mostly worried about the mild examples, because small daily hippos are worse than large rare hippos.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/047c57c0db8735560d6b91832f0761a29936129b6a47bef4.png)\n\n\"A major win for Biden\" here isn't just trying to give credit where credit's due; it's drawing attention to one of the *big interesting things* about this $1.9 trillion bill, which is its effect on Biden's future *popularity* (and thereby, maybe just maybe, the future balance of political power).\n\nThis is certainly *one of the interesting things about bills*, and I could imagine a college class that went through bill after bill and assessed it mainly through the frame of \"who's winning, the Republicans or the Democrats?\", which might teach some interesting things.\n\nBut \"who's winning?\" isn't the *only* topic you could teach in a college course by walking through a large number of historical bills.\n\nSo why has every major news outlet in the US settled on \"who's winning\" as the one true frame for public policy coverage? Is this what we'd pick if we were making a conscious effort to install good norms, or is it just a collective bad habit we've fallen into?\n\nIf you don't look at the road while you're driving, you get worse decision-making, and the polarization ratchet continues, and we all get dumber and [more sports-fan-ish](https://www.lesswrong.com/posts/6hfGNLf4Hg5DXqJCF/a-fable-of-science-and-politics).\n\nMy political views have changed in a big way a few times over the years. In each case, the main things that caused me to update weren't people yelling their high-level principles at me more insistently. I was persuaded by being hit over the head repeatedly with *specific object-level examples* showing I was wrong about which factual claims tend to be true and which tend to be false.\n\nIf you want to produce good outcomes from nations, try arguing more about *actual disagreements* people have. Policy isn't built out of vibes or branding any more than a car engine is.",
      "plaintextDescription": "... i.e., it doesn't spend enough time arguing about object-level things.\n\nThe way I'm using it in this post, \"object-level\" might include these kinds of things:\n\n * While serving as Secretary of State, did Hillary Clinton send classified information to an insecure email server? How large was the risk that attackers might get the information, and how much harm might that cause?\n * What are the costs and benefits of various communications protocols (e.g., the legal one during Clinton's tenure, the de facto one Clinton followed, or other possibilities), and how should we weight those costs and benefits?\n * How can we best forecast people's reliability on security issues? Are once-off mistakes like this predictive of future sloppiness? Are there better ways of predicting this?\n\n\"Meta\" might include things like:\n\n * How much do voters care about Clinton's use of her email server?\n * How much will reporters cover this story, and how much is their coverage likely to influence voters?\n * What do various people (with no special information or expertise) believe about Clinton's email server, and how might these beliefs change their behavior?\n\nI'll also consider discussions of abstract authority, principle, or symbolism more \"meta\" than concrete policy proposals and questions of fact.\n\nThis is too meta:\n\n \n\n \n\nMeta stuff is real. Elections are real, and matter. Popularity, status, controversy, and Overton windows have real physical effects.\n\nBut it's possible to focus too much on one part of reality and neglect another. If you're driving a car while talking on the phone, the phone and your eyes are both perfectly good information channels; but if you allocate too little attention to the road, you still die.\n\n \n\n\nWhen speaking the demon's name creates the demon\nI claim:\n\n> There are many good ideas that start out discussed by blogs and journal articles for a long time, then get adopted by policymakers.\n> \n> In many of these cases, you could delay adoption by many years by addi",
      "wordCount": 3212
    },
    "tags": [
      {
        "_id": "kdbs6xBndPkmrYAxM",
        "name": "Politics",
        "slug": "politics"
      },
      {
        "_id": "gHCNhqxuJq2bZ2akb",
        "name": "Social & Cultural Dynamics",
        "slug": "social-and-cultural-dynamics"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb150",
        "name": "Bystander Effect",
        "slug": "bystander-effect"
      },
      {
        "_id": "DHZhAxgE5edGxxh8d",
        "name": "Object level and Meta level",
        "slug": "object-level-and-meta-level"
      },
      {
        "_id": "YyGDbZhGtws5hEPda",
        "name": "Self Fulfilling/Refuting Prophecies",
        "slug": "self-fulfilling-refuting-prophecies"
      },
      {
        "_id": "2EFq8dJbxKNzforjM",
        "name": "Social Status",
        "slug": "social-status"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb17d",
        "name": "Status Quo Bias",
        "slug": "status-quo-bias"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "8DHocYocCbhpjz4Gt",
    "title": "Deflationism isn't the solution to philosophy's woes",
    "slug": "deflationism-isn-t-the-solution-to-philosophy-s-woes",
    "url": null,
    "baseScore": 64,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 44,
    "createdAt": null,
    "postedAt": "2021-03-10T00:20:07.357Z",
    "contents": {
      "markdown": "\\[epistemic status: thinking out loud; reporting high-level impressions based on a decent amount of data, but my impressions might shift quickly if someone proposed a promising new causal story of what's going on\\]\n\n\\[context warning: If you're a philosopher whose first encounter with LessWrong happens to be this post, you'll probably be very confused and put off by my suggestion that LW outperforms analytic philosophy.\n\nTo that, all I can really say [without](https://www.lesswrong.com/rationality) [starting](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs) [a](https://intelligence.org/2017/04/07/decisions-are-for-making-bad-outcomes-inconsistent/) [very](https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ) [long](https://www.lesswrong.com/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation) [conversation](https://www.lesswrong.com/s/9bvAELWc8y2gYjRav) is: the typical Internet community that compares itself favorably to an academic field will obviously be crazy or stupid. And yet academic fields *can* be dysfunctional, and low-hanging fruit [*can* sometimes go unplucked](https://equilibriabook.com/toc/) for quite a while; so while it makes sense to have a low prior probability on this kind of thing, this kind of claim [*can* be true](https://medium.com/@ThingMaker/its-not-what-it-looks-like-cde2c6104455), and it's important to be able to update toward it (and talk about it) in cases where it does turn out to be true.\n\nThere are [about 6700](https://www.amacad.org/humanities-indicators/profile-philosophy-departments-hds-3) US philosophy faculty, versus about 6000 LessWrong commenters to date; but the philosophy faculty are doing this as their day job, while the LessWrong users are almost all doing it in their off time. So the claim that LW outperforms is *prima facie* interesting, and warrants some explanation.\n\nOK, enough disclaimers.\\]\n\n* * *\n\nA month ago, Chana Messinger said:\n\n> Rob says, \"as an analytic philosopher, I vote for resolving this disagreement by coining different terms with stipulated meanings.\"\n> \n> But I constantly hear people complain that philosophers are failing to distinguish different things they mean by words and if they just disambiguated, so many philosophical issues would be solved, most recently from Sam and Spencer [on Spencer's podcast](https://clearerthinkingpodcast.com/?ep=002).\n> \n> What's going on here? Are philosophers good at this or bad at this? Would disambiguation clear up philosophical disputes?\n> \n> My cards on the table: I understand analytic philosophers to be very into clearly defining their terms, and a lot of what happens in academic philosophy is arguments about which definitions capture which intuitions or have what properties, and how much, but I'm very curious to find out if that's wrong.\n\nSam Rosen replied:\n\n> Philosophers are good at coming up with distinctions. They are not good at saying, “the debate about the true meaning of knowledge is inherently silly; let’s collaboratively map out concept space instead.”\n\nAn edited version of my reply to Chana and Sam:\n\n* * *\n\nAlternative hypothesis: philosophers are OK at saying 'this debate is unimportant'; but...\n\n*   (a) ... if that's your whole view, there's not much to say about it.  \n      \n    Sometimes, philosophers do convince the whole field in one fell swoop. A Bertrand Russell comes along and closes the door on a lot of disputes, and future generations just don't hear about them anymore.  \n      \n    But if you fail to convince enough of your colleagues, then the people who think this is important will just keep publishing about it, while the people who think the debate is dumb will roll their eyes and work on something else. I think philosophers in a given subfield tend to think that a *large* number of the disputes in other subfields are silly and/or unimportant.  \n     \n*   (b) ... there's a culture of *being relaxed*, or something to that effect, in philosophy?  \n      \n    Philosophical fields are fine with playing around with cute conceptual questions, and largely feel no need to move on to more important things when someone gives a kinda-compelling argument for 'this is unimportant'.  \n      \n    Prominent 20th-century philosophers like David Lewis and Peter van Inwagen acquired a lot of their positive reputation from the fact that all their colleagues agreed that their view was *obviously* silly and stupid, but there was some disagreement and subtlety in saying *why* they were wrong, and they proved to be a useful foil for a lot of alternative views. Philosophers don't get nerd-sniped from their more important work; nerd-sniping *just is* the effective measure of philosophical importance.\n\nWe've still ended up with a *large* literature of philosophers arguing that this or that philosophical dispute is non-substantive.\n\nThere's a dizzying variety of different words used for a dizzying variety of different positions to the effect of 'this isn't important' and/or 'this isn't real'.\n\nThere are massive literatures drawing out the fine distinctions between different [deflationary](https://plato.stanford.edu/entries/truth-deflationary/) vs. [anti-realist](https://plato.stanford.edu/entries/moral-anti-realism/) vs. [nominalist](https://plato.stanford.edu/entries/nominalism-metaphysics/) vs. [nihilist](http://tedsider.org/papers/nihilism.pdf) vs. [reductionist](https://plato.stanford.edu/entries/scientific-reduction/) vs. [eliminativist](https://plato.stanford.edu/entries/materialism-eliminative/) vs. [skeptical](http://consc.net/papers/matrix.html) vs. [fictionalist](https://plato.stanford.edu/entries/fictionalism-mathematics/) vs. ... variants of positions.\n\n[Thousands of pages](http://tedsider.org/books/wbotw_sample.pdf?fbclid=IwAR1Her9N__Huave3o9XGpcAzNCFCuI7re4-DvpcN5MJVlUEh7m8duNPz3qM) have been written on 'what makes a dispute merely verbal, vs. substantive? and how do we tell the difference?'. Thousands of journal articles cite Goodman's '[grue and bleen](https://en.wikipedia.org/wiki/New_riddle_of_induction)' (and others discuss Hirsch's '[incar and outcar](https://www.jstor.org/stable/2108478?seq=1)', etc.) as classic encapsulations of the problem 'when are concepts joint-carving, and when are words poorly-fitted to the physical world's natural clusters?'. And then there's the legendary \"[Holes,](http://home.sandiego.edu/~baber/metaphysics/readings/Lewis&Lewis.Holes.pdf)\" written by analytic philosophers for analytic philosophers, satirizing and distilling the well-known rhythm of philosophical debates about which things are fundamental or real vs. derived or illusory.\n\nIt's obviously not that philosophers have never heard of 'what if this dispute isn't substantive?? what if it's merely verbal??'.\n\nThey hear about this constantly. This is one of the most basic and common things they argue about. Analytic philosophers sometimes seem to be trying to one-up each other about *how* deflationary and anti-realist they can be. (See \"[the picture of reality as an amorphous lump](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.472.6002&rep=rep1&type=pdf)\".) Other times, they seem to relish contrarian opportunities to show how metaphysically promiscuous they can be.\n\nI do think LW strikingly outperforms analytic philosophy. But the reason is *definitely not* 'analytic philosophers have literally never considered being more deflationary'.\n\nArguably the big story of 20th-century analytic philosophy is precisely 'folks like the logical positivists and behaviorists and Quineans and ordinary language philosophers express tons of skepticism about whether all these philosophical disputes are substantive, and they end up dominating the landscape for many decades, until in the 1980s the intellectual tide starts turning around'.\n\nNotably, I think the tide was *right* to turn around. I think mid-20th-century philosophers' skepticism (even though it touched on some very LW-y themes!) was coming from a correct place on an intuitive level, but their *arguments* for rejecting metaphysics were total crap. I consider it a *healthy* development that philosophy stopped *prejudicially* rejecting all '[unsciencey](https://www.lesswrong.com/posts/4Bwr6s9dofvqPWakn/science-as-attire)' things, and started demanding better arguments.\n\nWhy does LW outperform analytic philosophy? (Both in terms of having some individuals who have made surprisingly large progress on traditional philosophical questions; and in terms of the community as a whole successfully ending up with a better baseline set of positions and heuristics than you see in analytic philosophy? Taking into account that LW is putting relatively few person-hours into philosophy, many LWers lack formal training in philosophy, etc.)\n\nI suspect it's a few subtler differences.\n\n*   \"[Something to protect](https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect)\" is very much in the water here. It's normal and OK to actually care in your bones about figuring out which topics are unimportant—care in a tangible \"lives are on the line\" sort of way—and to avoid those.  \n      \n    No one will look at you funny if you make big unusual changes to your life to [translate your ideas into practice](https://www.lesswrong.com/tag/taking-ideas-seriously). If you're making ethics a focus area, you're expected to [actually](https://www.npr.org/sections/13.7/2013/01/07/168650666/new-year-s-resolutions-need-more-than-good-intentions) get [better](https://www.tandfonline.com/doi/abs/10.1080/09515089.2019.1587912?journalCode=cphp20&) results, and if you don't, it's not just a cute self-deprecating story to tell at dinner parties.  \n     \n*   LW has a culture of ambition, audacity, and 'rudeness', and historically (going back to Eliezer's sequence posts) there's been an established norm of 'it's socially OK to dive super deep into philosophical debates' *and* 'it's socially OK to totally dismiss and belittle philosophical debates when they seem silly to you'.  \n      \n    I... can't think of another example of a vibrant intellectual community in the last century that made both of those moves 'OK'? And I think this is a pretty damned important combination. You need both moves to be fully available.  \n     \n*   Likewise, LW has a culture of 'we love systematicity and grand Theories of Everything!' *combined* with the high level of skepticism and [fox-ishness](https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox) encouraged in modern science.  \n      \n    There are innumerable communities that have one or the other, but I think the magic comes from the combination of the two, which can keep a community from flanderizing in one direction or the other.  \n     \n*   More specifically, LWers are very into Bayesianism, and this actually matters a hell of a lot.  \n      \n    E.g., I think the lack of a background '[all knowledge requires thermodynamic work](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition)' model in the field explains the popularity of [epiphenomenalism-like](https://www.lesswrong.com/posts/7DmA3yWwa6AT5jFXt/zombies-redacted) views in philosophy of mind.  \n      \n    And again, there are plenty of Bayesians in academic philosophy. There's even [*Good and Real*](https://www.amazon.com/dp/B08N4M8733/), the philosophy book that independently discovered many of the core ideas in the sequences. But the philosophers of mind mostly don't study epistemology in depth, and there isn't a critical mass of 'enough Bayesians in analytic philosophy that they can just talk to each other and build larger edifices everywhere without constantly having to return to 101-level questions about why Bayes is good'.  \n     \n*   This maybe points at an underlying reason that academic philosophy hasn't converged on more right answers: some of those answers require more technical ability than is typically expected in analytic philosophy. So when someone publishes an argument that's pretty conclusive, but requires strong technical understanding and well-honed formal intuitions, it's a lot more likely the argument will go ignored, or will take decades (rather than months) to change minds. More subtly, the kinds of *questions* and *interests* that shape the field are ones that are (or seem!!) easier to tackle without technical intuitions and tools.  \n      \n    Ten years ago, Marcus Hutter [made a focused effort](https://arxiv.org/pdf/1105.5721.pdf) to bring philosophers up to speed on Solomonoff induction and AIXI. But his paper has only been cited 96 times (including self-citations and citations by EAs and non-philosophers), while Schaffer's 2010 paper on [whether wholes are metaphysically prior to their parts](https://read.dukeupress.edu/the-philosophical-review/article-abstract/119/1/31/2871) has racked up 808 citations. This seems to reflect a clear blind spot.  \n     \n*   A meta-explanation: LW was founded by damned good thinkers like Eliezer, Anna, Luke M, and Scott who (a) had lots of freedom to build a new culture from scratch (since they were just casually sharing thoughts with other readers of the same blog, not trying to win games within academia's existing norms), and (b) were smart enough to pick a pretty damned good mix of norms.  \n      \n    I don't think it's a *coincidence* that all these good things came together at once. I think there was deliberate reflection about what good thinking-norms and discussion-norms look like, and I think this reflection paid off in spades.  \n      \n    I think you can get an awful lot of the way toward understanding the discrepancy by just positing that communities try to emulate their heroes, and Anna is a better hero than Leibniz or Kant (if only by virtue of being more recent and therefore being able to build on better edifices of knowledge), *and* unlike most recent philosophical heroes, LW's heroes were irreverent and status-blind enough to create something closer to a clean break with the errors of past philosophy, keeping the good while thoroughly shunning and stigmatizing the clearly-bad stuff. Otherwise it's too easy for any community that drinks deeply of the good stuff in analytic philosophy to end up imbibing the bad memes too, and recapitulate the things that make analytic philosophy miss the mark pretty often.\n\nWeirdly, when I imagine interventions that could [help philosophy along](https://www.lesswrong.com/posts/3Lyki5DCHnJgeNXww/what-i-d-change-about-different-philosophy-fields), I feel like philosophy's mild academic style gets in the way?\n\nWhen I think about why LW was able to quickly update toward good decision-theory methods and views, I think of posts like \"[Newcomb's Problem and Regret of Rationality](https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality)\" that sort of served as a kick in the pants, an emotional reminder \"hold on, this line of thinking is totally bonkers.\" The shortness and informality is *good*, not just for helping system 1 sit up and pay attention, but for encouraging focus on a simple stand-alone argument that's *agnostic* to the extra theory and details you could then tack on.\n\nAbsent some carefully aimed kicks in the pants, people are mostly happy and content to stick with the easy, cognitively natural grooves human minds find themselves falling into.\n\nOf course, if you just dial up emotional kicks in the pants to 11, you end up with Twitter culture, not LW. So this seems like another smart-founder effect to me: it's important that smart self-aware people chose *very specific things to carefully and judiciously kick each other in the pants over*.\n\n(The fact that LW is a small community surely helps when it comes to not being Twitter. Larger communities are more vulnerable to ideas getting watered down and/or viral-ized.)\n\nCompare Eliezer's comically uncomplicated \"RATIONALISTS SHOULD WIN\" argument to the [mild-mannered analytic-philosophy version](https://static1.squarespace.com/static/56d45b30f699bb6f0beeaf5a/t/5abc9d0e88251b734ff195d4/1522310416943/Greene%2C+Success-First+Decision+Theories.pdf).\n\n(Which covers a lot of other interesting topics! But it's not clear to me that this has caused a course-correction yet. And the field's course-correction should have occurred in 2008–2009, at the latest, not 2018.)\n\n(Also, I hear that the latter paper was written by someone socially adjacent to the rationalists? And they cite MIRI papers. So I guess this progress also might not have happened without LW.)\n\n(Also, Greene's paper of course isn't the first example of an analytic philosopher calling for something like \"success-first decision theory\". As the paper notes, this tradition has a long history. I'm not concerned with priority here; my point in comparing Greene's paper to Eliezer's blog post is to speak to the sociological question of why, in this case, a community of professionals is converging on truth so much more slowly than a community of mostly-hobbyists.)\n\nMy story is sort of a Thiel-style capitalist account. It was hard to get your philosophy published and widely read/discussed except via academia. But academia had a lot of dysfunction that made it hard to innovate and change minds within that bad system.\n\nThe Internet and blogging made it much easier to compete with philosophers; a mountain of different blogs popped up; one happened to have a few unusually good founders; and once their stuff was out there and could compete, a lot of smart people realized it made more sense.\n\nLW *is* academic philosophy, rebooted with better people than Plato as its Pater Patriae.",
      "plaintextDescription": "[epistemic status: thinking out loud; reporting high-level impressions based on a decent amount of data, but my impressions might shift quickly if someone proposed a promising new causal story of what's going on]\n\n[context warning: If you're a philosopher whose first encounter with LessWrong happens to be this post, you'll probably be very confused and put off by my suggestion that LW outperforms analytic philosophy.\n\nTo that, all I can really say without starting a very long conversation is: the typical Internet community that compares itself favorably to an academic field will obviously be crazy or stupid. And yet academic fields can be dysfunctional, and low-hanging fruit can sometimes go unplucked for quite a while; so while it makes sense to have a low prior probability on this kind of thing, this kind of claim can be true, and it's important to be able to update toward it (and talk about it) in cases where it does turn out to be true.\n\nThere are about 6700 US philosophy faculty, versus about 6000 LessWrong commenters to date; but the philosophy faculty are doing this as their day job, while the LessWrong users are almost all doing it in their off time. So the claim that LW outperforms is prima facie interesting, and warrants some explanation.\n\nOK, enough disclaimers.]\n\n----------------------------------------\n\nA month ago, Chana Messinger said:\n\n> Rob says, \"as an analytic philosopher, I vote for resolving this disagreement by coining different terms with stipulated meanings.\"\n> \n> But I constantly hear people complain that philosophers are failing to distinguish different things they mean by words and if they just disambiguated, so many philosophical issues would be solved, most recently from Sam and Spencer on Spencer's podcast.\n> \n> What's going on here? Are philosophers good at this or bad at this? Would disambiguation clear up philosophical disputes?\n> \n> My cards on the table: I understand analytic philosophers to be very into clearly defining their term",
      "wordCount": 2514
    },
    "tags": [
      {
        "_id": "kCrvmjZhsAZANnZL6",
        "name": "Philosophy",
        "slug": "philosophy-1"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3Lyki5DCHnJgeNXww",
    "title": "What I'd change about different philosophy fields",
    "slug": "what-i-d-change-about-different-philosophy-fields",
    "url": null,
    "baseScore": 56,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 52,
    "createdAt": null,
    "postedAt": "2021-03-08T18:25:30.165Z",
    "contents": {
      "markdown": "\\[epistemic status: speculative conversation-starter\\]\n\nMy guess at the memetic shifts that would do the most to improve these philosophy fields' tendency to converge on truth:\n\n* * *\n\n### metaphysics\n\n1\\. Make reductive, 'third-person' models of the brain central to metaphysics discussion.\n\nIf you claim that humans can come to know X, then you should be able to provide a story sketch in the third person for how a physical, deterministic, evolved organism could end up learning X.\n\nYou don't have to go into exact neuroscientific detail, but it should be clear how a mechanistic [cause-and-effect chain](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition) could result in a toy agent verifying the truth of X within a physical universe.\n\n2\\. Care less about human intuitions and concepts. Care more about the actual subject matter of metaphysics — ultimate, objective reality. E.g., only care about the concept 'truth' insofar as we have strong reason to think an alien would arrive at the exact same concept, because it's [carving nature closer to its joints](https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace).\n\nConduct more *tests* to see which concepts look more joint-carving than others.\n\n(I think current analytic metaphysics is actually better at 'not caring about human intuitions and concepts' than most other philosophy fields. I just think this is still the field's biggest area for future improvement, partly because it's harder to do this right in metaphysics.)\n\n* * *\n\n### decision theory\n\nSimilar to metaphysics, it should be more expected that we think of decision theories in third-person terms. Can we build toy models of a hypothetical alien or robot that actually implements this decision procedure?\n\nIn metaphysics, doing this helps us confirm that a claim is coherent and knowable. In decision theory, there's an even larger benefit: a lot of issues that are central to the field (e.g., logical uncertainty and counterlogicals) are easy to miss if you stay in fuzzy-human-intuitions land.\n\nMuch more so than in metaphysics, 'adopting a mechanistic, psychological perspective' in decision theory should often involve actual software experiments with different proposed algorithms — not because decision theory is only concerned with algorithms (it's fine for the field to care more about human decision-making than about AI decision-making), but because the algorithms are the gold standard for clarifying and testing claims.\n\n(There have been lots of cases where decision theorists went awry because they under-specified a problem or procedure. E.g., the smoking lesion problem really needs a detailed unpacking of what step-by-step procedure the agent follows, and how 'dispositions to smoke' affect that procedure.)\n\n* * *\n\n### philosophy of mind (+ phenomenology)\n\n1\\. Be very explicit about the fact that 'we have immediate epistemic access to things we know for certain' is a contentious, confusing hypothesis. Note the obvious difficulties with making this claim make sense in any physical reasoning system. Try to make sense of it in third-person models.\n\nInvestigate the claim thoroughly, and try to figure out how a hypothetical physical agent could update toward or away from it, if the agent was initially uncertain or mistaken about whether it possesses infallible direct epistemic access to things.\n\nBe explicit about which other claims rely on the 'we have infallible immediate epistemic access' claim.\n\n2\\. More generally, make philosophy of mind heavily the same field as epistemology.\n\nThe most important questions in these two fields overlap quite a bit, and it's hard to make sense of philosophy of mind without spending half (or more) of your time on developing a background account of how we come to know things. Additionally, I'd expect the field of epistemology to be much healthier if it spent less time developing theory, and more time *applying* theories and reporting back about how they perform in practice.\n\n* * *\n\n### philosophy of religion\n\n1\\. Shift the model from 'scholasticism' to 'missionary work'. The key thing isn't to converge with people who already 99% agree with you. Almost all effort should instead go into debating people with wildly different religious views (e.g., Christianity vs. Buddhism) and debating with the nonreligious. Optimize for departments' intellectual diversity and interdisciplinary bridge-building.\n\nDivide philosophy of religion into 'universal consensus-seeking' (which is about debating the most important foundational assumptions of various religions with people of other faiths, with a large focus on adversarial collaborations and 101-level arguments) and 'non-universal-consensus studies' (which includes everything else, and is mostly marginalized and not given focus in the field).\n\n2\\. Discourage talking about 'religions' or 'faiths'; instead, talk about specific claims/hypotheses. Rename the field 'philosophy of religious claims', if that helps.\n\nWhen we say 'religion', (a) it creates the false impression that claims must be a package deal, so we can't incrementally update toward one specific claim without swallowing the entire package; and (b) it encourages people to think of claims like theism in community-ish or institution-ish terms, rather than in hypothesis-ish terms.\n\nChristianity is not a default it's fine to assume; Christianity is a controversial hypothesis which most religious and secular authorities in the world reject. Christian philosophers need to move fast, as if their hair's on fire. The rival camps need to fight it out *now* and converge on which hypothesis is right, exactly like if there were a massive scientific controversy about which of twenty competing models of photosynthesis were true.\n\nConsider popularizing this thought experiment:\n\n> \"Imagine that we'd all suddenly been plopped on Earth with no memories, and had all these holy texts to evaluate. We only have three months to figure out which, if any, is correct. What would you spend the next three months doing?\"\n\nThis creates some urgency, and also discourages complacency of the 'well, this has been debated for millennia, surely little old me can't possibly resolve all of this overnight' variety.\n\nEternal souls are at stake! People are dying every day! Until very recently, religious scholarship was almost uniformly shit! Assuming you can't possibly crack this open is lunacy.\n\n* * *\n\n### ethics + value theory\n\n1\\. Accept as a foundational conclusion of the field, 'human values seem incredibly complicated and messy; they're a giant evolved stew of competing preferences, attitudes, and feelings, not the kind of thing that can be captured in any short simple ruleset (though different rulesets can certainly perform better or worse *as simplified idealizations*).'\n\n2\\. Stop thinking of the project of ethics as 'figure out which simple theory is True'.\n\nStart instead thinking of ethics as a project of trying to piece together *psychological* models of this insanely complicated and messy thing, 'human morality'.\n\nBinding exceptionless commitments matter to understanding this complicated thing; folk concepts like courage and honesty and generosity matter; taboo tradeoffs and difficult attempts to quantify, aggregate, and weigh relative well-being matter.\n\nStop picking a 'side' and then losing all interest in the parts of human morality that aren't associated with your 'side': these are all just parts of the stew, and we need to work hard to understand them and reconcile them just right, not sort ourselves into Team Virtue vs. Team Utility vs. Team Duty.\n\n(At least, stop picking a side at that level of granularity! Biologists have long-standing controversies, but they don't look like 'Which of these three kinds of animal exists: birds, amphibians, or mammals?')\n\n3\\. Once again, apply the reductive third-person lens to everything. 'If it's true that X is moral, how could a mechanistic robot learn that truth? What would \"X is moral\" have to *mean* in order for a cause-and-effect process to result in the robot discovering that this claim is true?'\n\n4\\. Care less about the distinction between 'moral values' and other human values. There are certainly some distinguishing features, but these mostly aren't incredibly important or deep or joint-carving. *In practice*, it works better to freely bring in insights from the study of beauty, humor, self-interest, etc. rather than lopping off one slightly-arbitrary chunk of a larger natural phenomenon.",
      "plaintextDescription": "[epistemic status: speculative conversation-starter]\n\n \n\nMy guess at the memetic shifts that would do the most to improve these philosophy fields' tendency to converge on truth:\n\n----------------------------------------\n\n\nmetaphysics\n1. Make reductive, 'third-person' models of the brain central to metaphysics discussion.\n\nIf you claim that humans can come to know X, then you should be able to provide a story sketch in the third person for how a physical, deterministic, evolved organism could end up learning X.\n\nYou don't have to go into exact neuroscientific detail, but it should be clear how a mechanistic cause-and-effect chain could result in a toy agent verifying the truth of X within a physical universe.\n\n2. Care less about human intuitions and concepts. Care more about the actual subject matter of metaphysics — ultimate, objective reality. E.g., only care about the concept 'truth' insofar as we have strong reason to think an alien would arrive at the exact same concept, because it's carving nature closer to its joints.\n\nConduct more tests to see which concepts look more joint-carving than others.\n\n(I think current analytic metaphysics is actually better at 'not caring about human intuitions and concepts' than most other philosophy fields. I just think this is still the field's biggest area for future improvement, partly because it's harder to do this right in metaphysics.)\n\n----------------------------------------\n\n\ndecision theory\nSimilar to metaphysics, it should be more expected that we think of decision theories in third-person terms. Can we build toy models of a hypothetical alien or robot that actually implements this decision procedure?\n\nIn metaphysics, doing this helps us confirm that a claim is coherent and knowable. In decision theory, there's an even larger benefit: a lot of issues that are central to the field (e.g., logical uncertainty and counterlogicals) are easy to miss if you stay in fuzzy-human-intuitions land.\n\nMuch more so than in metaphysic",
      "wordCount": 1260
    },
    "tags": [
      {
        "_id": "kCrvmjZhsAZANnZL6",
        "name": "Philosophy",
        "slug": "philosophy-1"
      },
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      },
      {
        "_id": "EdRnMXBRbY5JDf5df",
        "name": "Epistemology",
        "slug": "epistemology"
      },
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "xeqCTxje765k79Q78",
        "name": "Phenomenology",
        "slug": "phenomenology"
      },
      {
        "_id": "6wzZZfW87aKGQ7Fwr",
        "name": "Reflective Reasoning",
        "slug": "reflective-reasoning"
      },
      {
        "_id": "NSMKfa8emSbGNXRKD",
        "name": "Religion",
        "slug": "religion"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "AyfDnnAdjG7HHeD3d",
    "title": "MIRI comments on Cotra's \"Case for Aligning Narrowly Superhuman Models\"",
    "slug": "miri-comments-on-cotra-s-case-for-aligning-narrowly",
    "url": null,
    "baseScore": 145,
    "voteCount": 52,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2021-03-05T23:43:54.186Z",
    "contents": {
      "markdown": "Below, I’ve copied comments left by MIRI researchers Eliezer Yudkowsky and Evan Hubinger on March 1–3 on a draft of Ajeya Cotra’s \"[Case for Aligning Narrowly Superhuman Models](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models).\" I've included back-and-forths with Cotra, and interjections by me and Rohin Shah.\n\nThe section divisions below correspond to the sections in [Cotra's post](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models).\n\n0\\. [Introduction](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models)\n----------------------------------------------------------------------------------------------------------------------\n\n> *How can we train GPT-3 to give “the best health advice it can give” using demonstrations and/or feedback from humans who may in some sense “understand less” about what to do when you’re sick than GPT-3 does?* \n\n**Eliezer Yudkowsky:** I've had some related conversations with Nick Beckstead.  I'd be hopeful about this line of work primarily because I think it points to a bigger problem with the inscrutable matrices of floating-point numbers, namely, we have no idea what the hell GPT-3 is thinking and cannot tell it to think anything else.  GPT-3 has a great store of medical knowledge, but we do not know where that medical knowledge is; we do not know how to tell it to internally apply its medical knowledge rather than applying other cognitive patterns it has stored.  If this is still the state of opacity of AGI come superhuman capabilities, we are all immediately dead.  So I would be relatively more hopeful about any avenue of attack for this problem that used anything other than an end-to-end black box - anything that started to address, \"Well, this system clearly has a bunch of medical knowledge **internally**, can we find that knowledge and cause it to actually be applied\" rather than \"What external forces can we apply to this solid black box to make it think more about healthcare?\"\n\n**Evan Hubinger:** +1 I continue to think that language model transparency research is the single most valuable current research direction within the class of standard ML research, for similar reasons to what Eliezer said above.\n\n**Ajeya Cotra:** Thanks! I'm also excited about language model transparency, and would love to find ways to make it more tractable as a research statement / organizing question for a field. I'm not personally excited about the connotations of transparency because it evokes the neuroscience-y interpretability tools, which don't feel scalable to situations when we don't get the concepts the model is using, and I'm very interested in finding slogans to keep researchers focused on the superhuman stuff.\n\n**Ajeya Cotra:** I've edited the description of the challenge to emphasize human feedback less. It now reads \"How can we get GPT-3 to give “the best health advice it can give” when humans in some sense “understand less” about what to do when you’re sick than GPT-3 does? And in that regime, how can we even tell/verify that it’s “doing the best it can”?\"\n\n**Rob Bensinger:** Nate and I tend to talk about \"understandability\" instead of \"transparency\" exactly because we don't want to sound like we're talking about normal ML transparency work.\n\n**Eliezer Yudkowsky:** Other possible synonyms:  Clarity, legibility, cognitive readability.\n\n**Ajeya Cotra:** Thanks all -- I like the project of trying to come up with a good handle for the kind of language model transparency we're excited about (and have talked to Nick, Evan, etc about it too) but I think I don't want to push it in this blog post right now because I haven't hit on something I believe in and I want to ship this.\n\n> In the end, we probably want to find ways to meaningfully supervise (or justifiably trust) models that are more capable than ~all humans in ~all domains.\n\n**Eliezer Yudkowsky:** (I think you want an AGI that is superhuman in engineering domains and infrahuman in [human-modeling-and-manipulation](https://www.lesswrong.com/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models) if such a thing is at all possible.)\n\n**Ajeya Cotra:** Fair point, added a footnote:\n\n“Though if we could pull off a path where we build an AI system that is superhuman in certain engineering capabilities but not yet human-level in modeling and manipulating people, and use that system to cut down on x-risk from other AI projects without having to figure out how to supervise arbitrary superhuman models, that could be really good.”\n\n1\\. [What aligning narrowly superhuman models could look like](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#What_aligning_narrowly_superhuman_models_could_look_like)\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n> First of all, it’s important to note that not all narrowly superhuman models are going to be equally interesting as alignment case studies. AlphaGoZero (AGZ) is narrowly superhuman in an extremely strong sense: it not only makes Go moves better than the moves made by top human players, but also probably makes moves that top players couldn’t even reliably *recognize* as good. But there isn’t really an alignment problem for Go: a precise, algorithmically-generated training signal (the win/loss signal) is capable of eliciting the “full Go-playing potential” of AGZ given enough training, and would keep working even as the model got much bigger.\n\n**Eliezer Yudkowsky:** Having access to an incorruptible ground truth solves some of the alignment problems but not all of them, in particular [inner alignment problems](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB).  In the limit of optimizing infinitely hard on [logical Tic-Tac-Toe](https://arbital.com/p/logical_game/), it won't kill you because it hits a capability bound early and stops; in the limit of optimizing infinitely hard on any real-world problem, there's no capability bound lower than extreme superintelligence so the thing inside keeps getting smarter and kills you.  It is not obvious to me where logical Go falls on this spectrum, or rather, it is obvious to me that the answer is \"it depends on the outer optimization method\".  (That is, some ways of optimizing a system to play Go will create an inner optimizer that plays Go and that will kill you; some ways might create a system that just played arbitrarily good Go.)\n\n**Ajeya Cotra:** Good point, changed to:\n\n\"But there isn’t really an outer alignment problem for Go: a precise, algorithmically-generated training signal (the win/loss signal) is capable of eliciting the “full Go-playing potential” of AGZ given enough training, and would keep working even as the model got much bigger (although at a certain scale inner alignment issues may crop up).\"\n\n> *Choose a helpful “fuzzy” task (e.g. summarization, question-answering, advice-giving, story-writing) for which we have suggestive evidence that makes us suspect a state-of-the-art model has the capacity to significantly outperform some reference set of humans (e.g. Mechanical Turk workers) given the right training signal. Then,*\n\n**Eliezer Yudkowsky:** I'd feel more cheerful about an open call \"Yo, try to do something about the fact that text transformers seem like they in some sense contain the capability to solve this problem but we can't make them use that capability to \\[do\\] what we want\" than \"Yo, retrain GPT-3 to do this using an outer training signal in a way that scales well with task and model complexity\".  The latter call is tuned to one particular approach to the first problem, which some, such as myself, would worry is not the most promising approach in the long run.\n\n**Evan Hubinger:** +1 I think this is very similar to my objection that a focus on solving downstream tasks is less likely to translate into important insights than focusing on the core problem directly—though having talked with Ajeya in the comment thread at the bottom, I think I'm now convinced that the alternative pitch of “get language models to actually answer questions honestly to the best of their ability” also has some problems getting people to work on the important part of the problem.\n\n**Ajeya Cotra:** Added some language to the bottom of this section in purple to emphasize that this is just one formulation.\n\nI want a concrete formulation and specific project ideas in this post, even if they're not the best ones, but I agree that the broader question of \"What the hell do we do about situations where models can do stuff to help us but don't want to?\" is where the focus should be.\n\n### 1.2. [What kinds of projects do and don’t “count”](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#What_kinds_of_projects_do_and_don_t__count_)\n\n> I think that if you can get the model to achieve superhuman performance at some task without collecting any human feedback or human demonstrations, the task is probably not “fuzzy” enough.\n\n**Eliezer Yudkowsky:** What we have with GPT-3 is a case of a clear outer optimization, \"predict the next character\", which creates a wide-ranging variety of inner knowledge that seems like it could in principle be useful for many many tasks; but we can't directly make GPT-3 use that knowledge for other tasks, because we have no idea what the hell GPT-3 is thinking or how to make it think anything in particular.  Instead we have to do elaborate dances to shoehorn our task into the shape of the predicted next character, i.e., prompt engineering.  If you pick a new mechanical task with no humans in the loop and a clear outer loss function, it doesn't force you to confront any interesting part of this problem - we already know how to pretrain and retrain nets.\n\nThings you do with humans in the loop can in principle also be boringly straightforward.  But humans in the loop are expensive; so this potentially forces you to do something more data-efficient than usual, and find an efficient way to apply leverage.  (This is how I'd interpret \"[Learning to summarize](https://openai.com/blog/learning-to-summarize-with-human-feedback/).\"  It didn't poke at the internals, but it at least found a higher-leverage way to apply external pressures.)\n\n**Ajeya Cotra:** I agree with what you've said -- I can't tell if your comment here was \"I agree and here's some elaboration\", or if it's objecting to the way I've framed it here?\n\n**Eliezer Yudkowsky:** Something like, \"I might have phrased that differently\".  I don't see a simple change of local phrasing that fits with the rest of the message in context; my inner editor worries that the message sounds here like \"Go make it wantonly fuzzy to impress me\" rather than \"Here is why going for the easy clarity is a bad sign.\"\n\n### 1.3. [Potential near-future projects: “sandwiching”](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Potential_near_future_projects___sandwiching_)\n\n> In all of these cases, my guess is that the way to get the less-capable group of humans to provide training signals of a similar quality to the more-capable group will involve some combination of:\n\n**Eliezer Yudkowsky:** In some ways this strikes me as a much **more** ambitious project than getting GPT-3 to cough up what it actually knows about health.  You're trying to make the system perform \"better\" than its training data, in a certain sense.  This focuses on some interesting parts of a capability problem.  It potentially leads into a much more important alignment problem:  \"Given that humans can be fooled by some inputs, how do you operate a more intelligent input-finder without that fooling the humans?\", where we could potentially test this by building a system that didn't fool easy-to-fool humans, while the capability levels are still too low to fool harder-to-fool humans, which gives us a possible check on whether the methodology actually succeeded, so long as we don't try too many times or try with an overly capable system.\n\nBut this all strikes me as a **different** project than getting better transparency and steering into a system that has only learned things implicit in the training data.  Of course it may be that the solutions to both problems end up related, somehow, but their outer form at least looks different.\n\nThe different problem is potentially interesting in its own right, if its solution forces a non-black-box system that is then more transparent or steerable (but note that you are basically asking for a capability solution and praying it will be more alignable); or the variant problem of \"How do you make a system not fool easy-to-fool humans, in a domain where there are hard-to-fool humans to check the answer\" (and the system is not an AGI yet and does not anticipate the existence of the hard-to-fool humans checking its answers &c).\n\n**Ajeya Cotra:** I think there's room for both types of work in this umbrella. I'm mostly trying here to create a slogan that points researchers at the exciting types of work within ML, so I think outer alignment issues and figuring out how to get humans who are less-foolable (which I've focused on a lot in this post) and inner alignment stuff / transparency could fit under this umbrella. I've emphasized the less-foolable humans stuff here because I think there's something juicily concrete about the problem statement, and it's very easy to tell if you've made progress.\n\n2\\. [How this work could reduce long-term AI x-risk](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#How_this_work_could_reduce_long_term_AI_x_risk)\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n> On the outside view, I think we should be quite excited about opportunities to get experience with the sort of thing we want to eventually be good at (aligning models that are smarter than humans). In general, it seems to me like building and iterating on prototypes is a huge part of how R&D progress is made in engineering fields, and it would be exciting if AI alignment could move in that direction. \n> \n> If there are a large number of well-motivated researchers pushing forward on making narrowly superhuman models as helpful as possible, we improve the odds that we first encounter serious problems [like the treacherous turn](https://docs.google.com/document/d/1-yZxsq-EVvdF4o2bTUZdD9V0HDl7r-csBjwZL5rDJos/edit#heading=h.n1dvphmxr01p) in a context where a) models are not smart enough to cause actually catastrophic harm yet, and b) researchers have the time and inclination to really study them and figure out how to solve them well rather than being in a mode of scrambling to put out fires and watching their backs for competitors. Holistically, this seems like a much safer situation to be in than one where the world has essentially procrastinated on figuring out how to align systems to fuzzy goals, doing only the minimum necessary to produce commercial products.\n\n**Eliezer Yudkowsky:** So as not to speak disagreements only, I remark that I agree with these two paragraphs.  I worry about how very far underwater we are on the [logistic success curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/), here, but that doesn't mean we should not throw resources at any hope of starting to swim (upward).\n\n**Ajeya Cotra:** Thanks -- honestly I felt like your comments were radically more positive than I was expecting overall, not just this one.\n\n> **Chance of discovering or verifying long-term solution(s):** I’m not sure whether a “one shot” solution to alignment (that is, a single relatively “clean” algorithm which will work at all scales including for highly superintelligent models) is possible. But if it is, it seems like starting to do a lot of work on aligning narrowly superhuman models probably allows us to discover the right solution sooner than we otherwise would have.\n\n**Eliezer Yudkowsky:** It's not possible.  Not for us, anyways.  A textbook that fell out of a wormhole from the future might have the simplest straightforward working solution with no extra gears, all of whose pieces work reliably.  We won't get it in time because it takes multiple decades to go from sigmoid activation functions to ReLUs, and so we will definitely be working with the AGI equivalent of sigmoid activation functions instead of ReLUs while the world is ending.  Hope that answers your question!\n\n> It also seems plausible that a solution will emerge directly from this line of work rather than the conceptual work -- the latter is mostly focused on finding a one-shot solution *that will work under ~pessimal empirical assumptions,*\n\n**Eliezer Yudkowsky:** \"Pessimal\" is a strange word to use for this apt description of humanity's entire experience with ML to date.  Unless by \"generalize\" you mean \"generalize correctly to one new example from the same distribution\" rather than \"generalize the underlying concept that a human would\".\n\n**Ajeya Cotra:** I used \"pessimal\" here in the technical sense that it's assuming if there are N generalizations equally valid on the training distribution the model will pick the one which is worst for humans. Even if there's a very high probability that the worst one is in fact picked, assuming the worst one will be picked is still \"assuming the worst case.\"\n\nFwiw, I know lots of ML alignment researchers who would not agree this is highly likely and e.g. point to lots of \"human-like\" generalization going on in neural networks that makes them hopeful about the empirical situation. I haven't found those arguments super compelling but am generally in a state of uncertainty rather than confidence one way or another.\n\nI recognize this is the sort of thing MIRI has been arguing with other people about for ages though, and I don't have an inside view here, so it probably doesn't make sense to have this discussion in the comments.\n\n3\\. [Advantages over other genres of alignment research](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Advantages_over_other_genres_of_alignment_research)\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n> I’m broadly supportive of all three of these other lines of work, but none of these approaches come as close to “practicing the thing we eventually want to be good at” as aligning narrowly superhuman models does.\n\n**Eliezer Yudkowsky:** I remark that it seems to me that work like \"[Learning to summarize](https://openai.com/blog/learning-to-summarize-with-human-feedback/)\", plus this, falls into a subcategory that's much closer to the subcategories of \"reliability+robustness\" and the subcategory \"interpretability\", than to gridworlds; and in turn, the category of gridworlds is much closer to that aforesaid supercategory than either of those categories is to \"conceptual research\".\n\n4\\. [Objections and responses](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Objections_and_responses)\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\n\n### 4.1. [How would this address treachery by a superintelligence?](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#How_would_this_address_treachery_by_a_superintelligence_)\n\n> I'm very uncertain how relevant the near-term work will turn out to be for more exotic problems like the treacherous turn, and I want to think more about ways to nudge it to be more relevant.\n\n**Eliezer Yudkowsky:** Having some idea of what the hell the system was thinking internally might be one good start.  It's not a one-shot solution, but you'd at least get a couple of warning lights before you went on ahead anyways and died.\n\nTo be more precise, I'd imagine the warning lights shut off at a higher capability level if that capability level is \"smart enough to conceal own thoughts from transparency mechanisms, without that intention itself lighting up in the transparency mechanisms\" instead of \"smart enough to socially manipulate operators through outer acts and shape the quoted inner intentions that operators infer from acts\".  Plausibly the former case results in everybody dying because surrounding arms race conditions caused people to press ahead ignoring warning signs; instead of everybody dying because they were wholly ignorant of the dangerous thoughts their AGI was thinking, and not especially focused on the missing knowledge.  Trying to die in a more dignified and self-aware fashion can be a first step in climbing the [logistic success curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/) towards eventually surviving, or at least that would be the optimistic take.\n\n**Ajeya Cotra:** I added a footnote about this idea, and also mentioned it in my description of your views. I am interested in it but I don't think I'm the right person to speak for this idea since I don't totally \"get it\" or have good inside view project ideas. I'd be excited for you or Evan to leave a comment on the blog post discussing this.\n\n\\[Added footnote: “One idea a couple of others have suggested here and which I’m generally interested in is ‘transparency in (narrowly superhuman) language models’: finding ways to understand ‘what models are thinking and why,’ especially when they know more about something than humans do. I like this idea but am very unsure about what execution could look like. E.g., would it look like [Chris Olah’s work](https://openai.com/blog/microscope/), which essentially ‘does neuroscience’ on neural networks? Would it look like training models to answer our questions about what they’re thinking? Something else?”\\]\n\n**Evan Hubinger:** I think that if we're in a situation where our model is actively trying to trick our transparency tools, we've already lost—but that we can potentially use transparency tools as a mechanism to understand what's happening during training enough to avoid ever entering that regime. Thus, I think current transparency research is likely to be useful because I think it will eventually enable us to build better training processes that avoid deception, rather than helping us detect deception directly. My guess is that that's sort of what Eliezer is saying in terms of knowing what's happening being a step towards preventing it, though I'm not sure I fully understand Eliezer's beliefs here.\n\n**Ajeya Cotra:** That's a good point re using transparency to create better training processes, and makes me more positive.\n\n**Rohin Shah:** The way I've summarized this point previously is that if our source of feedback can see how the model is reasoning, then we can select for models that get the right answer for the right reason, and not just models that get the right answer. One would hope that this selects heavily against treachery.\n\n**Ajeya Cotra:** Well, I'm kind of wary of selecting for looking good on the transparency tools we currently have available without more information, tbc: I'd rather set things up so the transparency tools themselves are not under optimization pressure.\n\n**Evan Hubinger:** I agree that ideally we wouldn't want to be applying optimization pressure against our transparency tools since we risk Goodharting on them, but imo I think this is a very manageable risk. I expect our training processes (e.g. SGD) to be dumb enough that a good overseer can mostly avoid being tricked by them, but I don't expect the same thing to be true about the model being overseen—that is, if you have to fight against SGD trying to trick you, that's probably fine, but if you have to fight against a deceptive model trying to trick you, that's probably not.\n\n> Though you could think that in an absolute sense it and all the other approaches that aren’t tackling treachery head-on are doomed.\n\n**Eliezer Yudkowsky:** Doomed, I tell you!  Doomed!\n\n### 4.3. [Isn’t this not neglected because lots of people want useful AI?](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Isn_t_this_not_neglected_because_lots_of_people_want_useful_AI_)\n\n**Eliezer Yudkowsky:** Strong agree with this entire section (as it appeared at the time this comment was written).\n\n\\[Editor’s note: this section is more or less completely unchanged.\\]\n\n5\\. [Current state of opinion on this work](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Current_state_of_opinion_on_this_work)\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n> I think most researchers at MIRI probably feel that this work is very unpromising or net-harmful -- I don’t totally understand their full reasoning, but my sense is that a strong version of [the treachery objection](https://docs.google.com/document/d/1-yZxsq-EVvdF4o2bTUZdD9V0HDl7r-csBjwZL5rDJos/edit#heading=h.n1dvphmxr01p) plays a part (although they might frame it differently in important ways). But MIRI’s view on virtually all non-MIRI research is that it’s near-useless, and if anything Eliezer personally seems maybe [marginally more positive](https://twitter.com/ESYudkowsky/status/1301954347933208578) on this type of thing than other non-MIRI alignment work -- so while it’s unendorsed by MIRI, I don’t think it’s *particularly* unendorsed relative to other things. \n\n**Eliezer Yudkowsky:** Doesn't seem especially fair?  Every year or two I check again with Chris Olah whether he's currently receiving all the resources he thinks he can use, and/or remind OpenPhil that I wish it was possible to spend a billion dollars on that research.\n\n**Ajeya Cotra:** I actually genuinely wasn't aware of this and thought that MIRI would pretty happily endorse this statement, my bad. I was also pretty surprised by your positive engagement here -- thanks for that!\n\nI was wrong about the MIRI view here and glad I checked with Rob. Will propose an edit.\n\n**Ajeya Cotra:** Okay I added in purple a different summary of MIRI's view: \"My understanding of Eliezer Yudkowsky’s position is one of “cautious relative optimism” about something in this general space compared to other non-MIRI alignment work, though he would frame the core concern differently, with more emphasis on transparency and interpretability (“GPT-3 has somewhere buried inside it knowledge of what to do when you’re sick; how do you extract all of that and how can you tell when you’ve succeeded?”). He was reasonably positive on [Stiennon et al., 2020](https://openai.com/blog/learning-to-summarize-with-human-feedback/) when it came out, and would be happy to see more work like that. Evan Hubinger’s position seems similar, and I’m not sure where others at MIRI would land on this work.\"\n\n**Eliezer Yudkowsky:** I endorse this statement of my position.\n\n**Ajeya Cotra:** Awesome, thanks!\n\n6\\. [Takeaways and possible next steps](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Takeaways_and_possible_next_steps)\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n> **If you disagree with this argument, say so** \\-\\- especially if you [think it would be harmful](https://docs.google.com/document/d/1-yZxsq-EVvdF4o2bTUZdD9V0HDl7r-csBjwZL5rDJos/edit#heading=h.8sc1ynrmwu5i) or would be dominated by a different line of work that shares [similar practical advantages](https://docs.google.com/document/d/1-yZxsq-EVvdF4o2bTUZdD9V0HDl7r-csBjwZL5rDJos/edit#heading=h.5iy1ctqv632y) of tangibility, good feedback loops, and potential-for-scale.\n\n**Eliezer Yudkowsky:** The closest thing I have to a disagreement with this draft is a general sense that 19 out of 20 bright ideas in machine learning don't work especially well; so I have more hope in calls for proposals that pinpoint key problems in a way that seems sufficiently crisp to technical thinkers, possibly including examples of potential bright ideas to show what the problem **is**, but leave wide open how those problems are to be addressed.\n\nCompared to saying, \"The problem is that we have no idea what the hell GPT-3 is thinking, and if that's still true at AGI then everybody dies\", I think you get better results if you say \"GPT-3 obviously contains a lot of medical knowledge but we would like a better way than prompt engineering to get GPT-3 to think using that knowledge, something that scales better than prompt engineering using a lot of human labor, and doesn't leave us wondering if today was the day that GPT-3 decided to emulate an Internet troll despite our best prompting\".  But you get worse results if you have your own bright idea for how to solve that, and ask for proposals to carry out your bright idea.  This draft doesn't descend particularly far into that - it's not proposing a particular exact architecture - but it also doesn't explicitly distinguish \"Here is what I think the problem is\" and \"Here is one example idea of how to go about it, which the call for proposals definitely isn't limited to because 19 out of 20 ideas don't work\".\n\nFor that matter, I think you want to explicitly signal that you are open to people reframing the problem (providing that you think they are still directly challenging a key step and not just telling you to care about something else instead; but in real life I imagine you'd get a lot of lousy distantly-related proposals with strained arguments, no matter what you say in the CfP, if you are getting lots of proposals at all).\n\n**Ajeya Cotra:** Thanks -- I basically agree with all of this on reflection (except for, ironically/appropriately, your particular framing of the problem). I've suggested some edits already (highlighted in purple) to make it more agnostic, and will look back at this and add more.\n\n**Ajeya Cotra:** I tweaked various things, and I don't think it's totally nailing the balance between not being too prescriptive and not being too vague, but I'll probably ship it anyway to get the ball rolling.\n\n\\[Editor’s note: Evan made the following comment before Ajeya made the Q&A section \"[Why not focus on testing a long-term solution?](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Why_not_focus_on_testing_a_candidate_long_term_solution_)\".\\]\n\n**Evan Hubinger:** Some comments after reading this, which I think broadly fall into the category of thinking that this is less valuable than other work that could be done:\n\nI think there is a very important problem that this line of research is pointing at, which is figuring out how to get models that are ascription universal—that is, models that tell you everything they know. Paul has a whole sequence of posts on this (that I was surprised to not see linked anywhere, but maybe I missed it, see [https://ai-alignment.com/towards-formalizing-universality-409ab893a456](https://ai-alignment.com/towards-formalizing-universality-409ab893a456)).\n\nThat being said, I feel pretty skeptical about telling people to just go work on something useful and hope that good work on universality comes out as a result. There's a lot of engineering work that goes into making useful systems that's pretty unrelated to actually solving universality-style problems and my guess is that most of the effort on a project like that would end up being unrelated engineering work rather than useful universality-relevant work.\n\nI think that the distinctions you draw between for-profit work and what you're proposing here might help mitigate the above problem somewhat, but I'm still not sure why this would be better than just working directly on the universality problem. I feel like I would much rather have someone doing a bunch of research trying to figure out how to get GPT-3 to give truthful answers than someone trying to get GPT-3 to give legitimately helpful medical advice. There's just so much random, domain-specific stuff you'd need to do to get GPT-3 to give good medical advice that doesn't feel particularly alignment-relevant to me.\n\nAnother way of thinking about this: if you're doing research directly on universality, you can have a much tighter feedback loop, where you can iterate with lots of different techniques and try different things. If you have to produce something that's actually going to be useful, I don't think you can really afford to do that same sort of open-ended iteration. Instead, I expect you'll have to do a lot of very specific fine-tuning for the downstream tasks you're trying to solve that will prevent you from being able to iterate quickly.\n\n**Ajeya Cotra:** Thanks for the thoughts! I don't think I expect most of the value of this work to be directly testing the specific concept of ascription universality or otherwise directly feeding into Paul's agenda, which is why I didn't link the post. The thing I'm saying is specifically trying to be pretty common sense and atheoretical, rather than focusing on resolving a particular uncertainty within a particular one shot solution agenda. \n\nI think telling people to train models to give truthful answers to a wide range of questions would be good, but not obviously better than the other projects and not obviously very optimized for testing IDA / ascription universality (which I think most people don't really understand well enough to have a good sense of whether they're testing it or what they can learn about it). I think that whole line of work needs more conceptual clarification, at which point Paul will probably propose specific projects that would test his views (which might end up looking very different than his current views when the dust settles).\n\nWith that said, I don't consider the *point* of this work to make models useful -- I just consider the usefulness of the final models a litmus test for whether you were choosing sufficiently challenging alignment problems to work on, and I agree it would  be bad to waste a bunch of time on faff productizing the thing. A model which answers a bunch of questions well and honestly would pass that bar.\n\nIf you have ideas for research projects that would provide good tests of ascription universality and could be executed by someone who doesn't deeply understand ascription universality or really buy into Paul's agenda, I'd be excited to add that to the list of suggested projects. But I think I would frame it as \"Here's a thing that looks like aligning a narrowly superhuman model. A special benefit of this project is that Evan Hubinger thinks it's particularly well-suited for shedding light on ascription universality\", rather than leading with \"testing out ascription universality\" as the main benefit you could get out of any of this work.\n\nAnother point here is that I'm focusing a lot on the long-run field growth potential, which \"directly testing (particular theory)\" seems to have less of.\n\n**Evan Hubinger:** Hmmm... I guess I'm just having difficulty, from an inside-view perspective, of understanding what non-universality-related insights we would get from building models which are useful in this sense. I think I do understand the outside view argument, though I would feel a lot happier if I also had an inside view argument.\n\nPerhaps my difficulty here is just coming from the fact that the primary example I have in mind is of trying to get GPT-3 to give more honest/helpful/useful answers. Do you have another example of a useful short-term alignment project that wouldn't fall into that category?\n\nIf it is just that category, though, it feels to me like it would just be better to tell people “work on getting GPT-3 to give more honest/helpful/useful answers” than “work on getting GPT-3 to be useful for concrete downstream task X.”\n\n**Ajeya Cotra:** Coding is an example that doesn't seem like it's about \"honest helpful answers\": I'm excited to figure out how to get non-software engineers to effectively supervise a coding model to write better code. I'm also interested in e.g. getting GPT-3 to write a good mystery story using feedback from people who aren't good writers, or maybe don't even speak the language.\n\nWhat's your take on the three sources of value I list in the \"How this work could reduce long-term x-risk\" section? They were: 1) Practical know-how and infrastructure, 2) Better AI situation in the run up to superintelligence, and 3) Chance of discovering or verifying a long-term solution. \n\nIt seems like your take is something like \"#3 is the main hope for impact, and getting insight about ascription universality is the main hope for #3, so if I don't see a strong route to getting insight about ascription universality from this work I'm not very bought in.\" Is that right? I think I feel much more agnostic about both of those claims: #1 and #2 seem pretty big to me, and it seems plausible that we'll discover routes to long-term solutions that look pretty different from the current Paul agenda. \n\nE.g. I think it's pretty likely the thing Paul is trying to do is impossible  because he is setting a certain bar like \"If we don't have an a priori argument that it should be safe, assume it won't be\", but at the same time something less stringent and more contingent on the actual  empirical situation would work fine. It also seems plausible to me that the ascription universality stuff \"just works out\"  and the main difficulty is elsewhere, etc.\n\nAs a more minor point, I think I don't like the frame of \"train GPT-3 to give honest, helpful answers\" because that doesn't put the focus on the \"narrowly superhuman\" part. That's kind of what I was getting at in my response to the \"Why not just stick with getting models not to do bad things?\" objection.\n\nI think the interesting part of the challenge is getting GPT-3 to *add something* to humans -- to be better than they are at something. I think a frame of \"get it to be honest\" is more likely to create a bunch of ho hum work on correcting errors humans can notice pretty easily themselves.\n\n**Evan Hubinger:** \\> I think the interesting part of the challenge is getting GPT-3 to *add something* to humans -- to be better than they are at something. I think a frame of \"get it to be honest\" is more likely to create a bunch of ho hum work on correcting errors humans can notice pretty easily themselves.\n\nThat's a good point that I hadn't considered; I definitely feel more convinced that this is a good idea now.\n\n\\> What's your take on the three sources of value I list in the \"How this work could reduce long-term x-risk\" section?\n\nI think I feel less convinced by (1) and (2) since they seem reasonably likely to just happen by default.\n\n\\> I think it's pretty likely the thing Paul is trying to do is impossible\n\nFwiw, I agree with this—I think this is one of my current biggest cruxes with Paul. Nevertheless, I think that a better understanding of the extent to which it is possible is likely to be very useful and teach us a lot.\n\n**Ajeya Cotra:** I agree that digging into the Paul thing is likely to teach us a lot even if it's impossible, fwiw. I think Paul should keep doing his thing, and I hope he comes out with experiment ideas soon. I think there's limited room for that kind of thinking and a limited number of people who could do well at it though, and the room to do this work (which could explore in parallel a bunch of worlds where Paul's thing is impossible but we're not doomed anyway) is much larger.\n\nI think of this stuff as a pretty strong baseline to beat -- if you think you have privileged insight that beats the baseline you should probably try that out for at least a couple years and return to the baseline if it's not going well. (I think even if you have privileged insight, I'd want to see more experimental work and less pure pen-and-paper stuff, but that's a deeper worldview disagreement that I'm less confident in.)\n\nI agree that 1) and 2) are likely to happen by default, but I also think 3) is pretty likely to happen by default too -- I see some gap there, just not that large of one. For all of #1-3 I'm thinking in terms of \"speed up\" relative to the default. I think right now if you look around, people could be doing this aligning narrowly superhuman models stuff and aren't, and I'd guess that if EAs don't make a push for it it'll get delayed by a couple more years still, and less of it will get done. E.g. Paul was the one to start making human feedback a thing at all at OpenAI.\n\n**Evan Hubinger:** Yeah—I definitely agree that Paul's work is good but hard to replicate. “Do more Paul stuff” was definitely not the alternative I was proposing. My thinking was just that it seems like focusing on downstream tasks with the hope of that leading to good insights into how to align language models feels less direct—and thus less likely to yield good insights by default—than just focusing on aligning language models directly. I buy that just framing it as “get GPT-3 to give honest answers” might lead people to not work on anything superhuman, though I don't yet feel fully convinced that there isn't still a yet better framing than either of those—that both emphasizes the importance of doing superhuman things but also focuses on the directly useful alignment task rather than things that are downstream of that.\n\n**Ajeya Cotra:** Hm I think there's still some  disconnect between us here -- the way I'm thinking about it, the activities I'm proposing here simply *are* aligning large models (most of them are language models but I don't want to be married to that). I don't see it as \"doing things that might give us insight into how to align narrowly superhuman models\"; it just seems like aligning them, i.e. getting them to try their best to use all their faculties to help us. I want to find ways to just directly practice the long-run thing.\n\nI'm definitely open to a better framing of the thing I'm saying that's more likely to inspire productive work, though.\n\n**Evan Hubinger:** Taking your examples from earlier:\n\n\\> Coding is an example that doesn't seem like it's about \"honest helpful answers\": I'm excited to figure out how to get non-software engineers to effectively supervise a coding model to write better code. I'm also interested in e.g. getting GPT-3 to write a good mystery story using feedback from people who aren't good writers, or maybe don't even speak the language.\n\nI think I wouldn't say that any of these things “simply *are* aligning large models”—they all feel like particular tasks which likely require some amount of alignment to get them to work, but also require lots of other non-alignment stuff as well. Ideally, I'd much prefer work that cuts out the non-alignment stuff and just focuses on the alignment stuff, but I think it's basically impossible to do that if you're trying to actually produce a model which is useful in practice for some downstream use case, since you're just not going to be able to do that without a ton of non-alignment-relevant work. I certainly think it's reasonable, even if you're just trying to do alignment work, to have some particular task in mind that you focus on, but once you start trying to do something like produce a product (not necessarily even for profit, just something that you want to be useful for real users), I expect most of the work that you'll end up needing to do for that won't be alignment-relevant.\n\n**Ajeya Cotra:** Hm, my take is like \"alignment = trying to do what we want.\" In the end we want models that are trying to run countries and companies and shepherd the future the way we want; in the near-term we could get models that are trying to do what we want as personal assistants and research assistants, and right now I think we should be tackling the hardest tasks where we could get models to try to do what we want, ideally where they are already better than us at the task.\n\nI think of \"seeing a glimmer of usefulness\" as a proxy for \"did you pick the hardest tasks\", not as the end goal. I agree you'll need to do a bunch of stuff to make it maximally useful that isn't the core part that seems like \"aligning it\" to me (basically training it with human feedback augmented/arranged in a certain way), and I think researchers working on aligning narrowly superhuman models should skip  that work. But I don't think I understand the view that alignment is something that can totally be divorced from a task.\n\nAs a concrete example of the usefulness bar I'm thinking will usually make sense, take the Paul lab paper [Stiennon et al 2020](https://openai.com/blog/learning-to-summarize-with-human-feedback/): it's definitely not  like a summarization product, and they left tons of things on the table (like collecting demonstrations from expert writers and hardcoding certain heuristics) that would feed into a real product. But it feels sort of markedly more useful than raw GPT-3, as a direct result of the fact that the model is now trying to use its faculties to be helpful instead of play an improv game. That's kind of the threshold that I think we should be aiming for.\n\n**Evan Hubinger:** I certainly don't think that alignment can be totally divorced from a task, at least no more so than capabilities—and in fact I think the analogy to capabilities is very apt here. When you focus on solving Go, if you try to do it in a task-agnostic way, you learn a lot about AI capabilities in general. On the other hand, if you try to do it in a way that is very specific to Go, you don't learn very much about AI capabilities at all. Similarly, I expect relatively open-ended research on getting large language models to do helpful things in a relatively generic way to be useful for learning about alignment in general, but narrow work on getting large language models to solve specific tasks in a relatively tasks-specific way to not be very useful.\n\nThat being said, if you think “Learning to summarize from human feedback” is a good example of what you're talking about, then maybe we just agree, because I feel like it's a good example of what I'm talking about also—that is, an example of relatively open-ended research that was just trying to get a large language model to do a helpful alignment thing, rather than actually produce anything that might actually be used in the real world as a summarization tool.\n\n**Ajeya Cotra:** Yeah I agree with trying to do task-general techniques and using tasks as case studies; I tried to emphasize this in the post but maybe I can tweak to make more clear or prominent.\n\n**Evan Hubinger:** Yeah, I definitely think that's good—though I think a part of what I'm saying also is that the message of “solve specific task X” is likely to lead to that sort of task-specific work, whereas something more like “figure out how to make GPT-3 honest” is less likely to do that, in my opinion.\n\n**Ajeya Cotra:** I want the one sentence summary to be \"align narrowly superhuman models\" rather than \"solve specific task X\"; the \"align narrowly superhuman models\" line doesn't seem less likely to lead to good work than the \"Make GPT-3 honest\" line to me (and right now I think it would lead to better work because of the problem of \"honesty\" calling to mind \"avoid certain specific lies\" and not sufficiently pushing the researcher to consider the hardest cases).\n\n**Evan Hubinger:** Yeah, I think that's a good pitch to try, but I still worry that you'll end up with a lot of useless product engineering.\n\n**Ajeya Cotra:** Cool thanks, this was a helpful discussion -- I just added a section on \"Why not test the Paul stuff\"\n\n**Evan Hubinger:** I'm glad I was able to be helpful! :)\n\n> As I said above, **Open Phil is not soliciting grant applications right now** from people who want to try it out -- this blog post is my personal viewpoint, and institutionally we’re still figuring out how much we want to prioritize this (discussion and arguments surrounding this post will feed into that).\n\n**Eliezer Yudkowsky:** If I thought a proposal in this subarea looked as good as \"[learning to summarize](https://openai.com/blog/learning-to-summarize-with-human-feedback/)\" and OpenPhil wasn't picking it up or was throwing paperwork in front of it, I'd start sending emails to other funding sources or possibly even have MIRI fund it directly.  We obviously have a lot less total funding, but **not** getting done what can be done in ML alignment is kind of... not acceptable at this point.\n\n7\\. [Appendix: beyond sandwiching?](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Appendix__beyond_sandwiching_)\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n> I’m definitely very unsure what this would look like, but an important starting assumption I have is that whatever techniques worked well to get less-capable humans to reproduce the judgments of more-capable humans in a “sandwich” setting stand a good chance of just continuing to work.\n\n**Eliezer Yudkowsky:** I remark that this is the kind of thought that needs to be hedged around **very** carefully on pain of undignified planetary extinction.  Did you scale the answers of less-capable humans to results checkable by more-capable humans, while operating the AI under the capability threshold for modeling human psychology in detail, and are you assuming the same technique will generalize if an AGI is that smart?  Did you scale worse human answers to checkable better answers while applying a small amount of optimization power, and are you assuming the same method will scale to using much more power than that?  Did you scale worse to better across an identical environmental distribution, and are you hoping the same applies when the environmental distribution is being effectively altered between training and testing by the impact of an AGI that's smarter than when it was trained?  And so on and so on.\n\nI'm not saying it's useless to poke around and find things that seem to work for scaling unreliable human answers to better-than-those-humans answers, that smarter humans can still check to see if the whole method worked.  I'm saying that if researchers actually believe the part where the journalists are like \"and lo the entire alignment problem has been solved!\", and the authors don't explicitly list out five stability conditions that held inside the experiment that might be necessary conditions, that's not what I'd call dignified.",
      "plaintextDescription": "Below, I’ve copied comments left by MIRI researchers Eliezer Yudkowsky and Evan Hubinger on March 1–3 on a draft of Ajeya Cotra’s \"Case for Aligning Narrowly Superhuman Models.\" I've included back-and-forths with Cotra, and interjections by me and Rohin Shah.\n\nThe section divisions below correspond to the sections in Cotra's post.\n\n \n\n\n0. Introduction\n> How can we train GPT-3 to give “the best health advice it can give” using demonstrations and/or feedback from humans who may in some sense “understand less” about what to do when you’re sick than GPT-3 does? \n\nEliezer Yudkowsky: I've had some related conversations with Nick Beckstead.  I'd be hopeful about this line of work primarily because I think it points to a bigger problem with the inscrutable matrices of floating-point numbers, namely, we have no idea what the hell GPT-3 is thinking and cannot tell it to think anything else.  GPT-3 has a great store of medical knowledge, but we do not know where that medical knowledge is; we do not know how to tell it to internally apply its medical knowledge rather than applying other cognitive patterns it has stored.  If this is still the state of opacity of AGI come superhuman capabilities, we are all immediately dead.  So I would be relatively more hopeful about any avenue of attack for this problem that used anything other than an end-to-end black box - anything that started to address, \"Well, this system clearly has a bunch of medical knowledge internally, can we find that knowledge and cause it to actually be applied\" rather than \"What external forces can we apply to this solid black box to make it think more about healthcare?\"\n\nEvan Hubinger: +1 I continue to think that language model transparency research is the single most valuable current research direction within the class of standard ML research, for similar reasons to what Eliezer said above.\n\nAjeya Cotra: Thanks! I'm also excited about language model transparency, and would love to find ways to make it more trac",
      "wordCount": 7725
    },
    "tags": [
      {
        "_id": "YWzByWvtXunfrBu5b",
        "name": "GPT",
        "slug": "gpt"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "slug": "outer-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "BisjoDrd3oNatDu7X",
      "tag_name": "Outer Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "9N4ZmbSqjgghDzcfW",
    "title": "Utilitarian doppelgangers vs. making everything smell like bananas",
    "slug": "utilitarian-doppelgangers-vs-making-everything-smell-like",
    "url": null,
    "baseScore": 22,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2021-02-20T23:57:34.724Z",
    "contents": {
      "markdown": "\\[epistemic status: attempt to quickly summarize how I feel about efforts to [promote](http://rationallyspeakingpodcast.org/show/episode-252-understanding-moral-disagreements-jonathan-haidt.html) more diverse moral foundations, [push back against](https://twitter.com/CT_Bergstrom/status/1269572831202828290) cost-benefit comparisons as \"utilitarianism\", or [split the difference](https://www.slowboring.com/p/vaccinate-elderly) between this notion of \"utilitarianism\" and rival approaches\\]\n\nI think human value is incredibly complicated; and I don't think it's strictly about \"harm/care\" (to use Haidt's term), or happiness, or suffering. Indeed, I suspect a utopian society would value transhuman versions of \"beauty\", \"sanctity\", etc. that sometimes have [nothing to do with](https://www.lesswrong.com/posts/HXyGXq9YmKdjqPseW/rob-b-s-shortform-feed?commentId=4vpMkDBR8v7yMLXvG) optimizing anyone's subjective experience.\n\nAnd yet, in everyday decision-making and applied ethics, I think the utilitarians, effective altruists, welfare-maximizers, etc. are basically always right. In deciding on an ideal initial response to COVID-19, or an ideal budget breakdown for an aid program, or anything else that affects large numbers of people, it would be unimaginably foolish to try to find a \"balance\" between the simple \"prevent as much disability and death as you can\" goal and some other moral framework.\n\nWhy?\n\nWell, I think the world is pretty messed up. I think that noticing this makes the case for the SLLRNUETSSTAWMC view pretty trivial. (SLLRNUETSSTAWMC = \"superficially looks like (reflective, non-two-boxing) utilitarianism even though strictly speaking things are way more complicated\".)\n\nImagine that you lived in a world dominated by a bunch of political coalitions with organizing principles like \"all other moral concerns should be subordinate to making everything smell as much like rancid meat as possible\" and \"our overriding duty is to make everything smell as much like isoamyl acetate as possible\".\n\nImagine further that this has caused people who care about things like insecticide-treated bednets and world peace to self-identify as \"oriented toward harm/care\", in contrast to the people who are \"oriented toward smell\".\n\nThe take-away from this shouldn't be \"there's nothing good or valuable about making things smell nicer\". The take-away should be \"the people who agitate about smell in today's world are largely optimizing for totally the wrong smells, and their level of concern for smell is radically out of step with what's actually going on in the world\". So out of step, in fact, that if you literally just completely ignore smell in all your altruistic activities (at least until *after* we've ended disease, warfare, hunger, etc.), you'll do way, way better at improving the future than if you tried to optimize some compromise between the coalitions' views.\n\nOr, to quote from [Feeling Moral](https://www.lesswrong.com/posts/Nx2WxEuPSvNBGuYpo/feeling-moral) (critiquing those who base humanitarian decisions on \"which option feels more just and righteous and pure\" rather than \"which option actually helps others the most\"):\n\n> You know what? This isn’t about your feelings. A human life, with all its joys and all its pains, adding up over the course of decades, is worth far more than your brain’s feelings of comfort or discomfort with a plan. Does computing the expected utility feel too cold-blooded for your taste? Well, that feeling isn’t even a feather in the scales, when a life is at stake. Just shut up and multiply.\n\nAnd, from [The \"Intuitions\" Behind \"Utilitarianism\"](https://www.lesswrong.com/posts/r5MSQ83gtbjWRBDWJ/the-intuitions-behind-utilitarianism):\n\n> I don't say that morality should always be simple.  I've already said that the meaning of music is [more than happiness alone](https://www.lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/), more than just a pleasure center lighting up.  I would rather see music composed by people than by nonsentient machine learning algorithms, so that someone should have the joy of composition; I care about the journey, as well as the destination.  And I am ready to hear if you tell me that the value of music is deeper, and involves more complications, than I realize - that the valuation of this one event is more complex than I know. \n> \n> But that's for *one event.*  When it comes to multiplying by quantities and probabilities, complication is to be avoided - at least if you care more about the destination than the journey.  When you've reflected on enough intuitions, and corrected enough absurdities, you start to see a common denominator, a meta-principle at work, which one might phrase as \"Shut up and multiply.\"\n> \n> Where music is concerned, I care about the journey.\n> \n> When lives are at stake, I shut up and multiply.\n> \n> It is more important that lives be saved, than that we conform to any particular ritual in saving them.  And the *optimal* path to that destination is governed by [laws that are simple, because they are math](https://www.lesswrong.com/lw/mt/beautiful_probability/).\n\nOr: there's a difference between cognitive operations that *feel* affectively \"cold\", versus acting in ways that are *actually* \"cold-blooded\". If your child is dying of cancer and you're staying up late night after night googling research papers to try to figure out how to save their life, the process may not feel as vital and alive as fighting off a hungry lion to save your child, but... who cares how vital and alive it feels?\n\nI'll care about that stuff after the world's immediate catastrophes have been solved. And during my off hours, when I'm *actually* listening to music.",
      "plaintextDescription": "[epistemic status: attempt to quickly summarize how I feel about efforts to promote more diverse moral foundations, push back against cost-benefit comparisons as \"utilitarianism\", or split the difference between this notion of \"utilitarianism\" and rival approaches]\n\n \n\nI think human value is incredibly complicated; and I don't think it's strictly about \"harm/care\" (to use Haidt's term), or happiness, or suffering. Indeed, I suspect a utopian society would value transhuman versions of \"beauty\", \"sanctity\", etc. that sometimes have nothing to do with optimizing anyone's subjective experience.\n\nAnd yet, in everyday decision-making and applied ethics, I think the utilitarians, effective altruists, welfare-maximizers, etc. are basically always right. In deciding on an ideal initial response to COVID-19, or an ideal budget breakdown for an aid program, or anything else that affects large numbers of people, it would be unimaginably foolish to try to find a \"balance\" between the simple \"prevent as much disability and death as you can\" goal and some other moral framework.\n\nWhy?\n\nWell, I think the world is pretty messed up. I think that noticing this makes the case for the SLLRNUETSSTAWMC view pretty trivial. (SLLRNUETSSTAWMC = \"superficially looks like (reflective, non-two-boxing) utilitarianism even though strictly speaking things are way more complicated\".)\n\nImagine that you lived in a world dominated by a bunch of political coalitions with organizing principles like \"all other moral concerns should be subordinate to making everything smell as much like rancid meat as possible\" and \"our overriding duty is to make everything smell as much like isoamyl acetate as possible\".\n\nImagine further that this has caused people who care about things like insecticide-treated bednets and world peace to self-identify as \"oriented toward harm/care\", in contrast to the people who are \"oriented toward smell\".\n\nThe take-away from this shouldn't be \"there's nothing good or valuable about maki",
      "wordCount": 814
    },
    "tags": [
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "behpTpTDEaxv9Jxwg",
    "title": "MIRI: 2020 Updates and Strategy",
    "slug": "miri-2020-updates-and-strategy",
    "url": "https://intelligence.org/2020/12/21/2020-updates-and-strategy",
    "baseScore": 77,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2020-12-23T21:27:39.206Z",
    "contents": {
      "markdown": "Highlights from MIRI's year-end strategy update:\n\n*   MIRI executive director Nate Soares reports that the \"[new research directions](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/)\" we initiated [in 2017](https://intelligence.org/2017/04/30/2017-updates-and-strategy/) have \"at this point, largely failed, in the sense that neither Eliezer nor I have sufficient hope in it for us to continue focusing our main efforts there. \\[...\\] We are currently in a state of regrouping, weighing our options, and searching for plans that we believe may yet have a shot at working.\"\n*   Our most interesting (to us) research we've written up publicly this year is Scott Garrabrant's [Cartesian Frames](https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames) and Vanessa Kosoy and Alex Appel's [Infra-Bayesianism](https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence).\n*   Our funding situation is good, and we aren't running a fundraiser this winter.\n*   We're (still) seriously thinking about the pros and cons of [moving out of the Bay Area](https://www.lesswrong.com/posts/FghubkDy6Dp6mnxk7/the-rationalist-community-s-location-problem?commentId=WggEETNGjd6d2XRgR#MPM6gaXJiCnGgEmDJ), and thinking about what other locations have to offer.\n\nYou can find the full post [here](https://intelligence.org/2020/12/21/2020-updates-and-strategy/).",
      "plaintextDescription": "Highlights from MIRI's year-end strategy update:\n\n * MIRI executive director Nate Soares reports that the \"new research directions\" we initiated in 2017 have \"at this point, largely failed, in the sense that neither Eliezer nor I have sufficient hope in it for us to continue focusing our main efforts there. [...] We are currently in a state of regrouping, weighing our options, and searching for plans that we believe may yet have a shot at working.\"\n * Our most interesting (to us) research we've written up publicly this year is Scott Garrabrant's Cartesian Frames and Vanessa Kosoy and Alex Appel's Infra-Bayesianism.\n * Our funding situation is good, and we aren't running a fundraiser this winter.\n * We're (still) seriously thinking about the pros and cons of moving out of the Bay Area, and thinking about what other locations have to offer.\n\nYou can find the full post here.",
      "wordCount": 151
    },
    "tags": [
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "jZF2jwLnPKBv6m3Ag",
        "name": "Organization Updates",
        "slug": "organization-updates"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  }
]