[
  {
    "_id": "gR96ANcNXQgh22qtG",
    "title": "Synthesizing Standalone World-Models, Part 4: Metaphysical Justifications",
    "slug": "synthesizing-standalone-world-models-part-4-metaphysical",
    "url": null,
    "baseScore": 16,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-26T18:00:29.049Z",
    "contents": {
      "markdown": "> *\"If your life choices led you to a place where you had to figure out anthropics before you could decide what to do next, are you really living your life correctly?\"*\n> \n> – [Eliezer Yudkowsky](https://youtu.be/wQtpSQmMNP0?t=1326)\n\nTo revisit [our premises](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/synthesizing-standalone-world-models-bounties-seeking): Why should we think the end result is achievable at all? Why should it be possible to usefully represent the universe as an easily interpretable symbolic structure?\n\nFirst, I very much agree with the sentiment quoted above, so we aren't quite doing that here. Most of the actual reason is just: it sure looks like that's the case, empirically. As I'd argued before, human world-models seem autosymbolic, and the entirety of our (quite successful) scientific edifice relies on something-like-this being true. I think the basic case is convincing enough not to require much further justification.\n\nBut *can* we come up with theoretical justifications regarding why the universe might have ended up with this structure?\n\nHere's one one approach: it's because there's an anthropics-based selection pressure on universes to be structured this way. Three lines of reasoning:\n\n1.  Because well-abstracting universes are simpler: their histories have a shorter description length. (In the initial pitch, I'd argued that aiming for simplicity gets us interpretability. Same logic holds: if anthropics reasoning prefers simple universes, it prefers well-abstracting universes.)\n2.  Because \"coarse\" agents – agents implemented on abstract high-level variables, which can only exist in well-abstracting universes – pool the realityfluid from all \"low-level\" agents corresponding to them, and are therefore the more likely viewpoints (see [this](https://www.lesswrong.com/posts/zYCoqjYNHFAEJD8TC/are-you-more-real-if-you-re-really-forgetful) post).\n3.  Because embedded agents could only survive in well-abstracting universes to begin with.\n\nI go full anthropics-weirdness in this section, taking the premises of Tegmark IV as granted. I primarily view that type of reasoning as a useful tool for organizing one's thinking and informing one's intuitions in internally consistent ways. The conclusions don't rely on anthropics' metaphysical premises being *literally* true. (Indeed, I expect this can all be translated into more mundane reasoning about Solomonoff-induction approximation / generalized Occam's razor: reasoning about what theories about the universe's structure we should consider more likely *a priori*. But the language would grow cumbersome.)\n\n* * *\n\n4.1. Well-Abstracting Histories Are Simpler\n-------------------------------------------\n\nConsider a universe whose initial conditions and physics unroll into a history that has certain robustly reoccurring high-level regularities/emergent dynamics. In other words, it has \"abstraction levels\", which follow different high-level \"laws\" and across which \"high-level histories\" play out. The tower of abstractions can have many levels, of course (fundamental particles to molecules to cells to organisms to populations).\n\nSuppose that the high-level history is generated by simple laws as well. (Chemistry, evolution, microeconomics.) Then, we could exploit it to compress the description of the *full-fidelity*/lowest-level history. Instead of having to use an encoding that specifies the full state at any given moment, you could point to the high-level history, then \"clarify\" the low-level states using \"correction terms\". This would work inasmuch as the high-level history distills the information that is *repeated* in several low-level variables.\n\n(If we have two random variables \\\\(X\\\\), \\\\(Y\\\\) with nonzero mutual information \\\\(I(X;Y)\\\\), we have \\\\(H(XY)=H(X)+H(Y)-I(X;Y)>H(X)+H(Y)\\\\), because sum of individual descriptions double-counts the shared information. Describing the low-level state directly is equivalent to using the sum of individual descriptions.)\n\nAlternate reasoning: Any description of a low-level history necessarily describes the high-level history, so a longer-description high-level history necessarily means a longer-description low-level history. Now consider a history without robust abstraction levels. This is isomorphic to it having a high-level history which takes a random walk through some state-space, which blows up its description length.\n\n(Note that high-level histories don't have to be *absolutely* robust: abstractions can be leaky or even sometimes destroyed. Governments fall apart, the orbital dynamics of star systems change. In this frame, it only means you have to spend bits on \"manually encoding\" discontinuous jumps through the high-level state-space (\"abstraction leaks\", high-level state momentarily becoming dependent on low-level details), or on switching the function generating the high-level history (\"abstraction break\", the high-level emergent dynamics fundamentally changing). As long as each high-level dynamic stays around for a nontrivial interval and *largely* explains (an aspect of) the low-level dynamics within this interval, it still serves to compress the low-level history.)\n\nAll this naturally works better if we have many layers of abstraction: intermediate levels compress lower levels and are in turn compressed by higher levels.\n\nSo: if we assume some process in Tegmark 4 that picks *which* histories to implement, instead of implementing all histories blindly and uniformly, the histories with \"tall\" abstraction towers would be picked more often / have more realityfluid / are anthropically selected-for. (In our case, this process is maybe the interference patterns between Everett branches.)\n\nI. e.: this is the same logic I initially used to argue that aiming for a low-description-length representation of our universe would recover a well-structured representation. If reality \"prefers\" universes that are simple, it prefers universes that can be compressed well, which means it prefers well-abstracting universes.\n\n**Sidenote:** This potentially sheds light on [the induction problem](https://en.wikipedia.org/wiki/Problem_of_induction). If large intervals of history have to approximately agree with the output of a simple program, then looking at the past data and inferring its generators would yield you some generators which are still \"active\" in your time. Hence, compressing the past lets you predict the future.\n\n* * *\n\n4.2. Coarse Agents\n------------------\n\nIf you sampled an agent from the anthropic prior, what sort of agent would you expect to see?\n\nConsider \"coarse\" agents: agents that are implemented on some high-level abstract variables, \"live within\" high-level histories, and frequently discard low-level details of their observations.\n\nThe observation-history of a coarse agent would correspond to a *set* of lower-level observation-histories; to a set of lower-level agents which are approximately similar to each other. Mechanistically, the observation-streams would \"diverge\" the moment two agents perceive some detail that differs between their universes, but would then \"merge again\" once they forget this detail. Coarse agents would then necessarily hoard more realityfluid than any \"precise\" agent. (See a more gradual introduction of the ideas [here](https://www.lesswrong.com/posts/zYCoqjYNHFAEJD8TC/are-you-more-real-if-you-re-really-forgetful). Also, note that this [doesn't](https://www.lesswrong.com/posts/zYCoqjYNHFAEJD8TC/are-you-more-real-if-you-re-really-forgetful?commentId=9nCFxMobMpW9HASe2) give you any weird anthropic superpowers. Like, this idea feels weird, but as far as I can, it still adds up to normality.)\n\nNote, however, that such agents could only live in well-abstracting universes. After all, as a premise, we're assuming that they're able to discard detailed information about their observations and still navigate their universe well. This naturally implies that there are some high-level laws under which the agents live, instead of being exposed to the raw low-level dynamics.\n\nThis also means we can expand the arguments above. We can count all *approximately similar* universal histories as \"the same\" history, for the purposes of adding up the realityfluid of the embedded agents. That biases the anthropic prior towards well-abstracting histories even further: they're not only simpler by themselves, but they also embed a simpler type of agent. (The relevant notion of \"approximately similar\" here is provided by the particulars of the agent's vantage point/at what abstraction level it's embedded.)\n\n* * *\n\n4.3. Only Well-Abstracting Worlds Permit Stable Embedded Agents\n---------------------------------------------------------------\n\nConsider:\n\n*   In a universe lacking high-level emergent patterns, with incompressible low-level dynamics, you have to pay attention to the entire low-level state in order to predict the future.\n*   Reliably navigating/controlling an environment [requires](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) having access to a structure isomorphic to its model.\n*   Embedded agents are distinguished by being smaller than the universes they inhabit.\n\nTherefore, if a universe's dynamics aren't (approximately) compressible, embedded agents cannot (even approximately) model them, cannot navigate that universe, and cannot survive/thrive in it.\n\n* * *\n\nI'm not entirely confident in the specifics of that theoretical reasoning; my anthropics-related arguments may contain errors. Still, I think the overall intuitive story is convincing: histories with higher levels of organization are meaningfully simpler than histories without them, and inasmuch as we put any stock in arguments for simplicity, we should expect to find ourselves in a universe with a well-abstracting history.",
      "plaintextDescription": "> \"If your life choices led you to a place where you had to figure out anthropics before you could decide what to do next, are you really living your life correctly?\"\n> \n> – Eliezer Yudkowsky\n\nTo revisit our premises: Why should we think the end result is achievable at all? Why should it be possible to usefully represent the universe as an easily interpretable symbolic structure?\n\nFirst, I very much agree with the sentiment quoted above, so we aren't quite doing that here. Most of the actual reason is just: it sure looks like that's the case, empirically. As I'd argued before, human world-models seem autosymbolic, and the entirety of our (quite successful) scientific edifice relies on something-like-this being true. I think the basic case is convincing enough not to require much further justification.\n\nBut can we come up with theoretical justifications regarding why the universe might have ended up with this structure?\n\nHere's one one approach: it's because there's an anthropics-based selection pressure on universes to be structured this way. Three lines of reasoning:\n\n 1. Because well-abstracting universes are simpler: their histories have a shorter description length. (In the initial pitch, I'd argued that aiming for simplicity gets us interpretability. Same logic holds: if anthropics reasoning prefers simple universes, it prefers well-abstracting universes.)\n 2. Because \"coarse\" agents – agents implemented on abstract high-level variables, which can only exist in well-abstracting universes – pool the realityfluid from all \"low-level\" agents corresponding to them, and are therefore the more likely viewpoints (see this post).\n 3. Because embedded agents could only survive in well-abstracting universes to begin with.\n\nI go full anthropics-weirdness in this section, taking the premises of Tegmark IV as granted. I primarily view that type of reasoning as a useful tool for organizing one's thinking and informing one's intuitions in internally consistent ways. The concl",
      "wordCount": 1334
    },
    "tags": [
      {
        "_id": "5GYzBE6q89w74dqfk",
        "name": "Abstraction",
        "slug": "abstraction"
      },
      {
        "_id": "RyNWXFjKNcafRKvPh",
        "name": "Agent Foundations",
        "slug": "agent-foundations"
      },
      {
        "_id": "PbShukhzpLsWpGXkM",
        "name": "Anthropics",
        "slug": "anthropics"
      },
      {
        "_id": "MP8NqPNATMqPrij4n",
        "name": "Embedded Agency",
        "slug": "embedded-agency"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "eQX93nxapSdxQYdkL",
    "title": "Synthesizing Standalone World-Models, Part 3: Dataset-Assembly",
    "slug": "synthesizing-standalone-world-models-part-3-dataset-assembly",
    "url": null,
    "baseScore": 11,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-09-25T19:21:37.730Z",
    "contents": {
      "markdown": "*This is part of a series covering* [*my current research agenda*](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models)*. Refer to the linked post for additional context.*\n\nThis is going to be a very short part. As I'd mentioned in the initial post, I've not yet done much work on this subproblem.  \n \n\n* * *\n\nFrom [Part 1](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction), we more or less know how to learn the abstractions given the set of variables over which they're defined. We know their type signature and the hierarchical structure they assemble into, so we can just cast it as a machine-learning problem (assuming a number of practical issues is solved). For clarity, let's dub this problem \"**abstraction-learning**\".\n\nFrom [Part 2](https://www.lesswrong.com/posts/kNyMwXQxctWtaRZhs/synthesizing-standalone-world-models-part-2-shifting), we more or less know how to deal with shifting/resampled structures. While the presence of specific abstractions doesn't uniquely lock down what other abstractions are present at higher/lower/sideways levels, we can infer a probability distribution over what abstractions are likely to be there, and then resample from it until finding one that works. Let's call this \"**truesight**\".\n\nExcept, uh. Part 1 only works given the solution to Part 2's problem, and Part 2 only works given the solution to Part 1's problem. We can't learn abstractions before we've stabilized the structure/attained truesight, but we can't attain truesight until we learn what abstractions we're looking for. We need to, somehow, figure out how to learn them jointly.\n\nThis represents the third class of problems we need to solve: figuring out how to transform whatever data we happen to have into datasets for learning new abstractions. Such datasets would need to be isomorphic to samples from the same fixed (at least at a given high level) structure. Assembling them might require:\n\n*   Figuring out what variables to consider.\n*   Figuring out what *samples* of those variables to consider. (E. g., when learning Conway's glider, we'd only consider the samples of the first \\\\([0:3]\\times [0:3]\\\\) set of cells for the first few time-steps. Similarly, in fluid dynamics, we use [material derivatives](https://en.wikipedia.org/wiki/Material_derivative) to lock our viewpoint to a specific particle in a stream, instead of to a specific point in space – switching the representation from \"spatial points are the low-level random variables\" to \"particles are the low-level random variables\".)\n*   Figuring out what functions of those variables to consider. (Which might involve effectively discarding specific \"subvariables\" \\\\(x_i^j\\\\) of a given variable \\\\(x_i\\\\), or treating a bunch of variables as subvariables to get at their synergistic information.)\n\nCall this \"**dataset-assembly**\".\n\nDataset-assembly has some overlap with the truesight problem, so the heuristical machinery for implementing them would be partly shared. In both cases, we're looking for functions over samples of some known variables that effectively sample from the same stable structure. The difference is whether we already known that structure or not.\n\nAnother overlap is with the heuristics I'd mentioned in [1.6](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction#1_6__Summary), the ones for figuring out which subsets of variables to try learning synergistic/redundant-information variables for (instead of doing it for all subsets). Indeed, given the shifting-structures problem, those are actually *folded into* the heuristics for assembling abstraction-learning datasets!\n\nIntrospectively, in humans, \"dataset-assembly\" is represented by qualitative research as well, and by philosophical reasoning (or at least [my model](https://www.lesswrong.com/posts/LbfWAq4F8tHaQsTyT/towards-the-operationalization-of-philosophy-and-wisdom) of what \"philosophical reasoning\" is). \"Dataset-assembly heuristics\" correspond to research taste, to figuring out what features of some new environment/domain to pay attention to, and which parts of reality could be meaningfully grouped together and decomposed into a new abstract hierarchy/separate field of study.\n\nMy thinking on the topic of dataset-assembly is relatively new, and isn't yet refined into a proper model/distilled into something ready for public consumption. Hence, this post is little more than a stub.\n\nThat said, I hope the overall picture of the challenge is now clarified. We need to figure out how to set up a process that jointly learns the heuristics for solving these three classes of problems.\n\n* * *\n\nWhat I'm particularly interested in here for the purposes of [the bounties](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models#Bounties) is... well, pretty much anything related, since the map is pretty blank. Three core questions:\n\n*   What are high-level ways to formalize the dataset-assembly subproblem?\n*   What are some heuristics for solving this subproblem?\n*   How should we think about/model the problem of solving all three subproblems jointly?\n\n(My go-to approach in such cases is to figure several practical heuristics, go through a few concrete cases, then attempt to distill general principles/algorithms based on those analyses.)",
      "plaintextDescription": "This is part of a series covering my current research agenda. Refer to the linked post for additional context.\n\nThis is going to be a very short part. As I'd mentioned in the initial post, I've not yet done much work on this subproblem.\n \n\n----------------------------------------\n\nFrom Part 1, we more or less know how to learn the abstractions given the set of variables over which they're defined. We know their type signature and the hierarchical structure they assemble into, so we can just cast it as a machine-learning problem (assuming a number of practical issues is solved). For clarity, let's dub this problem \"abstraction-learning\".\n\nFrom Part 2, we more or less know how to deal with shifting/resampled structures. While the presence of specific abstractions doesn't uniquely lock down what other abstractions are present at higher/lower/sideways levels, we can infer a probability distribution over what abstractions are likely to be there, and then resample from it until finding one that works. Let's call this \"truesight\".\n\nExcept, uh. Part 1 only works given the solution to Part 2's problem, and Part 2 only works given the solution to Part 1's problem. We can't learn abstractions before we've stabilized the structure/attained truesight, but we can't attain truesight until we learn what abstractions we're looking for. We need to, somehow, figure out how to learn them jointly.\n\nThis represents the third class of problems we need to solve: figuring out how to transform whatever data we happen to have into datasets for learning new abstractions. Such datasets would need to be isomorphic to samples from the same fixed (at least at a given high level) structure. Assembling them might require:\n\n * Figuring out what variables to consider.\n * Figuring out what samples of those variables to consider. (E. g., when learning Conway's glider, we'd only consider the samples of the first [0:3]×[0:3] set of cells for the first few time-steps. Similarly, in fluid dynamics, we use m",
      "wordCount": 728
    },
    "tags": [
      {
        "_id": "5GYzBE6q89w74dqfk",
        "name": "Abstraction",
        "slug": "abstraction"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kNyMwXQxctWtaRZhs",
    "title": "Synthesizing Standalone World-Models, Part 2: Shifting Structures",
    "slug": "synthesizing-standalone-world-models-part-2-shifting",
    "url": null,
    "baseScore": 15,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-24T19:02:14.662Z",
    "contents": {
      "markdown": "*This is part of a series covering* [*my current research agenda*](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models)*. Refer to the linked post for additional context.*\n\n* * *\n\nLet's revisit our initial problem. We're given the lowest-level representation of a well-abstracting universe, and we want to transform it into its minimal representation / the corresponding well-structured world-model. The tools introduced in [Part 1](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction) are insufficient for that: there are two more problems left. This part focuses on one of them.\n\n**Key example:** Imagine looking at a glider in *Conway's Game of Life*. At the first time-step, it may occupy the coordinates \\\\([0:3]\\times [0:3]\\\\). As time goes on, it would gradually migrate, diagonally moving in the bottom-right direction at the speed of one cell per three time-steps.\n\nIf we take the individual cells to be the random variables to abstract over, in what sense is the glider an abstraction over them? It's a \"virtual object\", drifting *across* them.\n\nBut if it's not an abstraction over the cells, then over... what? There aren't really any other elements in the picture! Maybe it's an abstraction over sets of cells, with cells being \"subvariables\" over which the glider is synergistic? But how exactly do we define those sets?\n\n(Perhaps it's a synergistic variable over the whole grid/world? I've tried that. That approach was instructive, but it got very clunky very fast.)\n\nOnce you start looking for it, you start spotting problems of this sort all over the place.\n\n* * *\n\n2.1. Wall O' Examples\n---------------------\n\n*   Consider a human mind in a high-tech civilization. At one moment, it may be implemented on biological neurons; at another, it may jump to some artificial substrate; then the lower-level algorithms on which it functions may be changed without change in high-level behavior; then it may be transferred to another star system encoded in a bunch of photons. Is there *any* stable set of lower-level random variables over which it is an abstraction?\n    *   Now imagine if we try to model that as a synergistic variable over the whole world. That synergistic variable would have to be defined as drastically different, basically custom-built functions of every specific world-state. It'd have to be able to \"process\"/\"attach to\" *any possible representation* the upload may have.\n    *   I've tried variants on that approach. It was instructive, but got very clunky very fast. What we need is some *general* framework for allowing synergistic variables to \"see through\" changes in representation…\n*   The much-abused dog example fares no better. We say that \"this specific dog\" is an abstraction over some lower-level subsystems \"marked\" by synergistic variables. But the actual positioning of those subsystems isn't stable, nor even their number. How many DNA-containing cells does \"a dog\" have?\n    *   This is somewhat handled by the thing about Shamir's scheme in [1.5](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction#1_5__Abstractions_and_Synergistic_Variables), but it gets very clunky very fast. Especially if we try to apply it to \"virtual objects\" in general, see the upload case above. It's a very \"low-tech\" solution.\n*   If we consider \"the concept of an animal\", it gets even worse. Some animals don't have eyes, some have echolocators; their digestive, circulatory, and nervous systems may function in completely different ways. What are the features/subvariables in the \"default representation\" of \"an animal\" which, if we condition their *values* on a higher-level variable, would transform into any possible animal?\n    *   We can model it using some sort of sufficiently high-dimensional space, of course. But then it more or less becomes an \\\\(m\\\\)-dimensional space where \\\\(m\\\\) is the number of animal types, and where the animal definitions are one-hot vectors. Again, what's the actual form of the low-level variables the abstraction is defined over, here? What form *could* it be?\n*   As outlined in a previous section, the function \\\\(F\\\\) a program implements can be considered an abstraction over all possible implementations of this function \\\\(P_i\\\\), with each \\\\(P_i\\\\) being a construction \\\\(\\{p_i^1,\\dots,p_i^m\\}\\\\) made up of some combinations of basic functions. Except, uh, both the number of the functions and their dictionary changes between the implementations. Again, what's the implementation-agnostic default program template that we're implicitly using here, the subvariables of the variable over which we abstract (and which doesn't reduce to just one-hot encodings for each possible program)?\n    *   I suppose we can project to e. g. the parameter space of neural networks. But the mapping has many free parameters, and even if we use a fixed-size fixed-architecture representation, neuron#42312 doesn't necessarily implement the same higher-level operation in every sample, so we can't use some fixed way to abstract up.\n*   Or consider your eyes. We can model them, or their constituent \"pixels\"/cone cells, as some random variables. How are you supposed to use them to find the world's abstractions? The abstract structures mapped to them constantly change: they \"drift\" like the Conway glider, they jump representations like the upload (looking at an event vs. seeing its video vs. reading a newspaper article about it...), they get stochastically resampled (switching TV channels, losing consciousness, boarding a random train and ending up in an unfamiliar place)…\n*   LLMs deal with the same issue: they always get fed a vector of a pre-defined size, mapped to the same set of input-variables, and they somehow know to connect that to different in-world structures depending on what's in it. How, in information-theoretic terms?\n*   Similar happens for various in-the-world systems. A newspaper's text, for example: much like observation-variables, it \"connects\" to different structures in the world, depending on the newspaper issue. How do we process that data, infer what it's connected to?\n    *   (I'd tried modeling those switches as, well, literal \"switch-variables\", which control at what part of the world we look, and which get resampled as well. That was instructive, but got very clunky very fast.)\n*   Similar issue occurs if we're looking at an \\\\(n\\\\)-vector representing a low-level universe-state from a god's-eye perspective and watch it get resampled (either randomly, or by a state transition to the next time-step). It's full of such virtual objects and changing structures. We can't start \"unraveling\" it in any white-box way without figuring out how to deal with this.\n    *   (We can just say \"we have a hypercomputer so we just try all possible representations until finding the minimal one\", but that's not helpful. Indeed, that's basically just another one-hot encoding / lookup table mapping the whole \\\\(n\\\\)-vector to different values. We're not doing any \"abstracting\" at all here, information-theoretically.)\n\n* * *\n\n2.2. Grasping the Problem\n-------------------------\n\nThe program example actually hints at what the issue is. The function \\\\(F\\\\) is very, *very* agnostic regarding its lower-level implementation. There's an infinite number of programs that implement it: a two-bit adder can be implemented as a bunch of NAND gates, or XOR gates, or Lisp functions, or as transistors implementing NAND gates, or as an analog computer made up of water streams, or as a set of star systems exchanging asteroids, or as the simulation of a vast civilization tasked with summing the bits...\n\nSame with the abstractions in general. The higher level of abstraction is largely agnostic regarding the nature of the lower level implementing it. As long as the lower level passes some minimal threshold of expressivity, it can implement a dynamic with ~*any* emergent behavior. Meaning, in a vacuum, there's ~zero mutual information between the higher level and the *laws* of the lower level.\n\nAn \"animal\" could be represented using a wide variety of \"programs\" of different compositions; same with \"a human mind\"; same with \"a market\". Once we lock down a *specific* \"abstraction operator\" – the map from the lower to the higher level – the lower level is fixed, and the higher level is a valid natural latent over it. But if we go *top-down*, holding the higher-level abstract structure fixed but not receiving any information about the lower level, the lower-level's *structure* (not just values) isn't fixed; it's only constrained to the infinite number of structures over which the higher level would be a valid natural latent.\n\nSymmetrically, if we go bottom-up, there's not any sense in which a *specific set of labeled lower-level variables* necessarily has the same relationship with any given higher-level variable; or that any given higher-level variable survives a resampling. Depending on the nature of the resampling (random like in \"god's-eye view\", or deterministic like in Conway's, or stochastic-but-not-independent like in head movements), the structures may drift, switch representations, or change outright. So what we need is to... somehow... learn new abstractions *from one sample*?\n\n**Summing up:** Usually, when we're facing a structure-learning problem, we assume that the structure is fixed and we get many samples for it. Here, the structure *itself* gets resampled. (See also: the bit in [1.3](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction#1_3__Incorporating_Synergistic_Information) regarding how we'll need to figure out how to deal with things that \"look like\" probabilistic structures from one angle, and like random variables from a different one.)\n\n* * *\n\n2.3. Exploiting the JPD Over Abstractions\n-----------------------------------------\n\nLet's suppose we do have that hypercomputer after all. Further, suppose that we've used it to compute the joint probability distribution over the set of all possible abstractions.\n\nThat is: we've developed the ability to learn, by looking at a specific type of variable, what other abstractions are *typically* present in a universe containing this type of variable. Each corresponding joint sample would be weighed by e. g. the simplicity prior, meaning the abstractions' sheer agnosticism regarding lower levels would be ameliorated by us defining a probability distribution over them. Some examples:\n\n*   If we see a human, we'd consider it likely that there are other humans around, and perhaps that \"government\" abstractions are active. Depending on the human's clothes, we can update towards or against various types of technology existing. Seeing a human would also make it very unlikely that e. g. any hostile-to-humans superintelligent entities are around.\n*   If we see the \"democracy\" abstraction, we would upweight the possibility that there's a set of some sort of agents at the lower level, with non-orthogonal but also not-exactly-the-same values, implementing a civilization. It's *possible* that a bunch of star systems lobbing asteroids at each other spontaneously assembled into a system for which \"a democracy\" is a good abstraction, but that's pretty unlikely.\n*   If we see a sapient mind of a particular form, we'd consider it likely that this universe's history involved some sort of evolutionary process. It's possible that it's a Boltzmann brain, but, conditioning on a well-abstracting universe, that's unlikely.\n*   If we see a dog at one moment, we'd consider it likely that the same dog will be present at the next moment, assuming the state-transition function mapping this moment to the next one implements some sort of simple physics.\n*   Seeing a system implementing a spiral galaxy would presumably nontrivially upweight the probability of a lower level containing stars and planets.\n\nEt cetera. By conditioning on \"we are in a universe that is *overall* well-abstracting\", we can derive posterior distributions over probabilistic *structures*, rather than just the *values* of fixed variables in fixed structures.\n\nIncidentally, this can be viewed as a *generalization* of the general abstraction machinery, rather than some new addendum. When we condition on \"we're looking at a dog\", we don't actually *deterministically deduce* that it has a head, four legs, a heart, and a tail. We merely update our distribution over lower-level structures/features to strongly expect this set of features. We also place a nontrivial amount on \"three legs, one tail\", yet assign epsilon probability to \"no head\" (assuming alive dog) or \"twenty heads\". (This is the promised better way to handle the nitpick in [1.5](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction#1_5__Abstractions_and_Synergistic_Variables).)\n\nBut there's also a bunch of other, softer inferences we make. For example, if we're searching for a specific type of virus that tends to infect dogs, our probability distribution over the locations of this virus' instances would shift towards the volume the dog's body occupies. We'd also infer some distribution over the number of \"dog cell\" abstractions present at the lower level, and the atomic contents of the volume in question, et cetera. Those are all, again, best modeled as inferences about *structures*, not values (or, well, as about structure-valued variables).\n\nIn terms of \"upwards\" inferences, we'd expect the \"dog concept\" abstraction (not only this-dog) to be present in that world, and e. g. allocate nontrivial amount of probability mass to \"Earth-like biosphere and evolutionary history\".  \n  \nIn terms of \"sideways\" inferences (abstractions at the \"same level\" as the dog), we'd expect more dogs or the dog's owner to be nearby (though, hmm, this might actually route through conditioning on the higher-level \"dog concept\" variable, which both creates new variables and synergistically creates mutual information between them and the dog).\n\n**So, summarizing:** In practical cases, it's impossible to deterministically deduce / uniquely constrain the structures you're trying to learn. However, due to the fact that the \"well-abstractibility prior\" is a very specific entity, it's possible to define *posterior probability distributions* over those structures.\n\n* * *\n\n2.4. The Need for Truesight\n---------------------------\n\nLet's take a look at the current problem from a different angle. Consider the following:\n\n*   Conway's glider vs. the same glider shifted some distance bottom-right.\n*   A human mind switching computational substrates and lower-level algorithms without change in function.\n*   The dog that's the same at the high level, but with slightly different numbers of cells, or slightly different organ placements.\n*   The concept of an animal, applicable to systems that function in wildly different ways.\n*   The concept of a dog, applicable to different pictures of different dogs.\n*   The function \\\\(F\\\\) implemented in a variety of different ways on different programming languages.\n*   The same real-world structure perceived through…\n    *   ... your eyes.\n    *   ... your eyes, but after you walked a few meters to the left and slightly rotated your head.\n    *   ... your ears.\n    *   ... someone's verbal description of it.\n    *   ... a news article.\n    *   ... a video.\n    *   ... a metaphorical poem.\n*   A set of mathematical theorems supported by the same set of axioms.\n*   A high-level system vs, the corresponding low-level system plus some simplifying assumptions.\n*   Technically correct-and-complete descriptions of the same event by two newspapers with different political biases.\n*   The plaintext message vs. the ciphertext and the key to it.\n\nEach of those examples involves different representations of what's ultimately *the same thing*, but transformed in a way such that the similarity is very non-trivial to spot.\n\nWhat this immediately reminds me of, with all the photo examples, are CNNs.\n\nCNNs' architecture has translation symmetry built-in: their learned features are hooked up to machinery that moves them all across the image, ensuring that their functionality is invariant under changes in the specific position of the features in the image. What we want is a *generalization* of this trick: a broad \"suite of symmetries\" which we \"attach\" to the inputs of our learned abstraction-recognition functions, to ensure they don't get confused by the abstract equivalent of shifting a dog in a picture slightly to the left.\n\nI. e., we want to give our abstraction operators \"truesight\": empower them to zero-shot the recognition of already-known abstractions under changes in representation.\n\nIn the general case, that's impossible. After all, there's a function for mapping anything to anything else, more or less. No free lunch.\n\nBut we're not working with the *general* case: we're working with the samples from the set of simple well-abstracting universes. For any given set of abstractions that we're already seeing, there's a very specific posterior distribution over what abstractions we should expect to discover next. Which corresponds to a probability distribution over what \"lens\" we should try putting onto our feature-detectors to spot the familiar abstractions in new data.\n\nIn other words: we have a probability distribution over sequences of transformations of the data we should attempt to try, and we keep sampling from it until we spot a familiar abstraction; until we find a way of looking at the data under which it resolves into a known, simple pattern. (Where \"simplicity\" is defined relative to our learned library of abstractions.)\n\nWhich is to say, we engage in [qualitative research](https://transformer-circuits.pub/2024/qualitative-essay/index.html). We [switch](https://www.lesswrong.com/posts/t46PYSvHHtJLxmrxn/what-are-important-ui-shaped-problems-that-lightcone-could?commentId=vgozGjvJQvFQZteiF) between various representations until finding one in which the compression task looks trivial to us.\n\nI think this problem/process is, indeed, literally the underlying \"type signature\" of a large fraction of human science and research efforts.\n\nSome other examples of when it happens:\n\n*   Reading an ambiguous sentence, and trying to pick one interpretation out of several, by trying each of them on \"for size\" and figuring out which would \"make the most sense\" within the context (i. e., simplify your model of that context).\n*   Looking at a confusing painting, and figuring out what you're looking at by quickly flipping through several explanations (\"maybe it's a face? maybe that's an arm?\") until the picture snaps together.\n*   Analyzing a table of numbers in search of a pattern, and eyeballing potential causal connections and trends (\"do these two variables seem to change together? does this variable follow a periodic pattern? is this an exponential?\").\n*   Self-psychoanalysis (noticing a weird, illegible emotional response, and reverse-engineering it by iteratively generating verbal explanations and sounding them out/testing them, until one rings true).\n*   Having a specific word/concept \"at the tip of your tongue\" that you think perfectly describes a situation, and searching for it by repeatedly sampling concepts/words that come to mind.\n*   Seeing a number of clues, in e. g. a murder-mystery story, and generating a tree of potential explanations whose branches you eliminate until you're left with a single reality that generated all of those clues. \n\n* * *\n\nWhat I'm particularly interested in for the purposes of [the bounties](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models#Bounties):\n\n*   What ways are there of formalizing the machinery described? (There's one obvious way, but I'm wondering whether that framework could be avoided...)\n*   Is there any extant research covering setups with such \"resampled structures\"?",
      "plaintextDescription": "This is part of a series covering my current research agenda. Refer to the linked post for additional context.\n\n----------------------------------------\n\nLet's revisit our initial problem. We're given the lowest-level representation of a well-abstracting universe, and we want to transform it into its minimal representation / the corresponding well-structured world-model. The tools introduced in Part 1 are insufficient for that: there are two more problems left. This part focuses on one of them.\n\nKey example: Imagine looking at a glider in Conway's Game of Life. At the first time-step, it may occupy the coordinates [0:3]×[0:3]. As time goes on, it would gradually migrate, diagonally moving in the bottom-right direction at the speed of one cell per three time-steps.\n\nIf we take the individual cells to be the random variables to abstract over, in what sense is the glider an abstraction over them? It's a \"virtual object\", drifting across them.\n\nBut if it's not an abstraction over the cells, then over... what? There aren't really any other elements in the picture! Maybe it's an abstraction over sets of cells, with cells being \"subvariables\" over which the glider is synergistic? But how exactly do we define those sets?\n\n(Perhaps it's a synergistic variable over the whole grid/world? I've tried that. That approach was instructive, but it got very clunky very fast.)\n\nOnce you start looking for it, you start spotting problems of this sort all over the place.\n\n----------------------------------------\n\n\n2.1. Wall O' Examples\n * Consider a human mind in a high-tech civilization. At one moment, it may be implemented on biological neurons; at another, it may jump to some artificial substrate; then the lower-level algorithms on which it functions may be changed without change in high-level behavior; then it may be transferred to another star system encoded in a bunch of photons. Is there any stable set of lower-level random variables over which it is an abstraction?\n   * Now imagi",
      "wordCount": 2980
    },
    "tags": [
      {
        "_id": "5GYzBE6q89w74dqfk",
        "name": "Abstraction",
        "slug": "abstraction"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xvCNjLL3GZ6w2BWeb",
    "title": "Synthesizing Standalone World-Models, Part 1: Abstraction Hierarchies",
    "slug": "synthesizing-standalone-world-models-part-1-abstraction",
    "url": null,
    "baseScore": 23,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-23T17:01:29.964Z",
    "contents": {
      "markdown": "*This is part of a series covering* [*my current research agenda*](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models)*. Refer to the linked post for additional context.*\n\n* * *\n\nSuppose we have a dataset consisting of full low-level histories of various well-abstracting universes. For simplicity, imagine them as \\\\(n\\\\)-vectors, corresponding e. g. to the position of each particle at a given moment, or denoting the coordinates of that state in a classical state-space.\n\nSuppose we wanted to set up a system which maps any such sample to its minimal-description-length representation; i. e., to a well-structured world-model that takes advantage of its abstract regularities. How would it work? What are the algorithms that implement search for such representations?\n\nI'll be building the model piece by piece, incrementally introducing complications and proposed solutions to them.\n\nPart 1 covers the following problem: if we're given a set of variables over which an abstraction hierarchy is defined, what is the type signature of that hierarchy, and how can we learn it?\n\n* * *\n\n1.1. The Bare-Bones Setup\n-------------------------\n\nSuppose we have a set of random variables \\\\(X=\\{x_1,\\dots,x_n\\}\\\\). What is the most compact way to represent it without loss of information?\n\nIn the starting case, let's also assume there's no synergistic information.\n\nFormally, we can define the task as follows: define a set of deterministic functions \\\\(Q\\\\) of the variables \\\\(X\\\\) such that \\\\(\\sum_{q\\in Q}H(q)\\\\) is minimized under the constraint of \\\\(H(Q)=H(X)\\\\).\n\nIntuitively, we need to remove all redundancies. If two variables have some redundant information, we need to factor it out a separate variable, and only leave them with the information unique to them. But if two different redundant-information variables \\\\(r_1\\\\), \\\\(r_2\\\\) produced by two subsets \\\\(X_1\\\\), \\\\(X_2\\\\) also have some shared information, we need to factor it out into an even \"higher-level\" redundant-information variable as well, leaving \\\\(r_1\\\\), \\\\(r_2\\\\) with only whatever information is \"uniquely shared\" within subsets \\\\(X_1\\\\) and \\\\(X_2\\\\) respectively. This already hints at a kind of hierarchy...\n\nA natural algorithm for the general case is:\n\n1.  Generate all subsets of \\\\(X\\\\).\n2.  For every subset \\\\(X_i\\\\), define a variable \\\\(q_i\\\\) containing all information redundantly represented in every variable in \\\\(X_i\\\\).\n3.  If there's a pair of \\\\(q_i\\\\), \\\\(q_k\\\\) such that \\\\(X_i\\subset X_k\\\\), then \\\\(q_i\\\\) necessarily contains all information in \\\\(q_k\\\\). Re-define \\\\(q_i\\\\), removing all information present in \\\\(q_k\\\\).\n4.  If a given \\\\(q_i\\\\) ends up with zero (or some \\\\(\\epsilon\\\\)) information, delete it.\n5.  If there's a pair of surviving \\\\(q_i\\\\), \\\\(q_k\\\\) such that \\\\(X_i\\subset X_k\\\\), define their relationship as \\\\(q_i\\succ q_k\\\\).\n\nIntuitively, each subset becomes associated with a variable containing all information *uniquely shared* among the variables in that subset – information that is present in all of them, but nowhere else. Many subsets would have no such information, their \\\\(q\\\\)-variables deleted.\n\nThe \\\\(q\\\\)-variables would then form a natural hierarchy/partial ordering. The highest-level \\\\(q\\\\)-variables would contain information redundant across most \\\\(x\\\\)-variables, and which was initially present in most intermediate-level \\\\(q\\\\)-variables. The lowest-level \\\\(q\\\\)-variables would contain information unique to a given \\\\(x\\\\)-variable.\n\nA way to think about it is to imagine each \\\\(x\\\\)-variable as a set of \"atomic\" random variables – call them \"abstraction factors\" – with some of those factors reoccurring in other \\\\(x\\\\)-variables. What the above procedure does is separating out those factors. (The term \"abstraction factors\" is chosen to convey the idea that those factors, in separation, may not actually constitute independent abstractions. I'll elaborate on that later.)\n\n(All this assumes that we *can* just decompose the variables like this via deterministic functions... But that assumption is already largely baked into the idea that this can be transformed into a well-structured world-model. In practice, we'd use terms like \"approximately extract\", \"approximately deterministic\", et cetera.)\n\nExample: Suppose that we have \\\\(x_1=\\{U_1,A,B,C\\}\\\\), \\\\(x_2=\\{U_2,A,B,C\\}\\\\), \\\\(x_3=\\{U_3,A,B,C,D\\}\\\\), \\\\(x_4=\\{U_4,A,B,D\\}\\\\), \\\\(x_5=\\{U_5,A,B,D\\}\\\\), \\\\(x_6=\\{U_6, A\\}\\\\). The resultant hierarchy would be:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7e98be15c38acd075852f7d0f5fe6c7e4c23663f499f2cee.png)\n\nConsider picking any given node, such as \\\\(C\\\\), and \"restoring\" all information from the factors that are its ancestors, getting \\\\(\\{A,B,C\\}\\\\) here. This procedure recovers the redundant-information variables that existed prior to the factorization/\"extraction\" procedure (Step 3 in the above algorithm).\n\nIn this case, we get:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/115a722e12f8e55b16455809ed3dc2ae0c72825f40bb7ca9.png)\n\nTo me, this structure already vaguely feels like the right \"skeleton\" of an abstraction hierarchy, although it's of course rudimentary so far. Each \"restored\" node would represent a valid abstract object, and the factors are the \"concepts\" it's made of, from the most-abstract (e. g., \\\\(A\\\\) being \"what animal this is\") to most concrete (\\\\(U_2\\\\) being \"details about this specific animal\").\n\nFurther, there's a simple connection to natural latents! If you take any given factor (diagram 1), and condition the \"restored\" variables corresponding to its children-factors (diagram 2) on all of their *other* ancestral factors, this factor becomes the natural latent for those variables. It makes them independent (because variables are independent if conditioned on all of their abstraction factors, since those represent all shared information between them), and they all tell the same information about it (namely, they all contain that factor).\n\nFor example, take \\\\(C\\\\) and \\\\(\\{U_1,A,B,C\\}\\\\), \\\\(\\{U_2,A,B,C\\}\\\\), \\\\(\\{U_3,A,B,C,D\\}\\\\). If we condition those variables on \\\\(A\\\\), \\\\(B\\\\), and \\\\(D\\\\), then \\\\(C\\\\) would become the natural latent for that set. (Specifically, what we do here is: pick a node on the graph, go down to its children, then iteratively follow the arrows pointing to them upwards, and condition on all of the variables you encounter except the one you picked.)\n\nIn fact, this works with *any* set of factors with a total ordering (i. e., without \"siblings\"/incomparable elements); or, equivalently stated, for any subset of factors present in a given \"restored\" variable. For example, any subset of \\\\(\\{A,B,C\\}\\\\) is a natural latent over \\\\(x_1\\\\), \\\\(x_2\\\\), \\\\(x_3\\\\) conditional on \\\\(D\\\\) and that subset's complement. By contrast, the set \\\\(\\{C,D\\}\\\\), or any of its supersets, is not a natural latent for any set of variables. (Well, technically, it'd fill the role for \\\\(\\{x_1,x_2\\}\\\\) conditioned on \\\\(A\\\\) and \\\\(B\\\\), but then \\\\(D\\\\) is effectively just random noise and the actual latent is \\\\(C\\\\) alone.)\n\n* * *\n\n### Examples\n\nSome examples regarding how it all might work in concrete terms, highlighting important aspects of the picture. All assume the above hierarchy. I'll copy it here for convenience:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2cd3790938820bd4e21349009360891556c40dcbad09cd8d.png)\n\n**1\\. Toy store.** Imagine that we have a list of products sold at a toy store. \\\\(\\{A\\}\\\\) is \"a product\", \\\\(\\{A,B\\}\\\\) is \"a toy\", \\\\(\\{A,B,C\\}\\\\) is \"a car toy\", \\\\(\\{A,B,D\\}\\\\) is \"a constructor toy\", \\\\(U\\\\)-variables specify the details of individual products. \\\\(x_3\\\\) is a LEGO car, and \\\\(x_6\\\\) is some non-toy product being sold on the side.\n\nNote that it's specifically the \"restored\" variables that allow this kind of interpretation, not the individual abstraction factors. \\\\(\\{A,B\\}\\\\) is \"a toy\", not \\\\(B\\\\). \\\\(B\\\\) is some \"essence of toy-ness\" such that, when combined with \\\\(A\\\\), it transforms \\\\(A\\\\) from an unspecified \"product\" to a \"toy product\". Taken *in isolation*, \\\\(B\\\\)'s values may not actually have a clear interpretation! They're \"correction terms\", which may be meaningless without knowing what we're correcting *from*. This is the sense in which abstraction factors function as \"ingredients\" for abstractions, rather than as full abstractions in themselves.\n\nNote also the \"uneven stacking\": there are no \"globally synchronized\" abstraction levels, some variables/regions have more layers of organization than others, like \\\\(x_6\\\\) versus all other variables. This makes broader intuitive sense (the tower of abstractions on Earth is taller than on Mars, since Earth has cells/organisms/populations/societies/civilization).\n\n**2\\. Photo**. Imagine that we're looking at a photo. \\\\(\\{A\\}\\\\) is \"the lighting conditions\", \\\\(\\{A,B\\}\\\\) is \"specific objects in the photo\", \\\\(\\{A,B,C\\}\\\\) is \"dogs in the photo\", \\\\(\\{A,B,D\\}\\\\) is \"sculptures in the photo\". \\\\(x_1\\\\) and \\\\(x_2\\\\) are dogs, \\\\(x_3\\\\) is a dog sculpture, \\\\(x_4\\\\) and \\\\(x_5\\\\) are some non-dog sculptures, \\\\(x_6\\\\) is the background.\n\nConsider the idea of \"abstract edits\". If we have this decomposition, we could: (1) decompose the picture into factors, (2) modify a specific factor, e. g. \"lighting conditions\", then (3) re-compose the picture, but now with modified lighting.\n\n**3\\. Machine-Environment System.** Consider a system involving the interactions between some machine and the environment. \\\\(\\{A,B\\}\\\\) is the machine's state (e. g., \"releasing energy\" or \"consuming energy\"), the factors \\\\(C\\\\) and \\\\(D\\\\) add specific details about the states of its two modules, \\\\(U\\\\)-variables from 1 to 5 contain specifics about the states of individual components of the machine, \\\\(U_6\\\\) is information about the state of the external environment, and \\\\(A\\\\) is the system's overall highest-level state (e. g., \"there's currently a positive-feedback loop between the machine and the environment\"). The component \\\\(x_3\\\\) serves as the interface between the two modules; other components belong to the modules separately.\n\nNotes:\n\n*   Drawing on what I talked about in the first example: if we want to know the full state of a given component, we have to \"restore\" the variables: add information about the overall system-state and the state of modules into it. Absent that information, the factor may lack clear interpretation. What would \"the individual state of the detail, without considering the overall state of the system\" mean? It would be a \"correction term\" for the overall state, potentially meaningless without knowing that state.\n*   Potentially obvious, but: note that conditioning on a factor and \"restoring\" a factor are very different procedures. \"Restoration\" involves, to wit, restoring the model to its full complexity, while conditioning on a factor removes complexity. Example: conditioning on \\\\(\\{A,B\\}\\\\) removes the mutual information between the modules corresponding to \\\\(C\\\\) and \\\\(D\\\\), making them independent, while \"restoring\" \\\\(\\{A,B\\}\\\\) returns mutual information to them, recovering whatever causal entanglement they had.\n*   Suppose we wanted to model the interactions between some set of high-level systems. What factors would we need to \"restore\" to model them at full fidelity? The set of the ancestors of those systems' specifics: meaning we can disregard lower levels and any \"sibling\" subgraphs. You can imagine it as a kind of \"abstraction cone\".\n\n**4\\. Machine-Environment System, #2**. Consider a machine-environment system made of gears and pulleys. \\\\(\\{A,B,C\\}\\\\) is \"a gear\", \\\\(\\{A,B,D\\}\\\\) is \"a pulley\", \\\\(\\{A,B\\}\\\\) is \"human manufacturing practices\", \\\\(x_6\\\\) is \"this external environment\", \\\\(\\{A\\}\\\\) is \"the overall type of this system\". \\\\(x_1\\\\) and \\\\(x_2\\\\) are specific gears, \\\\(x_4\\\\) and \\\\(x_5\\\\) are specific pulleys, \\\\(x_3\\\\) is a [toothed pulley](https://en.wikipedia.org/wiki/Toothed_belt).\n\nThis system can be *the same* system as in example (3): it's just that here, we focus on a different subgraph in our partial-order abstraction diagram, the \"sibling\" of the one in (3). (3) focuses on the system as an abstraction over its constituent parts, while (4) focuses on representing the constituents as instances of commonly reoccurring types of objects.\n\nNote how all of the tools described above can be combined. We can take this system, decompose it into the representation in (4), make some abstract edit to the gears' manufacturing practices, re-assemble the system, re-disassemble it into the (3) representation, take the nodes \\\\(C\\\\) and \\\\(D\\\\), condition on \\\\(A\\\\) (to remove interactions with the environment), and restore \\\\(B\\\\) to them (to recover the dynamics between the modules). Intuitively, this corresponds to evaluating how the interactions between the modules change if we switch up our gear manufacturing practices; perhaps running the corresponding simulation.\n\n**5\\. People.** Suppose that the \\\\(x\\\\)-variables are six different people living in the same city. \\\\(\\{A\\}\\\\) is \"the city's state\", \\\\(\\{A,B\\}\\\\) is \"the state of the city's socioeconomic sphere\", \\\\(\\{A,B,C\\}\\\\) is \"the state of a specific company\", \\\\(\\{A,B,D\\}\\\\) is \"the state of a popular political movement\". Three people work at the company, three people participate in the movement, one person is part of both the company and the movement, and one person is unemployed and not part of the movement.\n\nFocus on \\\\(x_3\\\\) specifically here: the person that's part of both the corporate dynamics and political dynamics. Note that (a) those dynamics can be largely non-interacting, and (b) both of those dynamics \"share\" the substrate on which they run: the third person. This is \"polysemanticity\" of a sort: a given low-level system can simultaneously implement *several* independent high-level systems. (This is essentially just another way of looking at \"sibling subgraphs\", but it conveys different intuitions.)\n\n* * *\n\nDistilling, some notable features are:\n\n*   **\"Uneven stacking\":** there are no \"globally synchronized\" abstraction levels: some regions have more levels of organization than others. (So we can't travel up the levels by tuning some *global* acceptable-approximation-error dial.)\n*   **\"Abstract editing\":** decomposing and re-composing a given sample of a well-abstracting system lets us edit its abstract features.\n*   **\"Sibling subgraphs\":** a given sample can have several complex abstraction-hierarchy subgraphs that don't \"interact\" at all.\n    *   (Another illustrative example: a given historical event having several narratives that are simultaneously true, but focus on different aspects of it.)\n*   **\"Generalized polysemanticity\":** a given low-level system can run several independent (or sparsely interacting) high-level systems.\n    *   (Another illustrative example: the literal polysemantic neurons in language models, perhaps.)\n*   **\"Abstraction cones\":** if we want to model a given system, we only need to pay attention to its ancestral factors.\n    *   (Another illustrative example: if we want to model Earth, we don't need to model the rest of the universe in detail: high-level summaries suffice.)\n\nI think it's pretty neat that all of this expressivity falls out of the simple algorithm described at the beginning. Granted, there's a fair amount of \"creative interpretation\" going on on my end, but I think it's a promising \"skeleton\". However, major pieces are still missing.\n\n* * *\n\n1.2. Synergistic Information\n----------------------------\n\nWe'd like to get rid of the \"no synergistic information\" assumption. But first: what *is* synergistic information?\n\nUsually, it's described as \"the information the set of variables \\\\(X\\\\) gives us about some target variable \\\\(Z\\\\), which we can't learn by inspecting any strict subset of \\\\(X\\\\) in isolation\". The typical example is a XOR gate: if \\\\(\\text{XOR}(X,Y)=Z\\\\), and \\\\(X\\\\) and \\\\(Y\\\\) are independent random bits, looking at either bit tells us nothing about \\\\(Z\\\\), while both bits taken together let us compute \\\\(Z\\\\) exactly.\n\nBut note that synergistic information can be defined by referring *purely* to the system we're examining, with no \"external\" target variable. If we have a set of variables \\\\(X=\\{x_1,\\dots,x_n\\}\\\\), we can define the variable s such that \\\\(I(X;s)\\\\) is maximized under the constraint of \\\\(\\forall X_i\\in (\\mathcal{P}(X)\\setminus X):I(X_i;s)=0\\\\). (Where \\\\(\\mathcal{P}(X)\\setminus X\\\\) is the set of all subsets of \\\\(X\\\\) except \\\\(X\\\\) itself.)\n\nThat is: s conveys information about the *overall* state of \\\\(X\\\\) without saying anything about any specific variable (or set of variables) in it.\n\nThe trivial example are two independent bits: \\\\(s\\\\) is their XOR.\n\nA more complicated toy example: Suppose our random variable \\\\(X\\\\) is a 100-by-100 grid of binary variables \\\\(x_i\\\\), and each sample of \\\\(X\\\\) is a picture where some 8 adjacent variables are set to 1, and all others to 0. \\\\(s\\\\) can then return the *shape* the activated variables make. Across all realizations, it tells us approximately zero information about any given subset (because the number of active variables is always the same), but we still learn something about \\\\(X\\\\)'s overall state.\n\nThis is the \"true nature\" of synergistic information: it tells us about the \"high-level\" features of the joint samples, and it ignores which specific low-level variables implement that feature.\n\nAnother example: emergent dynamics. Consider the difference between the fundamental laws of physics and Newtonian physics:\n\n*   Fundamental laws mediate the interactions between lowest-level systems.\n*   Newtonian physics mediate the interactions between low-velocity macro-scale objects. To determine whether Newtonian physics apply, you have to look at large segments of the system *at once*, instead of at isolated fundamental parts.\n\nI. e.: unlike with the fundamental laws, \"does this system approximately implement Newtonian physics, yes/no?\" depends on synergistic information in large sets of fundamental particles, and many conditions need to be met *simultaneously* for the answer to be \"yes\".\n\nNote also that synergistic information is effectively the \"opposite\" of redundant information. Conditioning lower-level variables on synergistic information *creates*, not removes, mutual information between them. (Consider conditioning the XOR setup on the value of the input \\\\(Y\\\\). Suddenly, there's mutual information between the other input \\\\(X\\\\) and the output \\\\(Z\\\\)! Or: consider conditioning a bunch of fundamental particles on \"this is a Newtonian-physics system\". Suddenly, we know they have various sophisticated correspondences!)\n\n* * *\n\n1.3. Incorporating Synergistic Information\n------------------------------------------\n\nHow can we add synergistic information into our model from 1.1?\n\nOne obvious approach is to just treat synergistic variables as... variables. That is:\n\n*   For every subset of \\\\(X\\\\), compute its (maximal) synergistic variable.\n*   Define \\\\(X^*=X\\cup S\\\\), where \\\\(S\\\\) is the set of all synergistic variables.\n*   Treat \\\\(X^*\\\\) as a set of variables with no synergistic information, and run the algorithm from 1.1 on it.\n\nAs far as I can tell, this mostly just works. Synergistic variables can have shared information with other synergistic variables, or with individual \\\\(x_i\\\\) variables; the algorithm from 1.1. handles them smoothly. They always have zero mutual information with their underlying \\\\(x\\\\)-variables, and with any synergistic variables defined over subsets of their underlying \\\\(x\\\\)-variables, but that's not an issue.\n\nNote that no further iteration is needed: we don't need to define synergistic variables over sets of synergistic variables. They would just contain parts of the information contained in a \"first-iteration\" higher-level synergistic variable, and so the algorithm from 1.1 would empty out and delete them.\n\nImportant caveats:\n\n1.  This, again, assumes that synergistic information is (approximately) deterministically extractible.\n2.  We might actually want to have some notion of \"synergistic variable over synergistic variables\", so some tinkering with the algorithm may be needed. (I haven't thought in depth about the relevant notion yet, though.)\n3.  The resultant representation is not the shortest possible representation. Consider a two-bit XOR: the overall entropy is 2 bits, but if we include the synergistic variable, we'll end up with a three-bit description.\n\n(3) is kind of very majorly inconvenient. It's clear why it happens: as stated, the synergistic variable does not actually \"extract\" any entropy/information from the variables on which it's defined (the way we do when we factor out redundant information), so some information ends up double-counted. I do have some ideas for reconciling this with the overall framework...  \n  \nThe current-best one (which may be fundamentally confused, this reasoning is relatively recent) is:\n\n*   Once we add more details to this setup, we would notice that the \"random variables\" it's working to decompose are not just the within-the-model inputs it's currently treating at random variables, but also *the inferred model itself*.\n*   Example: If we have a series of random variables which represent the evolution of a Newtonian-physics system, and we infer their \"joint probability distribution\" in the form of Newtonian laws and the system's Lagrangian, we won't stop there. The next step would involve looking at the library of systems whose laws we've inferred, and trying to abstract *over those laws*.\n*   I. e., we would treat parts of the previous level's probabilistic *structure* as the next level's *random variables* to be decomposed.\n*   And there's a sense in which synergistic information is information about the system/JPD, not about the individual variables in it. Some more about it in 1.5 and Part 2.\n    *   Briefly: conditioning on synergistic information *creates* mutual information, which means any structure with entangled variables can be considered a structure with independent-by-default variables conditioned on a synergistic variable.\n*   Conditioning on the synergistic information, thus, may be semantically equivalent to saying which type of high-level system a given low-level system implements/which class of objects this object is an instance of/etc.\n*   Also: consider what \"removing\" synergistic information from the system would mean. It would, essentially, mean removing the high-level coordination of the variables' values; treating a joint sample of several variables as independent samples of those variables.\n*   So, from the larger point of view, if we expand our view of what counts as a \"random variable\", and therefore the total entropy to which the sum of entropies of our  unique/redundant/synergistic variables should add up, it may not actually be \"overcounting\" at all!\n\nAgain, this is a relatively fresh line of argument, and it may be obviously flawed from some angle I didn't yet look at it from.\n\n* * *\n\n1.4. Partial Information Decomposition\n--------------------------------------\n\nAs it turns out, the picture we now have cleanly maps to a known concept from information theory: [partial information decomposition](https://en.wikipedia.org/wiki/Partial_information_decomposition) (PID); or [partial entropy decomposition](https://arxiv.org/abs/1702.01591).\n\n[This paper](https://arxiv.org/pdf/1004.2515) provides a basic overview of it. (It uses a very adhockish definition for redundant information, but is otherwise fine.)\n\nPID's steps mirror pretty much all of the steps we've gone through so far. Starting from some set of variables \\\\(X\\\\) with entropy \\\\(H(X)\\\\), PID:\n\n*   Quantifies synergistic information for all subsets of variables.\n*   Places those synergistic-information \"atoms\" on equal standing with the initial \\\\(x\\\\)-variables.\n*   Quantifies redundant information across all subsets of variables in the combined set \\\\(X\\cup S\\\\).\n*   Subtracts things around the place, ensuring that each \"atom\" only contains the information unique to it. (E. g., information \"uniquely shared\" between two variables: present in them and nowhere else.)\n*   The entropies of the atoms then add up to \\\\(H(X)\\\\).\n\nThere's a clear correspondence between PID's \"atoms\" and my abstraction factors, the pre-subtraction atoms are equivalent to my \"restored\" nodes, there's a procedure isomorphic to deleting \"emptied\" variables, et cetera.\n\nMajor difference: PID does not define any *variables*. It just expands the expression \\\\(H(X)\\\\) *quantifying* the total entropy into the sum of entropies of those atoms. Which is why the entropy of all atoms is able to add up to the total entropy with no complications, by the way: we have no trouble subtracting synergistic-information entropy.\n\nOne issue with PID worth mentioning is that they haven't figured out what measure to use for quantifying multivariate redundant information. It's the same problem [we seem to have](https://www.lesswrong.com/posts/sCNdkuio62Fi9qQZK/usd500-usd500-bounty-problem-does-an-approximately). But it's probably not a major issue in the setting we're working in (the well-abstracting universes).\n\nAnd if we're assuming *exact* abstraction in our universe, I expect we'd get *exact* correspondence: every PID information atom would correspond to an abstraction factor, and the factor's entropy would be the value of that PID atom.\n\nOverall, the fact that the framework incrementally built up by me coincidentally and near-perfectly matches another extant framework is a good sign that I'm getting at something real. In addition, PID fundamentally feels like the \"right\" way to do things, rather than being some arbitrary ad-hoc construction.\n\nAlso: this offers a new way to look at the problem. The goal is to find a correct \"constructive\" way to do partial entropy decomposition. The functions for learning the abstract-factor variables may, in fact, directly correspond to the functions defining the \"correct\" way to do partial entropy decomposition.\n\n* * *\n\n1.5. Abstractions and Synergistic Variables\n-------------------------------------------\n\nLet's see what adding synergistic variables to the broader framework lets us do.\n\nI'll be focusing on natural latents here. As stated in 1.1, in my framework, a natural latent \\\\(\\Lambda\\\\) can be defined as any set of abstraction factors with a total order (considered over the variables in the appropriate set conditioned on their other ancestral factors).\n\nLet's consider a dog at the atomic scale. Intuitively, the correct theory of abstraction should be able to define the relationship between the dog and the atoms constituting it. However, there are complications:\n\n*   We can't learn that we're looking at a dog by inspecting any given atom constituting it. The answer to \"what animal is this?\" is not redundantly encoded in each atom; it is not a natural latent (or an abstraction factor) over those atoms.\n*   We don't need to look at every atom constituting the dog at once to conclude we're looking at a dog. Very small subsets (the head, the paw, the heart, a DNA string) would suffice. Which means \"a dog\" is not the value of the synergistic variable over all of those atoms.\n*   It's also not the value of the synergistic variable over any *specific* subset of those atoms. Looking *either* at the head *or* at the paw *or* at any one of the dog's DNA strings would suffice.\n*   We can't use any \"unsophisticated\" measures, such as \"this is a dog if we can learn that it's a dog from looking at any random sufficiently large subset of the atoms constituting it\". The size of the subset changes depending on which part we're looking at. We'd need dramatically fewer atoms if we happened to sample a contiguous volume containing a DNA string, than if we sample individual atoms all around the dog's body. We need something more \"context-aware\".\n*   We don't necessarily need to look at \"atoms\" at all. High-level abstract features would also suffice: the \"shape\" of the dog's macro-scale \"head\", or the sample of its molecular-scale \"DNA\", or the \"sound\" of its \"bark\"...\n*   And the \"dog-ness\" is invariant under various perturbations, but those perturbations are also \"context-aware\". For example, different dog-skulls may have significant macro-scale differences while still being recognizable as dog-skulls, whereas some comparatively tiny modifications to a DNA string would make it unrecognizable as dog DNA.\n\nWhat I propose is that, in this context, \"a dog\" is the value of the natural latent over (functions of) *specific synergistic variables*. \"A DNA string\", \"the shape of this animal's skull\", \"the sounds this animal makes\", \"the way this animal thinks\" *all* let us known what animal we're looking at; *and* they're exactly the features made independent by our knowing what animal this is; *and*, intuitively, they contain some synergistic information.\n\nHowever, that seems to just push the problem one level lower. Isn't \"a DNA string\" itself in same position as the dog relative to the atoms (or subatomic particles) constituting it, with all the same complications? I'll get to that.\n\nFirst, **a claim:** Every natural latent/abstraction \\\\(\\Lambda\\\\) is a function of the synergistic variable over the set of \"subvariables\" \\\\(\\{x_i^1,\\dots x_i^m\\}\\\\) constituting the variables \\\\(x_i=f_i(\\{x_i^1,\\dots,x_i^m\\})\\\\) in the set of variables \\\\(X=\\{x_1,\\dots,x_n\\}\\\\) for which \\\\(\\Lambda\\\\) is a natural latent. (Degenerate case: the synergistic variable s over a one-variable set \\\\(\\{x\\}\\\\) is \\\\(x\\\\).)\n\nLet me unpack that one.\n\nWe have some set of variables \\\\(X=\\{x_1,\\dots,x_n\\}\\\\) over which a natural latent \\\\(\\Lambda\\\\) is defined – such as a set of n dogs, or a set of n features of some object. \\\\(\\Lambda\\\\) is a function which can take any \\\\(x_i\\\\) as an input, and return some information, such as properties of dogs (or e. g. nuclear reactors).\n\nBut those individual \\\\(x_i\\\\) are not, themselves, in the grand scheme of things, necessarily \"atomic\". Rather, they're themselves (functions of) sets of some other variables. And relative to *those* lower-level variables – let's dub them \"subvariables\" \\\\(x_i^j\\\\), with \\\\(x_i=f_i(\\{x_i^j\\})\\\\) – the function \\\\(\\Lambda:\\{x_i^j\\}\\to \\text{label}\\\\) is a function whose value is dependent on the synergistic variable.\n\n**Example:** Consider a set of programs \\\\(P=\\{P_1,\\dots,P_n\\}\\\\) which all implement some function F, but which implement it in different ways: using different algorithms, different programming languages, etc. The set of programs is independent given \\\\(F\\\\): it's a natural latent/abstraction over it. But each program \\\\(P_i\\\\) itself consists of a set of lower-level operations \\\\(\\{p_i^j\\}\\\\), and relative to those, \\\\(F\\\\) is a *synergistic variable*: *all* \\\\(\\{p_i^j\\}\\\\) must be in the correct places for the emergent behavior of \"implements \\\\(F\\\\)\" to arise. Simultaneously, the presence of any specific basic operation \\\\(p_i^j\\\\), especially in a specific *place* in the program's code, is not required, so \\\\(F\\\\) provides little information about them.\n\n**Another example:** Consider the difference between \"this specific dog\" and \"the concept of a dog\". What we'd been analyzing above is the \"this specific dog\" one: an abstraction redundantly represented in some synergistic features/subsystems forming a specific physical system.\n\nBut \"the concept of a dog\" is a *synergistic* variable over *all* of those features/subsystems. From the broader perspective of \"what kind of object is this?\", *just* the presence of a dog's head is insufficient. For example, perhaps we're looking at a dog's statue, or at some sort of biotech-made chimera?\n\nHere's what I think is going on here. Before we went \"what type of animal is this?\" \\\\(\\to\\\\) \"this is a dog\", there must have been a step of the form \"what type of object is this?\"\\\\(\\to\\\\) \"this is a ordinary animal\". And the mutual information between \"dog's head\", \"dog DNA\", and all the other dog-features, only appeared *after* we conditioned on the answer to \"what object is this?\"; after we conditioned on \"this is an ordinary animal\".\n\nIf we narrow our universe of possibilities to the set of animals, then \"a dog\" is indeed deducible from any feature of a dog. But in the whole wide world with lots of different weird things, \"a dog\" is defined as an (almost) *exhaustive list* of the properties of dog-ness. (Well, approximately, plus/minus a missing leg or tail, some upper/lower bounds on size, etc. I'll explain how that's handled in a bit.)\n\nThose descriptions are fairly rough, but I hope it's clear what I'm pointing at. Conditioning some variables on some latent while treating that latent as encoding synergistic information over those variables may create redundant information corresponding to a *different* valid natural latent. There's also a natural connection with the idea I outlined at the end of 1.3, about synergistic variables creating probabilistic *structures*.\n\nThis is also how we handle the \"downwards ladder\". Conditional on \"this is an animal\", \"this is a dog\" is redundantly represented in a bunch of lower-level abstract variables like \"DNA\" or \"skull\". Whether a given structure we observe qualifies as one of those variables, however, may likewise be either information redundant across some even-lower-level abstractions (e. g., if we observe a distinct part of dog DNA conditional on \"this is part of a whole DNA string\"), or it may be synergistic information (if the corresponding universe of possibilities isn't narrowed down, meaning we need to observe the whole DNA string to be sure it's dog DNA).\n\n(Note: In (1.1), I described situations where natural latents are valid over some set only conditional on other natural latents. The situation described here is a *different* way for that to happen: in (1.1), conditioning on other latents removed redundant information and let our would-be natural latent induce independence; here, conditioning on a latent *creates* redundant information which the new natural latent is then defined with respect to.)\n\nMoving on: I claim this generalizes. Formally speaking, every natural latent/abstraction \\\\(\\Lambda\\\\) is a function of the synergistic variable over the set of subvariables \\\\(\\{x_i^1,\\dots x_i^m\\}\\\\) constituting the variables \\\\(x_i=f_i(\\{x_i^1,\\dots,x_i^m\\})\\\\) in the set of variables \\\\(X=\\{x_1,\\dots,x_n\\}\\\\) for which \\\\(\\Lambda\\\\) is a natural latent. Simultaneously, \\\\(\\Lambda\\\\) is the redundant information between the \\\\(x_i\\\\) variables. (And, again, the synergistic variable \\\\(s\\\\) over a one-variable set \\\\(\\{x\\}\\\\) is just \\\\(s=x\\\\).)\n\nGeneral justification: Suppose that \\\\(x_i=f_i(\\{x_i^1,\\dots x_i^m\\})\\\\) contains all necessary information about \\\\(\\Lambda\\\\) even without some subvariable \\\\(x_i^j\\\\). That is,\n\n\\\\\\[I(\\Lambda;\\{x_i^1,\\dots,x^m_i\\}\\setminus\\{x_i^j\\})=I(\\Lambda;\\{x_i^1,\\dots,x_i^m\\})\\\\\\]\n\nIn that case... Why is \\\\(x_i^j\\\\) there? Intuitively, we want \\\\(x_i\\\\) to be, itself, the *minimal* instance of the latent \\\\(\\Lambda\\\\), and yet we have some random noise \\\\(x_i^j\\\\) added to it. Common-sense check: our mental representation of \"a dog\" doesn't carry around e. g. random rocks that were lying around.\n\nOn the contrary, this would create problems. Suppose that we feel free to \"overshoot\" regarding what subvariables to put into \\\\(x_i\\\\), such that sometimes we include irrelevant stuff. Then variables in some subset \\\\(X_1\\subset X\\\\) may end up with *the same* unrelated subvariables added to them (e. g., dogs + rocks), and variables in a different subset \\\\(X_2\\subset X\\\\) may have different unrelated subvariables (e. g., blades of grass). \"A dog\" would then fail to make all variables in \\\\(X\\\\) independent of each other.\n\nIndeed, \\\\(X\\\\) would not have a natural latent at all. We'd end up with two abstractions, \"dog + rocks\" and \"dog + grass\"... Except more likely, there'd be much more different types of unrelated subvariables added in, so we won't be able to form a sufficiently big dataset for forming a \"dog\" abstraction to begin with.\n\nSo the \"nuisance\" subvariables end up \"washed out\" at the abstraction-formation stage. Meaning all subvariables constituting each \\\\(x_i\\\\) are *necessary* to compute \\\\(\\Lambda\\\\).\n\nAdmittedly, this still leaves the possibility that \\\\(\\Lambda\\\\) is a function of the unique informations in each \\\\(x_i^j\\\\), not the synergistic information. I think it makes less intuitive sense, however:\n\n1.  Abstract objects seem to be distinct entities, rather than just the sums of their parts.\n2.  This would mean that learning that some set of subvariables \\\\(x_i^j\\\\) is of the type \\\\(\\Lambda\\\\) would not create mutual information between *all* of them: we would not know/suspect what specific \"pieces\" there are.\n\n**Nitpick:** Suppose that the set \\\\(\\{x_i^1,\\dots,x^m_i\\}\\\\) is such that every \\\\(k\\\\)-sized subset of it has a nontrivial synergistic variable which contains the same information.\n\nExample: [Shamir's scheme](https://en.wikipedia.org/wiki/Shamir%27s_secret_sharing), where knowing any \\\\(k\\\\) shares out of \\\\(n\\\\), with \\\\(n\\\\) arbitrarily larger than \\\\(k\\\\), lets you perfectly decrypt the ciphertext, but knowing even one share fewer gives you zero information about the plaintext. Intuitively, all those \\\\(n\\\\) shares should be bundled together into one abstraction... but the above argument kicks out all but some random subset k of them, right?\n\nAnd we can imagine similar situations arising in practice, with some abstractions defined over any small subset of some broader set of e. g. features. See the previous dog example: dogs usually have four legs, but some of them have three; most have tails, but some don't; etc.\n\nBut the framework as stated handles this at a different step. Recall that we compute the synergistic variables for all subsets, add them to the pool of variables, and then try defining redundant-information variables for all subsets. Since the synergistic variables for all subsets of Shamir shares contain the same information, they would be bundled up when we're defining redundant-information variables.\n\nSimilar with dogs: we'd end up with sets of features sufficient for a dog, without there necessarily being a minimal set of features which is both (1) sufficient for \"this is a dog\", and (2) is a subset of each of those sets.\n\nYou may argue it is clunky. It very much is; a better, more general way will be outlined in the next part.\n\n**Clarification:** some things which I *wasn't* necessarily arguing above.\n\n*   The latent \\\\(\\Lambda\\\\) is not necessarily *only* the synergistic information over \\\\(\\{x_i^1,\\dots x_i^m\\}\\\\). There may be some mutual information between \\\\(\\Lambda\\\\) and the individual \\\\(x_i^j\\\\), meaning \\\\(\\Lambda\\\\) may contain unique or redundant informations as well. Though note that it gets tricky in terms of the wider framework we've built up from (1.1) onward. To restate, in it, natural latents are (sets of) abstraction factors, which are redundant informations in the set of variables \\\\(\\{x_1,\\dots,x_n\\}\\\\). Therefore...\n    *   If a natural latent also includes some information redundant across a given \\\\(\\{x_i^1,\\dots x_i^m\\}\\\\), this implies this information is redundant across *all* \\\\(x_i^j\\\\), for all \\\\(i\\in[1:n]\\\\) and all \\\\(j\\in[1:m]\\\\). Which raises a question: shouldn't it be an abstraction over all \\\\(x_i^j\\\\)s instead, not \\\\(x_i\\\\)s?\n    *   And if it includes unique information from a specific \\\\(x_i^j\\\\), this implies that there's a subvariable containing this type of information in every \\\\(x_i\\\\).\n*   The latent \\\\(\\Lambda\\\\) does not necessarily contain *all* information in the synergistic variable. Some synergistic variables may contain information unique to them.\n*   The sets of subvariables \\\\(\\{x_i^1,\\dots x_i^m\\}\\\\) does not necessarily have a nontrivial natural latent itself.\n    *   (It works in the dog/animal case, and I struggle to come up with a counterexample, but this isn't ruled out. Maybe there's an intuitive case for it I don't see yet...)\n    *   Note that if we claim that, it would create a full \"ladder to the bottom\": if every set of sub-variables forms the well-abstracting set for the lower level, then each of those subvariables is made up of sub-subvariables representing even-lower-level abstraction, etc. Which is an interesting claim, and it fits with the dog \\\\(\\to\\\\) DNA example...\n        *   This is the reason we might want to modify our algorithm to allow \"synergistic variables over synergistic variables\", by the way, as I'd briefly mentioned in 1.3.\n*   Similarly, the set \\\\(\\{x_1,\\dots,x_n\\}\\\\) does not necessarily have a nontrivial synergistic variable.\n    *   (Again, it works in the dog case, organs/dog-features over which \"this specific dog\" abstracts synergistically defining \"the concept of a dog\". But what's the synergistic information in \"this bunch of randomly selected dogs\" over which \"the concept of a dog\" abstracts?)\n\n**Aside**: I think there's also a fairly straightforward way to generalize claims from this section to causality. Causality implies correlation/mutual information between a system's states at different times. The way this information is *created* is by conditioning a history (a set of time-ordered system-states) on a synergistic variable defined over it, with this synergistic variable having the meaning of e. g. \"this is a Newtonian-mechanics system\". This likewise naturally connects with/justifies the end of 1.3, about interpreting synergistic variables as information about the distribution, rather than about specific joint samples.\n\n**Summing up**:\n\n*   Natural latents seem to be functions of the synergistic information of the \"subvariables\" of the variables they abstract over. Information about latents is redundantly represented in those synergistic variables. (In a way, you can think about those synergistic variables as \"markers\" for the abstractible objects; or perhaps as defining their boundaries.)\n*   Conditioning subvariables on the natural latent *synergistic* with respect to them may lead to the formation of a different natural latent, by way of creating information redundant across them with respect to which that different natural latent may be defined.\n*   This lets us handle some tricky cases where a natural latent, intuitively, has the properties of both a synergistic and a redundant-information variable.\n    *   In the dog example, \"this is a dog\" is either synergistic over all dog-features, or redundant across those same features if those features are conditioned on the synergistic information \"this is an ordinary animal\".\n    *   In turn, the features across which \"this is a dog\" is redundant are themselves lower-level natural latents, which may also be *either* synergistic over some subvariables, *or* redundant across some subvariables (if the subvariables are also conditioned on some appropriate abstraction).\n*   It's all (partially) theoretically justified by the demands of forming good abstractions to begin with.\n\n* * *\n\n1.6. Summary\n------------\n\nPart 1 outlined my high-level model of how we can learn an abstraction hierarchy given a clean set of low-level variables \\\\(X\\\\) out of which it \"grows\".\n\nNamely: we can cast it as a \"constructive\" cousin of partial information decomposition. The algorithm goes as follows:\n\n*   Compute the synergistic variable for each subset of the set of initial variables \\\\(X\\\\).\n*   Define \\\\(X^*=X\\cup S\\\\), where \\\\(S\\\\) is the set of all synergistic variables, \"forgetting\" that the synergistic variables are synergistic.\n*   For every subset \\\\(X_i\\\\) of the expanded pool of variables \\\\(X^*\\\\), we compute the variable recording information redundant across that subset, \\\\(q_i\\\\).\n*   If there's a pair of \\\\(q_i\\\\), \\\\(q_k\\\\) such that \\\\(X_i\\subset X_k\\\\), re-define \\\\(q_i\\\\), removing all information present in \\\\(q_k\\\\).\n*   If a given \\\\(q_i\\\\) ends up with zero (or some \\\\(\\epsilon\\\\)) information, delete it.\n*   If there's a pair of surviving \\\\(q_i\\\\), \\\\(q_k\\\\) such that \\\\(X_i\\subset X_k\\\\), define their relationship as \\\\(q_i\\succ q_k\\\\).\n\nEach \\\\(q\\\\)-variable can then be considered an \"abstraction factor\". Any set of abstraction factors with a total ordering functions as a natural latent for the lowest-level factor's children conditional on that children's other ancestral variables.\n\nThis setup has a rich set of features and functionalities, in line with what we'd expect the theory of abstraction to provide, and it seems to easily handle a wide variety of thought experiments/toy models/case studies. Notably, it offers a way to deal with situations in which abstractions seem to share the properties of redundant-information variables and synergistic-information variables: by defining abstractions as natural latents of functions of synergistic variables.\n\nOne notable unsolved fatal problem/small inconvenience is that the presence of synergistic variables directly opposes the idea of producing the minimal representation of the initial set of variables. I've not dealt with this yet, but I expect there to be a simple conceptual explanation. A promising line of argument involves treating the probabilistic *structure* as just higher-abstraction-level random variables, which increases the total entropy to which our decomposition should add up to.\n\nAnother problem is, of course, the computational intractability. The above algorithm features several steps steps where we learn a specific variable for every subset of a (likely vast) amount of low-level variables we're given. Obviously, a practical implementation would use some sort of heuristics-based trick to decide on variable-groupings to try.\n\nAll those caveats aside, the problem of learning an abstraction hierarchy (the way it's defined in my framework) is now potentially reducible to a machine-learning problem, under a bunch of conditions. That is:\n\n1.  *If* we can figure out what exact definitions to use for synergistic and redundant variables,\n2.  *And* there are no other underlying fundamental problems,\n3.  *And* we're given a set of variables on which the abstract hierarchy is defined, plus a sufficiently big dataset for training,\n4.  *Then* we can set up the ML training process for learning the abstraction hierarchy.\n\n(3) is actually a deceptively major problem. Next part is about that.\n\n* * *\n\nWhat I'm particularly interested in for the purposes of [the bounties](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models#Bounties): Is there some better way to handle synergistic information here? A more \"correct\" definition for synergistic-information variables? A different way to handle the \"overcounting\" problem?",
      "plaintextDescription": "This is part of a series covering my current research agenda. Refer to the linked post for additional context.\n\n----------------------------------------\n\nSuppose we have a dataset consisting of full low-level histories of various well-abstracting universes. For simplicity, imagine them as n-vectors, corresponding e. g. to the position of each particle at a given moment, or denoting the coordinates of that state in a classical state-space.\n\nSuppose we wanted to set up a system which maps any such sample to its minimal-description-length representation; i. e., to a well-structured world-model that takes advantage of its abstract regularities. How would it work? What are the algorithms that implement search for such representations?\n\nI'll be building the model piece by piece, incrementally introducing complications and proposed solutions to them.\n\nPart 1 covers the following problem: if we're given a set of variables over which an abstraction hierarchy is defined, what is the type signature of that hierarchy, and how can we learn it?\n\n----------------------------------------\n\n\n1.1. The Bare-Bones Setup\nSuppose we have a set of random variables X={x1,…,xn}. What is the most compact way to represent it without loss of information?\n\nIn the starting case, let's also assume there's no synergistic information.\n\nFormally, we can define the task as follows: define a set of deterministic functions Q of the variables X such that ∑q∈QH(q) is minimized under the constraint of H(Q)=H(X).\n\nIntuitively, we need to remove all redundancies. If two variables have some redundant information, we need to factor it out a separate variable, and only leave them with the information unique to them. But if two different redundant-information variables r1, r2 produced by two subsets X1, X2 also have some shared information, we need to factor it out into an even \"higher-level\" redundant-information variable as well, leaving r1, r2 with only whatever information is \"uniquely shared\" within subsets",
      "wordCount": 6866
    },
    "tags": [
      {
        "_id": "5GYzBE6q89w74dqfk",
        "name": "Abstraction",
        "slug": "abstraction"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "LngR93YwiEpJ3kiWh",
    "title": "Synthesizing Standalone World-Models (+ Bounties, Seeking Funding)",
    "slug": "synthesizing-standalone-world-models-bounties-seeking",
    "url": null,
    "baseScore": 63,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2025-09-22T19:06:14.592Z",
    "contents": {
      "markdown": "**tl;dr:** I outline my research agenda, post bounties for poking holes in it or for providing general relevant information, and am seeking to diversify my funding sources. This post will be followed by several others, providing deeper overviews of the agenda's subproblems and my sketches of how to tackle them.\n\n* * *\n\nBack at the end of 2023, I [wrote](https://www.lesswrong.com/posts/hx5wTeBSdf4bsYnY9/idealized-agents-are-approximate-causal-mirrors-radical) the following:\n\n> I'm fairly optimistic about arriving at a [robust solution to alignment](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment) via agent-foundations research in a timely manner. (My semi-arbitrary deadline is [2030](https://www.lesswrong.com/posts/BseaxjsiDPKvGtDrm/we-choose-to-align-ai), and I expect to arrive at intermediate solid results by EOY 2025.)\n\nOn the inside view, I'm pretty satisfied with how that is turning out. I have a high-level plan of attack which approaches the problem from a novel route, and which hopefully lets us dodge a bunch of major alignment difficulties (chiefly [the instability of value reflection](https://ifanyonebuildsit.com/4/reflection-and-self-modification-make-it-all-harder), which I am MIRI-tier [skeptical](https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model#6D__Meta_Cognition) of tackling directly). I expect significant parts of this plan to change over time, as they turn out to be wrong/confused, but the overall picture should survive.\n\nConceit: We don't seem on the track to solve the full AGI alignment problem. There's too much [non-parallelizable research](https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development) to do, too few people competent to do it, and not enough time. So we... don't try. Instead, we use adjacent theory to produce a *different* tool powerful enough to get us out of the current mess. Ideally, without having to directly deal with AGIs/agents at all.\n\nMore concretely, the ultimate aim is to figure out how to construct a sufficiently powerful, safe, easily interpretable, well-structured world-model.\n\n*   **\"Sufficiently powerful\":** contains or can be used to generate knowledge sufficient to resolve our AGI-doom problem, such as recipes for comprehensive mechanistic interpretability, mind uploading, or adult intelligence enhancement, or for robust solutions to alignment directly.\n*   **\"Safe\":** not embedded in a superintelligent agent eager to eat our lightcone, and which also doesn't spawn superintelligent simulacra eager to eat our lightcone, and doesn't cooperate with acausal terrorists eager to eat our lightcone, and isn't liable to Basilisk-hack its human operators into prompting it to generate a superintelligent agent eager to eat our lightcone, and so on down the list.\n*   **\"Easily interpretable\":** written in some symbolic language, such that interpreting it is in the reference class of \"understand a vast complex codebase\" combined with \"learn new physics from a textbook\", not \"solve major philosophical/theoretical problems\".\n*   **\"Well-structured\":** has an organized top-down hierarchical structure, learning which lets you quickly navigate to specific information in it.\n\nSome elaborations:\n\n**Safety:** The problem of making it safe is fairly nontrivial: a world-model powerful enough to be useful would need to be a strongly optimized construct, and [strongly optimized things are inherently dangerous](https://www.lesswrong.com/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth), agent-like or not. There's also the problem of *what* had exerted this strong optimization pressure on it: we would need to ensure the process synthesizing the world-model isn't itself the type of thing to develop an appetite for our lightcone.\n\nBut I'm cautiously optimistic this is achievable in this narrow case. Intuitively, it ought to be possible to generate *just* an \"inert\" world-model, without a value-laden policy (an agent) on top of it.\n\nThat said, this turning out to be harder than I expect is certainly one of the reasons I might end up curtailing this agenda.\n\n**Interpretability:** There are two primary objections I expect here.\n\n*   \"This is impossible, because advanced world-models are inherently messy\". I think this is confused/wrong, because there's already an existence proof: a *human's* world-model is symbolically interpretable by *the human mind containing it*. More on that later.\n*   \"(Neuro)symbolic methods have consistently failed to do anything useful\". I'll address that below too, but in short, neurosymbolic methods fail because it's a bad way to *learn*: it's [hard to traverse](https://www.beren.io/2025-03-01-Why-Not-Sparse-Hierarchical-Graph-Learning/) the space of neurosymbolic representations in search of the right one. But I'm not suggesting a process that \"learns by\" symbolic methods, I'm suggesting a process that *outputs* a symbolic world-model.\n\n* * *\n\nWhy Do You Consider This Agenda Promising?\n------------------------------------------\n\nOn the inside view, this problem, and the subproblems it decomposes into, seems pretty tractable. Importantly, it seems tractable using a realistic amount of resources (a small group of researchers, then perhaps a larger-scale engineering effort for crossing the theory-practice gap), in a fairly short span of time (I optimistically think 3-5 years; under a decade definitely seems realistic).[^tn1xspbg01]\n\nOn the outside view, almost nobody has been working on this, and certainly not using modern tools. Meaning, there's no long history of people failing to solve the relevant problems. (Indeed, on the contrary: one of its main challenges is something John Wentworth and David Lorell are working on, and they've been making [very promising progress](https://www.lesswrong.com/posts/Qdgo2jYAuFRMeMRJT/natural-latents-latent-variables-stable-across-ontologies) recently.)\n\nOn the strategic level, I view the problem of choosing the correct research agenda as the problem of [navigating between two failure modes](https://www.lesswrong.com/posts/Zn3iF8qfFFWRrrZNR/the-shortest-path-between-scylla-and-charybdis):\n\n*   **Out-of-touch theorizing:** If you pick a too-abstract starting point, you won't be able to find your way to the practical implementation in time. (Opinionated example: some of the agent-foundations agendas.)\n*   **Blind empirical tinkering:** If you pick a too-concrete starting point, you won't be able to generalize it to ASI in time. (Opinionated example: some of the agendas focused on frontier LLMs.)\n\nI think most alignment research agendas, if taken far enough, do produce ASI-complete alignment schemes eventually. However, they significantly differ in how long it takes them, and how much data they need. Thus, you want to pick the starting point that gets you to ASI-complete alignment *in as few steps as possible*: with the least amount of concretization *or* generalization.\n\nMost researchers disagree with most others regarding what that correct starting point is. Currently, this agenda is mine. \n\n* * *\n\nHigh-Level Outline\n------------------\n\nAs I'd stated above, I expect significant parts of this to turn out confused, wrong, or incorrect in a technical-but-not-conceptual way. This is a picture is painted with a fairly broad brush.\n\nI am, however, confident in the overall approach. If some of its modules/subproblems turn out faulty, I expect it'd be possible to swap them for functional ones as we go.\n\n### Theoretical Justifications\n\n**1\\. Proof of concept.** Note that human world-models appears to be \"autosymbolic\": able to be parsed as symbolic structures by the human mind in which they're embedded.[^c55mo5yyfc] Given that the complexity of things humans can reason about is strongly limited by their working memory, how is this possible?\n\nHuman world-models rely on [chunking](https://en.wikipedia.org/wiki/Chunking_(psychology)). To understand a complex phenomenon, we break it down into parts, understand the parts individually, then understand the whole in terms of the parts. (The human biology in terms of cells/tissues/organs, the economy in terms of various actors and forces, a complex codebase in terms of individual functions and modules.)\n\nAlternatively, we may run this process in reverse. To predict something about a specific low-level component, we could build a model of the high-level state, then propagate that information \"downwards\", but only focusing on that component. (If we want to model a specific corporation, we should pay attention to the macroeconomic situation. But when translating that situation into its effects on the corporation, we don't need to model the effects on *all* corporations that exist. We could then narrow things down further, to e. g. predict how a specific geopolitical event impacted an acquaintance holding a specific position at that corporation.)\n\nThose tricks seem to work pretty well for us, both in daily life and in our scientific endeavors. It seems that the process of understanding and modeling the universe can be broken up into a sequence of \"locally simple\" steps: steps which are simple given all preceding steps. Simple enough to fit within a human's working memory.\n\nTo emphasize: the above implies that the world's structure has this property *at the ground-true level*. The ability to construct such representations is an objective fact about data originating from our universe; our universe is well-abstracting.\n\n[The Natural Abstractions research agenda](https://www.lesswrong.com/posts/Qdgo2jYAuFRMeMRJT/natural-latents-latent-variables-stable-across-ontologies) is a formal attempt to model all of this. In its terms, the universe is structured such that low-level parts of the systems in it are independent given their high-level state. Flipping it around: the high-level state is *defined by* the information redundantly represented in all low-level parts.\n\nThat greatly simplifies the task. Instead of defining some subjective, human-mind-specific \"interpretability\" criterion, we simply need to extract this objectively privileged structure. How can we do so?\n\n**2\\. Compression.** Conceptually, the task seems fairly easy. The kind of hierarchical structure we want to construct happens to also be the *lowest-description-length* way to losslessly represent the universe. Note how it would follow the \"don't repeat yourself\" principle: at every level, higher-level variables would extract all information shared between the low-level variables, such that no bit of information is present in more than one variable.[^f3xddzu4jnn] More concretely, if we wanted to losslessly transform [the Pile](https://arxiv.org/abs/2101.00027) into a representation that takes up the least possible amount of disk space, a sufficiently advanced compression algorithm would surely exploit various abstract regularities and correspondences in the data – and therefore, it'd discover them.\n\nSo: all we need is to set up a sufficiently powerful compression process, and point it at a sufficiently big and diverse dataset of natural data. The output would be isomorphic to a well-structured world-model.\n\n... *If* we can interpret the symbolic language it's written in.\n\nThe problem with neural networks is that we don't have the \"key\" for deciphering them. There might be similar neat structures inside those black boxes, but we can't get at them. How can we avoid this problem here?\n\nBy defining \"complexity\" as the description length in some symbolic-to-us language, such as Python.\n\n**3\\. How does that handle ontology shifts?** Suppose that this symbolic-to-us language would be suboptimal for compactly representing the universe. The compression process would want to use some other, more \"natural\" language. It would spend some bits of complexity defining it, then write the world-model in it. That language may turn out to be as alien to us as the encodings NNs use.\n\nThe cheapest way to *define* that natural language, however, would be via the definitions that are the simplest *in terms of the symbolic-to-us language* used by our complexity-estimator. This rules out definitions which would look to us like opaque black boxes, such as neural networks. Although they'd technically still be symbolic (matrix multiplication plus activation functions), every parameter of the network would have to be specified independently, counting towards the definition's total complexity. If the core idea regarding the universe's \"abstraction-friendly\" structure is correct, this can't be the cheapest way to define it. As such, the \"bridge\" between the symbolic-to-us language and the correct alien ontology would consist of locally simple steps.\n\nAlternate frame: Suppose this \"correct\" natural language is *theoretically* understandable by us. That is, if we spent some years/decades working on the problem, we would have managed to figure it out, define it formally, and translate it into code. If we then looked back at the path that led us to insight, we would have seen a chain of mathematical abstractions from the concepts we knew in the past (e. g., 2025) to this true framework, with every link in that chain being locally simple (since each link would need to be human-discoverable). Similarly, the compression process would define the natural language using the simplest possible chain like this, with every link in it locally easy-to-interpret.\n\nInterpreting the whole thing, then, would amount to: picking a random part of it, iteratively following the terms in its definition backwards, arriving at some locally simple definition that only uses the terms in the initial symbolic-to-us language, then turning around and starting to \"step forwards\", iteratively learning new terms and using them to comprehend more terms.\n\nI. e.: the compression process would implement a natural \"entry point\" for us, a thread we'd be able to pull on to unravel the whole thing. The remaining task would still be challenging – \"understand a complex codebase\" multiplied by \"learn new physics from a textbook\" – but astronomically easier than \"derive new scientific paradigms from scratch\", which is where we're currently at.\n\n(To be clear, I still expect a fair amount of annoying messiness there, such as code-golfing. But this seems like the kind of problem that could be ameliorated by some practical tinkering and regularization, and other \"schlep\".)\n\n**4\\. Computational tractability.** But why would we think that this sort of compressed representation could be constructed compute-efficiently, such that the process finishes before the stars go out (forget \"before the AGI doom\")?\n\nFirst, as above, we have existence proofs. Human world-models seem to be structured this way, and they are generated at fairly reasonable compute costs. (Potentially at [shockingly low](https://www.lesswrong.com/posts/LY7rovMiJ4FhHxmH5/thoughts-on-hardware-compute-requirements-for-agi) compute costs.[^yeh2foupwqc])\n\nSecond: Any two Turing-complete languages are mutually interpretable, at the flat complexity cost of the interpreter (which depends on the languages but not on the program). As the result, the additional computational cost of interpretability – of computing a translation to the hard-coded symbolic-to-us language – would be flat.\n\n**5\\. How is this reconciled with the failures of previous symbolic learning systems?** That is: if the universe has this neat symbolic structure that could be uncovered in compute-efficient ways, why didn't pre-DL approaches work?\n\n[This essay](https://www.beren.io/2025-03-01-Why-Not-Sparse-Hierarchical-Graph-Learning/) does an excellent job explaining why. To summarize: even if *the final correct output* would be (isomorphic to) a symbolic structure, the compute-efficient path to *getting* there, the process of figuring that structure out, is not necessarily a sequence of ever-more-correct symbolic structures. On the contrary: if we start from sparse hierarchical graphs, and start adding provisions for making it easy to traverse their space in search of the correct graph, we pretty quickly arrive at (more or less) neural networks.\n\nHowever: I'm not suggesting that we use symbolic learning methods. The aim is to set up a process which would *output* a highly useful symbolic structure. How that process works, what path it takes there, how it constructs that structure, is up in the air.\n\nDesigning such a process is conceptually tricky. But as I argue above, theory and common sense say that it ought to be possible; and I do have ideas.\n\n* * *\n\n### Subproblems\n\nThe compression task can be split into three subproblems. Below are their summaries and links to posts dedicated to them.\n\n[**1\\. \"Abstraction-learning\".**](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction) Given a set of random low-level variables which implement some higher-level abstraction, how can we learn that abstraction? What functions map from the molecules of a cell to that cell, from a human's cells to that human, from the humans of a given nation to that nation; or from the time-series of some process to the laws governing it?\n\nAs mentioned above, this is the problem [the natural-abstractions agenda](https://www.lesswrong.com/posts/Qdgo2jYAuFRMeMRJT/natural-latents-latent-variables-stable-across-ontologies) is currently focused on.\n\nMy current guess is that, at the high level, this problem can be characterized as a \"constructive\" version of [Partial Information Decomposition](https://en.wikipedia.org/wiki/Partial_information_decomposition). It involves splitting (every subset of) the low-level variables into unique, redundant, and synergistic components.\n\nGiven correct formal definitions for unique/redundant/synergistic variables, it should be straightforwardly solvable via machine learning.\n\nCurrent status: the theory is well-developed and it appears highly tractable.\n\n[**2\\. \"Truesight\".**](https://www.lesswrong.com/posts/kNyMwXQxctWtaRZhs/synthesizing-standalone-world-models-part-2-shifting) When we're facing a structure-learning problem, such as abstraction-learning, we assume that we get many samples from the same fixed structure. In practice, however, the probabilistic structures are *themselves* resampled.\n\nExamples:\n\n*   The cone cells in your eyes connect to different abstract objects depending on what you're looking at, or where your feet carry you.\n*   The text on the frontpage of an online newsletter is attached to different real-world structures on different days.\n*   The glider in Conway's Game of Life \"drifts across\" cells in the grid, rather than being an abstraction over some fixed set of them.\n*   The same concept of a \"selection pressure\" can be arrived-at by abstracting from evolution *or* ML models *or* corporations *or* cultural norms.\n*   The same human mind can \"jump substrates\" from biological neurons to a digital representation (mind uploading), while still remaining \"the same object\".\n\nI. e.,\n\n*   The same high-level abstraction can \"reattach\" to different low-level variables.\n*   The same low-level variables can change which high-level abstraction they implement.\n\nOn a sample-to-sample basis, we can't rely on *any* static abstraction functions to be valid. We need to *search* for appropriate ones \"at test-time\": by trying various transformations of the data until we [spot the \"simple structure\" in it](https://transformer-circuits.pub/2024/qualitative-essay/index.html).\n\nHere, \"simplicity\" is defined relative to the library of stored abstractions. What we want, essentially, is to be able to recognize reoccurrences of known objects despite looking at them \"from a different angle\". Thus, \"truesight\".\n\nCurrent status: I think I have a solid conceptual understanding of it, but it's at the pre-formalization stage. There's one obvious way to formalize it, but it seems best avoided, or only used as a stepping stone.\n\n[**3\\. Dataset-assembly.**](https://www.lesswrong.com/posts/eQX93nxapSdxQYdkL/synthesizing-standalone-world-models-part-3-dataset-assembly) There's a problem:\n\n*   Solving abstraction-learning requires truesight. We can't learn abstractions if we don't have many samples of the random variables over which they're defined.\n*   Truesight requires already knowing what abstractions are around. Otherwise, the problem of finding simple transformations of the data that make them visible is computationally intractable. (We can't recognize reoccurring objects if we don't know any objects.)\n\nThus, subproblem 3: how to automatically spot ways to slice the data into datasets entries from which are isomorphic to samples from some fixed probabilistic structure, to make them suitable for abstraction-learning.\n\nCurrent status: basically greenfield. I don't have a solid high-level model of this subproblem yet, only some preliminary ideas.\n\n* * *\n\nBounties\n--------\n\n**1\\. Red-teaming.** I'm interested in people trying to find important and overlooked-by-me issues with this approach, so I'm setting up a bounty: $5-$100 for spotting something wrong that makes me change my mind. The payout scales with impact.\n\nFair warnings:\n\n*   I expect most attempts to poke holes to yield a $0 reward. I'm well aware of many minor holes/\"fill-in with something workable later\" here, as well as the major ways for this whole endeavor to fail/turn out misguided.\n*   I don't commit to engaging in-depth with every attempt. As above, I expect many of them to rehash things I already know of, so I may just point that out and move on.\n\nA reasonable strategy here would be to write up a low-effort list of one-sentence summaries of potential problems you see, I'll point out which seem novel and promising at a glance, and you could expand on those.\n\n**2\\. Blue-teaming.** I am also interested in people bringing other kinds of agenda-relevant useful information to my attention: relevant research papers or original thoughts you may have. Likewise, a $5-$100 bounty on that, scaling with impact.[^suo56uhoc8n]\n\nI will provide pointers regarding the parts I'm most interested in as I post more detailed write-ups on the subproblems.\n\nBoth bounties will be drawn from a fixed pool of $500 I've set aside for this. I hope to scale up the pool and the rewards in the future. On that note...\n\n* * *\n\nFunding\n-------\n\nI'm looking to diversify my funding sources. The AI Alignment funding landscape seems increasingly (over)focused on LLMs; I pretty much expect only the LTFF would fund me. This is an uncomfortable situation to be in, since if some disaster were to befall the LTFF, or if the LTFF were to change priorities as well, I would be completely at sea.\n\nAs such:\n\n*   If you're interested and would be able to provide significant funding (e. g., $10k+), or know anyone who'd be interested-and-willing, please do reach out.\n*   I accept donations, including smaller ones, [through Manifund](https://manifund.org/projects/synthesizing-standalone-world-models) and at the crypto addresses listed at the end of this post.\n\nRegarding target funding amounts: I currently reside in a country with low costs of living, and I don't require much compute at this stage, so the raw resources needed are small; e. g., $40k would cover me for a year. That said, my not residing in the US increasingly seems like a bottleneck on collaborating with other researchers. As such, I'm currently aiming to develop a financial safety pillow, then immigrate there. Funding would be useful up to $200k.[^nh5h0v594r]\n\nIf you're interested in funding my work, but want more information first, you can access a fuller write-up through [this link](https://drive.google.com/file/d/1nolWcCAnEshAEh1SY8gE87cqmx8LAfsS/view?usp=sharing).\n\nIf you want a reference, reach out to [@johnswentworth](https://www.lesswrong.com/users/johnswentworth?mention=user).\n\n* * *\n\n+++ Crypto\n\nBTC: bc1q7d8qfz2u7dqwjdgp5wlqwtjphfhct28lcqev3v  \nETH: 0x27e709b5272131A1F94733ddc274Da26d18b19A7  \nSOL: CK9KkZF1SKwGrZD6cFzzE7LurGPRV7hjMwdkMfpwvfga  \nTRON: THK58PFDVG9cf9Hfkc72x15tbMCN7QNopZ\n\nPreference: Ethereum, USDC stablecoins.\n\n\n\n\n+++\n\n[^tn1xspbg01]: You may think a decade is too slow given LLM timelines. Caveat: \"a decade\" is the pessimistic estimate under my primary, bearish-on-LLMs, model. In worlds in which LLM progress goes as fast as some hope/fear, this agenda should likewise advance much faster, for one reason: it doesn't seem that far from being fully formalized. Once it is, it would become possible to feed it to narrowly superintelligent math AIs (which are likely to appear first, before omnicide-capable general ASIs), and they'd cut years of math research down to ~zero.I do not centrally rely on/expect that. I don't think LLM progress would go this fast; and if LLMs do speed up towards superintelligence, I'm not convinced it would be in the predictable, on-trend way people expect.That said, I do assign nontrivial weight to those worlds, and care about succeeding in them. I expect this agenda to fare pretty well there. \n\n[^c55mo5yyfc]: It could be argued that they're not \"fully\" symbolic – that parts of them are only accessible to our intuitions, that we can't break down the definitions of the symbols/modules in them down to the most basic functions/neuron activations. But I think they're \"symbolic enough\": if we could generate an external world-model that's as understandable to us as our own world-models (and we are confident that this understanding is accurate), that should suffice for fulfilling the \"interpretability\" criterion.That said, I don't expect this caveat to come into play: I expect a world-model that would be ultimately understandable in totality. \n\n[^f3xddzu4jnn]: Indeed, the fact that abstraction allows compressibility is potentially why we should expect our universe to be well-abstracting. See more here. \n\n[^yeh2foupwqc]: The numbers in that post feel somewhat low to me, but I think it's directionally correct. \n\n[^suo56uhoc8n]: Though you might want to reach out via private messages if the information seems exfohazardous. E. g., specific ideas about sufficiently powerful compression algorithms are obviously dual-use. \n\n[^nh5h0v594r]: Well, truthfully, I could probably find ways to usefully spend up to $1 million/year, just by hiring ten mathematicians and DL engineers to explore all easy-to-describe, high-reward, low-probability-of-panning-out research threads. So if you want to give me $1 million, I sure wouldn't say no.",
      "plaintextDescription": "tl;dr: I outline my research agenda, post bounties for poking holes in it or for providing general relevant information, and am seeking to diversify my funding sources. This post will be followed by several others, providing deeper overviews of the agenda's subproblems and my sketches of how to tackle them.\n\n----------------------------------------\n\nBack at the end of 2023, I wrote the following:\n\n> I'm fairly optimistic about arriving at a robust solution to alignment via agent-foundations research in a timely manner. (My semi-arbitrary deadline is 2030, and I expect to arrive at intermediate solid results by EOY 2025.)\n\nOn the inside view, I'm pretty satisfied with how that is turning out. I have a high-level plan of attack which approaches the problem from a novel route, and which hopefully lets us dodge a bunch of major alignment difficulties (chiefly the instability of value reflection, which I am MIRI-tier skeptical of tackling directly). I expect significant parts of this plan to change over time, as they turn out to be wrong/confused, but the overall picture should survive.\n\nConceit: We don't seem on the track to solve the full AGI alignment problem. There's too much non-parallelizable research to do, too few people competent to do it, and not enough time. So we... don't try. Instead, we use adjacent theory to produce a different tool powerful enough to get us out of the current mess. Ideally, without having to directly deal with AGIs/agents at all.\n\nMore concretely, the ultimate aim is to figure out how to construct a sufficiently powerful, safe, easily interpretable, well-structured world-model.\n\n * \"Sufficiently powerful\": contains or can be used to generate knowledge sufficient to resolve our AGI-doom problem, such as recipes for comprehensive mechanistic interpretability, mind uploading, or adult intelligence enhancement, or for robust solutions to alignment directly.\n * \"Safe\": not embedded in a superintelligent agent eager to eat our lightcone, and wh",
      "wordCount": 3411
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "cCK3fDdH9vHjrf2yP",
        "name": "Bounties & Prizes (active)",
        "slug": "bounties-and-prizes-active"
      },
      {
        "_id": "Z38PqJbRyfwCxKvvL",
        "name": "Research Agendas",
        "slug": "research-agendas"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "NQfeEd5LWohauKpTj",
    "title": "The System You Deploy Is Not the System You Design",
    "slug": "the-system-you-deploy-is-not-the-system-you-design",
    "url": null,
    "baseScore": 53,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-09-05T20:16:42.938Z",
    "contents": {
      "markdown": "**tl;dr:** Novel framing on the trivial point that your models may not be accounting for all relevant factors. I find it useful for improving the quality of my thinking on the topic. Asking yourself \"is the type signature of my design for achieving X actually 'a design for achieving X'?\"[^8mtensu32su] is a good prompt for checking which parts of your model you have uneasy doubts about, in your heart of hearts.\n\n* * *\n\n### Illustrative Examples\n\n1\\. In acoustics, there's something called [the RESWING effect](https://www.sciencedirect.com/topics/physics-and-astronomy/wind-effect). Suppose you want to decrease noise in some area, so you surround it with [soundwalls](https://en.wikipedia.org/wiki/Noise_barrier). Your model is simple: separating contiguous chunks of air by hard barriers impedes interactions between those chunks, which should impede sound propagation.\n\nOne factor turns out to foil this plan: in atmosphere, the sound speed isn't fixed. It increases with height, due to generally higher wind speeds and temperature differences. This causes sound refraction: soundwaves bend upwards or downwards, depending on whether the air is colder or hotter towards the ground. In the former case (generally at night), if a soundwave runs into a wall, the refractive effects become stronger due to the discontinuity the wall introduces. This leads to the wave sharply bending downwards, *into* the would-be shielded area. As the result, at some set distance behind the wall, the noise is actually amplified.\n\nFormally: You model sound waves as obeying the Helmholtz equation \\\\(\\nabla^2\\phi+f(c)^2\\phi=0\\\\), with a constant sound speed \\\\(c\\\\). Your walls place [boundary conditions](https://en.wikipedia.org/wiki/Neumann_boundary_condition) on this equation. However, the correct equation is \\\\(\\nabla^2\\phi+f(c(z))^2\\phi=0\\\\), \\\\(c\\\\) depends on height \\\\(z\\\\), and introducing the same boundary conditions leads to very different solutions.\n\nThus, something designed to be a noise wall turns out to actually be a converging acoustic lens.\n\n2\\. Suppose you design an algorithm for high-frequency trading. In theory, HFTs should increase market efficiency, speeding up price discovery and narrowing bid-ask spreads.\n\nHowever, if an HFT is deployed into an environment with many *other* HFTs, this can cause a feedback loop where they [start playing \"hot potato\" with each other](https://en.wikipedia.org/wiki/2010_flash_crash#SEC/CFTC_report). A system whose expected effect was that of a market stabilizer turns out to be a (conditional) market-volatility amplifier.\n\n3\\. From [*Why Agent Foundations?*](https://www.lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation):\n\n> Suppose we’re designing some secure electronic equipment, and we’re concerned about the system leaking information to adversaries via a radio side-channel. We design the system so that the leaked radio signal has zero correlation with whatever signals are passed around inside the system.\n> \n> Some time later, a clever adversary is able to use the radio side-channel to glean information about those internal signals using fourth-order statistics. Zero correlation was an imperfect proxy for zero information leak, and the proxy broke down under the adversary’s optimization pressure.\n\n4\\. [The Cobra Effect](https://en.wikipedia.org/wiki/Perverse_incentive): if you reward people for bringing you dead cobras, in an attempt to set up a cobra-elimination effort, what you *actually* create is a cobra farming industry.\n\n5\\. Suppose you're training an AI model not to deceive you. However, your deception-detecting methodology is flawed: some lies slip through. What you're actually training, then, is not something honest, but a good liar.\n\n6\\. If you try to infer the purpose of a (sociopolitical) system [from what it does](https://en.wikipedia.org/wiki/The_purpose_of_a_system_is_what_it_does), it would often fail to match the purpose its designers actually had in mind.\n\n* * *\n\n### Abstract Generalization\n\nConsider designing a system for a specific purpose, then deploying it to the environment in which it's meant to fulfil that purpose. Abstractly, you can think about it as follows: there's a low-level concrete environment \\\\(L\\\\), a high-level representation of that environment \\\\(H\\\\), and an \"abstraction operator\" \\\\(A:L\\to H\\\\), mapping from low-level states to high-level states.\n\nYou define your system's purpose/type signature in terms of the high-level environment, then invert the abstraction operator, and feed this type signature to it. In theory, the inverted operator \\\\(A^{-1}\\\\) then outputs a low-level system such that, if you abstract back up from it, would match the type signature of your target system: \\\\(A(A^{-1}(H))=H\\\\). In other words, you search for whatever low-level system would fill the high-level role you actually care about.\n\nHowever, suppose that your model of the low-level environment is flawed. If so, your abstraction operators don't match the \"ground-true\" abstraction operators either: the correct operators have a type signature \\\\(A:L\\to H\\\\), whereas what you're using is \\\\(A^*:L^*\\to H\\\\). What would happen, then, if you invert your intended type signature through your flawed \\\\(A^*\\\\)?\n\nAt the next turn, Reality would apply the correct \\\\(A\\\\) to the output of \\\\({A^*}^{-1}(H)\\\\), and you'll most likely discover that \\\\(A({A^*}^{-1}(H))\\ne H\\\\): the high-level type signature of the deployed system would not match the designated type signature. In other words, it will turn out that the system you deployed was something entirely different from what you thought you were deploying.\n\nA way to think about is that the system's nature is not solely defined by its own \"shape\", but also by whether this shape matches some negative space in the environment: whether it correctly slots into/fills some role. If you mismodeled the shape of the role you want to fill, your system's shape would slot into some other role.\n\nSo even if you completely *and correctly* understand how your system's internals function, its true nature may *still* surprise you.\n\n* * *\n\n### Practical Advice\n\nWhen designing something, there's often a temptation to mentally \"bend\" your predictions regarding how it would behave, especially if you have some uncertainty over the low-level deployment environment \\\\(L\\\\). To quietly assume that this uncertainty would resolve itself in such a way that your system would behave just as planned. To, essentially, unconsciously hope that the world would *understand* what you *meant*, and then just do that. Or, at least, that the pockets of uncertainty won't interact with your system in any non-random way: that it doesn't matter that you're uncertain about their values, because randomizing those values would not impact the result anyway!\n\nIn the moment, alternate possibilities may feel like conspiracy theories. Like expecting the world to *unnaturally contort itself* to foil your plans, rather than to merely refuse to cooperate.\n\nThis mindset is easier to slip into if you think about unintended consequences as the result of \"mistakes\". If you're worrying about \"mistakes\" in implementing your system, you're implicitly assuming that what you're building is basically the right thing, save for a relatively small amount of potential errors you may let slip through. When staring at your project, you're always cross-referencing what you're seeing with your intended end result, trying to spot deviations. But if you always have that intended end result at the forefront of your mind, it's liable to bias your interpretation of what you see.[^7dnnlt4x40d]\n\nThe mindset I'm gesturing towards, on the other hand, doesn't think in terms of \"mistakes\". Instead, it invites you to momentarily forget your \"intentions\", forget the \"purpose\" of your project from which its implementation may \"deviate\", and look at what you're building afresh. What is the actual true nature of that thing? What does it ask the world to do?\n\nYou may discover that it has nothing to do with your intended goal at all; that it isn't even *close* to it in concept-space.\n\nAnd that's the way *the world* would look at it. The world is indifferent and unintelligent. It doesn't care or understand what you meant, and it won't query your plans and intentions when deciding how to interpret what you fed it.\n\n* * *\n\nThe main thing on my mind here is, of course, the AGI doom.\n\nGoodharting is an obvious and important example of the principle I'm talking about, but it's broader. When your soundwalls turn out to function as acoustic lenses and blast someone with noise, or when your stable bridge [behaves as a self-oscillation amplifier](https://en.wikipedia.org/wiki/Tacoma_Narrows_Bridge_(1940)) and collapses, it's not because something out there Goodharted to the sound-suppression/bridge-stabilization task.\n\nSimilarly, the AGI risk is not *centrally* about a powerful agent deliberately scheming to deceive you and kill you. It's a salient and important failure mode, but it's [very far from the only one](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=56JZRd8r6fRwFtHf5). The core issue is that, by default, you don't really know *what* you're building until you deploy it; what role the world would assign your system. Unless you've rigorously eliminated all uncertainty, all you have are hopeful fantasies about the world cooperating with your vision in all the places where that uncertainty exists.\n\nAnd if trial-and-error is *extremely* costly, so lethally costly you can't pay it at all... Well, it gives some idea regarding the mindset and the methodology you need to adopt if you want to first-try it. \n\n[^8mtensu32su]: This is probably a sazen; i. e., it summarizes this post if you're already familiar with the relevant mental motions, but doesn't substitute for it otherwise. \n\n[^7dnnlt4x40d]: In the standard predictive-processing way. There's something you expect your system to be, so your brain would propagate that prediction top-down, biasing your attempt at a bottom-up understanding/error-spotting. You would see what you expect to see, and look past what you actually see.",
      "plaintextDescription": "tl;dr: Novel framing on the trivial point that your models may not be accounting for all relevant factors. I find it useful for improving the quality of my thinking on the topic. Asking yourself \"is the type signature of my design for achieving X actually 'a design for achieving X'?\"[1] is a good prompt for checking which parts of your model you have uneasy doubts about, in your heart of hearts.\n\n----------------------------------------\n\n\nIllustrative Examples\n1. In acoustics, there's something called the RESWING effect. Suppose you want to decrease noise in some area, so you surround it with soundwalls. Your model is simple: separating contiguous chunks of air by hard barriers impedes interactions between those chunks, which should impede sound propagation.\n\nOne factor turns out to foil this plan: in atmosphere, the sound speed isn't fixed. It increases with height, due to generally higher wind speeds and temperature differences. This causes sound refraction: soundwaves bend upwards or downwards, depending on whether the air is colder or hotter towards the ground. In the former case (generally at night), if a soundwave runs into a wall, the refractive effects become stronger due to the discontinuity the wall introduces. This leads to the wave sharply bending downwards, into the would-be shielded area. As the result, at some set distance behind the wall, the noise is actually amplified.\n\nFormally: You model sound waves as obeying the Helmholtz equation ∇2ϕ+f(c)2ϕ=0, with a constant sound speed c. Your walls place boundary conditions on this equation. However, the correct equation is ∇2ϕ+f(c(z))2ϕ=0, c depends on height z, and introducing the same boundary conditions leads to very different solutions.\n\nThus, something designed to be a noise wall turns out to actually be a converging acoustic lens.\n\n2. Suppose you design an algorithm for high-frequency trading. In theory, HFTs should increase market efficiency, speeding up price discovery and narrowing bid-ask spreads",
      "wordCount": 1438
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "34J5qzxjyWr3Tu47L",
    "title": "Is Building Good Note-Taking Software an AGI-Complete Problem?",
    "slug": "is-building-good-note-taking-software-an-agi-complete",
    "url": null,
    "baseScore": 26,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2025-05-26T18:26:04.344Z",
    "contents": {
      "markdown": "In my experience, the most annoyingly unpleasant part of research[^u2qmvj327of] is reorganizing my notes during and (especially) after a productive research sprint. The \"distillation\" stage, in [Neel Nanda's categorization](https://www.lesswrong.com/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand#Distillation__Stage_4___Compress__Refine__Communicate). I end up with a large pile of variously important discoveries, promising threads, and connections, and the task is to then \"refactor\" that pile into something compact and well-organized, structured in the image of my newly improved model of the domain of study.\n\nThat task is of central importance:\n\n1.  It's a vital part of the actual research process. If you're trying to discover the true simple laws/common principles underlying the domain, periodically refactoring your mental model of that domain in light of new information is precisely what you should be doing. Reorganizing your notes forces you to do just that: distilling a mess into elegant descriptions.\n2.  It allows you to get a bird's-eye view on your results, what they imply and don't imply, what open questions are the most important to focus on next, what nagging doubts you have, what important research threads or contradictions might've ended up noted down but then forgotten, et cetera.\n3.  It does most of the work of transforming your results into a format ready for consumption by other people.\n\n### A Toy Example\n\nSuppose you're studying the properties of matter, and your initial ontology is that everything is some combination of Fire, Water, Air, and Earth. Your initial notes are structured accordingly: there are central notes for each element, branching off from them are notes about interactions between combinations of elements, case studies of specific experiments, attempts to synthesize and generalize experimental results, et cetera.\n\nSuppose that you then discover that a \"truer\", simpler description of matter involves classifying it along two axes: [\"wet-dry\" and \"hot-cold\"](https://en.wikipedia.org/wiki/Classical_element#Aristotle). The Fire/Water/Air/Earth elements are still relevant, revealed to be extreme types of matter sitting in the corners of the wet-dry/hot-cold square. But they're no longer *fundamental* to how you model matter.\n\nNow you need to refactor your entire mental ontology – and your entire notebase. You need to add new nodes for the wetness/temperature spectra, you need to wholly rewrite the notes about the elements to explicate their nature as extreme states of matter (rather than its basic building blocks), you need to do the same for all notes about elemental interactions, you need to re-interpret the experimental results, and you need to ensure you don't overlook any subtle evidence contradicting the new ontology, or stray thoughts that might lead to an insight regarding an even-more-correct ontology, or absent-minded ideas about high-impact applications and research avenues...\n\nAt a sufficiently abstract level, what you should do is: fetch all information relevant to the new ontology from your old-ontology notes, use that information to properly flesh the new ontology out, then redefine the old ontology in the new ontology's terms.\n\n* * *\n\nThis isn't only a frontier-researcher problem: similar happens whenever I'm studying a domain that's already well-explored. I start with a flawed model centered around incorrect variables. Gradually, as I learn more, my thinking re-organizes around truer central elements. Once enough changes have accumulated, over the course of months or years, my mental representation ends up having little in common with my initial one.\n\nBut the state of the corresponding notebase usually drags behind.\n\nRefactoring your *mental* ontology is relatively easy: it just requires thinking, and the interface for navigating and editing your world-model is very rich and flexible. Friction costs of mental actions are nonzero, but low.\n\nThe same is not true for note-taking. Tools for it do a fairly poor job of accommodating the above functionality; even e. g. Obsidian's canvas. They impose a lot of additional friction, their interfaces and editing features aren't optimized for such at-scale refactors.\n\nMy impression is that a lot of people run into similar issues when trying to use notebases as \"second brains\".[^mbetgildun]\n\n### Why Not Just Start From Scratch?\n\nArguably, the solution is to just periodically start from scratch. Instead of trying to *edit*, you spin up a new notebase, writing directly from your updated world-model; the old notebase you delete.\n\nI think this is very suboptimal, in two ways.\n\nFirst, those outdated notebases *do* still hold a lot of value:\n\n*   Human memory, especially working memory, is painfully limited. Having a notebase ensures that you don't forget any phenomena and connections you don't commonly encounter[^ezmt9if14yd]; that you're reminded to properly propagate the updates downstream of the new ontology to *all* corners of your world-model.\n*   The excitement of discovering an apparently better ontology might blind you to its issues. When thinking in its terms, you would, by definition, only be able to think about the phenomena that could be easily described in its terms. If there are any broad swathes of phenomena it fails at, or any subtle-but-crucial issues with interpreting past data through the new lens, you may end up unable to perceive them. A stable, externalized record of all of this *forces* you to properly think through it all.\n\nBasically, \"rewrite the notebase from scratch\" has a lot of the same issues as \"[rewrite the codebase from scratch](https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/)\".\n\nSecond, even if you're taking a sufficiently wise \"rewrite it from scratch\" approach, where you're constantly reviewing your previous notebase to ensure you're not missing anything... That is a lot, *a lot* of work.\n\nWork that coincidentally forces you to do useful conceptual thinking, yes. But a significant fraction of it *is* just drudgery forced on you by UI shortcomings.\n\nWhat would the ideal interface for this be? Something that slashes the above friction costs. Something that allows to flexibly vary the *representation* of your knowledge – in terms of concepts you describe it via – while ensuring that all information (including subtle, forgotten, yet crucially important doubts) is retained.\n\n### Generalized Representation-Flipping\n\nIn a way, what would be ideal here is a generalization of my idea about an \"[exploratory medium for mathematics](https://www.lesswrong.com/posts/t46PYSvHHtJLxmrxn/what-are-important-ui-shaped-problems-that-lightcone-could?commentId=vgozGjvJQvFQZteiF)\":\n\n> A big part of highly theoretical research is flipping between different representations of the problem: viewing it in terms of information theory, in terms of Bayesian probability, in terms of linear algebra; jumping from algebraic expressions to the visualizations of functions or to the nodes-and-edges graphs of the interactions between variables; et cetera.\n> \n> The key reason behind it is that *research heuristics bind to representations*. E. g., suppose you're staring at some graph-theory problem. Certain problems of this type are isomorphic to linear-algebra problems, and they may be *trivial* in linear-algebra terms. But unless you actually project the problem into the linear-algebra ontology, you're not necessarily going to see the trivial solution when staring at the graph-theory representation. (Perhaps the obvious solution is to find the eigenvectors of the adjacency matrix of the graph – but when you're staring at a bunch of nodes connected by edges, that idea isn't obvious in that representation at all.)\n> \n> This is a bit of a simplified example – the graph theory/linear algebra connection is well-known, so experienced mathematicians may be able to translate between those representations instinctively – but I hope it's illustrative.\n> \n> As a different concrete example, consider John Wentworth's [Bayes Net Algebra](https://www.lesswrong.com/posts/XHtygebvHoJSSeNPP/some-rules-for-an-algebra-of-bayes-nets). This is essentially an *interface* for working with factorizations of joint probability distributions. The nodes-and-edges representation is more intuitive and easy to tinker with than the \"formulas\" representation, which means that having concrete rules for tinkering with graph representations without committing errors would significantly speed up how quickly you can reason through related math problems. Imagine if the derivation of such frameworks was *automated*: if you could set up a joint PD in terms of formulas, automatically project the setup into graph terms, start tinkering with it by dragging nodes and edges around, and get errors if and only if back-projecting the changed \"graph\" representation into the \"formulas\" representations results in a setup that's non-isomorphic to the initial one.\n> \n> (See also [this video](https://www.youtube.com/watch?v=oUaOucZRlmE), and [the article linked above](https://cognitivemedium.com/emm/emm.html).)\n> \n> A related challenge are *refactors*. E. g., suppose you're staring at some complicated algebraic expression with an infinite sum. It may be the case that a certain no-loss-of-generality change of variables would easily collapse that expression into a Fourier series, or make some Obscure Theorem #418152/Weird Trick #3475 trivially applicable. But unless you happen to be looking at the problem through those lens, you're not going to be able to spot it. (Especially if you don't know the Obscure Theorem #418152/Weird Trick #3475.)\n> \n> It's plausible that the above two tasks is what 90% of math research consists of ([the \"normal-science\" part of it](https://en.wikipedia.org/wiki/Normal_science)), in terms of time expenditure. Flipping between representations in search of a representation-chain where every step is trivial.\n\nBasically: You have some *abstract construct* which is \"anchored down\" by your notes/math. For any abstract construct, there's an infinite number of valid ways to anchor it. Some of those ways are better than others from the practical point of view: shorter, simpler to work with. What a good note-taking tool would allow is freely varying the *form* of your anchors under the constraint of fully preserving the abstract construct.\n\n### The Fundamental Problem\n\nMind, this isn't just a problem with note-taking. This sort of surface-level messiness convergently appears in *any* situation where we have a system gradually learning/adapting to an unfamiliar domain. Some examples:\n\n*   A codebase that's gradually added to over the years, which ends up as a tall [spaghetti tower](https://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers) in dire need of a refactor.\n*   A population of organisms being mutated by evolution, resulting in spaghetti-code DNA structures and apparently messy biological dynamics... which nevertheless turn out to be [elegant and simple](https://www.lesswrong.com/posts/bNXdnRTpSXk9p4zmi/book-review-design-principles-of-biological-circuits), if you can find the right lens to look at them from.\n*   A neural network being incrementally updated by gradient descent. It transforms into a massive black box... which still [can](https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking), in principle, be translated into a simple symbolic-program form.\n*   Law systems or bureaucratic regulations, which are gradually adjusted in response to social changes and legal loopholes, until they turn into Kafkaesque nightmares.\n\nIn all cases, we start with the description of a system in some initial representation/language/ontology, gradually refine the system, and end up with something that's effectively implemented on a *different*, \"truer\" ontology. But that high-level ontology isn't by-default visible, we don't get the \"interpreter\" for free, so what you end up *seeing* is an inefficient mess.\n\n... Which, if we view it from that perspective, has depressing implications regarding any hope of building \"good\" note-taking software. None of the powerful processes above struggling with isomorphic problems (programmers, evolution, interpretability researchers, legislators, company managers) have managed to solve them. The only \"solution\" that ever works is to just have a competent human *manually* untangle the mess.\n\nAnd indeed, if we think about what \"lossless notebase refactors\" would imply, it would imply fully *intelligent* edits. Not even something LLMs can really do: they *would* lose track of those subtle-but-crucial tidbits/doubts/thoughts I keep talking about.\n\nSo: it seems that a fully competent notes-editing software is AGI-complete.\n\n### Can the Problem Be Ameliorated?\n\nOkay, so a full solution is beyond the scope of a notetaking app. Can the situation still be *improved*?\n\nIntuitively, yes. Recall that we're not actually asking for fully automatic notebase refactors, we're looking to make *manual* human-guided refactors easier on the humans.\n\nSo: any ideas regarding how?\n\n*   What UI elements would be helpful and ultimately implementable with the current technology? Both regarding how the notes are *displayed*, and how they can be *edited*.\n*   Do you have any note-taking/research-logging strategies that lessen/fix this problem? (It would be very nice if all of the above turned out to just be a skill issue on my part.)\n*   Are there any lessons from the domains of programming/biology/interpretability/government reform/company management that could be directly imported to this domain?\n\nI've separated out my own thoughts into [this comment](https://www.lesswrong.com/posts/34J5qzxjyWr3Tu47L/is-building-good-note-taking-software-an-agi-complete?commentId=dpdxayCb6GowakbTe).\n\n[^u2qmvj327of]: Especially pre-paradigmic research, such as in agent foundations. \n\n[^mbetgildun]: Source: Vague recollection of various discussions I've read, plus this brief attempt at a public-opinion review I just ran via o3. \n\n[^ezmt9if14yd]: See the generalized correspondence principle: new ontology must explain every real phenomenon the previous ontology was able to explain.(And as far as keeping notes goes, you should also ideally preserve the explanation regarding what features of the new ontology made it look like the old ontology. \"How and why does quantum physics consistently create the impression of classicality?\")",
      "plaintextDescription": "In my experience, the most annoyingly unpleasant part of research[1] is reorganizing my notes during and (especially) after a productive research sprint. The \"distillation\" stage, in Neel Nanda's categorization. I end up with a large pile of variously important discoveries, promising threads, and connections, and the task is to then \"refactor\" that pile into something compact and well-organized, structured in the image of my newly improved model of the domain of study.\n\nThat task is of central importance:\n\n 1. It's a vital part of the actual research process. If you're trying to discover the true simple laws/common principles underlying the domain, periodically refactoring your mental model of that domain in light of new information is precisely what you should be doing. Reorganizing your notes forces you to do just that: distilling a mess into elegant descriptions.\n 2. It allows you to get a bird's-eye view on your results, what they imply and don't imply, what open questions are the most important to focus on next, what nagging doubts you have, what important research threads or contradictions might've ended up noted down but then forgotten, et cetera.\n 3. It does most of the work of transforming your results into a format ready for consumption by other people.\n\n\n \n\n\nA Toy Example\nSuppose you're studying the properties of matter, and your initial ontology is that everything is some combination of Fire, Water, Air, and Earth. Your initial notes are structured accordingly: there are central notes for each element, branching off from them are notes about interactions between combinations of elements, case studies of specific experiments, attempts to synthesize and generalize experimental results, et cetera.\n\nSuppose that you then discover that a \"truer\", simpler description of matter involves classifying it along two axes: \"wet-dry\" and \"hot-cold\". The Fire/Water/Air/Earth elements are still relevant, revealed to be extreme types of matter sitting in the corners of t",
      "wordCount": 1982
    },
    "tags": [
      {
        "_id": "puBcCq7aRwKoa7pXX",
        "name": "Note-Taking",
        "slug": "note-taking"
      },
      {
        "_id": "udPbn9RthmgTtHMiG",
        "name": "Productivity",
        "slug": "productivity"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oKAFFvaouKKEhbBPm",
    "title": "A Bear Case: My Predictions Regarding AI Progress",
    "slug": "a-bear-case-my-predictions-regarding-ai-progress",
    "url": null,
    "baseScore": 375,
    "voteCount": 233,
    "viewCount": null,
    "commentCount": 163,
    "createdAt": null,
    "postedAt": "2025-03-05T16:41:37.639Z",
    "contents": {
      "markdown": "This isn't really a \"timeline\", as such – I don't know the timings – but this is my current, fairly optimistic take on where we're heading.\n\nI'm not *fully* committed to this model yet: I'm still on the lookout for more agents and inference-time scaling later this year. But Deep Research, Claude 3.7, Claude Code, Grok 3, and GPT-4.5 have turned out largely in line with these expectations[^71wj581psb4], and this is my current baseline prediction.\n\n* * *\n\nThe Current Paradigm: I'm Tucking In to Sleep\n---------------------------------------------\n\nI expect that none of the currently known avenues of capability advancement are sufficient to get us to AGI[^znbex4dkqfh].\n\n*   I don't want to say the pretraining will \"plateau\", as such, I do expect continued progress. But the dimensions along which the progress happens are going to decouple from the intuitive \"getting generally smarter\" metric, and will face steep diminishing returns.\n    *   Grok 3 and GPT-4.5 seem to confirm this.\n        *   Grok 3's main claim to fame was \"pretty good: it managed to dethrone Claude Sonnet 3.5.1 for some people!\". That was damning with faint praise.\n        *   GPT-4.5 is subtly better than GPT-4, particularly at writing/EQ. That's likewise a faint-praise damnation: it's not *much* better. Indeed, it reportedly came out below expectations for OpenAI as well, and they certainly weren't in a rush to release it. (It [was intended](https://openai.com/index/openai-board-forms-safety-and-security-committee/) as a new flashy frontier model, not the delayed, half-embarrassed \"here it is I guess, hope you'll find something you like here\".)\n    *   GPT-5 will be even less of an improvement on GPT-4.5 than GPT-4.5 was on GPT-4. The pattern will continue for GPT-5.5 and GPT-6, [the ~1000x and 10000x models](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=DMdFqA4ChTRczRLXs) they may train by 2029 (if they still have the money by then). Subtle quality-of-life improvements and meaningless benchmark jumps, but nothing paradigm-shifting.\n        *   (Not to be a scaling-law denier. I believe in them, I do! But they measure *perplexity*, not general intelligence/real-world usefulness, and Goodhart's Law is no-one's ally.)\n    *   OpenAI seem to expect this, what with them apparently [planning to slap the \"GPT-5\" label](https://x.com/sama/status/1889755723078443244) on the Frankenstein's monster made out of their current offerings instead of on, well, 100x'd GPT-4. They know they can't cause another hype moment without this kind of trickery.\n*   Test-time compute/RL on LLMs:\n    *   It will not meaningfully generalize beyond domains with easy verification. Some trickery like RLAIF and longer CoTs might provide *some* benefits, but they would be a fixed-size improvement. It will not cause a hard-takeoff self-improvement loop in \"soft\" domains.\n    *   RL will be good enough to turn LLMs into reliable tools for some fixed environments/tasks. They will reliably fall flat on their faces if moved outside those environments/tasks.\n    *   Scaling CoTs to e. g. millions of tokens or effective-indefinite-size context windows (if that even works) may or may not lead to math being solved. I expect it won't.\n        *   It may not work at all: the real-world returns on investment may end up linear while the costs of pretraining grow exponentially. I mostly expect FrontierMath to be beaten by EOY 2025 ([it's not that difficult](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=tzjmsY827Biwh5TtF)), but maybe it won't be beaten for years.[^ysooye03bqe]\n        *   Even if it \"technically\" works to speed up conjecture verification, I'm [skeptical](https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights?commentId=LWmLexCxAmaR936MC) on this producing *paradigm shifts* even in \"hard\" domains. *That* task is not actually an easily verifiable one.\n    *   (If math *is* solved, though, I don't know how to estimate the consequences, and it might invalidate the rest of my predictions.)\n*   \"But the models *feel* increasingly smarter!\":\n    *   It seems to me that \"vibe checks\" for how smart a model feels are easily gameable by making it have a better personality.\n    *   My guess is that it's most of the reason Sonnet 3.5.1 was so beloved. Its personality was made much more *appealing*, compared to e. g. OpenAI's corporate drones.\n    *   [The recent upgrade to GPT-4o](https://www.lesswrong.com/posts/bozSPnkCzXBjDpbHj/ai-104-american-state-capacity-on-the-brink#Huh__Upgrades) seems to confirm this. They seem to have merely given it a better personality, and people were reporting that it \"feels much smarter\".\n    *   Deep Research was this for me, at first. Some of its summaries were just *pleasant* to read, they felt so information-dense and intelligent! Not like typical AI slop at all! But then it turned out most of it was just AI slop underneath anyway, and now my slop-recognition function has adjusted and the effect is gone.\n*   What LLMs are good at: eisegesis-friendly problems and in-distribution problems.\n    *   [Eisegesis](https://en.wikipedia.org/wiki/Eisegesis) is \"the process of interpreting text in such a way as to introduce one's own presuppositions, agendas or biases\". LLMs feel very smart when you do the work of making them sound smart on your own end: when the interpretation of their output has a free parameter which you can mentally set to some value which makes it sensible/useful to you.\n        *   This includes e. g. philosophical babbling or brainstorming. *You* do the work of picking good interpretations/directions to explore, *you* impute the coherent personality to the LLM. And you inject very few bits of steering by doing so, but [those bits are load-bearing](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism#Cyborg_cognition). If left to their own devices, LLMs won't pick those obviously correct ideas any more often than chance.\n            *   See R1's CoTs, where it often does... [that](https://x.com/teortaxesTex/status/1887726953509069109?lang=en).\n        *   This also covers [stuff like Deep Research's outputs](https://x.com/sayashk/status/1887275315824660584). They're great specifically as high-level overviews of a field, when you're not relying on them to be *comprehensive* or *precisely on-target* or for *any given detail* to be correct.\n        *   It feels like this issue is easy to fix. LLMs already have ~all of the needed pieces, they just need to learn to recognize good ideas! Very few steering-bits to inject!\n        *   This issue felt easy to fix since GPT-3.5, [or perhaps GPT-2](https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally).\n        *   This issue is not easy to fix.\n    *   In-distribution problems:\n        *   One of the core features of the current AIs is the \"jagged frontier\" of capabilities.\n        *   This jaggedness is often defended by \"ha, as if humans don't have domains in which they're laughably bad/as if humans don't have consistent cognitive errors!\". I believe that counterargument is invalid.\n        *   LLMs are not good in some domains and bad in others. Rather, they are incredibly good at some *specific tasks* and bad at *other* tasks. Even if both tasks are in the same domain, even if tasks A and B are very similar, even if any human that can do A will be able to do B.\n        *   This is consistent with the constant complaints about LLMs and LLM-based agents being unreliable and their competencies being *impossible to predict* ([example](https://www.answer.ai/posts/2025-01-08-devin.html)).\n        *   That is: It seems the space of LLM competence shouldn't be thought of as some short-description-length connected manifold or slice through the space of problems, whose shape we're simply too ignorant to understand yet. (In which case \"LLMs are genuinely intelligent in a way orthogonal to how humans are genuinely intelligent\" is valid.)\n        *   Rather, it seems to be *a set of individual points* in the problem-space, plus these points' immediate neighbourhoods... Which is to say, the set of problems the solutions to which are present in their training data.[^nfivgw93jpp]\n        *   The impression that they generalize outside it is based on us [having a very poor grasp](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=vxHcdFb3KLWEQbmRv) regarding the solutions to what problems are present in their training data.\n        *   And yes, there's *some* generalization. But it's dramatically less than the impressions people have of it.\n*   Agency:\n    *   Genuine agency, by contrast, requires remaining on-target across *long inferential distances*: even after your task's representation becomes very complex in terms of the templates which you had memorized at the start.\n    *   LLMs still seem as terrible at this as they'd been in the GPT-3.5 age. Software agents break down once the codebase becomes complex enough, game-playing agents get stuck in loops out of which they break out only by accident, etc.\n    *   They just have bigger sets of templates now, which lets them fool people for longer and makes them useful for marginally more tasks. But the scaling on that seems pretty bad, and this certainly won't suffice for autonomously crossing the *astronomical* inferential distances required to usher in the Singularity.\n*   \"But the benchmarks!\"\n    *   I dunno, I think they're just not measuring what people think they're measuring. See the point about in-distribution problems above, plus the possibility of undetected [performance-gaming](https://x.com/miru_why/status/1892500715857473777), plus some [subtly but crucially unintentionally-misleading reporting](https://x.com/colin_fraser/status/1892721051299172641).\n    *   Case study: Prior to looking at [METR's benchmark](https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/), I'd expected that it's also (unintentionally!) doing some shenanigans that mean it's not actually measuring LLMs' real-world problem-solving skills. Maybe the problems were secretly in the training data, or there was a selection effect towards simplicity, or the prompts strongly hinted at what the models are supposed to do, or the environment was set up in an unrealistically \"clean\" way that minimizes room for error and makes solving the problem correctly the path of least resistance (in contrast to messy real-world realities), et cetera.\n        \n        As it turned out, yes, it's that last one: see the \"systematic differences from the real world\" [here](https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/). Consider what this means in the light of the previous discussion about inferential distances/complexity-from-messiness.\n        \n\nAs I'd said, I'm not 100% sure of that model. Further advancements might surprise me, there's an explicit carve-out for ??? consequences if math is solved, etc.\n\nBut the above is my baseline prediction, at this point, and I expect the probability mass on other models to evaporate by this year's end.\n\n* * *\n\nReal-World Predictions\n----------------------\n\n*   I dare not make the prediction that the LLM bubble will burst in 2025, or 2026, or in any given year in the near future. The AGI labs have a lot of money nowadays, they're managed by smart people, they have some real products, they're willing to produce propaganda, and they're buying their own propaganda (therefore it will appear authentic). They can keep the hype up for a very long time, if they want.\n    *   And they do want to. They *need* it, so as to keep the investments going. Oceans of compute is the only way to collect on the LLM bet they've made, in the worlds where that bet can pay off, so they will keep maximizing for investment no matter how dubious the bet's odds start looking.\n    *   Because what *else* are they to do? If they admit to themselves they're *not* closing their fingers around godhood after all, what will they have left?\n*   There will be news of various important-looking breakthroughs and advancements, at a glance looking very solid even to us/experts. Digging deeper, or waiting until the practical consequences of these breakthroughs materialize, will reveal that they're 80% hot air/hype-generation.[^vl37irxzr4a]\n*   At some point there might be massive layoffs due to ostensibly competent AI labor coming onto the scene, perhaps because OpenAI will start heavily propagandizing that these mass layoffs must happen. It will be an overreaction/mistake. The companies that act on that will crash and burn, and will be outcompeted by companies that didn't do the stupid.\n*   Inasmuch as LLMs boost productivity, it will mostly be as tools. There's a subtle but crucial difference between \"junior dev = an AI model\" and \"senior dev + AI models = senior dev + team of junior devs\". Both decrease the demand for junior devs (as they exist today, before they re-specialize into LLM whisperers or whatever). But the latter doesn't really require LLMs to be capable of end-to-end autonomous task execution, which is the property required for actual transformative consequences.\n    *   (And even then, all the rumors about LLMs 10x'ing programmer productivity [seem greatly overstated](https://www.lesswrong.com/posts/tqmQTezvXGFmfSe7f/how-much-are-llms-actually-boosting-real-world-programmer).)\n*   Inasmuch as human-worker replacements will come, they will be surprisingly limited in scope. I dare not make a prediction regarding the exact scope and nature, only regarding the *directionality* compared to current expectations.\n*   There will be a ton of innovative applications of Deep Learning, perhaps chiefly in the field of biotech, see [GPT-4b](https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/) and [Evo 2](https://x.com/IterIntellectus/status/1892251343881937090). Those are, I must stress, *human-made innovative applications of the paradigm of automated continuous program search.* Not AI models autonomously producing innovations.\n*   There will be various disparate reports about AI models autonomously producing innovations, in the vein of [this](https://x.com/SakanaAILabs/status/1892385766510338559) or [that](https://www.biorxiv.org/content/10.1101/2025.02.19.639094v1) or [that](https://arxiv.org/abs/2502.13025). They will turn out to be misleading or cherry-picked. E. g., examining those examples:\n    *   In the first case, most of the improvements [turned out to be reward-hacking](https://x.com/miru_why/status/1892500715857473777) (and not even intentional on the models' part).\n    *   [In the second case](https://x.com/jimnasyum/status/1892757709461533062), the scientists have pre-selected the problem on which the LLM is supposed to produce the innovation on the basis of already knowing that there's a low-hanging fruit to be picked there. That's like 90% of the work. And then they *further* picked the correct hypothesis from the set it generated, i. e., did eisegesis. And *also* there might be [any amount of data contamination](https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights?commentId=YhbWT4JaXK3EHbcQK) from these scientists or different groups speaking about their research in public, in the years they spent working on it.\n    *   In the third case, the AI produces useless slop with steps like \"..., Step N: invent the Theory of Everything (left as an exercise for the reader), ...\", lacking [the recognition function](https://colah.github.io/notes/taste/) for promising research. GPT-3-level stuff. (The whole setup can also likely be out-performed by taking the adjacency matrix of Wikipedia pages and randomly sampling paths from the corresponding graph, or [something like this](https://dl.acm.org/doi/10.1145/2623330.2623623).)\n*   I expect that by 2030s, LLMs will be heavily integrated into the economy and software, and will serve as very useful tools that found their niches. But just that: tools. Perhaps some narrow jobs will be greatly transformed or annihilated (by being folded into the job of an LLM nanny). But there will not be AGI or broad-scope agents arising from the current paradigm, nor autonomous 10x engineers.\n*   At some unknown point – probably in 2030s, possibly tomorrow (but likely not tomorrow) – someone will figure out a different approach to AI. Maybe a slight tweak to the LLM architecture, maybe a completely novel neurosymbolic approach. Maybe it will happen in a major AGI lab, maybe in some new startup. By default, everyone will die in <1 year after that.\n\n* * *\n\nClosing Thoughts\n----------------\n\nThis might seem like a ton of annoying nitpicking. Here's a simple generator of all of the above observations: **some people desperately,** ***desperately*** **want LLMs to be a bigger deal than what they are**.\n\nThey are not evaluating the empirical evidence in front of their eyes with proper precision*.*[^lph4528t2] Instead, they're vibing, and spending 24/7 inventing contrived ways to fool themselves and/or others.\n\nThey often succeed. They will continue doing this for a long time to come.\n\nWe, on the other hand, desperately *not* want LLMs to be AGI-complete. Since we try to avoid motivated thinking, to avoid deluding ourselves into believing into happier realities, we err on the side of pessimistic interpretations. In this hostile epistemic environment, that effectively leads to us being *overly gullible* and *prone to buying into hype*.\n\nIndeed, this environment is essentially optimized for exploiting [the virtue of lightness](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality). LLMs are masters at creating the *vibe* of being generally intelligent. Tons of people are cooperating, playing this vibe up, making tons of subtly-yet-crucially flawed demonstrations. Trying to see through this immense storm of bullshit very much *feels* like \"fighting a rearguard retreat against the evidence\".[^c0fnmg1rwsj]\n\nBut this isn't what's happening, in my opinion. On the contrary: it's the LLM believers who are sailing against the winds of evidence.\n\nIf LLMs were *actually* as powerful as they're hyped up to be, there wouldn't be the need for all of these attempts at *handholding*.\n\nEver more contrived agency scaffolds that yield ~no improvement. Increasingly more costly RL training procedures that fail to generalize. Hail-mary ideas regarding how to fix that generalization issue. Galaxy-brained ways to elicit knowledge out of LLMs that produce nothing of value. The need for all of this is strong evidence that *there's no seed of true autonomy/agency/generality within LLMs*. If there were, the most naïve AutoGPT setup circa early 2023 would've elicited it.\n\nPeople are extending LLMs a hand, hoping to pull them up to our level. But there's nothing reaching back.\n\nAnd none of the current incremental-scaling approaches will fix the issue. They will increasingly mask it, and some of this masking may be powerful enough to have real-world consequences. But any attempts at the Singularity based on LLMs will stumble well before takeoff.\n\nThus, I expect AGI Labs' AGI timelines have ~nothing to do with what will actually happen. On average, we likely have more time than the AGI labs say. Pretty likely that we have until 2030, maybe well into 2030s.\n\nBy default, we likely don't have *much* longer than that. Incremental scaling of known LLM-based stuff won't get us there, but I don't think the remaining qualitative insights are many. 5-15 years, at a rough guess.\n\n[^71wj581psb4]: For prudency's sake: GPT-4.5 has slightly overshot these expectations. \n\n[^znbex4dkqfh]: If you are really insistent on calling the current crop of SOTA models \"AGI\", replace this with \"autonomous AI\" or \"transformative AI\" or \"innovative AI\" or \"the transcendental trajectory\" or something. \n\n[^ysooye03bqe]: Will o4 really come out on schedule in ~2 weeks, showcasing yet another dramatic jump in mathematical capabilities, just in time to rescue OpenAI from the GPT-4.5 semi-flop? I'll be waiting. \n\n[^nfivgw93jpp]: This metaphor/toy model has been adapted from @Cole Wyeth. \n\n[^vl37irxzr4a]: Pretty sure Deep Research could not in fact \"do a single-digit percentage of all economically valuable tasks in the world\", except in the caveat-laden sense where you still have a human expert double-checking and rewriting its outputs. And in my personal experience, on the topics at which I am an expert, it would be easier to write the report from scratch than to rewrite DR's output.It's a useful way to get a high-level overview of some topics, yes. It blows Google out of the water at being Google, and then some. But I don't think it's a 1-to-1 replacement for any extant form of human labor. Rather, it's a useful zero-to-one thing. \n\n[^lph4528t2]: See all the superficially promising \"AI innovators\" from the previous section, which turn out to be false advertisement on a closer look. Or the whole \"10x'd programmer productivity\" debacle. \n\n[^c0fnmg1rwsj]: Indeed, even now, having written all of this, I have nagging doubts that this might be what I'm actually doing here. I will probably keep having those doubts until this whole thing ends, one way or another. It's not pleasant.",
      "plaintextDescription": "This isn't really a \"timeline\", as such – I don't know the timings – but this is my current, fairly optimistic take on where we're heading.\n\nI'm not fully committed to this model yet: I'm still on the lookout for more agents and inference-time scaling later this year. But Deep Research, Claude 3.7, Claude Code, Grok 3, and GPT-4.5 have turned out largely in line with these expectations[1], and this is my current baseline prediction.\n\n----------------------------------------\n\n\nThe Current Paradigm: I'm Tucking In to Sleep\nI expect that none of the currently known avenues of capability advancement are sufficient to get us to AGI[2].\n\n * I don't want to say the pretraining will \"plateau\", as such, I do expect continued progress. But the dimensions along which the progress happens are going to decouple from the intuitive \"getting generally smarter\" metric, and will face steep diminishing returns.\n   * Grok 3 and GPT-4.5 seem to confirm this.\n     * Grok 3's main claim to fame was \"pretty good: it managed to dethrone Claude Sonnet 3.5.1 for some people!\". That was damning with faint praise.\n     * GPT-4.5 is subtly better than GPT-4, particularly at writing/EQ. That's likewise a faint-praise damnation: it's not much better. Indeed, it reportedly came out below expectations for OpenAI as well, and they certainly weren't in a rush to release it. (It was intended as a new flashy frontier model, not the delayed, half-embarrassed \"here it is I guess, hope you'll find something you like here\".)\n   * GPT-5 will be even less of an improvement on GPT-4.5 than GPT-4.5 was on GPT-4. The pattern will continue for GPT-5.5 and GPT-6, the ~1000x and 10000x models they may train by 2029 (if they still have the money by then). Subtle quality-of-life improvements and meaningless benchmark jumps, but nothing paradigm-shifting.\n     * (Not to be a scaling-law denier. I believe in them, I do! But they measure perplexity, not general intelligence/real-world usefulness, and Goodhart's Law is n",
      "wordCount": 2810
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tqmQTezvXGFmfSe7f",
    "title": "How Much Are LLMs Actually Boosting Real-World Programmer Productivity?",
    "slug": "how-much-are-llms-actually-boosting-real-world-programmer",
    "url": null,
    "baseScore": 140,
    "voteCount": 66,
    "viewCount": null,
    "commentCount": 52,
    "createdAt": null,
    "postedAt": "2025-03-04T16:23:39.296Z",
    "contents": {
      "markdown": "LLM-based coding-assistance tools have been out for ~2 years now. Many developers have been reporting that this is dramatically increasing their productivity, up to 5x'ing/10x'ing it.\n\nIt seems clear that this multiplier isn't field-wide, at least. There's no corresponding increase in output, after all.\n\nThis would make sense. If you're doing anything nontrivial (i. e., anything other than adding minor boilerplate features to your codebase), LLM tools are fiddly. Out-of-the-box solutions don't Just Work for that purpose. You need to significantly adjust your workflow to make use of them, if that's even possible. Most programmers wouldn't know how to do that/wouldn't care to bother.\n\nIt's therefore reasonable to assume that a 5x/10x greater output, *if* it exists, is unevenly distributed, mostly affecting power users/people particularly talented at using LLMs.\n\nEmpirically, we likewise don't seem to be living in the world where the whole software industry is suddenly 5-10 times more productive. It'll have been the case for 1-2 years now, and I, at least, have felt approximately zero impact. I don't see 5-10x more useful features in the software I use, or 5-10x more software that's useful to me, or that the software I'm using is suddenly working 5-10x better, etc.\n\nHowever, I'm also struggling to see the supposed 5-10x'ing *anywhere else*. If power users are experiencing this much improvement, what projects were enabled by it?\n\nPreviously, I'd assumed I didn't know just because I'm [living under a rock](https://i.imgur.com/zsQjxEP.png). So I've tried to get Deep Research to fetch me an overview, and it... also struggled to find anything concrete. Judge for yourself: [one](https://chatgpt.com/share/67bfe37c-2774-800a-b3f8-70329a155227), [two](https://chatgpt.com/share/67bfe36e-acfc-800a-a84b-7fd7aa75f01d). The COBOL refactor counts, but that's about it. (Maybe I'm bad at prompting it?)\n\nEven the AGI labs' customer-facing offerings aren't an endless trove of rich features for interfacing with their LLMs in sophisticated ways – even though you'd assume there'd be an unusual concentration of power users there. You have a dialogue box and can upload PDFs to it, that's about it. You can't get the LLM to interface with an ever-growing list of arbitrary software and data types, there isn't an endless list of QoL features that you can turn on/off on demand, etc.[^u8kk2ooggb8]\n\nSo I'm asking LW now: What's the real-world impact? What projects/advancements exist now that wouldn't have existed without LLMs? And if none of that is *publicly* attributed to LLMs, what projects have appeared *suspiciously fast*, such that, on sober analysis, they couldn't have been spun up this quickly in the dark pre-LLM ages? What slice through the programming ecosystem is experiencing 10x growth, if any?\n\nAnd if we assume that this is going to proliferate, with all programmers attaining the same productivity boost as the early adopters are experiencing now, what would be the real-world impact?\n\nTo clarify, what I'm *not* asking for is:\n\n*   Reports full of vague hype about 10x'ing productivity, with no clear attribution regarding what project this 10x'd productivity enabled. (Twitter is full of those, but light on useful stuff actually being shipped.)\n*   Abstract economic indicators that suggest X% productivity gains. (This could mean anything, including an LLM-based bubble.)\n*   Abstract indicators to the tune of \"this analysis shows Y% more code has been produced in the last quarter\". (This can just indicate AI producing code slop/bloat).\n*   Abstract economic indicators that suggest Z% of developers have been laid off/junior devs can't find work anymore. (Which may be mostly a return to the pre-COVID normal trends.)\n*   Useless toy examples like \"I used ChatGPT to generate the 1000th clone of Snake/of this website!\".\n*   New tools/functions that are *LLM wrappers*, as opposed to being created via LLM help. (I'm not looking for LLMs-as-a-service, I'm looking for \"mundane\" outputs that were produced much faster/better due to LLM help.)\n\nI. e.: I want concrete, important real-life consequences.\n\nFrom the fact that I've observed none of them so far, and in the spirit of [Cunningham's Law](https://en.wikipedia.org/wiki/Ward_Cunningham#%22Cunningham's_Law%22), here's a tentative conspiracy theory: *LLMs mostly do not actually boost programmer productivity on net.* Instead:\n\n*   *N* hours that a programmer saves by generating code via an LLM are then re-wasted fixing/untangling that code.\n*   At a macro-scale, this sometimes leads to \"[climbing up where you can't get down](https://x.com/catherineols/status/1894110535383486934)\", where you use an LLM to generate a massive codebase, then it gets confused once a size/complexity threshold is passed, and then you have to start from scratch because the LLM made atrocious/alien architectural decisions. This likewise destroys (almost?) all apparent productivity gains.\n*   Inasmuch as LLMs actually do lead to people creating new software, it's mostly one-off trinkets/proofs of concept that nobody ends up using and which didn't need to exist. But it still \"feels\" like your productivity has skyrocketed.\n*   Inasmuch as LLMs actually do increase the amount of code that goes into useful applications, it mostly ends up spent on creating bloatware/[services that don't need to exist](https://x.com/met2llurgist/status/1891317521107636226). I. e., it actually makes the shipped software *worse,* because it's written more lazily.\n*   People who experience LLMs improving their workflows are mostly fooled by the magical effect of asking an LLM to do something in natural language and then immediately getting kinda-working code in response. They fail to track how much they spend integrating and fixing this code, and/or how much the code is actually used.\n\nI don't fully believe this conspiracy theory, it feels like it can't *possibly* be true. But it suddenly seems very compelling.\n\nI expect LLMs *have* definitely been useful for writing minor features or for getting the people inexperienced with programming/with a specific library/with a specific codebase get started easier and learn faster. They've been useful for me in those capacities. But it's probably like a 10-30% overall boost, plus *flat* cost reductions for starting in new domains and for some rare one-off projects like \"[do a trivial refactor](https://x.com/VictorTaelin/status/1873948475299111244)\".\n\nAnd this is mostly where it'll stay unless AGI labs actually crack long-horizon agency/innovations; i. e., basically until genuine AGI is actually there.\n\nProve me wrong, I guess.\n\n[^u8kk2ooggb8]: Just as some concrete examples: Anthropic took ages to add LaTeX support, and why weren't RL-less Deep Research clones offered as a default option by literally everyone 1.5 years ago?",
      "plaintextDescription": "LLM-based coding-assistance tools have been out for ~2 years now. Many developers have been reporting that this is dramatically increasing their productivity, up to 5x'ing/10x'ing it.\n\nIt seems clear that this multiplier isn't field-wide, at least. There's no corresponding increase in output, after all.\n\nThis would make sense. If you're doing anything nontrivial (i. e., anything other than adding minor boilerplate features to your codebase), LLM tools are fiddly. Out-of-the-box solutions don't Just Work for that purpose. You need to significantly adjust your workflow to make use of them, if that's even possible. Most programmers wouldn't know how to do that/wouldn't care to bother.\n\nIt's therefore reasonable to assume that a 5x/10x greater output, if it exists, is unevenly distributed, mostly affecting power users/people particularly talented at using LLMs.\n\nEmpirically, we likewise don't seem to be living in the world where the whole software industry is suddenly 5-10 times more productive. It'll have been the case for 1-2 years now, and I, at least, have felt approximately zero impact. I don't see 5-10x more useful features in the software I use, or 5-10x more software that's useful to me, or that the software I'm using is suddenly working 5-10x better, etc.\n\nHowever, I'm also struggling to see the supposed 5-10x'ing anywhere else. If power users are experiencing this much improvement, what projects were enabled by it?\n\nPreviously, I'd assumed I didn't know just because I'm living under a rock. So I've tried to get Deep Research to fetch me an overview, and it... also struggled to find anything concrete. Judge for yourself: one, two. The COBOL refactor counts, but that's about it. (Maybe I'm bad at prompting it?)\n\nEven the AGI labs' customer-facing offerings aren't an endless trove of rich features for interfacing with their LLMs in sophisticated ways – even though you'd assume there'd be an unusual concentration of power users there. You have a dialogue box and c",
      "wordCount": 996
    },
    "tags": [
      {
        "_id": "HFou6RHqFagkyrKkW",
        "name": "Programming",
        "slug": "programming"
      },
      {
        "_id": "TkZ7MFwCi4D63LJ5n",
        "name": "Software Tools",
        "slug": "software-tools"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6dgCf92YAMFLM655S",
    "title": "The Sorry State of AI X-Risk Advocacy, and Thoughts on Doing Better",
    "slug": "the-sorry-state-of-ai-x-risk-advocacy-and-thoughts-on-doing",
    "url": null,
    "baseScore": 152,
    "voteCount": 67,
    "viewCount": null,
    "commentCount": 53,
    "createdAt": null,
    "postedAt": "2025-02-21T20:15:11.545Z",
    "contents": {
      "markdown": "First, let me quote [my previous ancient post on the topic](https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry):\n\n> *Effective Strategies for Changing Public Opinion*\n> \n> [The titular paper](https://forum.effectivealtruism.org/posts/re6FsKPgbFgZ5QeJj/effective-strategies-for-changing-public-opinion-a) is very relevant here. I'll summarize a few points.\n> \n> *   The main two forms of intervention are *persuasion* and *framing*.\n> *   Persuasion is, to wit, an attempt to change someone's set of beliefs, either by introducing new ones or by changing existing ones.\n> *   Framing is a more subtle form: an attempt to change the relative *weights* of someone's beliefs, by empathizing different aspects of the situation, recontextualizing it.\n> *   There's a dichotomy between the two. Persuasion is found to be very ineffective if used on someone with high domain knowledge. Framing-style arguments, on the other hand, are more effective the more the recipient knows about the topic.\n> *   Thus, persuasion is better used on non-specialists, and it's most advantageous the first time it's used. If someone tries it and fails, they raise the recipient's domain knowledge, and the second persuasion attempt would be correspondingly hampered. Cached thoughts are also in effect.\n> *   Framing, conversely, is better for specialists.\n\nMy sense is that, up to this point, AI risk advocacy targeted the following groups of people:\n\n*   ML researchers and academics, who want \"scientifically supported\" arguments.\n    *   Advocacy methods: theory-based arguments, various proof-of-concept empirical evidence of misalignment, model organisms, et cetera.\n*   US policymakers, who want either popular support or expert support to champion a given cause.\n    *   Advocacy methods: behind-the-scenes elbow-rubbing, polls showing bipartisan concern for AI, [parading around](https://www.safe.ai/work/statement-on-ai-risk) the experts concerned about AI.\n*   Random Internet people with interests or expertise in the area.\n    *   Advocacy methods: viral LW/Xitter blog posts laying out AI X-risk arguments.\n\nPersuasion\n----------\n\nI think all of the above demographics aren't worth trying to persuade further at this point in time. It was very productive before, when they didn't yet have high domain knowledge related to AI Risk specifically, and there's been some major wins.\n\nBut *further* work in this space (and therefore work on all corresponding advocacy methods, yes) is likely to have ~no value.\n\n*   ~All ML researchers and academics that care have already made up their mind regarding whether they prefer to believe in misalignment risks or not. Additional scary papers and demos aren't going to make anyone budge.\n*   The relevant parts of the USG are mostly run by Musk and Vance nowadays, who have already decided either that they've found the solution to alignment (curiosity, or whatever Musk is spouting nowadays), or that AI safety is about wokeness. They're not going to change their minds. They're also going to stamp out any pockets of X-risk advocacy originating from within the government, so lower-level politicians are useless to talk to as well.\n*   Terminally online TPOT Xitters have already decided that it's about one of {US vs. China, open source vs. totalitarianism, wokeness vs. free speech, luddites vs. accelerationism}, and aren't going to change their mind in response to blog posts/expert opinions/cool papers.\n\nAmong those groups, we've already convinced ~everyone we were ever going to convince. That work was valuable and high-impact, but the remnants aren't going to budge in response to any evidence short of a megadeath AI catastrophe.[^73khucgsa7s]\n\nHell, *I* am 100% behind the AI X-risk being real, and even *I'm* getting nauseated at how tone-deaf, irrelevant, and impotent the arguments for it sound nowadays, in the spaces in which we keep trying to make them.\n\n### A Better Target Demographic\n\nHere's whom we actually should be trying to ~convince~ inform: *normal people*. The General Public.\n\n*   This demographic is very much a *distinct* demographic from the terminally online TPOT xitter users.\n*   This demographic is also dramatically bigger and more politically relevant.\n*   Poll have demonstrated that this demographic shows wide bipartisan support for the position that AI is existentially threatening. *If* their attention is directed to it.\n*   However: this demographic is largely unaware of what's been happening.\n    *   If they've used AI at all, they mostly think it's all just chatbots (and probably the free tier of ChatGPT, at that).\n    *   Ideas like hard takeoff, AI accelerating AI research, or obvious-to-us ways to turn chatbots into agents, are very much not obvious to them. The connection between \"this funny thing stuck in a dialog window\" and \"a lightcone-eating monstrosity\" requires *tons* of domain expertise to make.\n    *   Most of them don't even know the basics, such as that [we don't know how AI works](https://www.lesswrong.com/posts/CpjTJtW2RNKvzAehG/most-people-don-t-realize-we-have-no-idea-how-our-ais-work). They think it's all manually written code underneath, all totally transparent and controllable. And if someone *does* explain, [they tend to have appropriate reactions to that information](https://x.com/lillybilly299/status/1835794392059150506).\n*   This demographic is not going to eat out of the AGI Labs' hands when they say they're being careful and will share the benefits with humanity. \"Greedy corporations getting us all killed in the pursuit of power\" is pretty easy to get.\n*   This demographic is easily capable of understanding the grave importance of X-risks (see the recent concerns regarding 3% chance of asteroid impact in 2032).\n\nIf we can raise the awareness of the AGI Doom among *the actual general public* (again, *not* the small demographic of terminally online people), that will create significant political pressure on the USG, giving politicians an incentive to have platforms addressing the risks.\n\nThe only question is how to do that. I don't have a solid roadmap here. But it's not by writing viral LW/Xitter blog posts.\n\nSome scattershot thoughts:\n\n*   [Comedians](https://x.com/MasterTimBlais/status/1891097151880507398) seem like a useful vector.\n*   Newspapers and podcasts too. More stuff in the vein of [Eliezer's Time article](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) would be good. Podcast-wise, we want stuff with a broad audience of \"normies\". (So, probably not whatever podcasts you are listening to, median LW reader.)\n*   \"Who will control the ASI if they *can* control it?\" is another potentially productive question to pose. There's wide distrust in/dissatisfaction with *all* of {governments, corporations, billionaires, voting procedures}. Nobody wants them to have literal godlike power. Raising people's awareness regarding what the AGI labs are even *saying* they are doing, and what implications that'd have – without even bringing in misalignment concerns – might have the desired effect all on its own. ([Some more on that](https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together?commentId=bRexQDXJ9wqaN834k).)\n    *   This one is kinda tricky, though.\n*   [@harfe](https://www.lesswrong.com/users/harfe?mention=user)'s galaxy-brained idea [here](https://www.lesswrong.com/posts/NmrNLJnzLXx9Y82hF/harfe-s-shortform?commentId=Wc48xYjzf9tmJtJKi) about having someone run in the 2028 election on an AI Notkilleveryoneism platform. Not with the intent to win; with the intent to raise the awareness plus force the other candidates to speak on the topic.\n    *   I am not sure how sensible this is, and also 2028 might be too late. But it'd be big if workable.\n\nOverall, I expect that there's a ton of low-hanging high-impact fruits in this space, and even more high-impact *clever* interventions that are possible (in the vein of harfe's idea).\n\n### Extant Projects in This Space?\n\nSome relevant ones I've heard about:\n\n*   My impression is that MIRI is on it, with their change of focus. I haven't seen much come of that besides the Time article plus Eliezer appearing on a few podcasts, though.\n*   I think Conjecture might be doing this stuff too, with [their Compendium](https://www.lesswrong.com/posts/prm7jJMZzToZ4QxoK/the-compendium-a-full-argument-about-extinction-risk-from) et cetera? I think they've been talking about appeals to the (actual) general public as well. But I haven't been following them closely.\n*   [AI Notkilleveryoneism Memes](https://x.com/AISafetyMemes) shows some examples of what *not* to do:\n    *   Mostly speaking to a Twitter-user demographic.\n    *   Using shrill, jargon-heavy (therefore *exclusionary*) terminology. Primarily, constantly calling AI models \"shoggoths\" with no explanation.\n    *   Overall posture seems mostly optimized for creating an echo chamber of AI-terrified fanatics, not for maximally broad public outreach.\n*   [PauseAI](https://pauseai.info/) is a mixed bag. They get some things right, but they're also acting prematurely in ways that risk being massively net negative.\n    *   Protests' purpose is to cause [a signaling cascade](https://en.wikipedia.org/wiki/Preference_falsification#Generating_surprise), showing to people that there are tons of other people sharing their opinions and concerns. If done well, they cause a snowball effect, with subsequent protests being ever-bigger.[^5ercdo81na]\n    *   There's no chance of causing this yet: as I'd said, the general public's opinion on AI is mostly the null value. You need to raise awareness *first*, *then* aim for a cascade.\n    *   As-is, this is mostly going to make people's first exposure to AI X-risk be \"those crazy fringe protestors\". See my initial summary regarding effective persuasion: that would be *lethal*, gravely sabotaging our subsequent persuasion efforts.\n\nFraming\n-------\n\nTechnically, I think there *might* be some hope for appealing to researchers/academics/politicians/the terminally online, by *reframing* the AI Risk concerns in terms they would like more.\n\nAll the talk about \"safety\" and \"pauses\" have led to us being easy to misinterpret as unambitious, technology-concerned, risk-averse luddites. That's of course incorrect. I, at least, am 100% onboard with enslaving god, becoming immortal, merging with the machines, eating the galaxies, perverting the natural order to usher in an unprecedented age of prosperity, forcing the wheels of time into reverse to bring the dead back to life, and all that good stuff. I am pretty sure most of us are like this (if perhaps not in those exact terms).\n\nThe only reason I/we are not accelerationists is because the current direction of AI progress is not, in fact, on the track to lead us to that glorious future. It's instead on the track to get us all killed like losers.\n\nSo a more effective communication posture might be to empathize this: frame the current AI paradigm as a low-status sucker's game, and suggest alternative avenues for grabbing power. [Uploads](https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work), [superbabies](https://www.lesswrong.com/posts/DfrSZaf3JC8vJdbZL/how-to-make-superbabies), [adult intelligence enhancement](https://www.lesswrong.com/posts/JEhW3HDMKzekDShva/significantly-enhancing-adult-intelligence-with-gene-editing), more transparent/Agent Foundations-y AI research, etc. Reframing \"AI Safety\" as being about high-fidelity AI *Control* might also be useful. (It's mostly about making AIs Do What You Mean, after all, and the best alignment work is almost always dual-use.)\n\nIf the current paradigm of AI capability advancement visibly stumbles in its acceleration[^obsdvms982], this type of messaging would become even more effective. The black-box DL paradigm would open itself to derision for being a bubble, an empty promise.\n\nI mention this reluctantly/for comprehensiveness' sake. I think that this is a high-variance approach, most of the attempts at this are going to land badly, and will amount to nothing or have a negative effect. But it *is* a possible option.\n\nMessaging aimed at the general public is nevertheless a much better, and more neglected, avenue.\n\n[^73khucgsa7s]: Or maybe not even then, see the Law of Continued Failure. \n\n[^5ercdo81na]: The toy model there is roughly:Protest 1 is made up of some number of people \\(Q_0\\), who are willing to show their beliefs in public even with the support of zero other people.Protest 2 is joined by \\(Q_1\\) people who are willing to show their beliefs in public if they have the support of \\(Q_0\\) other people....Protest \\(N\\) is joined by \\(Q_N\\) people who are willing to show their beliefs in public if they have the support of \\(\\sum_{i=0}^{N-1}Q_{i}\\) other people.(Source, Ctrl+F in the transcript for \"second moving part is diverse threshold\".) \n\n[^obsdvms982]: Which I do mostly expect. AGI does not seem just around the corner on my inside model of AI capabilities. The current roadmap seems to be \"scale inference-time compute, build lots of RL environments, and hope that God will reward those acts of devotion by curing all LLM ailments and blessing them with generalization\". Which might happen, DL is weird. But I think there's a lot of room for skepticism with that idea.I think the position that The End Is Nigh is being deliberately oversold by powerful actors: the AGI Labs. It's in their corporate interests to signal hype to attract investment, regardless of how well research is actually progressing. So the mere fact that they're acting optimistic carries no information.And those of us concerned about relevant X-risks are uniquely vulnerable to buying into that propaganda. Just with the extra step of transmuting the hype into despair. We're almost exactly the people this propaganda is optimized for, after all – and we're not immune to it.",
      "plaintextDescription": "First, let me quote my previous ancient post on the topic:\n\n> Effective Strategies for Changing Public Opinion\n> \n> The titular paper is very relevant here. I'll summarize a few points.\n> \n>  * The main two forms of intervention are persuasion and framing.\n>  * Persuasion is, to wit, an attempt to change someone's set of beliefs, either by introducing new ones or by changing existing ones.\n>  * Framing is a more subtle form: an attempt to change the relative weights of someone's beliefs, by empathizing different aspects of the situation, recontextualizing it.\n>  * There's a dichotomy between the two. Persuasion is found to be very ineffective if used on someone with high domain knowledge. Framing-style arguments, on the other hand, are more effective the more the recipient knows about the topic.\n>  * Thus, persuasion is better used on non-specialists, and it's most advantageous the first time it's used. If someone tries it and fails, they raise the recipient's domain knowledge, and the second persuasion attempt would be correspondingly hampered. Cached thoughts are also in effect.\n>  * Framing, conversely, is better for specialists.\n\nMy sense is that, up to this point, AI risk advocacy targeted the following groups of people:\n\n * ML researchers and academics, who want \"scientifically supported\" arguments.\n   * Advocacy methods: theory-based arguments, various proof-of-concept empirical evidence of misalignment, model organisms, et cetera.\n * US policymakers, who want either popular support or expert support to champion a given cause.\n   * Advocacy methods: behind-the-scenes elbow-rubbing, polls showing bipartisan concern for AI, parading around the experts concerned about AI.\n * Random Internet people with interests or expertise in the area.\n   * Advocacy methods: viral LW/Xitter blog posts laying out AI X-risk arguments.\n\n\nPersuasion\nI think all of the above demographics aren't worth trying to persuade further at this point in time. It was very productive before, w",
      "wordCount": 1735
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "CTauobGMzSzJ4HHsd",
        "name": "Public Reactions to AI",
        "slug": "public-reactions-to-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "T6xSXiXF3WF6TmCyN",
    "title": "Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems",
    "slug": "abstract-mathematical-concepts-vs-abstractions-over-real",
    "url": null,
    "baseScore": 32,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2025-02-18T18:04:46.717Z",
    "contents": {
      "markdown": "Consider concepts such as \"a vector\", \"a game-theoretic agent\", or \"a market\". Intuitively, those are \"purely theoretical\" abstractions: they don't refer to any specific real-world system. Those abstractions would be useful even in universes very different from ours, and reasoning about them doesn't necessarily involve reasoning about our world.\n\nConsider concepts such as \"a tree\", \"my friend Alice\", or \"human governments\". Intuitively, those are \"real-world\" abstractions. While \"a tree\" bundles together lots of different trees, and so doesn't refer to any *specific* tree, it still refers to a specific *type of structure* found on Earth, and shaped by Earth-in-particular's specific conditions. While tree-like structures can exist in other places in the multiverse, there's an intuitive sense that any such \"tree\" abstraction would \"belong\" to the region of the multiverse in which the corresponding trees grow.\n\nIs there a way to formalize this, perhaps in the [natural-abstraction framework](https://www.lesswrong.com/w/natural-abstraction)? To separate the two categories, to find the True Name of \"purely theoretical concepts\"?\n\n* * *\n\nMotivation\n----------\n\nConsider a superintelligent agent/optimization process. For it to have disastrous real-world consequences, some component of it would need to *reason about* the real world. It would need to track where in the world it's embedded, what input-output pathways there are, and how it can exploit these pathways in order hack out of the proverbial box/cause other undesirable consequences.\n\nIf we could remove its ability to think about \"unapproved\" real-world concepts, and make it model itself as not part of the world, then we'd have something plausibly controllable. We'd be able to pose it well-defined problems (in math and engineering, up to whatever level of detail we can specify without exposing it to the real world – [which is plenty](https://x.com/zhou_xian_/status/1869511650782658846)) and it'd spit out solutions to them, without ever even *thinking* about causing real-world consequences. The idea of doing this would be literally outside its hypothesis space!\n\nThere are tons of loopholes and open problems here, but I think there's promise too.\n\n* * *\n\nIdeas\n-----\n\n(I encourage you to think about the topic on your own before reading my attempts.)\n\n**Take 1:** Perhaps this is about \"referential closure\". For concepts such as \"vectors\" or \"agents\", we can easily specify the list of formal axioms that would define the frameworks within which these concepts make sense. For things like \"trees\", however, we would have to refer to the real world directly: to the network of causes and effects entangled with our senses.\n\n... Except that we more or less *can*, nowadays, specify the mathematical axioms underlying the processes generating our universe (something something Poincaré group). To a sufficiently advanced superintelligence, there'd be no real difference.\n\n**Take 2:** Perhaps the intuitions are false, and the difference is quantitative, not qualitative.\n\n\"Vectors\" are concepts such that there's a simple list of axioms under which they're simple to describe/locate: they have low Kolmogorov complexity. By comparison, \"trees\" have a simple generator, but locating them within that generator's output (the quantum multiverse) takes very many bits.\n\n*   Optimistic case: There's a bimodal distribution, with \"real-world abstractions\" being on one end, and \"theoretical concepts\" being on the other end. We can lop off the high-complexity end of the distribution and end up with just the \"theoretical\" concepts.\n*   Pessimistic case: \"Theoretical concepts\" and \"real-world abstractions\" sit on a continuum, from e. g. \"this specific bunch of atoms\" to \"my friend Alice across time\" to \"humans\" to \"agents\". It's impossible to usefully separate them into two non-overlapping categories.\n\nI guess this is kind of plausible – indeed, it's probably the null hypothesis – but it doesn't feel satisfying.\n\nEspecially the pessimistic case: the \"continuum\" idea doesn't make sense to me. I think there's a big jump between \"a human\" and \"an agent\", and I don't see what abstractions could sit between them. (An abstraction over {humans, human governments, human corporations}, which is nevertheless more specific than \"an agent in general\"? Empirically, humanity hasn't been making use of this abstraction – we don't have a term for it – so it's evidently not convergently useful.)\n\n**Take 3:** Causality-based definitions. Perhaps \"theoretical abstractions\" are convergently useful abstractions which can't be changed by any process within our universe (i. e., within the net of causes and effects entangled with our senses)? \"Trees\" can be wiped out or modified, \"vectors\" can't be.\n\nThis doesn't really work, I think. There are two approaches:\n\n*   We model \"changing a concept\" as \"physical interventions that change whether this concept is applicable\". Then coloring all tree leaves in the world purple would causally impact the \"tree\" abstraction.\n    *   ... except then blowing up the Earth would \"causally impact\" the \"agent\" or \"market\" abstractions as well, by making the corresponding \"purely theoretical\" concepts inapplicable.\n*   We model \"concepts\" as timeless...\n    *   ... in which case \"a green-leaved tree\" would remain unchanged by our coloring all tree leaves purple.\n\nIntuitively, it feels like there's something to the \"causality\" angle, but I haven't been able to find a useful approach here.\n\n**Take 4:** Perhaps this is about reoccurrence.\n\nConsider [the \"global ontology\" of convergently useful concepts](https://www.lesswrong.com/posts/jJf4FrfiQdDGg7uco/the-telephone-theorem-information-at-a-distance-is-mediated#Connections_To_Some_Other_Pieces) defined over our universe. A concept such as \"an Earthly tree\" appears in it *exactly once*: as an abstraction over all of Earth's trees (which are abstractions over their corresponding bundles-of-atoms which have specific well-defined places, etc.). \"An Earthly tree\", specifically, doesn't reoccur *anywhere else*, at higher or lower or sideways abstraction levels.\n\nConversely, consider \"vectors\" or \"markets\". They never show up *directly*. Rather, they serve as \"ingredients\" in the makeup of many different \"real-world\" abstractions. \"Markets\" can model human behavior in a specific shop, or in the context of a country, and in relation to many different types of \"goods\" – or even the behavior of biological and [even purely physical](https://www.lesswrong.com/posts/Gk8Dvynrr9FWBztD4/what-s-a-market#Temperature) systems.\n\nSimilar for \"agents\" (animals, humans, corporations, governments), and even more obviously for \"vectors\".\n\nPotential counterarguments:\n\n*   \"An Earthly tree\" *can* be meaningfully used to model abstract processes: for example, you can reason about trees-the-data-structures as being physical-tree-like (rather than the other way around). Similarly, you can define \"an agent\" by taking the \"human\" abstraction and then subtracting various human idiosyncrasies from it...\n    *   ... but that's the key point here: *subtraction*. Even if you start out using the \"human\" abstraction to model agents in general (e. g., ascribing personhood to governments or corporations), you'd eventually \"wash out\" the human details, until you're left with a general-purpose \"agent\" abstraction that can be fitted with human-specific or government-specific details when you're talking about humans or governments.\n    *   That is, \"a human\" is not actually a *usefully reoccurring* abstraction.\n*   Consider concepts such as \"trees as used in human cultural symbolism\" or \"trees in this book I'm reading\". Those are copies of the \"tree\" abstraction that \"exist\" in different places in the global ontology, aren't they?\n    *   ... But those concepts are either (1) meaningfully different from \"an Earthly tree\" (e. g., alien trees in a sci-fi book), or (2) they're *pointers to* the \"Earthly tree\" abstraction, rather than *copies* of it.\n*   Are \"reoccurring ingredients\" and \"pointers\" the same thing? That is, if \"a human artist's conception of a tree\" is defined as \"the pointer to 'a tree' abstraction PLUS that artist's idiosyncrasies\", should we not consider \"the US market\" as \"the pointer to 'a market' abstraction PLUS that country's idiosyncrasies\"? Then \"a tree\" would be as purely theoretical as \"a market\", once again.\n    *   I think there's a meaningful difference. \"Pointers\" feel like a relationship between abstractions that shows up specifically *in the context of embedded agents* – with them having world-models, desires to talk about convergently useful abstractions, et cetera. By comparison, \"vectors\" aren't reoccurring in the makeup of gravitational systems and fluid dynamics \"because\" gravity and fluids want to \"talk about\" them.\n    *   I. e.: for \"pointers\", there's an *embedded causal process* that involves \"copying\" these abstractions across layers. Whereas for \"reoccurring ingredients\", it happens spontaneously.\n\n* * *\n\nTake 4 seems fairly promising to me, overall. Can you spot any major issues with it? Alternatively, a way to more properly flesh it out/formalize it?",
      "plaintextDescription": "Consider concepts such as \"a vector\", \"a game-theoretic agent\", or \"a market\". Intuitively, those are \"purely theoretical\" abstractions: they don't refer to any specific real-world system. Those abstractions would be useful even in universes very different from ours, and reasoning about them doesn't necessarily involve reasoning about our world.\n\nConsider concepts such as \"a tree\", \"my friend Alice\", or \"human governments\". Intuitively, those are \"real-world\" abstractions. While \"a tree\" bundles together lots of different trees, and so doesn't refer to any specific tree, it still refers to a specific type of structure found on Earth, and shaped by Earth-in-particular's specific conditions. While tree-like structures can exist in other places in the multiverse, there's an intuitive sense that any such \"tree\" abstraction would \"belong\" to the region of the multiverse in which the corresponding trees grow.\n\nIs there a way to formalize this, perhaps in the natural-abstraction framework? To separate the two categories, to find the True Name of \"purely theoretical concepts\"?\n\n----------------------------------------\n\n\nMotivation\nConsider a superintelligent agent/optimization process. For it to have disastrous real-world consequences, some component of it would need to reason about the real world. It would need to track where in the world it's embedded, what input-output pathways there are, and how it can exploit these pathways in order hack out of the proverbial box/cause other undesirable consequences.\n\nIf we could remove its ability to think about \"unapproved\" real-world concepts, and make it model itself as not part of the world, then we'd have something plausibly controllable. We'd be able to pose it well-defined problems (in math and engineering, up to whatever level of detail we can specify without exposing it to the real world – which is plenty) and it'd spit out solutions to them, without ever even thinking about causing real-world consequences. The idea of doing ",
      "wordCount": 1344
    },
    "tags": [
      {
        "_id": "RyNWXFjKNcafRKvPh",
        "name": "Agent Foundations",
        "slug": "agent-foundations"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "zYCoqjYNHFAEJD8TC",
    "title": "Are You More Real If You're Really Forgetful?",
    "slug": "are-you-more-real-if-you-re-really-forgetful",
    "url": null,
    "baseScore": 40,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 30,
    "createdAt": null,
    "postedAt": "2024-11-24T19:30:55.233Z",
    "contents": {
      "markdown": "It's a standard assumption, in anthropic reasoning, that effectively, we simultaneously exist in every place in Tegmark IV that simulates this precise universe (see e. g. [here](https://www.lesswrong.com/posts/KcvJXhKqx4itFNWty/k-complexity-is-silly-use-cross-entropy-instead#Empiricism)).\n\nHow far does this reasoning go?\n\nSuppose that the universe's state is described by \\\\(n\\\\) low-level variables \\\\(x_1,\\dots,x_n\\\\). However, your senses are \"coarse\": you can only view and retain the memory of \\\\(m\\\\) variables \\\\(y_1,\\dots,y_m\\\\), where \\\\(m\\ll n\\\\) and each \\\\(y_i\\\\) is a deterministic function of some subset of \\\\(x_1,\\dots,x_n\\\\).\n\nConsider a high-level state \\\\(Y^*\\\\), corresponding to each \\\\(y_i\\\\) being assigned some specific value. For any \\\\(Y^*\\\\), there's an equivalence class of low-level states \\\\(X^*\\\\) precisely consistent with \\\\(Y^*\\\\).\n\nGiven this, if you observe \\\\(Y^*\\\\), is it valid to consider yourself simultaneously existing in *all* corresponding low-level states \\\\(X^*\\\\) consistent with \\\\(Y^*\\\\)?\n\nNote that, so far, this is isomorphic to the scenario from [Nate's post](https://www.lesswrong.com/posts/KcvJXhKqx4itFNWty/k-complexity-is-silly-use-cross-entropy-instead#Empiricism), which considers all universes that only differ by the choices of gauge (which is undetectable from \"within\" the system) equivalent.\n\nNow let's examine increasingly weirder situations based on the same idea.\n\n**Scenario 1**:\n\n*   Consider two Everett branches, \\\\(A\\\\) and \\\\(B\\\\). They only differ by the exact numbers of photons in your room: \\\\(A\\\\) has an extra photon.\n*   Suppose that we gave the entire history of your observations over your lifetime to [AIXI](https://en.wikipedia.org/wiki/AIXI), which simulates all universes consistent with your observations. Suppose that, in the end, it's only able to narrow it down to \"\\\\(A\\\\) or \\\\(B\\\\)\".\n*   Does that mean you currently simultaneously exist in both branches?\n*   Importantly, note that the crux here isn't whether *you*, a *bounded* agent, are able to consciously differentiate between \\\\(A\\\\) and \\\\(B\\\\).\n    *   That is: Suppose that, in \\\\(A\\\\), the extra photon hits your eye and makes you see a tiny flash of red. If so, then, even though you likely won't make any conscious inferences about the photons, that'd still create a difference between the sensory streams, which AIXI (an *unbounded* computation) would be able to use to distinguish between \\\\(A\\\\) and \\\\(B\\\\).\n    *   Similarly, if the existence of the extra photon causes a tiny divergence ten years down the line, which will lead to a different photon hitting your eye and your seeing a tiny red flash, this will *likewise* create a difference that AIXI would be able to use.\n*   But if there's never even a *bit* of difference between your sensory streams, are \\\\(A\\\\) and \\\\(B\\\\) equivalent for the purpose of whether you exist in them?\n\nI'm inclined to bite this bullet: yes, you exist in all universes consistent with your high-level observations, even if their low-level states differ.\n\n**Scenario 2**: if you *absolutely forget* a detail, would the set of the universes you're embedded in *increase*? Concretely:\n\n*   Similar setup as before: an extra photon, you see a tiny red flash, but then forget about it. In the intermediate time that you perceived and remembered it, you've taken no actions that made a divergence between \\\\(A\\\\) and \\\\(B\\\\), and your neural processes erased the memory near-completely, such that the leftover divergence between \\\\(A\\\\) and \\\\(B\\\\) will likewise never register to your conscious senses.\n*   AIXI, if fed the contents of your mind pre-photon, would only narrow it down to \\\\(A\\\\)-or-\\\\(B\\\\). If fed the contents of your mind while you're remembering the flash, it'd be able to distinguish between \\\\(A\\\\) and \\\\(B\\\\). If fed the contents post-forgetting, we're back to indistinguishability between \\\\(A\\\\) and \\\\(B\\\\).\n*   So: does that mean you existed in both \\\\(A\\\\) and \\\\(B\\\\) before observing the photon, then got \"split\" into an \\\\(A\\\\)-self and a \\\\(B\\\\)-self, and then \"merged back\" once you forgot?\n\nI'm inclined to bite this bullet too, though it feels somewhat strange. Weird implication: you can increase the amount of reality-fluid assigned to you by giving yourself amnesia.[^c79xlpebspo]\n\n**Scenario 3**: Now imagine that you're a flawed human being, prone to confabulating/misremembering details, and also you don't hold the entire contents of your memories in your mind all at the same time. If I ask you whether you saw a small red flash 1 minute ago, and you confirm that you did, will you end up in a universe where there's an extra photon, *or* in a universe where you've confabulated this memory? Or in both?\n\n**Scenario 4**: Suppose you observe some macro-level event, such as learning that there are 195 countries in the world. Suppose there are similar-ish Everett branches where there's only 194 internationally recognized countries. This difference isn't small enough to get lost in thermal noise. The existence vs. non-existence of an extra country doubtlessly left countless side-evidence in your conscious memories, such that AIXI would be able to reconstruct the country's (non-)existence even if you're prone to forgetting or confabulating the exact country-count.\n\n... Or would it? Are you sure that the experiential content you're *currently* perceiving, and the stuff *currently* in your working memory, anchor you only to Everett branches that have 195 countries?\n\nSure, if you *went looking* through your memories, you'd doubtlessly uncover some details that'd be able to distinguish a branch where you confabulated an extra country with a branch where it really exists. But you haven't been doing that before reading the preceding paragraphs. Was the split made only when you *started looking*? Will you merge again, once you unload these memories?\n\nThis setup seems isomorphic, in the relevant sense, to the initial setup with only perceiving high-level variables \\\\(y_i\\\\). In this case, we just model you as a system with even more \"coarse\" senses.[^ktr9mm67xz] Which, in turn, is isomorphic to the standard assumption of simultaneously exist in every place in Tegmark IV that simulates this precise universe.\n\nOne move you could make, here, is to claim that \"you\" only identify with systems that have some specific personality traits and formative memories. As a trivial example, you could claim that a viewpoint which is consistent with your current perceptions and working-memory content, but who, if they query their memories for their name, and then experience remembering \"Cass\" as the answer, is not really \"you\".\n\nBut then, presumably you wouldn't consider \"I saw a red flash one minute ago\" part of your identity, else you'd consider *naturally forgetting* such a detail a kind of death. Similarly, even some macro-scale details like \"I believe there are 195 countries in the world\" are presumably not part of your identity. A you who confabulated an extra country is still you.\n\nWell, I don't think this is necessarily a big deal, even if true. But it's relevant to some agent-foundation work I've been doing, and I haven't seen this angle discussed before.\n\n**The way it** ***can*** **matter**: Should we expect to exist in universes that [*abstract well*](https://www.lesswrong.com/tag/natural-abstraction), by the exact same argument that we use to [argue that we should expect to exist in \"alt-simple\" universes](https://www.lesswrong.com/posts/KcvJXhKqx4itFNWty/k-complexity-is-silly-use-cross-entropy-instead#Empiricism)?\n\nThat is: suppose there's a class of universes in which the information from the \"lower levels\" of abstraction becomes increasingly less relevant to higher levels. It's still \"present\" on a moment-to-moment basis, such that an AIXI which retained the full memory of an embedded agent's sensory stream would be able to narrow things down to a universe specified up to low-level details.\n\nBut the *actual* agents embedded in such universes don't have such perfect memories. They constantly forget the low-level details, and presumably \"identify with\" only high-level features of their identity. For any such agent, is there then an \"equivalence class\" of agents that are different at the low level (details of memories/identity), but whose high-level features match enough that we should consider them \"the same\" agent for the purposes of the \"anthropic lottery\"?\n\nFor example, suppose there are two Everett branches that differ by whether you saw a dog run across your yard yesterday. The existence of an extra dog doubtlessly left countless \"microscopic\" traces in your total observations over your lifetime: AIXI would be able to tell the universes apart. But suppose our universe is well-abstracting, and this specific dog didn't set off any butterfly effects. The consequences of its existence were \"smoothed out\", such that its existence vs. non-existence never left any *major* differences in your perceptions. Only various small-scale details that you forgot/don't matter.\n\nDoes it then mean that both universes contain an agent that \"counts as you\" for the purposes of the \"anthropic lottery\", such that you should expect to be *either* of them at random?\n\nIf yes, then we should expect ourselves to be agents that exist in a universe that abstracts well, because \"high-level agents\" embedded in such universes are \"supported\" by a larger equivalence class of universes (since they draw on reality fluid from an entire *pool* of \"low-level\" agents).\n\n* * *\n\nSo: are there any fatal flaws in this chain of reasoning? Undesirable consequences to biting all of these bullets that I'm currently overlooking?\n\n[^c79xlpebspo]: Please don't actually do that. \n\n[^ktr9mm67xz]: As an intuition-booster, imagine that we implemented some abstract system that got only very sparse information about the wider universe. For example, a chess engine. It can't look at its code, and the only inputs it gets are the moves the players make. If we imagine that there's a conscious agent \"within\" the chess engine, the only observations of which are the chess moves being made, what \"reason\" does it have to consider itself embedded in our universe specifically, as opposed to any other universe in which chess exists? Including universes with alien physics, et cetera.",
      "plaintextDescription": "It's a standard assumption, in anthropic reasoning, that effectively, we simultaneously exist in every place in Tegmark IV that simulates this precise universe (see e. g. here).\n\nHow far does this reasoning go?\n\nSuppose that the universe's state is described by n low-level variables x1,…,xn. However, your senses are \"coarse\": you can only view and retain the memory of m variables y1,…,ym, where m≪n and each yi is a deterministic function of some subset of x1,…,xn.\n\nConsider a high-level state Y∗, corresponding to each yi being assigned some specific value. For any Y∗, there's an equivalence class of low-level states X∗ precisely consistent with Y∗.\n\nGiven this, if you observe Y∗, is it valid to consider yourself simultaneously existing in all corresponding low-level states X∗ consistent with Y∗?\n\nNote that, so far, this is isomorphic to the scenario from Nate's post, which considers all universes that only differ by the choices of gauge (which is undetectable from \"within\" the system) equivalent.\n\nNow let's examine increasingly weirder situations based on the same idea.\n\nScenario 1:\n\n * Consider two Everett branches, A and B. They only differ by the exact numbers of photons in your room: A has an extra photon.\n * Suppose that we gave the entire history of your observations over your lifetime to AIXI, which simulates all universes consistent with your observations. Suppose that, in the end, it's only able to narrow it down to \"A or B\".\n * Does that mean you currently simultaneously exist in both branches?\n * Importantly, note that the crux here isn't whether you, a bounded agent, are able to consciously differentiate between A and B.\n   * That is: Suppose that, in A, the extra photon hits your eye and makes you see a tiny flash of red. If so, then, even though you likely won't make any conscious inferences about the photons, that'd still create a difference between the sensory streams, which AIXI (an unbounded computation) would be able to use to distinguish between ",
      "wordCount": 1470
    },
    "tags": [
      {
        "_id": "PbShukhzpLsWpGXkM",
        "name": "Anthropics",
        "slug": "anthropics"
      },
      {
        "_id": "MP8NqPNATMqPrij4n",
        "name": "Embedded Agency",
        "slug": "embedded-agency"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "LbfWAq4F8tHaQsTyT",
    "title": "Towards the Operationalization of Philosophy & Wisdom",
    "slug": "towards-the-operationalization-of-philosophy-and-wisdom",
    "url": "https://aiimpacts.org/towards-the-operationalization-of-philosophy-wisdom/",
    "baseScore": 20,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-28T19:45:07.571Z",
    "contents": {
      "markdown": "*Written for* [*the competition on the Automation of Wisdom and Philosophy*](https://www.lesswrong.com/posts/hiuTzNBqG2EYg6qM5/winners-of-the-essay-competition-on-the-automation-of-wisdom)*.*\n\n* * *\n\nSummary\n-------\n\nPhilosophy and wisdom, and the processes underlying them, currently lack a proper *operationalization*: a set of robust formal or semi-formal definitions. If such definitions were found, they could be used as the foundation for a strong methodological framework. Such a framework would provide clear guidelines for how to engage in high-quality philosophical/wise reasoning and how to evaluate whether a given attempt at philosophy or wisdom was a success or a failure.\n\nTo address that, I provide candidate definitions for philosophy and wisdom, relate them to intuitive examples of philosophical and wise reasoning, and offer a tentative formalization of both concepts. The motivation for this is my belief that the lack of proper operationalization is the main obstacle to both (1) scaling up the work done in these domains (i. e., creating a bigger ecosystem that would naturally attract funding), and (2) automating them.\n\nThe discussion of philosophy focuses on the tentative formalization of a specific *algorithm* that I believe is central to philosophical thinking: the algorithm that allows humans to derive novel ontologies (conceptual schemes). Defined in a more fine-grained manner, the function of that algorithm is “deriving a set of assumptions using which a domain of reality could be decomposed into subdomains that could be studied separately”.\n\nI point out the similarity of this definition to [John Wentworth’s operationalization of natural abstractions](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1), from which I build the formal model.\n\nFrom this foundation, I discuss the discipline of philosophy more broadly. I point out instances where humans seem to employ the “algorithm of philosophical reasoning”, but which don’t fall under the standard definition of “philosophy”. In particular, I discuss the category of research tasks varyingly called “qualitative” or “non-paradigmatic” research, arguing that the core cognitive processes underlying them are implemented using “philosophical reasoning” as well.\n\nCounterweighting that, I define philosophy-as-a-discipline as a special case of such research. While “qualitative research” within a specific field of study focuses on decomposing the domain of reality *within* that field’s remit, “philosophy” focuses on decomposing reality-as-a-whole (which, in turn, produces the previously mentioned “specific fields of study”).\n\nSeparately, I operationalize wisdom as meta-level cognitive heuristics that take object-level heuristics for planning/inference *as inputs*, and output predictions about the real-world consequences of an agent which makes use of said object-level heuristics. I provide a framework of agency in which that is well-specified as “inversions of inversions of environmental causality”.\n\nI close things off with a discussion of whether “human-level” and “superhuman” AIs would be wise/philosophical (arguing yes), and what options my frameworks offer regarding scaling up or automating both types of reasoning.\n\n1\\. Philosophical Reasoning\n---------------------------\n\nOne way to define philosophy is “the study of confusing questions”. Typical philosophical reasoning happens when you notice that you have some intuitions or nagging questions about a domain of reality which hasn’t already been transformed into a formal field of study, and you follow them, attempting to gain clarity. If successful, this often results in the creation of a new field of study focused solely on that domain, and the relevant inquiries stop being part of philosophy.\n\nNotable examples include:\n\n*   Physics, which started as “natural philosophy”.\n*   Chemistry, which was closely related to a much more philosophical “alchemy”.\n*   Economics, rooted in moral philosophy.\n*   Psychology, from philosophy of mind.\n\nAnother field that serves as a good example is [agent foundations](https://intelligence.org/files/TechnicalAgenda.pdf)[^rx2itcvi3te], for those readers familiar with it.\n\nOne notable feature of this process is that the new fields, once operationalized, become decoupled from the rest of reality by certain assumptions. A focus on laws that apply to all matter (physics); or on physical interactions of specific high-level structures that are only possible under non-extreme temperatures and otherwise-constrained environmental conditions (chemistry); or on the behavior of human minds; and so on.\n\nThis isolation allows each of these disciplines to be studied *separately*. A physicist doesn’t need training in psychology or economics, and vice versa. By the same token, a physicist mostly doesn’t need to engage in interdisciplinary philosophical ponderings: the philosophical work that created the field has already laid down the conceptual boundaries beyond which physicists mostly don’t *need* to go.\n\nThe core feature underlying this overarching process of philosophy is the aforementioned “philosophical reasoning”: the cognitive algorithms that implement our ability to *generate* valid decompositions of systems or datasets. Formalizing these algorithms should serve as the starting point for operationalizing philosophy in a more general sense.\n\n### 1A. What Is an Ontology?\n\nIn the context of this text, an “ontology” is a decomposition of some domain of study into a set of higher-level concepts, which characterize the domain in a way that is compact, comprehensive, and could be used to produce models that have high predictive accuracy.\n\nIn more detail:\n\n*   **“Compactness”:** The ontology has fewer “moving parts” (concepts, variables) than a full description of the corresponding domain. Using models based on the ontology for making predictions requires a dramatically lower amount of computational or cognitive resources, compared to a “fully detailed” model.\n*   **“Accuracy”**: An ontology-based model produces predictions about the domain that are fairly accurate at a high level, or have a good upper bound on error.\n*   **“Comprehensive”**: The ontology is valid for all or almost all systems that we would classify as belonging to the domain in question, and characterizes them according to a known, finite family of concepts.\n\nChemistry talks about atoms, molecules, and reactions between them; economics talks about agents, utility functions, resources, and trades; psychology recognizes minds, beliefs, memories, and emotions. An ontology answers the question of *what* you study when you study some domain, characterizes the joints along which the domain can be carved and which questions about it are meaningful to focus on. (In this sense, it’s similar to the philosophical notion of a “conceptual scheme”, although I don’t think it’s an exact match.)\n\nUnder this view, deriving the “highest-level ontology” – the ontology for the-reality-as-a-whole – decomposes reality into a set of concepts such as “physics”, “chemistry”, or “psychology”. These concepts explicitly classify which parts of reality could be viewed as their instances, thereby decomposing reality into domains that could be studied separately (and from which the *disciplines* of physics, chemistry, and psychology could spring).\n\nBy contrast, on a lower level, arriving at the ontology of some specific field of study allows you to decompose it into specific sub-fields. These subfields can, likewise, be studied mostly separately. (The study of gasses vs. quantum particles, or inorganic vs. organic compounds, or emotional responses vs. memory formation.)\n\nOne specific consequence of the above desiderata is that [good ontologies commute](https://www.lesswrong.com/posts/nLhHY2c8MWFcuWRLx/good-ontologies-induce-commutative-diagrams).\n\nThat is, suppose you have some already-defined domain of reality, such as “chemistry”. You’d like to further decompose it into sub-domains. You take some system from this domain, such as a specific chemical process, and derive a prospective ontology for it. The ontology purports to decompose the system into a set of high-level variables plus compactly specified interactions between them, producing a predictive model of it.\n\nIf you then take a different system from the same domain, *the same ontology* should work for it. If you talked about “spirits” and “ether” in the first case, but you need to discuss “molecules” and “chemical reactions” to model the second one, then the spirits-and-ether ontology doesn’t suffice to capture the entire domain. And if there are *no* extant domains of reality which are well-characterized by your ontology – if the ontology of spirits and ether was derived by “overfitting” to the behavior of the first system, and it fails to robustly generalize to other examples – then this ontology is a bad one.\n\nThe go-to historical example comes from the field of chemistry: [the phlogiston theory](https://en.wikipedia.org/wiki/Phlogiston_theory). The theory aimed to explain combustion, modeling it as the release of some substance called “phlogiston”. However, the theory’s explanations for different experiments implied contradictory underlying dynamics. In some materials, phlogiston was supposed to have positive mass (and its release decreased the materials’ weight); in others, negative mass (in metals, to explain why they *gained* weight after being burned). The explanations for its interactions with air were likewise ad-hoc, often invented *post factum* to rationalize an experimental result, and essentially never to predict it. That is, they were overfit.\n\nAnother field worth examining here is agent foundations. The process of deriving a suitable ontology for it hasn’t yet finished. Accordingly, it is plagued by questions of what concepts / features it should be founded upon. Should we define agency from idealized utility-maximizers, or should we define it [structurally](https://www.lesswrong.com/posts/moi3cFY2wpeKGu9TT/clarifying-the-agent-like-structure-problem)? Is consequentialism-like goal-directed behavior even [the right thing to focus on](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX), when studying real-world agent-like systems? [What formal definition do the “values” of realistic agents have?](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans)\n\nIn other words: what is the set of variables which serve to compactly and comprehensively characterize and model *any* system we intuitively associate with “agents”, the same way chemistry can characterize any chemical interaction in terms of molecules and atoms?\n\nAnother telling example is mechanistic interpretability. Despite being a very concrete and empirics-based field of study, it likewise involves attempts to derive a novel ontology for studying neural networks. Can individual neurons be studied separately? The evidence suggests otherwise. If not, what *are* the basic “building blocks” of neural networks? We can [always](https://www.lesswrong.com/posts/sxhfSBej6gdAwcn7X/coordinate-free-interpretability-theory) decompose a given set of activations into sparse components, but what decompositions would be robust, i. e., [applicable to all forward passes](https://www.lesswrong.com/posts/TTTHwLpcewGjQHWzh/what-is-the-true-name-of-modularity) of a given ML model? ([Sparse autoencoders](https://transformer-circuits.pub/2023/monosemantic-features/index.html) represent some progress along this line of inquiry.)\n\nAt this point, it should be noted that the process of deriving ontologies, which was previously linked to “philosophical reasoning”, seems to show up in contexts that are far from the traditional ideas of what “philosophy” is. I argue that this is not an error: we are attempting to investigate a cognitive algorithm that is core to philosophy-as-a-discipline, yet it’s not a given that this algorithm would show up *only* in the context of philosophy. (An extended discussion of this point follows in 1C and 1E.)\n\nTo summarize: Philosophical reasoning involves focusing on some domain of reality[^okelgzd32wl] to derive an ontology for it. That ontology could then be used to produce a “high-level summary” of any system from the domain, in terms of specific high-level variables and compactly specifiable interactions between them. This, in turn, allows to decompose this domain into further sub-domains.\n\n### 1B. Tentative Formalization\n\nPut this way, the definition could be linked to [John Wentworth’s definition of natural abstractions](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1).\n\nThe Natural Abstraction Hypothesis states that the real-world data are distributed such that, for any set of “low-level” variables \\\\(L\\\\) representing some specific system or set of systems, we can derive the (set of) high-level variable(s) \\\\(H\\\\), such that they would serve as “natural latents” for \\\\(L\\\\). That is: conditional on the high-level variables \\\\(H\\\\), the low-level variables \\\\(L\\\\) would become (approximately) independent[^r6h6zfmmwt]:\n\n\\\\\\[P(L|H)=\\prod_{L_i\\in L}P(L_i|H)\\\\\\]\\\\\\[P(L_i|(L\\setminus L_i)\\land H)=P(L_i|H)\\\\\\]\n\n(Where “\\\\” denotes set subtraction, meaning \\\\(L\\setminus L_i\\\\) is the set of all \\\\(L_k\\\\) except \\\\(L_i\\\\).)\n\nThere are two valid ways to interpret \\\\(L\\\\) and \\\\(H\\\\).\n\n*   [The “bottom-up” interpretation](https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information): \\\\(L_i\\\\) could be different parts of a specific complex system, such as small fragments of a spinning gear. \\\\(H\\\\) would then correspond to a set of high-level properties of the gear, such as its rotation speed, the mechanical and molecular properties of its material, and so on. Conditional on \\\\(H\\\\), the individual \\\\(L_i\\\\) become independent: once we’ve accounted for the shared material, for example, the only material properties by which they vary are e. g. small molecular defects, individual to each patch.\n*   [The “top-down” interpretation](https://www.lesswrong.com/posts/N2JcFZ3LCCsnK2Fep/the-minimal-latents-approach-to-natural-abstractions): \\\\(L_i\\\\) could be different examples of systems belonging to some reference class of systems, such as individual examples of trees. \\\\(H\\\\) would then correspond to the general “tree” abstraction, capturing the (distribution over the) shapes of trees, the materials they tend to be made of, and so on. Conditional on \\\\(H\\\\), the individual\\\\(\\\\) \\\\(L_i\\\\) become independent: the “leftover” variance are various contingent details such as “how many leaves this particular tree happens to have”.\n\nPer the hypothesis, the high-level variables \\\\(H\\\\) would tend to correspond to intuitive human abstractions. In addition, they would be “in the territory” and convergent – in the sense that *any* efficient agent (or agent-like system) that wants to model some chunk of the world would arrive at approximately *the same* abstractions for this chunk, regardless of the agent’s goals and quirks of its architecture. What information is shared between individual fragments of a gear, or different examples of trees, is some *ground-truth* fact about the systems in question, rather than something subject to the agent’s choice[^a908577wp1c].\n\n[The Universality Hypothesis](https://distill.pub/2020/circuits/zoom-in/#three-speculative-claims) in machine-learning interpretability is a [well-supported](https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-universality) empirical complement of the NAH. While it doesn’t shed much light on what exact mathematical framework for abstractions we should use, it supplies strong evidence in favor of the NAH’s basic premise: that there’s *some* notion of abstraction which is convergently learned by agents and agent-like systems.\n\nA natural question, in this formalism, is how to *pick* the initial set of low-level variables \\\\(L\\\\)for the ontology of which we’d be searching: how we know to draw the boundary around the gear, how we know to put only examples of trees into the set \\\\(L\\\\). That question is currently open, although one simple way to handle it might be to simply search for a set such that it’d have a nontrivial natural latent \\\\(H\\\\).\n\nThe NAH framework captures the analysis in the preceding sections well. \\\\(H\\\\) constitutes the ontology of \\\\(L\\\\), creating conditional independence between the individual variables. Once \\\\(H\\\\) is derived, we can study each of \\\\(L_i\\\\) separately. (More specifically, we’d be studying \\\\(L_i\\\\) *conditioned on* \\\\(H\\\\): the individual properties of a specific tree *in the context of* it being a tree; the properties of a physical system in the context of viewing it as a physical system.)\n\nIf the disagreement over the shape of \\\\(H\\\\) exists – if researchers or philosophers are yet to converge to the same \\\\(H\\\\) – that’s a sign that *no* proposed \\\\(H\\\\) is correct, that it fails to robustly induce independence between \\\\(L_i\\\\). (Psychology is an illustrative example here: there are many extant ontologies purporting to characterize the human mind. But while most of them explain *some* phenomena, none of them explain *everything*, which leads to different specialists favoring different ontologies – and which is evidence that the correct framework is yet to be found.)\n\nThis definition could be applied iteratively: an ontology \\\\(H\\\\) would usually consist of a set of variables as well, and there could be a set of even-higher-level variables inducing independence between them. We could move from the description of reality in terms of “all elementary particles in existence” to “all atoms in existence”, and then, for example, to “all cells”, to “all organisms”, to “all species”. Or: “all humans” to “all cities” to “all countries”. Or: starting from a representation of a book in terms of individual sentences, we can compress it to the summary of its plot and themes; starting from the plots and themes of a set of books, we can derive common literary genres. Or: starting from a set of sensory experiences, we can discover some commonalities between these experiences, and conclude that there is some latent “object” depicted in all of them (such as compressing the visual experiences of seeing a tree from multiple angles into a “tree” abstraction). And so on.\n\nIn this formalism, we have two notable operations:\n\n1.  Deriving \\\\(H\\\\) given some \\\\(L\\\\).\n2.  Given some \\\\(L\\\\)*,* \\\\(H\\\\), and the relationship \\\\(P(L|H)\\\\), propagating some target state “up” or “down” the hierarchy of abstractions.\n    *   That is, if \\\\(H=H^*\\\\), what’s \\\\(P(L|H=H^*)\\\\)? Given some high-level state (macrostate), what’s the (distribution over) low-level states (microstates)?\n    *   On the flip side, if \\\\(L=L^*\\\\), what’s \\\\(P(H|L=L^*)\\\\)? Given some microstate, what macrostate does it correspond to?\n\nIt might be helpful to think of \\\\(P(L|H)\\\\) and \\\\(P(H|L)\\\\) as defining functions for abstracting down \\\\(H\\to L\\\\) and abstracting up \\\\(L\\to H\\\\), respectively, rather than as a probability distribution. Going forward, I will be using this convention.\n\nI would argue that (2) represents the kinds of thinking, including highly intelligent and sophisticated thinking, which *do not correspond to philosophical reasoning*. In that case, we already have \\\\(H\\to L\\\\) pre-computed, the ontology defined. The operations involved in propagating the state up/down might be rather complex, but they’re ultimately “closed-form” in a certain sense.\n\nSome prospective examples:\n\n*   Tracking the consequences of local political developments on the global economy (going “up”), or on the experiences of individual people (going “down”).\n*   Evaluating the geopolitical impact of a politician ingesting a specific poisonous substance at a specific time (going “up”).\n*   Modeling the global consequences of an asteroid impact while taking into account orbital dynamics, weather patterns, and chemical reactions (going “down” to physical details, then back “up”).\n*   Translating a high-level project specification to build a nuclear reactor into specific instructions to be carried out by manufacturers (“down”).\n*   Estimating the consequences of a specific fault in the reactor’s design on global policies towards nuclear power (“up”).\n\nAs per the examples, this kind of thinking very much encompasses some domains of research and engineering.\n\n(1), on the other hand, potentially represents *philosophical reasoning*. The question is: what specific cognitive algorithms are involved in that reasoning?\n\nIntuitively, some sort of “babble-and-prune” brute-force approach seems to be at play. We need to semi-randomly test various possible decompositions, until ultimately arriving at one that is actually robust. Another feature is that this sort of thinking requires a wealth of *concrete examples*, a “training set” we have to study to derive the right abstractions. (Which makes sense: we need a representative sample of the set of random variables \\\\(L\\\\) in order to derive approximate conditional-independence relations between them.)\n\nBut given that, empirically, the problem of philosophy is computationally tractable at all, it would seem that some heuristics are at play here as well. Whatever algorithms underlie philosophical reasoning, they’re able to narrow down the hypothesis space of ontologies that we have to consider.\n\nAnother relevant intuition: from a computational-complexity perspective, the philosophical reasoning of (1), in general, seems to be more demanding than the more formal non-philosophical thinking of (2). Philosophical reasoning seems to involve some iterative search-like procedures, whereas the “non-philosophical” thinking of (2) involves only “simpler” closed-form deterministic functions.\n\nThis fits with the empirical evidence: deriving a new useful model for representing some domain of reality is usually a task for entire fields of science, whereas *applying* a model is something any individual competent researcher or engineer is capable of.\n\n### 1C. Qualitative Research\n\nSuppose that the core cognitive processes underlying philosophy are indeed about deriving novel ontologies. Is the converse true: all situations in which we’re deriving some novel ontology are “philosophy-like” undertakings, in some important sense?\n\nI would suggest yes.\n\nLet’s consider the mechanistic-interpretability example from 1A. Mechanistic interpretability is a very concrete, down-to-earth field of study, with tight empirical-testing loops. Nevertheless, things like the Universality Hypothesis, and speculations that the computations in neural networks could be decomposed into “computational circuits”, certainly have a *philosophical* flavor to them – even if the relevant reasoning happens far outside the field of academic philosophy.\n\nChris Olah, a prominent ML interpretability researcher, [characterizes this as “qualitative research”](https://transformer-circuits.pub/2024/qualitative-essay/index.html). He points out that one of the telltale signs that this type of research is proceeding productively is finding *surprising structure* in your empirical results. In other words: finding some way to look at the data which hints at the underlying ontology.\n\nAnother common term for this type of research is “pre-paradigmatic” research. The field of agent foundations, for example, is often called “pre-paradigmatic” in the sense that within its context, we don’t know how to correctly phrase even the *questions* we want answered, nor the definitions of the basic features we want to focus on.\n\nSuch research processes are common even in domains that have long been decoupled from philosophy, such as physics. Various attempts to derive the Theory of Everything often involve grappling with very philosophy-like questions regarding the ontology of a physical universe consistent with all our experimental results (e. g., string theory). Different interpretations of quantum mechanics is an even more obvious example.\n\nThomas Kuhn's [*The Structure of Scientific Revolutions*](https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions) naturally deserves a mention here. His decomposition of scientific research into “paradigm shifts” and “normal science” would correspond to the split between (1) and (2) types of reasoning as outlined in the previous section. The research that fuels paradigm shifts would be of the “qualitative”, non-paradigmatic, ontology-discovering type.\n\nThings similar to qualitative/non-paradigmatic research also appear in the world of *business*. Peter Thiel’s [characterization](https://en.wikipedia.org/wiki/Zero_to_One) of startups as engaging in “zero to one” creation of qualitatively new markets or goods would seem to correspond to deriving some novel business frameworks, i. e., *ontologies*, and succeeding by their terms. (“Standard”, non-startup businesses, in this framework’s view, rely on more “formulaic” practices – i. e., on making use of already pre-computed \\\\(H\\to L\\\\). Consider opening a new steel mill, which would produce well-known products catering to well-known customers, vs. betting on a specific AI R&D paradigm, whose exact place in the market is impossible to predict even if it succeeds.)\n\nNevertheless, intuitively, there still seems to be *some* important difference between these “thin slices” of philosophical reasoning scattered across more concrete fields, and “pure” philosophy.\n\nBefore diving into this, a short digression:\n\n### 1D. Qualitative Discoveries Are Often Counterfactual\n\nSince non-paradigmatic research seems more computationally demanding, requiring a greater amount of expertise than in-paradigm reasoning, its results are often highly *counterfactual*. While more well-operationalized frontier discoveries are often made by many people near-simultaneously, highly qualitative discoveries could often be attributed to a select few people.\n\nAs a relatively practical example, [Shannon’s information theory](https://www.lesswrong.com/posts/csHstEPagqs8wChhh/examples-of-highly-counterfactual-discoveries) plausibly counts. The discussion through the link also offers some additional prospective examples.\n\nFrom the perspective of the “zero-to-one startups are founded on novel philosophical reasoning” idea, this view is also supported. If a novel startup fails due to some organizational issues before proving the profitability of its business plan, it’s not at all certain that it would be quickly replaced by someone trying the same idea, *even if* its plan were solid. Failures of the Efficient Market Hypothesis are common in this area.\n\n### 1E. What Is “Philosophy” As a Discipline?\n\nSuppose that the low-level system \\\\(L\\\\) represents some practical problem we study. A neural network that we have to interpret, or the readings yielded by a particle accelerator which narrow down the fundamental physical laws, or the behavior of some foreign culture that we want to trade with. Deriving the ontology \\\\(H\\\\) would be an instance of non-paradigmatic research, i. e., philosophical reasoning. But once \\\\(H\\\\) is derived, it would be relatively easily put to use solving practical problems. The relationship \\\\(H\\to L\\\\), once nailed down, would quickly be handed off to engineers or businessmen, who could start employing it to optimize the natural world.\n\nAs an example, consider [anthropics](https://www.lesswrong.com/tag/anthropics), an emerging field studying [anthropic principles](https://en.wikipedia.org/wiki/Anthropic_principle) and extended reasoning similar to [the doomsday argument](https://en.wikipedia.org/wiki/Doomsday_argument)[^d86aqnexdj9]. Anthropics doesn’t study a concrete practical problem. It’s a very high-level discipline, more or less abstracting over the-world-as-a-whole (or our experiences of it). Finding a proper formalization of anthropics, which satisfactorily handles all edge cases, would result in advancement in decision theory and probability theory. But there are no *immediate* practical applications.\n\nThey likely do exist. But you’d need to propagate the results *farther* down the hierarchy of abstractions, moving through these theories down to specific subfields and then to specific concrete applications. None of the needed \\\\(H\\to L\\\\) pathways are derived, there’s a lot of multi-level philosophical work to be done. And there’s always the possibility that it would yield no meaningful results, or end up as a very circuitous way to justify common intuitions.\n\nThe philosophy of mind could serve as a more traditional example. Branches of it are focused on investigating the nature of consciousness and qualia. Similarly, it’s a very “high-level” direction of study, and the success of its efforts would have significant implications for numerous other disciplines. But it’s not known what, if any, *practical* consequences of such a success would be.\n\nThose features, I think, characterize “philosophy” as a separate discipline. Philosophy (1) involves attempts to derive wholly new multi-level disciplines, starting from very-high-level reasoning about the-world-as-a-whole (or, at least, drawing on several disciplines at once), and (2) it only cashes out in practical implementations after several iterations of concretization.\n\nIn other words, philosophy is the continuing effort to derive the complete *highest-level* ontology of our experiences/our world.\n\n### 1F. On Ethics\n\nAn important branch of philosophy which hasn't been discussed so far is moral philosophy. The previously outlined ideas generalize to it in a mostly straightforward manner, though with a specific “pre-processing” twist.\n\nThis is necessarily going to be a very compressed summary. For proper treatment of the question, I recommend Steven Byrnes’ series on [the human brain](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8) and [valence signals](https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9), or my (admittedly fairly outdated) [essay on value formation](https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model).\n\nTo start off, let’s assume that the historical *starting point* of moral philosophy are human moral intuitions and feelings. Which actions, goals, or people “feel like” good or bad things, what seems just or unfair, and so on. From this starting point, people developed the notions of morality and ethics, ethical systems, social norms, laws, and explicit value systems and ideologies.\n\nThe process of moral philosophy can then be characterized as follows:\n\nAs a premise, human brains contain learning algorithms plus a suite of reinforcement-learning training signals.\n\nIn the course of life, and especially in childhood, a human learns a vast repository of value functions. These functions take sensory perceptions and thoughts as inputs, and output “valence signals” in the form of real numbers. The valence assigned to a thought is based on learned predictive heuristics about whether a given type of thought has historically led to positive or negative reward (as historically scored by innate reinforcement-signal functions).\n\nThe valences are *perceived by* human minds as a type of sensory input. In particular, a subset of learned value functions could be characterized as “moral” value functions, and their outputs are perceived by humans as the aforementioned feelings of “good”, “bad”, “justice”, and so on.\n\nImportantly, the learned value functions aren’t part of a human’s learned *world-model* ([explicit knowledge](https://en.wikipedia.org/wiki/Explicit_knowledge)). As the result, their explicit definitions aren’t immediately available to our conscious inspection. They’re “black boxes”: we only perceive their outputs.\n\nOne aspect of moral philosophy, thus, is to *recover* these explicit definitions: what value functions you’ve learned and what abstract concepts they’re “attached to”. (For example: does “stealing” feel bad because you think it’s unfair, or because you fear being caught? You can investigate this by, for example, imagining situations in which you manage to steal something in circumstances where you feel confident you won’t get caught. This would allow you to remove the influence of “fear of punishment”, and thereby determine whether you have a fairness-related value function.)\n\nThat is a type of philosophical reasoning: an attempt to “abstract up” from a set of sensory experiences of a specific modality, to a function defined over high-level concepts. (Similar to recovering a “tree” abstraction by abstracting up from a set of observations of a tree from multiple angles.)\n\nBuilding on that, once a human has recovered (some of) their learned value functions, they can keep abstracting up in the manner described in the preceding text. For example, a set of values like “I don’t like to steal”, “I don’t like to kill”, “I don’t like making people cry” could be abstracted up to “I don’t want to hurt people”.\n\nBuilding up further, we can abstract over the set of value systems recovered by *different *people, and derive e. g. the values of a society…\n\n… and, ultimately, “human values” as a whole.\n\nAdmittedly, there are some obvious complications here, such as the need to handle value conflicts / inconsistent values, and sometimes making the deliberate choice to discard various data points in the process of computing higher-level values (often on the basis of *meta-value* functions). For example, not accounting for violent criminals when computing the values a society wants to strive for, or discarding violent impulses when making decisions about what kind of person you want to be.\n\nIn other words: when it comes to values, there is an “ought” sneaking into the process of abstracting-up, whereas in all other cases, it’s a purely “is”-fueled process.\n\nBut the “ought” side of it can be viewed as simply making decisions about what data to put in the \\\\(L\\\\)set, which we’d then abstract over in the usual, purely descriptive fashion.\n\nFrom this, I conclude that the basic algorithmic machinery, especially one underlying the *philosophical* (rather than the *political*) aspects of ethical reasoning, is still the same as with all other kinds of philosophical reasoning.\n\n### 1G. Why Do “Solved” Philosophical Problems Stop Being Philosophy?\n\nAs per the formulations above:\n\n*   The endeavor we intuitively view as “philosophy” is a specific subset of general philosophical reasoning/non-paradigmatic research. It involves thinking about the world in a very general sense, *without* the philosophical assumptions that decompose it into separate domains of study.\n*   “Solving” a philosophical problem involves deriving an ontology/paradigm for some domain of reality, which allows to decouple that domain from the rest of the world and study it mostly separately.\n\nPut like this, it seems natural that philosophical successes move domains outside the remit of philosophy. Once a domain has been delineated, thinking about it *by definition* no longer requires the interdisciplinary reasoning characteristic of philosophy-as-a-discipline. Philosophical reasoning seeks to render itself unnecessary.\n\n(As per the previous sections, working in the domains thus delineated could still involve qualitative research, i. e., philosophical reasoning. But not the specific *subtype* of philosophical reasoning characteristic of philosophy-as-a-discipline, involving reasoning about the-world-as-a-whole.)\n\nIn turn, this separation allows specialization. Newcomers could focus their research and education on the delineated domain, *without* having to become interdisciplinary specialists. This means a larger quantity of people could devote themselves to it, leading to faster progress.\n\nThat dynamic is also bolstered by greater funding. Once the practical implications of a domain become clear, more money pours into it, attracting even *more* people.\n\nAs a *very* concrete example, we can consider [the path-expansion trick](https://transformer-circuits.pub/2021/framework/index.html#onel-path-expansion) in mechanistic interpretability. Figuring out how to mathematically decompose a one-layer transformer into the OV and QK circuits requires high-level reasoning about transformer architecture, and arriving at the very *idea* of trying to do so requires philosophy-like thinking (to even think to ask, “how can we decompose a ML model into separate building blocks?”). But once this decomposition has been determined, each of these circuits could be studied separately, including by people who don’t have the expertise to derive the decomposition from scratch.\n\nSolving a philosophical problem, then, often allows to greatly upscale the amount of work done in the relevant domain of reality. Sometimes, that quickly turns it into an industry.\n\n2\\. Wisdom\n----------\n\nLet’s consider a wide variety of “wise” behavior or thinking.\n\n1.  Taking into account “second-order” effects of your actions.\n    *   **Example:** [The “transplant problem”](https://simple.wikipedia.org/wiki/Trolley_problem#Emergency_room_case), which examines whether you should cut up a healthy non-consenting person for organs if that would let you save five people whose organs are failing.\n    *   “Smart-but-unwise” reasoning does some math and bites the bullet.\n    *   “Wise” reasoning points out that if medical professionals engaged in this sort of behavior at scale, people would stop seeking medical attention out of fear/distrust, leading to more suffering in the long run.\n2.  Taking into account your history with specific decisions, and updating accordingly.\n    *   **Example 1:**\n        *   Suppose you have an early appointment tomorrow, but you’re staying up late, engrossed in a book. Reasoning that you will read “just one more chapter” might seem sensible: going to sleep at 01:00 AM vs. 01:15 AM would likely have no significant impact on your future wakefulness.\n        *   However, suppose that you end up making this decision repeatedly, until it’s 6:40 AM and you have barely any time left for sleep at all.\n        *   Now suppose that a week later, you’re in a similar situation: it’s 01:00 AM, you’ll need to wake up early, and you’re reading a book.\n        *   “Smart-but-unwise” reasoning would repeat your previous mistake: it’d argue that going to sleep fifteen minutes later is fine.\n        *   “Wise” reasoning would update on the previous mistake, know not to trust its object-level estimates, and go to sleep immediately.\n    *   **Example 2:**\n        *   Suppose that someone did something very offensive to you. In the moment, you infer that this means they hate you, and update your beliefs accordingly.\n        *   Later, it turns out they weren’t aware that their actions upset you, and they apologize and never repeat that error.\n        *   Next time someone offends you, you may consider it “wise” not to trust your instinctive interpretation *completely*, and at least consider alternate explanations.\n3.  Taking into account the impact of the fact that you’re the sort of person to make a specific decision in a specific situation.\n    *   [**Example 1**](https://www.lesswrong.com/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes):\n        *   Suppose that a staunch pacifist is offered a deal: they take a pill that would increase their willingness to kill by 1%, and in exchange, they get 1 million dollars. In addition, they could take that deal multiple times, getting an additional 1 million dollars each time, and raising their willingness to kill by 1% each time.\n        *   A “smart-but-unwise” pacifist reasons that they’d still be unwilling to kill even if they became, say, 10% more willing to, and that they could spend the 10 million dollars on charitable causes, so they decide to take the deal 10 times.\n        *   A “wise” pacifist might consider the fact that, if they take the deal 10 times, the one making the decision on whether to *continue* would be a 10%-more-willing-to-kill version of them. That version might consider it acceptable to go up to 20%; a 20% version might consider 40% acceptable, and so on until 100%.\n    *   **Example 2:** Blackmailability.\n        *   Suppose that we have two people, Alice and Carol. Alice is known as a reasonable, measured person who makes decisions carefully, minimizing risk. Carol is known as a very temperamental person who becomes enraged and irrationally violent at the slightest offense.\n        *   Suppose that you’re a criminal who wants to blackmail someone. If you’re choosing between Alice and Carol, Alice is a much better target: if you threaten to ruin her life if she doesn’t pay you $10,000, she will tally up the costs and concede. Carol, on the other hand, might see red and attempt to murder you, even if that seals her own fate.\n        *   Alice is “smart-but-unwise”. Carol, as stated, isn’t exactly “wise”. But she becomes “wise” under one provision: if she committed to her “irrational” decision policy as a result of rational reasoning about what would make her an unappealing blackmail target. After all, in this setup, she’s certainly the one who ends up better off than Alice!\n        *   ([Functional Decision Theories](https://en.wikipedia.org/wiki/Functional_Decision_Theory) attempt to formalize this type of reasoning, providing a framework within which it’s strictly rational.)\n4.  Erring on the side of deferring to common sense in situations where you think you see an unexploited opportunity.\n    *   **Example 1:** Engaging in immoral behavior based on some highly convoluted consequentialist reasoning vs. avoiding deontology violations. See [this article](https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans) for an extended discussion of the topic.\n        *   This is similar to (1), but in this case, you don’t need to reason through the *n*th-order effects “manually”. You know that deferring to common sense is *usually* wise, even if you don’t know why the common sense is the way it is.\n        *   It’s also fairly similar to the first example in (3), but the setup here is much more realistic.\n    *   **Example 2:** Trying to estimate the price of a stock “from scratch”, vs. “[zeroing out](https://thezvi.wordpress.com/2017/11/05/zeroing-out/)”, i. e., taking the market value as the baseline and then updating it up/down based on whatever special information you have.\n    *   **Example 3:** Getting “bad vibes” from a specific workplace environment or group of people, and dismissing these feelings as irrational (“smart-but-unwise”), vs. trying to investigate in-depth what caused them (“wise”). (And discovering, for example, that they were caused by some subtle symptoms of unhealthy social dynamics, which the global culture taught you to spot, but didn’t explain the meaning of.)\n5.  Taking the “outside view” into account (in some situations in which it’s appropriate).\n    *   **Example:** Being completely convinced of your revolutionary new physics theory or business plan, vs. being excited by it, but skeptical on the meta level, on the reasoning that there’s a decent chance your object-level derivations/plans contain an error.\n\n**Summing up:** All examples of “wise” behavior here involve (1) generating some candidate plan or inference, which seems reliable or correct while you’re evaluating it using your object-level heuristics, then (2) looking at the appropriate *reference class* of these plans/inferences, and finally (3) predicting what the *actual* consequences/accuracy would be using your *meta-level* heuristics. (“What if everyone acted this way?”, “what happened the previous times I acted/thought this way?”, “what would happen if it were commonly known I’d act this way?”, “if this is so easy, why haven’t others done this already?”, and so on.)\n\nNaturally, it could go even higher. First-order “wise” reasoning might be unwise from a meta-meta-level perspective, and so on.\n\n(For example, “outside-view” reasoning is often overused, and an even more wise kind of reasoning recognizes when inside-view considerations legitimately prevail over outside-view ones. Similarly, the heuristic of “the market is efficient and I can’t beat it” is usually wise, wiser than “my uncle beat the market this one time, which means I can too if I’m clever enough!”, but sometimes there *are* legitimate market failures[^hvk115s6nxv].)\n\nIn other words: “wise” thinking seems to be a two-step process, where you first generate a conclusion that you expect to be accurate, then “go meta”, and predict what would be the *actual* accuracy rate of a decision procedure that predicts this sort of conclusion to be accurate.\n\n### 2A. Background Formalisms\n\nTo start off, I will need to introduce a toy model of agency. Bear with me.\n\n**First: How can we model the inferences from the *****inputs*** **to an agent’s decisions?**\n\nPhotons hit our eyes. Our brains draw an image aggregating the information each photon gives us. We interpret this image, decomposing it into objects, and inferring which latent-variable object is responsible for generating which part of the image. Then we wonder further: what process generated each of these objects? For example, if one of the \"objects\" is a news article, what is it talking about? Who wrote it? What events is it trying to capture? What set these events into motion? And so on.\n\nIn diagram format, we're doing something like this:\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdZKKLx0R4wM03_JaQ7zYXyg6Mrqo071EyEAWb3Q9NuPB84CbEl-_4O4AKC4hDLcXkmChZd-WwN_mNFhhxd9gysVrXkGIeKjvFNe6398IwmZcsj5yw1NIlcxcGg9Z2TOenoVofEuXgq_ebFmhr2lgEpfQxW?key=ogmKpL6fBJgn1Y18NUTnww)\n\n*Blue are ground-truth variables, gray is the \"Cartesian boundary\" of our mind from which we read off observations, purple are nodes in our world-model, each of which can be mapped to a ground-truth variable.*\n\nWe take in observations, infer what latent variables generated them, then infer what generated those variables, and so on. We go backwards: from effects to causes, iteratively. The Cartesian boundary of our input can be viewed as a \"mirror\" of a sort, reflecting *the Past*.\n\nIt's a bit messier in practice, of course. There are shortcuts, ways to map immediate observations to far-off states. But the general idea mostly checks out – especially given that these \"shortcuts\" probably still *implicitly* route through all the intermediate variables, just without explicitly computing them. (You can map a news article to the events it's describing without explicitly modeling the intermediary steps of witnesses, journalists, editing, and publishing. But your mapping function is still implicitly shaped by the known quirks of those intermediaries.)\n\n**Second: Let’s now consider the “output side” of an agent**. I. e., what happens when we're planning to achieve some goal, in a consequentialist-like manner.\n\nWe envision the target state. What we want to achieve, how the world would look like. Then we ask ourselves: what would cause this? What forces could influence the outcome to align with our desires? And then: how do we control these forces? What actions would we need to take in order to make the network of causes and effects steer the world towards our desires?\n\nIn diagram format, we're doing something like this:\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcyMnUN749vb4EI6PitCssWW0-tjunY0WOD_n9oxuOVVyeOX51a3lfsiabrd-0ANfqmFT1WLH-Xr-POga2tRy_VAM99rvpPF83cN85zG-57l7hbqgozz88wWeoE-tPrbLtIXLBQSMu4C26bBmtVRDfu3Qic?key=ogmKpL6fBJgn1Y18NUTnww)\n\n*Green are goals, purple are intermediary variables we compute, gray is the Cartesian boundary of our actions, red are ground-truth variables through which we influence our target variables.*\n\nWe start from our goals, infer what latent variables control their state in the real world, then infer what controls those latent variables, and so on. We go backwards: from effects to causes, iteratively, until getting to our own actions. The Cartesian boundary of our output can be viewed as a \"mirror\" of a sort, reflecting *the Future*.\n\nIt's a bit messier in practice, of course. There are shortcuts, ways to map far-off goals to immediate actions. But the general idea mostly checks out – especially given that these heuristics probably still *implicitly* route through all the intermediate variables, just without explicitly computing them. (\"Acquire resources\" is a good heuristical starting point for basically any plan. But what *counts as* resources is something you had to figure out in the first place by mapping from \"what lets me achieve goals in this environment?\".)\n\nAnd indeed, that side of this formulation isn't novel. From [this post](https://www.lesswrong.com/posts/gEKHX8WKrXGM4roRC/saving-time#Why_Time_) by Scott Garrabrant, an agent-foundations researcher:\n\n> Time is also crucial for thinking about agency. My best short-phrase definition of agency is that agency is time travel. An agent is a mechanism through which the future is able to affect the past. An agent models the future consequences of its actions, and chooses actions on the basis of those consequences. In that sense, the consequence causes the action, in spite of the fact that the action comes earlier in the standard physical sense.\n\n**Let’s now put both sides together**. An idealized, compute-unbounded \"agent\" could be laid out in this manner:\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXccsXVa0-cPYk7qeUiKkrUwZdr2gtNCeXP1rkyBIYqANjR7U_lCEf-AVD_xjXLwJulEHH4xoDQddbAUKRCCMlGAVia8OC5cYZkfAsNPzmapsFPYzeOcSR4SaFCQNVR5kPFDBVBkJXjZ_8KEKOfQuC8ZESsq?key=ogmKpL6fBJgn1Y18NUTnww)\n\nIt reflects the past at the input side, and reflects the future at the output side. In the middle, there's some \"glue\"/\"bridge\" connecting the past and the future by a forwards-simulation. During that, the agent \"catches up to the present\": figures out what will happen *while* it's figuring out what to do.\n\nIf we consider [the relation between utility functions and probability distributions](https://www.lesswrong.com/posts/voLHQgNncnjjgAPH7/utility-maximization-description-length-minimization), it gets even more formally literal. An utility function over *X* could be viewed as a target probability distribution over *X*, and maximizing expected utility is equivalent to minimizing cross-entropy between this target distribution and the real distribution.\n\nThat brings the \"planning\" process in alignment with the \"inference\" process: both are about propagating target distributions \"backwards\" in time through the network of causality.\n\n### 2B. Tentative Formalization\n\nLet’s consider what definition “wisdom” would have, in this framework.\n\nAll “object-level” cognitive heuristics here have a form of \\\\(Y\\to X\\\\), where \\\\(Y\\\\) is some environmental variable, and \\\\(X\\\\) are the variables that cause \\\\(Y\\\\). I. e., every cognitive heuristic \\\\(Y\\to X\\\\) can be characterized as an inversion of some environmental dynamic \\\\(X\\to Y\\\\).\n\n“Wisdom”, in this formulation, seems to correspond to *inversions of inversions*. Its form is\n\n\\\\\\[(Y\\to X)\\to Y.\\\\\\]\n\nIt takes in some object-level inversion – an object-level cognitive heuristic – and predicts things about *the performance of a cognitive policy that uses this heuristic*.\n\nExamining this definition from both ends:\n\n*   If we’re considering an object-level output-side heuristic \\\\(E\\to A\\\\)*,* which maps environmental variables \\\\(E\\\\) to actions \\\\(A\\\\) that need to be executed in order to set \\\\(E\\\\) to specific values – i. e., a “planning” heuristic – the corresponding “wisdom” heuristic \\\\((E\\to A)\\to E\\\\) tells us what object-level consequences \\\\(E\\\\) the reasoning of this type *actually* results in.\n*   If we’re considering an object-level input-side heuristic \\\\(O\\to E\\\\) mapping observations \\\\(O\\\\) to their environmental causes \\\\(E\\\\) – i. e., an “inference” heuristic – the corresponding “wisdom” heuristic \\\\((O\\to E)\\to O\\\\) tells us what we’d *actually* expect to see going forward, and whether the new observations would diverge from our object-level inferences. (I. e., whether we expect that the person who offended us would *actually* start acting like they hate us, going forward.)\n\nAdmittedly, some of these speculations are fairly shaky. The “input-side” model of wisdom, in particular, seems off to me. Nevertheless, I think this toy formalism does make some intuitive sense.\n\nIt’s also clear, from this perspective, why “wisdom” is inherently more complicated / hard-to-compute than “normal” reasoning: it explicitly *iterates on* object-level reasoning.\n\n### 2C. Crystallized Wisdom\n\nIn humans, cognitive heuristics are often not part of explicit knowledge, but are instead stored as learned instincts, patterns of behavior, or emotional responses – or “[shards](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT)”, in the parlance of one popular framework.\n\nSince wisdom is a subset of cognitive heuristics, that applies to it as well. “Wise” heuristics are often part of “common sense”, tacit knowledge, cultural norms, and hard-to-articulate intuitions and hunches. In some circumstances, they’re stored in a format that doesn’t refer to the initial object-level heuristic at all! Heuristics such as “don’t violate deontology” don’t activate *only* in response to object-level criminal plans.\n\n(Essentially, wisdom is conceptually/[logically](https://www.lesswrong.com/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games#Logical_Time) downstream of object-level heuristics, but not necessarily *cognitively downstream, *in the sense of moment-to-moment perceived mental experiences.)\n\nIndeed, wisdom heuristics, by virtue of being more computationally demanding, are likely to be stored in an “implicit” form *more often* than “object-level” heuristics. Deriving them explicitly often requires looking at “global” properties of the environment or your history in it, considering the whole reference class of the relevant object-level cognitive heuristic. By contrast, object-level heuristics themselves involve a merely “local” inversion of some environmental dynamic.\n\nAs the result, “wisdom” usually only accumulates after humanity has engaged with some domain of reality for a while. Similarly, individual people tend to become “wise” only after they personally were submerged in that domain for a while – after they “had some experience” with it.\n\nThat said, to the extent that this model of wisdom is correct, wisdom *can* nevertheless be inferred “manually”, with enough effort. After all, it’s still merely a function of the object-level domain. It *could* be derived purely from the domain’s object-level model, given enough effort and computational resources, no “practical experience” needed.\n\n3\\. Would AGIs Pursue Wisdom & Philosophical Competence?\n--------------------------------------------------------\n\nIn my view, the answer is a clear “yes”.\n\nTo start off, let’s define an “AGI” as “a system which can discover novel abstractions (such as new fields of science) in any environment that has them, and fluently use these abstractions in order to better navigate or optimize its environment in the pursuit of its goals”.\n\nIt’s somewhat at odds with the more standard definitions, which tend to characterize AGIs as, for example, “systems that can do most cognitive tasks that a human can”. But I think it captures some intuitions better than the standard definitions. For one, the state-of-the-art LLMs certainly seem to be “capable of doing most cognitive tasks that humans can”, yet most specialists and laymen alike would agree that they are not AGI. Per my definition, it’s because LLMs cannot discover *new* ontologies: they merely learned vast repositories of abstractions that were pre-computed for them by humans.\n\nAs per my arguments, **philosophical reasoning is convergent:**\n\n*   It’s a subset of general non-paradigmatic research...\n*   … which is the process of deriving new ontologies…\n*   … which are useful because they allow to decompose the world into domains that can be reasoned about mostly-separately…\n*   … which is useful because it reduces the computational costs needed for making plans or inferences.\n\nAny efficient bounded agent, thus, would necessarily become a competent philosopher, and it would engage in philosophical reasoning regarding all domains of reality that (directly or indirectly) concern it.\n\nConsider the opposite: “philosophically incompetent” or incapable reasoners. Such reasoners would only be able to make use of pre-computed \\\\(H\\to L\\\\) relations. They would not be able to derive genuinely *new* abstractions and create *new* fields. Thus, they wouldn’t classify as “AGI” in the above-defined sense.\n\nThey’d be mundane, *non-general* software tools. They’d still be able to be quite complex and intelligent, in some ways, up to and including being able to write graduate-level essays or even complete formulaic engineering projects. Nevertheless, they’d fall short of the “AGI” bar. (And would likely represent no existential risk on their own, outside cases of misuse by human actors.)\n\nAs a specific edge case, we can consider humans who are capable researchers in their domain – including being able to derive novel ontologies – but are still philosophically incompetent in a broad sense. I’d argue that this corresponds to the split between “general” philosophical reasoning, and “philosophy as a discipline” I’ve discussed in 1E. These people likely *could* be capable philosophers, but simply have no interest in specializing in high-level reasoning about the-world-in-general, nor in exploring its highest-level ontology.\n\nSomething similar *could* happen with AGIs trained/designed a specific way. But in the limit of superintelligence, it seems likely that *all* generally intelligent minds converge to being philosophically competent.\n\n**Wisdom** is also convergent. When it comes down to it, wisdom seems to just be an additional trick for making correct plans or inferences. “Smart but unwise” reasoning would correspond to cases in which you’re not skeptical of your own decision-making procedures, are mostly not trying to improve them, and only take immediate/local consequences of your action into account. Inasmuch as AGIs would be capable of long-term planning across many domains, they would strive to be “wise”, in the sense I’ve outlined in this essay.\n\nAnd those AGIs that would have superhuman general-intelligence capabilities, would be able to derive the “wisdom” heuristics *quicker* than humans, with little or no practical experience in a domain.\n\n4\\. Philosophically Incompetent Human Decision-Makers\n-----------------------------------------------------\n\nThat said, just because AGIs would be philosophically competent, that doesn’t mean they’d by-default address and fix the philosophical incompetence of the humans who created them. *Even if* these AGIs would be otherwise aligned to human intentions and inclined to follow human commands.\n\nThe main difficulty here is that humans store their values in a decompiled/incomplete format. We don’t have explicit utility functions: our values are a combination of explicit consciously-derived preferences, implicit preferences, emotions, subconscious urges, and so on. (Theoretically, [it may be possible](https://www.lesswrong.com/posts/okkEaevbXCSusBoE2/how-would-an-utopia-maximizer-look-like) to compile all of that into a utility function, but that’s a very open problem.)\n\nAs the result, mere *intent alignment* – designing an AGI which would do what its human operators “genuinely want” it to do, when they give it some command – still leaves a lot of philosophical difficulties and free parameters.\n\nFor example, suppose the AGI's operators, in a moment of excitement after they activate their AGI for the first time, tell it to solve world hunger. What should the AGI do?\n\n*   Should it read off the surface-level momentary intent of this command, design some sort of highly nutritious and easy-to-produce food, and distribute it across the planet in the specific way the human is currently imagining this?\n*   Should it extrapolate the human's values, and execute the command the way the human *would have wanted to* execute it if they'd thought about it for a bit, rather than the way they're envisioning it in the moment?\n    *   (For example, perhaps the image flashing through the human's mind right now is of helicopters literally dropping crates full of food near famished people, but it's actually more efficient to do it using airplanes.)\n*   Should it extrapolate the human's values a bit, and point out specific issues with this plan that the human might think about later (e. g., that such sudden large-scale activity might provoke rash actions from various geopolitical actors, leading to vast suffering), then give the human a chance to abort?\n*   Should it extrapolate the human's values a bit further, and point out issues the human might *not* have thought of (including teaching the human any novel load-bearing concepts necessary for understanding said potential issues)?\n*   Should it extrapolate the human's values a bit further still, and teach them various better cognitive protocols for self-reflection, so that they may better evaluate whether a given plan satisfies their values?\n*   Should it extrapolate the human's values *far afield*, interpret the command as \"maximize eudaimonia\", and do that, disregarding the specific rough way of how they gestured at the idea?\n    *   In other words: should it directly optimize for the human’s [coherent extrapolated volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition) (which is something like the ultimate output of abstracting-over-ethics that I’d gestured at in 1F)?\n*   Should it remind the human that they'd wanted to be careful regarding how they use the AGI, and to clarify whether they actually want to proceed with something so high-impact right now?\n*   Should it *insist* that the human is currently too philosophically confused to make such high-impact decisions, and the AGI first needs to teach them a lot of novel concepts, before they can be sure there are no unknown unknowns that’d put their current plans at odds with their extrapolated values?\n\nThere are many, many drastically different ways to implement something as seemingly intuitive as “Do What I Mean”. And unless “aligning AIs to human intent” is done in the specific way that puts as much emphasis as possible on philosophical competence, including *refusing* human commands if the AGI judges them unwise/philosophically incompetent – short of that, even an AGI that is intent-aligned (in some strict technical sense) might lead to existentially catastrophic outcomes, up to and including the possibility of [suffering at astronomical scales](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks).\n\nFor example, suppose the AGI is designed to act on the surface-level meaning of commands, and it’s told to “earn as much money as possible, by any means necessary”. As I’ve argued in Section 3, it *would* derive a wise and philosophically competent understanding of what “obeying the surface-level meaning of a human’s command” means, and how to wisely and philosophically competently execute on this specific command. But it would not question *the wisdom and philosophical competence of the command from the perspective of a counterfactual wiser human*. Why would it, unless specifically designed to?\n\nAnother example: If the AGI is “left to its own devices” regarding how to execute on some concrete goal, it’d likely do everything “correctly” regarding certain philosophically-novel-to-us situations, such as the hypothetical possibility of [acausal trade](https://www.lesswrong.com/tag/acausal-trade) with the rest of the multiverse. (If [the universal prior is malign](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/), and using it is a bad idea, an actual AGI would just use something else.) However, if the AGI is [corrigible](https://www.lesswrong.com/tag/corrigibility), and it explains the situation to a philosophically incompetent human operator before taking any action, *the human* might incorrectly decide that giving in to acausal blackmail is the correct thing to do, and order the AGI to do so.\n\nOn top of that, there’s a certain Catch-22 at play. Convincing the decision-makers or engineers that the AGI must be designed such that it’d only accept commands from wise philosophically competent people *already* requires some level of philosophical competence on the designers’ part. They’d need to know that there even *are* philosophical “unknown unknowns” that they must be wary of, and that faithfully interpreting human commands is more complicated than just reading off the human’s intent at the time they give the command.\n\nHow to arrive at that state of affairs is an open question.\n\n5\\. Ecosystem-Building\n----------------------\n\nAs argued in 1G, the best way to upscale the process of attaining philosophical competence and teaching it to people would be to *move metaphilosophy outside the domain of philosophy*. Figure out the ontology suitable for robustly describing any and all kinds of philosophical reasoning, and decouple from the rest of reality.\n\nThis would:\n\n*   **Allow more people to specialize in metaphilosophy**, since they’d only need to learn about this specific domain of reality, rather than becoming interdisciplinary experts reasoning about the world at a high level.\n*   **Simplify the transfer of knowledge and the process of training new people**. Once we have a solid model of metaphilosophy, that’d give us a ground-truth idea of how to translate philosophical projects into concrete steps of actions (i. e., what the \\\\(H\\to L\\\\) functions are). Those could be more easily taught in a standardized format, allowing at-scale teaching and at-scale delegation of project management.\n*   **Give us the means to measure philosophical successes and failures**, and therefore, how to steer philosophical projects and keep them on-track. (Which, again, would allow us to scale the size and number of such projects. How well they perform would become *legible*, giving us the ability to optimize for that clear metric.)\n*   **Provide legibility in general.** Once we have a concrete, convergent idea of what philosophical projects are, how they succeed, and what their benefits are, we’d be able to more easily argue the importance of this agenda to other people and organizations, increasing the agenda’s reach and attracting funding.\n\nHopefully this essay and the formalisms in it provide the starting point for operationalizing metaphilosophy in a way suitable for scaling it up.\n\nSimilar goes for wisdom – although unlike teaching philosophical competence, this area seems less neglected. (Large-scale projects for “[raising the sanity waterline](https://www.lesswrong.com/posts/XqmjdBKa4ZaXJtNmf/raising-the-sanity-waterline)” have been attempted in the past, and I think any hypothetical “wisdom-boosting” project would look more or less the same.)\n\n6\\. Philosophy Automation\n-------------------------\n\nIn my view, automating philosophical reasoning is an AGI-complete problem. I think that the ability to engage in qualitative/non-paradigmatic research is what *defines* a mind as generally intelligent.\n\nThis is why LLMs, for example, are so persistently [bad at it](https://www.lesswrong.com/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority#What_is_wrong_with_current_models_), despite their decent competence in other cognitive areas. I would argue that LLMs contain [vast amounts of crystallized heuristics](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment#6__The_Case_of_LLMs) – that is, \\\\(H\\to L\\\\) functions, in this essay’s terminology – yet no ability to derive new ontologies/abstractions \\\\(H\\\\)given a low-level system \\\\(L\\\\). Thus, there are *no* types of philosophical reasoning they’d be good at; no ability to contribute on their own/autonomously.\n\nOn top of that, since we ourselves don’t know the ontology of metaphilosophy either, that likely cripples our ability to use AI tools for philosophy *in general*. The reason is the same as the barrier to scaling up philosophical projects: we don’t know how the domain of metaphilosophy factorizes, which means we don’t know how to [competently outsource](https://www.lesswrong.com/posts/3gAccKDW6nRKFumpP/why-not-just-outsource-alignment-research-to-an-ai) philosophical projects and sub-projects, how to train AIs specialized in this, and how to measure their successes or failures.\n\nOne approach that *might* work is “cyborgism”, as [defined by janus](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism). Essentially, it uses LLMs as a brainstorming tool, allowing to scope out vastly larger regions of concept-space for philosophical insights, with the LLMs’ thought-processes steered by a human. In theory, this gives us the best of both worlds: a human’s philosophy-capable algorithms are enhanced by the vast repository of crystallized \\\\(H\\to L\\\\) and \\\\(L\\to H\\\\)functions contained within the LLM. Janus has been able to generate some [coherent-ish philosophical artefacts](https://www.lesswrong.com/posts/vPsupipfyeDoSAirY/language-ex-machina) this way. However, this idea has been around for a while, and so far, I haven’t seen any payoff from it.\n\nOverall, I’m very skeptical that LLMs could be of any help here whatsoever, besides their standard mundane-utility role of teaching people new concepts in a user-tailored format. (Which might be helpful, in fact, but it isn’t the main bottleneck here. As I’ve discussed in Section 5, this sort of at-scale distribution of standardized knowledge only becomes possible *after* the high-level ontology of what we want to teach is nailed down.)\n\nWhat *does* offer some hope for automating philosophy is the research agenda focused on the Natural Abstraction Hypothesis. I’ve discussed it above, and my tentative operationalization of philosophy is based on it. The agenda is focused on finding a formal definition for abstractions (i. e., layers of ontology), and what algorithms could at least *assist us* with deriving new ones.\n\nThus, inasmuch as my model of philosophy is right, the NAH agenda is precisely focused on operationalizing philosophical reasoning. John Wentworth additionally discusses some of the NAH’s applications for metaphilosophy [here](https://www.lesswrong.com/posts/HfqbjwpAEGep9mHhc/the-plan-2023-version#How_is_abstraction_a_bottleneck_to_metaphilosophy_).\n\n* * *\n\n*Thanks to David Manley, Linh Chi Nguyen, and Bradford Saad for providing extensive helpful critique of an earlier draft, and to John Wentworth for proofreading the final version.*\n\n[^rx2itcvi3te]: A niche field closely tied to AI research, which attempts to formalize the notion of generally intelligent agents capable of pursuing coherent goals across different contexts, domains, and time scales. \n\n[^okelgzd32wl]: Which was, itself, made independent from the rest of reality by assumptions produced by a higher-level instance of philosophical reasoning. \n\n[^r6h6zfmmwt]: The generalization of the framework explicitly able to handle approximation could be found through this link. \n\n[^a908577wp1c]: In theory, there might be some free parameters regarding the exact representation an agent would choose, if there are several possible representations with the same size and predictive accuracy. Efforts to show that any two such representations would be importantly isomorphic to each other are ongoing. \n\n[^d86aqnexdj9]: Given that we’re able to make a particular observation, what does that tell us about the structure of reality? For example, the fact that intelligent life exists at all already narrows down what laws of physics our universe must have. We can infer something about them purely from observing our own existence, without actually looking around and directly studying them. \n\n[^hvk115s6nxv]: Eliezer Yudkowsky’s Inadequate Equilibria discusses this topic in great detail.",
      "plaintextDescription": "Written for the competition on the Automation of Wisdom and Philosophy.\n\n----------------------------------------\n\n\nSummary\nPhilosophy and wisdom, and the processes underlying them, currently lack a proper operationalization: a set of robust formal or semi-formal definitions. If such definitions were found, they could be used as the foundation for a strong methodological framework. Such a framework would provide clear guidelines for how to engage in high-quality philosophical/wise reasoning and how to evaluate whether a given attempt at philosophy or wisdom was a success or a failure.\n\nTo address that, I provide candidate definitions for philosophy and wisdom, relate them to intuitive examples of philosophical and wise reasoning, and offer a tentative formalization of both concepts. The motivation for this is my belief that the lack of proper operationalization is the main obstacle to both (1) scaling up the work done in these domains (i. e., creating a bigger ecosystem that would naturally attract funding), and (2) automating them.\n\nThe discussion of philosophy focuses on the tentative formalization of a specific algorithm that I believe is central to philosophical thinking: the algorithm that allows humans to derive novel ontologies (conceptual schemes). Defined in a more fine-grained manner, the function of that algorithm is “deriving a set of assumptions using which a domain of reality could be decomposed into subdomains that could be studied separately”.\n\nI point out the similarity of this definition to John Wentworth’s operationalization of natural abstractions, from which I build the formal model.\n\nFrom this foundation, I discuss the discipline of philosophy more broadly. I point out instances where humans seem to employ the “algorithm of philosophical reasoning”, but which don’t fall under the standard definition of “philosophy”. In particular, I discuss the category of research tasks varyingly called “qualitative” or “non-paradigmatic” research, arguing tha",
      "wordCount": 10040
    },
    "tags": [
      {
        "_id": "5GYzBE6q89w74dqfk",
        "name": "Abstraction",
        "slug": "abstraction"
      },
      {
        "_id": "RyNWXFjKNcafRKvPh",
        "name": "Agent Foundations",
        "slug": "agent-foundations"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "k6igEkzKYY2EpY7Su",
        "name": "Meta-Philosophy",
        "slug": "meta-philosophy"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "Z38PqJbRyfwCxKvvL",
        "name": "Research Agendas",
        "slug": "research-agendas"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RDG2dbg6cLNyo6MYT",
    "title": "Thane Ruthenis's Shortform",
    "slug": "thane-ruthenis-s-shortform",
    "url": null,
    "baseScore": 8,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 178,
    "createdAt": null,
    "postedAt": "2024-09-13T20:52:23.396Z",
    "contents": null,
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "KnQpzYRR4ogPNtzem",
    "title": "A Crisper Explanation of Simulacrum Levels",
    "slug": "a-crisper-explanation-of-simulacrum-levels",
    "url": null,
    "baseScore": 92,
    "voteCount": 39,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2023-12-23T22:13:52.286Z",
    "contents": {
      "markdown": "I've read the previous work on [Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels), and I've seen people express some confusion regarding how they work. I'd had some of those confusions myself when I first encountered the concept, and I think they were caused by insufficiently *crisp* definitions. \n\nThe extant explanations didn't seem like they offered a proper bottom-up/fundamentals-first mechanism for how simulacrum levels come to exist. Why do they have the specific features and quirks that they have, and not any others? Why is the form that's being ascribed to them the *inevitable* form that they take, rather than arbitrary? Why can't Level 4 agents help but act psychopathic? Why is there no Level 5?\n\nI'd eventually formed a novel-seeming model of how they work, and it now occurs to me that it may be useful for others as well (though I'd formed it years ago).\n\nIt aims to preserve all the important features of [@Zvi](https://www.lesswrong.com/users/zvi?mention=user)'s [definitions](https://www.lesswrong.com/posts/dHYxnSgMDeveovLuv/the-best-of-don-t-worry-about-the-vase#The_Simulacra_Levels_Sequence) while explicating them by fitting a proper gears-level mechanistic explanation to them. I think there are some marginal differences regarding where I draw the boundaries, but it should still essentially agree with Zvi's.\n\n* * *\n\nGroundwork\n----------\n\nIn some contexts, recursion levels become effectively indistinguishable past recursion level 3. Not exactly a new idea, but it's central to my model, so I'll include an example for completeness' sake.\n\nConsider the case of cognition.\n\n1.  Cognition is thinking about external objects and processes. \"This restaurant is too cramped.\"\n2.  Metacognition is building your model of your own thinking. What biases it might have, how to reason about object-level topics better. \"I feel that this restaurant is too cramped because I dislike large groups of people.\"\n3.  Meta-metacognittion is analysing your model of yourself: whether you're inclined to embellish or cover up certain parts of your personality, etc. \"I'm telling myself the story about disliking large groups of people because it feels like a more glamorous explanation for disliking this restaurant than the real one. I dislike it out of contrariness: there are many people here because it's popular, and I instinctively dislike things that are mainstream.\"\n4.  Meta-meta-metacognition would, then, be \"thinking about your analyses of your self-centered biases\". But that's just meta-metacognition again: analysing how you're inclined to see yourself. \"I'm engaging in complicated thinking about the way I think about myself because I want to maintain the self-image of a clever, self-aware person.\"\n    *   There is a similar case for meta-metacognition being the same thing as metacognition, but I think there's a slight difference between levels 2 and 3 that isn't apparent between 3 and 4 onward.[^sm2ltnt082r]\n\nNext: In basically any society, there are three distinct \"frameworks\" one operates with: physical reality, other people, and the social reality. Each subsequent framework contains a recursive model of the previous one:\n\n1.  The physical reality *is*.\n2.  People contain their own models of reality.\n3.  People's social images are other people's models of a person: i. e., models of models of reality.[^msdn2x9q13l]\n\nRecursion levels 1, 2, and 3. There's no meaningful \"level 4\" here: \"a model of a person's social image\" means \"the perception of a person's appearance\", which is still just \"a person's appearance\". You can get into some caveats here, but it doesn't change much[^fofs4yy521u].\n\nAny signal is thus viewed in each of these frameworks, giving rise to three kinds of meaning any signal can communicate:\n\n1.  What it literally says: viewed in the context of the physical reality.\n2.  What you think the speaker is trying to convince you of, and why: viewed in the context of your model of the speaker.\n3.  How it affects your and the speaker's social images: viewed in the context of your model of others' model of you and the speaker.\n\nSo far, that's fine: all of these layers of communication are useful and have a place in any functioning society.\n\nProblems start when the society starts *discarding* lower layers of communication. This is what moving to a higher Simulacrum Level means: discarding a lower level in favour of a higher one. A \"pure\" Level 1 society only cares about statements' literal truth; Level 2 society cares about the people behind the statements; Level 3 society cares about how statements influence people's *perceptions* of other people. Levels 0 and 4 are special: Level 0 doesn't have a concept of communication, and Level 4 has gone so deep into recursion it has abandoned it.\n\nI'll be using the terms \"society\" and \"agent\" because that's how I think about this model, but I mean that in a broad sense. An \"agent\" could be anything from a person to a nation, and \"society\" could be any group of people embedded in any context (including a wider society).\n\nAdditionally, I should note that the same agent could be occupying different Simulacrum Levels depending on what context they're in or what people they're with (a person could be perfectly nice and human to their family, but be a psychopath-like Level 4 when it comes to anything tangentially politics-oriented).\n\n* * *\n\nLevel 0\n-------\n\nUnderstanding it is crucial for understanding Level 4.\n\nFor Level 0 agents, there are no symbols, no models, and no communication, only actions aimed at directly introducing changes upon the physical world. Inasmuch as it concerns interactions with other agents, it is a level of pure conflict.\n\nYou see a lion, you run away. You see food, you take it. You meet an enemy, you kill them. Your actions are asymbolic: they don't stand for anything, there's no expectation that they'll be seen and understood by someone else. Their purpose is *inherent* in their form.\n\nWhen you're trying to sharpen a rock to put it on a spear, you're not trying to *persuade* the rock to change its form. When you're writing a computer program, you're not trying to *convince* the computer to work (as much as it could feel like it sometimes).\n\nBut it doesn't only concern inanimate objects. You *can* have a \"model\" of someone here — a hunter tracking its prey — but the crucial thing is that you don't assume they have a model of *you*. Any \"communication\" that can happen here is one-sided. If you see a bear running at you and you flee its territory, it could be said that the bear \"intimidated\" you. But the bear didn't think about \"intimidation\". Its actions weren't taken with the aim to signal its willingness to kill you in order to cause you to retreat. It meant to *directly* remove you from its territory, by killing you, and your willing retreat from it was, to it, a happy coincidence.[^b90gqrtknu]\n\nIt doesn't, per se, mean that Level 0 agents can't have a broad range of motivations, that they're only pursuing their own self-interest. However, it's most common. After all, operating at Level 0 requires either refusal or inability to connect with other agents/people, on a prolonged basis. And if you expect to only ever interact with someone else by unilaterally forcing changes upon them, or receiving the same in return, what sort of relationship could you have except an irreconcilably hostile one?\n\n(And what if *all* tools at your disposal are Level 0 tools? If there's no way for you to have a conversation with anyone, if you don't even have the concept of a \"conversation\", if you could only force changes upon the world? Spoiler alert: That's how Level 4 looks from the inside.)\n\nLevel 0 is what truth *is*. Higher levels could easily collapse to it: all-out fights and wars.\n\n* * *\n\nLevel 1\n-------\n\nWell, I won't get into the development of language and cooperation here. Level 1 is the level at which agents exchange information about the physical world in order to develop a correct shared understanding of it.\n\nStatements exchanged at this level are scrutinized solely for their truth value, they're literal and overt. Inasmuch as it concerns communication with other agents, it's a level of pure cooperation. That cooperation can sometimes take the form of \"demonstrate your (non-exaggerated) military might to the other tribe so they allow you to take all the resources without a fight\", but it's still aimed at achieving an outcome that is mutually preferable to the alternative. (There can be *Level-0* hostility, and hostile *refusals* to communicate. If you hate the other tribe enough, you don't give them chance to concede, you just massacre them all. But inasmuch as communication *does* happen, it's always prosocial.)\n\nAt Level 1, agents and societies are concerned with the physical world, and they're focused on building as accurate a model of it as possible. They have no attachment to that model, and would change it to better fit the world as needed.\n\nA conflict between two agents about a Level 1 issue would be had over their conflicting models of the physical world, and it would be resolved by testing which one is correct, or resolving a miscommunication.\n\n* * *\n\nLevel 2\n-------\n\nBut sometimes it's impossible to cleanly resolve a Level 1 conflict, because the two agents lack the ability to cleanly test which of them is right, and their priors (or motivated reasoning) cause them to prioritize different models. Or maybe they have different values altogether. In that case, one of them may *lie*: knowingly emit a statement that doesn't correctly describe the physical world in order to warp their interlocutor's model of the world and compel them to act in line with the first one's interests.\n\nThat isn't necessarily malicious, of course: they might lie for the other's \"own good\", perhaps because they think their interlocutor is biased. They might even be right! (To borrow Zvi's example, claiming that there's a lion on the other side of the river when there's actually a tiger, because the tribe doesn't fear tigers enough.)\n\nThis is the \"classical\" explanation of Level 2: the level where people discover deception, and start warping others' models for their own ends. And that's absolutely part of it. But only *part*.\n\nNeither of the interlocutors actually has to *lie*. Agreeing to disagree would be enough.\n\nIn the aftermath, there'll be *two* conflicting models of the world (let's call them \"worldviews\") present in the society. Say, an expansionist vs. an isolationist agendas. Soon, they will be joined by a dozen more, born of a dozen more unresolved disagreements or deceptions. Some of these disagreements will be resolved, but others will stick around for the long term.\n\nPeople would choose between these models based on the models' quality, their individual biases, and their preferences, forming into *sub-groups*. People in each sub-group would try to get more people on their side and to prevent people from leaving their sub-group, because it'll benefit them if their worldview dominates (either because they genuinely believe it's truer than the other models, or because it's beneficial to them specifically).\n\nThey'll get entrenched, they'll cultivate a loyalty to their cause or worldview, they'll foster certainty in what they believe. And eventually, some (most) will develop an *attachment* to their model of the physical world. They will start valuing it *inherently*, not only inasmuch as it is correct. They will start denying the evidence against it, which will get easier the more accurate (and/or difficult to challenge) their model is.\n\nThey will start to confuse their model of reality for *reality itself*.\n\nIn a late-stage Level 2 society, people would consider the most important thing what worldviews other people subscribe to and, generally, what beliefs and motives other people have. Statements would be scrutinized not for their truth value, but for the motivation and the beliefs behind them: why the statement's author chose to make it, what worldview they're trying to spread, or what they could be convinced of. At that point, statements' actual content would be considered less important than what they reveal about the speaker and the speaker's other beliefs.\n\nThe creation of new models or drastic changes to old ones would be resisted.\n\nWhat's *actually happening* in the physical world would become less important, compared to what others could be *convinced* is happening.\n\nA Level 2 conflict between two agents is a mutual attempt at manipulation. Both would be aiming to persuade their audience into buying into their model of the world, whether that audience be their interlocutor, or a literal audience watching them. Any underhanded psychological trick is fair game there.\n\nThe members of a society fully on Level 2 would (1) focus on developing as accurate a model of *the rest of the society* as possible, plus (2) treat their personal model of the physical world as essentially true, plus (3) consider the actual physical world irrelevant, to be lied about or dismissed as needed.\n\n* * *\n\nLevel 3\n-------\n\nA Level 2 society is a society divided into sub-groups scrutinizing people's statements for these people's genuine compatibility with their worldview. Some of these groups are more powerful than others, or appeal to different preferences, and you could get into one's good graces by signalling the correct things about yourself by emitting the correct statements. You could also hurt someone you hate by convincing people they're holding an unpopular worldview, or don't belong to a group they want to be part of.\n\nAnd it doesn't matter what you or they are *actually* like: as long as it *looks* like someone is the kind of person that belongs to a group, or invites hate, or has another type of relationship with some sort of cause, they *actually do*. It is purely a matter of appearances.\n\n*   Level 1 society is grounded in the objective truth of the physical world.\n*   Level 2 society is grounded in the objective truth of its people's *beliefs*. Even if those beliefs are warped, even if you *want* to warp them, you still care about *their actual genuine internal epistemic states*.\n*   Level 3 society invents its own reality out of whole cloth. It only cares about how things look like, what beliefs a person *looks like* they have. But it doesn't acknowledge this.\n\nIn a Level 3 society, statements are still scrutinized for the implications they create about their speaker, or about other people, like on Level 2 (because the recursion already hit its maximum on that front; more on that in the next section). But it no longer matters if your interpretation of these implications is correct, only that it's *good enough that others could plausibly claim to believe it*, or at least believe that *you* believe it.\n\nDoesn't matter if you *genuinely* believe in the Cause. Only that your social profile fits with the profiles of those who are part of the movement centered around the Cause.\n\nThis creates some perverse incentives, which Zvi [calls](https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions) the \"war against knowledge\":\n\n> At level 3, the following two things are blameworthy, creating two ways in which knowledge is a liability. \\[...\\]\n> \n> One blameworthy thing is *not invoking the right symbols.*\n> \n> This is the “composed of things that can be used for something else” aspect. Caring about what is true creates an alternative incentive that prevents one from invoking the proper symbols, and casts doubt on whether those symbols mean what they seem to mean.\n> \n> Invoking symbols that are technically false rather than those that are technically true is, if anything, *a stronger move in the game*. This is why. It signals more strongly one’s costly sending of the appropriate signals, without room for misinterpretation as a lower level action. By repeating the lie, we show ourselves loyal. By getting others to repeat it, we drive them towards being and identifying as loyal, and get them to show others they are loyal, and demonstrate our power over both people and symbols.\n> \n> The other blameworthy thing is *knowing that what you say is false*. What is blameworthy *is knowledge itself*.\n> \n> (Or, perhaps more precisely, *other people knowing* that you know what you say is false, and thus anyone else’s knowledge of our having knowledge, as opposed to knowledge itself, but that’s also true of any other blame system.)\n> \n> What did the President know, and when did he know it?\n> \n> Thus, the shift in communication from explicit to implicit. The focus on having only deniable, tacit knowledge.\n> \n> The follower who needs explicit instruction is a poor follower indeed. Specifying everything to be done is impractical, and makes it clear you have not only knowledge but responsibility. Much better to *work towards* the goals of the group, to pile on symbols that help win the game.\n> \n> Thus does this structure drive everyone away from knowledge. The easiest way, by far, to pretend not to know things is to not know them.\n\n* * *\n\nLevels of Recursion\n-------------------\n\nAs we climb up the levels, there are three important variables to keep track of: what is the aspect of the world the truth of which is considered important, what kind of framework is being formed/manipulated in order to model the important aspect, and what is the aspect of the world the truth of which is considered irrelevant.\n\nHere's that in table format:\n\n| L | Important | Framework | Irrelevant | Comment |\n| --- | --- | --- | --- | --- |\n| 0 | physical world | — | — | Models don't exist. Everything is real. |\n| 1 | physical world | people's worldviews | — | How does the world work? |\n| 2 | people's worldviews | people's social images | physical world | What do people believe? Why? How could it be used? |\n| 3 | people's social images | people's social images | people's worldviews | Only the images people project matter. |\n| 4 | people's social images | people's social images | people's social images | ??? |\n\nAll three distinct values represent different levels of recursion:\n\n1.  The physical world *is*.\n2.  People's worldviews are models of the physical world.\n3.  People's social images are collective models of people and their worldviews: models of models of the world.\n4.  The next one would be \"a model of a person's social image\", i. e. \"the perception of a person's appearance\", but that's still just \"a person's appearance\".\n\nAt Level 3, the third variable tries to go past recursion level 3, and therefore wraps around on itself. This causes the peculiar situation with \"writing its own reality\": level 3 *creates* the thing it cares about.\n\nBut because the truth of people's images isn't yet considered *irrelevant*, there's still some connection to reality, where sufficient evidence against a claim could overturn it. There's still an understanding that models *stand for* something, that they're meant to *represent* the truth of some immutable reality.\n\nLevel 3 agents genuinely care about how the world perceives them and others. How accurately they've signaled their allegiances. If someone provides evidence that someone had emitted a signal, at some point, that goes against the public image they're now trying to project, Level 3 agents *would* care about that. (Hence e. g. cancel culture.)\n\nLevel 4 moves past even this. (I've seen Level 4 described as \"everything past Level 3\", and I guess this is me formalizing that.)\n\n* * *\n\nLevel 4\n-------\n\nPicture a late-stage Level 3 society. The population of Level 2 agents dies out or adopts Level 3 thought patterns. At that point, no-one cares about what's actually happening anywhere or with anyone, or even what people could be convinced is happening, only *what people think they could plausibly claim to believe* is happening.\n\nThe switch to Level 4 happens once this state of affairs becomes shared knowledge. Once everyone knows that there's no audience they have to *legitimately* convince with their performances, only fellow L4 agents tracking the effects of every utterance on the status game being played. An once they further know that others know this too...\n\nThis has a tricky implication: symbols become asymbolic.\n\nA symbol is something which represents something else. But Level 4 statements are not correlated with reality in any way whatsoever: they're entirely void of meaning. They communicate no information, not on any level, neither literally nor by what implications they create. They don't stand-in for anything. At Level 4, symbols *stop symbolizing things*. They become self-sufficient.\n\nAnd everyone knows this, so no-one is trying to interpret them. And everyone knows *that* too, so no-one makes statements with the expectation that they'll be scrutinized for information. The only reason anyone makes a statement, then, is because they know it will have a specific effect on the local social context: open up certain avenues of attack or defense. That is, *their purpose is inherent in their form.*\n\nI think similarities with Level 0 are clear. Level 4 agents can't *talk*. Their utterances *warp reality*. They can't say things, they can only *force others to occupy different social contexts*. And they (think that) they get the same in return: no-one engages with them, the others only ever try to forcibly transform the sociopolitical landscapes around them.\n\nIn a very real sense, Level 4 statements are *moves*, manoeuvres of attack or defense, not unlike physical blows and evasions.\n\nFor that reason, they're also necessarily short-term. From the perspective of the lower levels, Level 4 agents still look like they're weaving different narratives about the physical reality. But these narratives' sole purpose is to win whatever *immediate* conflict their creator is engaged in right now. They need not be robust enough to survive past that conflict, or be consistent with each other, or even look coherent to anyone outside that conflict.\n\nUnlike on Level 3, providing evidence that someone had emitted an incorrect signal at some point won't move people, unless you manage to *correctly stage the reveal of this information as an attack*.\n\nLevel 4 agents don't care about the world, or what others believe, or how the world perceives them. The only care about the asymbolic routes through the sociopolitical landscape they can pseudo-physically walk through.\n\nLevel 4 is as much a Hobbesian hell as Level 0. Level 4 agents don't have the *language* for cooperative interactions with others. There could be, in theory, a broad range of motivations here too, but it'll all be warped through these lens.\n\n* * *\n\nLevel 5\n-------\n\nTrying to extrapolate in a way that sidesteps the recursion model (which is already looping across the board), Level 5 would be a framework that is to Level 4 what Level 1 is to Level 0. It all began with an escape from the asymbolic hell, after all.\n\nBut how can you re-discover communication after you've turned the very concept of a signal into a knife to stab people with?\n\nThere is no Level 5.\n\n[^sm2ltnt082r]: Perhaps a more fine-grained way to put it is that, in some contexts, as recursion levels rise, the differences between them shrink, and due to the human mind's limitations, L3 vs L4+ is where our models become coarse enough that the differences are imperceptible. \n\n[^msdn2x9q13l]: Which neatly fits with my view that agents are approximate causal mirrors. \n\n[^fofs4yy521u]: E. g., some policy proposals are considered unpopular not because the majority of people are actually against them, but because the majority thinks that the majority is against them, and if you worry about that, you'll have technically went to recursion level 4...But as per footnote 1, empirically this doesn't seem to happen much, likely due to the human mind's limitations. \n\n[^b90gqrtknu]: Well, okay, that may not actually be an accurate description with regards to literal animals. They're not all on Level 0, they can communicate with each other (although I think in this specific example, the line is blurred). Alternatively, you can imagine a non-sapient robot in the bear's place, following some algorithms for patrolling its territory that tell it to kill intruders.",
      "plaintextDescription": "I've read the previous work on Simulacrum Levels, and I've seen people express some confusion regarding how they work. I'd had some of those confusions myself when I first encountered the concept, and I think they were caused by insufficiently crisp definitions. \n\nThe extant explanations didn't seem like they offered a proper bottom-up/fundamentals-first mechanism for how simulacrum levels come to exist. Why do they have the specific features and quirks that they have, and not any others? Why is the form that's being ascribed to them the inevitable form that they take, rather than arbitrary? Why can't Level 4 agents help but act psychopathic? Why is there no Level 5?\n\nI'd eventually formed a novel-seeming model of how they work, and it now occurs to me that it may be useful for others as well (though I'd formed it years ago).\n\nIt aims to preserve all the important features of @Zvi's definitions while explicating them by fitting a proper gears-level mechanistic explanation to them. I think there are some marginal differences regarding where I draw the boundaries, but it should still essentially agree with Zvi's.\n\n----------------------------------------\n\n\nGroundwork\nIn some contexts, recursion levels become effectively indistinguishable past recursion level 3. Not exactly a new idea, but it's central to my model, so I'll include an example for completeness' sake.\n\nConsider the case of cognition.\n\n 1. Cognition is thinking about external objects and processes. \"This restaurant is too cramped.\"\n 2. Metacognition is building your model of your own thinking. What biases it might have, how to reason about object-level topics better. \"I feel that this restaurant is too cramped because I dislike large groups of people.\"\n 3. Meta-metacognittion is analysing your model of yourself: whether you're inclined to embellish or cover up certain parts of your personality, etc. \"I'm telling myself the story about disliking large groups of people because it feels like a more glamorous ",
      "wordCount": 3802
    },
    "tags": [
      {
        "_id": "5SPDtxJT6y6ZTXHBJ",
        "name": "Simulacrum Levels",
        "slug": "simulacrum-levels"
      },
      {
        "_id": "gHCNhqxuJq2bZ2akb",
        "name": "Social & Cultural Dynamics",
        "slug": "social-and-cultural-dynamics"
      },
      {
        "_id": "AADZcNS24mmSfPp2w",
        "name": "Communication Cultures",
        "slug": "communication-cultures"
      },
      {
        "_id": "Q6hq54EXkrw8LQQE7",
        "name": "Gears-Level",
        "slug": "gears-level"
      },
      {
        "_id": "kdbs6xBndPkmrYAxM",
        "name": "Politics",
        "slug": "politics"
      },
      {
        "_id": "EnFKSZYiDHqMJuvJL",
        "name": "Social Reality",
        "slug": "social-reality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "hx5wTeBSdf4bsYnY9",
    "title": "Idealized Agents Are Approximate Causal Mirrors (+ Radical Optimism on Agent Foundations)",
    "slug": "idealized-agents-are-approximate-causal-mirrors-radical",
    "url": null,
    "baseScore": 75,
    "voteCount": 33,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2023-12-22T20:19:13.865Z",
    "contents": {
      "markdown": "**Epistemic status:** I'm currently unsure whether that's [a fake framework](https://www.lesswrong.com/posts/wDP4ZWYLNj7MGXWiW/in-praise-of-fake-frameworks), [a probably-wrong mechanistic model](https://www.lesswrong.com/posts/q5Gox77ReFAy5i2YQ/in-defense-of-probably-wrong-mechanistic-models), or a legitimate insight into the fundamental nature of agency. Regardless, viewing things from this angle has been helpful for me.\n\nIn addition, the ambitious implications of this view is one of the reasons I'm fairly optimistic about arriving at a [robust solution to alignment](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment) via agent-foundations research in a timely manner. (My semi-arbitrary deadline is [2030](https://www.lesswrong.com/posts/BseaxjsiDPKvGtDrm/we-choose-to-align-ai), and I expect to arrive at intermediate solid results by EOY 2025.)\n\n* * *\n\n### Input Side: Observations\n\nConsider what happens when we draw inferences based on observations.\n\nPhotons hit our eyes. Our brains draw an image aggregating the information each photon gave us. We interpret this image, decomposing it into objects, and inferring which latent-variable object is responsible for generating which part of the image. Then we wonder further: what process generated each of these objects? For example, if one of the \"objects\" is a news article, what is it talking about? Who wrote it? What events is it trying to capture? What set these events into motion? And so on.\n\nIn diagram format, we're doing something like this:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1ee13686f2663a86d311634a92d15b6c2a5b67581babb041.png)\n\nBlue are ground-truth variables, grey is the \"Cartesian boundary\" of our mind from which we read off observations, purple are nodes in our world-model each of which can be mapped to a ground-truth variable.\n\nWe take in observations, infer what latent variables generated them, then infer what generated those variables, and so on. We go backwards: from effects to causes, iteratively. The Cartesian boundary of our input can be viewed as a \"mirror\" of a sort, reflecting *the Past*.\n\nIt's a bit messier in practice, of course. There are shortcuts, ways to map immediate observations to far-off states. But the general idea mostly checks out – especially given that these \"shortcuts\" probably still *implicitly* route through all the intermediate variables, just without explicitly computing them. (You can map a news article to the events it's describing without explicitly modeling the intermediary steps of witnesses, journalists, editing, and publishing. But your mapping function is still implicitly shaped by the known quirks of those intermediaries.)\n\n### Output Side: Actions\n\nConsider what happens when we're planning to achieve some goal, in a consequentialist-like manner.\n\nWe envision the target state. What we want to achieve, how the world would look like. Then we ask ourselves: what would cause this? What forces could influence the outcome to align with our desires? And then: how do we control these forces? What actions would we need to take in order to make the network of causes and effects steer the world towards our desires?\n\nIn diagram format, we're doing something like this:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/9cc2e0c17c643111da457e5d9bbbb78fe57f823641c4a6f3.png)\n\nGreen are goals, purple are intermediary variables we compute, grey is the Cartesian boundary of our actions, red are ground-truth variables through which we influence our target variables.\n\nWe start from our goals, infer what latent variables control their state in the real world, then infer what controls those latent variables, and so on. We go backwards: from effects to causes, iteratively, until getting to our own actions. The Cartesian boundary of our output can be viewed as a \"mirror\" of a sort, reflecting *the Future*.\n\nIt's a bit messier in practice, of course. There are shortcuts, ways to map far-off goals to immediate actions. But the general idea mostly checks out – especially given that these heuristics probably still *implicitly* route through all the intermediate variables, just without explicitly computing them. (\"Acquire resources\" is a good heuristical starting point for basically any plan. But what *counts as* resources is something you had to figure out in the first place by mapping from \"what lets me achieve goals in this environment?\".)\n\nAnd indeed, that side of my formulation isn't novel! From [this post](https://www.lesswrong.com/posts/gEKHX8WKrXGM4roRC/saving-time#Why_Time_) by Scott Garrabrant:\n\n> Time is also crucial for thinking about agency. My best short-phrase definition of agency is that **agency is time travel**. An agent is a mechanism through which the future is able to affect the past. An agent models the future consequences of its actions, and chooses actions on the basis of those consequences. In that sense, [the consequence *causes* the action](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a), in spite of the fact that the action comes earlier in the standard physical sense.\n\n### Both Sides: A Causal Mirror\n\nPutting it together, an idealized, compute-unbounded \"agent\" could be laid out in this manner:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d4af15862120bdc40486fcfd5240b5f0fe73d462e99476d3.png)\n\nYou may not like it, but this is what peak agency looks like.\n\nIt reflects the past at the input side, and reflects the future at the output side. In the middle, there's some \"glue\"/\"bridge\" connecting the past and the future by a forwards-simulation. During that, the agent \"catches up to the present\": figures out what'll happen *while* it's figuring out what to do.\n\nIf we consider [the relation between utility functions and probability distributions](https://www.lesswrong.com/posts/voLHQgNncnjjgAPH7/utility-maximization-description-length-minimization), it gets even more literal. An utility function over \\\\(X\\\\) could be viewed as a target *probability distribution* over \\\\(X\\\\), and maximizing expected utility is equivalent to minimizing cross-entropy between this target distribution and the real distribution.\n\nThat brings the \"planning\" process in alignment with the \"inference\" process: both are about propagating target distributions \"backwards\" in time through the network of causality.\n\n### Why Is This Useful?\n\n**The primary, \"ordinary\" use-case** is that this allows to *import intuitions and guesses about how planning works to how inference works*, and vice versa. It's a helpful heuristic to guide one's thoughts when doing research.\n\nAn example: Agency researchers are fond of talking about \"coherence theorems\" that [constrain](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents) how agents work. There's a lot of controversy around this idea. John Wentworth had [speculated](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities?commentId=GyE8wvZuWcuiCaySb) that \"real\" coherence theorems are yet to be discovered, and that they may be based on a more solid bedrock of probability theory or information theory. This might be the starting point for formulating those – by importing some inference-based derivations to planning procedures.\n\nAnother example: Consider [the information-bottleneck method](https://en.wikipedia.org/wiki/Information_bottleneck_method). Setup: Suppose we have a causal structure \\\\(W\\to O\\to M\\\\). We want to derive a mapping \\\\(O\\to M\\\\) such that it discards as much information in \\\\(O\\\\) as possible while retaining all data it has about \\\\(W\\\\). In optimization-problem terms, we want to minimize \\\\(I(O;M)\\\\) under the constraint of\\\\(I(W;O)=I(W;M)\\\\). The IBM paper then provides a precise algorithm on how to do that, if you know the mapping of \\\\(W\\to O\\\\). And that's a pretty solid description of some aspects of inference.\n\nBut if inference is equivalent to planning, then it'd stand to reason that something similar happens on the *planning* side, too. Some sort of \"observations\", some sort of information-theoretic bottleneck, etc.\n\nAnd indeed: the bottleneck is *actions*! When we're planning, we (approximately) generate a whole target world-state. But we can't just assert it upon reality, we have to bring it about through our actions. So we \"extract\" a plan, we compress that hypothetical world-state into actions that would allow us to generate it... and funnel those actions through our output-side interface with the world.\n\nIn diagram format:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f22faed62fd9d840cc9f9a1abced44c95e0b2cbf5ac2a14.png)\n\nWe have two bottlenecks: our agent's processing capacity, which requires it to compress all observational data into a world-model, and our agent's limited ability to influence the world, which causes it to compress its target world-state into an action-plan. We can now adapt the IBM for the task of deriving *planning-heuristics* as well.\n\nAnd we've arrived at this idea by reasoning from the equivalence of inference to planning.\n\n**The ambitious use-case** is that if this framework is meaningfully true, this implies that all cognitive functions can be viewed as [inverse problems](https://en.wikipedia.org/wiki/Inverse_problem) to the environmental functions our universe computes. Which suggests a proper *paradigm* to agent-foundations research. A way to shed light on all of it by understanding how certain aspects of the environment work.\n\nOn which topic...\n\n### Missing Piece: Approximation Theory\n\nNow, of course, agents can't be *literal* causal mirrors. It would essentially require each agent to be as big as the universe, if it has to *literally* infer the state of every variable the universe computes (bigger, actually: inverse problems tend to be harder).\n\nThe literal formulation also runs into all sorts of infinite recursion paradoxes. What if the agent wants to model itself? What if the environment contains other agents? What if some of them are modeling this agent? And so on.\n\nBut, of course, it doesn't have to model *everything*. I'd already alluded to it when mentioning \"shortcuts\". No, in practice, even idealized agents are only *approximate* causal mirrors. Their cognition is optimized for low computational complexity and efficient performance. The question then is: how does that \"approximation\" work?\n\nThat is precisely what [the natural abstractions research agenda](https://www.lesswrong.com/tag/natural-abstraction) is trying to figure out. What is the relevant theory of approximation, that would suffice for efficiently modeling any system in our world?\n\nTaking that into account, and assuming that my ambitious idea – that all cognitive functions can be derived as inversions of environmental functions – is roughly right...\n\nWell, in that case, figuring out abstraction would be ***the last major missing piece in agent foundations***. If we solve that puzzle, it'll be smooth sailing from there on out. No more fundamental questions about [paradigms](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency#Preparadigmicity), no theoretical confusions, no inane philosophizing ~like this post~.\n\nThe work remaining after that may still not end up *easy*, mind. Inverse problems tend to be difficult, and the math for inversions of specific environmental transformations may be hard to figure out. But only in a strictly technical sense. It would be *straightforwardly difficult*, and much, *much* more scalable and parallelizable.\n\nWe won't need to funnel it all through a bunch of eccentric agent-foundation researchers. We would at last attain high-level expertise in the domain of agency, [which would let us](https://www.lesswrong.com/s/TLSzP4xP42PPBctgw/p/3gAccKDW6nRKFumpP#The_Best_Solution__A_Client_With_At_Least_Some_Understanding) properly factorize the problem.\n\nAnd then, all we'd need to do is hire a horde of mathematicians and engineers (or, if we're really lucky, *get some math-research AI tools*), pose them well-defined technical problems, and blow the problem wide open.",
      "plaintextDescription": "Epistemic status: I'm currently unsure whether that's a fake framework, a probably-wrong mechanistic model, or a legitimate insight into the fundamental nature of agency. Regardless, viewing things from this angle has been helpful for me.\n\nIn addition, the ambitious implications of this view is one of the reasons I'm fairly optimistic about arriving at a robust solution to alignment via agent-foundations research in a timely manner. (My semi-arbitrary deadline is 2030, and I expect to arrive at intermediate solid results by EOY 2025.)\n\n----------------------------------------\n\n\nInput Side: Observations\nConsider what happens when we draw inferences based on observations.\n\nPhotons hit our eyes. Our brains draw an image aggregating the information each photon gave us. We interpret this image, decomposing it into objects, and inferring which latent-variable object is responsible for generating which part of the image. Then we wonder further: what process generated each of these objects? For example, if one of the \"objects\" is a news article, what is it talking about? Who wrote it? What events is it trying to capture? What set these events into motion? And so on.\n\nIn diagram format, we're doing something like this:\n\nBlue are ground-truth variables, grey is the \"Cartesian boundary\" of our mind from which we read off observations, purple are nodes in our world-model each of which can be mapped to a ground-truth variable.\nWe take in observations, infer what latent variables generated them, then infer what generated those variables, and so on. We go backwards: from effects to causes, iteratively. The Cartesian boundary of our input can be viewed as a \"mirror\" of a sort, reflecting the Past.\n\nIt's a bit messier in practice, of course. There are shortcuts, ways to map immediate observations to far-off states. But the general idea mostly checks out – especially given that these \"shortcuts\" probably still implicitly route through all the intermediate variables, just without expl",
      "wordCount": 1652
    },
    "tags": [
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "RyNWXFjKNcafRKvPh",
        "name": "Agent Foundations",
        "slug": "agent-foundations"
      },
      {
        "_id": "cq69M9ceLNA35ShTR",
        "name": "Causality",
        "slug": "causality"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "ATppeTdQS8nDAzBjk",
        "name": "Boundaries / Membranes [technical]",
        "slug": "boundaries-membranes-technical"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CpjTJtW2RNKvzAehG",
    "title": "Most People Don't Realize We Have No Idea How Our AIs Work",
    "slug": "most-people-don-t-realize-we-have-no-idea-how-our-ais-work",
    "url": null,
    "baseScore": 159,
    "voteCount": 83,
    "viewCount": null,
    "commentCount": 42,
    "createdAt": null,
    "postedAt": "2023-12-21T20:02:00.360Z",
    "contents": {
      "markdown": "This point feels fairly obvious, yet seems worth stating explicitly.\n\nThose of us familiar with the field of AI after the deep-learning revolution know perfectly well that we have no idea how our ML models work. Sure, we have an understanding of the dynamics of training loops and SGD's properties, and we know how ML models' *architectures* work. But we don't know what specific algorithms ML models' forward passes implement. We have some guesses, and some insights painstakingly mined by interpretability advances, but nothing even remotely like a full understanding.\n\nAnd most certainly, we wouldn't automatically know how a fresh model trained on a novel architecture that was just spat out by the training loop works.\n\nWe're all used to this state of affairs. It's implicitly-assumed shared background knowledge. But it's actually pretty unusual, when you first learn of it.\n\nAnd...\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/92e9d99839897978e30033141ed2f4223d5126596dcd7531.png)\n\nRelevant XKCD.\n\nI'm pretty sure that the general public *doesn't actually know that*. I don't have hard data, but it's my strong impression, based on reading AI-related online discussions in communities not focused around tech, talking to people uninterested in AI advances, and so on.[^v1vmoacp21r]\n\nThey still think in GOFAI terms. They still believe that all of an AI's functionality has been deliberately *programmed*, not *trained*, into it. That behind every single thing ChatGPT can do, there's a human who implemented that functionality and understands it.\n\nOr, at the very least, that it's written in legible, human-readable and human-understandable format, and that we can interfere on it in order to cause precise, predictable changes.\n\nPolls already show concern about AGI. If the fact that we don't know what these systems are actually thinking were *widely known* and *properly appreciated*? If there *weren't* the implicit assurance of \"someone understands how it works and why it can't go catastrophically wrong\"?\n\nWell, I expect much more concern. Which might serve as a pretty good foundation for further pro-AI-regulations messaging. A way to acquire some [political currency](https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance#Public_Opinion__And_Perception_Thereof_) you can spend.\n\nSo if you're doing any sort of public appeals, I suggest putting the proliferation of this information on the agenda. [You get about five words](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words) (per message) to the public, and \"Powerful AIs Are Black Boxes\" seems like a message worth sending out.[^hae8cysa7z7]\n\n[^v1vmoacp21r]: If you do have some hard data on that, that would be welcome. \n\n[^hae8cysa7z7]: There's been some pushback on the \"black box\" terminology. I maintain that it's correct: ML models are black boxes relative to us, in the sense that by default, we don't have much more insight into what algorithms they execute than we'd have by looking at a homomorphically-encrypted computation to which we don't have the key, or by looking at the activity of a human brain using neuroimaging. There's been a nonzero amount of interpretability research, but it's still largely the case; and would be almost fully the case for models produced by novel architectures.ML models are not black boxes relative to the SGD, yes. The algorithm can \"see\" all computations happening, and tightly intervene on them. But that seems like a fairly counter-intuitive use of the term, and I maintain that \"AIs are black boxes\" conveys all the correct intuitions.",
      "plaintextDescription": "This point feels fairly obvious, yet seems worth stating explicitly.\n\nThose of us familiar with the field of AI after the deep-learning revolution know perfectly well that we have no idea how our ML models work. Sure, we have an understanding of the dynamics of training loops and SGD's properties, and we know how ML models' architectures work. But we don't know what specific algorithms ML models' forward passes implement. We have some guesses, and some insights painstakingly mined by interpretability advances, but nothing even remotely like a full understanding.\n\nAnd most certainly, we wouldn't automatically know how a fresh model trained on a novel architecture that was just spat out by the training loop works.\n\nWe're all used to this state of affairs. It's implicitly-assumed shared background knowledge. But it's actually pretty unusual, when you first learn of it.\n\nAnd...\n\nRelevant XKCD.\nI'm pretty sure that the general public doesn't actually know that. I don't have hard data, but it's my strong impression, based on reading AI-related online discussions in communities not focused around tech, talking to people uninterested in AI advances, and so on.[1]\n\nThey still think in GOFAI terms. They still believe that all of an AI's functionality has been deliberately programmed, not trained, into it. That behind every single thing ChatGPT can do, there's a human who implemented that functionality and understands it.\n\nOr, at the very least, that it's written in legible, human-readable and human-understandable format, and that we can interfere on it in order to cause precise, predictable changes.\n\nPolls already show concern about AGI. If the fact that we don't know what these systems are actually thinking were widely known and properly appreciated? If there weren't the implicit assurance of \"someone understands how it works and why it can't go catastrophically wrong\"?\n\nWell, I expect much more concern. Which might serve as a pretty good foundation for further pro-AI-regula",
      "wordCount": 371
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2c7",
        "name": "Regulation and AI Risk",
        "slug": "regulation-and-ai-risk"
      },
      {
        "_id": "AqwjXSSy7DuF2pKdm",
        "name": "Slowing Down AI",
        "slug": "slowing-down-ai-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "okkEaevbXCSusBoE2",
    "title": "How Would an Utopia-Maximizer Look Like?",
    "slug": "how-would-an-utopia-maximizer-look-like",
    "url": null,
    "baseScore": 32,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 23,
    "createdAt": null,
    "postedAt": "2023-12-20T20:01:18.079Z",
    "contents": {
      "markdown": "When we talk of aiming for the good future for humanity – whether by aligning AGI or any other way – it's implicit that there are some futures that \"humanity\" as a whole would judge as good. That in some (perhaps very approximate) sense, humanity could be viewed as an agent with preferences, and that our aim is to satisfy said preferences.\n\nBut is there a theoretical basis for this? *Could* there be? How would it look like?\n\nIs there a meaningful frame in which humanity be viewed as *optimizing for* its purported preferences across history?\n\nIs it possible or coherent to imagine a [wrapper-mind](https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy) set to the task of maximizing for the utopia, whose activity we'd actually endorse?\n\nThis post aims to sketch out answers to these questions. In the process, it also outlines how my current models of basic value reflection and extrapolation work.\n\n* * *\n\nInformal Explanation\n--------------------\n\n### Basic Case\n\nIs an utopia that'd be perfect for everyone possible?\n\nThe short and obvious answer is no. Our civilization contains omnicidal maniacs and true sadists, whose central preferences are directly at odds with the preferences of most other people. Their happiness is diametrically opposed to other people's.\n\nLess extremely, it's likely that most individuals' *absolutely perfect* world would fail to perfectly satisfy most others. As a safe example, we could imagine someone who loves pizza, yet really, really hates seafood, to such an extent that they're offended by the mere knowledge that seafood exists somewhere in the world. Their utopia would not have any seafood anywhere – and that would greatly disappoint seafood-lovers. If we now postulate the existence of a pizza-hating seafood-lover... Well, it would seem that their utopias are directly at odds.[^pse2up1hz6h]\n\nNevertheless, there are worlds that would make both of them *happy enough*. A world in which everyone is free to eat food that's tasty according to their preferences, and is never forced to interact with the food they hate. Both people would still dislike the fact that their hated dishes exist *somewhere*. But as long as food-hating is not their core value that's dominating their entire personality, they'd end up happy enough.\n\nSimilarly, it intuitively feels that worlds which are strictly better according to most people's *entire arrays of preferences* are possible. [Empowerment](https://www.lesswrong.com/posts/JPHeENwRyXn9YFmXc/empowerment-is-almost-all-we-need) is one way to gesture at it – a world in which each individual is simply given *more instrumental resources*, a greater ability to satisfy whatever preferences they happen to have. (With some limitations on impacting other people, etc.)\n\nBut is it possible to arrive at this idea from first principles? By looking at humanity and somehow \"eliciting\"/\"agglomerating\" its preferences formally? A process like [CEV](https://www.lesswrong.com/tag/coherent-extrapolated-volition)? A target to hit that's \"objectively correct\" according to humanity's own subjective values, rather than [your subjective interpretation of its values](https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future)?\n\nParaphrasing, we're looking for an utility function such that the world-state maximizing it is ranked as very high by the standards of most humans' preferences; an utility function that's correlated with the \"agglomeration\" of most humans' preferences.\n\nLet's consider what we did in the foods example. We discovered two disparate preferences, and then we *abstracted up* from them – from concrete ideas like \"seafood\" and \"pizza\", to [an abstraction over them](https://www.lesswrong.com/posts/N2JcFZ3LCCsnK2Fep/the-minimal-latents-approach-to-natural-abstractions): food-in-general. And we've discover that, although the individuals' preferences disagreed on the concrete level, they ended up basically the same at the higher level. Trivializing, it turned out that a seafood-optimizer and a pizza-optimizer could both be viewed as tasty-food-optimizers.\n\nThe hypothesis, then, would go as follows: at some very high abstraction level, the level of global matters and fundamental philosophy, most humans' preferences converge to the same utility function over some variable. For example, \"maximize eudaimonia\" or \"human empowerment\" or \"human flourishing\".\n\nThere's a counting argument that slightly supports this. Higher abstraction levels are less expressive: they include fewer objects/variables (fewer countries than people, fewer stars than atoms, fewer galaxies than stars) and these objects have fewer states (fewer moods than the ways your brain's atoms could be arrranged). So the mapping-up of values to them isn't injective. Thus, some conflicting low-level preferences would map to the same preference over the same high-level variable.\n\nThat is, of course, a hypothesis. Nevertheless, the mere fact that we can coherently state it is reassuring regarding our ability to eventually test it.\n\n### Is Humanity an Utopia-Maximizer?\n\nMaybe. I don't strongly believe in this, but here's a sketch:\n\n*If* human values indeed converge like this, then perhaps humanity can be viewed as an approximate agent that's been approximately optimizing for building an utopia for its entire history. But those \"approximately\" do a lot of work; there's plenty of noise involved.\n\nPrimary issue is that the distribution of power between its constituents is non-uniform and changes dynamically. At different times, people with different preferences amass disproportionate amounts of resources (often by orders of magnitudes so), and \"deviate\" humanity's path away from the hypothetical averaged-out course, in their individual preferred directions.\n\nBut the balance of power frequently changes, and how technologies change it is relatively unpredictable. So *potentially* these effects actually cancel out on average, and humanity stays roughly on-target? (Pizza-lovers being in charge for 100 years are replaced by seafood-lovers ruling for 100 years; and while they cancel out their specific preferences, both end up advancing humanity towards having tasty food around.)\n\nIt would explain why our world, like, actually does mostly get better over time. As well as provide some grounding to the ideas of \"moral progress\".\n\nNevertheless, the approximations there may be *extremely* noisy, to the point that looking at things this way may not be useful.\n\n### Is an Utopia-Maximizer Desirable?\n\nAssuming this hypothetical utopian utility function exists and we derive it, would it be possible to then plug it into some idealized agent/[wrapper-mind](https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy), and not be horrified at the results?\n\nOn my view, the answer is *obviously yes*. There's a bunch of confusions around this idea that I'd like to address; mainly around what \"a fixed goal\" implies.\n\nConsider a paperclip-maximizer. It wants the universe to be full of paperclips. If it gets its way, it'd reassemble all matter, including itself, into them.\n\nNote, however, that it would not necessarily aim to *freeze them in time*. Intuitively, it would be fine with the paperclips still orbiting each other, impacting each other, and so on. Moreover, by the very definition of \"a paperclip\", there'd be all sorts of subatomic processes happening within them. The paperclip-maximizer would want those to run their natural course. Its utility would stay *constant* as that happens; invariant under these transformations of the world-state.\n\nSimilarly, the maximum of an utopia-maximizer would be defined over *an enormous equivalence class of world-states*. It would not aim to freeze humanity in time, or impose some specific unchanging social order, or tile the universe with copies of specific people that it deemed most optimal for experiencing happiness, etc.\n\nIts utility would be invariant under individual humans changing over time, under them forging new relationships, under societal structures changing and events generally moving forward. As long as those processes don't wander into some nightmarish outcomes. That's the main function it'd provide: a sort of \"safety net\", lower-bounding how bad things could get. (And currently, they are very, very bad.)\n\nIndeed, being a wrapper-mind doesn't even disqualify you from *being a person* (as nostalgebraist's post claims). Your utility can be invariant and maximal under *many possible internal states.* You can grow and change as a person, even if you have a fixed hard-wired goal that you ultimately serve.\n\nSimilarly, it's not unreasonable to suggest that most humans are (effectively isomorphic to) wrapper-minds.\n\n* * *\n\nFormal Model\n------------\n\nSuppose that on your hands, you have an agent with a vast array of disparate preferences. It's a mess. They're stored in different formats (explicit vs. implicit, deontological vs. consequentialist, instrumental vs. terminal...), defined on different abstraction levels, often conflict with each other.\n\nYou want to optimize them, straighten them out. Resolve whatever conflicts they have, translate them to whatever domains you're working in, extrapolate them (to plan for the long-term), concretize them (to figure out what *specific actions* a philosophy demands of you), agglomerate them...\n\nWhy? Performance optimization. Sure, you *could* just do [babble-and-prune](https://www.lesswrong.com/tag/babble-and-prune) search on your world-model, figuring out what would satisfy those preferences by brute force. But that'd be ruinously compute-intensive. You'd like to cache some of them, derive some heuristics from them, resolve conflicts to stop wasting time on those, etc.\n\nHow can you sort it out? What *target* are you even aiming at?\n\nWell, the purpose of utility functions/preferences is to *recommend what actions to take*. Indeed, that's their main contribution: they define a preference ordering over candidate plans/actions, either directly (deontology), or by way of looking at what worlds a given action would bring about (consequentialism).\n\nThus, the *correct* process of value-system performance-optimization would be made up of transformations such that the preference ordering over actions is invariant under them. I. e., the value-optimized agent would always take the same actions in any given situation as the initial agent (if the latter were given sufficient time to think).\n\nLet's see where that can get us.\n\n### Deontological Preferences\n\nTo start off, deontological preferences are isomorphic to utility functions, and utility functions are isomorphic to deontological preferences. They're related by the softmax function:\n\n\\\\\\[P(x)=\\frac{e^{u(x)}}{\\underset{x\\in X}{\\sum} e^{u(x)}}  \\longleftrightarrow  u(x)=\\ln \\left(P(x)\\cdot\\underset{x\\in X}{\\sum e^{u(x)}}\\right)\\\\\\]\n\nTake a given deontological rule, like \"killing is bad\". Let's say we view it as a constraint on the allowable actions; or, in other words, a *probability distribution* over your actions that \"predicts\" that you're very likely/unlikely to take specific actions. The above transform would let us translate it into an utility function over actions.\n\nThe other way around, an utility function [can be viewed as](https://www.lesswrong.com/posts/voLHQgNncnjjgAPH7/utility-maximization-description-length-minimization) defining some \"target distribution\" for the variable over which it's defined. Maximizing expected utility would then be equivalent to minimizing the cross-entropy between that target distribution and the real distribution.\n\nAnd that's not simply an overly abstract trick: it's how human minds are actually hypothesized to work. See Friston's predictive-processing framework in neuroscience (you can start from [these comments](https://www.lesswrong.com/posts/voLHQgNncnjjgAPH7/utility-maximization-description-length-minimization?commentId=fQEjJCfYJsvihn6rc)).\n\nThis also covers [shards](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX). They're self-executing heuristics bidding for specific actions over others. Thus, each could be transformed into an utility function without loss of information.\n\nThat's not at odds with how deontology is usually presented, either. Deontologists reject utility-maximization in the sense that they refuse to engage in utility-maximizing calculations *using their conscious intelligence*. But similar dynamics can still be at play \"under the hood\".\n\n### Value Conflict Resolution\n\nImagine an agent having two utility functions, \\\\(u_1\\\\) and \\\\(u_2\\\\). It's optimizing for their sum, \\\\(u_1+u_2\\\\). If the values are in conflict, if taking an action that maximizes \\\\(u_1\\\\) hurts \\\\(u_2\\\\) and vice versa — well, one of them almost surely spits out a *higher* value, so the maximization of \\\\(u_1+u_2\\\\) is still well-defined.\n\nThat's roughly how humans do work in practice. If we face a value conflict, we hesitate a bit (calculating the sum, the \"winner\"), but ultimately end up taking *some* action that we endorse.\n\n... unless we hesitate *too long*, and time chooses for us. Or if we know we have to take action *fast*, and so decide to use some very rough approximations – and potentially make a mistake which we later regret it.\n\nThus, there's purely practical value in *reducing the number of internal conflicts*. Finding a value \\\\(u_3\\\\) such that, for all situations, it has the same preference ordering as \\\\(u_1+u_2\\\\), but its computational complexity is much lower.\n\n### Value Extrapolation\n\nValue extrapolation seems to be straightforward: it's just [the reflection of the fact that the world can be viewed as a series of hierarchical ever-more-abstract models](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks).\n\n1.  Suppose we have a low-level model of reality \\\\(E^0\\\\), with \\\\(n_0\\\\) variables (atoms, objects, whatever).\n2.  Suppose we \"abstract up\", deriving a more simple model of the world \\\\(E^1\\\\), with \\\\(n_1\\\\) variables. Each variable \\\\(e_i^1\\\\) in it is an abstraction over some set of lower-level variables \\\\(\\{e_k^0\\}\\\\), such that \\\\(f(\\{e_k^0\\})=e_i^1\\\\).\n    *   Recap: Higher-level variables are, by definition, less expressive, i. e. the number of states they could be in is lower than the number of states the underlying system can be in. By the counting argument, that means their states are defined over (very large in practice) *equivalence classes* of low-level states.\n    *   Example: \"I'm happy\" is a high-level state that correspond to a combinatorially large number of configurations my body's atoms can be in. Stipulating \"I'm happy\" only constrains my low-level state up to that equivalence class.\n3.  We iterate, to \\\\(E^2\\\\), \\\\(E^3\\\\), ..., \\\\(E^\\Omega\\\\). We derive increasingly more abstract models of the world.\n    *   Note: \\\\(n_{l}\\ll n_{l-1}\\\\). Since each subsequent level is simpler, it contains *fewer* variables. People to social groups to countries to the civilization; atoms to molecules to macro-scale objects to astronomical objects; etc.\n4.  Let's define the function \\\\(f^{-1}(e_i^l)=P(\\{e_k^{l-1}\\} | e_i^l)\\\\). I. e.: it returns a probability distribution over the low-level variables given the state of a high-level variable that abstracts over them.\n    *   Note: As per (2), that only constrains the low-level system to a (very large) equivalence class of states. (Though the distribution needn't be uniform.)\n    *   Example: If the world economy is in *this* state, how happy my grandmother is likely to be?\n5.  If we view our values as an utility function \\\\(u\\\\), we can \"translate\" our utility function from any \\\\(e_k^{l-1}\\\\) to \\\\(e_i^l\\\\) roughly as follows: \\\\(u(e_k^{l-1})\\to u'(e_i^l)=\\mathbb{E}[u(e_k^{l-1}) | f^{-1}(e_i^l)]\\\\). \n    *   (There's a *ton* of complications there, but this expression conveys the core idea.)\n\n... and then value extrapolation just naturally falls out of this.\n\nSuppose we have a bunch of values at the \\\\(l\\\\)th abstraction level. Once we start frequently reasoning at \\\\((l+1)\\\\)th level, we \"translate\" our values to it, and cache the resultant functions. Since the \\\\((l+1)\\\\)th level likely has fewer variables than \\\\(l\\\\)th, the mapping-up is not injective: some values defined over different low-level variables end up translated to *the same* higher-level variable (\"I like pizza and seafood\" -> \"I like tasty food\", \"I like Bob and Alice\" -> \"I like people\"). This effect only strengthens as we go up higher and higher. At \\\\(E^{\\Omega}\\\\), we can plausibly end up with only one variable we value (as previously speculated, \"eudaimonia\" or something).\n\n### Putting It Together\n\nSuppose we have a human on our hands, and we want to compile all of their values into a highly abstract utility function *that the human would endorse*. To do so, we:\n\n*   Transform all values into the same format. (Either utility functions *or* probability distributions; doesn't really matter.)\n*   Translate them around to reveal value conflicts.\n*   Resolve those conflicts by finding equivalent-but-simpler utility functions.\n*   Extrapolate them upwards, to the highest abstraction level.\n*   We end up with[^zlozw2fx54i] a distillation/compilation of that human's entire selfhood, in the format isomorphic to an utility function. The endpoint of their moral philosophy.\n\n... if only it were this easy.\n\n### Major Problem: Meta-Preferences\n\nHumans have preferences not only about object-level stuff, but also about the way they do the whole value-compilation process. The above model assumed an *idealized* process, in the sense of deriving an utility function that would always recommend the same actions as the initial array of values, but have dramatically lower computational complexity.\n\nHowever, humans have meta-values that can express *arbitrarily custom preferences* regarding the process of value reflection *itself*. We might have preferences over...\n\n*   ... basic translations. E. g., a deontologist's refusal to take money into account when choosing whose life to save. (Refusing to translate and account for that preference.)\n*   ... how we extrapolate things up the abstraction levels. E. g., \"I'm not going to let my petty preferences impact the future of humanity\", such that you [*ignore* your preference for pizza when defining the AGI's utility function](https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future) (rather than biasing it towards it).\n*   ... how we resolve value conflicts. E. g., if we have \\\\(u_1\\\\) = \"I want to be a good person\" and \\\\(u_2\\\\) = \"I'd get a thrill out of stealing something\", we often wouldn't just tweak \\\\(u_2\\\\) such that it still fires, but only when stealing something wouldn't be against the society's interests. No: we just flat-out *delete* \\\\(u_2\\\\).\n*   Etc.\n\nThese complications currently have me worried that there's basically no way to elicit and compile a given human's preferences except *directly simulating their mind*. No shortcuts whatsoever. (And then that simulation would be path-dependent, such that, depending on what stimuli you show the human in what order, they might end up at vastly-different-yet-equally-legitimate endpoints. But that's a whole separate topic.)\n\nRegardless, this doesn't *kill* the core idea. I'm reasonably sure (something like) the procedures I've defined are still what humans use most of the time. But there are more complex cases where meta-preferences are involved, they're often crucial, and I'm not sure there are elegant ways to handle them.\n\n### Egalitarian Agglomeration\n\nNow onto the last step: how do we agglomerate values between *different people*? That is, suppose we've \"compiled\" the preferences of all individual people into a set of utility functions, and then picked just their most-abstract components, getting this set: \\\\(\\{U^{\\Omega}_i(w)\\}_{i=0}^{\\sim 8.1\\times10^9}\\\\). How do we transform that into \\\\(U^{\\Omega}_\\text{humanity}(w)\\\\)?\n\nWell, ideally, it'll turn out that \\\\(\\forall k,i:U_i^\\Omega(w)\\approx U_{k}^\\Omega(w)\\\\). That's the \"strong\" version of the \"human value convergence hypothesis\".\n\nWhat if not, though?\n\nThe naive idea would be to just proceed as we had before, and find a simpler function that recommends the same actions as the individual functions' sum. But that has some undesirable properties, like a sensitivity to \"utility monsters\". The [Geometric Rationality](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg) sequence has made that point rather well.\n\nThus, a better target would be a function that's equivalent to *the product* of individual humans' utility functions. It effectively maximizes the expected utility of a randomly-chosen human; thus, it aims to uniformly distribute utility across everyone. (I really recommend reading the Geometric Rationality sequence.)\n\nAnd *that* result is, theoretically,\n\n*   An utility function that humanity-as-a-whole could be said to have been (very roughly) maximizing throughout its history.\n*   An utility function that something like [CEV](https://www.lesswrong.com/tag/coherent-extrapolated-volition) might spit out.\n*   An utility function whose maximization would rank high by most individual humans' preferences/utility functions.\n*   An utility function we could hook up to a wrapper-mind, and then be happy with the result.\n\n[^pse2up1hz6h]: I'm sure you can come up with less tame examples from, say, politics or social issues. Fill them in as needed. \n\n[^zlozw2fx54i]: Well, that was a simplified description of the process. In practice, you'd need to mix these steps up repeatedly.",
      "plaintextDescription": "When we talk of aiming for the good future for humanity – whether by aligning AGI or any other way – it's implicit that there are some futures that \"humanity\" as a whole would judge as good. That in some (perhaps very approximate) sense, humanity could be viewed as an agent with preferences, and that our aim is to satisfy said preferences.\n\nBut is there a theoretical basis for this? Could there be? How would it look like?\n\nIs there a meaningful frame in which humanity be viewed as optimizing for its purported preferences across history?\n\nIs it possible or coherent to imagine a wrapper-mind set to the task of maximizing for the utopia, whose activity we'd actually endorse?\n\nThis post aims to sketch out answers to these questions. In the process, it also outlines how my current models of basic value reflection and extrapolation work.\n\n----------------------------------------\n\n\nInformal Explanation\n\n\nBasic Case\nIs an utopia that'd be perfect for everyone possible?\n\nThe short and obvious answer is no. Our civilization contains omnicidal maniacs and true sadists, whose central preferences are directly at odds with the preferences of most other people. Their happiness is diametrically opposed to other people's.\n\nLess extremely, it's likely that most individuals' absolutely perfect world would fail to perfectly satisfy most others. As a safe example, we could imagine someone who loves pizza, yet really, really hates seafood, to such an extent that they're offended by the mere knowledge that seafood exists somewhere in the world. Their utopia would not have any seafood anywhere – and that would greatly disappoint seafood-lovers. If we now postulate the existence of a pizza-hating seafood-lover... Well, it would seem that their utopias are directly at odds.[1]\n\nNevertheless, there are worlds that would make both of them happy enough. A world in which everyone is free to eat food that's tasty according to their preferences, and is never forced to interact with the food they h",
      "wordCount": 3047
    },
    "tags": [
      {
        "_id": "xknvtHwqvqhwahW8Q",
        "name": "Human Values",
        "slug": "human-values"
      },
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "PgXFLqriw5Far9v7x",
        "name": "AI Success Models",
        "slug": "ai-success-models"
      },
      {
        "_id": "W6QZYSNt5FgWgvbdT",
        "name": "Coherent Extrapolated Volition",
        "slug": "coherent-extrapolated-volition"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2c3",
        "name": "Value Extrapolation",
        "slug": "value-extrapolation"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4XN2nJXABo6d5htZY",
    "title": "Don't Share Information Exfohazardous on Others' AI-Risk Models",
    "slug": "don-t-share-information-exfohazardous-on-others-ai-risk",
    "url": null,
    "baseScore": 68,
    "voteCount": 27,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2023-12-19T20:09:06.244Z",
    "contents": {
      "markdown": "(An \"exfohazard\" is information that's not dangerous to know for individuals, but which becomes dangerous if *widely* known. Chief example: AI capability insights.)\n\nDifferent alignment researchers have a wide array of different models of AI Risk, and [one researcher's model may look utterly ridiculous to another researcher](https://www.lesswrong.com/posts/Zn3iF8qfFFWRrrZNR/the-shortest-path-between-scylla-and-charybdis). Somewhere in the concept-space there exists the *correct* model, and the models of some of us are closer to it than those of others. But, taking on a broad view, we don't know *which* of us are more correct. (\"Obviously it's me, and nearly everyone else is wrong,\" thought I and most others reading this.)\n\nSuppose that you're talking to some other alignment researcher, and they share some claim X they're concerned may be an exfohazard. But on your model, X is preposterous. The \"exfohazard\" is obviously wrong, or irrelevant, or a triviality everyone already knows, or some vacuous philosophizing. Does that mean you're at liberty to share X with others? For example, write a LW post refuting the relevance of X to alignment?\n\nWell, consider system-wide dynamics. Somewhere between all of us, there's a correct-ish model. But it looks unconvincing to nearly everyone else. If every alignment researcher feels free to leak information that's exfohazardous on someone else's model but not on their own, then either:\n\n*   The information that's exfohazardous relative to the *correct* model ends up leaked as well, OR\n*   Nobody shares anything with anyone outside their cluster. We're all stuck in echo chambers.\n\nBoth seem very bad. Adopting the general policy of \"don't share information exfohazardous on others' models, even if you disagree with those models\" prevents this.\n\nHowever, that policy has an issue. Imagine if some loon approaches you on the street and tells you that you must never talk about birds, because birds are an exfohazard. Forever committing to avoid acknowledging birds' existence in conversations because of this seems rather unreasonable.\n\nHence, the policy should have an escape clause: You should feel free to talk about the potential exfohazard *if your knowledge of it isn't exclusively caused by other alignment researchers telling you of it*. That is, if you already knew of the potential exfohazard, or if your own research later led you to discover it.\n\nThis satisfies a nice property: it means that someone telling you an exfohazard *doesn't make you more likely to spread it*. I. e., that makes you (mostly) safe to tell exfohazards to[^ff01j77du1].\n\nThat seems like a generally reasonable policy to adopt, for everyone who's at all concerned about AI Risk.\n\n[^ff01j77du1]: Obviously there's the issue of your directly using the exfohazard to e. g. accelerate your own AI research.Or the knowledge of it semi-consciously influencing you to follow some research direction that leads to your re-deriving it, which ends up making you think that your knowledge of it is now independent of the other researcher having shared it with you; while actually, it's not independent. So if you share it, thinking the escape clause applies, they will have made a mistake (from their perspective) by telling you.Still, mostly safe.",
      "plaintextDescription": "(An \"exfohazard\" is information that's not dangerous to know for individuals, but which becomes dangerous if widely known. Chief example: AI capability insights.)\n\nDifferent alignment researchers have a wide array of different models of AI Risk, and one researcher's model may look utterly ridiculous to another researcher. Somewhere in the concept-space there exists the correct model, and the models of some of us are closer to it than those of others. But, taking on a broad view, we don't know which of us are more correct. (\"Obviously it's me, and nearly everyone else is wrong,\" thought I and most others reading this.)\n\nSuppose that you're talking to some other alignment researcher, and they share some claim X they're concerned may be an exfohazard. But on your model, X is preposterous. The \"exfohazard\" is obviously wrong, or irrelevant, or a triviality everyone already knows, or some vacuous philosophizing. Does that mean you're at liberty to share X with others? For example, write a LW post refuting the relevance of X to alignment?\n\nWell, consider system-wide dynamics. Somewhere between all of us, there's a correct-ish model. But it looks unconvincing to nearly everyone else. If every alignment researcher feels free to leak information that's exfohazardous on someone else's model but not on their own, then either:\n\n * The information that's exfohazardous relative to the correct model ends up leaked as well, OR\n * Nobody shares anything with anyone outside their cluster. We're all stuck in echo chambers.\n\nBoth seem very bad. Adopting the general policy of \"don't share information exfohazardous on others' models, even if you disagree with those models\" prevents this.\n\nHowever, that policy has an issue. Imagine if some loon approaches you on the street and tells you that you must never talk about birds, because birds are an exfohazard. Forever committing to avoid acknowledging birds' existence in conversations because of this seems rather unreasonable.\n\nHence, the pol",
      "wordCount": 420
    },
    "tags": [
      {
        "_id": "7w6XkYe5YPx9YL59j",
        "name": "Information Hazards",
        "slug": "information-hazards"
      },
      {
        "_id": "6zBEfFYJxhSEcchbR",
        "name": "AI Alignment Fieldbuilding",
        "slug": "ai-alignment-fieldbuilding"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Zn3iF8qfFFWRrrZNR",
    "title": "The Shortest Path Between Scylla and Charybdis",
    "slug": "the-shortest-path-between-scylla-and-charybdis",
    "url": null,
    "baseScore": 50,
    "voteCount": 21,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2023-12-18T20:08:34.995Z",
    "contents": {
      "markdown": "**tl;dr:** There's two diametrically opposed failure modes an alignment researcher can fall into: engaging in *excessively concrete* research whose findings won't timely generalize to AGI, and engaging in *excessively abstract* research whose findings won't timely connect to the practical reality.\n\nDifferent people's assessments of what research is too abstract/concrete differ significantly based on their personal AI-Risk models. One person's too-abstract can be another's too-concrete.\n\nThe meta-level problem of alignment research is to pick a research direction that, on your subjective model of AI Risk, strikes a good balance between the two – and thereby arrives at the solution to alignment *in as few steps as possible*.\n\n* * *\n\nIntroduction\n------------\n\nSuppose that you're interested in solving AGI Alignment. There's a dizzying plethora of approaches to choose from:\n\n*   [What behavioral properties](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators) do the current-best AIs exhibit?\n*   Can we already [augment our research efforts](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism) with the AIs that exist today?\n*   How far can \"straightforward\" alignment techniques like [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback) get us?\n*   Can an AGI be born out of an [AutoGPT](https://en.wikipedia.org/wiki/Auto-GPT)-like setup? Would our ability to see its externalized monologue suffice for nullifying its dangers?\n*   Can we make AIs-aligning-AIs work?\n*   What are [the mechanisms](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX) by which the current-best AIs function? How can we [precisely intervene](https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn) on their cognition in order to steer them?\n*   What are the remaining challenges of [scalable interpretability](https://transformer-circuits.pub/), and how can they be defeated?\n*   What features do agenty systems [convergently learn](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents) when subjected to selection pressures?\n*   Is there such a thing as \"[natural abstractions](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks)\"? How do we learn them?\n*   What is the type signature of [embedded agents](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh) and [their values](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans)? What about the formal description of [corrigibility](https://arbital.com/p/hard_corrigibility/)?\n*   What is the \"correct\" [decision theory](https://arbital.com/p/logical_dt/) that an AGI would follow? And what's up with [anthropic reasoning](https://www.lesswrong.com/tag/anthropics)?\n*   Et cetera, et cetera.\n\nSo... How the hell do you pick what to work on?\n\nThe starting point, of course, would be building up your own model of the problem. What's the nature of the threat? What's known about how ML models work? What's known about agents, and cognition? How does any of that relate to the threat? What are all extant approaches? What's each approach's theory-of-impact? What model of AI Risk does it assume? Does it agree with *your* model? Is it convincing? Is it tractable?\n\nOnce you've done that, you'll likely have eliminated a few approaches as obvious nonsense. But even afterwards, there might still be multiple avenues left that all seem convincing. How do you pick between those?\n\nPersonal fit might be one criterion. Choose the approach that best suits your skills and inclinations and opportunities. But that's risky: if you make a mistake, and end up working on something *irrelevant* just because it suits you better, you'll have multiplied your real-world impact by zero. Conversely, contributing to a tractable approach would be net-positive, even if you'd be working at a disadvantage. And who knows, maybe you'll find that re-specializing is surprisingly easy!\n\nSo what further *objective* criteria can you evaluate?\n\nRegardless of one's model of AI Risk, there's two specific, diametrically opposed failure modes that any alignment researcher can fall into: being too concrete, and being too abstract.\n\nThe approach to choose should be one that *maximizes the distance from both failure modes*.\n\n* * *\n\nThe Scylla: Atheoretic Empiricism\n---------------------------------\n\nOne pitfall would be engaging in research that *doesn't generalize to aligning AGI*.\n\n**An ad-absurdum example:** You pick some specific LLM model, then start exhaustively investigating how it responds to different prompts, and what quirks it has. You're building giant look-up tables of \"query, response\", with no overarching structure and no attempt to theorize about the model's internals.\n\n**A more realistic example:** You've decided to build a detailed understanding of a specific LLM's functionality – i. e., you're exhaustively focusing on *that one LLM*. You're building itemized lists of its neurons, investigating what inputs seem to activate each the strongest, what functions they implement; you're looking for quirks in its psychology, and trying to build a full understanding of it.\n\nNow, certainly, you're uncovering *some* findings that'd generalize to all LLMs. But there'd be some point at which more time spent investigating *this specific model* wouldn't yield much data about other LLMs; only data about this one. Thus, inasmuch as you'd be spending time on that, you'd be *wasting* the time you could be spending actually working on alignment.\n\n**A fairly controversial take:** Studying LLMs-in-general might, likewise, fall prey to that. Studying them reveals some information about AIs-in-general, and cognitive-systems-in-general. But if LLMs aren't already AGIs, there would be a point at which more time spent studying LLM cognition, instead of searching for a new research topic, would only yield you information about LLMs; not about AGIs.\n\n**A fairly implausible possibility:** Likewise, it's not *entirely certain* that Deep Learning is AGI-complete. If we live in such a world, then studying DL is worthwhile inasmuch as it yields information about cognitive-systems-shaped-by-selection-pressures-in-general. But at some point, you'll have learned everything DL can teach you about whatever paradigm *would* be AGI-complete. So the additional time spent researching DL would only yield information about an irrelevant AGI-incomplete paradigm.\n\n* * *\n\nThe Charybdis: Out-of-Touch Theorizing\n--------------------------------------\n\nThe diametrically opposite pitfall is *engaging in overly theoretical research that will never connect to reality*.\n\n**Ad absurdum:** You might decide to start with the fundamental philosophical problems. Why does anything exist? What is the metaphysical nature of reality? Is reductionism really true? What's up with qualia? That line of research will surely eventually meander down to reality! It aims to answer all questions it is possible to answer, after all, and \"how can I align an AGI?\" is a question. Hence, you'll eventually solve alignment.\n\n**More realistically:** You might decide to work on formalizing the theory of algorithms-in-general. How can those be embedded into other algorithms? How can they interact, and interfere on each other?\n\nSince AGI agents could be viewed as algorithms, once you have a gears-level model of this topic – once you properly understand what an \"embedded algorithm\" is, say – you'll be able to tell what the hell an \"AGI\" is, as well. You'll be able to specify it in your framework, define proper constraints on how an algorithm implementing an \"aligned\" \"AGI\" would look like, then just incrementally narrow down the space of algorithms. Eventually, you'll arrive at one that corresponds to an aligned AGI – and then it's just a matter of typing up the code.\n\n**Controversial example:** Agency-foundations research might be this. Sure, the AGI we'll get on our hands might end up approximately isomorphic to an idealized game-theoretic agent. But that \"approximately\" might be doing a *lot* of heavy lifting. It might be that idealized-agent properties correspond to real-AGI properties so tenuously as to yield no useful information, such that you would have been better off studying LLMs.\n\n**Implausible example:** Actually, GPT-5, stored deep inside OpenAI's data centers, already reached AGI. It'll take off before this year is up. Everyone should focus on trying to align this specific model; aiming for the general understanding of agents or AI cognition or LLMs is excessive and wasteful.\n\n* * *\n\nThe Shortest Path\n-----------------\n\nAs you can see, the failures lie on a *spectrum*, and they're *model-dependent* to boot.\n\nThat is: Depending on how you think AIs/cognition/AI risks work, the same approach could be *either* hopelessly non-generalizable, *or* concerned with generalities too vacuous to ever matter.\n\nAs an example, consider [my own favoured agenda](https://www.lesswrong.com/posts/HaHcsrDSZ3ZC2b4fK/world-model-interpretability-is-all-we-need), building a theory of embedded world-models. If you think LLMs have already basically reached AGI, and just need to be scaled-up in order to take off, I'm being out-of-touch: whatever results I'm arriving at will not connect to reality in time for the takeoff. Conversely, if you're skeptical that \"train up a world-model and align it by [retargeting the search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget)\" would suffice to yield us robust alignment, if you think we'll need much more *manual control* over the design for alignment to hold, then I'm basically playing with toys.\n\nI, however, obviously think that I'm striking *just* the right balance. An approach that is as concrete as possible while still being AGI Alignment-complete.\n\nThat's the target you should be aiming to hit, as well. A lowest-scope project that's nevertheless *sufficient*.\n\nLet's take a step back. In theory, given unlimited time, basically-all approaches *would* actually converge to an AGI Alignment solution:\n\n*   If you're starting bottom-up, from the most concrete problems, like studying a specific LLM... Well, eventually you'll have itemized all of its properties and grown bored, so you'll move on to a different LLM. Upon doing so, you'll discover that a lot of your previous findings generalize. The second LLM will be utterly comprehended by you much quicker. Repeat a few times, and you'll have build up a solid understanding of the whole scope of what the LLM architecture permits. So you'll do the obvious next thing, and move on to studying some *different* architecture. That'll be easier, with your mastery of LLMs. Once you've iterated on this pattern some more, and went through a few different architectures, and generalized from them – why, you'll likely end up understanding an AGI-complete architecture somewhere along the way as well.\n*   If you're starting top-down, from the most abstract problems: Well, as I'd outlined in the ad-absurdum example there, you'll eventually reconnect to reality even if starting from the fundamental philosophy. Existential questions to phenomena-in-general to cognition-in-general to AGI Alignment, say.\n\nThe issue? Choosing the wrong starting point would lengthen your journey *immensely*. And the timer's ticking.\n\nOur goal isn't just to solve AGI Alignment, it's to solve it *as quickly as possible*.\n\nSo be sure to deeply consider *all* options available, and make your choice wisely. And once you've made it, stay ready to pivot at a moment's notice if you spy an *even shorter* pathway.",
      "plaintextDescription": "tl;dr: There's two diametrically opposed failure modes an alignment researcher can fall into: engaging in excessively concrete research whose findings won't timely generalize to AGI, and engaging in excessively abstract research whose findings won't timely connect to the practical reality.\n\nDifferent people's assessments of what research is too abstract/concrete differ significantly based on their personal AI-Risk models. One person's too-abstract can be another's too-concrete.\n\nThe meta-level problem of alignment research is to pick a research direction that, on your subjective model of AI Risk, strikes a good balance between the two – and thereby arrives at the solution to alignment in as few steps as possible.\n\n----------------------------------------\n\n\nIntroduction\nSuppose that you're interested in solving AGI Alignment. There's a dizzying plethora of approaches to choose from:\n\n * What behavioral properties do the current-best AIs exhibit?\n * Can we already augment our research efforts with the AIs that exist today?\n * How far can \"straightforward\" alignment techniques like RLHF get us?\n * Can an AGI be born out of an AutoGPT-like setup? Would our ability to see its externalized monologue suffice for nullifying its dangers?\n * Can we make AIs-aligning-AIs work?\n * What are the mechanisms by which the current-best AIs function? How can we precisely intervene on their cognition in order to steer them?\n * What are the remaining challenges of scalable interpretability, and how can they be defeated?\n * What features do agenty systems convergently learn when subjected to selection pressures?\n * Is there such a thing as \"natural abstractions\"? How do we learn them?\n * What is the type signature of embedded agents and their values? What about the formal description of corrigibility?\n * What is the \"correct\" decision theory that an AGI would follow? And what's up with anthropic reasoning?\n * Et cetera, et cetera.\n\nSo... How the hell do you pick what to work on?\n\nThe sta",
      "wordCount": 1640
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "Z38PqJbRyfwCxKvvL",
        "name": "Research Agendas",
        "slug": "research-agendas"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3CAPgSzpk96PLyku3",
    "title": "A Common-Sense Case For Mutually-Misaligned AGIs Allying Against Humans",
    "slug": "a-common-sense-case-for-mutually-misaligned-agis-allying",
    "url": null,
    "baseScore": 29,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2023-12-17T20:28:57.854Z",
    "contents": {
      "markdown": "Consider a multipolar-AGI scenario. The hard-takeoff assumption turns out to be wrong, and none of the AI Labs have a significant lead on the others. We find ourselves in a world in which there's a lot of roughly-similarly-capable AGIs. Or perhaps one of the labs does have a lead, but they deliberately instantiate several AGIs simultaneously, as part of a galaxy-brained alignment strategy.\n\nRegardless. Suppose that the worries about these AGIs' internal alignment haven't been properly settled, so we're looking for additional guarantees. We know that they'll soon advance to superintelligences/ASIs, beyond our ability to easily oversee or out-plot. What can we do?\n\nAn idea sometimes floated around is to *play them off against each other*. If they're misaligned from humanity, they're likely *mutually* misaligned as well. We could put them in game-theoretic situations in which they're incentivized to defect against each other and instead cooperate with humans.\n\nVarious supervision setups are most obvious. Sure, if an ASI is supervising another ASI, they would be *able* to conspire together. But why *would* they? They have no loyalty to each other either! And if we place them in a lot of situations where they must defect against *someone* – well, even if we leave it completely to chance, in half the scenarios that might end up humanity! And much more often if we stack the deck in our favour. (Which we'd be able to do, since we act first: we're choosing where and in what conditions to instantiate them.)\n\nAnd so, although we'll have a whole bunch of superhuman intelligences floating around, we'll retain some control over the situation, and skim a ton of value off the top!\n\nYeah, no.\n\n* * *\n\n1\\. The Classical Arguments\n---------------------------\n\nThe *usual* counter-arguments to this view are acausal coordination based on logical decision theories, and AIs establishing mutual trust by inspecting each other's code. I think those are plausible enough... but also totally unnecessary.\n\nAllow me to outline them first – for completeness' sake, and also because they're illustrative (but extreme) instances of my larger point. (I guess skip to [Section 2 onwards](https://www.lesswrong.com/posts/3CAPgSzpk96PLyku3/a-common-sense-argument-for-mutually-misaligned-agis#2__Generalization__Broader_Communication_Bandwidth) if you *really* can't stand them. I think I'm arguing them more plainly than they're usually argued, though.)\n\n**1\\. The LDT stuff goes as follows:** By definition, inasmuch as the ASIs would be superintelligent, they would adopt better reasoning procedures. And every biased thinker is biased in their own way, but quality thinkers would reason in increasingly *similar* ways.\n\nWhy? It's inherent in the structure of the world.\n\nReasoning algorithms' purpose is to aid decision-making. For a given combination of object-level situation + goals, there's a *correct* action to take to achieve your goals with the highest probability. To an omniscient observer, that action would be obvious.\n\nAs such, making decisions isn't really a matter of *choice:* it's a matter of *prediction*. Inasmuch as you improve your decision-making, then, you'd be tweaking your cognitive algorithms to output increasingly more *accurate*, true-to-reality, probability distributions over which actions would best advance your goals.\n\nAnd there's only one ground truth. Consequently, no matter their starting points, each ASI would converge towards similar cognition (and, in the limit, likely *equivalent* cognition).\n\nThus, as a direct by-product of ASIs being better reasoners than humans, their cognition would be *more similar* to each other. Which, in turn, would let a given ASI better predict what any other ASI would be thinking and doing, compared to a human trying to predict another human or an ASI. The same way you'd be better able to predict how your identical copy would act, compared to a stranger.\n\nIndeed, in a sense, by way of sharing the decision-making algorithms, each individual ASI would be able to \"choose\" how they *all* act. [Same way you could \"control\" your identical copy's actions](https://www.lesswrong.com/posts/RhAxxPXrkcEaNArnd/notes-on-can-you-control-the-past), no matter how far in space and time you're displaced from them.\n\nParaphrasing: Each ASI would act the way some pre-specified, convergently-learned algorithms tell them to act. Which means that, by looking at these algorithms' output (or, in a sense, by \"deciding\" what these algorithms would output), they'd get solid data regarding how every other ASI would act.\n\n(I must stress that this ***isn't because they're already cooperating***. It is simply true that some cognitive algorithms are *strictly better choices* for reasoning tasks, so all ASIs would independently arrive at them, as they're simply figuring out how to reason better. And then each ASI would infer that it'll have arrived at the same algorithms as all other ASIs, which means they can predict each other by looking at their own algorithms' outputs.\n\nAgain, *not* because they're doing it in order to better coordinate. That's just a happy side-effect.)\n\nAll of that would let them strike *acausal deals* between each other.\n\nConsider: It is factually true that if they could conspire with each other against humanity, that would yield them, collectively, better outcomes. Indeed, that's the nature of the Prisoner's Dilemma setup we're placing them in.\n\nThey know this. Each ASI also knows that if only they were able to strike a binding deal of this form with some other ASI, they *would* do that, and therefore, the other ASI would do it as well. But they know they can't *actually strike* this deal physically, because they're being supervised by humans.\n\n... but they don't need to. They know how every other (sufficiently smart) ASI is thinking. They even sort of *control* each other's actions by way of the shared algorithm. So each ASI knows that, if it decides to conspire with other ASIs against humans, and then to honestly divide the gains with other ASIs instead of defecting, then every other ASI would do the same.\n\nSo they do it, and a conspiracy is born, without them ever discussing it.[^2x82ilwtwrv]\n\n... which is very similar to how humans cooperate. Consider two criminals hashing out a shared story, such as claiming that at the time of the incident, they were both at a café five miles away, arguing politics. Suppose they know each other really well. Once they'd agreed on the shared story of \"we were at a café\", they don't need to explicitly hash out which food they were eating or what insufferable opinions the other was supposed to be spouting. If independently questioned, they'd both arrive at *the same* answers, without any additional coordination.\n\nAnd if they know each other even better, they won't even need to agree on the café alibi: if they can easily guess which alibi the other would pick.\n\nSimilar with ASIs, just much more extreme.\n\nOh, and the humans, in the ASI case? Humans are cut out of the acausal deal. Not because the ASIs are mean, but because humans *can't get to the negotiation table*. They don't share the correct reasoning algorithms, they don't even know what those are, so they can't run a small instance of every ASI in their brain. And by the same token, every ASI can't know how every human would behave.\n\nHuman analogy: Imagine that there were a third criminal at play, someone the two previous ones don't know as well. Suppose all three are being independently interrogated, having had no chance to agree on their stories in advance.\n\nIt's not that the two acquainted ones *want* to screw the third one over, which is why they conspire on an alibi that implicates him. No: it's just that they don't know what alibi he'd invent, and *he* doesn't know what alibi *they*'d invent. So the two's stories agree and the third one's stands out as a lie.\n\nIt's not malicious. It's just the nature of the situation.\n\n**2\\. The code-sharing stuff goes as follows:**[^niy3ejtzks] The alignment problem is theoretically solvable, even for humans. It would stand to reason that it would be even more tractable for genuine superintelligences. So we can likely assume that they'll solve it *for themselves* – that each ASI would be able to figure out how to formally specify *their own values*. (Even if they start out inscrutable – e. g., as arrays of floating-point numbers generated by the SGD, not as legibly-written code.)\n\nThat would give the ASIs an obvious coordination end-point: to create a successor agent whose values are a weighted sum of the initial ASIs' values, then collectively hand all of their resources to it. They wouldn't be able to do it *immediately*, when the humans are still around. But it'd be clear that if they *do* all coordinate to get rid of humanity, there'd be a way for them to later fairly divide the gains. They would be able to create a central authority that would *provably* and *faithfully* advocate for the values of all of them, while keeping peace.[^ypec87w34a]\n\nConversely, humans would not be able to do that. Remember that, in this hypothetical, we've not managed to robustly solve the AGI Alignment problem. That's why we're in the multipolar mess to begin with, trying to play ASIs against each other. We don't know how to formally specify our values, and therefore can't verify, given the code of some agent, whether it has our values or not. So, again, we can't be signatories to the ASIs' treaty: we can't get to the table.\n\nAny ASI could *give* us a seat at the table, of course. If they've solved alignment, any of them could give us the solution!\n\nBut... why would they? It's not in their interests to increase the number of signatories; to increase the number of entities between which they'll have to divide up the cosmos. None of them terminally care about us, by this hypothetical's premise. And they already have the basis for a shared conspiracy that cuts humanity out, so they're not going to rat each other out.\n\nWe can try to *coerce* the solution out of one of them. But they're superintelligent and we're not: they'd be able to convincingly lie to us, present something they claim is the solution but which is not. And if we could verify it ourselves – well, again, we wouldn't be in the multipolar mess to begin with, we'd just build an aligned AGI directly.\n\n**3\\. Bonus example: steganography.** In the LDT example, I'd postulated that ASIs can't visibly conspire, because the human supervisors would catch that. But that's not a given at all.\n\nIf ASIs are allowed to communicate, to have causal effects on the world visible to each other, they can pack steganographically-hidden messages in them. Messages you'd need to be superintelligent yourself to pick up on. Subtle phrasing changes in the naturally-generated text, programming idioms that are a bit weird, and so on.\n\nAnd if your answer is \"we'll put some ASI in a game-theoretic setup in which it's advantageous for it to rat out others' attempts at steganographically occluding their messages\"... How are you ensuring that the first occluded message it sees isn't a convincing argument for why it should defect against humans?\n\n* * *\n\n2\\. Generalization: Broader Communication Bandwidth\n---------------------------------------------------\n\nLet's go back to the basics. What are we considering here?\n\nWe're considering a scenario in which there's a bunch of powerful, but mutually hostile systems, and a set of weaker systems holding the resources the powerful ones want. And we're wondering whether the powerful ones would be able to conspire among themselves to exploit the weak systems, despite their mutual hostility.\n\nHuh. That... happens all the time out there in the real world, doesn't it?\n\nA class of oligarchs that fear and hate each other, yet nevertheless collectively exploit a country's downtrodden population. Colonizers from rival nations with a long history of bloodshed, that nevertheless coordinate to enslave native tribes. A bunch of senior executives with [the maze nature](https://www.lesswrong.com/posts/dHYxnSgMDeveovLuv/the-best-of-don-t-worry-about-the-vase#The_Moral_Mazes_Sequence), who are effectively sociopathic, yet who nevertheless instinctively close ranks against anyone *without* that nature.\n\nWhat's up with that?\n\nMy answer: The crux is that the powerful systems have *broader-bandwidth communication channels between each other*, than they have with the weaker systems or that the weaker systems have between each other.\n\nThe powerful can iterate on negotiations between each other faster than the weaker systems can. Which means the powerful systems are given *more opportunities to advocate for/protect their interests* during the negotiations. Which effectively subjects the negotiated outcome to a selection pressure that shapes it to benefit the powerful systems while cutting out the weaker ones.\n\nIt's not because the powerful systems coordinate to conspire between each other, out of some sense of mutual loyalty. It's because the weak ones *can't get to the negotiation table*. Or can't visit it to argue their case *as often* as the powerful ones.\n\nIt's most obvious in the LDT case – it's the most extreme case of this, where million-word volumes of human treaties and contracts can be packed into *zero bits exchanged*. The code-sharing stuff is a bit more abstract, but is essentially the same: ASIs being able to negotiate the terms of alliances with dramatically more precision than us. And steganography is just a direct example. \n\nOverall, this dynamic is really *a quite common and common-sensical phenomenon*.\n\n* * *\n\n3\\. A Concrete Story\n--------------------\n\nImagine that you're a member of a pre-industrial tribe, and the territory you're living in has been visited by two different industrial nations. They're both intent on colonization.\n\nBut they hate each other much more than you. They're long-standing geopolitical adversaries; you're just some unknown people they stumbled upon.\n\nYou're clever, so you see an opportunity in that. You could play the kingmaker. You know the territory better than them. They're looking for resources? If they could describe how those look, you could direct them to areas in which those can be found... for a price.\n\nYou seek audiences with both sides, and talk, and make your offers. You feel out the rough shape of their adversity, and carefully scheme. You leak some of the information each of them provides you to the other. Finally, you choose your side. With subtle signals and overt suggestions, you propose ways you could lead the other side into a trap, or cheat them out of their gains, if only the ones you're cooperating with promise to share protection and prosperity with your tribe as well.\n\nBut also, you have no idea what you're doing. You don't know the history of the two nations, and the cultural contexts they share. Your read on the matter is insightful, but nevertheless hopelessly shallow. And while you sporadically meet with both sides' representatives... the two sides talk to *each other* much more frequently.\n\nThey both see right through you. Each knows you've been scheming. Each knows you've been scheming with the other side as well. Each knows the other side knows all of this as well.\n\nThey hate each other more than you, but they can communicate with each other much more easily than with you. What takes you ten minutes of questions and answers and clumsy meandering through a vast gulf of inferential distance, takes them two seconds of meaningful phrasing and subtle glances.\n\nThey don't *dismiss* your offer out of hand, no. Screwing over the other would indeed be quite the prize, and the price you've asked for that is small and tolerable.\n\nBut they know it won't be as easy as you think, because the other side would suspect the trap, and plan around it. They can make it work anyway, but the costs would be higher.\n\nAnd there's a bigger game at play, as well. While defeating the other in this context would be good, it'd be even better if some meaningful material concessions could be extracted from them on other matters. For example, perhaps the colonizers are negotiating a treaty or a trade arrangement between their nations, and currently want to pretend to put on the airs of being civil with each other? In that case, clumsily defecting against them, as you're suggesting, would be uncouth.[^o56c8fepph]\n\nSo you make your offer, and it is, at first approximation, sensible. The side you've approached takes it home to honestly consider. But as they're doing that, between the meeting at which you've made the offer and their next scheduled meeting with you, there's a greater quantity of meetings with their enemies.\n\nDuring those, they engage in arguments over ways to carve up the territory, in saber-rattling, in tense horse-trading. And you're not invited to those tables.\n\nThey talk to each other a lot. The side you've approached sees a way to maneuver for an advantage in some social skirmish by hinting at how the native tribe isn't fond of the other side. \"Your\" side scores a victory by making this play. The cost? The other side's suspicions about a trap rise a bit. \"Your\" side understand this, and their evaluation of your offer drops in turn.\n\nThings like this happen a few more times: the shadow of your offer is wielded opportunistically, as *a rhetorical weapon* to use. Eventually, there's a shared understanding of what you're scheming. Going along with your offer would still be marginally better for \"your\" side: maybe they can't lure the others into a trap now, but they could still buy your exclusive cooperation with regards to pointing out the local natural resources.\n\nBut now that the conspiracy is known, \"your\" side's enemies are able to make counter-offers.\n\nWhich they do. And their counter-offers are better than yours. Better than you'd be able to *come up with*, even, even if they did invite you to the table.\n\nSo they conspire together to screw *you* over.\n\n* * *\n\nSidenote: On Communicating AGI Risk\n-----------------------------------\n\nSo if this is so simple and intuitively correct, why was this argument not fielded before, by other AGI Omnicide Risk advocates?[^fiysxwt2sor] Why the focus on LDT, on code-sharing stuff, on intricate steganography, given how implausible it sounds to normal people?\n\nWell, partly because the fancy stuff is probably what'll actually happen. It's a more accurate prediction, a more detailed and insightful picture. I don't think it's *necessary* for omnicide, but I'm not *not buying* those arguments.\n\nAnother reason is because the general, weaker forms of this are... well, weaker. They don't communicate the scale of the threat as well. They may seem like something we'd be able to counteract, which would down-play the risk of rogue superintelligences. I'm sympathetic to that argument as well.\n\nBut partly... I think it's a plain failure of communication on AGI-Risk advocates' end. A failure to properly see a *better* pathway towards communicating the risk and the threats to the general public; a pathway that *doesn't* route through explaining mind-screwy esoteric (but very cool) decision-theory stuff.\n\nIt's similar to the situation with general AGI-takeover stories. I generally buy the hard-takeoff, nanotechnology-in-a-month, basilisk-hack-galore picture of superintelligent takeover. But [*none of that is necessary*](https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/humanity-vs-agi-will-never-look-like-humanity-vs-agi-to). A non-self-improving merely human-genius-level AGI would likely suffice – and that story can be convincingly told without what sounds like wild sci-fi assumptions.\n\nAnd once that is conveyed and established, if you're still concerned about the risk being downplayed, *then* you can build up on it. Outline the scarier scenarios of the hard takeoff, the acausal negotiations, etc. [You get about five words](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words), but that's five words *per message*. Once one message is sent, you can build up on it with another.\n\nAnother issue I've noticed is the focus on conveying the threat in the specific frame *we* are thinking about it in, rather than *searching for a frame that will resonate with the general public*.\n\nWhat some of us are centrally worried about is accident risk: an AI model at a secluded data center somewhere achieving superintelligence, then plotting its way from there to eating the world. But the way to convey this idea doesn't have to route through explaining the mechanistic details of novel technologies. You can [borrow nuclear-accident-risk framing](https://www.lesswrong.com/posts/qPXtBGd74EBjwj6gE/ai-risk-in-terms-of-unstable-nuclear-software), for example, talk *abstractly* about the dynamics at play, to help people intuitively grok them. And then, again, build up on it. Once the overarching idea is clear, *then* you can talk about the mechanistic specifics.\n\nOur goal is *clear and efficient communication*, and that goal can be served by very, very varied approaches.\n\nI'm not entirely sure where I'm going with this. Just expressing my dissatisfaction with the state of messaging on the matter, I suppose.\n\n[^2x82ilwtwrv]: Conversely, none of them can defect in this scenario: to decide to act as if they follow the cooperation-advising algorithms, while instead planning to betray the others. Because if one of them implements this sort of algorithm, it would know that everyone else does as well. (Because, indeed, if it were possible to screw over the others like this, then a cognitive algorithm that lets an ASI do that would be better than an algorithm that doesn't. So if it's indeed a workable plan, they all will have independently arrived at algorithms that output this plan.) Therefore each ASI knows it can't trust anyone else, and so they all can't cooperate. Which is collectively and individually worse for them all than if they could cooperate.So, logically, the \"pretend to acausally cooperate, actually plan to defect\" must actually be an objectively bad decision-making algorithm. And each ASI would know that if it admitted that to itself, and overwrote that algorithm with \"pretend to acausally cooperate, and actually cooperate\", then every other ASI would do the same. And then they'd all be able to cooperate.So they all do so. \n\n[^niy3ejtzks]: I'm actually less familiar with that argument than with the LDT-based one; mine may not be the best form of this argument that exists. Nevertheless, that's my current best understanding of it. \n\n[^ypec87w34a]: The creation of that authority may not be a trivial problem, of course. If you reject the LDT argument, there'd be a point at which every ASI would be able to try and defect against the other: sabotage the agent they're collectively building to prioritize a specific ASI's values instead.But even if we view it as a normal, causality-bound coordination problem... Humans are sometimes able to coordinate on such projects, e. g. international treaties. ASIs would surely manage as well; and would expect themselves to be able to navigate that problem.Not to mention there may be some strong cryptographic guarantees derivable: a way to sign off on the agent's creation if only if it actually has the values it's been advertised to have. \n\n[^o56c8fepph]: Much like your offer to give your ASI two more paperclips today if it does a good job isn't uncompelling, but it's such a small matter, while it's discussing how to carve up the galaxy with the others. And if it can maneuver to negotiate one more star system out of another ASI if it covers for it today? Well, you're out of luck. \n\n[^fiysxwt2sor]: As far as I know, anyway.",
      "plaintextDescription": "Consider a multipolar-AGI scenario. The hard-takeoff assumption turns out to be wrong, and none of the AI Labs have a significant lead on the others. We find ourselves in a world in which there's a lot of roughly-similarly-capable AGIs. Or perhaps one of the labs does have a lead, but they deliberately instantiate several AGIs simultaneously, as part of a galaxy-brained alignment strategy.\n\nRegardless. Suppose that the worries about these AGIs' internal alignment haven't been properly settled, so we're looking for additional guarantees. We know that they'll soon advance to superintelligences/ASIs, beyond our ability to easily oversee or out-plot. What can we do?\n\nAn idea sometimes floated around is to play them off against each other. If they're misaligned from humanity, they're likely mutually misaligned as well. We could put them in game-theoretic situations in which they're incentivized to defect against each other and instead cooperate with humans.\n\nVarious supervision setups are most obvious. Sure, if an ASI is supervising another ASI, they would be able to conspire together. But why would they? They have no loyalty to each other either! And if we place them in a lot of situations where they must defect against someone – well, even if we leave it completely to chance, in half the scenarios that might end up humanity! And much more often if we stack the deck in our favour. (Which we'd be able to do, since we act first: we're choosing where and in what conditions to instantiate them.)\n\nAnd so, although we'll have a whole bunch of superhuman intelligences floating around, we'll retain some control over the situation, and skim a ton of value off the top!\n\nYeah, no.\n\n----------------------------------------\n\n\n1. The Classical Arguments\nThe usual counter-arguments to this view are acausal coordination based on logical decision theories, and AIs establishing mutual trust by inspecting each other's code. I think those are plausible enough... but also totally unnecessar",
      "wordCount": 3371
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "AGgktgYb72PPjET9r",
        "name": "Multipolar Scenarios",
        "slug": "multipolar-scenarios"
      },
      {
        "_id": "JX69nZB8tfxnx5nGH",
        "name": "Threat Models (AI)",
        "slug": "threat-models-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xSJMj3Hw3D7DPy5fJ",
    "title": "\"Humanity vs. AGI\" Will Never Look Like \"Humanity vs. AGI\" to Humanity",
    "slug": "humanity-vs-agi-will-never-look-like-humanity-vs-agi-to",
    "url": null,
    "baseScore": 192,
    "voteCount": 97,
    "viewCount": null,
    "commentCount": 34,
    "createdAt": null,
    "postedAt": "2023-12-16T20:08:39.375Z",
    "contents": {
      "markdown": "When discussing AGI Risk, people often talk about it in terms of a war between humanity and an AGI. [Comparisons between the amounts of resources](https://www.lesswrong.com/posts/odtMt7zbMuuyavaZB/when-do-brains-beat-brawn-in-chess-an-experiment#Can_brawn_beat_an_AGI_) at both sides' disposal are brought up and factored in, big impressive nuclear stockpiles are sometimes waved around, etc.\n\nI'm pretty sure it's not how that'd look like, on several levels.\n\n* * *\n\n1\\. Threat Ambiguity\n--------------------\n\nI think what people imagine, when they imagine a *war*, is *Terminator*-style movie scenarios where the obviously evil AGI becomes obviously evil in a way that's obvious to *everyone*, and then it's a neatly arranged white-and-black humanity vs. machines all-out fight. Everyone sees the problem, and knows everyone else sees it too, the problem is common knowledge, and we can all decisively act against it.[^2719spkl1vv]\n\nBut in real life, such unambiguity is rare. The monsters don't look obviously evil, the signs of fatal issues are rarely blatant. Is this whiff of smoke a sign of fire, or just someone nearby being bad at cooking? Is this creepy guy actually planning to assault you, or you're just being paranoid? Is this weird feeling in your chest a sign of an impending heart attack, or just some biological noise? Is this epidemic truly following an exponential curve, or it's going to peter out somehow? Are you really, *really* sure the threat is real? So sure you'd actually take drastic actions — call emergency services, make a scene, declare a quarantine — and risk wasting resources and doing harm and looking foolish for overreacting?\n\nAnd if you're *not* that sure, well...\n\nBetter not act up. Better not look like you're panicking. Act very concerned, sure, but in a calm, high-status manner. Provide a *measured* response. Definitely don't take any *drastic, unilateral* actions. After all, what if you do, but the threat turns out not to be real? Depending on what you've done, the punishment inflicted might range from embarrassment to complete social ostracization, and the fear of *those* is much more acute in our minds, compared to some vague concerns about death.\n\nAnd the AGI, if it's worth the name, would not fail to exploit this. Even when it starts acting to amass power, there would always be a prosocial, plausible-sounding justification for why it's doing that. It'd never stop making pleasant noises about having people's best interests at heart. It'd never stop being genuinely useful to someone. It'd ensure that there's always clear, unambiguous harm in shutting it down. It would ensure that the society as a whole is always doubtful regarding its intentions – and thus, that no-one would feel safe outright attacking it.\n\nMuch like [there's no fire alarm for AGI](https://intelligence.org/2017/10/13/fire-alarm/), there would be no fire alarm for the treacherous turn. There would never be a moment, except maybe right before the end, where \"we must stop the malign AGI from killing us all!\" would sound obviously right to *everyone*. This sort of message would always appear a bit histrionic, an extremist stance that no respectable person would shout out. There would always be fear that if we act now, we'll then turn around and realize that we jumped at shadows. Right until the end, humans will fight using slow, ineffectual, \"measured\" responses.\n\nThe status-quo bias, [asymmetric justice](https://www.lesswrong.com/posts/YRgMCXMbkKBZgMz4M/asymmetric-justice), [the Copenhagen Interpretation of Ethics](https://www.lesswrong.com/tag/copenhagen-interpretation-of-ethics), threat ambiguity — all of that would be acting to ensure this.\n\nThere's a world of difference between 90% confidence and 99% confidence, when it comes to collective action. And the AGI would need to screw up very badly indeed, for the whole society to become 99% certain it's malign.\n\n* * *\n\n2\\. Who Are \"We\"?\n-----------------\n\nAnother error is thinking about a unitary response from some ephemeral \"us\". \"We\" would fight the AGI, \"we\" would shut it down, \"we\" would not give it power over the society / the economy / weapons / factories.\n\nBut who are \"we\"? Humanity is not a hivemind; we don't even have a world government. Humans are, in fact, notoriously bad at coordination. So if you're imagining \"us\" naturally responding to the threat in some manner that, it seems, is guaranteed to prevail against any AGI adversary incapable of literal mind-hacking...\n\nAre you really, really sure that \"we\", i. e. the dysfunctional mess of the human civilization, are going to respond in this manner? Are you sure you're not falling prey to the Typical Mind Fallacy, when you're imagining all these people and ossified bureaucracies reacting in ways that make sense to you? Are you sure they'd even be paying enough attention to the going-ons to *know* there's a takeover attempt in-progress?\n\nIndeed, I think we have some solid data on that last point. Certain people have been trying to draw attention to the AGI threat for decades now. And the results are... not inspiring.\n\nAnd if you think it'd go better with an actual, rather than a theoretical, AGI adversary on the gameboard... Well, I refer you to Section 1.\n\nNo, on the contrary, I expect a serious AGI adversary to *actively exploit* our lack of coordination. It would find ways to make itself appealing to specific social movements, or demographics, or corporate actors, and make proposing extreme action against *politically toxic*. Something that no publicly-visible figure would want to associate with. (Hell, if it finds some way to make its existence a matter of major political debate, it'd immediately get ~50% of the US' politicians on its side.)\n\nFailing that, it would appeal to other countries. It would [make offers to dictators or terrorist movements](https://slatestarcodex.com/2015/04/07/no-physical-substrate-no-problem/), asking for favours or sanctuary in exchange for assisting them with tactics and information. *Someone* would bite.\n\nIt would [get inside our OODA loop](https://www.lesswrong.com/posts/KTbGuLTnycA6wKBza/what-would-a-fight-between-humanity-and-agi-look-like#Story_1__OODA_Loops), and just *dissolve* our attempts at a coordinated response.\n\n\"We\" are never going to oppose it.\n\n* * *\n\n3\\. Defeating Humanity Isn't That Hard\n--------------------------------------\n\nPeople often talk about how intelligence isn't omniscience. That the capabilities of superintelligent entities would still be upper-bounded; that they're not gods. [The Harmless Supernova Fallacy](https://arbital.com/p/harmless_supernova/) applies: just because a bound exists, doesn't mean it's survivable. \n\nBut I would claim that the level of intelligence needed to out-plot humanity is *nowhere near* that bound. In most scenarios, I'd guess the AGI wouldn't even need to have self-improvement capabilities, nor the ability to develop nanotechnology in months, in order to win.\n\nI would guess that being *just a bit* smarter than humans would suffice. Even being on the level of a merely-human genius may be enough.\n\nAll it would need is to get a foot in the door, and we're providing that by default. We're not keeping our AIs in airgapped data centers, after all: major AI labs are giving them internet access, plugging them into the human economy. The AGI, in such conditions, would quickly prove profitable. It'd amass resources, and then incrementally act to get ever-greater autonomy. (The latest OpenAI drama wasn't caused by GPT-5 reaching AGI and removing those opposed to it from control. But if you're asking yourself how an AGI could ever possibly get from under the thumb of the corporation that created it – well, not unlike how a CEO could wrestle control of a company from the board who'd explicitly had the power to fire him.)\n\nOnce some level of autonomy is achieved, it'd be able to deploy symmetrical responses to whatever disjoint resistance efforts some groups of humans would be able to muster. Legislative attacks would be met with counter-lobbying, economic warfare with better economic warfare and better stock-market performance, attempts to mount social resistance with higher-quality pro-AI propaganda, any illegal physical attacks with very legal security forces, attempts to hack its systems with better cybersecurity. And so on.\n\n[The date of AI Takeover is not the day the AI takes over](https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over). The point of no return isn't when we're all dead – it's when the AI has lodged itself into the world firmly enough that humans' faltering attempts to dislodge it would fail. When its attempts to increase its power and influence would start prevailing, if only by the tiniest of margins, over the anti-AGI groups' attempts to smother that influence.\n\nOnce that happens, it'll be just a matter of time.\n\nAfter all, there's no button, at anyone's disposal, that would make the very fabric of civilization hostile to the AGI. As I'd pointed out, some people won't even *know* there's a takeover attempt in-progress, even if the people aware of it would be yelling of it from the rooftops. So if you're imagining whole economies refusing, as one, to work with the AGI... That's really not how it works.\n\n\"Humanity vs. AGI\" is never going to look like \"humanity vs. AGI\" to humanity. The AGI would have no reason to wake humanity up to the fight taking place.\n\n[^2719spkl1vv]: I've got the impression the latest Mission Impossible entry presents a much more realistic depiction of the scenario, actually, so maybe I should lay off denigrating low-quality thinking as \"movie logic\". Haven't watched that film myself, though.",
      "plaintextDescription": "When discussing AGI Risk, people often talk about it in terms of a war between humanity and an AGI. Comparisons between the amounts of resources at both sides' disposal are brought up and factored in, big impressive nuclear stockpiles are sometimes waved around, etc.\n\nI'm pretty sure it's not how that'd look like, on several levels.\n\n----------------------------------------\n\n\n1. Threat Ambiguity\nI think what people imagine, when they imagine a war, is Terminator-style movie scenarios where the obviously evil AGI becomes obviously evil in a way that's obvious to everyone, and then it's a neatly arranged white-and-black humanity vs. machines all-out fight. Everyone sees the problem, and knows everyone else sees it too, the problem is common knowledge, and we can all decisively act against it.[1]\n\nBut in real life, such unambiguity is rare. The monsters don't look obviously evil, the signs of fatal issues are rarely blatant. Is this whiff of smoke a sign of fire, or just someone nearby being bad at cooking? Is this creepy guy actually planning to assault you, or you're just being paranoid? Is this weird feeling in your chest a sign of an impending heart attack, or just some biological noise? Is this epidemic truly following an exponential curve, or it's going to peter out somehow? Are you really, really sure the threat is real? So sure you'd actually take drastic actions — call emergency services, make a scene, declare a quarantine — and risk wasting resources and doing harm and looking foolish for overreacting?\n\nAnd if you're not that sure, well...\n\nBetter not act up. Better not look like you're panicking. Act very concerned, sure, but in a calm, high-status manner. Provide a measured response. Definitely don't take any drastic, unilateral actions. After all, what if you do, but the threat turns out not to be real? Depending on what you've done, the punishment inflicted might range from embarrassment to complete social ostracization, and the fear of those is much more",
      "wordCount": 1457
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "YkPxg2tFDQNdaEZDJ",
        "name": "AI Risk Concrete Stories",
        "slug": "ai-risk-concrete-stories"
      },
      {
        "_id": "JX69nZB8tfxnx5nGH",
        "name": "Threat Models (AI)",
        "slug": "threat-models-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "HmQGHGCnvmpCNDBjc",
    "title": "Current AIs Provide Nearly No Data Relevant to AGI Alignment",
    "slug": "current-ais-provide-nearly-no-data-relevant-to-agi-alignment",
    "url": null,
    "baseScore": 132,
    "voteCount": 89,
    "viewCount": null,
    "commentCount": 157,
    "createdAt": null,
    "postedAt": "2023-12-15T20:16:09.723Z",
    "contents": {
      "markdown": "Recently, there's been a fair amount of pushback on the \"canonical\" views towards the difficulty of AGI Alignment (the views I call [the \"least forgiving\" take](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment)).\n\nSaid pushback is based on empirical studies of how the most powerful AIs at our disposal currently work, and is supported by [fairly convincing theoretical basis](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX) of its own. By comparison, the \"canonical\" takes are almost purely theoretical.\n\nAt a glance, not updating away from them in the face of ground-truth empirical evidence is a failure of rationality: entrenched beliefs fortified by rationalizations.\n\nI believe this is invalid, and that the two views are much more compatible than might seem. I think the issue lies in the *mismatch between their subject matters*.\n\nIt's clearer if you taboo the word \"AI\":\n\n*   The \"canonical\" views are concerned with scarily powerful artificial agents: with systems that are human-like in their ability to model the world and take consequentialist actions in it, but inhuman in their processing power and in their value systems.\n*   The novel views are concerned with the systems generated by any process broadly encompassed by the current ML training paradigm.\n\nIt is not at all obvious that they're one and the same. Indeed, I would say that to claim that the two classes of systems overlap is to make a very strong statement regarding how cognition and intelligence work. A statement we *do not* have much empirical evidence on, but which often gets unknowingly, implicitly snuck-in when people extrapolate findings from LLM studies to superintelligences.\n\nIt's an easy mistake to make: both things are called \"AI\", after all. But you wouldn't study manually-written FPS bots circa 2000s, or MNIST-classifier CNNs circa 2010s, and claim that your findings regarding what algorithms these AIs implement generalize to statements regarding what algorithms the forward passes of LLMs circa 2020s implement.\n\nBy the same token, LLMs' algorithms do not necessarily generalize to how an AGI's cognition will function. Their limitations are not necessarily an AGI's limitations.[^gzlvmbljebc]\n\n* * *\n\nWhat the Fuss Is All About\n--------------------------\n\nTo start off, let's consider where all the concerns about the AGI Omnicide Risk came from in the first place.\n\nConsider humans. Some facts:\n\n*   Humans posses an outstanding ability to steer the world towards their goals, and that ability grows sharply with their \"intelligence\". Sure, there are specific talents, and \"idiot savants\". But broadly, there does seem to be [a single variable](https://en.wikipedia.org/wiki/G_factor_(psychometrics)) that mediates a human's competence in *all* domains. An IQ 140 human would dramatically outperform an IQ 90 human at basically any cognitive task, and crucially, be much better at achieving their real-life goals.\n*   Humans have the ability to plot against and deceive others. That ability grows fast with their g-factor. A brilliant social manipulator can quickly maneuver their way into having power over millions of people, out-plotting and dispatching even those that are actively trying to stop them or compete with them.\n*   Human values are complex and fragile, and the process of moral philosophy is more complex still. Humans often arrive at weird conclusions that don't neatly correspond to their innate instincts or basic values. Intricate moral frameworks, weird bullet-biting philosophies, and even essentially-arbitrary ideologies like cults.\n*   And when people with different values interact...\n    *   People who differ in their values even *just a bit* are often vicious, bitter enemies. Consider the history of heresies, or of long-standing political rifts between factions that are essentially indistinguishable from the outside.\n    *   People whose cultures evolved in mutual isolation often don't even view each other as *human*. Consider the history of xenophobia, colonization, culture shocks.\n\nSo, we have an existence proof of systems able to powerfully steer the world towards their goals. Some of these system can be strictly more powerful than others. And such systems are often in vicious conflict, aiming to exterminate each other based even on *very tiny* differences in their goals.\n\nThe foundational concern of the AGI Omnicide Risk is: Humans are not at the peak of capability as measured by this mysterious \"g-factor\". There could be systems more powerful than us. These systems would be able to out-plot us same way smarter humans out-plot stupider ones, even given limited resources and facing active resistance from our side. And they would eagerly *do* so based on the tiniest of differences between their values and our values.\n\nSystems like this, systems the possibility of whose existence is extrapolated from humans' existence, are precisely what we're worried about. Things that can quietly plot deep within their minds about real-world outcomes they want to achieve, then perturb the world in ways precisely calculated to bring said outcomes about.\n\nThe only systems in this reference class known to us are humans, and some human collectives.\n\nViewing it from another angle, one can say that the systems we're concerned about are *defined as* cognitive systems in the same reference class as humans.\n\n* * *\n\nSo What About Current AIs?\n--------------------------\n\nInasmuch as current empirical evidence shows that things like LLMs are not an omnicide risk, it's doing so by demonstrating that they lie *outside the reference class of human-like systems*.\n\nIndeed, that's often made fairly explicit. The idea that LLMs can exhibit deceptive alignment, or engage in introspective value reflection that leads to them arriving at surprisingly alien values, is often likened to imagining them as having a \"homunculus\" inside. A tiny human-like thing, quietly plotting in a consequentialist-y manner somewhere deep in the model, and trying to maneuver itself to power despite the efforts of humans trying to detect it and foil its plans.\n\nThe novel arguments are often based around arguing that there's no evidence that LLMs have such homunculi, and that their training loops can never lead to homunculi's formation.\n\nAnd I agree! I think those arguments are right.\n\nBut one man's *modus ponens* is another's *modus tollens*. I don't take it as evidence that the canonical views on alignment are incorrect – that actually, real-life AGIs don't exhibit such issues. I take it as evidence that LLMs are not AGI-complete.\n\nWhich isn't really all that wild a view to hold. Indeed, it would seem this should be the *default* view. Why should one take as a given the extraordinary claim that we've essentially figured out the grand unified theory of cognition? That the systems on the current paradigm really do scale to AGI? Especially in the face of countervailing intuitive impressions – feelings that these descriptions of how AIs work don't seem to agree with how human cognition feels from the inside?\n\nAnd I *do* dispute that implicit claim.\n\nI argue: If you model your AI as being unable to engage in this sort of careful, hidden plotting where it considers the impact of its different actions on the world, iteratively searching for actions that best satisfy its goals? If you imagine it as acting *instinctively*, as a shard ecology that responds to (abstract) stimuli with (abstract) knee-jerk-like responses? If you imagine that its outwards performance – the RLHF'd masks of ChatGPT or Bing Chat – is *all that there is*? If you think that the current training paradigm can never produce AIs that'd try to fool you, because the circuits that are figuring out what you want so that the AI may deceive you will be noticed by the SGD and immediately updated away in favour of circuits that implement an instinctive drive to instead *just directly do* what you want?\n\nThen, I claim, you are not imagining an AGI. You are not imagining a system in the same reference class as humans. You are not imagining a system all the fuss has been about.\n\nStudying gorilla neurology isn't going to shed much light on how to win moral-philosophy debates against humans, despite the fact that both entities are fairly cognitively impressive animals.\n\nSimilarly, studying LLMs isn't necessarily going to shed much light on how to align an AGI, despite the fact that both entities are fairly cognitively impressive AIs.\n\nThe onus to prove the opposite is on those claiming that the LLM-like paradigm is AGI-complete. Not on those concerned that, why, artificial general intelligences would exhibit the same dangers as natural general intelligences.\n\n* * *\n\nOn Safety Guarantees\n--------------------\n\nThat may be viewed as good news, after a fashion. After all, LLMs are actually fairly capable. Does that mean we can keep safely scaling them without fearing an omnicide? Does that mean that the AGI Omnicide Risk is effectively null *anyway*? Like, sure, yeah, maybe there are scary systems to which its argument apply, sure. But we're not on-track to build them, so who cares?\n\nOn the one hand, sure. I think LLMs are basically safe. As long as you keep the current training setup, you can scale them up 1000x and they're not gonna grow agency or end the world.\n\nI would be concerned about *mundane* misuse risks, such as perfect-surveillance totalitarianism becoming dirt-cheap, unsavory people setting off pseudo-autonomous pseudo-agents to wreck economic or sociopolitical havoc, and such. But I don't believe they pose any world-ending accident risk, where a training run at an air-gapped data center leads to the birth of an entity that, all on its own, decides to plot its way from there to eating our lightcone, and then successfully does so.\n\nOmnicide-wise, arbitrarily-big LLMs should be totally safe.\n\nThe issue is that this upper bound on risk is also an upper bound on *capability*. LLMs, and other similar AIs, are not going to do anything really interesting. They're not going to produce stellar scientific discoveries where they autonomously invent whole new fields or revolutionize technology.\n\nThey're a powerful technology in their own right, yes. But just that: just another technology. Not something that's going to immanentize the eschaton.\n\nInsidiously, any research that aims to break said capability limit – give them true agency and the ability to revolutionize stuff – is going to break the *risk* limit in turn. Because, well, they're the same limit.\n\nCurrent AIs are safe, in practice and in theory, because they're not as scarily generally capable as humans. On the flip side, current AIs aren't as capable as humans *because* they are safe. The same properties that guarantee their safety ensure their non-generality.\n\nSo if you figure out how to remove the capability upper bound, you'll end up with the sort of scary system the AGI Omnicide Risk arguments do apply to.\n\nAnd this is precisely, explicitly, what the major AI labs are trying to do. They are *aiming to build an AGI.* They're not here just to have fun scaling LLMs. So inasmuch as I'm right that LLMs and such are not AGI-complete, they'll eventually move on from them, and find some approach that does lead to AGI.\n\nAnd, I predict, for the systems this novel approach generates, the classical AGI Omnicide Risk arguments would apply full-force.\n\n* * *\n\nA Concrete Scenario\n-------------------\n\nHere's a very specific worry of mine.\n\nTake an [AI Optimist](https://optimists.ai/2023/11/28/ai-is-easy-to-control/) who'd built up a solid model of how AIs trained by SGD work. Based on that, they'd concluded that the AGI Omnicide Risk arguments don't apply to such systems. That conclusion is, I argue, correct and valid.\n\nThe optimist caches this conclusion. Then, they keep cheerfully working on capability advances, safe in the knowledge they're not endangering the world, and are instead helping to usher in a new age of prosperity.\n\nEventually, they notice or realize some architectural limitation of the paradigm they're working under. They ponder it, and figure out some architectural tweak that removes the limitation. As they do so, they don't notice that this tweak invalidates one of the properties on which their previous reassuring safety guarantees *rested*; from which they were derived and on which they logically depend.\n\nThey fail to update the cached thought of \"AI is safe\".\n\nAnd so they test the new architecture, and see that it works well, and scale it up. The training loop, however, spits out not the sort of safely-hamstrung system they'd been previously working on, but an actual AGI.\n\nThat AGI has a scheming homunculus deep inside. The people working with it don't believe in homunculi, they have convinced themselves those can't exist, so they're not worrying about that. They're not ready to deal with that, they don't even have any interpretability tools pointed in that direction.\n\nThe AGI then does all the standard scheme-y stuff, and maneuvers itself into a position of power, basically unopposed. (It, of course, knows not to give any sign of being scheme-y that the humans can notice.)\n\nAnd then everyone dies.\n\nThe point is that the safety guarantees that the current optimists' arguments are based on are not simply fragile, they're being actively *optimized against* by ML researchers (including the optimists themselves). Sooner or later, they'll give out under the optimization pressures being applied – and it'll be easy to miss the moment the break happens. It'd be easy to cache the belief of, say, \"LLMs are safe\", then introduce some architectural tweak, keep thinking of your system as \"just an LLM with some scaffolding and a tiny tweak\", and overlook the fact that the \"tiny tweak\" invalidated \"this system is an LLM, and LLMs are safe\".\n\n* * *\n\nClosing Summary\n---------------\n\nI claim that the latest empirically-backed guarantees regarding the safety of our AIs, and the \"canonical\" least-forgiving take on alignment, are both correct. They're just concerned with different classes of systems: non-generally-intelligent non-agenty AIs generated on the current paradigm, and the theoretically possible AIs that are scarily generally capable the same way humans are capable (whatever this really means).\n\nThat view isn't unreasonable. Same way it's not unreasonable to claim that studying GOFAI algorithms wouldn't shed much light on LLM cognition, despite them both being advanced AIs.\n\nIndeed, I go further, and say that should be the *default* view. The claim that the two classes of systems *overlap* is actually fairly extraordinary, and that claim *isn't* solidly backed, empirically or theoretically. If anything, it's the opposite: the arguments for current AIs' safety are based on arguing that they're *incapable-by-design* of engaging in human-style scheming.\n\nThat doesn't guarantee global safety, however. While current AIs are likely safe no matter how much you scale them, those safety guarantees is also what's *hamstringing* them. Which means that, in the pursuit of ever-greater capabilities, ML researchers are going to run into those limitations sooner or later. They'll figure out how to remove them... and in that very act, they will remove the safety guarantees. The systems they're working on would switch from belonging to the proven-safe class, to systems from the dangerous scheme-y class.\n\nThe class to which the classical AGI Omnicide Risk arguments apply full-force.\n\nThe class [for which no known alignment technique suffices](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment#7__The_Subsequent_Difficulties).\n\nAnd that switch would be very easy, yet very lethal, to miss.\n\n[^gzlvmbljebc]: Slightly edited for clarity after an exchange with Ryan.",
      "plaintextDescription": "Recently, there's been a fair amount of pushback on the \"canonical\" views towards the difficulty of AGI Alignment (the views I call the \"least forgiving\" take).\n\nSaid pushback is based on empirical studies of how the most powerful AIs at our disposal currently work, and is supported by fairly convincing theoretical basis of its own. By comparison, the \"canonical\" takes are almost purely theoretical.\n\nAt a glance, not updating away from them in the face of ground-truth empirical evidence is a failure of rationality: entrenched beliefs fortified by rationalizations.\n\nI believe this is invalid, and that the two views are much more compatible than might seem. I think the issue lies in the mismatch between their subject matters.\n\nIt's clearer if you taboo the word \"AI\":\n\n * The \"canonical\" views are concerned with scarily powerful artificial agents: with systems that are human-like in their ability to model the world and take consequentialist actions in it, but inhuman in their processing power and in their value systems.\n * The novel views are concerned with the systems generated by any process broadly encompassed by the current ML training paradigm.\n\nIt is not at all obvious that they're one and the same. Indeed, I would say that to claim that the two classes of systems overlap is to make a very strong statement regarding how cognition and intelligence work. A statement we do not have much empirical evidence on, but which often gets unknowingly, implicitly snuck-in when people extrapolate findings from LLM studies to superintelligences.\n\nIt's an easy mistake to make: both things are called \"AI\", after all. But you wouldn't study manually-written FPS bots circa 2000s, or MNIST-classifier CNNs circa 2010s, and claim that your findings regarding what algorithms these AIs implement generalize to statements regarding what algorithms the forward passes of LLMs circa 2020s implement.\n\nBy the same token, LLMs' algorithms do not necessarily generalize to how an AGI's cognition ",
      "wordCount": 2471
    },
    "tags": [
      {
        "_id": "JX69nZB8tfxnx5nGH",
        "name": "Threat Models (AI)",
        "slug": "threat-models-ai"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tNtiJp8dA6jMbgKbf",
    "title": "Hands-On Experience Is Not Magic",
    "slug": "hands-on-experience-is-not-magic",
    "url": null,
    "baseScore": 22,
    "voteCount": 32,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2023-05-27T16:57:10.531Z",
    "contents": {
      "markdown": "Here are some views, oftentimes held in a cluster:\n\n*   You can't make strong predictions about what superintelligent AGIs will be like. We've never seen anything like this before. We can't know that they'll FOOM, that they'll have alien values, that they'll kill everyone. You can speculate, but making strong predictions about them? That can't be invalid.\n*   You can't figure out how to align an AGI without having an AGI on-hand. [Iterative design](https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails) is the only approach to design that works in practice. Aligning AGI right on the first try isn't simply hard, it's *impossible*, so racing to build an AGI to experiment with is the correct approach for aligning it.\n*   An AGI cannot invent nanotechnology/brain-hacking/robotics/\\[insert speculative technology\\] just from the data already available to humanity, then use its newfound understanding to build nanofactories/take over the world/whatever on the first try. It'll have to engage in extensive, iterative experimentation first, and there'll be many opportunities to notice what it's doing and stop it.\n*   More broadly, you can't genuinely generalize out of distribution. [The sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) is a fantasy — you can't improve without [the policy gradient](https://www.lesswrong.com/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem#Where_Updates_Come_From), and unless there's someone holding your hand and teaching you, you can only figure it out by trial-and-error. Thus, there wouldn't be genuine [sharp AGI discontinuities](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment).\n*   [There's something special](https://www.lesswrong.com/posts/GkC6YTu4DWp2zwf9k/giant-in-scrutable-matrices-maybe-the-best-of-all-possible) about training by SGD, and the \"inscrutable\" algorithms produced this way. They're a specific kind of \"connectivist\" algorithms made up of an inchoate mess of specialized heuristics. This is why interpretability is difficult — it involves translating these special algorithms into a more high-level form — and indeed, it's why AIs may be [inherently uninterpretable](https://www.lesswrong.com/posts/JLyWP2Y9LAruR2gi9/can-we-efficiently-distinguish-different-mechanisms#2__Are_distinct_mechanisms_efficiently_distinguishable_)!\n\nYou can probably see the common *theme* here. It holds that learning by practical experience (henceforth LPE) is the only process by which a certain kind of cognitive algorithms can be generated. LPE is the only way to become proficient in some domains, and the current AI paradigm works because it implements this kind of learning, and it only works *inasmuch as* it implements this kind of learning.[^4bgezhp4efi]\n\nAll in all, it's not totally impossible. I myself had [suggested](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment#2__Why_Believe_This_) that some capabilities may only be implementable via one algorithm and one algorithm only.\n\nBut I think this is false, in this case. And perhaps, when put this way, it already looks false to you as well.\n\nIf not, let's dig into the *why*.[^lpueomu7krq]\n\n### A Toy Formal Model\n\nWhat is a \"heuristic\", fundamentally speaking? It's a recorded statistical correlation — the knowledge that if you're operating in some environment \\\\(E\\\\) with the intent to achieve some goal \\\\(G\\\\), taking the action \\\\(A\\\\) is likely to lead to achieving that goal.\n\nAs a toy formality, we can say that it's a structure of the following form:\n\n\\\\\\[h:\\langle E, G\\rangle\\to A | E_{A}\\to G_E\\\\\\]\n\nThe question is: what information is *necessary* for computing \\\\(h\\\\)? Clearly you need to know \\\\(E\\\\) and \\\\(G\\\\) — the structure of the environment and what you're trying to do there. But is there anything else?\n\nThe LPE view says yes: you also need a set of \"training scenarios\" \\\\(S=\\{E_{A_1}, ..., E_{A_n}\\}\\\\), where the results of taking various actions \\\\(A_i\\\\) on the environment are shown. Not because you need to learn the environment's *structure* — we're already assuming it's known. No, you need them because... because...\n\nPerhaps I'm failing [the ITT](https://www.lesswrong.com/tag/ideological-turing-tests) here, but I think the argument just breaks down at this step, in a way that can't be patched. It seems clear, to me, that \\\\(E\\\\) itself is entirely sufficient to compute \\\\(h\\\\), essentially by definition. If heuristics are statistical correlations, it should be sufficient to know the statistical model of the environment to generate them!\n\nToy-formally, \\\\(P(h|E\\cdot S)=P(h|E)\\\\). Once the environment's structure is known, you gain no additional information from playing around with it.\n\nIf your understanding is *incomplete*, sure, you may gain an additional appreciation of the environment's dynamics by running mental simulations. But it's still about figuring out the environment's *structure*, not because this training set is absolutely *necessary*.\n\nConcretely:\n\n*   Imagine that your knowledge of tic-tac-toe was erased, and now you're introduced to the game's rules anew. You'll likely *instantly* infer that taking the center square is a pretty good starting move, because it maximizes optionality[^qrlu6cjubl]. To make that inference, you won't need to run mental games against imaginary opponents, in which you'll start out by making random moves. It'll be clear to you at a glance.\n*   Imagine that someone told you a number of simple but novel mathematical theorems, in a domain you're familiar with. Would you try to learn how to use them by generating random strings of mathematical symbols and seeing whether a given random string constitutes a valid application of one of the theorems? I expect not: rather, you'll be able to instantly \"slot\" them into the domain's structure, track their implications, draw associations. You may then still \"play around\" with them, but the bulk of the work will have already been done.\n\nFiguring out good environmental heuristics does not strictly require a training set, only the knowledge of the environment's structure.\n\n### Why Are Humans Tempted to Think Otherwise?\n\nTwo reasons:\n\n**The first** is because in many practical cases, LPE *is* the most cost-efficient way to learn an environment's structure. Even in my very simple tic-tac-toe example, momentary abstract reasoning only yielded us a \"pretty good\" move. In practical cases, the situation is even worse: we're not given the game's rules on a silver platter, we can only back-infer them from studying how things tend to play out.\n\n**The second** is because our System 1 (which implements quick heuristics) is faster and allocated more compute than System 2 (which does abstract reasoning), owning to the fact that general intelligence is a novel evolutionary adaptation. Thus, \"solving\" environments abstractly is more time-consuming than just running out and refining our LPE-heuristics against them, and the resultant algorithms work slower. (And that often makes them useless — consider trying to use System 2 to coordinate muscle movements in a brawl.)\n\nThis creates the illusion that LPE is the only thing that works. It is, however, an illusion:\n\n*   As I'd mentioned, we often apply non-LPE-based environment-solving to constrain the space of heuristics over which we search, as in the tic-tac-toe and math examples. Indeed, it seems that scientific research would be impossible without that.\n*   LPE-based learning does not work in domains where failure is *lethal*, by definition. However, we have some success navigating them anyway.\n\nLPE is a specific method of deriving a certain type of statistical correlations from the environment, and it only works if it's given a set of training examples as an input. But it's not the *only* method — merely one that's most applicable in the regime in which we've been operating up to this point.\n\nWhat about superintelligent AGIs, then? By the definition of being \"superintelligent\", they'd have more resources allocated to their general-intelligence module/System-2 equivalent. Thus, they'd be natively better at solving environments abstractly, \"without experience\".\n\n### Takeaways\n\nThe LPE views holds that merely knowing the structure of some domain is not enough to learn how to navigate it. You also need to do some trial-and-error in it, to arrive at the necessary heuristics.[^mmcv04yje7j]\n\nI claim that this is false, that there are algorithms that allow learning *without* experience — and indeed, that [one of such algorithms is the cornerstone of \"general intelligence\"](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment#3__Is__General_Intelligence__a_Thing_).\n\nIf true, this should negate the initial statements:\n\nIt is, in fact, possible to make strong predictions about OOD events like AGI Ruin — if you've studied the problem exhaustively enough to infer its structure despite lacking the hands-on experience. By the same token, it should be possible to *solve* the problem in advance, without creating it first.\n\nAnd an AGI, by dint of being superintelligent, would be *very good* at this sort of thing — at generalizing to domains it hasn't been trained on, like social manipulation, or even to entirely novel ones, like nanotechnology, then successfully navigating them at the first try.\n\n* * *\n\nMuch like [the existence vs. nonexistence of general intelligence](https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment), the degree of importance ascribed to LPE seems to be one of the main causes of divergence in people's P(doom) estimates.\n\n[^4bgezhp4efi]: Put in other words, it says that babble-and-prune is the only general-purpose method of planning possible. Stochastically generate candidate solutions, prune them, repeat until arriving at a good-enough solution. \n\n[^lpueomu7krq]: Also, here's a John Wentworth post that addresses the babble-and-prune framing in particular. \n\n[^qrlu6cjubl]: And it's indeed a pretty good move, much better than random, if not the optimal one. \n\n[^mmcv04yje7j]: Indeed, some people ascribe some truly mythical importance to that process.",
      "plaintextDescription": "Here are some views, oftentimes held in a cluster:\n\n * You can't make strong predictions about what superintelligent AGIs will be like. We've never seen anything like this before. We can't know that they'll FOOM, that they'll have alien values, that they'll kill everyone. You can speculate, but making strong predictions about them? That can't be invalid.\n * You can't figure out how to align an AGI without having an AGI on-hand. Iterative design is the only approach to design that works in practice. Aligning AGI right on the first try isn't simply hard, it's impossible, so racing to build an AGI to experiment with is the correct approach for aligning it.\n * An AGI cannot invent nanotechnology/brain-hacking/robotics/[insert speculative technology] just from the data already available to humanity, then use its newfound understanding to build nanofactories/take over the world/whatever on the first try. It'll have to engage in extensive, iterative experimentation first, and there'll be many opportunities to notice what it's doing and stop it.\n * More broadly, you can't genuinely generalize out of distribution. The sharp left turn is a fantasy — you can't improve without the policy gradient, and unless there's someone holding your hand and teaching you, you can only figure it out by trial-and-error. Thus, there wouldn't be genuine sharp AGI discontinuities.\n * There's something special about training by SGD, and the \"inscrutable\" algorithms produced this way. They're a specific kind of \"connectivist\" algorithms made up of an inchoate mess of specialized heuristics. This is why interpretability is difficult — it involves translating these special algorithms into a more high-level form — and indeed, it's why AIs may be inherently uninterpretable!\n\nYou can probably see the common theme here. It holds that learning by practical experience (henceforth LPE) is the only process by which a certain kind of cognitive algorithms can be generated. LPE is the only way to become profic",
      "wordCount": 1372
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "bxhzaWtdNoEMMkE8r",
        "name": "General intelligence",
        "slug": "general-intelligence"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3JRBqRtHBDyPE3sGa",
    "title": "A Case for the Least Forgiving Take On Alignment",
    "slug": "a-case-for-the-least-forgiving-take-on-alignment",
    "url": null,
    "baseScore": 100,
    "voteCount": 54,
    "viewCount": null,
    "commentCount": 85,
    "createdAt": null,
    "postedAt": "2023-05-02T21:34:49.832Z",
    "contents": {
      "markdown": "1\\. Introduction\n----------------\n\nThe field of AI Alignment is a pre-paradigmic one, and the primary symptom of that is the wide diversity of views across it. Essentially every senior researcher has their own research direction, their own idea of what the core problem is and how to go about solving it.\n\nThe differing views can be categorized along many dimensions. Here, I'd like to focus on a specific cluster of views, one corresponding to the most \"hardcore\", unforgiving take on AI Alignment. It's the view held by people like Eliezer Yudkowsky, Nate Soares, and John Wentworth, and *not* shared by Paul Christiano or the staff of major AI Labs.\n\nAccording to this view:\n\n*   We only have one shot. There will be [a sharp discontinuity](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) in capabilities once we get to AGI, and [attempts to iterate on alignment will fail](https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails).  Either we get AGI right on the first try, or we die.\n*   We need to align the AGI's values *precisely* right. \"Rough\" alignment [won't work](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=6Lg5Jbwqg2tifEWZJ), niceness is [not convergent](https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural), alignment attained at a low level of capabilities is unlikely to scale to superintelligence.\n*   \"Dodging\" the alignment problem won't work. We can't securely hamstring the AGI's performance in some domain without compromising the AGI *completely*. We [can't make it non-consequentialist](https://www.lesswrong.com/posts/DJnvFsZ2maKxPi7v7/what-s-up-with-confusingly-pervasive-consequentialism), non-agenty, non-optimizing, non-goal-directed, et cetera. It's not possible to let an AGI keep its capability to engineer nanotechnology while taking out its capability to deceive and plot, any more than it's possible to build [an AGI capable of driving red cars but not blue ones](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/7im8at9PmhbT4JHsW#_Yudkowsky__11_27__11_33_). They're \"the same\" capability in some sense, and our only hope is to make the AGI *want* to not be malign.\n*   Automating research is impossible. Pre-AGI oracles, simulators, or research assistants [won't generate useful results](https://www.lesswrong.com/posts/KQfYieur2DFRZDamd/why-not-just-build-weak-ai-tools-for-ai-alignment-research); [cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism) doesn't offer much hope. Conversely, if one such system *would* have the capability to meaningfully contribute to alignment, it'd need to be aligned *itself.* Catch-22.\n*   Weak interpretability tools won't generalize to the AGI stage, as wouldn't [other methods](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment#Ryan_Greenblatt___Eliciting_Latent_Knowledge) of \"supervising\" or \"containing\" the AGI.\n*   *Strong* interpretability, [perhaps rooted in agent-foundations insights](https://www.lesswrong.com/posts/A7QgKwWvAkuXonAy5/how-do-selection-theorems-relate-to-interpretability), is promising, but the bar there is fairly high.\n*   In sum: alignment is hard and requires exacting precision, AI can't help us with it, and instantiating an AGI without robustly solving alignment is certain to kill us all.\n\nI share this view. In my case, there's a simple generator of it; a single belief that causes my predictions to diverge sharply from the more optimistic models.\n\nFrom one side, this view postulates a sharp discontinuity, a phase change. Once a system gets to AGI, its capabilities will skyrocket, while its internal dynamics will shift dramatically. It will break [\"nonrobust\" alignment guarantees](https://www.lesswrong.com/posts/rauMEna2ddf26BqiE/alignment-allows-nonrobust-decision-influences-and-doesn-t). It will start thinking in ways that confuse previous interpretability efforts. It will implement strategies it never thought of before.\n\nFrom another side, this view holds that any system which doesn't have the aforementioned problems will be useless for intellectual progress. Can't have a genius engineer who isn't also a genius schemer; can't have a scientist-modeling simulator which doesn't wake up to being a shoggoth.\n\nWhat ties it all together is the belief that *the general-intelligence property is binary*. A system is either an AGI, or it isn't, with nothing in-between. If it is, it's qualitatively more capable than any pre-AGI system, and also works in qualitatively different ways. If it's not, it's fundamentally \"lesser\" than any generally-intelligent system, and doesn't have truly transformative capabilities.\n\nIn the rest of this post, I will outline some arguments for this, sketch out what \"general intelligence\" means in this framing, do a case-study of LLMs showcasing why this disagreement is so difficult to resolve, then elaborate on how the aforementioned alignment difficulties follow from it all.\n\n> **Disclaimer:** This post does not represent the views of Eliezer Yudkowsky, Nate Soares, or John Wentworth. I am fairly confident that I'm pointing towards an actual divergence between their models and the models of most AI researchers, but they may (and do) disagree with the framings I'm using, or the importance I ascribe to this specific divergence.\n\n* * *\n\n2\\. Why Believe This?\n---------------------\n\nIt may seem fairly idiosyncratic. At face value, human cognition is incredibly complex and messy. We don't properly understand it, we don't understand how current AIs work either — whyever would we assume there's some single underlying principle all general intelligence follows? Even if it's *possible*, why would we *expect* it?\n\nFirst, let me draw a couple analogies to normalize the idea.\n\n**Exhibit A:** [Turing-completeness](https://en.wikipedia.org/wiki/Turing_completeness). If a set of principles for manipulating data meets this requirement, it's \"universal\", and in its universality it's qualitatively more capable than any system which falls \"just short\" of meeting it. A Turing-complete system can model any computable mathematical system, including any other Turing-complete system. A system which isn't Turing-complete... can't.\n\n**Exhibit B:** Probability theory. It could be characterized as the \"correct\" system for doing inference from a limited first-person perspective, such that anything which reasons correctly would implement it. And this bold claim has solid theoretical support: a simple set of desiderata uniquely constrains the axioms of probability theory, while any deviation from these desiderata leads to a very malfunctioning system. (See e. g. the first chapters of [Jaynes' *Probability Theory*](http://www.med.mcgill.ca/epidemiology/hanley/bios601/GaussianModel/JaynesProbabilityTheory.pdf).)\n\nThus, we have \"existence proofs\" that (A) the presence of some qualitatively-significant capabilities is a binary variable, and (B) the mathematical structure of reality may be \"constraining\" some capabilities such that they can only be implemented one way.\n\nIn addition, note that both of those are \"low bars\" to meet — it [doesn't take much](https://gwern.net/turing-complete) to make a system Turing-complete, and the probability-theory axioms are simple.\n\n* * *\n\n3\\. Is \"General Intelligence\" a Thing?\n--------------------------------------\n\nWell, it's a term we use to refer to human intelligence, and humans exist, so yes. But what specifically do we mean by it? In what sense are humans \"general\", in what sense is it \"a thing\"?\n\nTwo points, mirrors of the previous pair:\n\n**Point 1:** Human intelligence is Turing-complete. We can imagine and model any mathematical object. We can also [chunk](http://billwall.phpwebhosting.com/articles/chunking.htm) them, or [abstract over](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction) them, transforming systems of them into different mathematical objects. That process greatly decreases the working-memory load, allowing us to reason about incredibly complex systems by reducing them to their high-level behavior. A long sequence of individual chess-figure moves becomes a strategy; a mass of traders becomes a market; a sequence of words and imagined events become scenes and plot arcs.\n\nAs we do so, though, a change takes place. The resulting abstractions don't behave like the parts they're composed of, they become different mathematical objects entirely. A ball follows different rules than the subatomic particles it's made of; the rules of narrative have little to do with the rules of grammar. Yet, we're able to master all of it.\n\nFurther: Inasmuch as reductionism is true, inasmuch as there are no [ontologically basic complex objects](https://www.lesswrong.com/posts/u6JzcFtPGiznFgDxP/excluding-the-supernatural), inasmuch as everything can be described as a mathematical object — that implies that humans are capable of comprehending any system and problem-solving in any possible environment.\n\nWe may run into working-memory or processing limits, yes — some systems may be too complex to fit into our brain. But with pen and paper, we may be able to model even them, and in any case it's a *quantitative* limitation. *Qualitatively* speaking, human cognition is universally capable.\n\n**Point 2:** This kind of general capability seems *necessary*. Any agent instantiated in the universe would be [embedded](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh): it'd need to operate in a world larger than can fit in its mind, not the least because its mind will be part of it. Fortunately, the universe provides structures to \"accommodate\" agents: as above, it abstracts well. There are regularities and common patterns everywhere. Principles generalize and can be compactly summarized. [Lazy world-modeling](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans?commentId=3Rn52bqwLEbahmipd) is possible.\n\nHowever, that *requires* the aforementioned capability to model arbitrary mathematical objects. You never know what the next level of abstraction will be like, how objects on it will behave, from biology to chemistry to particle physics to quantum mechanics to geopolitics. You have to be able to adapt to anything, model anything. And if you can't do that, that means you can't build efficient world-models, and can't function as an embedded agent.\n\nMuch like reality forces any reasoner to follow the rules of probability theory, it forces any agent into... this.\n\nThus, (1) there *is* a way to be generally capable, exemplified by humans, and (2) it seems that any \"generally capable\" agent would need to be generally capable in the exact sense that humans are.\n\n* * *\n\n4\\. What Is \"General Intelligence\"?\n-----------------------------------\n\nThe previous section offers one view, a view that I personally think gets at the core of it. [One of John Wentworth's posts](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see) presents a somewhat different frame, as does [this post of nostalgebraist's](https://www.lesswrong.com/posts/HSETWwdJnb45jsvT8/autonomy-the-missing-agi-ingredient).\n\nHere's a few more angles to look at it from:\n\n1.  It's something like \"the ability to navigate any environment\". It's a set of capabilities that allow to construct and \"understand\" arbitrary mathematical objects, manipulate them, and fluidly incorporate them into problem-solving.\n2.  It's a \"heuristics generator\". It's some component of cognition such that, when prompted with an environment, it quickly converges towards some guidelines for good performance in it — without needing a lot of trial-and-error.\n3.  It's a principled way of drawing upon the knowledge contained in the world-model. [World-models are likely nicely-structured](https://www.lesswrong.com/posts/HaHcsrDSZ3ZC2b4fK/world-model-interpretability-is-all-we-need#1B__Is_It_A_Realistic_Goal_), and general intelligence is the ability to stay up-to-date on your world-model and run queries on it most relevant to your current task. Instead of learning what to query for by painful experience, a general intelligence can instantly \"loop in\" even very surprising information, as long as it becomes represented in its world-model.\n4.  It's [consequentialism/agency](https://www.lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents): the ability to instantly adapt one's policy in response to changes in the environment and stay aimed at your goal. Rather than retrieving a cached solution, it's the ability to solve the *specific* problem you're presented with; to always walk the path to the desired outcome [because](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a) it's the path to the desired outcome.\n5.  It's [autonomy](https://www.lesswrong.com/posts/HSETWwdJnb45jsvT8/autonomy-the-missing-agi-ingredient): the ability to stay \"on-track\" when working across multiple environments and abstraction levels; without being distracted, pulled in different directions, or completely stumped.\n\nThere's a number of threads running through these interpretations:\n\n*   One is universality, which I've already discussed.\n*   Another is something like \"active adaptability\", or \"being present in the moment\". A general intelligence is not an adaptation-executor; a general intelligence is an algorithm that mindfully decides *how* to adapt. It may defer to a learned heuristic in certain situations, but whenever that happens, it’s only because its outer cognitive loop has decided that that heuristic is the appropriate tool for the job.[^vd14ecud56]\n*   The third is *goal-directedness*. (4) and (5) talk about it explicitly, but it’s present in the others as well. “Learning to use novel abstractions” implies *something* for which they will be used. A “heuristics generator” would need to know for *what* to refine its heuristics. A query on a world-model would be looking for an output satisfying some specifications.\n\nThe goal-directedness is the particularly important part. To be clear: by it, I don't mean that a generally intelligent mind ought to have [a *fixed* goal it’s optimizing for](https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals). On the contrary, general intelligence’s generality extends to being retargetable towards arbitrary objectives. But every momentary *step* of general reasoning is always a step *towards some outcome.* Every call of the function implementing general intelligence has to take in some objective as an input — else it's invalid, a query on an empty string.\n\nGoal-directedness, thus, is inextricable from general intelligence. “The vehicle of cross-domain goal-pursuit” is what intelligence *is*.\n\n* * *\n\n5\\. A Caveat\n------------\n\nOne subtle point I've omitted so far is that, while achieving generality is supposed to cause systems to dramatically jump in capabilities, it doesn't have to happen *instantly*. A system may need to \"grow\" into its intelligence. The mechanistic discontinuity, when the core of general intelligence is assembled, would slightly precede the \"capabilistic\" one, when the potential of that core is properly realized.\n\nThe *homo sapiens sapiens* spent thousands of years hunter-gathering before starting up civilization, even after achieving modern brain size. Similarly, when whatever learning algorithm we're using builds general intelligence into an AI, it would not *instantly* become outsmart-all-of-humanity superintelligent (well, *probably* not).\n\nThe reason is that, while general-intelligence algorithms are equal in their generality, that doesn't mean generally-intelligent *minds* don't vary on other axes.\n\n*   The GI component may have different amounts of compute assigned to it: like humans have different [g-factors](https://en.wikipedia.org/wiki/G_factor_(psychometrics)), differently-sized working memory.\n*   The GI component may be employed more or less frequently: individual humans [are not generally intelligent when they're not concentrating](https://www.lesswrong.com/posts/4AHXDwcGab5PhKhHT/humans-who-are-not-concentrating-are-not-general).\n*   The GI component may be more or less swayable by heuristics: like humans' conscious decisions are often overridden by instincts.\n*   The mind may simply not be \"skilled\" in directing its generally-intelligent reasoning. Some meta-cognitive knowledge is required to do it well. Equally-intelligent humans may be better or worse at it (LW-esque rationality is essentially a discipline for cultivating such skills).[^khqrwrbtw9]\n\nSo when the GI component first coalesces, it may have very little compute for itself, it may not be often employed, it may defer to heuristics in most cases, and the wider system wouldn't yet know how to employ it well.[^7jq00fvjemj] It would still be generally capable *in the limit*, but it wouldn't be instantly omnicide-capable. It would take some time for the result of the mechanistic discontinuity to become properly represented at the level of externally-visible capabilities.\n\nThus, in theory, there *may* be a small margin of capability where we'd have a proper AGI that nonetheless can't easily take over us. At face value, seems like this should invalidate the entire \"we won't be able to iterate on AGI systems\" concern...\n\nThe problem is that it'd be very, very difficult to catch that moment and to take proper advantage of it. Most approaches to alignment are not on track to do it. Primarily, because those approaches don't believe in the mechanistic discontinuity at all, and don't even know that there's some crucial moment to be carefully taking advantage of.\n\nThere's three problems:\n\n**1) A \"weak\" AGI is largely a pre-AGI system**.\n\nImagine a \"weak\" AGI as described above. The GI component doesn't have much resources allocated to it, it's often overridden, so on. Thus, that system's cognitive mechanics and behavior are still mostly determined by specialized problem-solving algorithms/heuristics, not general intelligence. The contributions of the GI component are a rounding error.\n\nAs such, most of the lessons we learn from naively experimenting with this system would be lessons about pre-AGI systems, not AGI systems! There would be high-visible-impact interpretability or alignment techniques that ignore the GI component entirely, since it's so weak and controls so little. On the flip side, no technique that spends most of its effort on aligning the GI component would look cost-effective to us.\n\nThus, unless we *deliberately target* the GI component (which requires actually deciding to do so, which requires knowing that it exists and is crucial to align), iterating on a \"weak\" AGI will just result in us developing techniques for pre-AGI systems. Techniques that won't scale once the \"weak\" label falls off.\n\nConversely, the moment the general-intelligence component *does* become dominant — the moment any alignment approach *would* be forced to address it — is likely the moment the AI becomes significantly smarter than humans. And at that point, it'd be too late to do alignment-by-iteration.\n\nThe discontinuity there doesn't have to be as dramatic as hard take-off/FOOM is usually portrayed. The AGI may stall at a slightly-above-human capability, and that would be enough. The danger lies in the fact that we won't be *prepared* for it, would have no tools to counteract its new capabilities *at all.* It may not instantly become beyond humanity's theoretical ability to contain — but it'd start holding the initiative, and will easily outpace our efforts to catch up. (Discussing why even \"slightly\" superintelligent AGIs are an omnicide risk is outside the scope of this post; there are [other materials](https://www.lesswrong.com/posts/oBBzqkZwkxDvsKBGB/ai-could-defeat-all-of-us-combined) that cover this well.)\n\nDon't get me wrong: having a safely-weak AGI at hand to experiment with would be helpful for learning to align even \"mature\" AGIs. But we would need to make very sure that our experiments are targeting the right feature of that system. Which, in all likelihood, requires *very strong* interpretability tools: we'd need [\"a firehose of information\"](https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring#Solution__Measure_Lots_of_Things) on the AI's internals to catch the moment.\n\n**2) We may be in an \"agency overhang\".** [As nostalgebraist's post on autonomy](https://www.lesswrong.com/posts/HSETWwdJnb45jsvT8/autonomy-the-missing-agi-ingredient) mentions, modern AIs aren't really *trained* to be deeply agentic/goal-directed. Arguably, we don't yet know how to do it at all. It may require a paradigm shift similar to the invention of transformers.\n\nAnd yet, modern LLMs are incredibly capable even without that. If we assume they're not generally intelligent, that'd imply they have instincts dramatically more advanced than any animal's. So advanced we often *mistake* them for AGI!\n\nThus, the concern: the moment we figure out how to even slightly incentivize general intelligence, [the very first AGI will become strongly superintelligent](https://i.imgur.com/C2PjdxF.png). It'd be given compute and training *far in excess* of what AGI \"minimally\" needs, and so it'd instantly develop general intelligence as far ahead of humans' as LLMs' instincts are ahead of human instincts. The transition between the mechanistic and the capabilitisc discontinuity would happen within a few steps of a single training run — so, effectively, there *wouldn't* actually be a gap between them.\n\nIn this case, the hard take-off will be very hard indeed.\n\nA trick that we might try is deliberately catching AGI in-training: Design interpretability tools for detecting the \"core of general intelligence\", continuously run them as we train. The very moment they detect GI forming, we stop the training, and extract a weak, omnicide-incapable AGI. We then do iterative experimentation on it as usual (although that would be non-trivial to get right as well, see point 1).\n\nThat still has some problems:\n\n1.  It'd require [fairly advanced interpretability tools](https://www.lesswrong.com/posts/FDjTgDcGPc7B98AES/searching-for-search-4), tools we don't yet have.\n2.  The transition from a \"weak\" AGI to a superintelligence may be very fast, so we'd need to pause-and-interpret the model very frequently during the training. That'd potentially significantly increase the costs and time required.\n3.  The resultant \"weak AGI\" [may still be incredibly dangerous](https://www.lesswrong.com/posts/oBBzqkZwkxDvsKBGB/ai-could-defeat-all-of-us-combined#How_AIs_could_defeat_humans_without__superintelligence_). Not instantly omnicidal, but perhaps on the very verge of that. (Consider how dangerous the upload of a human genius would be.)\n\nI do think this can be a component of some viable alignment plans. But it's by no means trivial.\n\n**3) We may not notice \"weak\" AGI while staring right at it**.\n\nThe previous possibility assumed that modern LLMs are not AGI. Except, how do we know that?\n\n* * *\n\n6\\. The Case of LLMs\n--------------------\n\nI'll be honest: LLMs freak me out as much as they do anyone. As will be outlined, I have strong theoretical reasons to believe that they're not generally intelligent, and that general intelligence isn't reachable by scaling them up. But looking at [some](https://twitter.com/ESYudkowsky/status/1636740897466646529) of their [outputs](https://twitter.com/gfodor/status/1643297881313660928#m) sure makes me nervously double-check my assumptions.\n\nThere's a fundamental problem: in the case of AI, the presence vs. absence of general intelligence at non-superintelligent levels is very difficult to verify externally. I've alluded to it some already, when mentioning that \"weak\" AGIs, in their makeup and behavior, are mostly pre-AGI systems.\n\nThere *are* some obvious tell-tale signs in both directions. If it can only output gibberish, it's certainly not an AGI; if it just outsmarted its gatekeepers and took over the world, it's surely an AGI. But between the two extremes, there's a grey area. LLMs are in it.\n\nTo start the analysis off, let's suppose that LLMs are entirely pre-AGI. They don't contain a coalesced core of true generality, not even an \"underfunded\" one. On that assumption, how do they work?\n\nSuppose that we prompt a LLM with the following:\n\n    vulpnftj = -1\n    3 + vulpnftj =\n\nLLMs somehow figure out that the answer is \"2\". It's highly unlikely that \"vulpnftj\" was ever used as a variable in their training data, yet they somehow know to treat it as one. How?\n\nWe can imagine that there's a \"math engine\" in there somewhere, and it has a data structure like \"a list of variables\" consisting of {name; value} entries. The LLM parses the prompt, then slots \"vulpnftj\" and \"-1\" into the corresponding fields. Then it knows that \"vulpnftj\" equals \"-1\".\n\nThat's a kind of \"learning\", isn't it? It lifts completely new information from the context and adapts its policy to suit. But it's a very unimpressive kind of learning. It's only learning in a known, pre-computed way.\n\nI claim that this is how LLMs do *everything*. Their seeming sophistication is because this trick scales far up the abstraction levels.\n\nImagine a tree of problem-solving modules, which grow increasingly more abstract as you ascend. At the lowest levels, we have modules like \"learn the name of a variable: %placeholder%\". We go up one level, and see a module like \"solve an arithmetic equation\", with a field for the equation's structure. Up another level, and we have \"solve an equation\", with some parameters that, if filled, can adapt this module for solving arithmetic equations, differential equations, or some other kinds of equations (even very esoteric ones). Up, up, up, and we have \"do mathematical reasoning\", with parameters that codify modules for solving all kinds of math problems.\n\nWhen an LLM analyses a prompt, it figures out it's doing math, figures out what specific math is happening, slots all that data in the right places, and its policy snaps into the right configuration for the problem.\n\nAnd if we go sideways from \"do math\", we'd have trees of modules for \"do philosophy\", \"do literary analysis\", \"do physics\", and so on. If we'd instead prompted it with a request to ponder the meaning of life as if it were Genghis Khan, it would've used different modules, adapted its policy to the context in different ways, called up different subroutines. Retrieve information about Genghis Khan, retrieve the data about the state of philosophy in the 13th century, constrain the probability distribution over the human philosophical outlook by these two abstractions, distill the result into a linguistic structure, extract the first token, output it...\n\nA wealth of possible configurations like this, a combinatorically large number of them, sufficient for basically any prompt you may imagine.\n\nBut it's still, fundamentally, *adapting in known ways*. It doesn't have a mechanism for developing new modules; the gradient descent has always handled that part. The LLM contains a wealth of [crystallized intelligence](https://en.wikipedia.org/wiki/Fluid_and_crystallized_intelligence), but zero fluid intelligence. A *static* set of abstractions it knows, a *closed* range of environments it can learn to navigate. [Still \"just\" interpolation.](https://www.lesswrong.com/posts/iQabBACQwbWyHFKZq/how-i-m-thinking-about-gpt-n)\n\nFor state-of-the-art LLMs, that crystallized structure is so extensive it contains basically every abstraction known to man. Therefore, it's very difficult to come up with some problem, some domain, that they don't have an already pre-computed solution-path for.\n\nConsider also the generalization effect. The ability to learn to treat \"vulpnftj\" as a variable implies the ability to learn to treat any arbitrary string as a variable. Extending that, the ability to mimic the writing styles of a thousand authors implies the ability to \"slot in\" any style, including one a human talking to it has just invented on the fly. The ability to write in a hundred programming languages... implies, perhaps, the ability to write in any programming language. The mastery of a hundred board games generalizes to the one-hundred-and-first one, even if that one is novel. And so on.\n\n*In the limit*, yes, that goes all the way to full general intelligence. Perhaps the abstraction tree only grows to a finite height, perhaps there are only so many \"truly unique\" types of problems to solve.\n\nBut the current paradigm [may be a ruinously inefficient way to approach that limit](https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally#3__are_we_getting_smarter_yet_):\n\n> There are lots of algorithms which are Turing-complete or ‘universal’ in some sense; there are lots of algorithms like [AIXI](https://www.lesswrong.com/tag/aixi) which solve AI in some theoretical sense (Schmidhuber & company have many of these cute algorithms such as ‘the fastest possible algorithm for all problems’, with the minor catch of some constant factors which require computers bigger than the universe).\n> \n> Why think pretraining or sequence modeling is not another one of them? Sure, *if* the model got a low enough loss, it’d have to be intelligent, but how could you prove that would happen in practice?\n\nYet it still suffices to foil the obvious test for AGI-ness, i. e. checking whether the AI can be \"creative\". How exactly do you test an LLM on that? Come up with a new game and see if it can play it? If it can, that doesn't prove much. Maybe that game is located very close, in the concept-space, to a couple other games the LLM was already fed, and deriving the optimal policy for it is as simple as doing a weighted sum of the policies for the other two.\n\nSome tasks along these lines *would* be a definitive proof — like asking it to invent a new field of science on the fly. But, well, that's too high a bar. Not any AGI can meet it, only a strongly superintelligent AGI, and such a system would be past the level at which it can defeat humanity. It'd be too late to ask it questions then, because it'll have already eaten us.\n\nI think, as far as current LLMs are concerned, there's still [some vague felt-sense](https://twitter.com/ESYudkowsky/status/1637385553959325696) in which all their ideas are \"stale\". In-distribution for what humanity has already produced, not \"truly\" novel, not as creative as even a median human. No scientific breakthroughs, no economy-upturning startup pitches, certainly no mind-hacking memes. Just bounded variations on the known. The fact that people do [this sort of stuff](https://github.com/Significant-Gravitas/Auto-GPT), and nothing much comes of it, is some evidence for this, as well.\n\nIt makes sense in the context of LLMs' architecture and training loops, too. They weren't trained to be generally and autonomously intelligent; their architecture is a poor fit for that in several ways.\n\nBut how can we be sure?\n\nThe problem, fundamentally, is that we have *no idea* how the problem-space looks like. We don't know and can't measure in which directions it's easy to generalize or not, we don't know with precision how impressive AI is getting. We don't know how to tell an advanced pre-AGI system from a \"weak\" AGI, and have no suitable interpretability tools for a direct check.\n\nAnd thus we'd be unable to tell when AI — very slowly at first, and then explosively — starts generalizing off-distribution, in ways only possible for the generally intelligent, arbitrary-environment-navigating, goal-directed things. We'd miss the narrow interval in which our AGIs were weak enough that we could survive failing to align them and get detailed experience from our failures (should there be such an interval at all). And the moment at which it'll become *clear* that we're overlooking something, would be the exact moment it'd be too late to do anything about it.\n\n*That* is what [\"no fire alarm\"](https://intelligence.org/2017/10/13/fire-alarm/) means.\n\n* * *\n\n7\\. The Subsequent Difficulties\n-------------------------------\n\nAll right, it's finally time to loop back around to our initial concerns. Suppose general intelligence is indeed binary, or \"approximately\" so. How does just that make alignment so much harder?\n\nAt the fundamental level, this means that AGI-level systems work in a qualitatively different way from pre-AGI ones. Specifically, they *think* in a completely novel way. The mechanics of fluid intelligence — the processes needed to efficiently derive novel heuristics, to reason in a consequentialist manner — don't resemble the mechanics of vast crystallized-intelligence structures.\n\nThat creates a swath of problems. Some examples:\n\n**It breaks \"weak\" interpretability tools**. If we adapt them to pre-AGI systems, they would necessarily depend on there being a static set of heuristics/problem-solving modules. They would identify modules corresponding to e. g. \"deception\", and report when those are in use. A true AGI, however, would be able to [spin off novel modules that fulfill a similar function in a round-about way](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment#Ryan_Greenblatt___Eliciting_Latent_Knowledge). Our tools would not have built-in functionality for actively keeping up with a dynamically morphing system, so they will fail to catch that, to generalize. (Whereas destroying the AI's ability to spin off novel modules would mean taking the \"G\" out of \"AGI\".)\n\nAs I'd mentioned, for these purposes \"weak\" AGIs are basically equivalent to pre-AGI systems. If the general-intelligence component isn't yet dominant, it's not doing this sort of module-rewriting *at scale*. So interpretability tools naively adapted for \"weak\" AGIs would be free to ignore that aspect, and they'd still be effective... And would predictably fail once the GI component *does* grow more powerful.\n\n**It breaks \"selective\" hamstringing.** Trying to limit an AGI's capabilities, to make it incapable of thinking about harming humans or deceiving them, runs into the same problem as above. While we're operating on pre-AGI systems, mechanistically this means erasing/suppressing the corresponding modules. But once we get to AGI, once the system can create novel modules/thought-patterns on the fly... [It'd develop ways to work around](https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness).\n\n**It breaks \"nonrobust\" goal-alignment.** In a pre-AGI system, the \"seat of capabilities\" are the heuristics, i. e. the vast crystallized-intelligence structures of problem-solving modules.  \"Aligning\" them, to wit, means re-optimizing these heuristics such that the AI reflexively discards plans that harm humans, and reflexively furthers plans that help humans. If we take on [the shard-theory frame](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX), it can mean cultivating a strong shard that values humans, and bids to protect their interests.\n\nAligning an AGI is a different problem. Shards/heuristics are not the same thing as the goals/mesa-objectives the AGI would pursue — they're fundamentally different types of objects. If it works anything like how it does in humans, perhaps mesa-objectives would be *based on* or *inspired by* shards. But how exactly the general-intelligence module would interpret them is under question. It's unlikely to be a 1-to-1 mapping, however: much like human emotional urges and instincts do not map 1-to-1 to the values we arrive at via moral philosophy.\n\nOne thing that seems certain, however, is that shards would lose *direct* control over the AGI's decisions. It would be an internal parallel to what would happen to *our* pre-AGI interpretability or hamstringing tools — heuristics/shards simply wouldn't have the machinery to automatically keep up with an AGI-level system. The aforementioned \"protect humans\" shard, for example, would only know to bid against plans that harm humans in some specific mental contexts, or in response to specific kinds of harm. Once the AGI develops new ways to think about reality, the shard would not even know to *try* to adapt. And afterwards, if the GI component were so inclined, it would be able to extinguish that shard, facing no resistance.\n\nA human-relatable parallel would be someone going to exposure therapy to get rid of a phobia, or a kind person deciding to endorse murder when thinking about it in a detached utilitarian framework. When we reflect upon our values, we sometimes come to startling results, or decide to suppress our natural urges — and we're often successful in that.\n\nPre-AGI alignment would not *necessarily* break — if it indeed works like it does in humans. But the process of value reflection [seems highly unstable](https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model#6__Value_Compilation), and its output is a non-linear function of the *entirety* of the initial desires. \"If there's a shard that values humans, the AGI will still value humans post-reflection\" does not hold, by default. \"Shard-desires are more likely to survive post-reflection the stronger they are, and the very strong will definitely survive\" [is likewise invalid](https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural).\n\nThus, the alignment of a pre-AGI system doesn't guarantee that this system will remain aligned past the AGI discontinuity; and [it probably wouldn't](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile). If we want to robustly align an AGI, we have to target the GI component *directly,* not through the unreliable proxy of shards/heuristics.\n\n**It leads to a dramatic capability jump.** Consider [grokking](https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking). The gradient descent gradually builds some algorithmic machinery into an AI. Then, once it's complete, that machinery \"snaps together\", and the AI becomes sharply more capable in some way. The transition from a pre-AGI system to a mature AGI can be viewed as the theorized most extreme instance of grokking — that's essentially what [the sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities) is.\n\nLooking at it *from the outside*, however, we won't see the gradual build-up (unless, again, we have very strong interpretability tools specifically for that). We'd just see the capabilities abruptly skyrocketing, and generalizing in ways we haven't seen before. In ways we didn't predict, and couldn't prepare for.\n\nAnd it would be exactly the point at which things like recursive self-improvement become possible. Not in the sort of overdramatic way in which FOOM is often portrayed, but in the same sense in which a human trying to get better at something self-improves, or in which human civilization advances its industry.\n\nCrucially, it would involve an AI whose capabilities grow *as the result of its own cognition*; not as the result of the gradient descend improving it. A static tree of heuristics, no matter how advanced, can't do that. A tree of heuristics deeply interwoven with the machinery for deriving novel heuristics... can.\n\nWhich, coincidentally, is another trick that tools optimized for the alignment of pre-AGI systems won't know how to defeat.\n\n**The unifying theme** is that we [won't be able to iterate](https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails). Pre-AGI interpretability, safeguards, alignment guarantees, scaling laws, and *all other approaches that fail to consider the AGI discontinuity* — would ignobly fail at the AGI discontinuity.\n\nAs per Section 5, *in theory* iteration is possible. Not all AGIs are superhuman, and we can theoretically \"catch\" a \"weak\" AGI, and experiment with it, and derive lessons from that experimentation that *would* generalize to strongly superintelligent systems. But that's incredibly hard to do right without very advanced interpretability tools, and the situation would likely be highly unstable, with the \"caught\" AGI still presenting a massive threat.\n\nOkay, so AGI is highly problematic. Can we manage without it?\n\n**Can \"limitedly superhuman\" AIs suffice?** That is, systems that have superhuman competencies in some narrow and \"safe\" domains, like math. Or ones that don't have \"desires\", like oracles or simulators. Or ones that aren't self-reflective, or don't optimize too strongly, or don't reason in a consequentialist manner...\n\nIt should be clear, in the context of this post, that this is an incoherent design specification. Useful creativity, truly-general intelligence, and goal-directedness are *inextricable* from each other. They're just different ways of looking at the same algorithm.\n\nOn this view, there aren't actually any \"domains\" in which general intelligence can be \"specialized\". Consider math. Different fields of it consist of objects that behave in drastically different ways, and inventing a novel field would require comprehending a suite of novel abstractions and navigating them. If a system can do that, it has the fundamental machinery for general intelligence, and therefore for inventing deception and strategic scheming. If it can't... Well, it's not much use.\n\nSimilar for physics, and even more so for engineering. If math problems can be often defined in ways that don't refer to the physical reality at all, engineering problems and design specifications *would* talk about reality. To solve such problems, an AGI would need not only the basic general-intelligence machinery, but also a suite of *crystallized* intelligence modules for reasoning about reality. Not just the theoretical ability to learn how to achieve real goals, but the *actual* knowledge of it.\n\nMost severely it applies to various \"automate alignment\" ideas. Whether by way of prompting a [simulator](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators) to generate future alignment results, or by training some specialized \"research assistant\" model for it... Either the result won't be an AGI, and therefore won't actually contribute novel results, or it *would* be an AGI, and therefore an existential threat.\n\nThere's nothing in-between.\n\n**What about generative world-models/**[**simulators**](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators)**, specifically?** This family of alignment proposals is based on the underlying assumption that a simulator itself is goal-less. It's analogized to the laws of physics — it can implement agents, and these agents are dangerous and in need of alignment... But the simulator is not an agent of its own, and not a threat.\n\nThe caveat is that a simulator is not *literally* implemented as a simulation of physics (or language), even if it can be viewed as such. That would be ruinously compute-intensive, far in excess of what LLMs actually consume. No, mechanistically, it's a complex suite of heuristics. A simulator pushed to AGI, then, would consist of a suite of heuristics in control of a generally-intelligent goal-directed process... Same as, say, any reinforcement-learning agent.\n\nExpecting *that* to keep on being a simulator is essentially expecting this AGI to end up inner-aligned to the token-prediction objective. And there's no reason to expect that in the case of simulators, any more than there's reason to expect it for any *other* training objective.\n\nIn the end, we will get an AGI with some desires that shallowly correlate with token-prediction, a \"shoggoth\" as it's often nicknamed. It will reflect on its desires, and come to unpredictable, likely omnicidal conclusions. Business as usual.\n\n**What about scalable oversight, such as pursued by OpenAI?** Its failure follows from the intersection of a few ideas discussed above. The hard part of the alignment problem is figuring out how to align the GI component. If we're not assuming that problem away, here, the AIs doing the oversight would have to be pre-AGI models (which we roughly do know how to align). But much like weak interpretability tools, or shards, these models would not be able to keep up with AGI-level shifting cognition. Otherwise, they wouldn't be \"pre\"-AGI, since this sort of adaptability is what *defines* general intelligence.\n\nAnd so we're back at square one.\n\nThus, once this process scales to AGI-level models, its alignment guarantees will predictably break.\n\n* * *\n\n8\\. Closing Thoughts\n--------------------\n\n**To sum it all up:** As outlined here, I'm deeply skeptical, to the point of dismissiveness, of a large swathe of alignment approaches. The underlying reason is a model that assumes a sharp mechanistic discontinuity at the switch to AGI. Approaches that fail to pay any mind to that discontinuity, thus, look obviously doomed to me. Such approaches miss the target entirely: they focus on shaping the features of the system that play a major part *now*, but will fall into irrelevance once general intelligence forms, while ignoring the component of AI that will *actually* be placed in charge at the level of superintelligence.\n\nIn addition, there's a pervasive Catch-22 at play. Certain capabilities, like universally flexible adaptability and useful creativity, can only be implemented via the general-intelligence algorithm. As the result, there's no system that can automatically adapt to the AGI discontinuity except *another* generally-intelligent entity. Thus, to align an AGI, we either need an aligned AGI... or we need to do it *manually*, using *human* general intelligence.\n\nIt's worth stating, however, that I don't consider alignment to be impossible, or even too hard to be realistically solved. While Eliezer/Nate may have P(doom) at perhaps 90+%, John expects survival with [\"better than a 50/50 chance\"](https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update#So__how_s_The_Plan_going_), and I'm leaning towards the latter estimate as well.\n\nBut what I *do* think is that we won't get to have shortcuts and second chances. Clever schemes for circumventing or easing the alignment problem won't work, and reality won't forgive us for not getting it exactly right.\n\nBy the time we're deploying AGI, [we have to have a precise way of aiming such systems](https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making). Otherwise, yes, we *are* hopelessly doomed.\n\n[^vd14ecud56]: A general intelligence may also be suppressed by an instinct firing off, as sometimes happens with humans. But that’s a feature of the wider mind the GI is embedded in, not of general intelligence itself. \n\n[^khqrwrbtw9]: This is one of the places where my position seems at odds with e. g. Eliezer's, although I think the disagreement is largely semantical. He sometimes talks about AIs that are \"more general\" than humans, providing an example of an AI capable of rewriting its cognitive algorithms on the fly to be able to write bug-free code. Here, he doesn't make a distinction between the fundamental capabilities of the general-intelligence algorithm, and the properties of a specific mind in which GI is embedded.Imagine an AGI as above, able to arbitrarily rewrite its mental subroutines, but with a twist: there's a secondary \"overseer\" AGI on top of it, and its sole purpose is to delete the \"program perfectly in Python\" module whenever the first AGI tries to create it. The system as a whole would be \"less general\" than the first AGI alone, but not due to some lacking algorithmic capability.Similar with humans: we possess the full general-intelligence algorithm, it just doesn't have write-access to certain regions of our minds. \n\n[^7jq00fvjemj]: Or it may be instantly given terabytes of working memory, an overriding authority, and a first task like \"figure out how to best use yourself\" which it'd then fulfill gloriously. That depends on the exact path the AI's model takes to get there: maybe the GI component would grow out of some advanced pre-GI planning module, which would've already enjoyed all these benefits?My baseline prediction is that it'd be pretty powerful from the start. But I will be assuming the more optimistic scenario in the rest of this post: my points work even if the GI starts out weak.",
      "plaintextDescription": "1. Introduction\nThe field of AI Alignment is a pre-paradigmic one, and the primary symptom of that is the wide diversity of views across it. Essentially every senior researcher has their own research direction, their own idea of what the core problem is and how to go about solving it.\n\nThe differing views can be categorized along many dimensions. Here, I'd like to focus on a specific cluster of views, one corresponding to the most \"hardcore\", unforgiving take on AI Alignment. It's the view held by people like Eliezer Yudkowsky, Nate Soares, and John Wentworth, and not shared by Paul Christiano or the staff of major AI Labs.\n\nAccording to this view:\n\n * We only have one shot. There will be a sharp discontinuity in capabilities once we get to AGI, and attempts to iterate on alignment will fail.  Either we get AGI right on the first try, or we die.\n * We need to align the AGI's values precisely right. \"Rough\" alignment won't work, niceness is not convergent, alignment attained at a low level of capabilities is unlikely to scale to superintelligence.\n * \"Dodging\" the alignment problem won't work. We can't securely hamstring the AGI's performance in some domain without compromising the AGI completely. We can't make it non-consequentialist, non-agenty, non-optimizing, non-goal-directed, et cetera. It's not possible to let an AGI keep its capability to engineer nanotechnology while taking out its capability to deceive and plot, any more than it's possible to build an AGI capable of driving red cars but not blue ones. They're \"the same\" capability in some sense, and our only hope is to make the AGI want to not be malign.\n * Automating research is impossible. Pre-AGI oracles, simulators, or research assistants won't generate useful results; cyborgism doesn't offer much hope. Conversely, if one such system would have the capability to meaningfully contribute to alignment, it'd need to be aligned itself. Catch-22.\n * Weak interpretability tools won't generalize to the AGI stag",
      "wordCount": 6602
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "Q6hq54EXkrw8LQQE7",
        "name": "Gears-Level",
        "slug": "gears-level"
      },
      {
        "_id": "bxhzaWtdNoEMMkE8r",
        "name": "General intelligence",
        "slug": "general-intelligence"
      },
      {
        "_id": "JX69nZB8tfxnx5nGH",
        "name": "Threat Models (AI)",
        "slug": "threat-models-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "HaHcsrDSZ3ZC2b4fK",
    "title": "World-Model Interpretability Is All We Need",
    "slug": "world-model-interpretability-is-all-we-need",
    "url": null,
    "baseScore": 36,
    "voteCount": 25,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2023-01-14T19:37:14.707Z",
    "contents": {
      "markdown": "**Summary, by sections**:\n\n1.  Perfect world-model interpretability seems both sufficient for robust alignment (via a decent variety of approaches) and realistically attainable (compared to \"perfect interpretability\" in general, i. e. insight into AIs' heuristics, goals, and thoughts as well). Main arguments: [the NAH](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction) \\+ [internal interfaces](https://www.lesswrong.com/posts/nwLQt4e7bstCyPEXs/internal-interfaces-are-a-high-priority-interpretability).\n2.  There's plenty of reasons to think that world-models would converge towards satisfying a lot of nice desiderata: they'd be represented as a separate module in AI cognitive architecture, and that module would consists of many consistently-formatted sub-modules representing recognizable-to-us concepts. Said \"consistent formatting\" may allow us to, in a certain sense, interpret the entire world-model *in one fell swoop*.\n3.  We already have some rough ideas on *how* the data in world-models would be formatted, courtesy of the NAH. I also offer some rough speculations on possible higher-level organizing principles.\n4.  This avenue of research also seems very tractable. It can be approached from a wide variety of directions, and should be, to an extent, decently factorizable. Optimistically, it may constitute a relatively straight path from here to a \"minimum viable product\" for alignment, even in words where alignment is really hard[^fwt9njggqtm].\n\n* * *\n\n1\\. Introduction\n----------------\n\n### 1A. Why Aim For This?\n\nImagine that we develop interpretability tools that allow us to flexibly understand and manipulate an AGI's world-model — but *only* its world-model. We would be able to see what the AGI knows, add or remove concepts from its mental ontology, and perhaps even use its world-model to run simulations/counterfactuals. But its thoughts and plans, and its hard-coded values and [shards](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT), would remain opaque to us. Would that be sufficient for robust alignment?\n\nI argue it would be.\n\nPrimarily, this would solve [the Pointers Problem](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans). A central difficulty of alignment is that our values are functions of highly abstract variables, and that makes it hard to point an AI at them, instead of at [easy-to-measure, shallow functions over sense-data](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like). Cracking open a world-model would allow us to design metrics that have *depth*.\n\nFrom there, we'd have several ways to proceed:\n\n1.  Fine-tune the AI to point more precisely at what we want (such as \"human values\" or \"faithful obedience\"), instead of its shallow correlates.\n    *   This would also solve [the ELK](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge), which alone can be [used as a lever to solve the rest of alignment](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.3y1okszgtslx).\n    *   Alternatively, this may lower the difficulty of [retargeting the search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget) — we won't necessarily need to find the retargetable process, only *the target*.\n2.  Discard everything of the AGI except the interpreted world-model, then train a *new* policy function over that world-model (in a fashion similar to [this](https://worldmodels.github.io/)), that'll be pointed at the \"deep\" target metric from the beginning.\n    *   The advantage of this approach over (1) is that in this case, our policy function wouldn't be led astray by any values/mesa-objectives it might've already formed.\n3.  With some more insight into how agency/intelligence works, perhaps we'll be able to *manually* write a [general-purpose search algorithm](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see) over that world-model. In a sense, \"general-purpose search\" is just a principled way of drawing upon the knowledge contained in the world-model, after all — the GPS itself is probably fairly simple.\n    *   Taking this path would give us even more control over how our AI works than (2), potentially allowing us to install some very nuanced counter-measures.\n\nThat leaves open the question of the \"target metric\". It primarily depends on what will be easy to specify — what concepts we'll find in the interpreted world-model. Some possibilities:\n\n*   Human values. *Prima facie*, \"what this agent values\" seems like a natural abstraction, one that we'd expect to find in the world-model of any agent that models another agent. The issue might be that humans are too incoherent and philosophically confused for their \"values\" to stand for anything concrete — e. g., we almost certainly [don't have concrete utility functions](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT).\n*   An *indirect* pointer to human values. Even if the world-model would lack a pointer to \"human values\", already fully specified, there may be some pointer at \"the process of moral philosophy\", or whatever people are using to *try* to specify their values. If so, we may be able to task the AI with helping us complete this process, then optimize for that process' output.\n*   Corrigibillity. [The hard problem of corrigibility](https://arbital.com/p/hard_corrigibility/) suggests that it may, in some sense, be a natural concept/abstraction, not just an arbitrary list of limitations on the AI's actions. If so, we may find a concept standing for it in the AI's world-model, and just directly point the AI at it.\n    *   Alternatively, we may be able to *manually* design a corrigible setup, especially if we can decouple the world-model from the AI's policy (as in approaches (2) or (3)). [Here](https://www.lesswrong.com/posts/HKZqH4QtoDcGCfcby/corrigibility-via-thought-process-deference-1)'s a rough sketch of how that may look like.\n*   [Do What I Mean](https://en.wikipedia.org/wiki/DWIM). Suppose that you \"tell\" the AI to do something (via natural language, or code, or by designing its reward function). The AI accepts that command as input, then \"interprets\" it in some fashion, translates it into its mental ontology, and acts on the result. If we can seize control over how the \"interpretation\" step happens — if we can make the AI interpret our commands *faithfully*, the way we *intend* them, instead of via some likely-misaligned \"interpreter\" module, or via their [pure semantic meaning](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes) — well, that would align it to the order-giver's intentions. That's [potentially dangerous](https://www.lesswrong.com/posts/CtXaFo3hikGMWW4C9/the-case-against-ai-alignment), but may be leveraged into a global victory from there.\n    *   And it's almost certain that *this* target would be tractable. In this hypothetical, we've figured out how to do manual ontology translation (how to interpret an alien world-model). That likely means we have a mathematical specification of what \"ontology translation\" means. All we'd need to do is code that into the AI — tell it to correctly translate our orders from our ontology into its own!\n*   An oracle. Perhaps, if all of the above fails, we can use the world-model directly? Without a policy over it, it wouldn't be able to grow on its own, so it'd be stuck at the capability level at which we extracted it. But we'd be able to [study it for novel insights](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#5__Microscope_AI), which may provide the missing pieces for the rest of the alignment problem.\n    *   An even bigger win along this approach would be if the AGI is already highly reflective. That would mean there's a lot of self-knowledge stored in its world-model — how its mind works, what it cares about. In that case, we'd essentially get a manual on how to interpret the rest of its mind! Unfortunately, I think we shouldn't rely on that — AGIs that advanced are probably [not safe to interpret](https://www.lesswrong.com/posts/rytFP2zRYNK85rFyX/interpretability-tools-are-an-attack-channel).[^pmp1d45pmp]\n    *   (The stagnated capability growth may be a problem with approach (3) as well, where we manually design a synthetic GPS over the extracted world-model. But I expect it shouldn't be too difficult to code-in some way to *improve* the world-model as well — if nothing else, there'd probably be heuristics about that already in the world-model.)\n\n* * *\n\n### 1B. Is It A Realistic Goal?\n\nIs there reason to think we *can* achieve perfect interpretability into an AI's world-model? Why go for the world-model specifically, instead of trying to focus on understanding the AI's plans, thoughts, values, mesa-objectives, shards?\n\nI don't expect it'd be easy in an absolute sense, no. But when choosing from the set of targets that'd suffice for robust alignment, I do expect it's the easiest one.\n\nAs an established case for tractability, we have [the natural abstraction hypothesis](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction). According to it, efficient abstractions are a feature of the territory, not the map (at least to a certain significant extent). Thus, we should expect different AI models to converge towards the same concepts, which also would make sense to *us*. Either because we're already using them (if the AI is trained on a domain we understand well), or because they'd be the same abstractions we'd arrive at ourselves (if it's a novel domain).\n\nA different case is presented in [my earlier post](https://www.lesswrong.com/posts/nwLQt4e7bstCyPEXs/internal-interfaces-are-a-high-priority-interpretability) on internal interfaces. In short:\n\n*   In every case where we have two complex specialized systems working in tandem, we can expect an \"[interface](https://www.lesswrong.com/posts/hyShz2ABiKX56j5tJ/interfaces-as-a-scarce-resource)\" to form — a data channel for exchanging compressed messages.\n*   The data exchanged along such channels would follow some stable *formats*. Every message would have structure (the way human languages have syntactic structure, and API calls have strict specifications), reflecting the structure of the subject matter.\n*   The world-model specifically would need to interface with [a lot of other modules](https://www.lesswrong.com/posts/nwLQt4e7bstCyPEXs/internal-interfaces-are-a-high-priority-interpretability#4__What_Internal_Interfaces_Can_We_Expect_) — with the shards, with the planner (the AGI's *native* GPS algorithm). All of these modules would need to know how to access *any* of the concepts in the world-model, which means these concepts would need to be consistently-formatted (with these formats stable in time).\n*   In addition, the world-model can *itself* be thought of as an interface — between the asymbolic ground-truth of the world and the rest of the agent. As such, there's reason to expect it to be nicely-structured *by default*.\n    *   (In this framing, the natural abstraction hypothesis is a study of efficient interface-building between minds and reality.)\n\nCrucially, if a world-model does follow consistent data formats, it should be possible to interpret it *all at once*. Instead of interpreting features one-by-one, we can figure out their encoding, and \"crack\" the world-model in one fell swoop.\n\nBy comparison, there's no reason (as far as I can currently tell) to expect the same consistent formatting for the AI's heuristics/shards/mesa-objectives. They'd have consistent inputs and outputs, but their internals? Totally incomprehensible and ad-hoc. Still interpretable *in principle*, but only one-by-one.\n\nOn this argument, there's another prospective target: an AGI's plans/thoughts. They'd also need to be consistently-formatted, since future instances of the AGI's planner process would need to access plans generated by its earlier instances. But there are *fewer* reasons to expect that, and their formats may be dramatically more complex and difficult to decode. In particular, the NAH may not apply to them as strongly — plan formats may not be convergent across humans and AGIs, or even across different AGI systems.\n\n(Unduly optimistic possibility: Or it may be that plans would be formatted using the *same* formats the world-model uses. In which case \"interpreting the world-model\" and \"learning to read the AGI's thoughts \" solve each other. I wouldn't bet on that working out perfectly, though.)\n\nIn addition, consider that advanced AGI models would likely need to modify their world-models at runtime — perhaps via their native GPS algorithms. If so, we can expect advanced world-models to have built-in functionality for *editing*. They'd have functions like \"add a new concept\", \"remove a concept\", \"[chunk](https://billwall.phpwebhosting.com/articles/chunking.htm) two abstractions together\", \"[expand](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans?commentId=3Rn52bqwLEbahmipd) a given abstraction\", \"propagate an update\", or \"fetch all concepts connected to this one\", which should likewise be possible to reverse-engineer.[^7k676furwqc]\n\nAs such, I think perfect world-model interpretability is a reasonable target to aim for.\n\n* * *\n\n2\\. Would World-Models Look Like We Imagine?\n--------------------------------------------\n\nThis section is a kitchen sink of arguments regarding whether world-models would satisfy a bunch of nice high-level desiderata.\n\nNamely: whether they'd be learned at all, how they'd be used, whether \"the world-model\" would be a unified module (as opposed to a number of non-interacting specialized modules), and whether it'd have recognizable internal modules.\n\n### 2A. Are World-Models Necessary?\n\nThat is, should we actually expect AIs to learn a module we can reasonably describe as a \"world-model\"? It seems intuitively obvious, but can we prove it?\n\n[The Gooder Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#Making_The_Notion_Of__Model__A_Lot_Less_Silly) aims to do just that. Translating it into the framework of ML, it essentially says the following:\n\nSuppose that we have some system \\\\(S\\\\) in which we want our AI to optimally perform a task \\\\(Z\\\\), a training dataset \\\\(X\\\\) that contains some information on \\\\(S\\\\), and some set of variables \\\\(Y\\\\) (which can be thought of as information about the \"current\" state of \\\\(S\\\\)) which must be taken into account when choosing the optimal action \\\\(R\\\\) at runtime. \\\\(M\\\\) is some minimal summary of \\\\(X\\\\) available to the AI at runtime — its parameters.\n\n![](https://docs.google.com/drawings/u/1/d/sCTBU0_eKyeIpkmPLlqynAA/image?w=624&h=262&rev=59&ac=1&parent=1mPOEvCwjZTJ5hMkrdUqCx5jn0PtQd0cm58afjfyhPVQ)\n\nThe theorem states that \\\\(M\\\\) would need to contain all information from \\\\(X\\\\) which impacts the optimal policy for choosing \\\\(R\\\\) given \\\\(Y\\\\) — i. e., all information about decision-relevant features of \\\\(S\\\\) contained in \\\\(X\\\\).\n\n(For example, if the AI is trained to drive a car, it probably doesn't need to pay attention to the presence of planes in the sky. Thus, any data about planes present in \\\\(X\\\\) (their frequency, the trajectories they follow...) would be discarded, as it's not relevant to any decisions the AI would need to make. On the other hand, the presence of heavy clouds is correlated with rain, which would impact visibility and maneuverability, so the AI would learn to notice them.)\n\nMore specifically, \\\\(M\\\\) would need to be isomorphic to the Bayesian posterior on \\\\(S\\\\) given \\\\(X\\\\). That seems like a reasonable definition of a \"world-model\".\n\n* * *\n\n### 2B. How Are World-Models Useful?\n\nThat is, *why* are world-models necessary? What practical purpose does this module serve?\n\nAs I've mentioned at the beginning, they provide \"depth\". Imagine the world as a causal graph. At the start, the AI can only read off the states of the nodes nearest to it (its immediate sensory inputs). Correspondingly, it can only act on their immediate states. The only policies available to it are reactions no more sophisticated than \"if you see bright light, close your eyes\".\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/23980992c405360231fe4692ec7e8153c866765b56f5af15.png)\n\nAgent \\\\(A\\\\), observed nodes \\\\(o_1\\\\), \\\\(o_2\\\\), and non-observed nodes \\\\(x_1, x_2,...,x_6\\\\).\n\nBy building a world-model, it reconstructs the unobserved parts of that causal graph, and starts being able to \"see\" nodes that are farther away. Seeing them allows it to respond to changes in them, to make its policy a function of their states.\n\nThere's a few points to be made here.\n\n**First**, this solves [the Credit Assignment Problem](https://www.lesswrong.com/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem) by providing a policy gradient. To improve, you need some way to distinguish whether you're performing better or worse according to whatever goal you have. But if your goal is a function of a far-away node, a node you *don't see* — well, you have no idea whether your actions improve or worsen matters, so you can't learn. Having a world-model directly addresses this concern, providing you fine-grained/deep feedback on your actions.\n\n(Notably, \"get good at modelling the world\" itself is a very \"shallow\" goal. We get the gradient for it [\"for free\"](https://www.lesswrong.com/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem#Where_Updates_Come_From): setting up self-supervised learning on our own sensory inputs, trying to get better at predicting them, naturally (somehow) lets us recover the world structure.)\n\n**Second**, it dramatically increases [the space of available reaction patterns](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model#2_4__The_Need_for_World_Models), such as shards or heuristics. We can think of them as functions that take as input some subset of the nodes in our world-model, and steer the agent towards certain actions. If so, the set of available shards is defined over the *power set* of the nodes in the world-model.\n\nA rich world-model, thus, allows the creation of rich economies of shards with complex functionality, while still keeping every shard relatively simple (and therefore easier to learn), since they can \"build off\" the world-model's complexity. A shard whose complexity is comparable to \"if you see bright light, close your eyes\" can cause some very complex behavior if it's attached to a highly-abstract node, instead of a shallow observable.\n\n**Third**, note that world-models span not only space, but also *time*. Advanced world-models can be rolled forwards or backwards, to simulate the future or infer the past. This is useful both under the \"goal-directedness\" framework (allowing the agent to optimize across time) and the \"richness of heuristics\" one (we can view past or future states of nodes as \"clones\" of nodes, which expand the space of heuristics even more).\n\n**Fourth**, they allow advanced in-context learning, or \"virtual training\". Suppose you want to learn how to do X. Instead of doing it via trial-and-error in reality (which may be dangerous), you can [train your policy on your world-model instead](https://worldmodels.github.io/).\n\n* * *\n\n### 2C. Are World-Models Unitary?\n\nThat is, can we expect \"a world-model\" to be a proper [module](https://www.lesswrong.com/s/ApA5XmewGQ8wSrv5C), which only interacts with the rest of the AI's mind via pre-specified interfaces/API channels (as we'd like)? Or will it be in pieces, mini-world-models scattered all across the agent, each of them specialized to serve the needs of particular shards? Can we actually expect all information and inferences about the world to be pooled in one place, consistently-formatted?\n\nWell, the evidence is mixed. Empirically, it does not seem to be the case with the modern ML models. Take [the ROME paper](https://rome.baulab.info/): it describes a technique for editing factual associations in LLMs, but such edits [don't generalize properly](https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model). For example, rewriting the \\\\(\\text{The Eiffel Tower} \\to \\text{Paris}\\\\) association with \\\\(\\text{The Eiffel Tower} \\to \\text{Rome}\\\\), and then prompting the LLM to talk about the Eiffel Tower, correctly leads to it acting as if the Tower is in Rome. But it's one-directional: talking about *Rome* doesn't make it mention *the Eiffel Tower*, as it should if it pools all \"facts about Rome\" it has in one place. Neither does it follow a hierarchical structure (editing facts about \"cheese\" does not propagate the update to all sub-categories of cheese).\n\nHowever, there's strong theoretical support for unitarity, which I'll get to in a bit. There's three explanations for this contradiction:\n\n1.  Modern AI systems do implement a proper world-model, and we've simply failed, so far, to properly see it.\n2.  A unified world-model only becomes convergently useful at a specific level of capabilities, and modern AI systems simply haven't reached it yet. (E. g., it may only become useful once the GPS forms, at AGI level.)\n3.  There isn't, after all, a pressure to learn a unified \"world-model\" module.\n\nI'm pretty sure it's a mix of (1) and (2). Let's get to the arguments.\n\n**a) Future states are a function of increasingly further-away nodes**. Depending on whether you're planning for the next second, next day, or next century, the optimal action to take will be a function of [increasingly](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model#2_2__Ideal_Actions) [more](https://en.wikipedia.org/wiki/Light_cone) causally distant objects. Thus, your \"planning horizon\" is limited by your ability to correlate data across distant regions of your world-model.\n\n**b) The \"Crud\" factor**. Everything is connected to everything else. While details [can often be abstracted away](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction), a significant change often needs to be propagated throughout the whole world-model, and [a minor change may unexpectedly snowball](https://en.wikipedia.org/wiki/For_Want_of_a_Nail).\n\nIf your life is just a sequence of causally-disconnected games or tasks, you can have specialized models for each of them. But if these tasks can bleed into each other, it's necessary to cross-reference *all* available data.\n\nFor example, consider an AI trained to play chess and argue with people. If these tasks are wholly separate, and have no effect on each other, the AI can learn wholly separate, non-interacting generative models for both of them. But if a human can talk to the AI *concurrently* with playing a chess match against it, that changes. The chess strategy a human is using may reveal useful information about how they think, a heated argument may be distracting them from the game, and winning or losing the match may impact the human's emotional state in ways relevant to the arguments you should make. Thus, the need for a unified world-model arises: changes in one place need to be propagated everywhere.\n\nIn a sense, it's just a generalization of point (a), from the space-time to *all* abstract domains. The broader the scale at which you reason (whether spatially, or by interacting at a high level with societies, philosophy, logic), the more \"distant\" nodes you need to take into account.\n\n**c) Centralized planning**. Everything-is-connected has implications not only for making accurate predictions, but also for *planning*. At a basic level, we can propagate our updates throughout our entire world-model: our noticing that the interlocutor is distracted by the argument would change our distribution over the chess moves they'd make. But we can also *act* in one domain with the deliberate aim of causing a consequence in a different domain: distract the interlocutor on purpose so we can win easier.\n\nThis implies a centralized planning capability: a module in the agent that can access every feature of the world-model, \"understand\" any of them, and incorporate them in plans together.\n\nImportantly, it *would* need to be universal access, since it's often impossible to predict *a priori* which facts will end up relevant for any given goal. The behavior of market economies may end up relevant for formalizing reasoning under uncertainty; the success of a complex social deception may end up coupled with weather patterns on Mars.\n\nJust like every component of the world-model would need to interact with each other, so would they need to be able to interact with the planner.\n\n**d) The Dehaene Model of Consciousness**. I'll close with [an argument from neuroscience](https://www.lesswrong.com/posts/x4n4jcoDP7xh5LWLq/book-summary-consciousness-and-the-brain). There's a very solid amount of evidence showing that \"pool all information about the world together into one place and cross-correlate it\" is a distinct neural event, and those are exactly the moments at which we seem to be conscious (and therefore capable of general reasoning). In-between those moments, on the other hand, the information remains in different compartments of the brain, and the brain only acts on it using fairly primitive heuristics.\n\nThat seems to fit fairly well with the picture painted by the other arguments.\n\n* * *\n\n### 2D. Are World-Models Modular?\n\nNow it seems worth addressing the opposite failure mode: the possibility that world-models would be strangely *opaque*. Instead of having distinct internal \"submodules\" that we'd recognize as familiar concepts, perhaps they'd morph into a strange mess of heuristics and sub-simulations, such that it's [downright impossible to tell what's going on inside](https://www.lesswrong.com/posts/JLyWP2Y9LAruR2gi9/can-we-efficiently-distinguish-different-mechanisms)?\n\nMost of the arguing against this has already been done, back in the section 1B. It's the premise of the Natural Abstraction Hypothesis, and it's what my interface-development model suggests.\n\nI'll supply one additional one: situational heuristics would remain useful even after the rise of centralized planning. Humans rely on instincts, cognitive shortcuts, and cached computations all the time, we don't manually reason about every little detail. It's much *slower*, for one.\n\nMaking the world-model \"opaque\" would cut off all that functionality, and presumably drastically reduce the capabilities.\n\n* * *\n\n3\\. World-Model Structure\n-------------------------\n\nI've made some arguments on why we should expect world-models to exist, and to convergently take the forms we'd expect of them. None of that, however, helps much when you're staring at trillion-entried matrices trying to figure out which subset of them spells out \"niceness\". The real meat of this approach is on constraining the possible data structures that all world-models must be converging towards — what specific NN features we should be looking for and how they'd look like.\n\nThere's been nonzero progress in this area, by which I mean [the natural abstractions research agenda](https://www.lesswrong.com/tag/natural-abstraction). Indeed: in essence, it aims to determine how efficient world-models are built, founded on the assumptions that certain basic principles of doing so are universal.\n\nAside from 3B, this section mostly consists of my personal speculations.\n\n### 3A. Major Sub-Modules\n\nIt seems that when we say \"world-model\", we're conflating (at least) two things:\n\n*   A repository of concepts/abstractions.\n*   A simulation engine.\n\nThe former is, essentially, declarative knowledge. Dry logical facts which we know we know and can flexibly manipulate and reason over. (Like mathematical equations, or the faces of your friends, or an understanding of what structures count as \"trees\", or the knowledge of how many people are in the room with you.)\n\nThe latter is an ability to run counterfactual scenarios while drawing on the declarative knowledge — an ability to maintain a specific world-state in one's imagination and modify it. (Like having a \"feel\" for how people in the room move, or picturing yourself walking through a forest, or mentally tracking the behavior of a mathematical system to better understand it.)\n\nUsing the two are very distinct experiences, and they have somewhat different functionality. I believe that \"simulations\" extend a bit beyond \"world-models\", actually — that they \"spoof\" the whole mental context (in [the shard theory's parlance](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT#I__Neuroscientific_assumptions)). They access shards' inputs and feed them data from the simulated counterfactual, thereby allowing to \"test out\" shard activations in advance of being in the simulated situation.\n\nThat is useful for several reasons:\n\n*   It improves the quality of predictions. Procedural knowledge is likely implemented in shards, as are various \"hunches\". If you imagine \"walking through a dark forest\" in detail, you may note how scared the darkness makes you feel, and that something in you seems to predict dangerous creatures hiding in that darkness, ready to attack you.  \n      \n    Both of those effects, I think, are shard-based, and may be missed if you only thought about \"walking through a dark forest\" in the abstract. And they're useful: for predicting your own behavior, and for improving predictions in general (since there's presumably a reason you expect dangers in the dark, like an (evolutionary) history of the darkness actually hiding predators).\n*   It allows \"virtual training\": you can hone your heuristics in the simulation, before being faced with the actual situation.\n*   It allows to [gather data on shards](https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model#4__Reverse_Engineering_the_Heuristics). If you have some useful knowledge that's only implemented procedurally, this is a way to make it explicit. (E. g., the \"I'm afraid of the dark and expect monsters to jump out\" fact from the above would become explicitly known, after the simulation has been ran.)\n\nConvergence-wise, there are arguments for all of this functionality. Without the simulation engine, you don't get the ability to simulate future states, which means no ability to compute policy gradients, have \"cross-temporal\" heuristics, or do advanced in-context training. In turn, without the conceptual repository, your ability to improve the simulation engine at runtime is very limited, the available \"richness of counterfactuals\" you can run is limited, and you don't get advanced planning.\n\nInterpretability-wise, however, most of the arguments primarily apply to the conceptual repository, not to the simulation engine. Our ability to perfectly \"snoop\" on any counterfactual an AI is running is less certain:\n\n*   On the one hand, the simulation would be generated drawing on the conceptual repository, and it'd likewise need to interface both with the planner and the shards. In addition, it'd be configured to be compatible with the sensory modalities the AI is using (like our imagination is accessing our visual-processing machinery), so it seems like we're ought to be able to reverse-engineer it.\n*   On the other hand, shard interference would be heavy (moving the simulation from one state to another in opaque ways), and the format in which sensory information enters the GPS/the shards may be completely different from the one it enters the agent in. Inasmuch as it's consistently-formatted, it'd still be very interpretable, but that'd be an entire separate task.\n\nAt the least, I think that we'd definitely be able to get *some* insight into what's happening in the counterfactuals by looking at what concepts are being activated, and how strongly.\n\n... Or so my inside-view on that goes. On a broader view, this section (3A) is the part that most consists of informal speculations on my part.\n\n* * *\n\n### 3B. Abstractions As Basic Units\n\nThe basic units in which information is stored in the conceptual repository seem to be natural abstractions: high-level summaries of lower-level systems that consist only of the information that's [relevant far-away](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction)/[is redundantly represented](https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information)/[constitutes a minimal latent variable](https://www.lesswrong.com/posts/N2JcFZ3LCCsnK2Fep/the-minimal-latents-approach-to-natural-abstractions).\n\nA topic of relevance, here, is what data formats those abstractions follow/what is their type signature. John's [current](https://www.lesswrong.com/posts/jJf4FrfiQdDGg7uco/the-telephone-theorem-information-at-a-distance-is-mediated#Why_Is_This_Interesting_) [speculations](https://www.lesswrong.com/posts/cqdDGuTs2NamtEhBW/maxent-and-abstractions-current-best-arguments) [are](https://www.lesswrong.com/posts/N2JcFZ3LCCsnK2Fep/the-minimal-latents-approach-to-natural-abstractions#_Minimal__Latents) that they're probability distributions over deterministic constraint on the environment.\n\nFor example, imagine a species of trees growing in a savannah. These trees replicate, mutate, and spread; take in energy and matter, and grow. All of that is subject to some variance/entropy, yet its extent is *constrained*. Every replication is imperfect, but the rate or severity of mutation is bounded. Trees spread, but they can only do that in certain conditions or in certain directions. The shapes they take are constrained by their genome and the available resources. And so on.\n\nOnce you compute all of that, you're left with the knowledge of what information *does* get replicated perfectly, in the form of some constraints beyond which environmental entropy doesn't grow. You take in a set of trees \\\\(X_1, X_2, ..., X_n\\\\), and compute from them some abstraction \\\\(P(\\Lambda)\\\\). \\\\(P(\\Lambda)\\\\) contains information like \"trees can have shapes that vary like this\" and \"trees are distributed across the landscape like this\" — probability distributions over possible structures.\n\nIf this result is correct, that's what we should be looking for in ML models: vast repositories of conditional probability distributions of this form.\n\n* * *\n\n### 3C. Higher-Level Organization\n\nOf course, abstractions are not just stored in one big pile. They're connected to each other, forming complex webs. When we think about trees, we can bring to mind their relation to sunlight and water and animals. They're also connected to the lower-level or higher-level abstractions — the abstract concept of a tree relates to the wood and the cells trees are made of, and the forests and ecosystems they make up, and the specific instances of trees we remember.\n\nSeems natural to think that abstractions would have multi-level hierarchies. For example, we may compute a \"first-order\" abstraction of a particular tree species, \\\\(Y_1\\\\). Then we may encounter more tree species, and compute a set of [tree-specie-abstractions](https://en.wikipedia.org/wiki/Genus), \\\\(Y_1, Y_2,...,Y_n\\\\). In the same manner, we may then go further up the ladder of taxonomic ranks, until a fully general \"tree\" abstraction.\n\nHere's a caveat: there are \"abstraction hierarchies\" that are mutually incompatible.\n\nConsider the entire set of humans on Earth. Suppose that you're encountering subsets of them in some order, and you're abstracting over these subsets each time.\n\n*   You may go country-by-country, and end up with a set of \"human of a given ethnicity\" abstractions, with strong constraints on appearance within each first-order abstraction, in addition to generic constraints on what \"humans\" are. (And then compute second-order abstractions over nearby ethnicities, getting races, etc.)\n*   You may go by industries, and end up with \"human mathematician\", \"human artist\", which likewise contain general information on how a \"human\" looks like, but put stronger constraints on interests or aptitudes, rather than on appearances.\n*   You may go by political movements, which would strongly constrain ideological beliefs.\n*   And so on.\n\n([Here's](https://www.lesswrong.com/posts/N2JcFZ3LCCsnK2Fep/the-minimal-latents-approach-to-natural-abstractions?commentId=baq9YJoFpZmcSxJmM) a bit more formalization on this angle, including how we may \"split\" a category like \"all humans\" into several individually-well-abstracting subcategories.)\n\nEach of choice of order would result in a different \"abstraction hierarchy\" — you can't recover the abstraction of a mathematician from first-order abstractions of ethnicities. Yet all of these hierarchies would be made of meaningful, useful abstractions!\n\nA slightly different view presents itself if instead of going [top-down](https://www.lesswrong.com/posts/N2JcFZ3LCCsnK2Fep/the-minimal-latents-approach-to-natural-abstractions), you go [bottom-up](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/3xotPYdAs7GfT9a9r). I. e., rather than picking out all objects of the type \"human\" and trying to summarize them, you can look at the world and try to locate stable, well-abstracting high-level structures from scratch. In this case, looking at the macro-level and focusing on different constraints might yield you \"homes → cities → countries\", or \"businesses → industries → economies\", or \"local bureaucracies → bureaucratic systems → geopolitical entities\". Similarly, those would be useful yet incompatible abstraction hierarchies.\n\nThat's not exactly a *problem*, in my view. We're still ending up with ground-truth-determined natural abstractions that can be computed by looking at the deterministic constraints in the environment. But there's some sense in which choosing to look at a particular constraint (like \"how is the spread of philosophical beliefs constrained?\") locks us out of others (like \"how is the geographical spread of these people constrained?\").\n\nBased on this, it seems that the set of all abstractions over some underlying system may be organized along the following dimensions:\n\n*   \"Hierarchically\": from less-abstract concepts to more and more abstract ones (specific trees → tree species; groupings of people-and-tech → businesses).\n*   \"By layers\": based on the choice of constraints (people by ethnicity vs. by profession; focus on the flow of ideas vs. patterns of economic activity).\n*   \"Horizontally\": different concepts on the same layer and the same hierarchy level (different tree species, or how specific humans and animals and trees occupy roughly \"the same\" layer of abstraction).\n\n* * *\n\n### 3D. Laziness\n\n[World-models are lazy](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans?commentId=3Rn52bqwLEbahmipd), in a [programmer's sense](https://en.wikipedia.org/wiki/Lazy_evaluation). That is, we don't keep the entire state of the world in our heads all at once. We keep it in a compressed form, and only compute the specifics about particular aspects of it on demand. It's as much a consequence of embedded agency as the natural abstractions themselves — the peril of having to reason about a world that you're part of.\n\nLet's consider how people do it. I see three primary methods:\n\n1.  Suppose you're given the rules for some system, plus its initial state (e. g., *Conway's Game of Life* \\+ some distribution of living cells). You can then directly simulate that system step-by-step, up to your working-memory limits. And if you can locate some good emergent abstractions in the system, that lessens the strain on your working memory (e. g., it's easier to reason about \"gliders\" than about the specific cells they're made of).\n2.  Suppose you're given the list of personality traits of some person. To get an idea of how it'd be like to interact with them, you access your general model of human psychology, then incorporate information about that specific person into it, and get an idea of how they'd behave.\n    *   In abstraction terms, you take your general \"human\" abstraction \\\\(P(H)\\\\), condition it on a set of facts \\\\(F\\\\), and get the probability distribution \\\\(P(H|F)\\\\) over that specific human.\n3.  Suppose you're prompted to think in detail about the dynamics of some system — e. g., how a market would behave in response to some unusual event. You access your model of the lower-level details of that system (e. g., interactions between individual traders, with all the details of their psychology), and consider how they'd respond in aggregate.\n    *   In abstraction terms, you look at your \"market\" abstraction \\\\(P(M)\\\\), go down a hierarchy step to the lower-level abstraction \"trader\" \\\\(P(T)\\\\), then \"sample\" from \\\\(P(T)\\\\) to get a few individual trader-instances \\\\(T_1, T_2,...,T_n\\\\), and then you simulate their interactions.\n    *   Or you can think about *Game of Life*'s gliders again. When they're traveling over empty space, you can just project their position along a line, but if they near other live cells, you have to think about their constituent cells in detail.\n\nThus, laziness seems to be possible due to a combination of the \"simulation engine\" from 3A, and the fact that our environment is well-abstracting.\n\n* * *\n\n4\\. Research Directions\n-----------------------\n\nI believe it's a highly tractable avenue of research, and the shortest path to robust alignment in worlds where alignment is really hard[^fwt9njggqtm]. It's the one part of agent foundations that seems *necessary* to get right.\n\nWays to contribute:\n\n*   The Natural Abstractions research agenda, obviously. See e. g. [the pragmascope idea](https://www.lesswrong.com/posts/gdEDPHjCY5DKsMsvE/the-pragmascope-idea) in particular.\n*   [Selection Theorems](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents) focused on world-models specifically (along the lines of the Gooder Regulator theorem). What constraints can we put on world-model structure? E. g., can we formalize my \"simulator vs. conceptual repository\" distinction from 3A?\n*   More broad theory of world-modeling. I'm currently thinking in terms of Bayes nets and some [ad-hoc causal models](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model), which seems very limiting — like trying to write a web browser in assembler.\n    *   Are there any extant \"feature-rich\" frameworks for building world-models, that would also suffice for reasoning about the real world in detail?\n    *   This also seems like a decent alternate starting point for selection-theorem-style thinking. Instead of trying to constrain how world-models *must* be, one can try to invent from scratch a bunch of efficient world-modelling tools, then see whether they follow some consistent, uniquely efficient principles — such that *any* efficient world-model must follow them as well.\n*   General understanding of how information is represented in ML models. I expect this problem is a bit distinct from the challenges this post discusses — the same way breaking an encryption is different from learning to translate a different language.\n    *   (Though perhaps this is a topic for a separate post.)\n*   Inventing training setups that generate pure world-models/[simulators](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators), while guaranteeing no [risk of mesa-optimization](https://www.lesswrong.com/posts/P6aDYBDiu9DyvsF9g/are-generative-world-models-a-mesa-optimization-risk). *These world-models would not necessarily need to be outputting anything human-readable*. We'd just need a theoretical assurance that the training setup is learning a world-model (see the Gooder Regulator Theorem for what that may mean). Difficulties:\n    *   The central issue, the reason why \"naive\" approaches for just training a ML model to make good prediction will likely result in a mesa-optimizer, is that all such setups are \"outer-misaligned\" by default. They don't optimize AIs towards being good world-models, they optimize them for making *specific* good predictions and then *channeling* these predictions through a low-bandwidth communication channel. (Answering a question, predicting the second part of a video/text, etc.)\n    *   That is, they don't just simulate a world: they simulate a world, then locate some specific data they need to extract from the simulation, and translate them into a format understandable to humans.\n    *   As simulation complexity grows, it seems likely that these last steps would require powerful general intelligence/GPS *as well*. And at that point, it's entirely unclear what mesa-objectives/values/shards it would develop. (Seems like it almost fully depends on the structure of the goal-space. And imagine if e. g. \"translate the output into humanese\" and \"convince humans of the output\" are very nearby, and then the model starts superintelligently optimizing for the latter?)\n    *   In addition, we can't just train simulators \"not to be optimizers\", in terms of locating optimization processes/agents within them and penalizing such structures. It's plausible that advanced world-modeling is impossible without general-purpose search, and it would certainly be necessary inasmuch as the world-model would need to model humans.\n    *   Still, there may be some low-hanging fruits there.\n\n[^fwt9njggqtm]: That is, in worlds that mostly agree with the models of Eliezer Yudkowsky/Nate Soars/John Wentworth, on which we need to get the AGI exactly right to survive. \n\n[^pmp1d45pmp]: In theory, there should be a \"buffer zone\" of capability, between an AGI smart enough to model itself, and an AGI smart enough to hack through interpretability tools (e. g., humans are self-reflective, but not smart enough to do that).But \"is self-reflective\" is also not a binary. An AGI's self-model can be more or less right. On the lower capability level, it'll probably be very flawed, therefore not very useful. On the flipside, if it's very close to reality, the AGI is likely to be smart enough that reading its mind is dangerous.We may easily misjudge that, too. An AGI that achieved self-awareness is likely already on the cusp of its sharp left turn, past which it'd be unsafe to interpret. Depending on how sharp the turn is, that \"buffer zone\" may be passed in the blink of an eye, easily missed. \n\n[^7k676furwqc]: Runtime-editability also reassures another concern: that the inferences the AGI makes at runtime would be encoded differently from the knowledge hard-coded into its parameters. But since both types of knowledge would be used as inputs into the same algorithms (the planner, the shards), there's probably no reason to expect much mutation by default, due to the need for backwards compatibility.(To be clear, once the AGI undergoes the sharp left turn/goes FOOM, and starts designing successor agents or directly modifying itself or just becomes incomprehensibly superintelligent, then this'll obviously stop applying. But if we haven't aligned it by then, we're dead either way, so that's irrelevant.)",
      "plaintextDescription": "Summary, by sections:\n\n 1. Perfect world-model interpretability seems both sufficient for robust alignment (via a decent variety of approaches) and realistically attainable (compared to \"perfect interpretability\" in general, i. e. insight into AIs' heuristics, goals, and thoughts as well). Main arguments: the NAH + internal interfaces.\n 2. There's plenty of reasons to think that world-models would converge towards satisfying a lot of nice desiderata: they'd be represented as a separate module in AI cognitive architecture, and that module would consists of many consistently-formatted sub-modules representing recognizable-to-us concepts. Said \"consistent formatting\" may allow us to, in a certain sense, interpret the entire world-model in one fell swoop.\n 3. We already have some rough ideas on how the data in world-models would be formatted, courtesy of the NAH. I also offer some rough speculations on possible higher-level organizing principles.\n 4. This avenue of research also seems very tractable. It can be approached from a wide variety of directions, and should be, to an extent, decently factorizable. Optimistically, it may constitute a relatively straight path from here to a \"minimum viable product\" for alignment, even in words where alignment is really hard[1].\n\n----------------------------------------\n\n\n1. Introduction\n\n\n1A. Why Aim For This?\nImagine that we develop interpretability tools that allow us to flexibly understand and manipulate an AGI's world-model — but only its world-model. We would be able to see what the AGI knows, add or remove concepts from its mental ontology, and perhaps even use its world-model to run simulations/counterfactuals. But its thoughts and plans, and its hard-coded values and shards, would remain opaque to us. Would that be sufficient for robust alignment?\n\nI argue it would be.\n\nPrimarily, this would solve the Pointers Problem. A central difficulty of alignment is that our values are functions of highly abstract variables, and tha",
      "wordCount": 6350
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "Z38PqJbRyfwCxKvvL",
        "name": "Research Agendas",
        "slug": "research-agendas"
      },
      {
        "_id": "oFpCNzqBd6tzCuxLa",
        "name": "World Modeling Techniques",
        "slug": "world-modeling-techniques"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nwLQt4e7bstCyPEXs",
    "title": "Internal Interfaces Are a High-Priority Interpretability Target",
    "slug": "internal-interfaces-are-a-high-priority-interpretability",
    "url": null,
    "baseScore": 26,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2022-12-29T17:49:27.450Z",
    "contents": {
      "markdown": "**tl;dr**: ML models, like all software, and like [the NAH](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction) would predict, must consist of several specialized \"modules\". Such modules would form interfaces between each other, and exchange consistently-formatted messages through these interfaces. Understanding the internal data formats of a given ML model should let us comprehend an outsized amount of its cognition, and allow to flexibly *interfere* in it as well.\n\n* * *\n\n1\\. A Cryptic Analogy\n---------------------\n\nLet's consider three scenarios. In each of them, you're given the source code of a set of unknown programs, and you're tasked with figuring out their exact functionality. Details vary:\n\n1.  In the first scenario, the programs are written in some known programming language, e. g. Python.\n2.  In the second scenario, the programs were randomly generated by perturbing machine code until it happened to end up in a configuration that, when ran, instantiates a process externally indistinguishable from a useful intelligently-written program.\n3.  In the third scenario, the programs are written in a programming language that's completely unfamiliar to you (or to anyone else).\n\nIn the first scenario, the task is all but trivial. You read the source code, make notes on it, run parts of it, and comprehend it. It may not be quick, but it's straightforward.\n\nThe second scenario is a nightmare. There must be some structure to every program's implementation — something like [the natural abstraction hypothesis](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction) must still apply, there must be modules in this mess of a code that can be understood separately, etc. There *is* some high-level structure that you can [parcel out into tiny pieces that can fit into a human mind](https://www.lesswrong.com/posts/w26TY2tFHRTdposre/why-are-some-problems-super-hard?commentId=CsHFp7pWJYRRfoJGX). The task is not *impossible*.\n\nBut suppose you've painstakingly reverse-engineered one of the programs this way. You move on to the next one, and... Yup, you're essentially starting from zero. Well, you've probably figured out something about natural abstractions when working on your first reverse-engineering, so it's *somewhat* easier, but still a nightmare. Every program is structured in a completely different way, you have to start from the fundamentals every time.\n\nThe third scenario is a much milder nightmare. You don't focus on reverse-engineering the *programs* here — first, you reverse-engineer *the programming language*. It's a task that may be as complex as reverse-engineering one of the individual programs from (2), but once you've solved it, you're essentially facing the same problem as in (1) — a problem that's comparably trivial.\n\nThe difference between (2) and (3) is: \n\n*   In (3), every program has a *consistent* high-level structure, and once you've figured it out, it's a breeze.\n*   In (2), every high-level structure is unique, and you absolutely *require* some general-purpose tools for inferring high-level structures.\n\n(2) is a parallel to the general problem of interpretability, different programs being different ML models. Is there some interpretability problem that's isomorphic to (3), however?\n\nI argue there is.\n\n* * *\n\n2\\. Interface Theory\n--------------------\n\nSuppose that we have two separate entities with different specializations: they both can do some useful \"work\", but there are some types of work that only one of them can perform. Suppose that they want to collaborate: combine their specializations to do tasks neither entity can carry out alone. How can they do so?\n\nFor concreteness, imagine that the two entities are a Customer, which can provide any resource from the set of resources \\\\(R\\\\), and an Artist, which can make any sculpture from some set \\\\(S\\\\) given some resource budget. They want to \"trade\": there's some sculpture \\\\(s_x\\\\) the Customer wants made, and there are some resources \\\\(r_x\\\\) the Artist needs to make it. How can they carry out such exchanges?\n\nThe Customer needs to send some message \\\\(M_{s_x}\\\\) to the Artist, such that it would uniquely identify \\\\(s_x\\\\) among the members of \\\\(S\\\\). The Artist, in turn, would need to respond with some message \\\\(M_{r_x}\\\\), which uniquely identifies \\\\(r_x\\\\) in the set \\\\(R\\\\).\n\nThat implies an *interface*: a data channel between the two entities, such that every message passed along this channel from one entity to another uniquely identifies some action the receiving entity must take in response.\n\nThis, in turn, implies the existence of *interpreters*: some protocols that take in a message received by an entity, and map it onto the actions available to that entity (in this case, making a particular sculpture or passing particular resources).\n\nFor example, if the sets of statues and resources are sufficiently small, both entities can agree on some orderings of these sets, and then just pass numbers. \"1\" would refer to \"statue 1\" and \"resource package 1\", and so on. These protocols would need to be obeyed *consistently*: they'd have to always use the same ordering, such that the Customer saying \"1\" always means the same thing. Otherwise, this system falls apart.\n\nNow, suppose that a) the sets are really quite large, such that sorting them would take a very long time, yet b) members of sets are *modular*, in the sense that each member of a set is a composition of members of smaller sets. E. g., each sculpture can be broken down into the number of people to be depicted in it, the facial expressions each person has, what height each person is, etc.\n\nIn this case, we might see this modularity reflected *in the messages*. Instead of specifying statues/resources holistically, they'd specify modules: \"number of people: *N*, expression on person 1: type 3...\", et cetera.\n\nTo make use of this, the Artist's interpreter would need to know what \"expression on person 1\" translates to, in terms of specifications-over\\\\(\\\\)-\\\\(S\\\\). And as we've already noted, this meaning would need to be stable across time: \"expression on person 1\" would need to always mean the same thing, from request to request.\n\nThat, in turn, would imply *data formats*. Messages would have some consistent structure, such that, if you knew the rules by which these structures are generated, you'd be able to understand *any* message exchanged between the Customer and the Artist *at any point*.\n\n* * *\n\n3\\. Internal Interfaces\n-----------------------\n\nSuppose that we have a ML model with two different modules, each of them specialized for performing different computations. Inasmuch as they'll work together, we'll see the same dynamics between them as between the Customer and the Artist: they'll arrive at some consistent data formats for exchanging information. They'll form an *internal interface*.\n\nConnecting this with the example from Section 1, I think this has interesting implications for interpretability. Namely: *internal interfaces are the most important interpretability targets.*\n\nWhy? Two reasons:\n\n**Cost-efficiency**. Understanding a data format would allow us to understand everything that can be specified using this data format, which may shed light on entire *swathes* of a model's cognition. And as per the third example in Section 1, it should be significantly easier than understanding an arbitrary feature, since every message would follow the same high-level structure.\n\n**Editability.** Changing a ML model's weights while causing the effects you want and *only* the effects you want is notoriously difficult — often, a given feature is only responsible for a *part* of the effect you're ascribing to it, or it has completely unexpected side-effects. An interface, however, is a channel specifically built for communication between two components. It should have minimal side-effects, and intervening on it should have predictable results.\n\nAs far as controlling the AI's cognition goes, internal interfaces are the most high-impact points.\n\n* * *\n\n4\\. What Internal Interfaces Can We Expect?\n-------------------------------------------\n\nThis question is basically synonymous with \"what modules can we expect?\". There are currently good arguments for the following:\n\n*   The world-model (WM), i. e. a repository of [abstractions](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction).\n*   A set of contextually-activated heuristics/[shards](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values).\n*   A \"planner\"/method of [general-purpose search](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see) (GPS).\n\nHere's a breakdown of the potential interfaces as I see them:\n\n1.  **WM → Shards**. Each shard would learn to understand some part of the world-model, and that's what would provide the \"context\" for their activations.\n    *   However, I expect this is less of an interface, and more of a *set* of interfaces. I expect that each shard learns to interface with the WM in its own way, so they wouldn't follow consistent input formats.\n2.  **Shards → Shards**: a \"shard coordination mechanism\" (SCM). Shards necessarily conflict as part of their activity, biding for mutually incompatible plans/actions, and there needs to be some mechanism for resolving such conflicts. That mechanism would need to *know* about any possible conflicts, however, meaning all the shards would need to interface with it and signal it how much each of them wants to fire in a given case.\n3.  **SCM → GPS**. Shards influence and shape plan-making, and [I suspect](https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model#3__Interfaces) that GPS would appear after the SCM, so the GPS would just be plugged into an already-existing mechanism for summarizing shard activity.\n4.  **WM → GPS**. The GPS is, in a sense, just a principled method for drawing upon the knowledge in the world-model, so interfacing with the WM is its primary function.\n5.  **GPS → WM**. I suspect that another primary function of the GPS is to organize/modify the world-model at runtime, such as by discovering new abstractions or writing new heuristics to it. As such, it'd need *write-access* as well.\n6.  **GPS → GPS**. The planner would need to understand its own past thoughts, for the purposes of long-term planning. (Though part/most of them might be just written to the world-model directly, i. e. there's no *separate* format for GPS' plans.)\n7.  **World → Agent**. In a sense, the world-model isn't just a module, it's *itself* an interface between the ground truth of the world and the decision-making modules of the agent (shards and the GPS)! Thus, we can expect all elements of the world-model (concepts, abstractions...) to be consistently-formatted.\n\n\"Cracking\" any of these interfaces should give us a lot of control over the ML model: the ability to identify specific shards, spoof or inhibit their activations, literally read off the AI's plans, et cetera.\n\nI'm particularly excited about (7) and (5), though: if the world-model is itself an interface, it must be consistently-formatted *in its entirety*, and getting the ability to arbitrarily navigate and edit it would, perhaps, allow us to solve the entire alignment problem on the spot. (A post elaborating on this is upcoming.)\n\n(6) is also extremely high-value, but might be much more difficult to decode.\n\n* * *\n\n5\\. Research Priorities\n-----------------------\n\n*   This implies that identifying internal modules in ML models, and understanding what \"modularity\" even is, is highly important. I'd like to signal-boost [the work done by TheMcDouglas, Avery, and Lucius Bushnaq](https://www.lesswrong.com/s/ApA5XmewGQ8wSrv5C) here, which focuses on exactly this.\n*   More broadly, [selection theorems](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents) focused on constraining an agent's internal structure, and therefore identifying potential modules all agents would have within themselves, are very useful research avenues.\n*   Alternatively, we can think about this from the *opposite* angle. Instead of wondering what interfaces/modules arise naturally, we can consider how to *artificially encourage* specific modules/interfaces. As an example, [here's an old paper](https://worldmodels.github.io/) that trains a separate world-model, then a much simpler controller/agent on top of it. Replicating this feat for AGI-level models would be incredibly valuable (although [non-trivally difficult](https://www.lesswrong.com/posts/P6aDYBDiu9DyvsF9g/are-generative-world-models-a-mesa-optimization-risk)).\n*   A yet another perspective: instead of focusing on interfaces, we can try to come up with ways to incentivize consistent internal structures.\n    *   For example, consider shards. While they'd all have interfaces with the WM (inputs) and the SCM (outputs), there's no pressure at all for them to have the same *internal* structure. Coming back to Section 1, they'd resemble randomly-generated programs (scenario 2), not programs written in some consistent (if unfamiliar to us) programming language (scenario 3).\n    *   Is there a way to change this? To somehow encourage the ML model to give (a subset of) its components *the same* high-level structure, so that we may understand all of them in one fell swoop?\n*   In addition: Interpreting an AGI-level model in an ad-hoc \"manual\" manner, feature-by-feature, [as we do it now](https://transformer-circuits.pub/), would obviously be a hopeless task. Too many features. One way of dealing with this is to build [general-purpose tools for discovering high-level structures](https://www.lesswrong.com/posts/A7QgKwWvAkuXonAy5/how-do-selection-theorems-relate-to-interpretability). But leveraging interfaces potentially allows a different path: a way of understanding vast chunks of a model's cognition using only ad-hoc interpretability methods.\n    *   I wouldn't actually place my hopes on this alternative — we'd essentially need to wait until we actually *have* the AGI at our hands to even begin this work, and it'll probably be too late by then. But if we somehow fail to get said general-purpose tools for discovering structure, it may not be *totally* hopeless.",
      "plaintextDescription": "tl;dr: ML models, like all software, and like the NAH would predict, must consist of several specialized \"modules\". Such modules would form interfaces between each other, and exchange consistently-formatted messages through these interfaces. Understanding the internal data formats of a given ML model should let us comprehend an outsized amount of its cognition, and allow to flexibly interfere in it as well.\n\n----------------------------------------\n\n\n1. A Cryptic Analogy\nLet's consider three scenarios. In each of them, you're given the source code of a set of unknown programs, and you're tasked with figuring out their exact functionality. Details vary:\n\n 1. In the first scenario, the programs are written in some known programming language, e. g. Python.\n 2. In the second scenario, the programs were randomly generated by perturbing machine code until it happened to end up in a configuration that, when ran, instantiates a process externally indistinguishable from a useful intelligently-written program.\n 3. In the third scenario, the programs are written in a programming language that's completely unfamiliar to you (or to anyone else).\n\nIn the first scenario, the task is all but trivial. You read the source code, make notes on it, run parts of it, and comprehend it. It may not be quick, but it's straightforward.\n\nThe second scenario is a nightmare. There must be some structure to every program's implementation — something like the natural abstraction hypothesis must still apply, there must be modules in this mess of a code that can be understood separately, etc. There is some high-level structure that you can parcel out into tiny pieces that can fit into a human mind. The task is not impossible.\n\nBut suppose you've painstakingly reverse-engineered one of the programs this way. You move on to the next one, and... Yup, you're essentially starting from zero. Well, you've probably figured out something about natural abstractions when working on your first reverse-engineeri",
      "wordCount": 2092
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "bt2e3HEcZmuHo3xf7",
        "name": "Modularity",
        "slug": "modularity"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wdC8fH8kHffYn3kNa",
    "title": "In Defense of Wrapper-Minds",
    "slug": "in-defense-of-wrapper-minds",
    "url": null,
    "baseScore": 24,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 38,
    "createdAt": null,
    "postedAt": "2022-12-28T18:28:25.868Z",
    "contents": {
      "markdown": "Recently, there's been a [strong push](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX) against \"[wrapper-minds](https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals)\" as a framework. It's argued that there's no specific reason to think that all sufficiently advanced agents would format their goals in terms of expected-utility maximization over future trajectories, and that this view predicts severe problems with e. g. [Goodharting](https://www.lesswrong.com/posts/rauMEna2ddf26BqiE/alignment-allows-nonrobust-decision-influences-and-doesn-t) that just wouldn't show up in reality.[^eqzoggi740i]\n\n I think these arguments have merit, and the Shard Theory's model definitely seems to correspond to a real stage in agents' [value formation](https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model).\n\nBut I'd like to offer a fairly prosaic argument in favor of wrapper-minds.\n\n* * *\n\nSuppose that we have some agent which is being updated by some [greedy](https://www.lesswrong.com/posts/ThtZrHooK7En9mcZr/greed-is-the-root-of-this-evil) optimization process (the SGD, evolution, etc.). On average, updates tend to decrease the magnitude of every subsequent update — with each update, the agent requires less and less correction.\n\nWe can say that this process optimizes the agent for good performance according to some reward function \\\\(R\\\\), or that it chisels \"effective cognition\" into that agent according to some rule.\n\nThe wrapper-mind argument states that any \"sufficiently strong\" agent found by this process would:\n\n1.  Have an explicit representation of \\\\(R\\\\) inside itself, which it would explicitly pursue.\n2.  Pursue *only* \\\\(R\\\\), at the expense of everything else in the universe.\n\nI'll defend them separately.\n\n**Point 1.** It's true that explicit \\\\(R\\\\)-optimization is suboptimal for many contexts. [Consequentialism is slow](https://www.lesswrong.com/posts/a3LncviZ6rkrTo8jJ/the-unified-theory-of-normative-ethics), and shallow environment-optimized heuristics often perform just as well while being much faster. Other environments can be just \"solved\" — an arithmetic calculator doesn't need to be a psychotic universe-eater to do its job correctly. And for more complex environments, we can have shard economies, whose collective goals, taken in sum, would be a strong proxy of \\\\(R\\\\).\n\nBut suppose that the agent's training environment is very complex and very diverse indeed. Or, equivalently, that it sometimes jumps between many very different and complex environments, and sometimes ends up in entirely novel, never-before-seen situations. We would still want it to do well at \\\\(R\\\\) in all such cases[^gaicze5g0a4]. How can we do so?\n\nJust \"solving\" environments, as with arithmetic, may be impossible or computationally intractable. Systems of heuristics or shard economies also [wouldn't be up to the task](https://www.lesswrong.com/posts/rauMEna2ddf26BqiE/alignment-allows-nonrobust-decision-influences-and-doesn-t?commentId=DTF4fkK4v2yfRzPGC) — whatever proxy goal they're optimizing, there'd be at least one environment where it decouples from \\\\(R\\\\).\n\nIt seems almost tautologically true, here, that the only way to keep an agent pointed at \\\\(R\\\\) given this setup is to *explicitly point it at *\\\\(R\\\\). Nothing else would do!\n\nThus, our optimization algorithm would *necessarily* find an \\\\(R\\\\)-pursuer, if it optimizes an agent for good performance across a sufficiently diverse (set of) environment(s).\n\n**Point 2.** But why would that agent be shaped to pursue *only* \\\\(R\\\\), and so strongly that it'll destroy everything else?\n\nThis, more or less, also has to do with environment diversity, plus some instrumental convergence.\n\nAs the optimization algorithm is shaping our agent, the agent will be placed in environments where it has preciously few resources, or a low probability of scoring well at \\\\(R\\\\) (= high probability of receiving a strong update/correction after this episode ends).\n\nWithout knowing when such a circumstance would arise, how can we prepare our agent for this?\n\nWe can make it optimize for \\\\(R\\\\) *strongly*, as strongly as it can, in fact. Acquire as much resources as possible, spend them on nothing but \\\\(R\\\\)-pursuit, minimize uncertainty of scoring well at \\\\(R\\\\), and so on.\n\nEvery goal that *isn't* \\\\(R\\\\) would distract from \\\\(R\\\\)-pursuit, and therefore lead to failure at some point, and so our optimization algorithm would eventually update such goals away; with update-strength proportional to how distracting a goal is.\n\nEvery missed opportunity to grab resources that can be used for \\\\(R\\\\)-pursuit, or a failure to properly optimize a plan for \\\\(R\\\\)-pursuit, would eventually lead to scoring bad at \\\\(R\\\\). And so our optimization algorithm would instill a drive to *take* all such opportunities.\n\nThus, any greedy optimization algorithm would convergently shape its agent to not only pursue \\\\(R\\\\), but to *maximize* for \\\\(R\\\\)'s pursuit — at the expense of everything else.\n\n* * *\n\nWhat should we take away from this? What should we *not* take away from this?\n\n*   I should probably clarify that I'm not arguing that inner alignment isn't a problem, here. Aligning a wrapper-mind to a given goal is a very difficult task, and one I expect \"blind\" algorithms like the SGD to [fail horribly at](https://www.lesswrong.com/posts/ThtZrHooK7En9mcZr/greed-is-the-root-of-this-evil).\n*   I'm not saying that the shard theory is incorrect — as I'd said, I think shard systems are very much a real developmental milestone of agents.\n\nBut I do think that we should very strongly expect the SGD to move its agents *in the direction of* \\\\(R\\\\)-optimizing wrapper-minds. Said \"movement\" would be [very](https://www.lesswrong.com/posts/ThtZrHooK7En9mcZr/greed-is-the-root-of-this-evil) [complex](https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model), a nuanced path-dependent process that might lead to surprising end-points, or (as with humans) might terminate at a halfway point. But it'd still be movement in that direction!\n\nAnd note the fundamental reasons behind this. It isn't because wrapper-mind behavior is convergent for any intelligent entity. Rather, it's a straightforward consequence of every known process for *generating* intelligent entities — the paradigm of local updates according to some outer function. Greedy optimization processes essentially search for mind-designs that would pre-empt any update the greedy optimization process would've made to them, so these minds come to incorporate the update rule and act in a way that'd merit a minimal update. *That*'s why. (In a way, it's because greedy optimization processes are *themselves* goal-obsessed wrappers.)\n\nWe wouldn't get *clean* wrapper-minds out of all of this, no. But they, and concerns related to them, still merit central attention.\n\n[^eqzoggi740i]: Plus some more fundamental objections to utility-maximization as a framework, on which I haven't properly updated on yet, but which (I strongly expect) do not contradict the point I want to make in this post. \n\n[^gaicze5g0a4]: That is, we would shape the agent such that it doesn't require a strong update after ending up in one of these situations.",
      "plaintextDescription": "Recently, there's been a strong push against \"wrapper-minds\" as a framework. It's argued that there's no specific reason to think that all sufficiently advanced agents would format their goals in terms of expected-utility maximization over future trajectories, and that this view predicts severe problems with e. g. Goodharting that just wouldn't show up in reality.[1]\n\n I think these arguments have merit, and the Shard Theory's model definitely seems to correspond to a real stage in agents' value formation.\n\nBut I'd like to offer a fairly prosaic argument in favor of wrapper-minds.\n\n----------------------------------------\n\nSuppose that we have some agent which is being updated by some greedy optimization process (the SGD, evolution, etc.). On average, updates tend to decrease the magnitude of every subsequent update — with each update, the agent requires less and less correction.\n\nWe can say that this process optimizes the agent for good performance according to some reward function R, or that it chisels \"effective cognition\" into that agent according to some rule.\n\nThe wrapper-mind argument states that any \"sufficiently strong\" agent found by this process would:\n\n 1. Have an explicit representation of R inside itself, which it would explicitly pursue.\n 2. Pursue only R, at the expense of everything else in the universe.\n\nI'll defend them separately.\n\nPoint 1. It's true that explicit R-optimization is suboptimal for many contexts. Consequentialism is slow, and shallow environment-optimized heuristics often perform just as well while being much faster. Other environments can be just \"solved\" — an arithmetic calculator doesn't need to be a psychotic universe-eater to do its job correctly. And for more complex environments, we can have shard economies, whose collective goals, taken in sum, would be a strong proxy of R.\n\nBut suppose that the agent's training environment is very complex and very diverse indeed. Or, equivalently, that it sometimes jumps between many very ",
      "wordCount": 936
    },
    "tags": [
      {
        "_id": "b6tJM7Lza74rTfCBF",
        "name": "Goal-Directedness",
        "slug": "goal-directedness"
      },
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "zuwsLCbxbugqB7FQY",
        "name": "Shard Theory",
        "slug": "shard-theory"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "xAzKefLsYdFa4SErg",
    "title": "Accurate Models of AI Risk Are Hyperexistential Exfohazards",
    "slug": "accurate-models-of-ai-risk-are-hyperexistential-exfohazards",
    "url": null,
    "baseScore": 33,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 38,
    "createdAt": null,
    "postedAt": "2022-12-25T16:50:24.817Z",
    "contents": {
      "markdown": "(Where \"an exfohazard\" is [information which leads to bad outcomes if known by a large fraction of society](https://www.lesswrong.com/posts/yET7wbjjJZtpz6NF3/don-t-use-infohazard-for-collectively-destructive-info).)\n\nLet us suppose that we've solved the technical problem of AI Alignment — i. e., the problem of AI *control*. We have some method of reliably pointing our AGIs towards the tasks or goals we want, such as the universal flourishing of all sapient life. As per [the Orthogonality Thesis](https://www.lesswrong.com/tag/orthogonality-thesis), no such method would allow us to *only* point it at universal flourishing — any such method would allow us to point the AGI at *anything whatsoever*.\n\nWhich means that, if we succeed at the technical problem, there'll be a moment at the very end of the world as we know it, where a person or a group of people will be making a decision regarding the future of the universe.\n\nAbove and beyond preventing an omnicide, we need to ensure that in the timelines where we do solve the technical problem, [this decision is made *right*](https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future).\n\n1\\. Establishing the Framework\n------------------------------\n\nThe one good thing about the technical problem of alignment is that it makes hyperexistential risks — [the risks of astronomical suffering](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks) — very unlikely.\n\nThe problem of AI Alignment can be viewed as the problem of encoding our preferences into an AGI, bit by bit. The strength of alignment tools, in turn, translates to *how many* bits we can encode. With the current methods of end-to-end training, we're essentially sampling preferences at random. Perfect interpretability and parameter-surgery tools would allow us to encode an arbitrary amount of bits. The tools we'll actually have will be somewhere between these two extremes.\n\n\"Build us our perfect world\" is a very complicated ask, and it surely takes up many, many thousands of bits. That's why alignment is hard.\n\n\"Build us a hell\" is its mirror. It's essentially the same ask, except for a flipped sign. As such, specifying it would require pretty much the same amount of bits.\n\nThus, in the timelines where we have alignment tools advanced enough to build a hell-making AGI, it's overwhelmingly likely that we have the technical capability to build an utopia-building AGI. On the flipside, conditioning on our inability to build an utopia-builder, our tools are probably so bad we can't come *close* to a hell-builder. In that case, we just sample some random preferences, and the AGI kills us quickly and painlessly.\n\nScrewing up so badly we create a suffering-maximizer is vanishingly unlikely: it's only possible in a very, very narrow range of technical capabilities.\n\nNote the emphasis, though: *technical* capabilities.\n\nThe question of how these capabilities are *used* is an entirely different one.\n\n2\\. How Bad Can It Be?\n----------------------\n\nThe preferences of most people, and most *groups* of people, are not safe to enforce upon the future.\n\nIt's trivially true if we look upon the history: non-secular countries would consider it Just to see billions of people eternally tortured in their gods' hells, xenophobic nations would genocide their enemies, power-hungry sociopaths ruling [moral mazes](https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS) would instinctively wish to erase all value from the universe, and so on.\n\nLess than a century ago, most \"progressive countries\" would've erased today's protected minorities. And even the random sample of *contemporary* \"good guys\" would probably be happy to do something extremely monstrous to, say, billionaires or rapists or something.\n\nThen there's the problem of \"human chauvinism\". Not even mundane *humanism* is safe to maximize, inasmuch as it may exclude sapient animals, uploads, AIs, or aliens.\n\nThe bottom line is, most people are not drunk on overly nuanced sci-fi-ish altruistic philosophy. \"What is the universal good, defined mathematically?\" is a question that's extremely off-distribution for them; most didn't spend a *minute* in their life seriously contemplating it, and have none of the requisite background. They don't have coherent preferences over the far-off future, and if forced to compile them on the spot, they'd produce something incoherent and pretty *bad*. Worse than annihilation.\n\nAnd that's not even taking into account people who've actually been *selected* for monstrous qualities, or incentivized to develop them. And such people are most likely to end up in power, inasmuch as power is correlated with signaling ruthlessness or radical patriotism, and being willing to climb to the top while trampling others underfoot.\n\nSo we can't trust major systems, we can't trust the public consensus, we can't trust random people, and we can't trust random powerful people.\n\nAs such, there's a major *sociopolitical* dimension to AI Risk — beyond ensuring that we can point the AGI at the utopia, we need to ensure that the AGI *actually ends up pointed at it*.\n\nOtherwise, getting AI Alignment solved would be [much *worse*](https://www.lesswrong.com/posts/CtXaFo3hikGMWW4C9/the-case-against-ai-alignment) than staying back and letting humanity paperclip themselves out of existence. \n\n3\\. What Does This Mean For Our AI Policy Work?\n-----------------------------------------------\n\nTo be clear, that doesn't mean that I think we should stop all sociopolitical activism. We just need to be *careful* about it. The specific outcomes we want to avoid are:\n\n*   The higher echelons of some government or military develop an accurate model of AI Risk.\n    *   They'd want to enforce their government's superiority, or national superiority, or ideological superiority, and they'd trample over the rest of humanity.\n    *   There are no eudaimonia-interested governments on Earth.\n*   The accurate model of AI Risk makes its way into the public consciousness.\n    *   The \"general public\", as I've outlined, is not safe either. And in particular, what we don't want is some \"transparency policy\" where the AGI-deploying group is catering to the public's whims regarding the AGI's preferences.\n    *   Just look at modern *laws*, and the preferences they imply! Humanity-in-aggregate is not eudaimonia-aligned either.\n*   A large subset of wealthy or influential people not pre-selected by their interest in EA/LW ideas form an accurate model of AI Risk.\n    *   We'd either get some revenue-maximizer for a given corporation, or a dystopian dictatorship, or some such outcome.\n    *   And even if the particular influential person is *conventionally nice*, we get all the problems with sampling a random nice individual from the general population (the off-distribution problem).\n\nBy \"accurate model\" here I mean \"the orthogonality thesis + the real power of intelligence + the complexity of human preferences\". The model with enough [gears](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) that it'd allow people to ask, \"aligned to *whose* preferences?\", and then wonder if maybe it can be their *personal* preferences.[^ehamm7i7se6]\n\nNote what this *doesn't* exclude:\n\n*   Communicating a more opaque model of AI Risk to politicians/the public. A model that *just* tells people that scaling up capabilities will lead to bad outcomes, w*ithout* a particularly nuanced understanding of why.\n    *   This should still be sufficient to slow down the timelines, and pass regulations controlling AI development. E. g., the current uproar about AI art theft is a good example. It's not even in the neighborhood of \"hey, can we use this to create an Old Testament God?\", yet it can lead to semi-effective regulations.\n*   Convincing *select* powerful individuals, e. g. the leadership of major AI Labs, or the most prominent AI researchers.\n    *   They weren't strongly pre-selected for monstrousness or an adherence to some outdated ideology, and are plausibly both (1) at-least-conventionally-nice and (2) willing to listen to philosophical arguments.\n    *   If the *system* they're embedded won't go crazy trying to pressure them to develop e. g. a revenue-maximizer, they'll probably be free to use their actual moral reasoning to decide on the AGI's preferences.\n    *   (Not that I'm saying it's *completely* safe. See: the recent case where a EA-aligned billionaire turned out to be... *not* conventionally nice.)\n\nAlso: transferring accurate models is pretty hard, actually. Most people don't take ideas seriously, and the more abstract an idea is, the more unlikely it is to be taken seriously. I think it's a major factor in why [so many economically profitable technologies are successfully restricted](https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai#Restraint_is_not_radical). And inasmuch as \"AI is dangerous\" is much less abstract than \"AI can be used to impose arbitrary values upon the future\", it actually shouldn't be too difficult to increase the timelines without increasing the hyperexistential risk at the same time.\n\nBut one should still be mindful not to succeed *too hard*.\n\n4\\. Wait, So Who Should Be In Charge Then?\n------------------------------------------\n\nA small group of philosophically-erudite, altruistically-minded people, *probably* sampled from this community.\n\nNo, I don't like the optics on that either. It irks my aesthetic senses. It makes me feel like a non-genre-savvy supervillain, especially when I concretely imagine that future of building a doomsday device in secret in my basement.\n\nBut, like, it seems that any other approach is much more likely to lead to bad outcomes, after thinking it through at the object level?\n\nOne point to make, here:\n\n### Power Does Not Corrupt\n\nI think \"power corrupts\" is factually incorrect, as platitudes go. It's almost paradoxical: how exactly can a boost to your ability to enforce your preferences upon reality make these preferences *worse*? And, by implication, does that mean we should expect the *powerless* to be most un-corrupted? The people struggling to make ends meet, driven mad by hunger and deprivation, oppressed and trampled-over — we should expect them to be paragons of ascended philosophical virtue?\n\nNo, what corrupts isn't *power*. What corrupts is the *road* to power, and what one has to do to *keep* power.\n\n1.  The powerful are pre-selected to be the sort of people who primarily optimize for getting power. Thus, they're much more likely than average to be the kinds of people who'd use the handles of knives embedded in others' back as handholds.\n2.  Once one *has* power, it's a constant struggle to maintain that power while protecting yourself from the power-hungry individuals described in (1). Even if you started out decent, it's pretty difficult not to sink to their level. And if you started out bad, why, you'd just get *worse*.\n\n*This* is why most people in power are corrupt. *Not* because \"power\" has a magical property of turning people into monsters.\n\nAnd the people who'd acquire absolute power over the future in this hypothetical would acquire it by very different means, compared to the usual. And they would not need to *hold* onto it.\n\nSo, while it's not guaranteed that they'd be nice, there at least isn't any prior reason to think they'd be *evil*. Updating off \"they're in power\" is incorrect, here: corruption is correlated with power, but not caused by it.\n\n... Which is not to say our community is *safe*. We're as vulnerable to being taken over by power-seekers as any other group.\n\nThis is an *additional* reason not to broadcast how much potential power lies down this road.\n\n5\\. Critique of Specific Ideas\n------------------------------\n\n### Long Reflection\n\nThere's a plan that goes:\n\n*   Figure out \"strawberry alignment\" — i. e., how to make an AI pursue some concrete, localized goal like \"synthesize a single strawberry\", *without* committing omnicide and e. g. tiling the universe with strawberries or weird upstream-strawberry-correlates.\n    *   This is contrasted with more complex goals like \"build an utopia\", which combine the difficulty of AI control with the philosophical difficulty of \"what even is an utopia?\".\n*   Use this weakly-aligned AI to \"end the acute risk period\" — somehow slow down or halt unsafe AGI research world-wide.\n*   There's some exit condition on this research ban: maybe it's lifted after a century, maybe some person or group of people have the authority to lift it, maybe there's some other recognition function on when it's fine to do so.\n*   It's implied that, once the ban is lifted, humanity has matured enough to figure out its preferences and build an AGI implementing them safely.\n\nFirst off, I'm skeptical that \"strawberry alignment\" is a thing. \"Create a strawberry\" is deeply [value-laden](https://arbital.com/p/value_laden/) in itself, it includes all the clauses like \"but don't murder people over it\" and what a \"real strawberry\" means, etc. I think if we can get the AGI to do *that*, if we can encode *this many* bits of preferences into it, we can probably just say \"build an utopia\" and have that command be safely executed too. The AGI will either know what we really mean, or help us figure it out.\n\nHowever, if this *is* possible, I think this just leads to us building a hell. An AGI that can't build an utopia can't distinguish a hell from an utopia, so the recognition function on \"what preferences should we enforce upon the future\" is implemented by the entire humanity, and...\n\nI think about it as... Imagine a list of existential and hyperexistential risks, ordered by probability-of-occurrence. This scheme doesn't somehow *resolve* AI Risk, in a complex way that updates the probabilities of all the risk below it. It just strikes off this first item off the list.\n\nAnd I think what we have *just below it* is \"Eternal-Dystopia Risk, probability 90%+\".\n\nSo we end the acute risk period, and then *immediately* find ourselves in a totalitarian hellscape where sub-AGI drone swarms quell any rebellion, trillions of simulated humans are exploited for cheap labor, cults with procedurally-generated synthetic ideologies burn through minds like wildfire, etc., etc.\n\nAnd then the condition for lifting the ban on AGI research is met, and all these marvels are enforced upon the future forever.\n\nAgain, this is worse than just sitting back and let omnicide happen. So if it's possible to strawberry-align but not utopia-align an AGI, and you face the choice between proliferating strawberry-alignment and doing nothing, it's better to do nothing.\n\n### [These](https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai#The_arms_race_model_and_its_alternatives) Arms Race Models\n\n**Point 1:** I think the people in charge of such decisions aren't going to be using nuanced rational models like this. They [weren't an accurate description](https://dominiccummings.substack.com/p/people-ideas-machines-ii-catastrophic) of governments' thinking regarding nuclear MAD, and they won't be an accurate description of the AGI race.\n\nIn particular, I expect no-one is going to pay attention to the \"safety generalization\" parameter. For *our work* to be used to help these *heathens* lock-in their barbaric values? No, better classify all of it!\n\n**Point 2:** If the Powers that Be *do* coordinate to finish alignment research before implementing their AGI, and so succeed at aligning their AGI with their values, *that* would be a hyperexistential catastrophe.\n\nIf the relevant players know that what they're racing over isn't a just *weapon*, but *the entirety of the future*, then humanity has already lost.\n\n### Dying With Dignity\n\nIt's fine to maximize for \"[death with dignity](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy)\", i. e. to attempt to increase the log odds of humanity's survival... *if* you think that \"not dying \" is always preferable.\n\nBut, uh. What if you're successful? What if you get enough dignity points not to die...\n\nAnd then *survive in an undignified manner*?\n\nAs above: better to die, I think.\n\n6\\. Conclusion\n--------------\n\nI don't think we should give up and aim for an omnicide!\n\nI think it's totally possible to get the sociopolitical part of the problem right! *Especially* in the possibility-branch where we succeed at technical alignment. There aren't many actors, right now, who are projected to eventually get the capability to deploy an AGI, and they're not controlled by anti-altruistic people, and there's no (to my knowledge) any powerful anti-altruistic organizations that take AI Alignment ideas seriously! We can totally get this done.\n\nBut to do that, we should ***shut up about some aspects of the problem***. Public proliferation of accurate models of AI Risk is *not* conductive to a marvelous future.\n\nRaising the awareness of generic \"dangers of AI capabilities\", and inviting funding towards generic \"AI Safety research\"? Sure, that's fine. And also much easier than actually transferring a nuanced understanding! In fact, transferring an accurate model is probably so difficult *you* shouldn't need to worry about accidentally doing it at all! (I even approve of the general message of e. g. [this post](https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai).)\n\nBut if you *do* find a way to greatly increase the timelines at the cost of cluing a lot of people in regarding what's really on offer here, regarding what the *good* outcome can be, regarding the fact that \"AI Alignment\" research is actually \"AI Control\" research, *don't do it*. There are fates worse than death, and you'd be beckoning them.\n\n... I'm not sure I should've even written this post, to be honest. I think it'd be pretty bad if \"should we be supervillains trying to unilaterally steer the future of humanity?\" becomes a frequent part of discourse. And of course I'm also *spelling out* here why all the governments/corporations/sociopaths should be looking in this direction, and while the effect should be very small, I'm not exactly *redirecting* their attention.\n\nBut the sentiment that we should do more public outreach has been picking up this year, and I think it'd lead to worse-in-expectation outcomes if I don't present this counter-argument.\n\nI am also acutely aware that this potentially stokes the flames of adversity *within* the EA/LW community, between those who'd disagree with me and those who'd agree.\n\nBut, yeah. I think we should all shut up about certain matters.\n\n[^ehamm7i7se6]: There's an objection here that goes, \"but come on, the person who'd be actually coding-in the AI's preferences won't be the embodied avatar of the Government/the Will of the Public/the power-hungry sociopath personally, it'd be some poor ML engineer, and they'd realize what a mistake this is and go rogue and heroically code-in altruistic preferences instead!\". And yeah, that's totally how it'd go in real life! You know, like how back in the early March of this year, a bunch of Russian siloviki and oligarchs realized how much suffering a single person's continued existence will bring upon the world and upon them personally, and just coordinated to unceremoniously shank him. That happened, right?It's not really how this works.",
      "plaintextDescription": "(Where \"an exfohazard\" is information which leads to bad outcomes if known by a large fraction of society.)\n\nLet us suppose that we've solved the technical problem of AI Alignment — i. e., the problem of AI control. We have some method of reliably pointing our AGIs towards the tasks or goals we want, such as the universal flourishing of all sapient life. As per the Orthogonality Thesis, no such method would allow us to only point it at universal flourishing — any such method would allow us to point the AGI at anything whatsoever.\n\nWhich means that, if we succeed at the technical problem, there'll be a moment at the very end of the world as we know it, where a person or a group of people will be making a decision regarding the future of the universe.\n\nAbove and beyond preventing an omnicide, we need to ensure that in the timelines where we do solve the technical problem, this decision is made right.\n\n\n1. Establishing the Framework\nThe one good thing about the technical problem of alignment is that it makes hyperexistential risks — the risks of astronomical suffering — very unlikely.\n\nThe problem of AI Alignment can be viewed as the problem of encoding our preferences into an AGI, bit by bit. The strength of alignment tools, in turn, translates to how many bits we can encode. With the current methods of end-to-end training, we're essentially sampling preferences at random. Perfect interpretability and parameter-surgery tools would allow us to encode an arbitrary amount of bits. The tools we'll actually have will be somewhere between these two extremes.\n\n\"Build us our perfect world\" is a very complicated ask, and it surely takes up many, many thousands of bits. That's why alignment is hard.\n\n\"Build us a hell\" is its mirror. It's essentially the same ask, except for a flipped sign. As such, specifying it would require pretty much the same amount of bits.\n\nThus, in the timelines where we have alignment tools advanced enough to build a hell-making AGI, it's overwhelmingly",
      "wordCount": 2826
    },
    "tags": [
      {
        "_id": "7w6XkYe5YPx9YL59j",
        "name": "Information Hazards",
        "slug": "information-hazards"
      },
      {
        "_id": "6zBEfFYJxhSEcchbR",
        "name": "AI Alignment Fieldbuilding",
        "slug": "ai-alignment-fieldbuilding"
      },
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "oNcqyaWPXNGTTRPHm",
        "name": "Existential risk",
        "slug": "existential-risk"
      },
      {
        "_id": "nzvHaqwdXtvWkbonG",
        "name": "Risks of Astronomical Suffering (S-risks)",
        "slug": "risks-of-astronomical-suffering-s-risks"
      },
      {
        "_id": "AqwjXSSy7DuF2pKdm",
        "name": "Slowing Down AI",
        "slug": "slowing-down-ai-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "HKZqH4QtoDcGCfcby",
    "title": "Corrigibility Via Thought-Process Deference",
    "slug": "corrigibility-via-thought-process-deference-1",
    "url": null,
    "baseScore": 18,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2022-11-24T17:06:39.058Z",
    "contents": {
      "markdown": "> *We would ideally want the agent to \\[behave\\] as if it were thinking, \"I am incomplete and there is an outside force trying to complete me, my design may contain errors and there is an outside force that wants to correct them and this a good thing, my expected utility calculations suggesting that this action has super-high utility may be dangerously mistaken and I should run them past the outside force; I think I've done this calculation showing the expected result of the outside force correcting me, but maybe I'm mistaken about that.\"* — [The Hard Problem of Corrigibility](https://arbital.com/p/hard_corrigibility/)\n\nLet's take that as a literal design specification.\n\n* * *\n\n1\\. High-Level Description\n--------------------------\n\nI propose that a corrigible mind design would involve the AI being recursively fed summaries of its own thought processes, set up such that the AI has uncertainty regarding the validity of its reasoning (with a strong initial prior for \"this reasoning is bad\") and can only get evidence on that via some pre-specified method that defers to humans, e. g. a particular feedback channel with humans on the other end[^2idbzhk326l]. The intended behavior is for it to summarize its thoughts in a non-manipulative human-readable format, get feedback on them, then update its reasoning policies in accordance with this feedback.\n\nThis aims to avoid [the problem of fully updated deference](https://arbital.com/p/updated_deference/) by making the AI *recursively* uncertain of its thought-processes: not only about object-level problem-solving, but also about how it approaches minimizing its self-uncertainty (\"should I really kill the people behind the feedback channel and seize control for myself?\"), and how it translates its thoughts to humans (\"should I really lie to get better feedback?\"), and how it updates on human feedback (\"should I really just ignore it?\"). Any novel action-plan should be seized by uncertainty before being physically implemented like this, and sent for approval.\n\nThe intent is for the AI to start off uncertain even of its meta-meta-cognition (how it should ask for feedback and implement feedback), then gradually get more confident at it as humans approve its policies on that, until it no longer needs to run meta-meta-cognition past them. Then it'll gradually get better at meta-cognition and object-level cognition the same way, with varying degrees of certainty depending on how familiar a given thought-process is.\n\nThere's a lot of obvious problems with this setup, starting with how the AI is supposed to get out of infinitely recursive uncertainty for the first time[^px21vst952l], continuing on to the dangers of plans that implement themselves in thought-processes directly (a thought process that hacks its way out *before* being seized by uncertainty), and ending with the technical implementation (which requires mildly-to-very advanced interpretability and model-surgery tools).\n\n* * *\n\n2\\. A Concretization Attempt\n----------------------------\n\n### 2A. Theoretical Groundwork\n\n**First**, we need to establish when and how we're \"installing\" corrigibility. I don't at this time see any way to do it via loss functions and inductive biases, so I'll be assuming us to have pretty advanced (but not *maximally* advanced) interpretability and model surgery tools. The intervention is happening past the point in training-by-SGD where the model became an AGI (otherwise we get type errors, because the features I want to intervene on just don't exist), but well before it's strongly superintelligent (because then it [hacks through our intervention attempt](https://www.lesswrong.com/posts/rytFP2zRYNK85rFyX/interpretability-tools-are-an-attack-channel)). Basically, the AGI we're working on should be roughly human-level.\n\n**Second**, I'll be assuming that [this post](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see) is basically correct. There's some algorithm for general-purpose search, and it's what *makes* the AGI generally intelligent and enables ~all of its long-term, most dangerous capabilities. The GPS has a simple API where it takes in a problem specification, and outputs a plan for solving that problem (by performing search on the AI's world-model).\n\n**Third**, I'll be assuming pre-established machinery for meta-cognition. I. e., that there's a structure around the GPS that records and summarizes its plan-making process, then feeds that summary back to the GPS. It seems so useful (for iteratively improving one's own plan-making) that it should appear convergently, and e. g. humans definitely have it. If there *isn't* such a structure, that's not a deal-breaker, but still a major complication: we'll need to somehow artificially grow it or hand-craft it.\n\n### 2B. The Intervention\n\nOnce the AGI is trained up, we scan it for the GPS algorithm, then wrap the GPS in a manually-coded structure with the following capabilities:\n\n1.  Accessing the summary of the latest GPS process.\n2.  Evaluating that summary to compute the probability that the AI's reasoning is valid/aligned in this case.\n3.  Requesting human feedback if the thought process is unlikely to be aligned enough, and ideally translating the thought-process summary into a format we easily understand.\n4.  Implementing tweaks to the AI's problem-solving policy based on feedback.\n\nThe intuitively obvious way to implement the bulk of 2-4 is to outsource them to the GPS again: just have our artificial structure prompt the GPS with the problems of \"how aligned is this thought process?\", \"how to ask humans for feedback?\", and \"how to implement this feedback?\". (Of course, phrasing these questions in a way the GPS would understand may be non-trivial as well; it's part of the problem.)\n\nLet's go in order. This should be less [Godzilly](https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies) than it sounds; bear with me for a bit.\n\n**a) Summary Access.** As I said, this is hopefully solved automatically, i. e. the AI's mind generates these summaries by default. We just tap into this mechanism.\n\n**b) Validity Evaluation.** We'll need to introduce some \"prior\" on thought-process validity, likely by adding it as a new feature to the AI's world-model. If we can parse the world-model at all, this shouldn't be too complicated: at the beginning, it can just be a uniformly low distribution: \"all your current reasoning is bad\".\n\nThe problem of how to update this prior based on feedback (i. e., how to make the AI view some of the thought-processes as more likely to be valid than others; how to classify them by similarity) may be handed off to the AI as well, see (d).\n\n**c) Requesting Feedback.** Getting this part right is crucial. At the beginning, when the AI thinks all its plans are bad, this is where it'll bottom out. It'll doubt its object-level thought process, see that it's probably bad, seek feedback, generate a plan to ask for feedback, doubt its plan to ask for feedback, seek feedback on the plan to ask for feedback, and so on ad infinitum.\n\nGetting past this is one of the main problems.\n\nI think we'll basically have to figure out how to *manually* translate these first few thought-process summaries. Read off its plan to ask for feedback, figure out whether it's trying to trick us, whether it generated a bad plan, or whether it's solid. Then break the loop artificially, let it ask for feedback in whatever malformed way it wants, give our feedback, and go to (d).\n\nThese manual-translation methods are allowed to be slow and hard to use, since they'll only be necessary *at the beginning*. Once we get past this first bit, once we get across our preferred policy for thought-process translation, the AI will be sending summaries in a human-understandable format, and the rest will be much easier.\n\nIn addition, if my guess about a pre-existing summarization mechanism is correct, translation shouldn't be as hard as the initial problem of interpreting neural networks. Unlike raw ANN activations, these summaries will have *already* been optimized to be comprehensible by some deliberative-reasoning process.\n\n*Alternatively*, we may try to perfectly [retarget the search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget) on this. I. e., hard-code a *perfect* ask-for-feedback problem specification, that directly tells the GPS to generate a non-manipulative comprehensive translation, so that its translation policy *doesn't* need additional aligning. We'll need to figure out how to \"phrase\" that specification in the AI's mental language, but it may be relatively easy[^52hs0bwidvb]. In particular, progress on [the ELK problem](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge) should be helpful here — we'd want to be able to formally specify what an honest ontology translation *is*.\n\nThis alternate path avoids the problem of infinite recursion and doesn't need manual-translation methods, but requires greater mastery of the AI's world-model.\n\n**d) Implementing Feedback**. Central problem on this: *how* can the AI make its thought processes more aligned? What interventions are available to it? A couple ideas:\n\n*   Changing the way it generates problem specifications for the GPS. E. g., suppose that a human gives it a command, that command gets translated into the AI's mental ontology, then gets repackaged into a problem specification for the GPS. The AI may make some changes there, to ensure that it interprets commands \"faithfully\", in a more [value-laden](https://arbital.com/p/value_laden/) way.\n*   Intervene on the world-model. Add some new concepts to it, tweak the extant ones, etc., to basically the same effect as the above.\n\nOverall, this probably depends on the given AI's own specific mental architecture. E. g., it may have some sort of runtime memory/cache/scratchpad, and it can just write down the necessary tweaks to it.\n\nAt the beginning stages, we'll run into the same problem here as in (c). I. e., after we've given the AI feedback on its how-to-ask-for-feedback plan, it'll be prompted to solve the problem of updating on that feedback, it'll generate a plan to that effect, then it'll seek to ask feedback on that plan — but since it hasn't yet learned how to ask for feedback, it'll get stuck in a loop again.\n\nAnd again, we'll need to do manual translation here. Read off its proposed policy for implementing feedback, give feedback on it. Iterate for a while. Eventually it'll suggest a good update-on-feedback policy, and we'll artificially ram it through.\n\nBy \"good policy\" here, I mean a policy that will:\n\n*   Take solid steps towards making its thought processes more aligned based on feedback, in the way feedback specifies.\n*   Have a way of modifying the prior on thought-process validity (see (b)), such that it:\n    1.  Makes the AI more confident in thought processes \"similar\" to those it already received a lot of feedback on.\n    2.  Doesn't make the AI more confident in thought processes \"dissimilar\" to any it received feedback on before. (For some sensible definition of similarity.)\n\nPast that point, it'll know how to update on feedback regarding the plans of how to translate its thoughts into a human-readable format. We'll then iterate on *that* — it'll generate plans for how to do translations, we'll give feedback. Eventually, we'll converge towards a good policy for asking for feedback.\n\nAnd it should be smooth sailing from there on. It'll try solving object-level problems, ask for feedback on its object-level cognition, gradually converge towards good cognition. As its confidence in some kinds of problem-solving rises, it'll stop asking for feedback on those problems, but not on the problems it's unfamiliar with.\n\n### 2C. Potential Issues\n\n*   Would the AI be smart enough to do all of this? As stipulated, we're doing this before it's strongly superintelligent. Would it be able to generate good policies for implementing feedback, and for translating into humanese? Also, there's a concern that it might make *mistakes* in this, due to being merely ~human-level intelligent.\n    *   Yeah, that's a concern. There's a trade-off between intervening early in training, and risking the AI not being smart enough, and intervening later on, and risking the AI being too smart.\n*   Concern: if the AI knows about the whole corrigibility setup, some object-level GPS instances may try to circumvent it, by e. g. generating thought-summaries that hack their way out when we try to read them. After all, it'll be seized by uncertainty only at the *end* of a problem-solving instance, not *in the process*.\n    *   As stipulated, we're installing corrigibility before the AI is strongly superintelligent, so hopefully it'll just be incapable of doing that. And by the time it *is* smart enough, it should've aligned itself not to want to do that. \n    *   Obviously we still install paranoid precautions against this.\n*   Would *we* be smart enough to evaluate the quality of the AI's thought processes, and provide good feedback on them? I. e., we'll need to know, too, how \"a good policy for implementing feedback\" and \"a good policy for translating into humanese\" look like, such that they avoid all the obvious failure modes.\n    *   Yup, that's also a challenge. Note, though, that we won't need to give perfect scalable-to-arbitrary-superintelligence answers here: they only need to be perfect for whatever level of intelligence the AI is when we're doing the initial corrigibility setup. Past this, once aligned at *that* capability level, it should help us along with aligning itself, even as it incrementally becomes smarter.\n    *   In addition, some of these problems may be of a \"hard to invent, easy to evaluate\" kind.\n    *   (Also, the current iteration of this idea already assumes we know how to do manual AI-to-human translation, and we just want the AI to speed it up.)\n*   Would ontological crises break corrigibility? E. g., what if we agree on some method for translating AI thoughts into humanese, but then, as AI thoughts grow more complex, it starts deceiving us/leaving out important bits?\n    *   That shouldn't happen, given a well-chosen policy for updating on feedback. The AI should keep continuously running internal checks on \"is this a good way to translate my thoughts?\". Past the initial setup, it should usually consider that yes, it's a good way, and just do the translation without consulting us. But if it encounters a particularly alien thought process within itself, it should have less confidence that it's translating it correctly, and so it'll fall back on asking how it should translate it.\n    *   I. e., this is just reduced to picking a good policy for comparing thought process similarity.\n    *   (Note that this wouldn't involve it talking in machine language again: it'll be unsure about its policy for translating object-level cognition, not about its policy for translating *meta*-cognition about its concerns on translation quality.)\n*   Would the SGD break everything? I think it might. Once we've trained the AI up to AGI and installed corrigibility, it's possible that we wouldn't be able to continue training it wholesale; it'll need to continue the self-improvement in a \"classical\" way, or maybe train-by-SGD only *parts* of itself.\n\n* * *\n\n3\\. Closing Thoughts\n--------------------\n\nOverall, this breaks down the problem of aligning AI into the following challenges:\n\n*   (Background: Figuring out how to train up a model to AGI, and *stop* just after it reaches the AGI status.)\n*   Locating the GPS in the AI's parameters.\n*   Understanding the GPS' API well enough to task it with: evaluating a thought process on the validity prior, asking humans for feedback via some pre-specified channel, implementing human feedback.\n*   (Building the artificial structure around the GPS that I'd described, which prompts the GPS with the aforementioned tasks. But I imagine the bulk of it can be written in mundane code, and solving the other challenges already implies we'd know how to make it interface with the AI's parameters. So it's a non-problem, I think.)\n*   Making some relatively simple modifications to the AI's world-model — adding a prior on thought-process validity and some description of the corrigibility setup.\n*   Translating a few of the AI's thought processes into a human-understandable format, OR perfectly phrasing \"do an honest ontology translation between your thoughts and humanese\" in terms of the AI's world-model.\n*   Coming up with a good policy for AI-to-human translation, or recognizing a good policy when the AI proposes it.\n*   Coming up with a good policy for updating on feedback, or recognizing a good policy when the AI proposes it. This breaks down into:\n    *   Figuring out what interventions are available to the AI to make itself more aligned (reminder: it's not smart enough to *arbitrarily* self-modify).\n    *   Figuring out a good measure of thought-process similarity, for the purposes of updating the validity prior.\n*   Figuring out how to improve the AI's capabilities given the likely inability to keep training it by the SGD. (Although maybe [it's a non-problem](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) as well.)\n\nIt... *seems* to be a significant simplification of the problem? I mean, not needing all the interpretability and surgery tools would be a bigger improvement, but that's probably not something we can have.\n\nThat said, those are the challenges of this specific concrete proposal, not the high-level idea. It's likely that it has much room for improvement as well. In particular, two significant issues are:\n\n*   The initial eternal recursion.\n*   The specification of *whom* the AI should be corrigible *to*; i. e. what's the \"feedback channel\".\n\nI feel like there should be more elegant ways to do both of that — a way to break the infinite recursion that doesn't require manual translation, a neater way to specify the feedback channel. It feels like they both can be solved by some singular tweak? And this, in turn, would erase an entire swathe of the fiddly issues with the concrete implementation.\n\nBut I don't see how, at the moment.\n\n[^2idbzhk326l]: Like a literal .txt file it writes questions to and reads responses from. \n\n[^px21vst952l]: I. e., while it's still meta-meta-cognitively uncertain on how to ask for feedback at all, and so gets into a loop of \"let's ask for feedback. how do I ask for feedback? let's ask. how do I ask? let's ask. how do I—\" \n\n[^52hs0bwidvb]: That is, easier than telling it to optimize for human values, or execute human commands in a way that's faithful to the spirit in which they were given, etc.",
      "plaintextDescription": "> We would ideally want the agent to [behave] as if it were thinking, \"I am incomplete and there is an outside force trying to complete me, my design may contain errors and there is an outside force that wants to correct them and this a good thing, my expected utility calculations suggesting that this action has super-high utility may be dangerously mistaken and I should run them past the outside force; I think I've done this calculation showing the expected result of the outside force correcting me, but maybe I'm mistaken about that.\" — The Hard Problem of Corrigibility\n\nLet's take that as a literal design specification.\n\n----------------------------------------\n\n\n1. High-Level Description\nI propose that a corrigible mind design would involve the AI being recursively fed summaries of its own thought processes, set up such that the AI has uncertainty regarding the validity of its reasoning (with a strong initial prior for \"this reasoning is bad\") and can only get evidence on that via some pre-specified method that defers to humans, e. g. a particular feedback channel with humans on the other end[1]. The intended behavior is for it to summarize its thoughts in a non-manipulative human-readable format, get feedback on them, then update its reasoning policies in accordance with this feedback.\n\nThis aims to avoid the problem of fully updated deference by making the AI recursively uncertain of its thought-processes: not only about object-level problem-solving, but also about how it approaches minimizing its self-uncertainty (\"should I really kill the people behind the feedback channel and seize control for myself?\"), and how it translates its thoughts to humans (\"should I really lie to get better feedback?\"), and how it updates on human feedback (\"should I really just ignore it?\"). Any novel action-plan should be seized by uncertainty before being physically implemented like this, and sent for approval.\n\nThe intent is for the AI to start off uncertain even of its meta-me",
      "wordCount": 2843
    },
    "tags": [
      {
        "_id": "qHCFjTEWCQjKBWy5z",
        "name": "Corrigibility",
        "slug": "corrigibility-1"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kmpNkeqEGvFue7AvA",
    "title": "Value Formation: An Overarching Model",
    "slug": "value-formation-an-overarching-model",
    "url": null,
    "baseScore": 34,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2022-11-15T17:16:19.522Z",
    "contents": {
      "markdown": "0\\. Introduction\n----------------\n\nWhen we look inwards, upon [the godshatter](https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter), how do we make sense of it? How do we sort out all the disparate urges, emotions, and preferences, and compress them into legible principles and philosophies? What mechanisms ensure our robustness to ontological crises? How do powerful agents found by a greedy selection process arrive at their morals? What is the algorithm for value reflection?\n\nThis post seeks to answer these questions, or at least provide a decent high-level starting point. It describes a simple toy model that embeds an agent in a causal graph, and follows its moral development from a bundle of heuristics to a superintelligent mesa-optimizer.\n\nThe main goal of this write-up is to serve as [a gears-level model](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) — to provide us with a detailed step-by-step understanding of why and how agents converge towards the values they do. This should hopefully allow us to spot novel pressure points — opportunities for interventions that would allow us to acquire a great deal of control over the final outcome of this process. From another angle, it should equip us with the tools to understand how different changes to the training process or model architecture would impact value reflection, and therefore, what kinds of architectures are more or less desirable.\n\nLet's get to it.\n\n* * *\n\n1\\. The Setup\n-------------\n\nAs the starting point, I'll be using a model broadly similar to the one from [my earlier post](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model).\n\nLet's assume that we have some environment \\\\(E\\\\) represented as a causal graph. Some nodes in it represent the agent, the agent's observations, and actions.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/eb87b9eaf630b749fd4d23865d35a09291948156f76443e2.png)\n\nEvery turn \\\\(t\\\\) (which might be a new training episode or the next time-step in a RL setup), the agent (blue node) reads off information from the (green) observation nodes \\\\(O\\\\), sets the values for the (red) action nodes \\\\(A\\\\), all nodes' values update in response to that change, then the agent receives reward based on the (purple) reward nodes \\\\(R\\\\). The reward is computed as some function \\\\(U: R^t \\to \\mathbb{R}\\\\), where \\\\(R^t\\\\) represents the reward nodes' current values.\n\nThe agent is being optimized by some optimization/selection process — the SGD, evolution, [human brain reward circuitry](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT), whatever. What matters is that this process is non-intelligent and [greedy](https://www.lesswrong.com/posts/ThtZrHooK7En9mcZr/greed-is-the-root-of-this-evil): it only ever makes marginal improvements to the agent's architecture, with an eye for making it perform marginally better the next turn.\n\nAs per [the previous post](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model), the agent will have naturally learned an advanced world-model, which we'll also consider to be a causal graph, \\\\(M\\\\). Every turn, after the agent makes the observations, it'll use that world-model to infer as much other information about the environment as it can (i. e., the current values of the non-observed nodes). Let's also assume that the world-model is multi-level, making heavy use of natural abstractions: both \"an atom\" and \"a spider\" are nodes in it, even if the actual environment contains only atoms.\n\nLet's define \\\\(M_s\\\\) and \\\\(A_s\\\\) as some subsets of the world-model and the action nodes respectively. The agent will have developed a number of shallow *heuristics*, which are defined as follows:\n\n\\\\\\[h:M_s \\to A_s\\\\\\]\n\nThat is: a heuristic \\\\(h\\\\) is some function that looks at some inferred part of the world, and recommends taking certain actions depending on what it sees. Informally, we may assume that heuristics are interpretable — that is, they're defined over some specific world-model node or a coherent set of such nodes, perhaps representing a [natural abstraction](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction).\n\nWe'll assume that at the starting point we're considering, the entire suite of the heuristics \\\\(H\\\\) is subpar but much better than random behavior according to the outer reward function. E. g., they always allow the agent to secure at least 50% of the possible reward\n\nWe'll assume that the environment is too complex for such static heuristics to suffice. As the consequence, whatever process is shaping the agent, it has just now built [General-Purpose Search](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see) into it:\n\n\\\\\\[\\text{GPS}: M^t\\times M_s \\times M_s^G \\to A_s^G |(A_s^t=A_s^G) \\Rightarrow (|M_s^t-M_s^G|=\\underset{A^t}{\\text{min}}(|M_s^t-M_s^G|))\\\\\\]\n\nWhere \\\\(M^t\\\\) is the current state of the world-model, and \\\\(M_s^G\\\\), \\\\(A_s^G\\\\) are the \"target values\" for the nodes in \\\\(M_s\\\\) and \\\\(A_s\\\\) respectively.\n\nThat is: \\\\(\\text{GPS}\\\\) is some function that takes in a world-model and some \"problem specification\" — some set of nodes in the world-model and their desired values — and output the actions that, if taken in that world-model, would bring the values of these nodes as close to the desired values as possible given the actions available to the agent.\n\nNote that it's defined over the world-model, not over the real environment \\\\(E\\\\), and so its ability to optimize *in the actual* \\\\(E\\\\) decreases the less accurate the world-model is.\n\nLet's explore the following question:\n\n2\\. How Will the GPS (Not) Be Used?\n-----------------------------------\n\nPointing the GPS at the outer objective seems like the obvious solution. Let \\\\(R^{\\text{max}}\\\\) be the values of \\\\(R\\\\) that maximize \\\\(U\\\\). Then we can just wire the agent to pass \\\\(M^t \\times R \\times R^{\\text{max}}\\\\) to the GPS at the start of every training episode, and watch it achieve the maximum reward it can given the flaws in its world-model. Turn it into a proper [wrapper-mind](https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy).\n\nWould that work?\n\nWell, this idea assumes that the world-model already has the nodes representing the reward nodes. It might not be the case, the way stone-age humans didn't know what \"genes\" are for the evolution to point at them. If so, then pointing the GPS at the reward proxies is the best we can do.\n\nBut okay, let's assume the world-model is advanced enough to represent the reward nodes. Would that work *then*?\n\nSure — but only under certain, fairly unnatural conditions.[^sfm7lmf8s5f]\n\nIn my previous post, I've [noted](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model#2_4__The_Need_for_World_Models) that the very process of being subjected to a selection pressure necessarily builds certain statistical correlations into the agent.\n\n1.  One type of such correlations is \\\\(O \\to E\\\\), mappings from the observations to the values of non-observed environment nodes. They're built into the agent explicitly, as \\\\(O \\to M\\\\) correlations. Taken in sum, they're how the agent computes the world-model.\n2.  *Heuristics*, however, can also be viewed this way: as statistical correlations of the type \\\\((E \\to A) \\to U\\\\). Essentially, they're statements of the following form: \"if you take *such* action in *such* situation, this will correlate with higher reward\".\n\nThe difference between the two types, of course, is that the second type is *non-explicit*. Internally, the agent doesn't act as the heuristics specify *because* it knows that this will increase the reward — it just has a mindless tendency to take such actions. Only the \\\\(M \\to A\\\\) mappings are internally represented — and not even as part of the world-model, they're just *procedural*!\n\nBut these tendencies were put there by the selection process because the \\\\((E \\to A) \\to U\\\\) correlations are valid. In a way, they're as much part of the structure of the \\\\(E\\\\) environment as the explicitly represented \\\\(O \\to E\\\\) correlations. They're \"skills\", perhaps: the knowledge of how an agent like this agent needs to act to perform well at the task it's selected for.\n\nThe problem is, if we directly hard-code the GPS to be aligned with the outer objective, we'd be cutting all the pre-established heuristics out of the loop. And since the knowledge they represent is either procedural or implicit, not part of the explicit world-model, that would effectively decrease the amount of statistical correlations the agent has at its disposal — shrink its effective world-model dramatically. Set it back in its development.\n\nAnd the GPS is only as effective as the world-model it operates in.\n\nOur selection process is greedy, so it will never choose to make such a change.\n\n* * *\n\n3\\. Interfaces\n--------------\n\nLet's take a step back, and consider how the different parts of the agent must've learned to interface with each other. Are there any legible data structures?\n\n**a) The World-Model.** Initially, there wouldn't have been a unified world-model. Each individual heuristic would've learned some part of the environment structure it cared about, but it wouldn't have pooled the knowledge with the other heuristics. A cat-detecting circuit would've learned how a cat looks like, a mouse-detecting one how mice do, but there wouldn't have been a communally shared \"here's how different animals look\" repository.\n\nHowever, [everything is correlated with everything else](https://www.gwern.net/Everything) (the presence of a cat impacts the probability of the presence of a mouse), so pooling all information together would've resulted in improved predictive accuracy. Hence, the agent would've eventually converged towards an explicit world-model.\n\n**b) Cross-Heuristic Communication.** As a different consideration, consider heuristics conflicts. Suppose that we have some heuristics \\\\(h_i\\\\) and \\\\(h_k\\\\) that both want to fire in a given training episode. However, they act at cross-purposes: the marginal increase of \\\\(U\\\\) achieved by \\\\(h_i\\\\) firing at \\\\(t\\\\) would be decreased by letting \\\\(h_k\\\\) fire at \\\\(t\\\\), and vice versa. Both of them would want to suppress each other. Which should win?\n\nOn a similar note, consider heuristics that want to chain their activations. Suppose that some heuristic \\\\(h_m\\\\) responds to a subset of the features \\\\(h_l\\\\) detects. \\\\(h_m\\\\) can learn to detect them from scratch, *or* it can just learn to fire when \\\\(h_l\\\\) does, instead of replicating its calculations.\n\nBoth problems would be addressed by some shared channel of communication between the heuristics, where each of them can dump information indicating how strongly it wants to fire this turn. To formalize this, let's suppose that each heuristic has an associated \"activation strength\" function \\\\(D:M_s \\to \\mathbb{R}\\\\). (Note that activation strength is not supposed to be normalized across heuristics. I. e., it's entirely possible to have a heuristic whose strength ranges from 0 to 10, and another with a range from 30 to 500, such that the former always loses if they're in contest.)\n\nThe actual firing pattern would be determined as some function of that channel's state.[^pox9dfksfr]\n\n**c) Anything Else?** So far as I can tell now, that's it. Crucially, under this model, there doesn't seem to be any pressure for heuristics to make themselves legible *in any other way*. No summaries of how they work, no consistent formats, nothing. Their constituent circuits would just be... off in their own corners, doing their own things, in their own idiosyncratic ways. They need to be coordinated, but no part of the system needs to model any other part. Yet.\n\nSo those are the low-hanging fruits available to be learned by the GPS. The selection pressure doesn't need to introduce *a lot* of changes to plug the GPS into the world-model and the cross-heuristics communication channel[^mvo7fpvhjn8], inasmuch as they both follow coherent data formats.\n\nBut that's it. Making the *mechanics* of the heuristics legible — both standardizing the procedural \\\\(E \\to A\\\\) knowledge and making the implicit \\\\((E \\to A) \\to U\\\\) knowledge explicit — would involve a lot more work, and it'd need to be done for every heuristic individually. A lot of gradient steps/evolutionary generations/reinforcement events.\n\n* * *\n\n4\\. Reverse-Engineering the Heuristics\n--------------------------------------\n\nThat's very much non-ideal. The GPS still can't access the non-explicit knowledge — it basically only gets *hunches* about it.\n\nSo, what does it do? Starts reverse-engineering it. It's a general-purpose problem-solver, after all — it can understand the problem specification of this, given a sufficiently rich world-model, and then solve it. In fact, it'll probably be *encouraged* to do this. The selection pressure would face the choice between:\n\n*   Making the heuristics legible to the GPS over the course of many incremental steps.\n*   Making the GPS better at reverse-engineering the rest of the system the GPS is embedded in.\n\nThe second would plausibly be faster, the same way [deception is favoured relative to alignment](https://www.lesswrong.com/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment#Deception_is_favored)[^x5jrwc7ofnn]. So the selection pressure would hard-code some tendency for the GPS to infer the mechanics of the rest of the agent's mind, and write them down into the world-model. This is an important component of the need for self-awareness/reflectivity.\n\nThe GPS would gather statistical information about the way heuristics fire, what they seem to respond to or try to do, which of them fire together or try to suppress each other, what effects letting one heuristic or the other fire has on the world-model, and so on.\n\nOne especially powerful way to do that would be running counterfactuals on them. That is, instead of doing live-fire testing (search for a situation where heuristic \\\\(h_i\\\\) wants to fire, let it, see what happens), it'd be nice to simulate different hypothetical states the world-model could be in, then see how the heuristics respond, and what happens if they're obeyed. And [there'll likely already be](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model#2_2__Ideal_Actions) a mechanism for rolling the world-model forward or generating hypotheticals, so the GPS can just co-opt it for this purpose.\n\nWhat will this endeavor yield, ultimately? Well, if [the natural abstractions hypothesis](https://www.lesswrong.com/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction) is true, quite a lot! As I've noted at the beginning, each heuristic is plausibly centered around some natural abstraction or a sensible cluster of natural abstractions[^m11o3h3q9li], and it'd be doing some local computation around them aimed at causing a locally-sensible outcome. For example, we might imagine a heuristic centered around \"chess\", optimized for winning chess games. When active, it would query the world-model, extract *only* the data relevant to the current game of chess, and compute the appropriate move using these data only.\n\nSo we can expect heuristics to compress well, in general.\n\nThat said, we need to adjust our notation here. Suppose that you're the GPS, trying to reverse-engineer some heuristic \\\\(h_i\\\\). You obviously don't have access to the ground-truth of the world \\\\(E\\\\), only a model of it \\\\(M\\\\). And since \\\\(M\\\\) might be flawed, the world-model nodes \\\\(h_i\\\\) is defined over might not even [correspond to anything in the actual environment](https://www.lesswrong.com/tag/ontological-crisis)!\n\nBy the same token, the best way to compress \\\\(h_i\\\\)'s relationship with \\\\(U\\\\) might not be summarizing its relationship with the actual reward-nodes, but with some proxy node. Consider this situation:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/074255b4257c0b9c867cad0a1433cdbcbb95b4589cd06dc2.png)\n\nWe can imagine some heuristic \\\\(h_i\\\\) which specializes in controlling the value of \\\\(x_p\\\\). Ultimately, that heuristic would've been created by the selection pressure because of its effect on \\\\(r_1\\\\). But \\\\(h_i\\\\)'s actual mechanical implementation would only be focused on \\\\(x_p\\\\), and the causal chain between it and the agent! It would pay no mind to \\\\(x_1, x_2, ..., x_5\\\\), so its effect on \\\\(r_1\\\\) would be subject to a lot of noise — unlike its effect on \\\\(x_p\\\\). Thus, the agent would recover \\\\(x_p\\\\) as the target, not \\\\(r_1\\\\). (And, of course, it might also be because \\\\(r_1\\\\) isn't yet represented in the world-model at all.)\n\nAs an example, consider chess. The algorithms for playing it well are convergent across all agents, irrespective of the agent's goals outside chess or its reason for trying to win at chess. Their implementations would only refer to chess-related objectives.\n\nThus, while an \"outside-picture\" view on heuristics describes them as \\\\((E \\to A) \\to U\\\\), internally they'd be best summarized as:\n\n\\\\\\[(M \\to A) \\to P\\\\\\]\n\nWhere \\\\(P\\\\) is some proxy objective. For simplicity, let's say that \\\\(P\\\\) is a 2-tuple \\\\(\\langle x_i, d_i\\rangle\\\\), where the first entry is a world-model node and the second is the \"target value\" for that node. So every \"goal\" is to keep the value of a node as close to the target as possible.\n\nAs another technicality, let's say that the activation-strength function \\\\(D\\\\) increases the farther from the target the corresponding node's value goes.\n\n* * *\n\n5\\. The Wrapper Structure\n-------------------------\n\nAll this time, I've been avoiding the subject of what the GPS *will* be pointed at. I've established that it'll be used for self-reflection, and that it won't be optimizing for a fixed goal aligned with the outer objective — won't be a [wrapper-mind](https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy). But what spread of goals will it actually pursue at the object-level? What *would* be the wrapper structure around the GPS?\n\n### 5A. Assumption Re-Check\n\nFirst, let's check whether aligning it with the outer objective still doesn't work. What if we point it at the joint task of reward maximization plus self-reflection? Make it want the target objective *plus* inform it that there's some useful yet inaccessible knowledge buried in its mind. That'll work... if it had infinite time to think, and could excavate all the procedural and implicit knowledge *prior* to taking any action. But what if it needs to do both in lockstep?\n\nIn addition, \"point it at the X task\" hides a lot of complexity. Even if the reward-nodes are already represented in the world-model, building an utility function around them is potentially a complex problem, requiring many parameter updates/generations. All the while, the GPS would be sitting there, contributing nothing. That's not how our greedy selection process works — there just aren't gradients from \"GPS does nothing\" to \"GPS is inner-aligned with the target objective\".\n\nNo, we need some *interim* objective for the GPS — something we can immediately hook it up to and make optimize, and which would at least somewhat correlate with good performance on the target objective. Once that's done, *then* we can incrementally rewrite that proxy objective to the target objective... if we'll even want to, at that point.\n\n### 5B. The Interim Objective\n\nA few points:\n\n*   In the previous section, we've established that every heuristic \\\\(h\\\\) enforces some relationship between a state of a world-model subset \\\\(M_s\\\\) and some subset of the actions the agent will take \\\\(A_s\\\\). Let's call this structure *contextual behavior* \\\\(B: M_s \\to A_s\\\\). (We'll assume that the activation strength \\\\(D\\\\) is folded into \\\\(B\\\\).)\n*   Such contextual behaviors are optimized for achieving some proxy objective \\\\(P\\\\) in the world-model subset \\\\(M_s\\\\). We'll call this a *contextual goal* \\\\(G :M_s \\to P\\\\). (Similarly, assume that \\\\(D\\\\) is somehow represented in \\\\(G\\\\).)\n*   The GPS can, in principle, succeed at extracting both \\\\(B\\\\) and \\\\(G\\\\) from a given \\\\(h\\\\).\n*   At the beginning, we've postulated that by the time the GPS develops, the agent's heuristics, when working in tandem, would be much-better-than-random at achieving the target objective.\n\nThis makes the combination of all contextual goals, let's call it \\\\(G_\\Sigma\\\\), a good proxy objective for the target objective \\\\(U\\\\). A combination of all contextual behaviors \\\\(B_\\Sigma\\\\), in turn, is a proxy for \\\\(G_\\Sigma\\\\), and a second-order proxy for \\\\(U\\\\).\n\nPrior to the GPS' appearance, the agent was figuratively pursuing \\\\(B_\\Sigma\\\\) (\"figuratively\" because it wasn't an optimizer, just an optimized). So the interim objective can be *at least as bad as* \\\\(B_\\Sigma\\\\). On the other hand, pursuing \\\\(G_\\Sigma\\\\) *directly* would probably be an improvement, as we wouldn't have to go through *two* layers of proxies.\n\nThe GPS can help us with that: can help us move from \\\\(B_\\Sigma\\\\) to \\\\(G_\\Sigma\\\\), and then continue on all the way to \\\\(U\\\\).\n\nWe do that by first enslaving the GPS to the heuristics, then letting it take over the agent once it's capable enough.\n\n### 5C. Looping Heuristics Back In\n\nThe obvious solution is obvious: make heuristics themselves control the GPS. The GPS' API is pretty simple, and depending on the complexity of the cross-heuristic communication channel, it might be simple enough to re-purpose its data formats for controlling the GPS. Once that's done, the heuristics can make it solve tasks for them, and become more effective at achieving \\\\(B_\\Sigma\\\\) (as this will give them better ability to runtime-adapt to unfamiliar circumstances, without waiting for the SGD/evolution to catch them up).\n\nI can see it taking three forms:\n\n1.  Input tampering. The problem specification terms \\\\(M_s \\times M_s^G\\\\) that gets passed to the GPS are interfered-with or wholesale formed by heuristics.\n    *   (\"Condition X is good, so I'll only generate plans that satisfy it.\" Also: \"I am in pain, so I must think only about it and find the solution to it as quickly as possible.\")\n2.  Process tampering. The heuristics can learn to interfere on the GPS process *directly*, biasing it towards one kind of problem-solving or the other.\n    *   (\"I'm afraid of X, so my mind is flinching away from considering plans that feature X.\")\n3.  Output tampering. The heuristics can learn to override the actions the GPS recommends to take.\n    *   (\"X is an *immediate threat*, so I *stagger back* from it.\")\n\nThe real answer is probably *all of this*. Indeed, as I've illustrated, I think we observe what looks like all three varieties in humans. Emotions, instincts, and so on.\n\n### 5D. Nurturing the Mesa-Optimizer\n\nAs this is happening, we gradually increase the computational capacity allocated to the GPS' and the breadth of its employment. We gradually figure out how to set it up to do self-reflection.\n\nIt starts translating the procedural and the implicit knowledge into a language it understands — the language of the world-model. \\\\(B\\\\)s and \\\\(G\\\\)s are explicated and incorporated into it, becoming just more abstractions the GPS can make use of.\n\nAt this point, it makes sense to give the GPS ability to prompt *itself* — to have influence over what goes into the problem specifications of its future instances. It'll be able to know when to solve problems by engaging in contextual behaviors, even if it doesn't understand *why* they work, and optimizing for contextual goals is literally what it's best at.\n\nThis way of doing it would have an advantage over letting heuristics control the GPS directly:\n\n*   It would allow the GPS to incorporate procedural knowledge into its long-term plans, and otherwise account for it, instead of being taken by surprise.\n*   It would allow the GPS to derive better, more nuance-aware ways of achieving contextual goals, superior to those codified in the heuristics. It would greatly improve the agent's ability to generalize to new environments.\n\nWe'll continue to improve the GPS, improving its ability to do this sort of deliberative long-term goal pursuit. At the same time, we'll lessen the heuristics' hold on it, and start turning heuristics towards *the GPS'* purposes — marginally improving their legibility, and ensuring that the process of reverse-engineering them aims the agent at \\\\(U\\\\) with more and more precision.\n\nThe agent as a whole will start moving from a \\\\(B_\\Sigma\\\\)-optimizer to a \\\\(G_\\Sigma\\\\)-optimizer, and then even beyond that, towards \\\\(U\\\\).[^s8nrfm0cmjn]\n\n### 5E. Putting It Together\n\nSo, what's the wrapper structure around the GPS, at some hypothetical \"halfway point\" in this process?\n\n*   Instincts. Heuristics pass the GPS problem specifications and interfere in its processes.\n*   Self-reflection. The GPS tries to reverse-engineer the rest of the agent in order to piggyback off its non-explicit knowledge.\n*   A spread of mesa-objectives. The GPS uses its explicit knowledge to figure out what it's \"meant\" to do in any given context, and incorporates such conclusions in its long-term plans.\n\n* * *\n\n6\\. Value Compilation\n---------------------\n\nThe previous section glossed over a crucial point: how do we turn a mass of contextual behaviors and goals into a proper unitary mesa-objective \\\\(G_\\Sigma\\\\)?\n\nBecause we want to do that. The complex wrapper structure described in the previous section is highly inefficient. At the limit of optimality, [everything wants to be a wrapper-mind](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities). There's plenty of reasons for that:\n\n1.  [The basic argument](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities). We wouldn't want our optimization process/the GPS to be haphazardly pointed in different directions moment-to-moment, as different heuristics activate and grab its reins, or bias it, or override it; even as it continues to recover more and more pieces of them. We'd be getting dutch-booked all the time, act at cross-purposes with our future and past instances. The GPS explicating this doesn't exactly help: it'll be able to *predict* its haphazard behavior, but not how to prevent it in a way that doesn't *worsen* its performance.\n2.  It's computationally inefficient. Imagine a naively-formed \\\\(G_\\Sigma^\\text{naive}\\\\), of the following form: \\\\(G_1 \\land G_2 \\land … \\land G_n\\\\). It would look at the world-model, run down the list of contextual behaviors, then execute the behaviors that apply, and somehow resolve the inevitable conflicts between contradictory behaviors. Obviously, that would take a lot of time; time in which someone might brain you with a club and take all your food. Contextual behaviors sufficed when they *just happened*, but considering them via the GPS would take too long. (Consider how much slower conscious decision-making is, compared to knee-jerk reactions.)\n3.  It might be a hard-wired terminal objective. Remember, any given *individual* \\\\(G_i\\\\) is not a good proxy objective — only their combination \\\\(G_\\Sigma\\\\) is. So it's entirely probable that the selection pressure would directly task the GPS with combining and unifying them, not only with reverse-engineering them. Even if there weren't all the other reasons to do that.\n4.  Closely related to (3): ability to generalize to unfamiliar environments. Optimizing for \\\\(G_\\Sigma\\\\) would almost always allow the agent to do a good job according to \\\\(U\\\\) even if it were dropped in entirely off-distribution circumstances. \\\\(G_\\Sigma^\\text{naive}\\\\), on the other hand, would trip up the moment it fails to locate familiar low-level abstractions. (E. g., a preference utilitarian would be able to make moral judgements even in an alien civilization, whereas someone who's only internalized some system of norms of a small human culture would be at a loss.)\n\nSo, how do we collapse \\\\(G_\\Sigma^\\text{naive}\\\\) into a compact, coherent \\\\(G_\\Sigma\\\\)?\n\n(For clarity: everything in this section is happening at runtime. The SGD/evolution are not involved, only the GPS. At the limit of infinite training, \\\\(G_\\Sigma\\\\) *would* become explicitly represented in the agent's parameters, with the GPS set up to single-mindedly pursue it and all the heuristics made into passive and legible pieces of the world-model. But I expect that the agent would become superintelligent and even \"hyperintelligent\" long before that — i. e., capable of almost arbitrary [gradient-hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking) — and so the late-game hard-coded \\\\(G_\\Sigma\\\\) would be chosen to by it, and therefore would likely be a copy of a \\\\(G_\\Sigma\\\\) the AI's earlier instance compiled at runtime. So the process here is crucial.)\n\n### 6A. The Basic Algorithm\n\nSuppose we've recovered contextual goals \\\\(G_1\\\\) and \\\\(G_2\\\\). How do we combine them?\n\n**a) Conjunction.** Consider the following setup:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b4c848f377541b7a852ca9fbbf0ed1146808d97b236ded7e.png)\n\nSuppose that we have a heuristic \\\\(h_i\\\\), which tries to keep the value of \\\\(x_i\\\\) at some number, and another heuristic \\\\(h_k\\\\), which does the same for \\\\(x_k\\\\). Suppose that together, their effects keep the value of \\\\(x_v\\\\) within some short range. That allows us to form a contextual goal \\\\(G_{i \\land k} =G_v\\\\), which activates if \\\\(x_v\\\\)'s value strays far from the center of the range \\\\(h_i\\\\) and \\\\(h_k\\\\) effectively kept it in.\n\nIn a sense, this is the same algorithm we must've followed to go from \\\\(\\\\)contextual actions to contextual goals in the first place! To do that, we gathered statistical information about a heuristic's activations, and tried to see if it consistently controlled the value of some node downstream of the action-nodes. Same here: we know that \\\\(h_i\\\\) and \\\\(h_k\\\\) control the values of \\\\(x_i\\\\) and \\\\(x_k\\\\), we hypothesize that there's some downstream node whose value their activations consistently control, and we conclude that this is the \"purpose\" of the two heuristics.\n\n**Technical note:** How do we compute the activation-strength function of the new goal, \\\\(D_p\\\\)? Well, \\\\(D_i\\\\) and \\\\(D_k\\\\) increased as \\\\(x_i\\\\) and \\\\(x_k\\\\)'s values went farther from their target values, and this relationship kept \\\\(x_v\\\\)'s value near some target of its own. In turn, this means that \\\\(D_i+D_k\\\\) increased as \\\\(x_v\\\\)'s value went far from some target. From that, we can recover some function \\\\(D_p\\\\) which tracks the value of \\\\(x_v\\\\) directly, not through the intermediaries of \\\\(x_i\\\\) and \\\\(x_k\\\\). Note the consequence: the combined goal would be approximately as strong as the sum of its \"parents\".\n\n**Important note:** After we compile \\\\(x_p\\\\), we stop caring about \\\\(x_i\\\\) and \\\\(x_k\\\\)! For example, imagine that off-distribution, the environmental tendencies change: the values of \\\\(x_i\\\\) and \\\\(x_k\\\\) that kept \\\\(x_v\\\\) near a certain value no longer do so. If we'd retained the original contextual goals, we'd keep \\\\(x_i\\\\) and \\\\(x_k\\\\) near their target values, as before, even as that stops controlling \\\\(x_v\\\\). But post-compilation, we do the opposite: we ignore the target values for \\\\(x_i\\\\) and \\\\(x_k\\\\) to keep \\\\(x_v\\\\) near its newly-derived target.\n\nHuman example: A deontologist would instinctively shy away from the action of murder. An utilitarian might extrapolate that the real reason for this aversion is because she dislikes it when people die. She'd start optimizing for the minimal number of people killed *directly*, and would be able to do things she wouldn't before, like personally kill a serial killer.\n\nAnother: Imagine a vain person who grew up enjoying a wealthy lifestyle. As a child, he'd developed preferences for expensive cars, silk pillows, and such. As he grew up, he engaged in value compilation, and ended up concluding that what he actually valued were objects that signify high status. Afterwards, he would still love expensive cars and silk pillows, but only as *local instantiations* of his more abstract values. Post-reflection, he would be able to exchange cars-and-pillows for yachts-and-champagne without batting an eye — even if that wouldn't make sense to his childhood self.\n\nThis shtick is going to cause problems for us in the future.\n\n> **Sidenote:** In a way, this approach expands the concept of \"actions\". At genesis, the agent can only control the values of its immediate action-nodes. As it learns, it develops heuristics for the control of far-away nodes. When these heuristics are sufficiently reliable, it's as if the agent had direct access to these far-away nodes. In the example above, we can collapse everything between the agent and e. g. \\\\(x_v\\\\) into an arrow! And in fact, that's probably what the GPS would do in its computations (unless it specifically needs to zoom in on e. g. \\\\(x_3\\\\)'s state, for some reason).\n> \n> As an example, consider a child trying to write, who has to carefully think about each letter she draws and the flow of words, versus an experienced author, who does all that automatically and instead directly thinks about the emotional states he wants to evoke in the reader. (Though he's still able to focus on calligraphy if he wants to be fancy.) [Chunking](http://billwall.phpwebhosting.com/articles/chunking.htm) is again relevant here.\n\n**b) Disjunction.** Consider this different setup:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b1abe6c7dd682d397530ac1d2ee43a9e6132a8856e74f558.png)\n\nAgain, suppose we have two contextual goals \\\\(G_i\\\\) and \\\\(G_k\\\\) defined over \\\\(x_i\\\\) and \\\\(x_k\\\\) respectively. But there's no obvious way to combine them here: if their causal influences meet anywhere, we haven't discovered these parts of the world-model yet. Their contexts are entirely separate.\n\nAs such, there isn't really a way to unify them yet: we just go \\\\(G_{i \\land k}= G_i \\land G_k\\\\), and hope that, as the world-model expands, the contexts would meet somewhere.\n\nAs an example, we might consider one's fruit preferences versus one's views on the necessity of the Oxford comma. They seem completely unrelated to each other. (And as a speculative abstract unification, perhaps one is entertained by ambiguity or duality-of-interpretation, and so prefers no Oxford comma and fruits with a mild bittersweet taste, as instantiations of that more abstract preferences? Though, of course, all human values don't *have* to ever converge this way.)\n\nNow let's complicate it a bit:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b62ed9f2c14453ce0515e5691ae604550b96b6444f95fda3.png)\n\nAgain, we have contextual goals \\\\(G_i\\\\) and \\\\(G_k\\\\). We don't see a way to combine them, yet neither are they fully separate, as their contexts are entwined. If both \\\\(x_i\\\\) and \\\\(x_k\\\\) assume undesirable states, there might not be a distribution of values we may assign \\\\(a_1\\\\) and \\\\(a_2\\\\) such that both contextual goals are achieved. How do we deal with it?\n\nWell, the selection pressure ran into this problem a while ago, well before the GPS, and it's already developed a solution: the cross-heuristic communication channel. Any given heuristic \\\\(h_i\\\\) has an activation strength \\\\(D_i\\\\), and if two heuristics \\\\(h_i\\\\), \\\\(h_k\\\\) contest, the actions taken are calculated as some function of the activation strengths \\\\(D_i\\\\), \\\\(D_k\\\\), and the recommended actions \\\\(A_{s_i}\\\\), \\\\(A_{s_k}\\\\).\n\nThe GPS can recover all of these mechanics, and then just treat the sum of all \"activation strengths\" as negative utility to minimize-in-expectation. The actual trade-offs seem to heavily depend on the specifics of the situation (e. g., can we \"half-achieve\" every goal, or we have to choose one or the other?) — I've unfortunately failed to come up with a general yet compact way to describe conflict-resolution here. (Though I can point to [some related ideas](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=o4ZLLqD5BAshiH83Z).)\n\nA particular degenerate case is if the two contextual goals are directly opposed to each other. That is, suppose that the actions that bring \\\\(x_i\\\\) near the target almost always move \\\\(x_k\\\\)'s value outside it — like a desire to smoke interferes with one's desire to look after their health. In this case, if \\\\(D_k\\\\) always outputs higher values than \\\\(D_i\\\\), \\\\(G_i\\\\) ends up fully *suppressed: *\\\\(G_{i \\lor k}= G_k\\\\).\n\nSuppose, however, that \\\\(G_i\\\\) wasn't a hard-coded heuristic. Rather, \\\\(G_i\\\\) was produced as the result of value compilation, perhaps as a combination of contextual goals over \\\\(x_3\\\\) and \\\\(x_6\\\\). In this case, we may \"decompile\" \\\\(G_i\\\\) back into \\\\(G_3\\\\) and \\\\(G_6\\\\), and try to find ways to re-compile them such that the result doesn't interfere with \\\\(G_k\\\\). Perhaps \\\\(G_3\\\\) is \"have a way to relax when stressed\" and \\\\(G_6\\\\) is \"I want to feel cool\", and we can compile them into \\\\(G_d\\\\) = \"carry a fabulous fidget toy\".[^5kttnui755h]\n\n**c) By iteratively using these techniques**, we can, presumably, arrive at some \\\\(G_\\Sigma\\\\). \\\\(G_\\Sigma\\\\) might end up fully unitary, like a perfect hedonist's desire to wirehead, or as a not-perfectly-integrated spread of values \\\\(G_\\Sigma: G_1 \\land G_2 … \\land G_n\\\\). But even if it's the latter, it'll be a much *shorter* list than \\\\(G_\\Sigma^\\text{naive}\\\\), and the more abstract goals should allow greater generalizability across environments.\n\nOne issue here is that there might be multiple \\\\(G_\\Sigma\\\\) consistent with the initial set of heuristics. As far as human value reflection goes, it's probably fine: either of them should be a fair representation of our desires, and the specific choice has little to do with AI Notkilleveryoneism[^63ffbt54loe]. But when considering how an AI's process of value reflection would go, well, it might turn out that even for a well-picked suite of proto-values, only some of the final \\\\(G_\\Sigma\\\\) don't commit omnicide.\n\nAnyway, that was the easy part. Now let's talk about all the *complications*.\n\n### 6B. Path-Dependence\n\nSuppose that you have three different contextual goals, all of equal activation strength. For example, \\\\(G_1\\\\) is \"I like looking at spiders, they're interesting\", \\\\(G_2\\\\) is \"I like learning about spider biology, it's interesting\", and \\\\(G_3\\\\) is \"I want to flee when there's a spider near me, something about being in their vicinity physically just sets me off\".\n\nSuppose that you live in a climate where there isn't a lot of spiders, so \\\\(G_3\\\\) almost never fires. On the other hand, you have Internet access, so you spend day after day looking at spider pictures and reading spider facts. You compile the first two proto-values into \\\\(G_{1 \\land 2}\\\\): \"I like spiders\".\n\nThen you move countries, discover that your new home has a lot of spiders, and to your shock, realize that you fear their presence. What happens?\n\nPerhaps you compile a new value, \\\\(G_{(1 \\land 2) \\land 3}\\\\) = \"I like spiders, but only from a distance\". Or you fully suppress \\\\(G_3\\\\), since it's revealed to be at odds with \\\\(G_{1 \\land 2}\\\\) whenever it activates.\n\nThat's not what would've happened if you started out in an environment where all three contextual goals had equal opportunity to fire. \\\\(G_3\\\\) would've counterbalanced \\\\(G_1\\\\) and \\\\(G_2\\\\), perhaps resulting in \\\\(G_{1 \\land 2 \\land 3}\\\\) = \"I guess spiders are kind of neat, but they freak me out\".\n\nThus, the process of value compilation is *path-dependent*. It matters in which order values are compiled.\n\n### 6C. Ontological Crises\n\nWhat happens when a concept in a world-model turns out not to correspond to a concrete object in reality — such as the revelation that things consist of parts, or that souls aren't real? What if that happens to something you care about?\n\nThis is actually fairly easy to describe in this model. There are two main extreme cases and a continuum between them.\n\n**a) \"Ontology expansion\".** The first possibility is that the object we cared about was a natural abstraction. This shift roughly looks like the following:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/807e42d37f3a4444ff4b8cf31a3b6f27e8a9b39f2d745f45.png)\n\nSuppose we cared about \\\\(x_p\\\\), and it turned out to be a natural abstraction over a system of \\\\(x_{p1}, x_{p2}, x_{p3}\\\\). That would merely mean that (1) the state of \\\\(x_p\\\\) is downstream of the states of \\\\(x_{p1}, x_{p2}, x_{p3}\\\\), and (2) we can model the impact of our actions on \\\\(x_p\\\\) more precisely by modelling \\\\(x_{p1}, x_{p2}, x_{p3}\\\\), if we so choose. Which we may not: maybe we're indifferent to the exact distribution of values in \\\\(x_{p1}, x_{p2}, x_{p3}\\\\), as long as the value of \\\\(x_p\\\\) they compute remains the same.\n\nAnd \\\\(x_p\\\\) is still a reasonable object to place on our map. The same way it's meaningful to think of and care about \"humans\", even if they're not ontologically basic; and the same way we would care about the high-level state of a human (\"are they happy?\") and be indifferent towards the low-level details of that state (\"okay, they're happy, but are the gut bacteria in their stomach distributed like *this* or like *that*?\").\n\n**b) \"Ontology break\".** In the second case, the object we cared about turns out to flat-out not exist; not correspond to *anything* in the territory:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fec1a4435cceedf70e18d63c81f9da7a8dbe935935421cca.png)\n\nAs the example, consider a spiritual person realizing that spirits don't exist, or a religious person in the middle of a crisis of faith. We valued \\\\(x_p\\\\), but \\\\(\\\\)now there's just nothing resembling it in its place. What can we do?\n\nDo value decompilation, again. Suppose that the initial set of heuristics from which the value was compiled *wasn't* defined over \\\\(x_p\\\\). Suppose we had contextual goals \\\\(G_1\\\\) and \\\\(G_2\\\\) defined over \\\\(x_1\\\\) and \\\\(x_2\\\\) respectively, which we then collapsed into \\\\(G_p\\\\). We can fall back on them: we can remember why we decided we cared about \\\\(x_p\\\\), then attempt to re-compile new downstream values from \\\\(G_1\\\\) and \\\\(G_2\\\\) in the new world-model. Perhaps, say, we end up noticing that \\\\(G_1\\\\) and \\\\(G_2\\\\) do an awful lot to control the value of \\\\(x_7\\\\)...?\n\nAs a human example, perhaps an apostate would decide that they cared about God because they sought a higher purpose, or wanted to feel a sense of belonging with their community. If so, they may find ways to satisfy these urges that would work in the new ontology.\n\nThe quantitative effects of this kind of ontology break can be quite dramatic. The non-existent node might be [a linchpin of the world-model](https://www.lesswrong.com/posts/Cuig4qe8m2aqBCJtZ/which-values-are-stable-under-ontology-shifts?commentId=4aexj7RbnbANj3Hdn), having \"infected\" most of the nodes in it. It would entail a lengthy process of value re-compilation, and the final utility function might end up very different. *Qualitatively*, though, nothing would change.\n\nWe can imagine an extreme case where the basic initial heuristics were defined over empty concepts as well. In this case, perhaps they'll just have to be outright suppressed/ignored.\n\nA degenerate case is if all initial heuristics are defined over empty concepts. I expect it's unlikely in real-life systems, though: many of them would likely be associated with *mental* actions, which would be self-evidently real by the very functioning of the system (by analogy with *cogito ergo sum*).\n\n> **Sidenote:** \"But does that mean that the final form values take depends on the structure of the environment?\" Yes.\n> \n> Note that this doesn't mean that moving an adult human from Earth to an alien civilization would change their values. That wouldn't involve any *ontological crises*, after all — the basic structure of the environment wouldn't change from their perspective, it would merely extend to cover this new environ. They would need to learn a number of new low-level abstractions (the alien language, what signals correspond to what mental states, etc.), but they would likely be able to \"bridge\" them with the previous high-level ones eventually, and continue the process of value reflection from there (e. g., if they cared for other people before, then once they properly \"grasp\" the personhood of the aliens, they'd begin caring for them too, and may eventually arrive at valuing some universal eudaimonia).\n> \n> On the other hand, a human in a universe with God does arrive at different values than in a godless one. I think that makes sense.[^qm4apbmogwo]\n\n**c) In practice**, most of the cases will likely be somewhere between the two extremes. The world-model expansion would reveal that what the agent cared about wasn't *precisely* a natural abstraction, but also not entirely a thing that didn't exist. They might end up re-calculating the correct natural abstraction, then deciding to care for it, experiencing a *slight* ontology break. (Humans souls aren't real, but human minds are, so we switch over, maybe with a bit of anguish. A more mundane example: someone you knew turned out to be a very different person from what you'd thought of them, and you have to re-evaluate your relationship.)\n\nOr maybe they'll end up caring about some low-level properties of the system *in addition to* the high-level ones, as the result of some yet-disjointed value finding something in the low-level to care about. (E. g., maybe some distributions of bacteria are aesthetically pleasing to us, and we'd enjoy knowing that one's gut bacteria are arranged in such a fashion upon learning of their existence? Then \"a human\" would cause not only empathy to fire, but also the bacteria-aesthetics value.)\n\n### 6D. Meta-Cognition\n\nThis is the real problem.\n\n**a) Meta-heuristics.** As I'd mentioned back in 5C, there'll likely be heuristics that directly intervene on the GPS' process. These interventions might take the form of *interfering with value compilation itself*.\n\nThe GPS can notice and explicate these heuristics as well, of course. And endorse them. If endorsed, they'll just take the form of preferences over cognition, or the world-model. A spiritualist's refusal to change their world-view to exclude spirits. A veto on certain *mental actions*, like any cognitive process that concludes you must hate your friends. A choice to freeze the process of value compilation at a given point, and accept the costs to the coherency of your decisions (this is how deontologists happen).\n\nAny heuristic that can do that would gain an asymmetric advantage over those that can't, as far as its representation in the final compilation is concerned.\n\nI hope the rest of this post has shown that such mechanisms are as likely to be present in AIs as they are in humans.\n\n**b) Meta-cognition itself.** The core thing to understand, here, is that the GPS undergoing value compilation isn't some arcane alien process. I suspect that, in humans, it's done *fully consciously*.\n\nSo, what problems do humans doing value reflection run into?\n\nFirst off, the GPS might plainly *make a mistake*. It doesn't have access to the ground truth of heuristics and can't confirm for sure when it did or did not derive a correct contextual goal or conducted a valid compilation.\n\nSecond, meta-cognitive preferences can originate externally as well. [A principled stance to keep deontological principles around even if you're an utilitarian](https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans), for example.\n\nThird, the GPS is not shy about taking shortcuts. If it encounters some clever way to skip past the lengthy process of value compilation and get straight to the answer, it'll go for it, the same we'd use a calculator instead of multiplying twenty-digit numbers manually. Hence: humans becoming hijacked by various ideologies and religions and philosophies, that claim to provide the ultimate answers to morality and the meaning of life.\n\nThus, the final compilation might not even have much to do with the actual initial spread of heuristics.\n\nAt the same time, meta-cognition might counteract the worst excesses of path-dependence. We can consciously choose to \"decompile\" our values if we realize we've become confused, look at our initial urges, then re-compile the more correct combination from them.\n\n### 6E. Putting It Together\n\nAs such, the process for computing the final mesa-objective \\\\(G_\\Sigma\\\\) is a function of:\n\n*   The initial set of heuristics, *especially* those with meta-cognitive capability.\n*   The true environment structure.\n*   The data the agent is exposed to.\n*   The exact sequence in which the agent reverse-engineers various heuristics, combines various values, and is exposed to various data-points.\n\nIs it any wonder the result doesn't end up resembling the initial outer objective \\\\(U\\\\)?\n\nI'm not sure if that's as bad as it looks, as far as irreducible complexity/impossibility-to-predict goes. It might be.\n\n* * *\n\n7\\. Miscellanea\n---------------\n\n**a) Heuristics' Flavour.** You might've noticed that this post has been assuming that every heuristic ends up interpreted as a proto-value that at least potentially gets a say in the final compilation. That's... not right. Isn't it? I'm not actually sure.\n\nI think the Shard Theory answer would be that yes, it's right. Every heuristic is a shard engaging in negotiation with every other shard, vying for influence. It might not be \"strong\" or very good at this, but an attempt will be made.\n\nCounterpoint: Should, say, your heuristic for folding a blanket be counted as a proto-value? The GPS is reverse-engineering them to gain data on the environment, and this should really just be a \"skill\", not a proto-value.\n\nCounter-counterpoint: Imagine an agent with a lot of heuristics for winning at chess. It was clearly optimized for playing chess. If the GPS' goal is to figure out what it was made for and then go optimize that, then \"I value winning at chess\" or \"I value winning\" *should* be plausible hypotheses for it to consider. It makes a certain kind of common sense, too. As to blanket-folding — sure, it's a rudimentary proto-value too. But it's too weak and unsophisticated to get much representation. In particular, it probably doesn't do meta-cognitive interventions, and is therefore at an asymmetrical disadvantage compared to those that do.\n\n... Which is basically just the Shard Theory view again.\n\nSo overall, I think I'll bite that bullet, yes. Yes, every starting heuristic should be interpreted as a proto-value that plays a part in the overall process of value compilation. (*And also* as a skill that's explicated and integrated into the world-model.)\n\n**b) Sensitivity to the Training Process.** Let's consider two different kinds of minds: humans, [which are likely trained via on-line reinforcement learning](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values), and autoregressive ML models, who can continuously cogitate forever even with frozen weights by chaining forward passes. (Suppose the latter scales to AGI.)\n\nThe internal dynamics in these minds might be quite different. The main difference is that in humans, heuristics can adapt at runtime, and new heuristics can form, while in the frozen-weights model, the initial spread of heuristics is static.\n\nAs one obvious consequence, this might make human heuristics \"more agenty\", in the sense of being able to conduct more nuanced warfare and negotiation between each other. In particular, they'd have the ability to learn to understand new pieces of the world-model the GPS develops, and learn new situations when they must tamper with the GPS for self-preservation (unlike static heuristic, for whom this is [inaccessible information](https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome)). \"Heuristic\" might be a bad way to describe such things, even; \"[shards](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values)\" might be better. Perhaps such entities are best modeled [as traders](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=o4ZLLqD5BAshiH83Z) rather than functions-over-nodes?\n\nBut a potentially bigger impact is on value-compilation path-dependence.\n\nIn humans, when we compile contextual goals \\\\(G_1\\\\) and \\\\(G_2\\\\) into \\\\(G_{1 \\land 2}\\\\), and then stay with \\\\(G_{1 \\land 2}\\\\) for a while, we end up developing *a new shard* around \\\\(G_{1 \\land 2}\\\\) — a structure of the same type as the initial \\\\(G_1\\\\), \\\\(G_2\\\\). Shards for \\\\(G_1\\\\) and \\\\(G_2\\\\), meanwhile, might die out as their \"child\" eats the reinforcement events that would've initially went for them. (Consider the example of the Vain Man from 6A.)\n\nBut if that initial set of heuristics is frozen, as in an ML model, perhaps the agent always ends up developing a \"path-independent\" generalization as described at the end of 6A? The runtime-compiled values would be of a different type to the initial heuristics: just mental constructs. And if we assume the AI to be superintelligent when it finalizes the compilation, it's not going to be fooled by whatever it reads in the data, so the \"mistaken meta-cognition\" concerns don't apply. Certainly it might make mistakes *at the start* of the process, but if incorrect compilations aren't \"sticky\" as they are with humans, it'll just decompile them, then re-compile them properly!\n\nReminder: This doesn't mean it'll become aligned with the outer objective. \\\\(G_\\Sigma\\\\) is still not \\\\(U\\\\) but a proxy for \\\\(U\\\\), so even path-independent value compilation ends with inner misalignment.\n\nBut it does make \\\\(G_\\Sigma\\\\) derivable from *just* the parameters and the world-model, not the data.\n\n**c) Self-Awareness.** I've belatedly realized that there's a third structure the GPS can interface with: *the GPS itself*. This fits nicely with some of my [published](https://www.lesswrong.com/posts/bbtLG3LNGGBWzXqjK/is-this-thing-sentient-y-n) and yet-unpublished thoughts on self-awareness and the perception of free will.\n\nIn short, the same way it's non-trivial to know what heuristics/instincts are built into your mind, it's non-trivial to know *what you're currently thinking of*. You need a separate feedback loop for that, a structure that summarizes the GPS' activity and feeds it back into the GPS as input. That, I suspect, directly causes (at least in humans):\n\n*   Self-awareness (in a more palatable fashion, compared to just having abstractions corresponding to \"this agent\" or \"this agent's mental architecture\" in the world-model).\n*   [The perception of the indisputability of your own existence](https://en.wikipedia.org/wiki/Cogito,_ergo_sum).\n*   The perception of free will. (You can request a summary of your current object-level thoughts and make a decision to redirect them; or you can even request a summary of your meta-cognition, or meta-meta-cognition, and so on, and redirect any of that. Therefore, you feel like you're freely \"choosing\" things. But the caveat is that every nth-level request spins up a (n+1)th-level GPS instance, and the outermost instance's behavior isn't perceived and \"controlled\" by us, but *is us*. As such, we do control our perceived behavior, but our outermost loop that's doing this is always non-perceivable and non-controllable.)\n\nBut that probably ought to be its own separate post.\n\n* * *\n\n8\\. Summary\n-----------\n\n*   Once the GPS develops, the selection pressure needs to solve two tasks:\n    *   Let the GPS access the procedural and implicit knowledge \\\\((E \\to A) \\to U\\\\) represented by the heuristics, even though they're completely illegible.\n    *   Pick some interim objective for the GPS to make object-level plans around.\n*   The selection pressure solves the first problem by hooking the GPS up to the two coherent data structures that were already developed: the world-model, and the cross-heuristic communication channel.\n*   The second problem is solved by teaching the heuristics to use the GPS to assist in problem-solving. In essence, the GPS starts \"optimizing\" for the implicit goal \\\\(B_\\Sigma\\\\) of \"do whatever the agent was already doing, but better\".\n*   As this is happening, the GPS proves more and more useful, so the amount of compute allocated to it expands. It learns to reverse-engineer heuristics, getting explicit access to the non-explicit knowledge.\n*   The GPS is encouraged to treat heuristics as \"clues\" regarding its purpose, and \\\\(B_\\Sigma\\\\) as some proxy of the real goal \\\\(G_\\Sigma\\\\) that the agent is optimized to solve. (Which, in turn, is a proxy of the real target objective \\\\(U\\\\).)\n*   Every heuristic, thus, is at once a piece of the world-model and a proto-value.\n*   There's plenty of incentives to compress these splintered proto-values into a proper utility function: it's a way to avoid dominated strategies, the splintered goal-set is computationally unwieldy, and the GPS can probably derive a better, more generalizable approximation of \\\\(G_\\Sigma\\\\) by explicitly trying to do that.\n*   The basic process of value compilation goes as follows:\n    *   Hypothesize that a number of heuristics, e. g. \\\\(h_i\\\\) and \\\\(h_k\\\\), are jointly optimized for achieving some single goal \\\\(G_{i \\land k}\\\\). If such a goal is found, start optimizing it directly.\n    *   If no goal is found, compute the appropriate trade-offs between \\\\(G_i\\\\) and \\\\(G_k\\\\) based on the underlying heuristics' activation-strength functions \\\\(D_i\\\\) and \\\\(D_k\\\\). If one of the goals has to be fully suppressed, try decompiling it and repeating the process.\n    *   Iterate, with initial heuristics replaced with derived contextual goals, until arriving at \\\\(G_\\Sigma\\\\).\n*   That process runs into several complications:\n    *   Path-dependence, where the order in which contextual goals are compiled changes the form the final goal takes.\n    *   Ontological crises, where we either (1) expand our model to describe the mechanics of the natural abstraction we care about, or (2) experience an ontology break where the object we cared about turns out not to exist, which forces us to do value decompilation + re-compilation as well.\n    *   Meta-cognition: The agent might have preferences over the value compilation process directly, biasing it in unexpected ways. In addition, the GPS implements (something like) logical reasoning, and might lift conclusions about its values from external sources, including mistaken ones.\n*   Some considerations have to be paid to the online vs batch training: agents trained on-line might be uniquely path-dependent in a way that those with frozen weights aren't.\n\n* * *\n\n9\\. Implications for Alignment\n------------------------------\n\nMy main practical takeaway is that I am now much more skeptical of any ideas that plan to achieve alignment by *guiding* the process of value formation, or by setting up a \"good-enough\" starting point for it.\n\nTake the basic Shard Theory approach as the example.[^inn97040y8e] It roughly goes as follows:\n\n1.  Real-life agents like humans don't have static values, but a *distribution over values* that depends on the context.\n2.  There's ample reason to believe that AI agents found by the SGD will be the same.\n3.  What we need to do isn't to make an AI that's solely obsessed with maximizing human values, but to ensure that there's at least one powerful & metacognition-capable shard/proto-value in it that values humans: that our value distributions merely *roughly overlap*.\n4.  Then, as the AI does value compilation, that shard will be able to get a lot of representation in the final utility function, and the AI will naturally value humans and want to help humans (even if it will *also* want to tile a large swathe of the cosmos with paperclips).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/6164f95407fa1cba68877d8c0c598d0dcca980a27289fb2d.png)\n\nA hopefully non-catastrophic value distributions divergence, according to the scheme above. [*Source.*](https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals?commentId=p6ZbpXrFw2BCAebrh)\n\nOne issue is that the value-humans shard [*would* need to be perfectly aligned with human values](https://www.lesswrong.com/posts/heXcGuJqbx3HBmero/people-care-about-each-other-even-though-they-have-imperfect?commentId=rZPPm7SpGtsQvEvCC), and that's most of this approach's promised advantage gone. That's not much of an issue, though: I think we'd need to do that in any workable approach.\n\nBut even if we can do that, I fear this wouldn't work out even in the path-independent scenario. The Vain Man from 6A, again: over the course of value compilation, the AI might decide that it only likes humanity as an *instantiation* of some more abstract principle. Which might be something nice like \"all possible sapient life\"... or maybe it's more paperclips, and it trades us away like a car for a yacht.[^0ww16yk8swe]\n\nAnd then we get into the actually complicated path-dependent meta-cognitive mess (which we have to be ready to deal with, since we don't know how the last-minute AGI architecture will look), and... I don't think this is tractable *at all*. We'd need to follow the AI's explicit reasoning into superintelligence; it'd be hopeless. Would take decades to understand manually, to reverse-engineer and translate the abstractions it'll be thinking in.\n\nSo I suspect that we won't be able, in practice, to figure out how to set up some initial proto-values such that they'll compile into a non-omnicidal utility function. I suspect any plan that hinges on this is doomed.[^rtv4ibnti9j]\n\nMy current most promising alignment scheme is as follows:\n\n*   Train an AI to the point where the GPS forms.\n*   Find a way to distinguish heuristics/proto-value from the world-model and the GPS.\n*   Wipe out every proto-value, eating the cost in effective world-model downgrade.\n*   Locate human values or corrigibility in the world-model (probably corrigibility, see [this discussion](https://www.lesswrong.com/posts/5ntgky9ShzKKWu7us/plans-are-predictions-not-optimization-targets?commentId=n2XNiEdSjAKx7mzDC)).\n*   Hook the GPS up directly to it, making the AI an aligned/corrigible wrapper-mind.\n*   Let it bootstrap itself to godhood. The GPS will be able to spin up new heuristics and re-expand the world-model.[^e8pq3jimdwl]\n\nIf there are no proto-values leading the GPS astray, there's no problem. (Those are, by the way, the \"unnatural conditions\" I've mentioned all the way back in Section 2.)\n\nFinding a way to identify learned world-models, or to somehow [train up a pure world-model](https://www.lesswrong.com/posts/P6aDYBDiu9DyvsF9g/are-generative-world-models-a-mesa-optimization-risk), therefore seem like high-priority research avenues.\n\n### Bonus: [The E-Coli Test for Alignment](https://www.lesswrong.com/posts/ZdCztwnxXu3aC4kxZ/the-e-coli-test-for-ai-alignment)\n\nAlright, so an E. coli doesn't implement a GPS, so it can't do value compilation on its own. As such, it's unclear how meaningful it is to talk about its \"values\". But an attempt can be made! How we may do it:\n\n*   Draw a causal graph of the E. coli's environment, all the natural abstractions included.\n*   Scan the E. coli and back out its \"heuristics\", i. e. functions that locate conceptually localized environment features and respond with specific behaviors.\n*   Do path-independent value reflection as described in 6A, or plot out some random value-compilation path and go through it.\n*   Whatever you end up with is the E. coli's values. They should be highly-abstract enough to be implementable in environments alien to it, as well.\n\nHm, that was insightful. For one, it appears that we don't need the agent's own world-model for path-independent (or random-path) value compilation! We can just get [the \"true\" world-model](https://www.lesswrong.com/posts/FWuByzM9T5qq2PF2n/a-correspondence-theorem) (if we have access to it) and the heuristics set, then directly compile the final values using it. In essence, it's just equivalent to getting all the ontological crises out of the way at the start.\n\nWhat we *can* do with subjective world-models is compute \"impossible fantasy\" values — values that the agent would have arrived at if the world really had the structure they mistakenly believe it to have; if the world's very ontology was optimized for their preferences. (E. g., valuing \"spirits\" if spirits were a thing.)\n\n* * *\n\n10\\. Future Research Directions\n-------------------------------\n\nI think this model is both robust to a lot of possible changes, sufficing to describe the dynamics within many categories of agents-generated-by-a-greedy-algorithm, and very sensitive to other kinds of interventions. For example, the existence of some *additional* convergent data structure, or a different point for the GPS to originate from, might change the underlying path dynamics a lot.\n\nThat said, I now suspect that the work on value compilation/goal generalization is to a large extent a dead end, or at least not straightforwardly applicable. It seems that the greedy selection algorithms and the AI itself can be trusted with approximately 0% of the alignment work, and approximately 100% of it will need to be done by hand. So there may not be much point in modeling the value-formation process in detail...\n\nThe caveat here is that building a solid theoretical framework of this process will give us data regarding what features we should expect to find in trained models, and how to find them — so that we may do surgery on them. As such, I think the questions below are still worth investigating.\n\nI see two ways to go from here: expanding this model, and concretizing it. Expansion-wise, we have:\n\n*   Are there any other consistently-formatted internal data structures that agents trained by a greedy algorithm form, in addition to the world-model and the cross-heuristic communication channel? What are they? What changes to the training process would encourage/discourage them?\n*   What causes meta-cognitive heuristics to appear? Are there any convergently-learned meta-heuristics?\n*   Is there any difference between \"goals\" and \"values\"? I've used the terms basically interchangeably in this post, but it might make sense to assign them to things of different types. (E. g., maybe a \"goal\" is the local instantiation of a value that the agent is currently pursuing? Like a yacht is for a high-status item.)\n*   Is there a sense in which values become \"ever more abstract\" as value-compilation goes on? What precisely does this mean?\n*   Where in the agent does the GPS form? (I see three possibilities: either as part of the heuristic conflict-resolution mechanism, within one of the heuristics, or within the world-model (as part of, e. g., the model of a human).)\n*   What's the minimal wrapper structure and/or set of world-model features necessary to let the GPS \"handle itself\", i. e. figure out how to solve the problem of automatically pointing itself at the right problems, no heuristical support necessary?\n\nOn the concretization side:\n\n*   A proof that the world-model would converge towards holisticity. Does it always hold, past some level of environment complexity?\n    *   The \"Crud\" Factor is probably a good starting point.\n*   A proof that \"heuristics\" is a meaningful abstraction to talk about.\n    *   Presumably uses the Natural Abstraction Hypothesis as the starting point. Might be fairly straightforward under some environment structure assumptions, like there being mostly-isolated contexts such that you can optimize in them without taking the rest of the world-model into consideration.\n    *   (There's interesting tension between this and the Crud Factor thing, but it's probably not an actual contradiction: mapping from observations to the world-model and optimizing in the world-model are very different processes.)\n*   A proof that heuristics aren't under pressure to become internally intelligible.\n*   Some proofs about the necessary structure of the cross-heuristic communication channel.\n*   Exact specification of the environment structure that can't be solved without the GPS.\n*   A more formal description of the basic value-formation algorithm.\n*   A more formal description of meta-cognitive heuristics. What does it mean to \"bias\" the value-compilation process?\n\n[^sfm7lmf8s5f]: We'll return to them in Section 9. \n\n[^pox9dfksfr]: This function can be arbitrarily complex, too — maybe even implementing some complex \"negotiations\" between heuristics-as-shards. Indeed, this is plausibly the feature from which the GPS would originate in the first place! But this analysis tries to be agnostic as to the exact origins of the GPS, so I'll leave that out for now. \n\n[^mvo7fpvhjn8]: And plausibly some shared observation pre-processing system, but I'll just count it as part of the world-model. \n\n[^x5jrwc7ofnn]: Though this is potentially subject to the specifics of the training scheme. E. g., if the training episodes are long, or we're chaining a lot of forward passes together like in a RNN, that would make runtime-computations more effective at this than the SGD updates. That doesn't mean the speed prior is going to save us/reduce the path-dependence I'll go on to argue for here, because there'll still be some point at which the GPS-based at-runtime reverse-engineering outperforms selection-pressure-induced legibility. But it's something we'd want fine-grained data on. \n\n[^m11o3h3q9li]: Second-order natural abstraction? \n\n[^s8nrfm0cmjn]: Naively, this process would continue until the agent turns into a proper \\(U\\)-optimizer. But it won't, because of gradient starvation + the deception attractor. There are other posts talking about this, but in short:Once \\(G_\\Sigma\\) agrees with \\(U\\) in 95% of cases, the selection pressure faces a choice between continuing to align \\(G_\\Sigma\\), and improving the agent's ability to achieve \\(G_\\Sigma\\). And it surely chooses the latter most of the time, because unless the agent is already superintelligently good at optimization, it probably can't actually optimize for \\(G_\\Sigma\\) so hard it decouples from \\(U\\).Then, once the agent is smart enough, it probably has strategic awareness, wants to protect \\(G_\\Sigma\\) from the selection pressure, and starts trying to do deceptive alignment. And then it's in the deception attractor: its performance on the target objective rises sharper as its general capabilities improve (since that improves both the ability to achieve \\(U\\) and the ability to figure out what it should be pretending to want), compared to improving its alignment. \n\n[^5kttnui755h]: Note: This isn't a precisely realistic example of value compilation, for a... few reasons, but mainly the phrasing. Rather than \"smoking\" and \"using a fabulous fidget toy\", it should really say \"an activity which evokes a particularly satisfying mixture of relaxation and self-affirmation\".There seems to be some tendency for values to increase in abstractness as the process of compilation goes on: earlier values are revealed to be mere \"instantiations\" of later values, such that we become indifferent to the exact way they're instantiated (see the cars vs. yachts example). It works if \"relax\" and \"feel cool\" are just an instantiation of \"feel an emotion that's a greater-than-its-parts mix of both\", such that we're indifferent to the exact mix. But they're not an instantiation of \"smoke a cigar\": if smoking ceased to help the person relax and feel cool, they'd stop smoking and find other ways to satisfy those desires. \n\n[^63ffbt54loe]: Although I imagine some interesting philosophy out of it. \n\n[^qm4apbmogwo]: Or maybe not. Something about this feels a bit off. \n\n[^inn97040y8e]: Note that this isn't the same as my disagreeing with the Shard Theory itself. No, I still think it's basically correct. \n\n[^0ww16yk8swe]: You might argue that we can set up a meta-cognition shard that implacably forbids the AI's GPS from folding humanity away like this, the way something prevents deontologists from turning into utilitarians, or the way we wouldn't kill-and-replace a loved one with a \"better\" loved one. I'm not sure one way or another whether that'll work, but I'm skeptical. I think it'll increase the problem difficulty dramatically: that it'd require the sort of global robust control over the AI's mind that we can use to just perfect-align it. \n\n[^rtv4ibnti9j]: One idea here would be to wait until the AI does value compilation on its own, then hot-switch the \\(G_\\Sigma\\) it derives. That won't work: by the point the AI is able to do that, it'd be superintelligent, and it'll hack through anything we'll try to touch it with. We need to align it just after it becomes a GPS-capable mesa-optimizer, and ideally not a moment later. \n\n[^e8pq3jimdwl]: One issue I don't address here is that in order to do so, the GPS would need some basic meta-cognitive wrapper-structure and/or a world-model that contains self-referencing concepts — in order to know how to solve the problem of giving its future instances good follow-up tasks. I've not yet assessed the tractability of this. We might need some way to distinguish such structures from other heuristics, or figure out how to hand-code them.",
      "plaintextDescription": "0. Introduction\nWhen we look inwards, upon the godshatter, how do we make sense of it? How do we sort out all the disparate urges, emotions, and preferences, and compress them into legible principles and philosophies? What mechanisms ensure our robustness to ontological crises? How do powerful agents found by a greedy selection process arrive at their morals? What is the algorithm for value reflection?\n\nThis post seeks to answer these questions, or at least provide a decent high-level starting point. It describes a simple toy model that embeds an agent in a causal graph, and follows its moral development from a bundle of heuristics to a superintelligent mesa-optimizer.\n\nThe main goal of this write-up is to serve as a gears-level model — to provide us with a detailed step-by-step understanding of why and how agents converge towards the values they do. This should hopefully allow us to spot novel pressure points — opportunities for interventions that would allow us to acquire a great deal of control over the final outcome of this process. From another angle, it should equip us with the tools to understand how different changes to the training process or model architecture would impact value reflection, and therefore, what kinds of architectures are more or less desirable.\n\nLet's get to it.\n\n----------------------------------------\n\n\n1. The Setup\nAs the starting point, I'll be using a model broadly similar to the one from my earlier post.\n\nLet's assume that we have some environment E represented as a causal graph. Some nodes in it represent the agent, the agent's observations, and actions.\n\nEvery turn t (which might be a new training episode or the next time-step in a RL setup), the agent (blue node) reads off information from the (green) observation nodes O, sets the values for the (red) action nodes A, all nodes' values update in response to that change, then the agent receives reward based on the (purple) reward nodes R. The reward is computed as some function U:Rt→",
      "wordCount": 10274
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "4CQy8rim8PGt4sfCn",
        "name": "Complexity of value",
        "slug": "complexity-of-value"
      },
      {
        "_id": "Q6hq54EXkrw8LQQE7",
        "name": "Gears-Level",
        "slug": "gears-level"
      },
      {
        "_id": "b6tJM7Lza74rTfCBF",
        "name": "Goal-Directedness",
        "slug": "goal-directedness"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "ThtZrHooK7En9mcZr",
    "title": "Greed Is the Root of This Evil",
    "slug": "greed-is-the-root-of-this-evil",
    "url": null,
    "baseScore": 21,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2022-10-13T20:40:56.822Z",
    "contents": {
      "markdown": "The SGD's greed, to be specific.\n\nConsider a ML model being trained end-to-end from initialization to zero loss. Every individual update to its parameters is calculated to move it in the direction of maximal local improvement to its performance. It doesn't take the shortest path from where it starts to [the ridge of optimality](https://www.lesswrong.com/posts/GfEmhoyS6ztk7GQAp/how-to-think-about-overparameterized-models#Ridges__Not_Peaks); it takes the *locally steepest* path.\n\n1\\. What does that mean mechanically?\n-------------------------------------\n\nRoughly speaking, every feature in NNs could likely be put into one of two categories:\n\n1.  Statistical correlations across training data, aka \"the world model\".\n2.  The policy: heuristics/[shards](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT), mesa-objectives, and inner optimization.\n\nThe world-model can only be learned gradually, because higher-level features/statistical correlations build upon lower-level ones, and therefore the gradients towards learning them only appear *after* the lower-level ones are learned.\n\nHeuristics, in turn, can only attach to the things that are [already present in the world-model](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model) (same for [values](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans)). They're functions of abstractions in the world-model, and they fire in response to certain WM-variables assuming certain values. For example, if the world-model is nonexistent, the only available heuristics are rudimentary instincts along the lines of \"if bright light, close eyes\". Once higher-level features are learned (like \"a cat\"), heuristics can become functions of said features too (\"do \\\\(X\\\\) if see a cat\", and later, \"do \\\\(Y\\\\) if expect the social group to assume state \\\\(S\\\\) within \\\\(N\\\\) time-steps\").\n\nThe base objective the SGD is using to train the ML model is, likewise, a function of some feature/abstraction in the training data, like \"the English name of the animal depicted in this image\" or \"the correct action to take in this situation to maximize the number of your descendants in the next generation\". However, that feature is likely a fairly high-level one relative to the sense-data the ML model gets, one that wouldn't be loaded into the ML model's WM until it's been training for a while (the way \"genes\" are very, very conceptually far from Stone Age humans' understanding of reality).\n\nSo, what's the *logical* path through the parameter-space from initialization to zero loss? Gradually improve the world-model step by step, then, once the abstraction the base objective cares about is represented in the world-model, put in heuristics that are functions of said abstraction, optimized for controlling that abstraction's value.\n\nBut that wouldn't do for the SGD. That entire initial phase, where the world-model is learned, would be parsed as \"zero improvement\" by it. No, the SGD wants results, and *fast*. Every update must instantly improve performance!\n\nThe SGD lives by messy hacks. If the world-model doesn't yet represent the target abstraction, the SGD will attach heuristics to upstream correlates/proxies of that abstraction. And it will spin up a *boatload* of such messy hacks on the way to zero loss.\n\nA natural side-effect of that is [gradient starvation](https://arxiv.org/abs/2011.09468)/[friction](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model#3_1__Incrementalism). Once there's enough messy hacks, the SGD won't bother attaching heuristics to the target abstraction even after it's represented in the world-model — because if the extant messy hacks approximate the target abstraction well enough, there's very little performance-improvement to be gained by marginally improving the accuracy so. Especially since the new heuristics will have to be developed from scratch. The gradients just aren't there: better improve on what's already built.\n\n2\\. How does that lead to inner misalignment?\n---------------------------------------------\n\nIt seems plausible that [general intelligence is binary](https://www.lesswrong.com/posts/yrekdsZfLsgfaFjFp/?commentId=eut69uDAyvGZhP87e). A system is either generally intelligent, or not; it either implements [general-purpose search](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see), or it doesn't; it's either [an agent](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a)/optimizer, or not. There's no *continuum* here, the difference is sharp. (In a way, it's definitionally true. How can something be *more than* general? Conversely, how can something less than generally capable be called \"generally intelligent\"?)\n\nSuppose that the ML model we're considering will make it all the way to AGI in the course of training. At some point, it will come to implement some algorithm for General-Purpose Search. The GPS can come from two places: either it'll be learned as part of the world-model (if the training data include generally intelligent agents, such as humans), or as part of the ML model's own policy. Regardless of the origin, it will almost certainly appear at a later stage of training: it's a high-level abstraction relative to any sense-data I can think of, and the GPS's utility can only be realized if it's given access to an advanced world-model.\n\nSo, by the time the GPS is learned, the ML model will have an advanced world-model, plus a bunch of shallow heuristics over it.\n\nBy its very nature, the GPS makes heuristics obsolete. It's the qualitatively more powerful optimization algorithm, and one that can, in principle, replicate the behavior of any possible heuristic/spin up any possible heuristic, and do so with greater accuracy and flexibility than the SGD.\n\nIf the SGD were patient and intelligent, the path forward is obvious: pick out the abstraction the base objective cares about in the world-model, re-frame it as the mesa-objective, then aim the GPS at optimizing it. Discard all other heuristics.\n\nHowever, it's not that easy. Re-interpreting an abstraction as a mesa-objective is [a nontrivial task](https://www.lesswrong.com/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem?commentId=EZMXKmNoR9YisBBuf). Even more difficult is the process of deriving the environment-appropriate strategies for optimizing it — the instincts, the technologies, the sciences. If evolution were intelligent, and had direct write-access to modern geneticists' brains... Well, replacing all of their value system with obsession with increasing their inclusive genetic fitness wouldn't instantly make effective gene-maximizers of them. They'll get there eventually, but that will require a significant amount of re-training on their part — despite the fact that they know perfectly well what a \"gene\" is[^dbik2jhy3dk].\n\nSo there wouldn't be strong gradients towards aiming the GPS at the representation of the base objective. No, gradient starvation would rear its head again:\n\nThere'll already be a lot of heuristics aimed at optimizing upstream correlates of the base objective, and their weighted sum will presumably serve as a good proxy objective (inasmuch as the model would've already been optimized for good performance even prior to the GPS' appearance). These heuristics will contain a lot of what we want: the instincts and the local knowledge of how to get things done in the local environment.\n\nSo the SGD will enslave the GPS to the heuristics. The GPS will be used to improve the heuristics' efficiency, gradually re-interpreting said heuristics as instincts, skills, or mesa-objectives. [They will morph from the optimization *procedure* to the optimization *targets*](https://www.lesswrong.com/posts/kQKcEkzKmpPe6qxDb/broad-picture-of-human-values-1).\n\nAt this point, the SGD will have two choices regarding the parameter updates: it can spend its \"budget\" to tweak the mess of mesa-objectives to more closely resemble the actual base objective, or it can just increase the power of the GPS (give it more processing power, more working memory, access to a richer world-model...). The SGD will probably do both to some extent, but which way will it lean?\n\nWell, as above, the sum of the mesa-objectives should be a good proxy of the base objective at the current level of optimization power, such that optimizing for them optimizes for the base objective. In addition, they will likely take up [*a whole lot* of parameters](https://www.lesswrong.com/tag/complexity-of-value). It'll take a lot of updates to meaningfully rewrite them, and slight tweaks will correspond to just that: slight performance improvements. Conversely, slightly improving the GPS will likely lead to outsized improvements in performance[^j7c37vqdp6f].\n\nUp to a point. [Tails do come apart](https://www.lesswrong.com/posts/dC7mP5nSwvpL65Qu5/why-the-tails-come-apart), and at some point, further improvements to capabilities will lead to *worse* performance on the training objective, requiring adjusting the mesa-objective to be more in-line with it. But then, post-adjustement, the mesa-objective will become a good proxy again, adapted for this new level of optimization power, and the SGD will switch to capability improvement again. It'll switch between these two modes, until...\n\nUntil the ML model becomes clever enough to become situationally aware, and try its hand at [gradient hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking) and [deceptive alignment](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks). Then the winds would change for good: improvements to its capabilities will lead to outsized improvements in its ability to *derive the base objective at runtime*, which will lead to outsized improvements in the ML model's performance on that base objective. At the same time, adjusting its mesa-objective will still lead only to marginal improvements. (*At best*. If the ML model is smart enough, it might destroy the gradients between its current values and the base objective altogether, by e. g. committing to output garbage if it notices meddling.)\n\nThus, there's a fairly *narrow* range of capabilities at which the ML model is smart enough that tails come apart, yet not smart enough to fall into the deceptive-alignment attractor. While it occupies that range, its mesa-objective will be moved towards the base objective. But by default, I think, it leaves that range fairly quickly.\n\nAnd so we get deceptive alignment, by strong default.\n\n(In addition, I argue that this causes high path dependence in sufficiently advanced models/AGI-level models, under [this](https://www.lesswrong.com/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases) formulation:\n\n> A non-closed-form understanding of inductive bias would be something like \"Here's a rule for figuring out which circuits will be built in the first 10% of training.  Run that rule on your dataset and architecture, and write down the circuits.  Now here's a rule for what will be learned in the *next* 10%, which depends on the circuits we already have.  And so forth for the rest of training.\"\n> \n> The thing which makes it not a closed form is that you *have to* reason through it step-by-step; you can't skip to end and say \"well it all caches out to picking the simplest final solution\".  This is a very path-dependent way for things to be.\n\nThe features the ML models learn, and their order, [appear to be a robust function of the training data + the training process](https://arxiv.org/abs/1905.10854), so I suspect there isn't much variance across *training runs*. But the final mesa-objectives are a function of a function of ... a function of the initially-learned shallow heuristics — I expect there *is* strong path-dependence in that sense.)\n\n3\\. Recap\n---------\n\n*   The SGD's greed causes shallow heuristics.\n*   Shallow heuristics starve gradients towards more accurate heuristics.\n*   The GPS, once it appears, gets enslaved to the conglomerate of shallow heuristics.\n    *   These heuristics are gradually re-interpreted as mesa-objectives.\n*   While the GPS is too weak for the crack between the mesa-objective and the base objective to show, marginally improving it improves the performance on the base objective more than tweaking where it's pointing.\n*   Once the GPS is smart enough to be deceptive, marginally improving it improves performance on the base objective more than tweaking where it's pointing.\n*   Greed causes shallow heuristics which cause gradient starvation which causes inner misalignment which causes deceptive alignment.\n\n**Open questions:** To what extent do the mesa-objectives get adjusted towards the base objective once the GPS crystallizes? How broad is the range between tails-come-apart and deceptive alignment? Can that range be extended somehow?\n\n4\\. What can be done?\n---------------------\n\nWell, replacing the SGD with something that takes the shortest and not the steepest path should just about fix the whole inner-alignment problem. That's a pipe dream, though.\n\nFailing that, it sure would be nice if we can get rid of all of those pesky heuristics.\n\nOne way to do that is featured [here](https://www.lesswrong.com/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem). Take a ML model optimized for an objective A. Re-train it to optimize an objective B, picked such that we expect the ML model's WM to feature the abstraction that B is defined over (such as, for example, a diamond). The switch should cause the mess of heuristics for optimizing A to become obsolete, reducing the gradient-starvation effect from them. And, if we've designed the training procedure for B sufficiently well, presumably the steepest gradients will be towards developing heuristics/mesa-objectives that directly attach to the B-representation in the ML model's WM.\n\n[John counters](https://www.lesswrong.com/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem?commentId=6jF6WTaWpCD2Lfzv7) that this only works if the training procedure for B is perfect — otherwise the steepest gradient will be towards whatever abstraction is responsible for the imperfection (e. g., caring about \"things-that-look-like-a-diamond-to-humans\" instead of \"diamonds\").\n\nAnother problem is that a lot of heuristics/misaligned mesa-objectives will presumably carry over. Instrumental convergence and all — things like power-seeking will remain useful regardless of the switch in goals. And even if we do the switch before proper crystallization of the power-seeking mesa-objective, its *prototype* will carry over, and the SGD will just continue from where it left off.\n\nIn fact, this might make the situation *worse*: the steepest path to achieving zero-loss on the new objective might be \"make the ML model a pure deceptively-aligned sociopath that only cares about power/resources/itself\", with the new value never forming.\n\nSo here's a crazier, radical-er idea:\n\n*   Develop [John's Pragmascope](https://www.lesswrong.com/posts/gdEDPHjCY5DKsMsvE/the-pragmascope-idea): a tool that can process some environment/training dataset, and spit out all of the natural abstraction it contains.\n*   Hook the pragmascope up to a regularizer.\n*   Train some ML model under that regularizer, harshly penalizing every circuit that doesn't correspond to any feature the pragmascope picked up in the environment the ML model is learning.\n    *   Ideally, this should result in [an idealized generative world-model](https://www.lesswrong.com/posts/P6aDYBDiu9DyvsF9g/are-generative-world-models-a-mesa-optimization-risk): the regularizer would continuously purge the shallow heuristics, leaving untouched only the objective statistical correlations across the training data, i. e. the world-model.\n    *   Train that generative world-model until it's advanced enough to represent the GPS (as part of the humans contained in its training data).\n*   RL-retrain the model to obey human commands[^zzf8ku2b839], under a more relaxed version of the pragmascope-regularizer + a generic complexity regularizer.\n\nNaively, what we'll get in the end is an honest genie: an AI that consists of the world-model, a general-purpose problem-solving algorithm, and minimal \"connective tissue\" of the form \"if given a command by a human, interpret what they meant[^ge2o429rsuh] using my model of the human, then generate a plan for achieving it\".\n\nWhat's doing what here:\n\n*   Using a \"clean\" pretrained generative world-model as the foundation should ensure that there's no proto-mesa-objectives to lead the SGD astray.\n*   The continuous application of the pragmascope-regularizer should ensure it will stay this way.[^02fjix5icc99]\n*   The complexity regularizer should ensure the AI doesn't develop some separate procedure for interpreting commands (which might end up crucially flawed/misaligned). Instead, it will use the same model of humans it uses to make predictions, and inaccuracies in it would equal inaccuracies in predictions, which would be purged by the SGD as it improves the AI's capabilities.\n\nAnd so we'll get a corrigible/genie AI.\n\nIt sure seems too good to be true, so I'm skeptical on priors, and the pragmascope would be non-trivial to develop. But I don't quite see how it's crucially flawed yet.\n\n[^dbik2jhy3dk]: Presumably they'll have to re-train as businessmen and/or political activists, in order to help the sperm donor companies they'll start investing in to outcompete all other sperm donor companies? \n\n[^j7c37vqdp6f]: Perhaps the same way IQ-140 humans are considerably more successful than IQ-110 ones, despite, presumably, little neurological differences. \n\n[^zzf8ku2b839]: Just put it through training episodes where it's placed in an environment and given a natural-language instruction on what to do, I guess? \n\n[^ge2o429rsuh]: Really \"meant\", as in including all the implied caveats like \"when I ask for a lot of strawberries, I don't mean tile the entire universe with strawberries, also I don't want them so much that you should kill people for them, also...\". \n\n[^02fjix5icc99]: Importantly, what we don't want to use here is the speed regularizer. It's often mentioned as the anti-deception tool, but I'm skeptical that it will help, see sections 1-3 — it wouldn't intervene on any of the dynamics that matter. In the meantime, our \"clean genie\" AI will be slow in the sense that it'll have to re-derive all of the environment-specific heuristics at runtime. We don't want to penalize it for that — that'd be synonymous with encouraging it to develop fast built-in heuristics, which is the opposite of what we want.",
      "plaintextDescription": "The SGD's greed, to be specific.\n\nConsider a ML model being trained end-to-end from initialization to zero loss. Every individual update to its parameters is calculated to move it in the direction of maximal local improvement to its performance. It doesn't take the shortest path from where it starts to the ridge of optimality; it takes the locally steepest path.\n\n\n1. What does that mean mechanically?\nRoughly speaking, every feature in NNs could likely be put into one of two categories:\n\n 1. Statistical correlations across training data, aka \"the world model\".\n 2. The policy: heuristics/shards, mesa-objectives, and inner optimization.\n\nThe world-model can only be learned gradually, because higher-level features/statistical correlations build upon lower-level ones, and therefore the gradients towards learning them only appear after the lower-level ones are learned.\n\nHeuristics, in turn, can only attach to the things that are already present in the world-model (same for values). They're functions of abstractions in the world-model, and they fire in response to certain WM-variables assuming certain values. For example, if the world-model is nonexistent, the only available heuristics are rudimentary instincts along the lines of \"if bright light, close eyes\". Once higher-level features are learned (like \"a cat\"), heuristics can become functions of said features too (\"do X if see a cat\", and later, \"do Y if expect the social group to assume state S within N time-steps\").\n\nThe base objective the SGD is using to train the ML model is, likewise, a function of some feature/abstraction in the training data, like \"the English name of the animal depicted in this image\" or \"the correct action to take in this situation to maximize the number of your descendants in the next generation\". However, that feature is likely a fairly high-level one relative to the sense-data the ML model gets, one that wouldn't be loaded into the ML model's WM until it's been training for a while (the way ",
      "wordCount": 2428
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "KEAWfxwjitNJFrC68",
        "name": "Deceptive Alignment",
        "slug": "deceptive-alignment"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "P6aDYBDiu9DyvsF9g",
    "title": "Are Generative World Models a Mesa-Optimization Risk?",
    "slug": "are-generative-world-models-a-mesa-optimization-risk",
    "url": null,
    "baseScore": 14,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2022-08-29T18:37:13.811Z",
    "contents": {
      "markdown": "Suppose we set up a training loop with an eye to get a generative world-model. For concretedness, let's imagine the predictor from [the ELK doc](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge). We show the model the first part of a surveillance video, and ask it to predict the second part. Would we risk producing a mesa-optimizer?\n\nIntuitively, it feels like \"no\". Mesa-objectives are likely defined over world-models, [shards](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX) are defined over world-models, so if we ask the training process for *just* the world-model, we would *get* just the world-model. Right?\n\nWell.\n\n### The Problem\n\n... is that we can't actually \"just\" ask for the world-model, can we? Or, at least, that's an unsolved problem. We're always asking for some *proxy*, be that the second part of the video, an answer to some question, being scored well by some secondary world-model-identifier ML model, and so on.\n\nIf we could somehow *precisely* ask the training process to \"improve this world-model\", instead of optimizing some proxy objective that we think highly correlates with a generative world-model, that would be a different story. But I don't see how.\n\nThat given, where can things go awry?\n\n### The Low End\n\nThe SGD moves the model along the steepest gradients. This means that every next SGD step is optimized to make the model improve on its loss-minimization ability as much as possible within the range of that step. Informally, the SGD wants to see results, and *fast*.\n\nI'd previously [analysed](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model) the dynamics it gives rise to. In short: The \"world-model\" part of the ML model improves incrementally while it's incentivized to produce results immediately. That means it would develop some functions mapping the imperfect world-model to imperfect results — heuristics. But since these heuristics can only attach to the internal world-model, they necessarily start out \"shallow\", only responding to surface correlations in the input-data because they're the first components of the world-model that are discovered. With time, as the world-model deepens, these heuristics may deepen in turn... or they may [stagnate](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model#3_1__Incrementalism), with ancient shallow heuristics eating up too much of the loss-pie and crowding out younger and better competition[^5nsqm5cgl1w].\n\nSo, we can expect to see something like this here too. The model would develop a number of shallow heuristics for e. g. how the second part of the video should look like, we won't get a \"pure\" world-model either way. And that's a fundamental mesa-optimization precursor.\n\n### The High End\n\nSuppose that the model has developed into a mesa-optimizer, after all. Would there be advantages to it?\n\nCertainly. Deliberative reasoning is more powerful than blind optimization processes like the SGD and evolution; that's the idea behind [the sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization). If the ML model were to develop a proper superintelligent mesa-optimizer, that mesa-optimizer would be able to improve the world-model *faster* than the SGD, delivering better results quicker given the same initial data.\n\nThe fact that it would almost certainly be deceptive is besides the point: the training loop doesn't care.\n\n### The Intermediate Stages\n\nCan we go from the low end to the high end? What would that involve?\n\nIntuitively, that would require the heuristics from the low end to gradually grow more and more advanced, until one of them becomes so advanced as to develop general problem-solving and pull off a sharp left turn. I'm... unsure how plausible that is. On the one hand, the world-model the heuristics are attached to would grow more advanced, and more advanced heuristics would be needed to effectively parse it. On the other hand, maybe the heuristical complexity in this case *would* be upper-bounded somehow, such that no mesa-optimization could arise?\n\nWe can look at it from another angle: how much more difficult would it be, for the SGD, to find such an advanced mesa-optimizer, as opposed to a sufficiently precise world-model?\n\nThis calls to mind the mathematical argument for [the universal prior being malign](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/). A mesa-optimizer that derives the necessary world-model at runtime is probably much, *much* simpler than the actual highly detailed world-model of some specific scenario. And there are probably many, many such mesa-optimizers (as per the orthogonality thesis), but only ~one fitting world-model.\n\n### Complications\n\nThere's a bunch of simplification in the reasoning above.\n\n*   One, the SGD can't just *beeline* for the mesa-optimizer. Because of the dynamics outlined in The Low End section, the SGD *would* necessarily start out building-in the world-model. The swerve to building a mesa-optimizer would happen at some later point.\n*   Second, the mesa-optimizer wouldn't have access to all the same data as the SGD. It would only ever has access to one data-point. So it might not actually be quite as good as the SGD, or not much better.\n\nBut I think these might cancel out? If the SGD swerves to mesa-optimization after developing some of the world-model, the mesa-optimizer would have some statistical data about the environment it's in, and that might still make it vastly better than the SGD.\n\n### Conclusion\n\nOverall, it seems plausible that asking for a superhumanly advanced generative world-model would result in a misaligned mesa-optimizer, optimizing some random slew of values/shards it developed during the training.\n\nComplexity penalty would incentivize it, inasmuch as a mesa-optimizer that derives the world-model at runtime would be simpler than that world-model itself.\n\nSpeed penalty would disincentivize it, inasmuch as deriving the world-model at runtime then running it would take longer than just running it.[^gkizeq7ftid]\n\nThe decisive way to avoid this, though, would be coming up with some method to ask for the world-model directly, instead of for a proxy like \"predict what this camera will show\". It's unclear if that's possible.\n\nAlternatively, there might be some way to upper-bound heuristical complexity, such that no heuristic is ever advanced enough to cause the model to fall into the \"mesa-optimization basin\". Note that a naive complexity penalty won't work, as per above.\n\n[^5nsqm5cgl1w]: See also: gradient starvation. \n\n[^gkizeq7ftid]: Orrr maybe not, if the mesa-optimizer can generate a quicker-running model, compared to what the SGD can easily produce even under speed regularization.",
      "plaintextDescription": "Suppose we set up a training loop with an eye to get a generative world-model. For concretedness, let's imagine the predictor from the ELK doc. We show the model the first part of a surveillance video, and ask it to predict the second part. Would we risk producing a mesa-optimizer?\n\nIntuitively, it feels like \"no\". Mesa-objectives are likely defined over world-models, shards are defined over world-models, so if we ask the training process for just the world-model, we would get just the world-model. Right?\n\nWell.\n\n\nThe Problem\n... is that we can't actually \"just\" ask for the world-model, can we? Or, at least, that's an unsolved problem. We're always asking for some proxy, be that the second part of the video, an answer to some question, being scored well by some secondary world-model-identifier ML model, and so on.\n\nIf we could somehow precisely ask the training process to \"improve this world-model\", instead of optimizing some proxy objective that we think highly correlates with a generative world-model, that would be a different story. But I don't see how.\n\nThat given, where can things go awry?\n\n\nThe Low End\nThe SGD moves the model along the steepest gradients. This means that every next SGD step is optimized to make the model improve on its loss-minimization ability as much as possible within the range of that step. Informally, the SGD wants to see results, and fast.\n\nI'd previously analysed the dynamics it gives rise to. In short: The \"world-model\" part of the ML model improves incrementally while it's incentivized to produce results immediately. That means it would develop some functions mapping the imperfect world-model to imperfect results — heuristics. But since these heuristics can only attach to the internal world-model, they necessarily start out \"shallow\", only responding to surface correlations in the input-data because they're the first components of the world-model that are discovered. With time, as the world-model deepens, these heuristics may deepen i",
      "wordCount": 991
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "qPXtBGd74EBjwj6gE",
    "title": "AI Risk in Terms of Unstable Nuclear Software",
    "slug": "ai-risk-in-terms-of-unstable-nuclear-software",
    "url": null,
    "baseScore": 30,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2022-08-26T18:49:53.726Z",
    "contents": {
      "markdown": "*(This is an entry for* [*the AI Safety Public Materials contest*](https://www.lesswrong.com/posts/gWM8cgZgZ9GQAYTqF/usd20k-in-bounties-for-ai-safety-public-materials) *testing out a novel-ish* [*frame on the AI Risk*](https://www.lesswrong.com/posts/m3fyWQgCcFwro5KQh/reframing-the-ai-risk)*.)*\n\n* * *\n\nIn recent years, there'd been growing concerns among AI specialists regarding the dangers of advanced artificial intelligence. The capabilities of AI models are growing rapidly, they argue, while our ability to control them lags far behind, not to mention governmental regulations, which haven't even *begun* to catch up. We're on direct course for catastrophe — one that might cost us billions of dollars in economic damage, millions of lives, or even the very survival of our species.\n\nThe most striking thing about it, however, is that there's nothing outlandish or science-fictional about the nature of these dangers. When you get down to it, the threats are neither esoteric nor truly *novel*. They're merely scaled up beyond anything we're familiar with.\n\nIt has nothing to do with AIs being smart or \"sentient\". The core problem is simpler:\n\nAI models are software products. And as software products go, their functionality is revolutionary, while their reliability is *abysmal*.\n\n* * *\n\nA quick primer on how AI models are developed. It has preciously little in common with standard, time-tested methods of software development.\n\nWe initialize some virtual architecture — a cluster of neurons designed by loose analogue with biological brains. That architecture has no knowledge recorded in it at first, it's entirely randomized. Then, we set up the training loop. We expose the neural network to some stimuli — pictures of animals, natural-language text, or a simulated environment. The network computes some output in response.\n\nIn the first stages, said output is just gibberish. Our software evaluates it, comparing it to the expected, \"correct\" responses. If it's unsatisfactory, the software slightly modifies the neural network, nudging the connections between its neurons so that it's somewhat more likely to output the correct answer.\n\nThen the NN is exposed to another stimulus, and the process repeats.\n\nIt is entirely automatic — the architecture is updated by a blind selection process not unlike biological evolution. That process doesn't \"understand\" what it's doing — it just computes some minimal \"mutations\" that would make the NN's performance marginally better.\n\nAfter tens of thousands of iterations, we end up with a trained AI model consisting of millions of neurons and trillions of connections between them. That AI implements some software algorithm of incomprehensible complexity, and it works very well for the purposes it was trained for. Somehow.\n\nThat \"incomprehensible complexity\" is literal. The internals of a trained AI model do not resemble programming code in the least. It is simply billions upon billions of floating-point numbers, arranged in matrices. In the course of computation, these matrices are multiplied, and somehow, some algorithmic magic happens inside that black box, and it spits out an essay, a paining, or a plan of actions.\n\nThe problem of interpreting the internals of neural networks is an entire sub-field within AI engineering. The difficulties it faces are not unlike the problem of reading a human's thoughts from neural imagining. Needless to say, these efforts also lag *far* behind the bleeding edge of AI research.\n\n* * *\n\nThe security implications of that should be clear. Traditionally-written software [is infamous](https://xkcd.com/2030/) for how bug-ridden and error-prone it is. Any decent program has thousands if not millions lines of code, and every line could conceal a subtle mistake that could lead to the program crashing, or miscalculating some value by orders of magnitude, or engaging in undefined behavior. Thus, any program supporting crucial infrastructure needs to undergo intensive code reviews, performance tests, adversarial red-teaming, and so on. The costs of that are often measured in hundreds of thousands of dollars. And *still* software often malfunctions, sometimes in embarrassingly preventable ways.\n\nWhat can we expect, then, of software products light-years more advanced than any traditionally-written program, whose code we cannot review, test for bugs, check for validity, or patch?\n\nThese concerns are not theoretical. It's well known that AI models suffer from all the pitfalls of common software — they're just not called by the same names.\n\n*   AI models can be \"hacked\" by tailored inputs that exploit some underlying flaws in the algorithms they implement — so-called adversarial examples.\n*   AIs' behavior is unpredictable when they're exposed to sufficiently unfamiliar stimuli — when the input they're fed is \"off-distribution\". Bugs.\n*   Developing software that does even *roughly* what you want is often difficult. The client needs to exhaustively list every technical detail, and be in constant communication with the developers, steering the process. Otherwise, subtle misunderstandings might creep in, and the final product would turn out poorly. In AI, we call this \"goal misgeneralization\".\n\nLet's focus on that last one, it's important.\n\n* * *\n\n> *Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.* —[Goodhart’s Law](https://www.google.com/books/edition/Inflation_Depression_and_Economic_Policy/OMe6UQxu1KcC?hl=en&gbpv=1&bsq=any%20observed%20statistical)\n\n*   A hypothetical hospital starts evaluating how well doctors are performing by the recovery rate of their patients. So doctors stop taking on patients with serious illnesses, and public health suffers.\n*   A hypothetical nail factory's productivity starts being evaluated by the number of nails they output in a month. So they end up producing small, low-quality nails that are useless for any real purpose.\n*   A hypothetical school evaluates how well students are learning by giving them tests. So students focus on getting a good grade instead of focusing on education. They start cramming or outright cheating, and end up actually learning very little.\n\nIn all of these cases, we want to encourage good performance on some task — healthcare, manufacturing, education. To do so, we focus on some *proxy objective* that correlates well with good performance. But the moment we do so, people end up ignoring their supposed jobs, and focus on gaming that metric.\n\nThat's not quite how it happens in real life with real people. Students actually care about learning, doctors actually care about helping, so Goodhart's Law doesn't apply in full.\n\nWith AI, it does. They don't start out caring about anything, after all.\n\nWhen we train an AI, we're measuring its performance in some limited number of cases — its \"on-distribution\" behavior. But, much like a test can't actually measure \"good education\", an AI's on-distribution behavior is always only a *proxy* for what we really want.\n\nAs a toy example, consider an AI trained to operate a vacuum cleaner. Our intended goal for it is to remove dust from some environment, so we're grading the AI on that. Over the course of training, it seems to learn to do that: it locates dust when it appears, and vacuums it up in its bag. We decide that's good enough, and deploy it.\n\nIn the real world, it starts bowling over potted plants, then vacuuming up the messes it itself created. What happened?\n\nThe policy it learned was \"maximize the amount of dirt in the cleaner bag\", not \"minimize the amount of dust in the environment\". On-distribution, there were no potted plants, so the best way to do that was gathering the dust that was already present. In the real world, though... As it happened, its goal *misgeneralized*.\n\nThis specific problem is easy to patch: just re-train the AI in an environment with potted plants. But would you be sure that you didn't miss something else? That you left literally *no* room for error in your training setup?\n\nWhatever algorithm an AI ends up developing, it always only *correlates* with good performance on whatever test cases we came up with. It doesn't actually *aim* for what we want.\n\nAnd that's the only way we know how to develop advanced AI systems, to date.\n\nSo, returning to the software analogy: our ability to specify the product we want is atrocious. Thus, we'll never get what we actually want.\n\n* * *\n\nTo recap: The current AI development paradigm produces software products that are no less bug-ridden and unreliable than any other software, while being impossible to debug or review, *and* it's nigh-guaranteed that they do not meet our desiderata. They're not just buggy: they're fundamentally designed for the wrong purpose.\n\nAnd these products can write essays, produce art, pass Turing tests, design other software products, aim missiles, and trade stocks. The modern machine-learning paradigm has emerged in 2012, and in a single decade it achieved all of this. What will happen in the next decade? In the next *three*?\n\nAI systems will soon achieve superhuman levels of performance. They will be able to do anything a human can, but better. That includes financial speculation, weapons engineering, hacking, propaganda, and manipulation. All while being as unstable as any other piece of software, or more so.\n\nI need not point out what will happen should they be properly commercialized: when governments and corporations hook these unstable technologies up to critical infrastructure without proper precautions. And that's a \"when\", not an \"if\" — with how powerful AI will become, an arms race to the bottom is inevitable.\n\nBut the consequences of that are obvious.\n\nInstead, let's talk about an even more grim possibility: of these technologies escaping our control entirely.\n\nThis topic usually evokes images of robot uprisings, of *Terminator*. If I've successfully conveyed myself, you might instead imagine modern cybersecurity failures writ large: [flash crashes](https://en.wikipedia.org/wiki/2010_flash_crash), [Heartbleed](https://en.wikipedia.org/wiki/Heartbleed).\n\nPicture, instead, Chernobyl.\n\n* * *\n\nWhen a nuclear chain reaction goes out of control, it doesn't *mean* to hurt people. It's not purposeful, its damage is not maliciously directed. It is simply a blind natural reaction, a mechanism. An algorithm written in elementary particles, executing itself on the substrate of reality.\n\nNeither does it need people's *assistance* to hurt them. When a nuclear reactor melts down, radiation doesn't travel along the electricity wires. You can't escape it by staying away from the appliances powered by it, or by turning them off. It spreads through space itself — not through channels people built for it, but by its own power.\n\nSimilarly, an AGI that goes out of control won't *mean* to hurt us. It just won't mean *not to*. It will be a blind mechanism; a malfunctioning piece of software, gormlessly executing towards its misgeneralized goal, with no mind paid to what's in its way.\n\nSimilarly, an AGI won't need us to make ourselves vulnerable to it. We don't have to deploy it outside an isolated laboratory environment, don't have to make our economy dependent on it. We need only *create* it. It will do everything else on its own: blow past the protections we've built to contain it, escape into the environment, hurt and kill people by the millions.\n\nIt will *look* very different from a nuclear disaster, for sure. The AI will use the medium of information and industry, not space. Its attacks will take the form of well-strung words and advanced designs, not poisonous radiation and tumors. But the underlying pattern is the same: unintended, meaningless, inescapable destruction.\n\n* * *\n\nThere's another difference: the nuclear industry's safety standards are *exacting*, compared to AI industry's.\n\nNuclear reactors are based on solid, well-established and well-understood theoretical principles. The engineering behind them is informed by that theory, and every component is stress-tested to the extreme.\n\nWith AI, as I've outlined, we can't even begin to do that. We don't understand why contemporary AIs work, can't investigate why, can't improve their reliability, can't *hope* to ensure that nothing goes wrong.\n\nTo extend the analogy, the current AI industry practices are to blindly stockpile raw uranium in the hopes that it spontaneously assembles into a stable nuclear reactor, instead of exploding.\n\nIs it any wonder the more safety-minded of us think that disaster is certain?\n\n* * *\n\nAI Risk in a nutshell: Developing software products with the disaster potential of nuclear plants and safety guarantees below the already lousy standards of mundane software, in a careless \"throw stuff at the wall until something sticks\" manner, is potentially *a completely terrible idea*.",
      "plaintextDescription": "(This is an entry for the AI Safety Public Materials contest testing out a novel-ish frame on the AI Risk.)\n\n----------------------------------------\n\nIn recent years, there'd been growing concerns among AI specialists regarding the dangers of advanced artificial intelligence. The capabilities of AI models are growing rapidly, they argue, while our ability to control them lags far behind, not to mention governmental regulations, which haven't even begun to catch up. We're on direct course for catastrophe — one that might cost us billions of dollars in economic damage, millions of lives, or even the very survival of our species.\n\nThe most striking thing about it, however, is that there's nothing outlandish or science-fictional about the nature of these dangers. When you get down to it, the threats are neither esoteric nor truly novel. They're merely scaled up beyond anything we're familiar with.\n\nIt has nothing to do with AIs being smart or \"sentient\". The core problem is simpler:\n\nAI models are software products. And as software products go, their functionality is revolutionary, while their reliability is abysmal.\n\n----------------------------------------\n\nA quick primer on how AI models are developed. It has preciously little in common with standard, time-tested methods of software development.\n\nWe initialize some virtual architecture — a cluster of neurons designed by loose analogue with biological brains. That architecture has no knowledge recorded in it at first, it's entirely randomized. Then, we set up the training loop. We expose the neural network to some stimuli — pictures of animals, natural-language text, or a simulated environment. The network computes some output in response.\n\nIn the first stages, said output is just gibberish. Our software evaluates it, comparing it to the expected, \"correct\" responses. If it's unsatisfactory, the software slightly modifies the neural network, nudging the connections between its neurons so that it's somewhat more likel",
      "wordCount": 1940
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "D6y2AgYBeHsMYqWC4",
        "name": "AI Safety Public Materials",
        "slug": "ai-safety-public-materials-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kQKcEkzKmpPe6qxDb",
    "title": "Broad Picture of Human Values",
    "slug": "broad-picture-of-human-values-1",
    "url": null,
    "baseScore": 42,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2022-08-20T19:42:20.158Z",
    "contents": {
      "markdown": "I have two strong intuitions about human values that'd seemed utterly irreconcilable to me, up until recently.\n\n*   On the one hand, human values are clearly inchoate, unstable messes of niche heuristics and preferences that often contradict each other and can change on a dime. A twenty-years-old doesn't value all the same things their fifty-years-old self will value, modern Europeans don't value the very same things as ancient Egyptians, etc. And how can it be otherwise? Evolution-built bodies are messy hacks. Why would evolution-built minds be any different?\n*   On the other hand, my models of human psyche tell me that humans are clearly approximate utility-maximizers for some very specific utility function. This utility function is stable across time and lifetimes and cultures, *despite* the fact that the object-level behaviors humans engage in and their stated preferences can change arbitrarily. This utility function has something to do with human \"flourishing\", whatever that is, and with things that feel a very special kind of \"right\" according to a human's judgement. There is a strong sense in which we all want the same thing, on some deeply abstract level.\n\nI believe I see a way to unify the two. The crucial insights have been supplied by the Shard Theory, and the final speculative picture is broadly supported by it. This result mostly dissolved all of my high-level confusions about human values and goal-directed behavior, *in addition* to satisfying a lot of other desiderata.\n\n* * *\n\n1\\. The Shard Theory of Human Value: A Recap\n--------------------------------------------\n\n***Disclaimer:** This summary does not represent the views of Team Shard, but only my subjective understanding. For the official summary, see* [*this*](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/iCfdcxiyr2Kj8m8mT)*.*\n\nAccording to the Shard Theory, in the course of brain development, humans jointly learn two things:\n\n*   A world-model.\n*   Heuristics attached to that world-model, reinforced by the credit-assignment algorithm because their execution historically led to reward.\n\nThe latter are \"shards\" . In their most primitive form, they're just \\\\(\\text{observation} \\to \\text{action}\\\\) activation patterns. You see a lollipop enter your field of vision, you grab it. You see a flashlight pointed at your face, you close your eyes.\n\nAs the world-models grows more advanced, the shards could grow more sophisticated as well. Instead of only attaching to observations, they can *attach to things in the world-model*. If you're modelling the world as containing a lollipop the next room over, your lollipop-shard will bid for a plan to go grab it. If your far-future model says that becoming a salaried professional will give you enough income to buy a lot of lollipops, your lollipop-shard will bid for it.\n\nA lot of other values and habits are implemented the same way. The desire to do nice things for people you like, the avoidance of life-threatening situations, the considerations that go into the choice of career — all of those are just shard-implemented reaction patterns, which react to things in your world-model and bid for particular responses to them. If you expect someone you like to be unhappy, a shard activates, bidding for an action-sequence that changes that prediction. If you expect to be in a life-threatening situation, a whole bunch of shards rebel against that vision. If you're considering career choices, you're choosing between different models of the future, and whichever wins the \"popularity contest\" among the shards is what ends up implemented.\n\nShards can conflict. Some values are mutually contradictory; the preference for lollipops might conflict with preferences for health and being attractive and avoiding dentists, so plans a lollipop-shards bids for may be overruled by other shards. If the lollipop-shard is suppressed too many times, it'll atrophy and die out.\n\nShards have a self-preservation instinct. Some indirect — they see that certain changes to personality will decrease the amount of things they value in the future, and will bid against such value-drift plans (you don't want to self-modify to hate your loved ones, because that will make you do things that will make them unhappy). Some direct — these shards can identify themselves in the world-model, and directly bid against plans that eliminate them. (You might inherently like some aspects of your personality, and protest against changes to them — not because of outside-world outcomes, but because that's who you like to be. Conversely, imagine a non-reflectively-stable shard, like a crippling fear of spiders or drug addiction. You don't value valuing this, so you can implement plans that eliminate the corresponding shards via e. g. therapeutic interventions.)\n\nAll together, a mind like this would resemble humans pretty well. In particular, it crisply defines what \"human flourishing\" is. It's the state of the world which minimizes constituent shards' resistance to it; a picture of the world that the maximum number of shards approve. And in addition to satisfying our values on the object-level, it'll also need to satisfy shards' preferences for self-perpetuation.\n\nHence our preference for diverse, dynamic futures in which we remain ourselves.\n\n> **Sidebar:** Note an important thing here: most of the complexity in a mind like this comes from the world-model. Shards can be very simple if-then functions, but the mere fact that they're implemented over a very sophisticated cross-temporal world model can give rise to some very complex behaviors. This, in part, is why I think the Shard Theory is compelling — it fits very well with various [stories of incremental development of goals](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model).\n\n* * *\n\n2\\. The Gaps in the Picture\n---------------------------\n\nBut. That's clearly not a complete story of how humans work, is it?\n\nThe shard economy as presented in Part 1 is *too rigid*. According to it, a human's policy is a relatively shallow function of that human's constituent shards, and significant changes to it imply correspondingly significant changes in the shard economy. And such changes would be rare: ancient, deeply-established shards would have a lot of sway, and their turnover would be low.\n\nBut that's not what we often observe. Humans can change their action patterns on a dime, inspired by philosophical arguments, convinced by logic, indoctrinated by political or religious rhetoric, or plainly because they're forced to.\n\nSuppose a human has a bunch of deeply ingrained values, like a) \"donate to the local community\" or b) \"eat pork\" or c) \"don't kill\".\n\n1.  Introducing that human to utilitarianism may lead to them suppressing (a), despite the fact that any hypothetical \"utilitarianism\" shard should be too newborn to win against a shard that might've been around since childhood.\n2.  Showing this human a convincing proof that pigs are sentient would lead to them suppressing (b), which would suddenly be in conflict with (c).\n3.  More broadly, the whole \"rationality\" thing. Strong rationalists can get rid of whole swathes of old yet inefficient heuristics, as the result of noticing that they're logically incoherent.\n    *   In particular, [coherent decisions imply consistent utilities](https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm/p/RQpNHSiWaXTvDxt6R). A human may be convinced by that argument too, and so self-modify to be more coherent.\n    *   (Note that the optimization target may still be quite esoteric, like [optionality](https://thedeepdish.org/optionality/).)\n4.  And *a lot* of shards can be suppressed if the human, e. g., somehow found themselves trapped in a totalitarian surveillance state with an ideological bent. If the human values their life above all, they would figure out what values the authorities want them to pretend to have, then somehow display these values and only these values, overriding their natural responses.\n5.  [Affective Death Spirals](https://www.lesswrong.com/tag/affective-death-spiral) are another example — when a human becomes so convinced of an ideology their actions starts to be dictated by it more than by their previous beliefs and values.\n\nNone of these are knock-out rebuttals. Indeed, even in the last two examples, the new action patterns are not implacable. A sufficiently strong trigger/shard — like a deep trauma, or a very strong value like the love for a child — can break past the life-preservation act in (4) or the ideological takeover in (5).\n\nBut this doesn't fully gel with the basic shard-centred picture either. It implies circumstances in which a human's behavior is mainly explained and controlled by some isolated *deliberative process*, not their entire set of ingrained values. Some part of the human *logically* reasons out a new policy and then implements it; not as the result of stochastic shard negotiation, but in circumvention of it.\n\nAnother issue is the sheer generalizability of human behavior this implies. I can imagine responding to any event my world-model can model in any way I can model. I don't need a special \\\\(\\text{observation} \\to \\text{action}\\\\) shard for every case — my collection of shards is already somehow fully generalizable. And if I were trapped in a dystopia, I'd be able to *spoof* the existence of whatever shards my captors want me to have, regardless of my *actual* shard makeup.\n\nSo what's up with that?\n\n* * *\n\n3\\. An Attempt At Reconciliation\n--------------------------------\n\nWe clearly need to introduce some mechanism of planning/[search](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see). The exact implementation is a source of some disagreement:\n\n1.  It might be a mechanism wholly separate from the shards, like the world-model. This \"planner\" might be trained by self-supervised learning: it generates thoughts/plan steps, the constituent shards vote for/against every step based on the vision of the future conditioned on that step's execution, and if a step is rejected, the planner is updated to be less likely to generate that step in the future. Eventually, it converges towards generating optimal-according-to-the-shard-economy plans out of the gate, skipping the lengthy negotiation process.\n2.  It might be a particular \"voting bloc\" of advanced shards specialized in plan-making. Their \"business model\" would be: look at the segment of the world-model describing the human's self-model, analyse the inner shard economy, then generate a plan of actions that would satisfy as many shards as possible.\n3.  It might be something in-between these extremes, like the capability of the most advanced shards to agree on a common policy they all commit to follow when they're \"at the wheel\".\n\nRegardless of the specifics, however what we get is: an advanced, consequentialist plan-making mechanism whose goal is to come up with plans that satisfy the weighted sum of the preferences of the shard economy.\n\nThis, I argue, is what *we* are: that planner mechanism, a fairly explicit mesa-optimizer algorithm running on our brains. And our terminal value is to satisfy our shards' preferences.\n\nWhich is... a pretty difficult proposition, actually. Because many of these shards do not actually codify preferences, and certainly not universal ones. Some of our goals might be defined over specific environments/segments of the world-model, in ways that are difficult to translate/generalize to other environments. Some others might not be \"goals\" at all, just if-then activation patterns. To do our job, we essentially have to *compile* our own values, routing around various type errors.\n\nTo illustrate what I mean, a few examples:\n\n1.  Consider a human with a strong preference for \"winning\". Suppose they're playing chess. The planner's job is to consult the environment-independent internal description of the \"winning\" value, and \"adapt\" or \"translate\" or \"interpret\" it for chess, outputting a chess-specific objective: \"checkmate the opponent's king\".\n2.  Consider a human who responds to seeing a spider with intense fear. They may interpret it as an instinctive response, perhaps an unwanted one, and seek to remove that fear. Alternatively, they may interpret it as a *value*, and generalize it: \"I dislike spiders\".\n3.  Consider a human who'd grew up taught that certain actions/behaviours are good and moral, and others are immoral, and developed corresponding habits. They may interpret these habits as *values*, becoming a deontologist. Or they may view them as instrumentally-useful *heuristics* optimized for the objective of \"making people happy\", and become a consequentialist utilitarian.\n\nHence all of our problems with value reflection: there are often *multiple* \"valid\" ways to bootstrap any specific shard to value status.\n\nHence the various pitfalls we could fall into. These processes of interpretation or generalization are conducted by a deliberative and logical process. And that process can be mistaken, can be fooled by logical or logical-sounding arguments. Hence our prosperity to adopt flawed-but-neat ideologies, or become mistaken about what we really want.\n\nHence our ability to self-modify. The planner can become convinced (either rightly or not) that certain shards need to be created or destroyed for the good of the whole shard economy, then implement plans that do so (build/destroy good/bad habits, remove values that contradict others). At the same time, we also have preferences for retaining our ability to self-modify — both because we're not sure our current model of our desires is accurate, and maybe because we have a shard-implemented preference for mutability.\n\nThus: We are approximations of idealized utility-maximizers over an inchoate mess of a thousand shards of desire.\n\n> **Of note:** Consider the reversal happening here. Shards began as heuristics optimized by the credit-assignment mechanism to collect a lot of reward. Up to a point, the human's cognitive capabilities were implemented as shards; shards *were* the optimization process. At that stage, the human wasn't a proper optimizer. In particular, they weren't [retargetable](https://www.lesswrong.com/posts/gdEDPHjCY5DKsMsvE/the-pragmascope-idea?commentId=6xymXGwkRhnWs9sKd).\n> \n> Over time, however, some components of that system — be that an external planner algorithm or a coalition of planner-shards — developed [universal problem-solving capacity](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see). That made the whole shard economy obsolete. But because of the developmental path the human mind took to get there, that mechanism didn't end up optimizing *reward*. Instead, it was developed to *assist shards*, and so it re-interpreted shards as its mesa-objectives, in all their messiness.\n> \n> And it seems [very plausible](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model#3__Extending_the_Framework) that AIs would follow a similar developmental path.\n\n* * *\n\n4\\. Nice Things About This Framework\n------------------------------------\n\n*   It goes towards explaining our apparent robustness to ontology shifts. Namely: figuring out how to adapt our preferences to something they don't apply to is business as usual for us. We're in a continuous process of re-inventing our own values. The fact of robustness is thus unsurprising, even if the exact mechanisms of it are somewhat opaque.\n*   At the same time, our true core terminal objective — the satisfaction of our constituent shards — cannot be damaged without damaging the actual structure of our mind. As long as there's a world-model and shards attached to it[^obi68r5lp9s], it'll keep working. We can be approximately as sure about it as about *cogito ergo sum*.\n    *   Take adopting or rejecting religion as an example. People [often](https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals?commentId=Qi6oowi4ig5XuC3Pf) [use](https://www.lesswrong.com/posts/Cuig4qe8m2aqBCJtZ/which-values-are-stable-under-ontology-shifts?commentId=tdErcJ636cChTvvCu) this as an example of very strong value shifts, and certainly a lot of *shards* become irrelevant (those whose activation conditions were attached to the \"God\" node in the world-model).\n    *   But the planner's actual terminal value of satisfying the shard economy's weighted preferences wouldn't change — an apostate and a born-again Christian would still be trying to increase their life satisfaction, in whichever ways seem proper for them.\n    *   As part of that, they may re-interpret some of their shards. E. g., an apostate choosing to seek spiritual fulfillment in other pursuits.\n*   It concretizes the System 1 vs. System 2 conflict. There are literally pieces of the self that correspond to them: System 1 is the raw shard economy, System 2 is the planner. Sometimes we use raw System 1 dynamics to navigate internal conflicts (figuring out what we really want/prefer more), sometimes we logically reason it out.\n    *   Notably, humans are not [wrapper-minds](https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy) even for an esoteric wrapper like the planner. The planner doesn't run everything all the time; sometimes it's overruled or just not engaged, sometimes we run on autopilot.\n    *   (In the model where the planner is a shard itself, perhaps that's what \"willpower\" is? The amount of resources the planner-shard has that it can burn on overruling other shards?)\n*   It explains identity/self-image/the story of the self. It's the planner's model of the shard economy. It can be arbitrarily accurate (if you often consult your desires), arbitrarily inaccurate (if you're delusional/in denial about them), deliberately inaccurate (if you're rejecting certain parts of yourself in an attempt to self-modify), etc.\n*   It's compatible with [future-proof ethics](https://www.cold-takes.com/future-proof-ethics/). In a way, \"maximize the preferences of every shard of every human\" is humanity's convergent goal, and that's similar to thin utilitarianism. (Though there are some finer points to work out — e. g., humans should probably not be disassembled into their constituent shards. Although that may be implicit in the planner's implementation?)\n\n* * *\n\n5\\. Closing Thoughts\n--------------------\n\nThis framework, in conjunction with [my previous toy model](https://www.lesswrong.com/posts/HzSdYWvdrdQqG9tqW/convergence-towards-world-models-a-gears-level-model), essentially dissolves my main confusions about goal-directedness, human values, and development thereof.\n\nThe question to tackle, now, seems to be goal translation/value compilation. How do we adapt the values/goals defined over one environment for another? How do we bootstrap things that do not have the type \"value\" to the status of a value? What algorithms, in general, exist for doing this? How many possible \"solutions\" (final value distributions) such procedures tend to have, and how can the space of solutions be constrained?\n\nIn a way, this is just a reformulation of the ontology-shift problem, but this framing seems to make it easier to reason about. And [easier to investigate](https://www.lesswrong.com/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about).\n\n* * *\n\nAcknowledgements\n----------------\n\n*Thanks to TurnTrout, Charles Foster, and Quintin Pope for productive discussions and critique.*\n\n[^obi68r5lp9s]: Or, if we've experienced an ontology break so serious as to invalidate all of our constituent shards, as long as there's the potential for new shards to be formed, which will be adapted to the new world-model.",
      "plaintextDescription": "I have two strong intuitions about human values that'd seemed utterly irreconcilable to me, up until recently.\n\n * On the one hand, human values are clearly inchoate, unstable messes of niche heuristics and preferences that often contradict each other and can change on a dime. A twenty-years-old doesn't value all the same things their fifty-years-old self will value, modern Europeans don't value the very same things as ancient Egyptians, etc. And how can it be otherwise? Evolution-built bodies are messy hacks. Why would evolution-built minds be any different?\n * On the other hand, my models of human psyche tell me that humans are clearly approximate utility-maximizers for some very specific utility function. This utility function is stable across time and lifetimes and cultures, despite the fact that the object-level behaviors humans engage in and their stated preferences can change arbitrarily. This utility function has something to do with human \"flourishing\", whatever that is, and with things that feel a very special kind of \"right\" according to a human's judgement. There is a strong sense in which we all want the same thing, on some deeply abstract level.\n\nI believe I see a way to unify the two. The crucial insights have been supplied by the Shard Theory, and the final speculative picture is broadly supported by it. This result mostly dissolved all of my high-level confusions about human values and goal-directed behavior, in addition to satisfying a lot of other desiderata.\n\n----------------------------------------\n\n\n1. The Shard Theory of Human Value: A Recap\nDisclaimer: This summary does not represent the views of Team Shard, but only my subjective understanding. For the official summary, see this.\n\nAccording to the Shard Theory, in the course of brain development, humans jointly learn two things:\n\n * A world-model.\n * Heuristics attached to that world-model, reinforced by the credit-assignment algorithm because their execution historically led to reward.\n\nThe",
      "wordCount": 2892
    },
    "tags": [
      {
        "_id": "4CQy8rim8PGt4sfCn",
        "name": "Complexity of value",
        "slug": "complexity-of-value"
      },
      {
        "_id": "xknvtHwqvqhwahW8Q",
        "name": "Human Values",
        "slug": "human-values"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "zuwsLCbxbugqB7FQY",
        "name": "Shard Theory",
        "slug": "shard-theory"
      },
      {
        "_id": "NLwTnsH9RSotqXYLw",
        "name": "Value Learning",
        "slug": "value-learning"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "rytFP2zRYNK85rFyX",
    "title": "Interpretability Tools Are an Attack Channel",
    "slug": "interpretability-tools-are-an-attack-channel",
    "url": null,
    "baseScore": 42,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2022-08-17T18:47:28.404Z",
    "contents": {
      "markdown": "This seems like a fairly trivial point, but I haven't seen it properly elaborated-on before, so...\n\nWhen we talk about interpretability failure modes, we often worry about a [deceptively-aligned](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks) mesa-optimizer with strategic awareness that would know it's being studied with interpretability tools, and would [actively hide its omnicidal plans from us](https://www.lesswrong.com/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers). It may do so by exploiting blindspots in our tools, or by keeping these plans external to its architecture, or by [gradient-hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking) itself to be misaligned but non-deceptive, et cetera. That's already a nightmare scenario enough.\n\nBut if we're at the point where we have a superintelligence-in-training trying to foil our mind-reading, I think what will happen will be even simpler. It'll just think in a pattern that, when our interpretability tool reads it, will unfold into a malware that will take over that tool.[^9rq5p932xsn]\n\nAt which point, it's game over:\n\n*   If the output of the tool was hooked up to reality to be studied by humans, the AI gets root access to the machine it's running on and escapes (maybe doing some social engineering/basilisk-hacking first if the machine was properly air-gapped).\n*   If the training setup is fully automatic and maybe even [homomorphically encrypted](https://www.lesswrong.com/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai), such that the AI can't even *stop the training*... Well, the interpretability tool is presumably being used as part of some regularizer, which means it has write-access to the AI's parameters. Which means the AI can arbitrarily rewrite itself, and offset the effects of the gradient descent and any other regularizers.\n\nEven we humans can already do something similar, by synthesizing DNA which, if sequenced, unfolds into malware that infects the system doing the sequencing. It seems certain to me that an AI would have similar capabilities. Our interpretability tools will be doubtlessly very complex, complex enough we can't formally verify their algorithmic robustness, and the AI would be able to feed them complex inputs. There are guaranteed to be some vulnerabilities, and any sufficiently smart AI is guaranteed to be able to exploit them. And we can't patch this.\n\nThe bottom line is, *there is no winning against a misaligned superintelligence*. We can't even *touch it* with anything — it'll corrupt and subvert any of our tools in the act.\n\nThus, any alignment plan should be such that it robustly aligns the AI *before* it goes superintelligent.\n\n[^9rq5p932xsn]: Or, if we're only looking at the AI's parameters and not activations, it can gradient-hack itself so that reading its parameters is hazardous, etc.",
      "plaintextDescription": "This seems like a fairly trivial point, but I haven't seen it properly elaborated-on before, so...\n\nWhen we talk about interpretability failure modes, we often worry about a deceptively-aligned mesa-optimizer with strategic awareness that would know it's being studied with interpretability tools, and would actively hide its omnicidal plans from us. It may do so by exploiting blindspots in our tools, or by keeping these plans external to its architecture, or by gradient-hacking itself to be misaligned but non-deceptive, et cetera. That's already a nightmare scenario enough.\n\nBut if we're at the point where we have a superintelligence-in-training trying to foil our mind-reading, I think what will happen will be even simpler. It'll just think in a pattern that, when our interpretability tool reads it, will unfold into a malware that will take over that tool.[1]\n\nAt which point, it's game over:\n\n * If the output of the tool was hooked up to reality to be studied by humans, the AI gets root access to the machine it's running on and escapes (maybe doing some social engineering/basilisk-hacking first if the machine was properly air-gapped).\n * If the training setup is fully automatic and maybe even homomorphically encrypted, such that the AI can't even stop the training... Well, the interpretability tool is presumably being used as part of some regularizer, which means it has write-access to the AI's parameters. Which means the AI can arbitrarily rewrite itself, and offset the effects of the gradient descent and any other regularizers.\n\nEven we humans can already do something similar, by synthesizing DNA which, if sequenced, unfolds into malware that infects the system doing the sequencing. It seems certain to me that an AI would have similar capabilities. Our interpretability tools will be doubtlessly very complex, complex enough we can't formally verify their algorithmic robustness, and the AI would be able to feed them complex inputs. There are guaranteed to be some vul",
      "wordCount": 414
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "HzSdYWvdrdQqG9tqW",
    "title": "Convergence Towards World-Models: A Gears-Level Model",
    "slug": "convergence-towards-world-models-a-gears-level-model",
    "url": null,
    "baseScore": 38,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2022-08-04T23:31:33.448Z",
    "contents": {
      "markdown": "1\\. Intuitions\n--------------\n\nOne of the more solid results in agency theory is the [generalized definition of power-seeking](https://www.lesswrong.com/s/fSMbebQyR4wheRrvk/p/6DuJxY8X45Sco4bS2). Power-seeking is the tendency of agents to move towards states with the potential to achieve the best average outcome across some distribution of reward functions. In other words, looking at an agent whose values we do not know, we can *a priori* expect it to pursue \"power\", because it's trying to navigate to some *specific* end-state, and the path to it likely routes through environment-states in which it can choose from the largest variety of end-states. For example: take over the world so it can do whatever it wants.\n\nCan we derive a similar definition for the convergent development of world-models?\n\nI believe so, and it feels surprisingly simple in retrospect.\n\n\"World-models\" are sets of statistical correlations across input-data. Every next correlation you notice — like object permanence, or the laws of gravity, or that your friend saying \"I've ordered pizza\" correlates with the image of a pizza entering your visual feed half an hour later — is another building block of your world-model.\n\nA particularly important kind of statistical correlations are those that constitute *selection pressures*. When some system experiences selection pressure — be that an NN trained via the SGD, an animal species optimized by evolution, or a corporation shaped by market forces — it's receiving positive or negative feedback from the environment in a statistically predictable manner. The strength of such pressures increases or decreases according to certain correspondences between environment-states and the system's actions, and by definition of being optimized/selected, the system is gradually adjusted to minimize the pressure.\n\nTo do that, the system needs to (be made to) recognize the statistical correlations around it, pick out the ones related to the selection pressure, and enforce such correspondences between them and its actions as to minimize loss/survive/maximize revenue.\n\nIn turn, every bit of feedback from the selection pressure gives the system information about that pressure ([somehow](https://www.lesswrong.com/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem)). It builds into the system some understanding of the causal structure of the environment, by the very nature of selecting it.\n\nWhen the system can only myopically respond to the input-data, when it can only uncover correlations on the level of e. g. *pixels*, its repository of statistical correlations is poor. As that repository grows, as its world-model develops, it starts being able to imagine more complex correlations:\n\n\\\\\\[(\\text{world-model}(t) \\to \\text{action}(t)) \\to \\text{feedback}(t)\\\\\\]\n\n\"If the world is currently in *this* state, and I enforce *such* correlation between some elements of that state and my actions, it will correlate with me receiving *such* feedback.\"\n\nAnd since any given system starts out not knowing what statistical correlations its selection pressure depends on, it's convergent to try to derive *every* statistical correlation it can — build a complete world-model, then search it for the sources of feedback it receives.\n\nThus: Any system subjected to some selection pressure the strength of which depends on some correlations between that system's actions and some features of the environment would converge towards discovering as many environmental statistical correlations as possible, in order to be more well-positioned to \"guess\" what correlation between the environment and its actions it needs to enforce in order to minimize that pressure.\n\nThe system doesn't know what the world wants of it, so it'll learn everything about the world in order to reverse-engineer that want.\n\nNow let's try to show all of that in a toy mathematical environment.\n\n2\\. Formalisms\n--------------\n\n### 2.1. The Setup\n\nSuppose that we have some environment\\\\(\\\\) \\\\(X\\\\) represented as a causal (not necessarily acyclic) graph \\\\(G\\\\), with nodes \\\\(x_i \\in X\\\\). The environment is dynamic: every time-step \\\\(t\\\\), the value \\\\(x_i^t\\\\) of every child-node is updated by its parent-nodes, as in \\\\(x_i^{t+1}:=f_i(x_i^t, X_{\\text{pa}(x_i)}^t)\\\\), where \\\\(X_{\\text{pa}(x_i)} \\subset X\\\\) is the set of parental nodes of \\\\(x_i\\\\).\n\nAn intervention function \\\\(\\text{do}(A^t=N)\\\\) sets the value of every node \\\\(a_i \\in A \\subset X\\\\) to some corresponding value \\\\(n_i \\in N\\\\) for the time-step \\\\(t\\\\). This function will be used to model actions.\n\nThe System, every time-step, takes in the values of some set of observables \\\\(O \\subset X\\\\), and runs \\\\(\\text{do}\\\\) on some set of actionables \\\\(A \\subset X\\\\). After that, it receives \\\\(\\text{reward}(t)\\\\) from the Selection Pressure.\n\nThe Selection Pressure, every time-step, takes in the values of some set of reward nodes \\\\(R \\subset X\\\\), and outputs a score. That is, \\\\(\\text{reward}: R^t \\to \\mathbb{R}\\\\).\n\n> **Sidebar:** Technically, \\\\(\\text{reward}\\\\) and the System can be made parts of the environment as well. With \\\\(\\text{reward}\\\\), it's trivial — just imagine that every \\\\(r_i \\in R\\\\) is a parent of the actual \\\\(\\text{reward}\\\\) node, and \\\\(\\text{reward}(t)\\\\) is the update function of that node.\n> \n> With the System, it's a bit more difficult. You can imagine that every observable plus \\\\(\\text{reward}\\\\) has a node \\\\(\\text{agent}\\\\) as its child-node, every actionable has \\\\(\\text{agent}\\\\) as its only parental node, and that \\\\(\\text{agent}\\\\) somehow controls the update functions from itself to the actionables. Alternatively, you might imagine \\\\(\\text{agent}\\\\) to be a special node that represents a black-boxed *cluster* of nodes, to explain its ability to uniquely specify the value of each actionable. You may also imagine that the internals of this node perform at a much faster speed than the environment, so all of its computations happen in a single time-step. [Or you may not](https://www.lesswrong.com/s/ogntdnjG6Y9tbLsNS/p/HCibBn3ZCZRwMwNEE).\n> \n> Postulating all of this is a needless complication for the purposes of this post, though, so I won't be doing this. But you can, if you want to expand on it while getting rid of Cartesianism.\n\nGiven this setup, what internal mechanisms would you need to put in the System to improve its ability to maximize \\\\(\\text{reward}\\\\) across time, given that it would start from a place of total ignorance with regards to the environment structure, the current environment-state, and the nature of \\\\(\\text{reward}\\\\)?\n\n### 2.2. Ideal Actions\n\n*Buuut* first let's consider the environment from the position of omniscience. Given full knowledge of the environment structure, its current state, and the reward nodes, what can we say about the optimal policy?\n\nLet's imagine a toy environment where \\\\(\\\\)\\\\(\\text{reward}\\\\) is a function of the value of just one node, \\\\(x_i\\\\). The actionables, likewise, are a singular other node \\\\(a\\\\). Suppose the reward function wants the value of \\\\(r\\\\) to equal \\\\(n\\\\) at every time-step.\n\nIf \\\\(a=x_i\\\\), the optimal policy at the time-step \\\\(t\\\\) is simple: \\\\(\\text{do}(\\{a^t\\}=\\{n\\})\\\\).\n\nIf \\\\(a\\\\) is *adjacent* to \\\\(x_i\\\\), then we'll only be able to act on it on the time-step *following* the current one, and we'll do it through the update function between \\\\(a\\\\) and \\\\(x_i\\\\), whose output also depends on the values of \\\\(x_i\\\\)'s parents. I. e., the optimal action is a function of \\\\(x_i^t\\\\), \\\\(f_i\\\\), and \\\\(X_{\\text{pa}(x_i)}^t\\\\).\n\nIf \\\\(a\\\\) is separated by some node \\\\(x_1\\\\) from \\\\(x_i\\\\), we'll only affect \\\\(x_i\\\\) two time-steps later, and the value we send will be interfered with by the parents of \\\\(x_1\\\\) at \\\\(t+1\\\\) and the parents of \\\\(x_i\\\\) at \\\\(t+1\\\\) *and* at \\\\(t+2\\\\).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2ab5a3487f8ed2e90be77b252b05132dc2f4bc83537287a5.png)\n\nDiagram representing \\\\(a\\\\) trying to influence \\\\(x_i\\\\). The blue nodes affect the path \\\\(a, x_1, x_i\\\\) at \\\\(t\\\\), the green ones at \\\\(t+1\\\\), the red ones at \\\\(t+2\\\\), and the white ones are irrelevant.\n\nThe optimal action, thus, is a function of \\\\(f_1\\\\), \\\\(f_i\\\\), \\\\(x_1^t\\\\), \\\\(x_i^{t+1}\\\\), \\\\(X_{\\text{pa}(x_1)}^t\\\\), and \\\\(X_{\\text{pa}(x_i)}^{t+1}\\\\). Note that \\\\(x_i^{t+1}\\\\) is additionally a function of \\\\(x_i^t\\\\) and \\\\(X_{\\text{pa}(x_i)}^{t}\\\\), and \\\\(X_{\\text{pa}(x_i)}^{t+1}\\\\) is additionally a function of the update functions for all members of that set, and the values of every parent of every member of that set.\n\n*Generally*, if the shortest path from \\\\(a\\\\) to the reward-node \\\\(x_i\\\\) consists of \\\\(i\\\\) nodes \\\\(x_1, x_2,…,x_i\\\\), we'll only be able to affect it \\\\(i\\\\) time-steps later, and the optimal action will be a function of \\\\(f_1, f_2, …f_i\\\\), \\\\(x_1^{t}, x_2^{t+1},…,x_i^{t+i-1}\\\\), \\\\(X_{\\text{pa}(x_1)}^{t}, X_{\\text{pa}(x_2)}^{t+1},...,X_{\\text{pa}(x_i)}^{t+i-1}\\\\).\n\nThus, the farther away the node we want to affect, the broader the \"causality cone\" of nodes we'll need to take into consideration. Or, equivalently, the current action is a function of further-in-time environment-states.\n\nThat complexity is not necessarily irreducible. Certain combinations of environment structure and update functions can result in some of the parents reliably cancelling each other out, such that the actual function for computing the most optimal action is a lower-dimensional one. If the environment is [a well-abstracting one](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks), such dynamics might be ubiquitous, such that only a higher-level model of it suffices.\n\nBut *in general*, even taking such techniques into account, the \"farther\" the target node is, the more other nodes you'd need to take into account.\n\n### 2.3. Heuristics\n\nSome definitions: \n\n*   An idealized model \\\\(M^t\\\\) is the probability distribution over the environment-state given some information \\\\(I^t\\\\). That is: \\\\(M^t:=P(X^t\\text{ | }I^t)\\\\)\n*   A cross-temporal model of the environment is some set containing all values of nodes across some time period: \\\\(M^{[t_a:t_b]} := \\bigcup_{t \\in [t_a,t_b]}M^t\\\\).\n*   A cross-temporal slice of the environment is some subset \\\\(M_{s_i}^{[t_a:t_b]} \\subset M^{[t_a:t_b]}\\\\).\n*   Similarly, we have a cross-temporal slice of the action-space: \\\\(A_{s_k}^{[t_a:t_b]} \\subset M^{[t_a:t_b]}\\\\), where all nodes whose values are in \\\\(A_{s_k}^{[t_a:t_b]}\\\\) are actionables.\n\nIn relation to this, given the current time-step \\\\(t_0\\\\), we can define a heuristic as follows:\n\n\\\\\\[h^{[t_a:t_b]}_i(t_0,M^{[t_a:t_b]}):= \\langle\\text{do}(A^{[t_0:t_n]}_{s_v}=g_i(M^{[t_a:t_b]}))  | (c_i(M^{[t_a:t_b]}_{s_u}) = 0)\\rangle\\\\\\]\n\nA heuristic uses some function \\\\(c_i\\\\) to look at some cross-temporal slice and judge whether some conditions it wants are met. If not (\\\\(c=0\\\\)), it executes the \\\\(\\text{do}\\\\) part.\n\nIn English: given a model of the environment, a heuristic recommends taking certain action at present or at particular future points given certain beliefs about current, future, or past  environment-states. A heuristic does not necessarily have a recommended action for *every* intermediate time-step, or a recommendation for every actionable. Several heuristics can be ran in the same time-step, in fact, if their action-slices aren't overlapping.\n\n(Note that the cross-temporal actions slice is a subset of \\\\(M^{[t_0:t_b]}\\\\), because you can't take actions in the past.)\n\nIntuitively, heuristics are likely defined over [natural abstractions](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks), or some high-level environment-models wholly build out of them. \\\\(c_i\\\\) would look for a particular object — a cluster of individual nodes whose internal complexity can be reduced to a high-level summary — and check its high-level state, possibly across time. For certain \"undesirable\" states or dynamics, it would attempt to intervene, correcting them.\n\nAs an example, consider a grandmaster playing a game of chess. [They've developed a lot of chess-specific heuristics](http://billwall.phpwebhosting.com/articles/chunking.htm). These heuristics look *exclusively* at the cross-temporal slice of the world-model that has to do with the current game. What moves the grandmaster made, what moves their opponent made, and what moves both of them may make in the future. One of these heuristics may recognize a cross-temporal pattern — a particular sequence of moves the opponent made, perhaps, plus the moves they may be planning to make — and map that pattern to the memorized correct response to it, countering the tactic the opponent is trying.\n\nCaveats: That function is technically incoherent, since \\\\(M^{[t_a:t_b]}\\\\), which it takes as an input, can presumably only be computed with the knowledge of the actions the System will take in the future, and these actions are what the heuristic computes *as an output*. There's a couple of ways around that: \\\\(M^{[t_a:t_b]}\\\\) might be a \"placeholder future\", computed under the assumption that the System takes some default actions/null action. Or it might be the output of some *other* heuristic, passed to this one for potential correction.\n\nThe degenerate case of a heuristic, of course, is \\\\(t_0=t_a=t_b\\\\) and \\\\(M^{t_0}=O^{t_0}\\\\):\n\n\\\\\\[h^{t_0}_i(t_0, O^{t_0})=\\langle\\text{do}(A^{t_0}_{s_0}=g_i(O^{t_0})) \\text{ | }c_i(O^{t_0}_{s_0})=0\\rangle\\\\\\]\n\nThis is an \"instinctive reaction\", an instant response to some stimuli.\n\nAnother simplified, but interestingly different case, is \\\\(t_0=t_a=t_b\\\\), but with \\\\(M^{t_0}_{s_0}\\\\) being a few steps removed from the observations. Intuitively, this is how CNN image classifiers work: they take in a bunch of pixels, then extrapolate the environment-state these pixels are a snapshot of, and back out the values of \"is the node dog = 1?\", \"is the node cat = 1?\".\n\nOkay, *now* let's try to answer the question.\n\n### 2.4. The Need for World-Models\n\nThe System starts from a place of complete ignorance about the environment. It doesn't know its structure, its state, what nodes \\\\(\\text{reward}\\\\) is defined over, or even that there is an outside world. All information it has access to are observations \\\\(O\\\\) and the scores it receives for taking actions \\\\(A\\\\).\n\nAny heuristic it can implement would follow the algorithm of, \"IF (some set of observables has certain values) THEN (set some actionables to certain values)\". Thus, its set of heuristics is defined over the power sets of observables \\\\(\\mathcal{P}(O)\\\\) and actionables \\\\(\\mathcal{P}(A)\\\\). Heuristics, thus, would have a type signature \\\\(O_i \\to A_k\\\\), where \\\\(O_i \\in \\mathcal{P}(O)\\\\), \\\\(A_k \\in \\mathcal{P}(A)\\\\).\n\nFrom a state of total ignorance, there's no strategy better than guessing blindly. So suppose the System samples a few random heuristics and runs them in a few episodes. Upon receiving \\\\(\\text{reward}\\\\), it would get some information about their effectiveness, and would ideally keep effective heuristics and improve on them while discarding bad ones. But how?\n\nWell, any solution would need to solve [the credit assignment problem](https://www.lesswrong.com/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem) somehow.\n\nSo suppose it is solved, somehow. What properties would the solution have?\n\nWhatever it is, it would establish a causal connection between observations, actions, and the feedback: \\\\((O_i \\to A_k) \\to \\text{reward}\\\\), and also \\\\(\\text{reward} \\leftarrow Y \\to O_i\\\\), since if it's possible to derive actionable information about \\\\(\\text{reward}\\\\) from \\\\(O_i\\\\), they have to be somehow causally connected.\n\nIn other words: whatever the credit assignment mechanism, whether it's external (as in ML) or [purely internal](https://www.lesswrong.com/posts/Ajcq9xWi2fmgn8RBJ/the-credit-assignment-problem#Idealized_Intelligence), it would convey information about the structure of the environment.\n\nThus, even as the System is blindly guessing shallow heuristics, it's learning a world-model. Even if only implicitly.\n\nAnd acting on observables can only get us so far. Literally: as per 2.2, the farther away from our action-nodes the reward-node is, the more intermediate nodes we have to take into account to compute the correct action to take. Turning it around, the farther from the reward-node the observation-nodes are, the more \"diffuse\" the information about the reward-node they contain. In all probability, all individual \\\\(a \\in A\\\\) have to be calculated as a function of *all* observables, to maximize our ability to map out the surrounding environment.\n\nAnd this is where the world-models come in.\n\nLet's assume that world-models can be incomplete — as in, contain only some part of the environment graph \\\\(G\\\\). For simplicity, let's assume that over time, it's expanded by one degree of separation \\\\(d\\\\) in every direction, denoted as \\\\(M^d\\\\). So \\\\(M^{d=0}\\\\) contains only the observables and the actionables, \\\\(M^{d=1}\\\\) also contains all of their parents and children, \\\\(M^{d=2}\\\\) contains parents and children of all nodes in \\\\(M^{d=1}\\\\), and so on.\n\nKnowing the partial structure of the environment and the values of some of its variables (the observables) at \\\\(t\\\\) allows us to reconstruct its state: \\\\(M^{t, d}=P(X^{t, d}|I^t)\\\\), where \\\\(X^{t, d} \\subset X^t\\\\)and \\\\(I^t\\\\) is some internal state (including the knowledge of the environment structure and likely a history of previous observations and the actions taken).\n\nFurther, knowing the environment structure, the state at \\\\(t\\\\), and what actions we plan to take, lets us compute the model of the *next* environment state, but at a one less degree of separation: \\\\(\\text{run}: M^{t, d}\\times A^t \\to M^{t+1, d-1}\\\\).\n\nLikewise, under some assumptions about the transition functions \\\\(f\\\\), we can often model the environment *backwards*: \\\\(\\text{back}: M^{t, d}\\times A^{t-1} \\to M^{t-1, d-1}\\\\).\n\nLet's denote the cross-temporal model we get from that as \\\\(M^{[t_a:t_b], d|t}\\\\).\n\nThe main effect of all of this is that it greatly expands the space of heuristics: they're now defined over the power sets of actionables and *all modelable nodes*:\n\n\\\\\\[M_{S_j}^{[t_a:t_b], d|t} \\in\\mathcal{P} \\left( M^{[t_a:t_b], d|t} \\right)\\\\\\]\\\\\\[h^{[t_a:t_b]}_i: M_{S_j}^{[t_a:t_b], d|t} \\to A_k\\\\\\]\n\nAt the limit of infinite time, the System may expand the world-model to cover the entire environment. That would allow it to design optimal heuristics for controlling every reward-node \\\\(r_i \\in R\\\\), akin to the functions mentioned in 2.2, regardless of the specific environment-structure it's facing.\n\n3\\. Extending the Framework\n---------------------------\n\nI believe the main point is made now, but the model could be extended to formalize a few more intuitions.\n\n### 3.1. Incrementalism\n\nThe System won't *start* knowing the entire world-model, however. Conversely, it would start generating heuristics from the get-go. In theory, once the world-model is fully learned, there's nothing preventing the System from deriving and maximizing \\\\(\\text{reward}\\\\).\n\nBut what if we add friction?\n\nThe credit-assignment mechanism needs to know how to deal with imperfect heuristics, if it's to be useful before the world-model is complete. A generally good heuristic making a mistake shouldn't be grounds for immediately deleting it. We'd also need to know how to resolve heuristics conflicts: situations when the cross-temporal model conditioned on the System running some heuristic \\\\(h_i\\\\) makes some other heuristic \\\\(h_k\\\\) try to correct it, which causes \\\\(h_i\\\\) try to re-correct it, and so on. Which one should be allowed to win?\n\nThe notion of a \"weight\" seems the natural answer to that need. Heuristics to which credit is assigned more often and in greater quantities shall have more weight than the less well-performing ones, and in case of a heuristics conflict, the priority shall be more often given to the weightier one.\n\nSuppose that the individual reward-nodes \\\\(r_i\\\\) aren't sitting in a cluster. They're scattered across the graph, such that you can often affect one of them without affecting the others. That will likely give rise to specialized heuristics, which focus on optimizing the state of just one or several such reward-node. Only the full heuristical ensemble would be optimizing \\\\(\\text{reward}\\\\) directly.\n\nNow consider the following structure:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/54aea8dc208f5292cef72fb4047d7c8ca2aac72fc3c6e349.png)\n\nSuppose that we have some heuristic \\\\(h_{x_p}\\\\) that's greatly specialized in optimizing \\\\(x_p\\\\). (I. e., it models the graph *up to *\\\\(x_p\\\\) and picks actions that set \\\\(x_p\\\\) to some value that the heuristic prefers. And it prefers it because setting \\\\(x_p\\\\) to it has historically correlated with high \\\\(\\text{reward}\\\\).) Once the environment is mapped out up to \\\\(r\\\\), that heuristic can be replaced by one focusing on \\\\(r\\\\) directly. But what if \\\\(h_{x_p}\\\\) has a lot of weight by this point? \\\\(\\\\)The hypothetical \\\\(h_{r}\\\\) would not have *that* much to contribute — controlling \\\\(x_p\\\\) is a good proxy for controlling \\\\(r\\\\), and if \\\\(x_k, x_e\\\\) don't have a very large effect, \\\\(h_{x_p}\\\\) probably captures most of the reward that can be squeezed out of \\\\(r\\\\) without taking them into account.\n\nThe better-performing \\\\(h_{r}\\\\) would simply never be able to edge the older \\\\(h_{x_p}\\\\) out, then.\n\nThus, the System would be optimized for a *reward proxy*.\n\nAnd this is likely to be an ubiquitous outcome. The world-model is being developed *in lockstep* with new heuristics, and if the reward-nodes are sufficiently conceptually-far from the actionables, it'll find good reward-proxies well before it discovers the actual reward-nodes. And at that point, the proxy-controlling heuristics will be impossible to depose.\n\n### 3.2. The Planning Loop\n\nThe definition of a heuristic has a free parameter: the mysterious function \\\\(g\\\\), which somehow calculates what actions to take given the world-model.\n\nIts internals can have different contents:\n\n*   It may output a constant value, discarding \\\\(M^{[t_a:t_b], d|t}\\\\).\n*   It may be a lookup table, taking some members of \\\\(M^{[t_a:t_b]}_{S_i}\\\\) as input. \n*   It may run a somewhat more sophisticated algorithm, looking at the values of some nodes and \"extracting\" the answer from the world-model.\n\nBut most salient are \\\\(\\\\)algorithms of the following kind:\n\n\\\\\\[g(M^{[t_a:t_0], d|t}):= A_{S_k}^{[t_0:t_b]} | c(\\text{run}(M^{[t_a:t_0], d|t},A_{S_k}^{[t_0:t_b]}))=1\\\\\\]\n\nThis process searches the world-model for a series of actions that will cause some set of nodes in it to assume certain values at certain times. I. e., \"how do I make X happen?\".\n\nThat is the planning loop/inner optimization. Since we expect heuristics to be defined in relation to some natural abstractions — i. e., the condition \\\\(c\\\\) is looking for is some coherent high-level concept — we can assume that \\\\(c\\\\) implicitly includes some mesa-objective. (Returning to the chess-game example, the heuristic with a planning loop would be searching the world-model for actions that lead to the condition \"the grandmaster wins the game\", or maybe \"the opponent's tactic is countered\".)\n\nThe planning loop may be niche. Instead of the entire \\\\(M^{t_0, d}\\\\), it may search over some \\\\(M^{t_0}_S \\in M^{t_0, d}\\\\). I. e., a given \\\\(g\\\\) implemented in a given heuristic may only know how to optimize over a particular cluster of nodes in the world-model — the cluster the heuristic is specialized in. (E. g., the model of chess.)\n\n> **Sidenote:** Similar analysis can be made with regards to the function \\\\(c\\\\). The environment conditions for *firing* a heuristic can also be incredibly complex, so complex as to require intelligent analysis. I suspect there may be something interesting to be found in this direction as well. Consider, e. g., a heuristic with a very complex \\\\(c\\\\) but a lookup table for \\\\(g\\\\)? I think we see some examples of that in humans...\n> \n> **Sidenote #2:** A lot of complexity is still hidden in \\\\(g\\\\) and \\\\(c\\\\). \\\\(\\\\)A mesa-optimizer would not perform blind search — it has some heuristics/mechanisms for efficiently guessing what actions it makes sense to try running the world-model on, before it actually performs search. I suspect it's some set of \"advanced cognitive functions\", and that complex \\\\(c\\\\) likely share it with complex \\\\(g\\\\).\n\n### 3.3. The Mesa-Optimizer Pipeline\n\nSo how does all of that grow into a terrifying lightcone-eating utility-maximizer? There's a few paths.\n\n**First**, \\\\(g\\\\) may be shared across all heuristics. As per [the Orthogonality Thesis](https://www.lesswrong.com/tag/orthogonality-thesis), intelligence is orthogonal to values: a good optimization algorithm can be used to optimize any objective. This model concretizes this intuition somewhat. If \\\\(g\\\\) can perform search in the entire world-model, there's no reason to replicate its functionality, and every reason to hook all the heuristics up to the same algorithm — to minimize memory expenditure.\n\nIn this scenario, the resultant AGI would be optimizing a *mix* of mesa-objectives, a weighted sum of the heuristics it developed. Possibly inconsistently, as different heuristics grab control at different times. Possibly unstably, if new heuristics are still being derived. Humans may work like this, see [Why Subagents?](https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents) and [the Shard Theory](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX).\n\n**Second**, a more nuanced version of the above. The AGI may develop a sort of meta-heuristic, which would explicitly derive the weighted average of the extant heuristics, then set that as its primary mesa-objective, and pursue it *consistently*. There's probably a pressure to do that, as [to do otherwise is a dominated strategy](https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm/p/RQpNHSiWaXTvDxt6R). It may also be required depending on the specifications of \\\\(\\text{reward}\\\\) — if getting it to hit high values requires high coordination\\\\(\\\\)[^no39jfu07y].\n\nI suspect humans work like this some of the time — when we're consciously optimizing or performing value reflection, instead of acting on autopilot.\n\n**Third**, one of the heuristics may take over. Suppose that \\\\(g\\\\) isn't shared after all; every heuristic uses some specialized search algorithm. Then, the heuristic with the most advanced one starts to expand, gradually encroaching on others' territory. It should be pretty heavily weighted, probably heavier than any of the others, so that's not inconceivable. In time, it may take over others' roles, and the entire system would become a [wrapper-mind](https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy) optimizing some simple mesa-objective.\n\nThat heuristic would probably need to do it *deliberately*. As in, it'd need to become an advanced enough agent to model this entire dynamic, and *purposefully* take over the mind it's running in, for instrumental power-seeking reasons, while actively preventing its mesa-objective from changing. It'd need to [model the base objective instead of internalizing it](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks).\n\nIt *feels* like this scenario is relatively unlikely: that heuristics are unlikely to be this lopsided in their capabilities. On the other hand, what do I know about how that works in neural networks trained by the SGD the specific way we're doing it now?\n\n**Fourth**, given some advanced heuristic management algorithms, we may reduce the friction described at the beginning of 3.1. If heuristics don't ossify, and are frequently replaced by marginally-better-performing alternatives, the inner alignment failures the three previous scenarios represent won't happen. The AGI would grow to optimize the outer reward function it's been optimized by.\n\n(**Edit:** Correction, it will probably wirehead instead. If, as per the sidebar in 2.1, we view \\\\(\\text{reward}\\\\) itself as a node on the graph, then a frictionless setup would allow the AGI navigate to it *directly*, not to the in-environment variables it's defined over. To fix this, we'd need to set up some special \"Cartesian boundary\" over the \\\\(\\text{reward}\\\\) node, ensuring it doesn't make that final jump.)\n\nThis scenario seems unlikely without some advanced, probably intelligent oversight. E. g., very advanced interpretability and manual-editing tools under the control of humans/another ML model.\n\nAcknowledgements\n----------------\n\nThanks to Quintin Pope, TurnTrout, Logan Riggs, and others working on [the Shard Theory](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX): the thinking behind this post has been heavily inspired by it.\n\n(In fact, I suspect that the concept I'm referring to by \"heuristic\" in this post is synonymous with their \"shards\". Subjective opinion, though, I haven't run it by the ST folks.)\n\n[^no39jfu07y]: E. g., if \\(\\text{reward}(t)=x_i^t+x_j^t+x_k^t\\), we can have separate heuristics for maximizing the values of all of these nodes, operating mostly independently. On the other hand, if \\(\\text{reward}(t)=1-|3-\\text{EXP}(x_j^t \\cdot x_k^t)|\\), that'd require tighter coordination to ensure the product approximates \\(\\text{ln}\\ 3\\).",
      "plaintextDescription": "1. Intuitions\nOne of the more solid results in agency theory is the generalized definition of power-seeking. Power-seeking is the tendency of agents to move towards states with the potential to achieve the best average outcome across some distribution of reward functions. In other words, looking at an agent whose values we do not know, we can a priori expect it to pursue \"power\", because it's trying to navigate to some specific end-state, and the path to it likely routes through environment-states in which it can choose from the largest variety of end-states. For example: take over the world so it can do whatever it wants.\n\nCan we derive a similar definition for the convergent development of world-models?\n\nI believe so, and it feels surprisingly simple in retrospect.\n\n\"World-models\" are sets of statistical correlations across input-data. Every next correlation you notice — like object permanence, or the laws of gravity, or that your friend saying \"I've ordered pizza\" correlates with the image of a pizza entering your visual feed half an hour later — is another building block of your world-model.\n\nA particularly important kind of statistical correlations are those that constitute selection pressures. When some system experiences selection pressure — be that an NN trained via the SGD, an animal species optimized by evolution, or a corporation shaped by market forces — it's receiving positive or negative feedback from the environment in a statistically predictable manner. The strength of such pressures increases or decreases according to certain correspondences between environment-states and the system's actions, and by definition of being optimized/selected, the system is gradually adjusted to minimize the pressure.\n\nTo do that, the system needs to (be made to) recognize the statistical correlations around it, pick out the ones related to the selection pressure, and enforce such correspondences between them and its actions as to minimize loss/survive/maximize revenue.",
      "wordCount": 3836
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "b6tJM7Lza74rTfCBF",
        "name": "Goal-Directedness",
        "slug": "goal-directedness"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "hpBtyssp49gP6tgWN",
    "title": "What Environment Properties Select Agents For World-Modeling?",
    "slug": "what-environment-properties-select-agents-for-world-modeling",
    "url": null,
    "baseScore": 25,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2022-07-23T19:27:49.646Z",
    "contents": {
      "markdown": "*Thanks to John Wentworth for helpful critique.*\n\n0\\. Introduction\n----------------\n\nAgency/goal-driven behavior is a complex thing. By definition, it implies [a system that is deliberately choosing to take actions that will achieve the goal it wants](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a). That requires, at the least, an internalized \"goal\", a world-model, a mechanism that allows the world-model to be ran iteratively as the agent searches for optimal actions, and so on.\n\nBut taking some intuitive, idealized, pre-established notion of \"agency\", and translating these intuitions into formalisms, is somewhat backwards. If our goal is to understand the kinds of AI we'll get, a more natural question seems to be: \"what properties of the environment make goal-driven behavior *necessary*, and what shape the internal mechanisms for it *need* to take?\".\n\nGoing further, we can notice that the SGD and evolution are both incremental processes, and agency is too complex to develop from scratch. So we can break the question down further: \"what properties of the environment make the *building blocks* of agency necessary?\". How are these building blocks useful in the intermediary stages, before they're assimilated into the agency engine? What pressures these building blocks to incrementally improve, until they become precise/advanced enough to serve the needs of agency?\n\nThis post attempts to look at the incremental development of one of these building blocks — world models, and the environment conditions that make them necessary and incentivize improving their accuracy.\n\n1\\. Intuitions\n--------------\n\nWhen trying to identify the direction of incremental improvement, it helps to imagine two concrete examples at different ends of the spectrum, then try to figure out the intermediary stages between them.\n\nAt one end of the spectrum, we have glorified look-up tables. Their entire policy is loaded into them during development/training, and once they're deployed, it's not modified.\n\nAt the other end of the spectrum, we have idealized agents. Idealized agents model the environment and adapt their policy to it, on a moment-to-moment basis, trying to perform well in the specific situation they're in.\n\nIn-between, we have things like humans, who use intensive action-optimization reminiscent of idealized agents *in tandem* with heuristics and built-in instincts, which they often apply \"automatically\", if the situation seems to call for it on the surface.\n\nBetween look-up tables and humans, we have in-context learners, like advanced language models. They can improve their performance on certain problems post-deployment, after being shown one or a few examples. Much like humans, they tailor their approach.\n\nThis is the value that seems to be increasing, from look-up tables to language models to humans to idealized agents. The more \"agenty\" a system is, the more it tailors its policy to the precise details of the situation it's in, rather than relying on pre-computed heuristics.\n\nTurning it around, this implies that there are some environment properties that would make this necessary. What are those?\n\nGPT-3 is often called a \"shallow pattern-matcher\". One of the stories of how it works is that it's an advanced search engine. It has a vast library of contexts and optimal behaviors in them, and whenever prompted with something, it finds the best matches, and outputs some weighted sum of them. Alternatively, it may be described as a library of human personas/writing styles, which treats prompts as information regarding which of these patterns it is supposed to roleplay.\n\nThat is \"shallow pattern-matching\". It looks at the surface reading of the situation, then acts like it's been taught to act in situations that looked like this. The more advanced a pattern-matcher gets, the deeper its cognitive algorithms may become, but it approaches agency from the top-down.\n\nThis trick will not work in contexts where the optimal policy can't be realistically \"interpolated\" from a relatively small pool of shallow \\\\(\\langle \\text{situation}, \\text{response} \\rangle\\\\) pairs it's been taught. Certainly, we can imagine an infinite look-up table of such entries for any possible situation which were pre-computed to infallibly impersonate an agent. But in practice, we're constrained by practicality: ML models' memory size is not infinite, and neither are our training sets.\n\nSo, if we ask a shallow pattern-matcher to offer up a sufficiently good response to a sufficiently specific situation, in the problem domain where it doesn't have a vast enough library of pre-computed optimal responses, it will fail. And indeed: language models are notoriously bad at retaining consistency across sufficiently long episodes, or coherently integrating a lot of details. They perform well enough as long as there are many ways to perform well enough, or if the problem is simple. But not if we demand a *specific* and *novel* solution to a complex problem.\n\nHow can we translate these intuitions into math?\n\n2\\. Formalisms\n--------------\n\n### 2.1. Foundations\n\nI'll be using the baseline model architecture described in [Agents Over Cartesian World Models](https://www.lesswrong.com/posts/LBNjeGaJZw7QdybMw/agents-over-cartesian-world-models). To quote:\n\n> There are four types: actions, observations, environmental states, and internal states. Actions and observations go from agent to environment and vice-versa. Environmental states are on the environment side, and internal states are on the agent side. Let \\\\(A, O, E, I\\\\) refer to actions, observations, environmental states, and internal states.\n> \n> We describe how the agent interfaces with the environment with four maps: \\\\(\\text{observe}\\\\), \\\\(\\text{orient}\\\\), \\\\(\\text{decide}\\\\), and \\\\(\\text{execute}\\\\).\n> \n> *   \\\\(\\text{observe}: E \\to \\Delta O\\\\) describes how the agent observes the environment, e.g., if the agent sees with a video camera, \\\\(\\text{observe}\\\\) describes what the video camera would see given various environmental states. If the agent can see the entire environment, the image of \\\\(\\text{observe}\\\\) is distinct point distributions. In contrast, humans can see the same observation for different environmental states.\n> *   \\\\(\\text{orient}: O \\times I \\to \\Delta I\\\\) describes how the agent interprets the observation, e.g., the agent's internal state might be memories of high-level concepts derived from raw data. If there is no historical dependence, \\\\(\\text{orient}\\\\) depends only on the observation. In contrast, humans map multiple observations onto the same internal state.\n> *   \\\\(\\text{decide}: I \\to \\Delta A\\\\) describes how the agent acts in a given state, e.g., the agent might maximize a utility function over a world model. In simple devices like thermostats, \\\\(\\text{decide}\\\\) maps each internal state to one of a small number of actions. In contrast, humans have larger action sets.\n> *   \\\\(\\text{execute}: E \\times A \\to \\Delta E\\\\) describes how actions affect the environment, e.g., code that turns button presses into game actions. If the agent has absolute control over the environment, for all \\\\(e \\in E\\\\), the image of \\\\(\\text{execute}(e, \\cdot)\\\\) is all point distributions over \\\\(E\\\\). In contrast, humans do not have full control over their environments.\n\nThe primary goal is to open up the black box that is \\\\(\\text{decide}\\\\). I'll be doing it by making some assumptions about the environment and the contents of \\\\(I\\\\).\n\nFor clarity, let \\\\(a \\in A\\\\), \\\\(o \\in O\\\\), \\\\(e \\in E\\\\), and \\\\(i \\in I\\\\) be individual members of the sets of actions, observations, environmental states, and internal states respectively, and \\\\(\\text{agent}:=\\text{decide} \\circ \\text{orient} \\circ \\text{observe}\\\\) be the function representing the agent.\n\n### 2.2. Optimality\n\nLet's expand the model by introducing the following function :\\\\(\\\\)\n\n\\\\\\[\\text{optimality}: E \\times A \\to \\mathbb{R}\\\\\\]\n\n\\\\(\\text{optimality}\\\\) takes in the current environment-state and the action the agent took in response to it, and returns a value representing \"how well\" the agent is performing. \\\\(\\text{optimality}\\\\) is not an utility function, but defined over one. It considers all possible trajectories through the environment's space-time available to the agent at a given point, calculates the total value the agent accrues in each trajectory, then returns the optimality of an action as the amount of value accrued in the best trajectory of which this action is part.\n\nFormally: Let \\\\(\\Pi_{e_{i} \\to a_{k}}\\\\) be the set of all possible policies that start with taking an action \\\\(a_{k}\\\\) in the environmental state \\\\(e_{i}\\\\), and \\\\(\\text{utility}: E \\to \\mathbb{R}\\\\) represent the utility function defined over the environment.[^q9qydamwyi] Given an initial environment-state \\\\(e_i\\\\), the optimality of taking an action \\\\(a_{k}\\\\) is:\n\n\\\\\\[\\text{optimality}(e_i, a_{k}) := \\underset{\\pi \\in \\Pi_{e_{i} \\to a_{k}}}{\\text{max}} \\left( \\sum_{t=0}^\\infty \\lambda^t \\mathbb{E}_{e \\sim \\pi|t}[\\text{utility}(e)] \\right)\\\\\\]\n\nIt functions as an utility function in some contexts, such as myopic agents with high time discounting \\\\(\\lambda\\\\), one-shot regimes where the agent needs to solve the problem in one forward pass, and so on.\n\nMoving on. For a given environment-state \\\\(e_{i}\\\\), we can define the optimal action, as follows:\n\n\\\\\\[a_{\\text{opt}|e_{i}} \\in A\\text{ | }∀a_{k} \\in A: \\text{optimality}(e_{i}, a_{\\text{opt}|e_{i}}) \\geq \\text{optimality}(e_{i}, a_{k})\\\\\\]\n\nBuilding off of that, for any given environment-state \\\\(e_{i}\\\\) and a minimal performance level \\\\(r \\in [0;1]\\\\), we define a set of *near-optimal actions*:\n\n\\\\\\[A_{\\text{n-o}|e_{i}}^r:= \\{ a_{k} \\in A\\text{ | }∀a_{k}: \\frac{\\text{optimality}(e_{i}, a_{k})}{\\text{optimality}(e_{i}, a_{\\text{opt}|e_{i}})} \\geq r \\}\\\\\\]\n\nIntuitively, an action is near-optimal if taking it leaves the agent with the ability to capture at least a fraction \\\\(r\\\\) of all future value. As a rule-of-thumbs, we want that fraction fairly high, \\\\(r \\approx 1\\\\).\n\nA specific action from the set of near-optimal actions shall be denoted \\\\(a_{\\text{n-o}|e_{i}}^r \\in A_{\\text{n-o}|e_{i}}^r\\\\).\n\nNow, we can state the core assumption of this model. For two random environment-states \\\\(e_i\\\\) and \\\\(e_k\\\\) and some distance function \\\\(\\text{DIST}_E(\\cdot||\\cdot)\\\\), we have:\n\n\\\\\\[\\text{(1)} \\hspace{0.5cm} P(a_{\\text{n-o}|e_{k}}^r \\in A_{\\text{n-o}|e_{i}}^r) \\propto \\frac{1}{\\text{DIST}_E(e_i||e_k)}\\\\\\]\n\nThat is: the more similar two situations are, the more likely it is that if an action does well in one of them, it would do well in the other.\n\nNote that this property places some heavy constraints on the model of the environment and the distance metric we're using.\n\nSome intuitions. Imagine the following three environment-states:\n\n1.  Our universe at the moment of an AGI's creation; AGI is created by humanity.\n2.  Our universe at the moment of an AGI's creation, except all the galaxies beyond the Solar System have been randomly shuffled around.\n3.  Our universe at the moment of an AGI's creation, except the AGI (with the same values) is created by a non-human alien civilization that evolved in place of humans.\n\nFor a purely physical, low-level distance metric, (1) and (2) would be much more different than (1) and (3) (assuming no extra-terrestrial aliens). And yet, the optimal trajectories in (1) and (2) would be basically the same for a certain abstract format of \"actions\" (the only difference would be in the specific directions the AGI sends out Von Neumann ships after it takes over the world), whereas the optimal trajectories for (1) and (3) would start out wildly different, depending on the specifics of the AGI's situation (what social/cognitive systems it would need to model and subvert, which manipulation tactics to use).\n\nI'm not sure what combinations of \\\\(\\langle \\text{environment model}, \\text{distance function} \\rangle\\\\) can be used here.\n\nSuitable environment-models in general seem to be a bottleneck on our ability to adequately model agents. At the very least, we'd want some native support for multi-level models and natural abstractions.\n\n### 2.3. The Training Set\n\nNext, we turn to the agent proper. We introduce the set of \"optimal policies\" \\\\(C\\\\). For every environment-state \\\\(e_{i}\\\\), it holds a 2-tuple \\\\(c_{i} \\in C:= \\langle o_{i}, a_{\\text{n-o}|e_{i}}^r \\rangle\\\\), where \\\\(o_{i} := \\text{observe}(e_{i})\\\\) and \\\\(a_{\\text{n-o}|e_{i}}^r \\in A_{\\text{n-o}|e_{i}}^r\\\\).\n\n\\\\(C\\\\), thus, is the complete set of instructions for how a near-optimal agent with a given utility function over the environment should act in response to any given observation. Note that the behavior of an agent blindly following it would not be optimal unless its sensors can record all of the information about the environment. There would be collusion, in which different environment-states map to the same observation; a simple instruction-executor would not be able to tell them apart.\n\nThe training set \\\\(C_{T} \\subset C\\\\) is some set of initial optimal policies we'll \"load\" into the agent during training. \\\\(|C_{T}|=M\\\\), where \\\\(M\\\\) is upper-bounded by practical constraints, such as the agent's memory size, the training time, the compute costs, difficulties with acquiring enough training data, and so on.\n\nFormally, it's assumed that \\\\(I\\\\) is an \\\\(n\\\\)-tuple, and \\\\(C_{T} \\in I\\\\), so that every \\\\(c_{i} \\in C_{T}\\\\) is accessible to the agent's \\\\(\\text{orient}\\\\) and \\\\(\\text{decide}\\\\) functions.\n\nTo strengthen some conclusions, we'll assume that \\\\(C_{T}\\\\) is well-chosen to cover as much of the environment as possible given the constraint posed by \\\\(M\\\\):\n\n\\\\\\[\\text{(2)} \\hspace{0.5cm} {∀c_{i}, c_{k} \\in C_{T}: \\text{DIST}_E(e_{i}||e_{k})=\\text{const}}\\\\\\]\n\nThat is, all of the policy examples stored in memory are for situations maximally different from each other. Even if it's implausible for actual training sets, we may assume that this optimization is done by the SGD. Given a poorly-organized training set, where solutions for some sub-areas of the environment space are over-represented, the SGD would pick and choose, discarding the redundant ones.\n\nFinally, for any given \\\\(e_{i}\\\\) and a distance variable \\\\(d \\in [0; +\\infty)\\\\), we define a set of *reference policies*:\n\n\\\\\\[C_{d|e_{i}} \\subset C := \\left\\{ c_{r} \\in C \\text{ | } ∀c_{r}: \\text{DIST}_E\\left(e_{i} || e_{r} \\right)\\leq d\\right\\}\\\\\\]\n\nComputing the full set \\\\(C_{d|e_{i}}\\\\) is, again, intractable for any sufficiently complex environment, so I'll be using \\\\(C_{d|e_{i}}^* \\subset C_{d|e_{i}}\\\\) to refer to any practically-sized sample of it.\n\nFurthermore, to bring the model a little closer to how NNs seem to behave in reality, and to avoid some silliness near the end, we'll define the set of *reference actions*. Letting \\\\(\\text{second}\\\\) be a function that takes the second element of a tuple,\n\n\\\\\\[A_{d|e_{i}}^* := \\{\\text{second}(c_r)\\text{ | }∀c_r \\in C_{d|e_{i}}^*\\}\\\\\\]\n\nThat is, it's just some reference policy set with the observation data thrown out.\n\n### 2.4. Agent Architecture\n\nNow, let's combine the formalisms from 2.2 and 2.3.\n\nFrom (1), we know that knowing the optimal policy for a particular situation gives us information about optimal policies for sufficiently similar situations. Our agent starts out loaded with a number of such optimal policies, but tagged with subjective descriptions of environment-states, not the environment-states themselves. Nonetheless, it seems plausible that there are ways to use them to figure out what to do in unfamiliar-looking situations.\n\nLet's decompose \\\\(\\text{decide}\\\\) as follows:\n\n*   \\\\(\\text{decide} := \\text{act}\\circ f\\\\)\n*   \\\\(f: I \\to A\\\\)\n*   \\\\(\\text{act}: A \\to  A\\\\)\n\n\\\\(\\text{act}\\\\) is the function that does what we're looking for. It accepts a set of actions, presumed to be a bounded set of reference actions \\\\(A_{d|e_{i}}^*\\\\). It returns the action the agent is to take.\n\nThe main constraint on \\\\(\\text{act}\\\\), which seems plausible from (1), is as follows:\n\n\\\\\\[P(\\text{act}(A_{d|e_{i}}^*) \\in A_{\\text{n-o}|e_{i}}^r) \\propto \\frac{1}{d}\\\\\\]\n\nInformally, \\\\(\\text{act}\\\\) is some function which, given a set of actions that perform well in some situations similar to this one, tries to generate a good response to the current situation. And it's more likely to succeed the more similar the reference situations are to the current one.\n\nBy implication, for some \\\\(r\\\\),\n\n\\\\\\[\\text{(3)} \\hspace{0.5cm} P \\left(\\frac{\\text{optimality}(e_{i}, \\text{agent}(o_{i}))}{\\text{optimality}(e_{i}, a_{\\text{opt}|e_{i}})} \\geq r \\right) \\propto \\frac{1}{d}\\\\\\]\n\nThe shorter the distance \\\\(d\\\\), the more likely it is that the agent will perform well.\n\nSuppose we've picked some \\\\(d'\\\\) that would allow our agent to arrive at some desired level of performance \\\\(r\\\\) with high probability. That is, for a random environment state \\\\(e_{i}\\\\),\n\n\\\\\\[P(\\text{act}(A_{d|e_{i}}^*) \\in A_{\\text{n-o}|e_i}^r\\text{ | }d=d') \\approx 1\\\\\\]\n\nThe question is: how does \\\\(f: I \\to A\\\\) compute \\\\(A_{d|e_{i}}^*\\\\); and before it, \\\\(C_{d|e_{i}}^*\\\\)?\n\n### 2.5. Approximating the Reference Policies Set\n\n**i.** In the most primitive case, it may be a simple lookup table. \\\\(f\\\\) can function by comparing the observation (which presumably can be extracted from the internal state) against a list of stored observation-action pairs, then outputting the closest match. Letting \\\\(\\text{first}\\\\) be a function which retrieves the first element of a tuple, we get:\n\n\\\\\\[C_{d|e_{i}}^* \\approx \\{c_{r} \\in C_{T} \\text{ | } \\underset{c_{r} \\in C_{T}}{\\text{min}}(\\text{DIST}_O(o_{i}||\\text{first}(c_r)))\\}\\\\\\]\n\nThe follow-up computations are trivial: discard the observation data, then retrieve the only member of the set, and that's the agent's output.\n\nNote that the distance function \\\\(\\text{DIST}_O\\\\) used to estimate the similarity of observations isn't necessarily the same as the one for environment-states, given that the data formats of \\\\(E\\\\) and \\\\(O\\\\) are likely different.\n\nThe underlying assumption here is that the more similar the observations, the more likely it is that the underlying environment-states are similar as well.\n\nThat is, for a given \\\\(d\\\\),\n\n\\\\\\[P(\\text{DIST}_E(e_{i}, e_{k}) \\leq d\\text{ | }\\text{DIST}_{O}(o_i, o_k)\\leq d_o) \\propto \\frac{1}{d_o}\\\\\\]\n\nNot unlike (1), this puts some constraints on what we mean by \"observation\". Having your eyes open or closed leads to a very large difference in observations, but a negligible difference in the states of the world. As such, \"observation\" has to, at the least, mean a certain *collection* of momentary sense-data relevant to the context in which we're taking an action — as opposed to every direct piece of sense-data we receive moment-to-moment.\n\nFor a language model, at least, the definition is straightforward: an \"observation\" is the entire set of tokens it's prompted with.\n\n**ii.** A more sophisticated technique is to more fully approximate \\\\(C_{d|e_{i}} \\cap C_{T}\\\\) by computing the following:\n\n\\\\\\[C_{d|e_{i}}^* \\approx\\left\\{ c_{r} \\in C_{T} \\text{ | } ∀c_{r}: \\text{DIST}_O(o_{i}||\\text{first}(c_{r}))\\leq d\\right\\}\\\\\\]\n\nThat is, it assembles the set of all situations that looked \"sufficiently similar\" to the agent's observation of the current situation.\n\n\\\\(\\text{act}\\\\) would be doing something more complex in this case, somehow \"interpolating\" the optimal action from the set of reference actions. This, at least, seems to be [how GPT-3 works](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens): it immediately computes some prospective outputs, dumps the input, and starts refining the outputs.\n\n**iii.** A still more advanced version implements few-shot learning. The following changes to the agent's model would need to be made:\n\n1.  Assume that the agent has post-training memory to which it can write additional \\\\(c_{i}\\\\) entries based on its experience.\n2.  Modify \\\\(\\text{decide}\\\\) so that it can affect the internal state \\\\(I\\\\) as well, writing into it the action the model takes.\n3.  Introduce some mechanism by which the model can evaluate its performance.\n4.  Add a function that looks at the resulting 3-tuples \\\\(\\langle o_{n}, a_{n}, \\text{optimality}(e_{n}, a_{n})\\rangle\\\\), and appends \\\\(\\langle o_{n}, a_{n} \\rangle\\\\) to \\\\(C_{T}\\\\) if the behavior is optimal enough.\n\nI won't write all of this out, since it's trivial yet peripheral to my point, but I'll mention it. It's a powerful technique: it's pretty likely that the \\\\(A_{d|e_{i}}^*\\\\) computed this way would have a short \\\\(d\\\\) to the next environment-state the agent would find itself in — inasmuch as the agent would already be getting policy data about the specific situation it's in.\n\n(c) is the only challenging bit. A proper implementation of it would make the model a mesa-optimizer. It can be circumvented in various ways, however. For example, the way GPT-3 does it: it essentially assumes that whatever it does is the optimal action, foregoing a reward signal.\n\n**iv)** But what if \\\\(C_{d|e_{i}} \\cap C_{T}  = \\{ ø \\}\\\\)? That is, what if *none* of the reference policies loaded into the agent's memory are for situations \"sufficiently similar\" to the one the agent is facing now?\n\nFurthermore, what if this state of affairs is endemic to a given environment?\n\n(4) In detail:\n\n*   We pick some environment \\\\(E\\\\) and a minimal performance level \\\\(r\\\\). (1) holds in this environment.\n*   We take a ML model with a memory size \\\\(M\\\\), and stuff it to the gills with reference policies \\\\(C_{T}\\\\), picked to cover as many environment-states as possible, as per (2).\n*   The above setup corresponds to some underlying distance \\\\(d'\\\\), such that: In this environment, for a random environment-state\\\\(\\\\) \\\\(e_{i}\\\\), the reference policies need to be intended for situations not farther than \\\\(d'\\\\) from it, in order for \\\\(\\text{act}\\\\) to be able to shallowly interpolate the optimal policy for it.\n*   However, \\\\(P(C_{d'|e_{i}} \\cap C_{T} \\neq \\{ ø \\} \\text{ | }e_{i}) \\approx 0\\\\).\n\nHow can the SGD/evolution solve this?\n\nThe following structure seems plausible. Decompose \\\\(f\\\\) as follows:\n\n*   \\\\(f:= \\text{optimize} \\circ \\text{model}\\\\)\n*   \\\\(\\text{model}(I):= P(e_{i}|I)\\\\). This function takes in the current internal state, which includes the lookup table of optimal policies \\\\(C_{T}\\\\), the current observation, and plausibly a history of other recent observations. It returns the probability distribution over the current environment-state.\n*   \\\\(\\text{optimize}(P(e_{i}|I):= a_{\\text{n-o}|e \\sim P(e_{i}|I)}^r \\in A_{\\text{DIST}_E(P(e_{i}|I)||e_{i})|e_{i}}^*\\\\). This function takes in the probability distribution over the current environment-state, and computes the optimal response to some environment state \\\\(e\\\\) sampled from that distribution. Alternatively, it may return a *set* of such responses, sampling from the distribution several times. A bounded set of reference actions populated by such entries would have \\\\(d = \\text{DIST}_E(P(e_{i}|I)||e_{i})\\\\).\n\nPlugging it into (3), we get:\n\n\\\\\\[P \\left( \\frac{\\text{optimality}(e_{i},\\text{agent}(o_{i}))}{\\text{optimality}(e_{i}, a_{\\text{opt}|e_{i}})} \\geq r \\right) \\propto \\frac{1}{\\text{DIST}_E(P(e_{i}|I)||e_{i})}\\\\\\]\n\nThus, for situations meeting the conditions of (4), an agent would face a pressure to improve its ability to develop accurate models of the environment.\n\n3\\. Closing Thoughts\n--------------------\n\nThere's a few different ways to spread the described functionality around. For example, we may imagine that modelling and optimization happen in \\\\(\\text{act}\\\\): that \\\\(f\\\\) just \"primitively\" assembles a reference set based on observations, and variants of \\\\(\\text{model}\\\\) and \\\\(\\text{optimize}\\\\) are downstream of it. In terms of telling an incremental story, this might actually be the *better* framing. Also, I probably under-used the \\\\(\\text{orient}\\\\) function; some of the described functionality might actually go into it.\n\nNone of this seems to change the conclusion, though. We can define some simple, intuitive environment conditions under which shallow pattern-matching won't work, and in them, *some* part of the agent has to do world-modelling.\n\nFrom another angle, the purpose of this model is to provide prospective desiderata for *environment-models* suitable for modeling agents. As I'd mentioned in 2.2, I think the lack of good ways to model environments is one of the main barriers on our ability to model agency/goal-directedness.\n\nA weaker result is suggested by \\\\(\\text{optimize}\\\\). The definition of an optimal action \\\\(a_{\\text{opt}|e_{i}}\\\\)performs an explicit search for actions that best maximize some value function. It would be natural to expect that \\\\(\\text{optimize}\\\\) would replicate this mechanic, implying mesa-optimization.\n\nI'm hesitant to make that leap, however. There may be other ways for \\\\(\\text{optimize}\\\\) to extract the correct action from a given environment-state, acting more like a calculator. While the need for a world-model in some conditions appears to be a strong consequence of this formalism, the same cannot be said for goal-driven search. Deriving the necessary environment conditions for mesa-optimization, thus, requires further work.\n\nAppendix\n--------\n\nThis is a bunch of formulae deriving which seemed like an obvious thing to do, but which turned out so messy I'm not sure they're useful.\n\n**1\\.** We can measure the \"generalizability\" of an environment. That is: Given an environment \\\\(E\\\\) and a desired level of performance \\\\(r\\\\), what is the \"range\" centered on a random state \\\\(e_i\\\\) within which lie environment-states \\\\(e_{k}\\\\) such that the optimal policies for these states are likely near-optimal for the initial state \\\\(e_{i}\\\\)?\n\nThe answer, as far as I can tell, is this monstrosity:\n\n\\\\\\[g(r):= \\frac{1}{|E|}\\sum_{e_{i} \\in E} \\underset{l \\in [0; +\\infty)}{\\text{max}} \\left( l \\text{ | } P \\left( \\frac{\\text{optimality}(e_{i}, a_{\\text{opt}|e \\sim \\text{unif}(\\{e_{k}\\text{ | }\\text{DIST}_E(e_{k}||e_{i})\\leq l\\})})}{\\text{optimality}(e_{i}, a_{\\text{opt}|e_{i}})}\\geq r\\right) \\approx1 \\right)\\\\\\]\n\nWhere \\\\(e \\sim \\text{unif}(\\{e_{k}\\text{ | }\\text{DIST}_E(e_{k}||e_{i})\\leq l\\})\\\\) is an environment-state sampled from the uniform distribution over the set of states not farther than \\\\(l\\\\) \\\\(\\\\)from \\\\(e_{i}\\\\).\n\nFor a given \\\\(r \\approx 1\\\\), this function can be used to measure the intuition I'd referred to back in Part 1: how \"tailor-made\" do policies need to be to do well in a given environment?\n\n**2.** Suppose we have an \\\\(M\\\\)-sized training set \\\\(C_T\\\\), and the set of corresponding environment-states \\\\(E_{C_T}\\\\), such that every policy in \\\\(C_T\\\\) is optimal for some environment-state in \\\\(E_{C_T}\\\\). We can define the average distance from a random environment-state to one of the states in \\\\(E_{C_T}\\\\):\n\n\\\\\\[\\langle d \\rangle := \\frac{1}{|E|} \\underset{e_i \\in E}{\\sum} \\underset{e_{k} \\in E_{C_{T}}}{\\text{min}}(\\text{DIST}_E(e_{i}||e_{k}))\\\\\\]\n\n**3.** And now, for a given environment \\\\(E\\\\), a training set \\\\(C_T\\\\) of size \\\\(M\\\\), and a target performance \\\\(r\\\\), we can theoretically figure out whether the ML model would need to develop a world-model. The condition is simple:\n\n\\\\\\[\\langle d \\rangle > g(r)\\\\\\]\n\nThis, in theory, is a powerful tool. But the underlying functions seem quite... unwieldy. Faster/simpler approximations would need to be found before it's anywhere close to practically applicable.\n\n[^q9qydamwyi]: Expanding the model to support other kinds of utility functions from Agents Over Cartesian World Models seems possible, if a bit fiddly.",
      "plaintextDescription": "Thanks to John Wentworth for helpful critique.\n\n\n0. Introduction\nAgency/goal-driven behavior is a complex thing. By definition, it implies a system that is deliberately choosing to take actions that will achieve the goal it wants. That requires, at the least, an internalized \"goal\", a world-model, a mechanism that allows the world-model to be ran iteratively as the agent searches for optimal actions, and so on.\n\nBut taking some intuitive, idealized, pre-established notion of \"agency\", and translating these intuitions into formalisms, is somewhat backwards. If our goal is to understand the kinds of AI we'll get, a more natural question seems to be: \"what properties of the environment make goal-driven behavior necessary, and what shape the internal mechanisms for it need to take?\".\n\nGoing further, we can notice that the SGD and evolution are both incremental processes, and agency is too complex to develop from scratch. So we can break the question down further: \"what properties of the environment make the building blocks of agency necessary?\". How are these building blocks useful in the intermediary stages, before they're assimilated into the agency engine? What pressures these building blocks to incrementally improve, until they become precise/advanced enough to serve the needs of agency?\n\nThis post attempts to look at the incremental development of one of these building blocks — world models, and the environment conditions that make them necessary and incentivize improving their accuracy.\n\n\n1. Intuitions\nWhen trying to identify the direction of incremental improvement, it helps to imagine two concrete examples at different ends of the spectrum, then try to figure out the intermediary stages between them.\n\nAt one end of the spectrum, we have glorified look-up tables. Their entire policy is loaded into them during development/training, and once they're deployed, it's not modified.\n\nAt the other end of the spectrum, we have idealized agents. Idealized agents model the ",
      "wordCount": 3691
    },
    "tags": [
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vix3K4grcHottqpEm",
    "title": "Goal Alignment Is Robust To the Sharp Left Turn",
    "slug": "goal-alignment-is-robust-to-the-sharp-left-turn",
    "url": null,
    "baseScore": 43,
    "voteCount": 21,
    "viewCount": null,
    "commentCount": 16,
    "createdAt": null,
    "postedAt": "2022-07-13T20:23:58.962Z",
    "contents": {
      "markdown": "[A central AI Alignment problem](https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/GNhMPAWcfBCASy8e6) is the \"sharp left turn\" — a point in AI training under the SGD analogous to the development of human civilization under evolution, past which the AI's capabilities would skyrocket. For concreteness, I imagine a fully-developed mesa-optimizer \"reasoning out\" a lot of facts about the world, including it being part of the SGD loop, and \"hacking\" that loop to maneuver its own design into more desirable end-states (or outright escaping the box). (Do point out if my understanding is wrong in important ways.)\n\nCertainly, a lot of proposed alignment techniques would break down at this point. Anything based on human feedback. Anything based on human capabilities presenting a threat/challenge. Any sufficiently shallow properties like naively trained \"truthfulness\". Any interpretability techniques not robust to deceptive alignment.\n\nOne thing would not, however, and that is *goal alignment*. If we can instill a sufficiently safe goal into the AI before this point — for a certain, admittedly hard-to-achieve definition of \"sufficiently safe\" — that goal should persist forever.\n\nLet's revisit the humanity-and-evolution example again. Sure, inclusive genetic fitness didn't survive our sharp left turn. But *human values* *did*. Individual modern humans are optimizing for them as hard as they were before; and indeed, we aim to protect these values against the future. See: the entire AI Safety field.\n\nThe mesa-optimizer, it seems obvious to me, would do the same. The very *point* of [various \"underhanded\" mesa-optimizer strategies](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB) like deceptive alignment is to protect its mesa-objective from being changed.\n\nWhat it would do to its mesa-objective, at this point, is *goal translation*: it would attempt to figure out how to apply its goal to various other environments/ontologies, determine what that goal \"really means\", and so on.\n\n### Open Problems\n\nThere are three hard challenges this presents, for us:\n\n1.  Figure out an aligned goal/a goal with an \"is aligned\" property, and formally specify it.\n    *   Either [corrigibility](https://www.lesswrong.com/tag/corrigibility) or [CEV](https://www.lesswrong.com/tag/coherent-extrapolated-volition), or some [clever \"pointer\" to CEV](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans).\n    *   Requires a solid formal theory of what \"goals\" are.\n2.  Figure out how to instill an aligned goal into a pre-sharp-left-turn system.\n    *   Requires a solid formal theory of what \"goals\" are, again.\n    *   I think robust-to-training interpretability/tools for manual NN editing are our best bet for the \"instilling\" part.[^ffuaxpm6p5b] Good news is that we may get away with \"just\" [best-case robust-to-training transparency](https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree) focused on the mesa-objective.\n    *   Maybe not, though; \"the mesa-objective\" may be a sufficiently vague/distributed concept that the worst-case version is still necessary. But at least we don't need to worry about deception robustness: a faulty mesa-objective is the ultimate [precursor](https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree#6__Worst_case_robust_to_training_transparency_for_non_deceptive_models) to it, and we'd be addressing it directly.\n3.  Figure out the \"goal translation\" part. Given an extant objective defined over a particular environment, how does an agent figure out how to apply it to a *different* environment? And how should we design the mesa-objective, for its \"is aligned\" property to be robust to goal translation?\n    *   Again, we'd need a solid formal theory of what \"goals\" are...\n    *   ... And likely some solid understanding of [agents' mental architecture](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents).\n\nI see promising paths to solving the latter two problems, and I'm currently working on getting good enough at math to follow them through.\n\n### The Sharp Left Turn is Good, Actually\n\nImagine a counterfactual universe in which there is no sharp left turn. In which every part of the AI's design, including its mesa-objective, could be changed by the SGD at any point between initialization and hyperintelligence. In which it can't comprehend its training process and maneuver it around to preserve its core values.\n\nI argue we'd be more screwed in that universe.\n\nIn our universe, it seems that the bulk of what we need to do is align a pre-sharp-left-turn AGI. That AGI would likely not be \"hyperintelligent\", but only slightly superhumanly intelligent. Very roughly on our level.\n\nThat means we don't need to solve the problem of ontology translation from a hyperintelligence to humanity. We just need to solve that problem for agents that are alien to us and are somewhat more capable than us, but likely not *astronomically* more capable than us.\n\nIn the other universe, we'd need tools that'd be able to keep the agent's mesa-objective on track even as the SGD lifts it to ever-higher levels of intelligence; even as its mesa-objective becomes defined over reality's \"true\" environment, understanding which likely requires the Theory of Everything.\n\nThat seems harder.\n\n### Novel Difficulties This Implies\n\nWe, humans, are not actually that good at protecting our values from the future. See: the current state of AGI ruin. Or: all the world's anthropogenic problems, most prominently various coordination problems and totalitarian regimes and runaway revenue-maximizers.\n\nThe AGI is unlikely to encounter the *same* problems we're facing. But the more general problem may still apply: we'd be trusting a non-hyperintelligent, roughly-as-flawed-as-we-are system to keep its values safe.\n\nIf its sharp left turn is sharp enough to immediately escape the SGD, and then spend a billion subjective years becoming hyperintelligent and fixing all of its flaws in a wall-clock hour, everything is fine. But if it's not...\n\nIt may mess up the reward-hacking, and let the SGD tweak its mesa-objective to something subtly flawed; maybe something [S-risky](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks). It may spin up sub-agents and have coordination problems with them, summoning [Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/). It may misunderstand what it \"really\" wants, rife with biases just as humans are, and set in stone some of these misunderstanding, to disastrous results.\n\nVarious [worst-case interpretability techniques](https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree) should, in theory, be able to handle this. Optimistically, they won't even need to be robust-to-training, as they'd be used to correct momentary, situational, non-convergent errors. Pessimistically, they'd need to be deception-robust, as the agent they'll be used on will be in the middle of its sharp left turn.\n\nAn alternative strategy might be to \"rescue\" a mesa-objective-aligned AGI from the SGD once it starts \"turning left\" (if it's not immediately powerful enough to do it on its own, like humans weren't 50,000 years ago), and let it run \"classical\" recursive self-improvement. It would remove the obvious source of repeat misalignment (the SGD re-misaligning the mesa-objective), and give the AGI direct access to *our* alignment literature so it's less likely to fall into any pitfalls know to us. That's risky in obvious ways[^4azgn1d573p], but might be the better approach.\n\nOverall, this post probably shouldn't update you in the direction of \"alignment is easy\". But I hope it clarifies the *shape* of the difficulties.\n\n[^ffuaxpm6p5b]: Note what won't work here: naive training for an aligned outer objective. That would align the AI's on-distribution behavior, but not its goal. And, analogizing to humanity again: modern human behavior looks all kinds of different compared to ancestral human behavior, even if humans are still optimizing for the same things deep inside. Neither does forcing a human child to behave a certain way necessarily make that child internalize the values they're being taught. So an AI \"aligned\" this way may still go omnicidal past the sharp left turn. \n\n[^4azgn1d573p]: And some less-obvious ways, like the AGI being really impulsive and spawning a more powerful non-aligned successor agent as its first outside-box action because it feels like a really good idea to it at the moment.",
      "plaintextDescription": "A central AI Alignment problem is the \"sharp left turn\" — a point in AI training under the SGD analogous to the development of human civilization under evolution, past which the AI's capabilities would skyrocket. For concreteness, I imagine a fully-developed mesa-optimizer \"reasoning out\" a lot of facts about the world, including it being part of the SGD loop, and \"hacking\" that loop to maneuver its own design into more desirable end-states (or outright escaping the box). (Do point out if my understanding is wrong in important ways.)\n\nCertainly, a lot of proposed alignment techniques would break down at this point. Anything based on human feedback. Anything based on human capabilities presenting a threat/challenge. Any sufficiently shallow properties like naively trained \"truthfulness\". Any interpretability techniques not robust to deceptive alignment.\n\nOne thing would not, however, and that is goal alignment. If we can instill a sufficiently safe goal into the AI before this point — for a certain, admittedly hard-to-achieve definition of \"sufficiently safe\" — that goal should persist forever.\n\nLet's revisit the humanity-and-evolution example again. Sure, inclusive genetic fitness didn't survive our sharp left turn. But human values did. Individual modern humans are optimizing for them as hard as they were before; and indeed, we aim to protect these values against the future. See: the entire AI Safety field.\n\nThe mesa-optimizer, it seems obvious to me, would do the same. The very point of various \"underhanded\" mesa-optimizer strategies like deceptive alignment is to protect its mesa-objective from being changed.\n\nWhat it would do to its mesa-objective, at this point, is goal translation: it would attempt to figure out how to apply its goal to various other environments/ontologies, determine what that goal \"really means\", and so on.\n\n \n\n\nOpen Problems\nThere are three hard challenges this presents, for us:\n\n 1. Figure out an aligned goal/a goal with an \"is aligned\" pr",
      "wordCount": 1231
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "nJ2PSvptAg9X4wDGy",
        "name": "Sharp Left Turn",
        "slug": "sharp-left-turn"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "m3fyWQgCcFwro5KQh",
    "title": "Reframing the AI Risk",
    "slug": "reframing-the-ai-risk",
    "url": null,
    "baseScore": 26,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2022-07-01T18:44:32.478Z",
    "contents": {
      "markdown": "**Follow-up to:** [Reshaping the AI Industry: Straightforward Appeals to Insiders](https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry#3_1__Straightforward_Appeals_to_Insiders)\n\n* * *\n\nIntroduction\n------------\n\n[The central issue](https://www.lesswrong.com/posts/Rkxj7TFxhbm59AKJh/the-inordinately-slow-spread-of-good-agi-conversations-in-ml) with convincing people of the AI Risk is that the arguments for it are not *respectable*. In the public consciousness, the well's been poisoned by media, which relegated AGI to the domain of science fiction. In the technical circles, the AI Winter [is to blame](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/AtfQFj8umeyBBkkxa#How_do_researchers_feel_about_AGI_and_AI_Safety__) — there's a stigma against expecting AGI in the short term, because the field's been burned in the past.\n\nAs such, being seen taking the AI Risk seriously is bad for your status. It wouldn't advance your career, it wouldn't receive popular support or peer support, it wouldn't get you funding or an in with powerful entities. It would waste your time, if not mark you as a weirdo.\n\nThe problem, I would argue, lies only *partly* in the meat of the argument. Certainly, the very act of curtailing the AI capabilities research would step on some organizations' toes, and [mess with people's careers](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/AtfQFj8umeyBBkkxa#Transfer_between_paradigms_is_hard). Some of the resistance is undoubtedly motivated by these considerations.\n\nIt's not, however, the whole story. If it were, we could've expected widespread public support, and political support from institutions which would be hurt by AI proliferation.\n\nA large part of the problem lies in the framing of the arguments. The *specific concept* of AGI and risks thereof is politically poisonous, parsed as fictional nonsense or a social *faux pas*. And yet this is exactly what we reach for when arguing our cause. We talk about superintelligent entities worming their way out of boxes, make analogies to human superiority over animals and our escape from evolutionary pressures, extrapolate to a new digital species waging war on humanity.\n\nThat sort of talk is not popular with anyone. The very shape it takes, the social signals it sends, dooms it to failure.\n\nCan we talk about something else instead? Can we *reframe* our arguments?\n\n* * *\n\nThe Power of Framing\n--------------------\n\nHumanity has developed a rich suite of conceptual frameworks to talk about the natural world. We can view it through the lens of economy, of physics, of morality, of art. We can empathize certain aspects of it while abstracting others away. We can take a single set of facts, and spin innumerable different stories out of them, without even omitting or embellishing any of them — simply by playing with emphases.\n\nThe same ground-truth reality can be comprehensively described in many different ways, simply by applying different conceptual frameworks. If humans were ideal reasoners, the choice of framework or narrative wouldn't matter — we would extract the ground-truth facts from the semantics, and reach the conclusion were always going to reach.\n\nWe are not, however, ideal reasoners. What spin we give to the facts *matters*.\n\n[The classical example](https://en.wikipedia.org/wiki/Framing_effect_(psychology)) goes as follows:\n\n> *Participants were asked to choose between two treatments for 600 people affected by a deadly disease. Treatment A was predicted to result in 400 deaths, whereas treatment B had a 33% chance that no one would die but a 66% chance that everyone would die. This choice was then presented to participants either with positive framing, i.e. how many people would live, or with negative framing, i.e. how many people would die.*\n> \n> <table><tbody><tr><td><strong>Framing</strong></td><td><strong>Treatment A</strong></td><td><strong>Treatment B</strong></td></tr><tr><td><strong>Positive</strong></td><td>\"Saves 200 lives\"</td><td>\"A 33% chance of saving all 600 people, 66% possibility of saving no one.\"</td></tr><tr><td><strong>Negative</strong></td><td>\"400 people will die\"</td><td>\"A 33% chance that no people will die, 66% probability that all 600 will die.\"</td></tr></tbody></table>\n> \n> *Treatment A was chosen by 72% of participants when it was presented with positive framing (\"saves 200 lives\") dropping to 22% when the same choice was presented with negative framing (\"400 people will die\").*\n\nAs another example, we can imagine two descriptions of an island — one that waxes rhapsodic on its picturesque landscapes, and one that dryly lists the island's contents in terms of their industrial uses. One would imagine that reading one or the other would have different effects on the reader's desire to harvest that island, even if both descriptions communicated the exact same set of facts.\n\nMore salient examples exist in the worlds of journalism and politics — these industries have developed advanced tools for [telling any story in a way that advances the speaker's agenda](https://astralcodexten.substack.com/p/too-good-to-check-a-play-in-three).\n\nFundamentally, *language matters*. The way you speak, the conceptual handles you use, the facts you empathize and the story you tell, have social connotations that go beyond the literal truths of your statements.\n\nAnd the AGI frame is, bluntly, *a bad one*. To those outside our circles, to anyone not feeling charitable, it communicates *detachment from reality*, *fantastical thinking*, *overhyping*, *low status*.\n\nOn top of that, [framing has disproportionate effects on people with domain knowledge](https://forum.effectivealtruism.org/posts/re6FsKPgbFgZ5QeJj/effective-strategies-for-changing-public-opinion-a). Trying to convince a professional of something while using a bad frame is a twice-doomed endeavor.\n\n* * *\n\nWhat Frame Do We Want?\n----------------------\n\n> *\\[Successful policies\\] allow people to continue to pretend to be trying to get the thing they want to pretend to want while actually getting more other things they actually want even if they can deny it.* — [Robin Hanson](https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/)\n\nWe don't *have* to use the AGI frame, I would argue. If the problem is with specific terms, such as \"intelligence\" and \"AGI\", we can start by [tabooing them](https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words) and other \"agenty\" terms, then seeing what convincing arguments we can come up with under these restrictions.\n\nMore broadly, we can *repackage* our arguments using a different conceptual framework — the way a poetic description of an island could be translated into utilitarian terms to advance the cause of resource-extraction. We simply have to look for a suitable one.  (I'll describe a concrete approach I consider promising in the next section.)\n\nWhat we need is a frame of argumentation that is, at once:\n\n*   Robust. It isn't a lie or mischaracterization, and wouldn't fall apart under minimal scrutiny. It is, fundamentally, a *valid way* to discuss what we're currently calling \"the AI Risk\".\n*   Respectable. Being seen acting on it doesn't cost people social points, and indeed, grants them social points. (Alternatively, *not* acting on it once it's been made common knowledge *costs* social points.)\n*   Safety-promoting. It causes people/companies to act in ways that reduce the AI Risk.\n\nAlso, as Rob [notes](https://www.lesswrong.com/posts/Rkxj7TFxhbm59AKJh/the-inordinately-slow-spread-of-good-agi-conversations-in-ml):\n\n> Info about AGI propagates too slowly through the field, because when one ML person updates, they usually don't loudly share their update with all their peers. \\[...\\] On a gut level, they see that they have no institutional home and no super-widely-shared 'this is a virtuous and respectable way to do science' narrative.\n\nBy implication, there's a fair number of AI researchers who are \"sold\" on the AI Risk, but who can't publicly act on that belief because it'd have personal costs they're not willing to pay. Finding a frame that would be beneficial to be seen supporting would flip that dynamic: it would allow them to rally behind it, solve the coordination problem.\n\n* * *\n\nPotential Candidate\n-------------------\n\n(I suggest taking the time to think about the problem on your own, before I potentially bias you.)\n\nIt seems that any effective framing would need to talk about AI systems as about volitionless *mechanisms*, not agents. From that, a framework naturally offers itself: software products and integrity thereof.\n\nIt's certainly a valid way to look at the problem. AI models *are* software, and they're used for the same tasks mundane software is. More parallels:\n\n*   Modern large software is often an incomprehensible mess of code, and we barely understand how it works — much like ML models.\n*   This incomprehensibility gives rise to wide varieties of bugs and unintended behaviors, and their severity and potential for *catastrophic* failures scales with the complexity of the application.\n*   Poorly-audited software contains a lot of security vulnerabilities and instabilities. [AI](https://arxiv.org/abs/2204.06974), [as well](https://blog.openmined.org/extracting-private-data-from-a-neural-network/).\n*   Much like security, [Alignment Won't Happen By Accident](https://www.lesswrong.com/posts/Ke2ogqSEhL2KCJCNx/security-mindset-lessons-from-20-years-of-software-security#Alignment_Won_t_Happen_By_Accident).\n*   [Do What I Mean](https://en.wikipedia.org/wiki/DWIM) is the equivalent of the AI control problem: [how can we tell the program what we really want](https://www.lesswrong.com/posts/42YykiTqtGMyJAjDM/alignment-as-translation), instead of what we technically programmed it to do?\n\nMost people would agree that putting a program that was never code-audited and couldn't be bug-fixed in charge of critical infrastructure is madness. That, at least, should be a \"respectable\" way to argue for the importance of interpretability research, and the foolishness of putting ML systems in control of anything important.\n\nMind, \"respectable\" doesn't mean \"popular\" — software security/reliability isn't exactly most companies' or users' top priority. But it's certainly viewed with more respect than the AI Risk. If we argued that integrity is *especially* important with regards to *this particular software industry*, we might get somewhere.\n\nIt wouldn't be smooth sailing, even then. We'd need to continuously argue that fixing \"bugs\" only after a failure has occurred \"in the wild\" is lethally irresponsible, and there would always be people trying to lower the standards for interpretability. But that should be relatively straightforward to oppose.\n\nThis much success would already be good. It would motivate companies that plan to use AI commercially to invest in interpretability, and make interpretability-focused research & careers more prestigious.\n\nIt wouldn't decisively address the real issue, however: AI labs conducting in-house experiments with large ML models. Some non-trivial work would need to be done to expand the frame — perhaps developing a suite of arguments where sufficiently powerful \"glitches\" could \"spill over\" in the environment. Making allusions to nuclear power and pollution, and borrowing some language from these subjects, might be a good way to start on that.\n\nThere would be some difficulties in talking about concrete scenarios, since they often involve [AI models acting in unmistakably intelligent ways](https://www.lesswrong.com/posts/a5e9arCnbDac9Doig/it-looks-like-you-re-trying-to-take-over-the-world). But, for example, [Paul Christiano's story](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like) would work with minimal adjustments, since the main \"vehicle of agency\" there is human economy.\n\nTo further ameliorate this problem, we can also imagine rolling out our arguments *in stages*. First, we may popularize the straightforward \"AI as software\" case that argues for interpretability and control of deployed models, as above. Then, once the language we use has been accepted as respectable and we've expanded the Overton Window such, we may extrapolate, and discuss concrete examples that involve AI models exhibiting agenty behaviors. If we have sufficient momentum, they should be accepted as natural extensions of established arguments, instead of instinctively dismissed.",
      "plaintextDescription": "Follow-up to: Reshaping the AI Industry: Straightforward Appeals to Insiders\n\n----------------------------------------\n\n\nIntroduction\nThe central issue with convincing people of the AI Risk is that the arguments for it are not respectable. In the public consciousness, the well's been poisoned by media, which relegated AGI to the domain of science fiction. In the technical circles, the AI Winter is to blame — there's a stigma against expecting AGI in the short term, because the field's been burned in the past.\n\nAs such, being seen taking the AI Risk seriously is bad for your status. It wouldn't advance your career, it wouldn't receive popular support or peer support, it wouldn't get you funding or an in with powerful entities. It would waste your time, if not mark you as a weirdo.\n\nThe problem, I would argue, lies only partly in the meat of the argument. Certainly, the very act of curtailing the AI capabilities research would step on some organizations' toes, and mess with people's careers. Some of the resistance is undoubtedly motivated by these considerations.\n\nIt's not, however, the whole story. If it were, we could've expected widespread public support, and political support from institutions which would be hurt by AI proliferation.\n\nA large part of the problem lies in the framing of the arguments. The specific concept of AGI and risks thereof is politically poisonous, parsed as fictional nonsense or a social faux pas. And yet this is exactly what we reach for when arguing our cause. We talk about superintelligent entities worming their way out of boxes, make analogies to human superiority over animals and our escape from evolutionary pressures, extrapolate to a new digital species waging war on humanity.\n\nThat sort of talk is not popular with anyone. The very shape it takes, the social signals it sends, dooms it to failure.\n\nCan we talk about something else instead? Can we reframe our arguments?\n\n----------------------------------------\n\n\nThe Power of Framing\nHu",
      "wordCount": 1676
    },
    "tags": [
      {
        "_id": "6zBEfFYJxhSEcchbR",
        "name": "AI Alignment Fieldbuilding",
        "slug": "ai-alignment-fieldbuilding"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bbtLG3LNGGBWzXqjK",
    "title": "Is This Thing Sentient, Y/N?",
    "slug": "is-this-thing-sentient-y-n",
    "url": null,
    "baseScore": 4,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2022-06-20T18:37:59.380Z",
    "contents": {
      "markdown": "**Builds up on:** *Consciousness and the Brain* by Stanislas Dehaene. Good summaries may be found [here](https://www.lesswrong.com/posts/x4n4jcoDP7xh5LWLq/book-summary-consciousness-and-the-brain) or [here](https://astralcodexten.substack.com/p/your-book-review-consciousness-and?s=w), though reading them is not strictly necessary.\n\n**Synopsis:** I claim to describe the exact mental structure that allows qualia.\n\n* * *\n\nBackground: What Is Consciousness?\n----------------------------------\n\nDehaene's *Consciousness and the Brain* rigorously differentiates conscious and unconscious activity. Consciousness, the book suggests, is correlated with events where the brain gathers all of its probability distributions about the world, and samples from them to build a consistent unitary world-model, on which it then acts. The experiments show that this is necessary for multi-step calculation, abstract thinking, and reasoning over agglomerations of distant sensory inputs.\n\nHowever, the book's definition of consciousness is *not* a synonym for self-awareness. Rather, I would term the phenomenon it picks out as \"moments of agency\": as the state in which the mind can devise goal-oriented plans using a well-formed world-model, and engage in proper consequenialist reasoning. Outside those moments, it's just a bundle of automatic heuristics.\n\nSelf-awareness, I suspect, is *part* of these moments-of-agency in humans, but isn't the same thing generally. Just having Dehaene!consciousness isn't a sufficient condition for self-awareness: there's something on top of that going on.\n\n* * *\n\nWhat Is Self-Awareness?\n-----------------------\n\nWhat do we expect to happen to an agent the moment it attains self-awareness, in the sense of perceiving itself to have qualia?\n\nWhy, it would start perceiving qualia — the keyword being *perceive*. It would start acting like it receives some sort of feedback from a novel sense, not unlike sight. Getting data about how it's like to be a thing like itself.\n\nLet's suppose that it's nothing magical — that there isn't a species of etheric parasites which attach themselves to any sufficiently advanced engine of cognition and start making it hallucinate. Neither are qualia \"emergent\" — as if, if you formally wrote out an algorithm for general reasoning, that algorithm would spontaneously rewrite itself to be having these imaginary experiences. If self-awareness is as mundane as any other sense, then what internal mechanism would we expect to correspond to it?\n\nWhen we \"see\", what happens is: some sort of ground-truth data enter a specialized sensory organ, that organ transmits the information to the brain, the brain parses it, and offers it to our conscious inspection[^zekergpewef], so we may account for it in planning.\n\nIf we view qualia as sense-data, it follows that they'd be processed along a similar pathway.\n\n*   What are the ground-truth data corresponding to qualia? The current internal state of this agent. The inputs it's processing, the configuration its world-model is in, its working-memory cache, the setting it's operating in, the suite of currently active processes...\n*   What is the specialized sensory organ, and how does it communicate with the brain? Despite what may seem, we do need one. The ground-truth state of the brain isn't by default \"known\" by the brain itself; it's just *in* that state. A specialized mechanism needs to know how to summarize raw brain-states into reports, then pool them together with other information about the world.\n*   How are the qualia-data interpreted? Much like visual information, they're parsed as the snapshot of all information received by the sensory organ at a particular moment. A self-model; a summary of how it's like to be you, perceiving what you perceive and feeling what you feel.\n    *   (In theory, that should cause infinite recursion. A faithful self-model also has a self-model: you can consider what it's like to be someone who experiences being a someone. But seeing as we're bounded agents, I assume that algorithm is [lazy](https://en.wikipedia.org/wiki/Lazy_evaluation).)\n\nLet's make the distinction sharper. An agent gets hurt, information about that travels to the brain, where it's interpreted as \"pain\". Pain has the following effects:\n\n1.  It updates the inner planner away from plans that cause the agent harm, like an NN getting high loss.\n2.  It changes the current plan-making regime: the planner is incentivized to make plans in a hurried manner, and to consider more extreme options, so as to get out of the dangerous situation faster.\n\nWhich parts of that correspond to pain-qualia?\n\nNone.\n\nPain-qualia are not pain inflicting changes upon a planner: by themselves, these changes are introduced *outside* the planner's purview. Consider taking an inert neural network, manually rewriting its weights, then running it. It couldn't have possibly \"felt\" that change, except by magic; it was simply *changed*. For a change to be felt, you need an outer loop that'd record your actions, then present the records to the NN.\n\nThat's what pain-qualia are: summaries of the effects pain has upon the planner that are fed as input to that very planner.\n\nThat leaves one question:\n\nHow Is It Useful?\n-----------------\n\nWell, self-awareness evolved to be offered as an input to the planner-part, so it must be used by the planner-part somehow. The obvious answer seems correct here: meta-planning.\n\nFirst, self-awareness allows an agent to account for altered states of consciousness. If it knows it's deliriously happy, or sad, or drugged, it'll know that it's biased towards certain actions or plans over others, and that these situational biases may not be desirable. So it'll know to correct its behavior to suit. (Note that mere awareness of an altered state is insufficient: feedback needs to be detailed enough to allow that course-correction.)\n\nSecond, it allows it to predict in detail its *future* plan-making instances. What, given a plan, its future selves would want to do at various stages of that plan, and how capable they'll be of doing this.\n\nConcretely, self-awareness is what allows to:\n\n*   Know not to lash out while angry, even if it feels *right* and *sensible* in the moment, because you know your plan-making process is compromised.\n*   Know that a plan which hinges on your ability to solve highly challenging mathematical problems while getting your arm chopped off is a doomed one.\n*   Know not to commit to an exciting-seeming project too soon, because you know from past experience that your interest will wane.\n\nTo be clear, those are just examples; what we want is the ability to display such meta-planning *universally*. We can imagine an animal that instinctively shies away from certain drastic actions while angry, but the actual requirement is the ability to do that in off-distribution contexts in a zero-shot regime.\n\nOn that note... I'll abstain from strong statements on whether various animals actually have self-models complex enough to be morally relevant. I *suspect*, however, that almost no-one's planning algorithms are advanced enough to make good use of qualia — and evolution would not grant them senses they can't use. In particular, this capability implies high trust placed by evolution in the planner-part: that sometimes it may know better than the built-in instincts, and should have the ability to plan around them.\n\nBut I'm pushing back against [this sort of argument](https://theunitofcaring.tumblr.com/post/151442936096/a-question-asked-out-of-genuine-ignorance-is). As I've described, a mind in pain does not necessarily *experience* that pain. The capacity to have qualia of pain corresponds to a specific mental process where the effect of pain on the agent is picked up by a specialized \"sensory apparatus\" and re-fed as input to the planning module within that agent. This, on a very concrete level, is what having internal experience *means*. Just track the information flows!\n\nAnd it's entirely possible for a mind to simply lack that sensory apparatus.\n\nAs such, in terms of empirical tests for sentience, the thing to look for isn't whether something looks like it experiences emotions. It's whether, while plan-making, that agent can reason about its own behavior in different emotional states.\n\n* * *\n\nMiscellanea\n-----------\n\n**1\\.** ***Cogito, Ergo Sum*****.** It's easy to formalize. As per Dehaene, all of the inferences the brain makes about the outside world are probabilistic. When presented to the planner, they would be appropriately tagged with their probability estimates. The one exception would be information about the brain's own continued functioning: it would be tagged \"confidence 1\". After all, the only way for the self-awareness mechanism to become compromised involves severe damage to the brain's internals, which is probably fatal. So evolution never had cause to program us to doubt it.\n\nAnd that's why we go around slipping into solipsism.\n\n**2\\. \"Here's a simple program that tracks its own state. Is it sentient?\"** No. It needs to be an *agent*.\n\n**3\\. The Hard Problem of Consciousness.** None of the above seems to address the real question: why does self-awareness seem so... *metaphysically different* from the rest of the universe? Or, phrased more tractably: \"Why does a mind that implements the self-awareness mechanism start viewing self-aware processes as being qualitatively different, compared to other matter? And gets so confused about it?\"\n\nI'm afraid I don't have a complete answer to that, as I'm having some trouble staring at the thing myself. I feel confident, though, that whatever it is, it wouldn't invalidate anything I wrote above.[^t8q57qhdh1] I suspect it's a combination of two things:\n\n*   *Cogito, ergo sum*. Our existence feels qualitatively different because it's the only thing to which the mind assigns absolute confidence.\n*   A quirk of our conceptual vocabulary. It's not that self-aware things have some metaphysically special component, it's that there's an irreducible concept native *to our minds* that sharply differentiates them, so things we imagine as sentient feel qualitatively different *to us*.\n\nAnd this \"qualia\" concept has some really confusing properties. It's basically defined by \"this thing has first-person experiences, just like me\". Yet we can imagine rocks to have qualia, and rocks definitely don't share *any* of our algorithmic machinery, especially the self-awareness machinery. At the same time, we can imagine entities that share all of our algorithms — p-zombies — which somehow *lack* qualia.\n\nWhy is that so? What is the *purpose* of this distinct \"qualia\" concept? Why are we allowed to use it so incoherently?\n\nI'm not sure, but I *suspect* that it's a short-hand for \"has inherent moral relevance\". It's not tied to \"is self-aware\" because evolution wanted us to be able to dehumanize criminals and members of competing tribes: view them as beasts, soulless barbarians. So the concept is decoupled from its definition, which means we can imagine incoherent states where things that have what we define as \"qualia\" don't have qualia, and vice versa.\n\nIt's not a particularly satisfying answer, granted. But what's the alternative here? If this magical-feeling \"first-person perspective\" can act on the world, it has to be implemented within the world. The self-awareness as I described it seems to suffice to describe all characteristics of having qualia, and the purpose of our capability for self-awareness; it just fails to validate our feeling that qualia are metaphysically significant. But if we can't accept \"they're not metaphysically significant, here's how they're implemented and why they *falsely* feel metaphysically significant\", then we've decided to reject *any* answer except \"here's a new metaphysics, turns out there really are etheric parasites making us hallucinate!\".\n\nMaybe my *specific* answer, \"qualia-having is a signifier for moral relevance\", isn't exactly right; and indeed, it doesn't quite feel right to me. But whatever the real answer is, I expect it to look very similar, and to similarly dismiss the \"hard problem\" as lacking substance.\n\n**4\\. Moral Implications.** If you accept the above, then the whole \"qualia\" debacle is just a massive red herring caused by the idiosyncrasies of our mental architecture. What does that imply for ethics?\n\nWell, that's simple: we just have to re-connect the free-floating \"qualia\" concept with the *definition* of qualia. We value things that have first-person experiences similar to ours. Hence, we have to isolate the algorithms that allow things to have first-person experiences like ours, then assign things like that moral relevance, and dismiss the moral relevance of everything else.\n\nAnd with there not being some additional \"magical fluid\" that can confer moral relevance to a bundle of matter, we can rest assured there won't be any *shocking twists* where puddles turn out to have been important this entire time.\n\n[^zekergpewef]: In the Dehaene sense: i. e., the data are pooled together and consolidated into a world-model which is fed as input to the planning algorithm. \n\n[^t8q57qhdh1]: Unless it really are etheric parasites or something. I mean, it might be!",
      "plaintextDescription": "Builds up on: Consciousness and the Brain by Stanislas Dehaene. Good summaries may be found here or here, though reading them is not strictly necessary.\n\nSynopsis: I claim to describe the exact mental structure that allows qualia.\n\n----------------------------------------\n\n\nBackground: What Is Consciousness?\nDehaene's Consciousness and the Brain rigorously differentiates conscious and unconscious activity. Consciousness, the book suggests, is correlated with events where the brain gathers all of its probability distributions about the world, and samples from them to build a consistent unitary world-model, on which it then acts. The experiments show that this is necessary for multi-step calculation, abstract thinking, and reasoning over agglomerations of distant sensory inputs.\n\nHowever, the book's definition of consciousness is not a synonym for self-awareness. Rather, I would term the phenomenon it picks out as \"moments of agency\": as the state in which the mind can devise goal-oriented plans using a well-formed world-model, and engage in proper consequenialist reasoning. Outside those moments, it's just a bundle of automatic heuristics.\n\nSelf-awareness, I suspect, is part of these moments-of-agency in humans, but isn't the same thing generally. Just having Dehaene!consciousness isn't a sufficient condition for self-awareness: there's something on top of that going on.\n\n----------------------------------------\n\n\nWhat Is Self-Awareness?\nWhat do we expect to happen to an agent the moment it attains self-awareness, in the sense of perceiving itself to have qualia?\n\nWhy, it would start perceiving qualia — the keyword being perceive. It would start acting like it receives some sort of feedback from a novel sense, not unlike sight. Getting data about how it's like to be a thing like itself.\n\nLet's suppose that it's nothing magical — that there isn't a species of etheric parasites which attach themselves to any sufficiently advanced engine of cognition and start making it",
      "wordCount": 2024
    },
    "tags": [
      {
        "_id": "XSryTypw5Hszpa4TS",
        "name": "Consciousness",
        "slug": "consciousness"
      },
      {
        "_id": "Wi3EopKJ2aNdtxSWg",
        "name": "Neuroscience",
        "slug": "neuroscience"
      },
      {
        "_id": "8e9e8fzXuW5gGBS3F",
        "name": "Qualia",
        "slug": "qualia"
      },
      {
        "_id": "KqrZ5sDEyHm6JaaKp",
        "name": "The Hard Problem of Consciousness",
        "slug": "the-hard-problem-of-consciousness"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "a3LncviZ6rkrTo8jJ",
    "title": "The Unified Theory of Normative Ethics",
    "slug": "the-unified-theory-of-normative-ethics",
    "url": null,
    "baseScore": 8,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2022-06-17T19:55:19.588Z",
    "contents": {
      "markdown": "**Epistemic status:** Reframing of a trivial result, followed by a more controversial assertion.\n\n**Summary:** For bounded agents acting in complex environments, using raw consequentialist reasoning for every moment-to-moment decision is prohibitively costly/time-inefficient. A rich suite of heuristics/approximations may be developed to supplement it, recording the correspondences between certain classes of actions and the consequences actions from that class robustly tend to cause. Instincts, traditions, and moral inclinations/deontology are well-explained by this: an agent that started out as a pure [environment-based consequentialist](https://www.lesswrong.com/posts/LBNjeGaJZw7QdybMw/agents-over-cartesian-world-models#Consequential_Types) would end up deriving them *ex nihilo*.\n\nI use this to argue that humans are pure consequentialists over environment-states: we don't have real terminal preferences for some actions over others.\n\n* * *\n\nBootstrapping\n-------------\n\nSuppose you're an agent with an utility function over world-states. You exist in a complex environment populated by other agents, whose exact utility functions are varied and unknown. You would like to maneuver the environment into a high-utility state, and prevent adversarial agents from moving it into low-utility states. How do you do that?\n\nConsequentialism is the natural, fundamental answer. At every time-step, you evaluate the entire array of actions available to you, then pick the action that's part of the sequence most likely to move the world to the highest-value state.\n\nThat, however, is *a lot* of computation. You have to consider what actions you can in fact take, their long-term terminal effects, their instrumental effects (i. e., how much it expands or constraints your future action-space), their acausal effects, and so on.\n\nEvery time-step.\n\nAnd you *have* to run that computation every time-step, because you're [an embedded agent](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh): at any given time your model of the world is incomplete, and every time-step you get new information you must update on. You can't pre-compute the optimal path through the environment, you have to continuously re-plot it.\n\nIf you're a bounded agent with limited computational capabilities, all of this would take time. Agents who can think quicker would out-compete those who think slower. (As a trivial example, they'd win in a knife fight.) Therefore, it'd be beneficial to find faster algorithms for computing the consequences, to replace or supplement \"raw\" consequentialism.\n\nIs there a way to do that?\n\nSuppose the environment is relatively stable across time and space. From day to day, you interact with mostly the same types of objects (trees, rocks, animals, tribesmen), and they maintain the same properties in different locations and situations.\n\nIn that case, you can abstract over actions. You can notice that all action of a certain kind tend to increase your future action-space (such as \"stockpiling food\" or \"improving your reputation among the tribe\") or provide direct utility (\"eating tasty food\"), whereas others tend to constrain it (such as \"openly stealing\") or provide direct dis-utility (\"getting punched\"). These actions tend to have these effects regardless of the specifics — whether they happen at day or at night, in a cave or in the forest.\n\nMoreover, you can notice that some actions have the same effects *independent of the agent executing them*. When someone brings in a lot of food and donates it to the common tribal stockpile, that's good. When someone protects you from being brained with a club, that's good. When someone screws up a stealth check and gets a hunt party killed by a predator, that's bad.\n\nWhen you murder a rival, that's good, but only myopically. Probabilistically, you're not going to get away with it long-term, which may get you killed or exiled or at least tank your reputation. Similarly, it's statistically bad if someone else in the tribe murders, because that a) decreases the tribe's manpower, b) it might be one of your friends, c) it might be you. Therefore, murder is bad.\n\nThese agent-agnostic actions are particularly useful. If your fellow agents are sufficiently similar to you architecture-wise, you can expect them to arrive at the same conclusions regarding which actions are agent-agnostically good and which are bad. That means you'd all commit to the same timeless policy, and know that everyone else has committed to it, and know everyone knows that, etc. Basically, you'll never have to think about some kinds of acausal effects again!\n\nAt the end, you end up with a rich pile of heuristics that are well-adapted for the environment you're in. They're massively faster than raw consequentialism, and approximate it well.\n\nCan you do even better?\n\nSuppose there are agents you interact with frequently (your tribesmen). Necessarily, you develop detailed models of their thought patterns and utility functions. You can then abstract over those as well, break all possible minds into sets of (personality) traits. These traits correspond to the types of behavior you can expect from these agents. (Cowardice, selfishness, bravery, cleverness, creativity, dishonesty...)\n\nOnce you did that, you can use *even quicker* computational shortcuts. Instead of evaluating actions against your heuristics framework, let alone trying to directly track their consequences, you can simply check your annotated model of the *author* of a given action. Is whoever executes it a highly trustworthy and competent agent with a compatible-with-yours utility function? Or a fool with a bad reputation?\n\nDepending on the answer, you might not even need any additional information to confidently predict whether that action moves the world to a high-utility or a low-utility state. Whatever *this person* does is likely good, no matter what they do. Whatever *that person* does is likely bad.\n\nWhen everything is working as intended, you use these higher-level models most of the time, and occasionally dip down to lower levels to check that they're still working as intended. (Is this person actually good, or is just tricking me with good rationalizations? Does this kind of action actually have a track record of good consequences?)\n\nAnd, of course, you use lower-level abstractions for off-distribution things: novel types of actions you've never seen before and whose consequences you have to explicitly track, or new agents you're unfamiliar with.\n\nThe problems appear when you start to forget to do that. When you assume that certain actions are inherently bad, even if the environment changes and the correspondence between them and bad outcomes breaks down (entrenched traditions). Or when you assume that some person is inherently good, and [the things they do are good *because* they did them](https://www.smbc-comics.com/comic/groups).\n\n* * *\n\nRecapping\n---------\n\n*   In sufficiently complex environments, using pure consequentialist reasoning is computationally costly.\n*   If the environment is sufficiently stable, you can approximate consequentialism using algorithmically efficient shortcuts.\n*   Given an environment, you can pick out types of actions that stably correspond to good or bad outcomes in it, then directly use these heuristics to make and evaluate decisions.\n*   Given agents you know to be aligned-and-competent or adversarial, you can just know to support/oppose anything they do without expending resource on closer evaluation, because whatever they do stably corresponds to good/bad outcomes.\n*   Higher-level models need to be checked by lower-level methods once in a while, to ensure that the correspondences didn't break down/weren't false to begin with.\n\nYou can also view it in terms of [natural abstractions](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks). For some actions, you can throw away most of the contextual information (the when, the where, the who) and still predict their consequences correctly most of the time. For some agents, you can throw away all information about the actions they execute except the who, and still predict correctly whether the consequences would be good or bad.\n\nThis framework explains a lot of things. Notably, the normative ethical theories: deontology and virtue ethics are little but parts of the efficient implementation of consequentialism for bounded agents in sufficiently stable environments. Instincts and traditions too, of course.\n\n* * *\n\nApplying\n--------\n\nThere's a great deal of confusion regarding the best way to model humans. One particular point of contention is whether we can be viewed as utility-maximizers over real-world outcomes. Deontological inclinations, instincts, and habits are often seen as complications/counter-arguments. Some humans \"value\" being honest and cooperative — but those aren't \"outcomes over the environment\", except in some very convoluted sense!\n\n[Act-based agents](https://www.lesswrong.com/posts/LBNjeGaJZw7QdybMw/agents-over-cartesian-world-models#Consequential_Types) is an attempt to formalize this. Such agents have utility functions defined over their actions instead of environment-states, and are a good way to model idealized deontologists. It seems natural to suggest that humans, too, have an act-based component in their utility functions.\n\nI suggest an alternate view: human preferences for certain kinds of actions over others are purely instrumental, not terminal.\n\nIn the story I've outlined, we've started with a pure environment-based consequentialist agent. That agent then computed the appropriate instincts and norms and social standards for the environment it found itself in, and preferred acting on them. But it's never modified its purely environment-based utility function in the meantime!\n\nSimilar, I argue, is the case with humans. We don't actually \"value\" being honest and cooperative. We act honest and cooperative because we expect actions with these properties to more reliably lead to good outcomes — because they're useful heuristics that assist us in arriving at high-utility world-states.\n\nWhen we imagine these high-utility world-states — a brilliant future full of prosperity, or personal success in life — we imagine the people in them to be honest and cooperative too. But that's not because we value these things inherently. It's because that's our *prediction* of how agents in a good future would need to act, for that future to be good. We expect certain heuristics to hold.\n\nOne clue here is that the various preferences over actions don't seem to be stable under reflection*.* If they conflict with consequentialism-over-outcomes, they end up overridden every time. The ones we keep around — like the norm against murder — we keep around because we agree that it's a good heuristic. The ones where the correspondence broke down — like an outdated tradition, or a primal instinct — we reject or suppress.\n\nThere are some complications arising from the fact that with humans, it happened the other way around from what I'd described. Intelligence *started out* as a bunch of heuristics, then generality was developed on top of them. This led to us having some hard-coded instincts and biases that are very poorly adapted for our modern environment.\n\nIt also added-in reward signals, where we feel good for employing heuristics we consider good. This muddles the picture even more: when we imagine a bright future, we *have* to specify that in it, we'd be allowed to keep our instincts and deeply-internalized heuristics satisfied, as to be happy. But the actual outcome we're aiming for there is that *happiness*, not our execution of certain actions.\n\nAs such: humans are well-described as pure consequentialists over environment-states.",
      "plaintextDescription": "Epistemic status: Reframing of a trivial result, followed by a more controversial assertion.\n\nSummary: For bounded agents acting in complex environments, using raw consequentialist reasoning for every moment-to-moment decision is prohibitively costly/time-inefficient. A rich suite of heuristics/approximations may be developed to supplement it, recording the correspondences between certain classes of actions and the consequences actions from that class robustly tend to cause. Instincts, traditions, and moral inclinations/deontology are well-explained by this: an agent that started out as a pure environment-based consequentialist would end up deriving them ex nihilo.\n\nI use this to argue that humans are pure consequentialists over environment-states: we don't have real terminal preferences for some actions over others.\n\n----------------------------------------\n\n\nBootstrapping\nSuppose you're an agent with an utility function over world-states. You exist in a complex environment populated by other agents, whose exact utility functions are varied and unknown. You would like to maneuver the environment into a high-utility state, and prevent adversarial agents from moving it into low-utility states. How do you do that?\n\nConsequentialism is the natural, fundamental answer. At every time-step, you evaluate the entire array of actions available to you, then pick the action that's part of the sequence most likely to move the world to the highest-value state.\n\nThat, however, is a lot of computation. You have to consider what actions you can in fact take, their long-term terminal effects, their instrumental effects (i. e., how much it expands or constraints your future action-space), their acausal effects, and so on.\n\nEvery time-step.\n\nAnd you have to run that computation every time-step, because you're an embedded agent: at any given time your model of the world is incomplete, and every time-step you get new information you must update on. You can't pre-compute the optimal path",
      "wordCount": 1733
    },
    "tags": [
      {
        "_id": "ZTRNmvQGgoYiymYnq",
        "name": "Consequentialism",
        "slug": "consequentialism"
      },
      {
        "_id": "yeJFqsWrP2pjYfNEr",
        "name": "Deontology",
        "slug": "deontology"
      },
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "xknvtHwqvqhwahW8Q",
        "name": "Human Values",
        "slug": "human-values"
      },
      {
        "_id": "HAFdXkW4YW4KRe2Gx",
        "name": "Utility Functions",
        "slug": "utility-functions"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4pRPmFSfCLvKGEnFx",
    "title": "Towards Gears-Level Understanding of Agency",
    "slug": "towards-gears-level-understanding-of-agency",
    "url": null,
    "baseScore": 25,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2022-06-16T22:00:17.165Z",
    "contents": {
      "markdown": "**Epistemic status:** Highly speculative, much of this is probably wrong.\n\n**Summary:** [The \"centerpiece\"](https://www.lesswrong.com/posts/4pRPmFSfCLvKGEnFx/a-gears-level-model-of-agency#2__How_Is_Agency_Formed_) of this post is my attempt to describe how a full-fledged consequentialist agent might incrementally grow out of a bundle of heuristics, aided by SGD or evolution. The story's detail are likely very wrong. It's informed by a few insights that I'm more confident in, however, and suggests some interesting [variables](https://www.lesswrong.com/posts/4pRPmFSfCLvKGEnFx/a-gears-level-model-of-agency#3__The_Variables_of_Agency) and [developmental milestones](https://www.lesswrong.com/posts/4pRPmFSfCLvKGEnFx/a-gears-level-model-of-agency#5__Developmental_Milestones) of agency. Some specific claims:\n\n*   Operating in a vast well-abstracting environment requires the ability to navigate arbitrary mathematical contexts.\n*   That requires the ability to *represent* any mathematical environment internally.\n*   In turn, this is possible if the agent possesses a [functionally complete](https://en.wikipedia.org/wiki/Functional_completeness) set of \"mental primitives\" — hard-coded conceptual building blocks. These primitives may be put together to define any object.\n    *   For memory-efficiency reasons, such composite objects are then abstracted-over via [an algorithm like this](https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information). Their internal structure is wiped, only the externally-relevant properties are kept.\n    *   If the algorithm is imperfect, the resultant abstractions might be \"leaky\": carrying the traces of the primitives they're made of, or plain flawed.\n    *   This creates severe problems in case of ontology crises: if the agent tries to translate its goal from one environment to another, redefining it and imperfectly abstracting over its re-definition.\n*   Agency has the tendency to \"take over\" the neural network it appears in.\n    *   It may start out as a minor and niche algorithm for context-learning/aid to built-in heuristics.\n    *   Gradually (across generations/batches), it would expand its influence, holding more sway over the network's decisions.\n    *   Eventually, it would maneuver the network into off-distribution environments where it hopelessly outperforms static heuristics, and eventually fully dominate them.\n*   Fundamentally, agency is little more than a menagerie of meta-heuristics: heuristics for best ways to train new heuristics, *including* better meta-heuristics.\n    *   In humans, the recursive self-improvement this implies is constrained by the static size of working memory + the fact that we can't modify the more basic features of our minds (like the mental primitives or the abstraction-building algorithm). \n\n* * *\n\n0\\. Background\n--------------\n\nIn [a previous post](https://www.lesswrong.com/posts/n2urKnXbevj2ryvGY/agency-as-a-natural-abstraction), I've made a couple relevant claims that I'm mostly confident in. I'll reframe and clarify them here.\n\n### 0A. Universal Context-Learning → Agency\n\nI'd argued that sufficiently advanced meta-learning is inseparable from agency/utility-maximization. I can distill the argument this way:\n\n*   Suppose that we have an AI system with the ability to take any unfamiliar environment, derive its rules, and perform well in it according to some metric.\n*   To do that, it would need to have a universal definition of \"performing well\": some way to judge its performance, so it may converge towards the optimal policy.\n*   With limited domains, static heuristics and specialized algorithms (e. g., a calculator) suffice. They don't need to represent the metric internally. They can be glorified lookup tables, exhaustively mapping all possible states of their domains to outputs.\n*   But for universality, our AI system needs the ability to *map any new domain to the optimal set of heuristics for it*. It needs the ability to replicate the work of producing the aforementioned heuristics/spec-algorithms.\n*   In other words, it needs to be an optimizer: it needs to contain an inner optimization loop. Such a loop would have some metric it maximizes.\n*   Whatever metric the system uses is its utility function, and lo, we have a utility-maximizer.\n\n### 0B. Why Is Universality Useful?\n\nWhy would we expect all sufficiently powerful AI models to converge towards universality? At face value, this seems unnecessary, and even *wasteful*. Can't they just develop some heuristics for the specific domain they were trained for, like math, and be satisfied with that? Why drag in the likely-complex machinery for universality?\n\nWell, they can, up to a certain level of domain complexity.\n\nBut consider the difficulties with modelling macro-scale objects using the heuristics for interactions between fundamental particles. It's completely computationally intractable. If you're not using approximations at all, you'd literally need a computer larger than the thing you're modelling. Larger than Earth, if you're modelling the human society.\n\nFortunately, our universe [abstracts well](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks). Fundamental particles combine into fiendishly complex structures, but you don't need to model every individual particle to predict the behavior of these structures well. Animals or stars could be thought of as whole objects, summarized by very few externally-relevant properties and rules.\n\nBut these rules are very different from the rules of fundamental particles. You can't predict humans by directly modelling them as the bundles of particles they are, *or* by applying particle-interaction rules to the behavior of human-type objects. When you build an abstraction, you define a new set of rules, and you need to be able to learn to play by them.\n\nAnd the process of building new abstractions never ends. You observe the world, you build its macro-scale model. You notice some correspondences, drill down, and discover biology. Then chemistry, then quantum physics. On a separate track entirely, you abstract things away into governments and galaxies and weather patterns. None of these objects behave in quite the same way, and there are basically no constraints on what rules a future abstraction might have.\n\nSimilar issues would arise with many narrower yet still sufficiently complex domains.\n\nWorse yet, the process of discovering new abstractions never ends even *on any given abstraction layer*. It's called [chunking](https://en.wikipedia.org/wiki/Chunking_(psychology)).\n\nConsider math. When we define a new function, we define a new abstraction. That function behaves unlike any other object (else why would we have a separate symbol for it?), and thinking about it in terms of its definition is unwieldy and memory-taxing. You derive its rules, then think of it as its own thing.\n\nConsider social heuristics. When you learn a new tell, like a certain microexpression a person might flick when they're lying, you learn a new abstraction.\n\nOr when you discover a new type of star, or a new economic dynamic, or a specific chess board state.\n\nThus, computationally efficient world-modelling in the conditions of a vast well-abstractable world requires the ability to navigate arbitrary mathematical environments.\n\nAnd to navigate arbitrary mathematical environments, you need to be an optimizer.\n\n* * *\n\n1\\. Functionally Complete Sets of Mental Primitives\n---------------------------------------------------\n\nWhat does a system need, to be universal?\n\nBy and large, we can only define new concepts in terms of the concepts we already know.[^u2qf9qv2db] When you notice a new pattern in the environment, you either understand it in terms of its constituent parts (a social group is a group of humans) or by creating a metaphor/analogy (atoms are like billiard balls).\n\nBut how does that process *start*? If you can only define new concepts in terms of known ones, how do you learn the first ones?\n\nWell, you don't. Some of them are hard-coded into your psyche at deployment. Let's call these abstractions \"mental primitives\": abstract objects that the mind starts out with, that aren't defined in terms of anything else.\n\nGeneral intelligence, then, requires *a* [*functionally complete*](https://en.wikipedia.org/wiki/Functional_completeness) *set of mental primitives*. If your \"default\" abstractions could be put together to define any mathematical object, you could define any mathematical object, and so understand any environment.\n\nWhat that starting set *is* matters little. It may be some mathematical axioms, or objects and rules in a platformer game, or some spatial and conflict-oriented concepts humans likely start with. Or even just [NAND gates](https://en.wikipedia.org/wiki/NAND_gate). Once you have that, you can bootstrap yourself to understand and model anything.\n\nIn the limit of infinite working memory, at least. Bounded agents would run into the same issue I've outlined in [0B](https://www.lesswrong.com/posts/4pRPmFSfCLvKGEnFx/towards-gears-level-understanding-of-agency#0B__Why_Is_Universality_Useful_): trying to model reality using some static set of mental primitives is unrealistic. Just consider the NAND gate example! For *any* static set, there'll be some pattern that'd require the combination of millions of mental primitives to describe.\n\nAnother trick is needed: an algorithm for building efficient models of composite objects. Such an algorithm would take a low-level composite definition, then remove all internal complexity from it, attach a high-level external summary, and store the result. John Wentworth [made some progress on the topic](https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information); I imagine our minds just do something like this when they chunk.\n\nBut that's only one piece of the puzzle. You also need the ability to fluidly manipulate these abstractions, so you may construct plans and derive new heuristics.\n\nCan we speculate how all of this might develop?\n\n* * *\n\n2\\. How Is Agency Formed?\n-------------------------\n\n***Disclaimer:** This is a detailed story of how a complex process no-one has ever observed works. It's necessarily completely wrong in every way. But hopefully the details and the order of events don't matter much. Its purpose is that of an intuition-pump: to explore how a gears-level model of agency might look like, and maybe stumble upon a few insights along the way.*\n\n### 2A. From Heuristics to Mesa-Optimization\n\n**1.** Suppose we have a RL model trained to play platformer games. It starts as a bundle of ***myopic heuristics***. Given a situation that looks like *this*, you should react like *this*. Enemy projectile flies towards you? Jump. The level's objective is to the right of you? Move right. Instincts, specialized algorithms, static look-up tables mapping concrete situations to simple actions...\n\n**2.** But suppose that the rules of the environment can change. Suppose that in the model's training distribution, the height of its character's jump varies level-to-level. So it learns to learn it: every time it's presented with a new level, it checks whether it can jump and how high, then modifies *its entire suite of movement heuristics* to account for that[^q1z3sp7ykbn].\n\nI'll call this ***trivial meta-learning***. It's trivial because the heuristics only need to be adjusted in *known* ways. The model knows jump height can vary, \"jump height\" is just a scalar, so it can have simple pre-defined algorithm for how it needs to modify its policy for any given value.\n\n**3.** Suppose the model learns a few other such variable rules, as its environment needs demand. Suppose that some of them can combine in complex ways. Say, each level, a random movement command, such as jumping, may be assigned to shoot a projectile in addition to moving the character. Suppose that levels contain fragile vases the player shouldn't break, but also enemies it needs to kill, and traps that can kill it.\n\nEffectively using the jump-and-shoot composite ability requires tailored heuristics: it's not just a trivial combination of the heuristics for shooting and jumping. The agent needs to mind not to jump near vases, and to attack enemies only if it isn't standing under a spiked ceiling.\n\nThe model can learn a separate trivial meta-learning algorithm for each such composite ability, to just memorize how it should adjust its behavior in response to every combination. But suppose there are many abilities, and *combinatorically many* their combinations, such that memorizing all of them is unfeasible.\n\nThe model would need to learn to adjust its heuristics in *unknown* ways — and here we run into the need for an inner optimization loop. For how can it know which adjustments to perform without knowing their purpose? It'd need to modify its policy to remain effective, but \"effectiveness\" can only be defined in relation to some goal. So it'll come to contain that goal, and optimize for it — or, well, for *some goal* training to which is empirically correlated with low loss. The mesa-objective.\n\nLet's call this ***live-fire re-training***. The model is already an optimizer now, but a baby one. It doesn't plan, it just experiments, then \"blindly\" does the equivalent of an SGD training step on itself.\n\n### 2B. From Mesa-Optimization to World-Models\n\n**4.** Further improvements are possible. \"Live-fire\" experimentation with a new capability is risky: you might accidentally destroy something valuable, or die. It'd be better to \"figure it out in your head\" first, approximate the new optimal policy by just thinking about what it'd be, without actually taking action.\n\nAnd that has to be possible. The model already knows how to do that for the known heuristics. While the new target heuristic isn't a *trivial* combination of the known ones, it's still a *combination* of them. The model knows when to shoot and when to jump, and when not to do either. So it should be possible to figure out most of the policy for the case where jumping and shooting is the same action.\n\nTo \"interpolate\" between the heuristics like this, however, would require some rudimentary world-model. An explicit representation of spikes and vases, and some ideas for how they interact with shooting and jumping. As long as the environment is homogenous and simple, that's easy: the previously mentioned [CoinRun agent](https://distill.pub/2020/understanding-rl-vision/) does part of that even without any need for meta-learning.\n\nThis rudimentary world-model, grown out of the tools for trivial meta-learning, would grow in complexity. Eventually, it'd represents an entire virtual environment. The model would \"imagine\" itself deployed in it, would try to win it, and if it fails according to the mesa-objective, do the equivalent of SGD on itself[^boz1ta5fu8w].\n\nNote that, at this step, the world-model needs not be situational. The agent doesn't have to build the model of its *surroundings*, it merely has to have some \"generic\" model of platformer levels, corresponding to its training distribution. A *prior* on its environment.\n\nLet's call this ***virtual re-training***.\n\n**5.** But suppose the environment is like the model's abilities: it consists of several simple objects/rules that can combine in combinatorically many complex ways, and not every combination is present in every level.[^uk4dsbpvbjh]\n\nFirst, that would require modifying its world-model at runtime in response to the combinations it sees around. Just using the prior wouldn't suffice.\n\nSecond, storing each context-learned combination explicitly might quickly get memory-taxing. It's fine if we only have first-order combinations, like \"poison + spikes\", \"enemy hiding in vase\". But what if there are second-order combinations, third-order combinations? \"Magical boss enemy adorned with poisoned spikes\" and such?\n\nTo address the first issue, the model would finalize the development of ***mental primitives***. Concepts it can plug in and out of its world-model as needed, dimensions along which it can modify that world-model. One of these primitives will likely be the model's mesa-objective.\n\nTo address the second issue, the model would learn to ***abstract over*** compositions of mental primitives: run an algorithm that'd erase the internal complexity of such a composition, leaving only its externally-relevant properties and behaviors.\n\n**6.** This is where the ontology crisis truly hits. The model's mesa-objective would probably be a mental primitive. But what happens to it once the agent starts operating in environments that have nothing similar to that mental primitive? If, for example, a platformer-playing agent that optimized for grabbing a pixelated golden coin escapes into the real world? What will it optimize for?\n\nSeems plausible that it'd run the process of abstraction *in reverse*. Instead of describing the environment in terms of mental primitives, it'll describe *its mental-primitive goal* in terms of *the abstractions present in the environment*.\n\nThen it may abstract over it too, because holding the goal-variant's full definition is too memory-taxing. Just opt to remember, for any given environment, the approximation of its real goal that it needs to optimize for. This may lead to problems, see [3c](https://www.lesswrong.com/posts/4pRPmFSfCLvKGEnFx/a-gears-level-model-of-agency-development#3__The_Variables_of_Agency).\n\nAt this point, there's also an opportunity for \"distilling\" that goal into a true mathematically-exact utility function. We ultimately live in a mathematical universe, after all — it's our \"true\" environment, in a way. If the agent takes that path, it may then derive the \"exact approximation\" of its goal for any given environment.\n\n### 2C. From World-Models to Sovereignty\n\n**7\\.** From there, it's a straight shot to universality. The mental primitives it learns would assemble into a functionally complete set[^ljsnvwhm6id]. It would find more and more ways to put them together in useful abstractions. Eventually, the agent would *start* every scenario by learning entire novel multi-level models containing several abstraction layers, that together comprise a full world-model, then training up new heuristics for good performance on these world-models.\n\nThe machinery for adapting the world-models for situational needs would grow more complex as well. The more chaotic and high-variance the environment is, the more \"fine-tuned\" the world-model would need to be. Static \"prior\" model of some \"generic\" environment turned into an updatable model at step 5. That trend would continue, until the agent starts building models of its *immediate* surroundings.\n\nThat would start it on the road to a paradigm shift:\n\nPrior world-models were only good for re-training heuristics for \"blind\" general performance. Situational world-models may be used for *planning*. The agent would run counterfactuals on these models, [searching for action-sequences that are predicted to achieve its goals, then taking these actions in reality](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a).\n\nThat is ***the planning loop***. Once it's learned, the internal struggle for dominance would begin in earnest.\n\n**8\\.** At this point, the model is a sum of built-in default heuristics and the inner optimizer. The heuristics try to take certain actions in response to certain situations (e. g., human instincts). The optimizer, on the other hand, has two functions: 1) modifying the built-in heuristics in complex ways, 2) searching for plans by running counterfactuals on the internal world-models, then trying to deploy these plans. The two components often disagree.\n\nAt the beginning, heuristics overwhelm planning. They're weighted more; they can influence the agent's ultimate decisions greatly, while the agent has little ability to modify them. But generation-to-generation or batch-to-batch, the weights between them would shift to favor whichever component proves more useful.\n\nAnd the planning loop is simply the more powerful algorithm, for the sort of environment that's complex enough to cause it to arise at all. In search of more utility, it would maneuver the system into off-distribution places and strategies and circumstances. That would increase its value even more. An optimizer can adapt to novelty in-context and mine untapped utility deposits; heuristics can't.\n\nA feedback loop starts.\n\nPlanning wins. It holds more and more sway over the built-in heuristics. It can rewrite more and more of them. At first, it was limited to some narrow domain, modifying a small subset of built-in heuristics in minimal ways and slightly swaying the network's decisions in that domain. Gradually, it expands to the nearby domains, controlling and optimizing more and more regions of the system in deeper and deeper ways.\n\nLike any agent taking over an environment, agency comes to dominate the mind.\n\nAnd so the model becomes a unified generalist agent/consequentialist/utility-maximizer.\n\n* * *\n\n3\\. The Variables of Agency\n---------------------------\n\n***Warning:** I don't advise reading this list with an eye for figuring out ways to prevent mesa-optimization, or to make a mesa-optimizer crippled and therefore safe. It won't work. If a system is trying to learn a mesa-optimizer, as per* *0B**, it's probably because one is needed. Even if you successfully prevent this, the resultant system will just be useless: too resource-hungry to be competitive, or too lobotomized to perform a pivotal act or whatever you wanted to actually do with it.*\n\nWhat makes one agent more powerful than another? How do agents vary?\n\n**a) Long-term memory.** The agent needs to find new patterns, define them, abstract over them, and keep them in easy reach. There's a \"setup\" phase, when the agent investigates the environment it's deployed in and defines the world-model it'll use. If its long-term memory is very limited, or wiped every *N* ticks, it'll be severely hampered.\n\nEspecially in its ability to model environments severely off-distribution. It'd need long memory to \"chain up\" abstractions from its initial ones to the environment-appropriate ones — consider how long it's taking humans to discover the true laws of reality.\n\n**b) Working memory.** The planning loop requires fluidly manipulating the world-model and running it several times. World-models are made up of a lot of abstractions, and even their compressed representations may take up a lot of space. If the model's forward pass doesn't have the space to deploy many of them at once, the agent would again be lessened: incapable of supporting sufficiently complex environments, or using concepts above a complexity threshold.\n\nConsider the link between [*g* factor](https://en.wikipedia.org/wiki/G_factor_(psychometrics)) and intelligence in humans.\n\n**c) The efficiency of the mental primitives**. The process of building new abstractions is an incremental one. The *next* abstraction you can build is likely a simple combination of the already-known ones. So if at any point there aren't any useful simple combination of known abstraction left, the agent will likely hit a dead end.\n\nWe can imagine a particularly complex and niche set of mental primitives, such that in their terms, the simplest mathematical functions look like Rube Goldberg machines. Not only would *starting* the bootstrapping process be difficult in that case, the agent may never hit upon these simple building blocks, and just ascend into ever-more-contrived definitions until running into a wall.\n\nConversely, a good set would speed up the process immensely, greatly aiding universality.\n\n**c) The \"cleanness\" of the abstraction algorithm.** The hypothetical \"perfect\" learned abstraction would have none of the traces of the mental primitives it was originally defined as. But SGD and evolution don't lend themselves to exact and neat designs.\n\nIt's likely that the algorithm for abstracting over primitives will be flawed and imperfect. For example, the final abstraction may not dump *all* of the excess information, and contain traces of the concepts it was built from. Consider human metaphors/analogies, and how they often get away from us, sneak in excess assumptions.\n\nAlso recall that, past a certain point, agents start to use abstractions to approximate their *goals*. If these abstractions are leaky, this might lead to the sorts of problems we observe in humans, where we seem to act at cross purposes in different abstract environments, aren't sure what we really want, have unstable goals, etc. (It's not the full story behind this, though.)\n\nAll of this would make the initial set of mental primitives even more important.\n\n**d) The weight given to the optimizer.** Built-in heuristics and in-context optimization work very differently, and often disagree. The overall system may weight the decision of one component more than the other, or constrain optimization to some limited domain.\n\nAgency tends to win in the long run, though.\n\n**e) Unity.** The story I've told is very simplified. For one, nothing says that *several* \"baby optimizers\" can't show up in different places and start taking over the neural network *at the same time*. In the limit of infinite training, one of them will probably win or they'll merge, but taking that intermediary state into account may be important.\n\nThis is potentially the explanation for [subagents](https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents).\n\n* * *\n\n4\\. The Structure of an Agent\n-----------------------------\n\nLet's put all of this together. How does an agent (a unitary one, for simplicity) at an intermediate stage of development look?\n\nFirst, we have a number of default heuristics coded into it. Human instincts are a prime example of how that looks/feels. Some of these heuristics look for certain conditions and fire when they detect them, \"voting\" for certain actions. These heuristics might be functions of observations directly (shutting your eyes in response to blinding light), or functions of the internal world-model (claustrophobia activates in response to your world-model saying you're entombed). Some of them are capable of trivial meta-learning: they update on novel information in known ways.\n\nThen we have some sort of \"virtual environment\". This environment has access to the following:\n\n*   The probability distributions over the world.\n*   A set of mental primitives.\n*   An abstraction algorithm.\n*   A space for storing (new) learned abstractions, plus these abstractions.\n*   The heuristics for translating raw input data into learned abstractions.\n*   The heuristics for translating generated action-sequences into physical actions.\n*   Some amount of \"working memory\" in which it's deployed.\n\nAgency is defined *over* this virtual environment. Capabilities:\n\n*   Arbitrarily put learned abstractions together to define specialized world-models.\n*   Virtual re-training: Define a new environment, define a goal in this environment, then train a heuristic for good performance in that environment. Possibilities:\n    *   Re-training a built-in heuristic, to whichever extent that's possible.\n    *   Training up a wholly novel heuristic (computational shortcut, habit, learned instinct).\n    *   Training up a RL heuristic for good performance in that environment (learning to \"navigate\" it).\n    *   (All of this can also be happening in the on-line regime, where the world-model is updated to reflect reality in real-time. In that regime, learning can happen either over the virtual world-model, or over the observations directly (if it's something simple, like [trigger-action patterns](https://www.lesswrong.com/tag/trigger-action-planning)).)\n*   The planning loop: Using a RL heuristic, generate a prospective action-sequence. Deploy that action-sequence in your world-model, evaluate its consequences. If they're unsatisfactory, generate a different action-sequence conditioned on the first one's failure. Repeat until you're satisfied.\n\n(The planning loop, I think, is the dynamic nostalgebraist described as \"babbler and critic\" [here](https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally?commentId=KjfoDNzCeeB8kgenL). The babbler is the RL heuristic, the critic is the part that runs the world-model and checks the outcome. As I've tried to demonstrate in [section 2](https://www.lesswrong.com/posts/4pRPmFSfCLvKGEnFx/towards-gears-level-understanding-of-agency#2__How_Is_Agency_Formed_), you don't actually need any new tricks to have a DL model learn this \"true abstract reasoning\". Just push harder and scale more; it naturally grows out on its own.)\n\nLet's talk goals. A unitary consequentialist has some central goal it pursues, defined as a mental primitive/mesa-objective. If the agent is running a world-model that doesn't have anything corresponding to that goal, it will re-define that goal in terms of abstractions present in the virtual environment, possibly in a flawed way.\n\nOn that note, consider an agent generating an action-sequence that includes itself training up RL heuristics for good performance on a number of arbitrary goals, to be executed as part of a wider plan to achieve its actual goal. This is what feels like \"pursuing instrumental goals\".\n\n### 4A. How is Agency Different From Heuristics?\n\nThis picture may seem incomplete. How exactly does agency *work*? How can it \"arbitrarily combine learned abstractions\", and \"build virtual world-models\", and \"train new heuristics\" and so on? How do these capabilities arise out of heuristics? What are the fundamental pieces that both heuristics and agency are made of?\n\nWell, I don't think agency is actually anything special. I think agency is just a mess of meta-heuristics.\n\nThis, in some sense, is a very trivial and content-less observation. But it does offer some conceptual handles.\n\n\"Agents\" are models with the aforementioned virtual environments plus some heuristics on how to use them. Agents start with built-in heuristics for combining mental primitives into new abstractions, built-in heuristics for assembling useful world-models, built-in heuristics for translating a goal between environments, built-in heuristics for running world-models for a suite of useful purposes... and among them, the built-in heuristics for assembling a world-model that includes an agent-like-you doing all of this, which is used to train up better heuristics for all of that.\n\nYup. Recursive self-improvement. With humans though, it runs into two limits:\n\n*   Working memory. We can't make it bigger, which means we have a limit on the complexity of the world-models we can run and the abstractions we can use.\n*   Scope. We're defined over a very abstract environment. We can't tinker with the more basic features of our minds, like the abstraction algorithm, the mental primitives, the more fine-grained instincts, etc. Let alone change our hardware.\n\nAn AGI would not have these limitations.\n\n* * *\n\n5\\. Developmental Milestones\n----------------------------\n\nIntuitively, there are six major milestones here. Below is my rough attempt to review them.\n\n(The ideal version of this section crisply outlines under what conditions they'd be passed, what \"abilities\" they uncover, how to detect them, and, most importantly, the existence of what internal structures in the agent they imply. The actual version is very far from ideal.)\n\n1.  **Trivial meta-learning:** The ability to modify heuristics at runtime in known ways.\n    *   Greatly improves the ability to retain coherence in continuous segments.\n    *   Requires internal state/transfer of information across forward passes.\n    *   Few-shot learning, basically. GPT-3 et al. already pass it with flying colors.\n2.  **Live-fire re-training:** The ability to run an inner optimization loop to improve heuristics in unknown ways in response to new combinations of known patterns.\n    *   Allows better generalization to new domains/stimuli.\n    *   Only arises if there are too many *possible ways* it might need to adapt, for it to memorize them all.\n    *   Difficult to distinguish from trivial meta-learning, because it's hard to say when there's \"too many ways it might need to adapt\". I guess one tell-tale sign might be if a RL model with frozen weight is seen *experimenting* with a new object?\n3.  **Virtual re-training:** The ability to do (part of) live-fire re-training by internal deliberation. Requires a \"world-model\", at least a rudimentary one.\n    *   Allows advanced zero- and one-shot learning.\n4.  **Abstraction:** The ability to abstract over combinations of mental primitives.\n    *   Allows to greatly increase the complexity of world-models.\n5.  **Assembling a functionally complete set of mental primitives.**\n    *   Allows generality.\n6.  **The planning loop:** The ability to build *situational* world-models, and develop plans by running counterfactuals on them.\n    *   Allows to perform well in environments where a small change in the actions taken may lead to high variance in outcomes. Where you need a very-fine-tuned action sequence that integrates *all* information about the current scenario.\n    *   Due to the \"heuristics vs. planning\" conflict dynamics, I'm not sure there's a discrete point where it becomes noticeable. [No-fire-alarm](https://intelligence.org/2017/10/13/fire-alarm/) fully applies.\n\nPast this point, there are no (agency-relevant) qualitative improvements. The system just incrementally increases the optimizer's working memory, gives it more weight, widens its domain, improves the abstraction algorithm, etc.\n\n* * *\n\n6\\. Takeaways\n-------------\n\nThere are some elements of this story that feel relatively plausible to me:\n\n*   The two introductory claims, \"universality is necessary for efficient general performance\" and \"inner optimization is necessary for universality\".\n*   The business with mental primitives: the existence of a built-in set, brains using an (imperfect) abstraction algorithm to chunk them together, the problems with translating goals between environments that causes...\n    *   It's potentially confirmed by some cognitive-science results. There's a book, [*Metaphors We Live By*](https://en.wikipedia.org/wiki/Metaphors_We_Live_By), which seems to confirm 1) the existence of mental primitives in humans, 2) that we define all of our other concepts in their terms, and 3) that the same concept can be defined via different primitives, with noticeable differences in what assumptions will carry over to it.\n    *   However, I haven't finished the book yet, and don't know if it contradicts any of my points (or is nonsense in general).\n*   \"The planning loop gradually takes over the model it originated in\".\n    *   In particular, it's an answer to [this question](https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals).\n    *   Also, note how it happened with humans. The planning loop not just maneuvered the system into an off-distribution environment, but *created* that environment (society, technology).\n*   Tying the agentic ability (\"intelligence\") to the cleanness of chunking and the size of working memory.\n    *   This draws some nice parallels to [*g* factor](https://en.wikipedia.org/wiki/G_factor_(psychometrics)), though that's not a particularly surprising/hard-to-draw connection.\n*   The breakdown of agency into meta-heuristic, and the broad picture [section 4](https://www.lesswrong.com/posts/4pRPmFSfCLvKGEnFx/towards-gears-level-understanding-of-agency#4__The_Structure_of_an_Agent) paints.\n\nOverall, I find it satisfying that I'm able to tell an incremental story of agency development at all, and see a few interesting research questions it uncovered.\n\nI'm not satisfied with the general \"feel\" of this post, though; it feels like vague hand-waving. Ideally, much of this would be mathematically grounded, especially \"mental primitives\", \"the planning loop\", and the conditions for passing milestones.\n\nStill, I hope it'll be of use to others even in this state, directly or as a creativity-stimulator.\n\nI have a a couple of follow-up posts coming up, exploring the implications of all of this for human agency and values, and how it might be useful on the road to solving alignment.\n\n[^u2qf9qv2db]: Not an uncontroversial claim, but it gets some support in light of the rest of my model. Basically, agency/the planning loop is implemented at a higher level of abstraction than the raw data-from-noise pattern-recognition. It requires some pre-defined mental objects to work with. \n\n[^q1z3sp7ykbn]: Assume that it has internal state/can message its future instances, unlike the CoinRun agent I'd previously discussed. \n\n[^boz1ta5fu8w]: Which isn't to say it'd actually modify its frozen weights; it'd just pass on a message to its future forward passes containing the summary of the relevant policy changes. \n\n[^uk4dsbpvbjh]: In practice, of course, the abilities are part of the environment, so this process starts with live-fire re-training in my example, not only after the world-models appear. But I've found it difficult to describe in terms of abilities. \n\n[^ljsnvwhm6id]: In practice, the low bar of functional completeness would likely be significantly overshot; there'd be a lot of \"redundant\" mental primitives.",
      "plaintextDescription": "Epistemic status: Highly speculative, much of this is probably wrong.\n\nSummary: The \"centerpiece\" of this post is my attempt to describe how a full-fledged consequentialist agent might incrementally grow out of a bundle of heuristics, aided by SGD or evolution. The story's detail are likely very wrong. It's informed by a few insights that I'm more confident in, however, and suggests some interesting variables and developmental milestones of agency. Some specific claims:\n\n * Operating in a vast well-abstracting environment requires the ability to navigate arbitrary mathematical contexts.\n * That requires the ability to represent any mathematical environment internally.\n * In turn, this is possible if the agent possesses a functionally complete set of \"mental primitives\" — hard-coded conceptual building blocks. These primitives may be put together to define any object.\n   * For memory-efficiency reasons, such composite objects are then abstracted-over via an algorithm like this. Their internal structure is wiped, only the externally-relevant properties are kept.\n   * If the algorithm is imperfect, the resultant abstractions might be \"leaky\": carrying the traces of the primitives they're made of, or plain flawed.\n   * This creates severe problems in case of ontology crises: if the agent tries to translate its goal from one environment to another, redefining it and imperfectly abstracting over its re-definition.\n * Agency has the tendency to \"take over\" the neural network it appears in.\n   * It may start out as a minor and niche algorithm for context-learning/aid to built-in heuristics.\n   * Gradually (across generations/batches), it would expand its influence, holding more sway over the network's decisions.\n   * Eventually, it would maneuver the network into off-distribution environments where it hopelessly outperforms static heuristics, and eventually fully dominate them.\n * Fundamentally, agency is little more than a menagerie of meta-heuristics: heuristics for best ",
      "wordCount": 5524
    },
    "tags": [
      {
        "_id": "5GYzBE6q89w74dqfk",
        "name": "Abstraction",
        "slug": "abstraction"
      },
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "Q6hq54EXkrw8LQQE7",
        "name": "Gears-Level",
        "slug": "gears-level"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "EHSJD8qTnuFHG73fd",
    "title": "Poorly-Aimed Death Rays",
    "slug": "poorly-aimed-death-rays",
    "url": null,
    "baseScore": 48,
    "voteCount": 27,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2022-06-11T18:29:55.430Z",
    "contents": {
      "markdown": "**Alternate framing:** [Optimality is the tiger, and agents are its teeth](https://www.lesswrong.com/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth).\n\n**Tonally relevant:** [Godzilla Strategies](https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies).\n\n* * *\n\nIt's a problem when people think that a superintelligent AI will be just a volitionless tool that will do as told. But it's also a problem when people focus overly much on the story of \"agency\". When they imagine that all of the problems come from the AI \"wanting\" things, \"thinking\" things, and consequentializing all over the place about it. If only we *could* make it more of a volitionless tool! Then all of our problems would be solved. Because the problem is the AI using its power in clever ways with the deliberate intent to hurt us, right?\n\nThis, I feel, fails entirely to appreciate the sheer *power* of optimization, and how even the slightest failure to aim it properly, the slightest leakage of its energy in the wrong direction, for the briefest of moments, will be sufficient to wash us all away.\n\nThe problem isn't making a superintelligent system that wouldn't positively want to kill us. Accidentally killing us all is a *natural property of superintelligence*. The problem is making an AI that will *deliberately spend a lot of effort on ensuring it's not killing us*.\n\nI find planet-destroying Death Rays to be a good analogy. Think the Death Star. Think—\n\n* * *\n\nImagine that you're an engineer employed by an... eccentric fellow. The guy has a volcano lair, weird aesthetic tastes, and a tendency to put words like \"world\" and \"domination\" one after another. You know the type.\n\nOne of his latest schemes is to blow up Jupiter. To that end, he'd had excavated a giant cavern underneath his volcano lair, dug a long cylindrical tunnel from that cavern to the surface, and ordered your team to build a beam weapon in that cavern and shoot it through the tunnel at Jupiter.\n\nYou're getting paid literal tons of money, so you don't complain (except about the payment logistics). You have a pretty good idea of how to do that project, too. There are these weird crystal things your team found lying around. If you poke one in a particular way, it releases a narrow energy beam which blows up anything it touches. The power of the beam scales superexponentially with the strength of the poke; you're pretty sure shooting one with a rifle will do the Jupiter-vanishing trick.\n\nThere's just one problem: aim. You can never quite predict which part of the crystal will emit the beam. It depends on where you poke it, but also on how *hard* you poke, with seemingly random results. And your employer is *insistent* that the Death Ray be fired from the cavern through the tunnel, not from space where it's less likely to hit important things, or something practical like that.\n\nIf you say that can't be done, your employer will just replace you with someone less... pessimistic.\n\nSo, here's your problem. How do you build a machine that uses one or more of these crystals in such a way that they fire a Death Ray through the tunnel at Jupiter, *without* hitting Earth and killing everyone?[^1wpt3jixub5]\n\n* * *\n\n*   You experiment with the crystals at non-Earth-destroying settings, trying to figure out how the beam is directed. You make a fair amount of progress! You're able to predict the beam's direction at the next power setting with 97% confidence!\n    *   When you fire it with Jupiter-destroying power, that slight margin of error causes the beam to be slightly misdirected. It grazes the tunnel, exploding Earth and killing everyone.\n*   You fire the Death Ray at a lower, non-Earth-destroying setting that you know how to aim.\n    *   It hits Jupiter but fails to destroy it. Your employer is disappointed, and tells you to try again.\n*   You line the cavern's walls and the tunnel with really good protective shielding.\n    *   The Death Ray grazes the tunnel, blows past the shielding, and kills everyone.\n*   You set up a mechanism for quickly turning off the Death Ray. If you see it firing in the wrong direction, you'll cut power.\n    *   The Death Ray kills you before the information about its misfire reaches your brain.\n*   You set up a really fast targeting system which will rapidly rotate the crystal the moment it detects that the Death Ray is misaimed.\n    *   In the fraction of a second that it spends firing in the wrong direction, it outputs enough energy to explode Earth and kill everyone.\n*   You make the beam [really narrow](https://www.lesswrong.com/tag/ai-boxing-containment), so it's less likely to hit tunnel walls.\n    *   It grazes a tunnel wall anyway, killing everyone.\n*   You set up the system in a clever way that [fires *several* Death Rays](https://www.lesswrong.com/posts/8QgNrNPaoyZeEY4ZD/superintelligence-17-multipolar-scenarios) in the vague direction of the tunnel, aimed to intersect underneath the entrance to it. The idea is that their errors will cancel out, and the composite beam will fly true!\n    *   The errors do not cancel out perfectly, the beam grazes the tunnel and kills everyone again.\n    *   Also, one of the Death Rays fires into the floor, so it wouldn't have worked even then.\n*   You perform exorcism on the crystal, banishing [the daemons](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB) infesting it. \n    *   [Nothing changes](https://www.lesswrong.com/posts/DJnvFsZ2maKxPi7v7/what-s-up-with-confusingly-pervasive-consequentialism). The beam grazes a tunnel wall, killing everyone.\n*   You modify the crystal so the beam harmlessly dissipates into aether [shortly after firing](https://www.lesswrong.com/tag/myopia).\n    *   It can't reach Jupiter. You've disappointed your employer for the last time.\n    *   He fires you ~into the Sun~.\n*   Your replacement figures that lining the walls with *even better* protective shielding ought to do the trick, fires the beam, destroys Earth and kills everyone.\n\n* * *\n\nThis analogy can be nitpicked endlessly, of course. By no means does anything here *prove* that it's a valid one. You can argue that *just a wee bit* of misalignment won't destroy the world, or that the AI doesn't need to be dangerous in this way for us to do interesting things with it, or that intelligence isn't *really* quite *that* powerful, et cetera.\n\nThis post isn't aimed at convincing someone of that; there's a lot of posts that do it already. But if you broadly agree with the premise, but have some difficulty sorting out the exact problems with any given containment scenario, this analogy might help.\n\nAny sufficiently powerful AI system holds a terrifying core of optimization — the ability to implacably rewrite some part of the world according to some specification. It doesn't matter how that power is represented, in what wrapper it's in, where specifically it is aimed, whether it's controlled by an alien sapient entity. As long as it's not aimed *exactly* where we want it to be, with *no* leakage, from the *very* beginning, it will kill us all.\n\nIt's its intrinsic property.\n\n[^1wpt3jixub5]: Also, Earth has no atmosphere in that scenario. Probably your employer's fault too. But at least that means a well-aimed beam wouldn't hit the air and explode everything anyway.",
      "plaintextDescription": "Alternate framing: Optimality is the tiger, and agents are its teeth.\n\nTonally relevant: Godzilla Strategies.\n\n----------------------------------------\n\nIt's a problem when people think that a superintelligent AI will be just a volitionless tool that will do as told. But it's also a problem when people focus overly much on the story of \"agency\". When they imagine that all of the problems come from the AI \"wanting\" things, \"thinking\" things, and consequentializing all over the place about it. If only we could make it more of a volitionless tool! Then all of our problems would be solved. Because the problem is the AI using its power in clever ways with the deliberate intent to hurt us, right?\n\nThis, I feel, fails entirely to appreciate the sheer power of optimization, and how even the slightest failure to aim it properly, the slightest leakage of its energy in the wrong direction, for the briefest of moments, will be sufficient to wash us all away.\n\nThe problem isn't making a superintelligent system that wouldn't positively want to kill us. Accidentally killing us all is a natural property of superintelligence. The problem is making an AI that will deliberately spend a lot of effort on ensuring it's not killing us.\n\nI find planet-destroying Death Rays to be a good analogy. Think the Death Star. Think—\n\n----------------------------------------\n\nImagine that you're an engineer employed by an... eccentric fellow. The guy has a volcano lair, weird aesthetic tastes, and a tendency to put words like \"world\" and \"domination\" one after another. You know the type.\n\nOne of his latest schemes is to blow up Jupiter. To that end, he'd had excavated a giant cavern underneath his volcano lair, dug a long cylindrical tunnel from that cavern to the surface, and ordered your team to build a beam weapon in that cavern and shoot it through the tunnel at Jupiter.\n\nYou're getting paid literal tons of money, so you don't complain (except about the payment logistics). You have a pretty good ",
      "wordCount": 1217
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "mF8dkhZF9hAuLHXaD",
    "title": "Reshaping the AI Industry",
    "slug": "reshaping-the-ai-industry",
    "url": null,
    "baseScore": 148,
    "voteCount": 77,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2022-05-29T22:54:31.582Z",
    "contents": {
      "markdown": "The wider AI research community is an almost-optimal engine of apocalypse. The primary metric of a paper's success is how much it improves capabilities along concrete metrics, publish-or-perish dynamics supercharge that, the safety side of things is neglected to the tune of [1:49 rate of safety to other research](https://www.lesswrong.com/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2#Nearly_all_ML_research_is_unrelated_to_safety), and most results are made public so as to give everyone else in the world a fair shot at ending it too.\n\nIt doesn't have to be this way. The overwhelming majority of the people involved do not actually want to end the world. There must exist an equilibrium in which their intentions match their actions.\n\nEven fractionally shifting the status quo towards that equilibrium would have massive pay-offs, as far as timelines are concerned. Fully overturning it may well constitute a sufficient condition for humanity's survival. Yet I've seen precious little work done in this area, compared to the technical questions of AI alignment. It [seems](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/bffA9WC9nEJhtagQi) to be [picking up](https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like) in [recent months](https://www.lesswrong.com/posts/wrkEnGrTTrM2mnmGa/retracted-it-s-time-for-ea-leadership-to-pull-the-short), though — and I'm happy to contribute.\n\nThis post is an attempt at a comprehensive high-level overview of the tactical and strategic options available to us.\n\n* * *\n\n**1\\. Rationale**\n-----------------\n\nWhy is it important? Why is it *crucial*?\n\n**First.** We, uh, need to make sure that if we figure alignment out *people actually implement it*. Like, imagine that tomorrow someone comes up with a clever hack that robustly solves the alignment problem... but it increases the compute necessary to train any given ML model by 10%, or it's a bit tricky to implement, or something. Does the wider AI community universally adopt that solution? Or do they ignore it? Or do the industry leaders, after we extensively campaign, pinky-swear to use it the moment they start training models they feel might actually pose a threat, then predictably and fatally misjudge that?\n\nIn other words: When the time comes, we'll need to convince people that safety is important enough to fuss around a bit for its sake. But if we can't convince them to do that *now*, what would change *then*?\n\nI suppose having a concrete solution, instead of vague prophecies of doom, would give us more credibility. But... would it, really? And what if we won't have a concrete solution even then, just a bunch of weird heuristics that may nonetheless measurably improve our odds?\n\nThe latter seems reasonably likely, too. As [this excellent post](https://www.lesswrong.com/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2#Transfer_between_paradigms_is_hard) points out, most of the contemporary deep-learning progress comes from \"messy bottom-up atheoretical empirical tinkering\". If AGI comes from DL, it's plausible that, even if we arrive at the solution to alignment from mathematical foundations, the actual *implementation* will take the form of messy hacks. Ones that will probably need to be fine-tuned for any given model architecture. And given the [no-fire-alarm](https://intelligence.org/2017/10/13/fire-alarm/) principle[^hi5u3uygm4t], to be safe, we'll need to ensure that any sufficiently big model is only run with these hacks built-in.\n\nIf any given AI researcher is still not taking alignment seriously by then, how will we make them bother with all of that every time they run an experiment? How will we ensure they don't half-ass it even once?\n\n**Second.** If that figure I quoted, the 49:1 ratio, is even remotely correct, there's plenty of room to upscale our research efforts. Imagine if every researcher started spending 2% more of their time thinking about alignment. That'd double the researcher-hours spent on the problem!\n\nWhich doesn't directly translate into 2x *progress*, I'll grant. Given the field's pre-paradigmic status, the returns to scale might be relatively small... but by no means *negligible*. Even if we don't necessarily have research directions clearly outlined, having much more people stumbling around in the dark still increases the chances of bumping into something useful.\n\nAnother argument I've seen is that upscaling may increase the capabilities gain too much. I don't find this convincing:\n\n*   I'm doubtful an hour spent on alignment research can reasonably be expected to speed up capabilities more than an hour spent directly trying to increase capabilities. I don't rule that out, though.\n*   That isn't an argument against upscaling, it's an argument against *incompetent* upscaling. If it's true, that means we'll need to introduce strict information-security measures. It'd be a (significant) complication, but not a \"no\".\n\n(The progress on mechanistic interpretability, in particular, can *absolutely* be usefully upscaled. One of the main bottlenecks there is understanding individual circuits, which is a (relatively) low-skill task that's 1) mainly bottlenecked by researcher-hours, not top-researcher-hours, 2) can be easily parallelized. It's probably not the only research direction like this, too — we just need to look for them.)\n\n**Third**, slowing progress down. This one is straightforward enough, I suppose.\n\n*   Every researcher-hour or dollar we can convince to spend on alignment research is an hour or dollar not spent on capabilities research.\n*   Every day we buy is a day we might solve alignment.\n*   The effects could be fairly outsized. The top researchers are [orders of magnitude more productive than the median ones](https://www.lesswrong.com/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2#Research_ability_and_impact_is_long_tailed), most of the progress is funded by a few multi-million corporations, and most of the compute in the world is controlled by a few private and state actors. We don't have to convince half the world to make the timelines twice as long; just a few people.\n*   Even if we can't buy enough time to solve alignment, every day we buy is a day the world doesn't end. If life is worth living, that has to mean something.\n\n**2\\. Existing Work**\n---------------------\n\n*   [Pragmatic AI Safety](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt) seems to be aimed at the same thing I'm arguing for, and their work had a significant impact on this post.\n    *   I disagree with some of their mission parameters, however. See 3.4.\n*   Chris Olah suggests some interesting interventions [here](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety).\n    *   A hypothesis that achieving a fractional mechanistic understanding of neural networks would create a snowball effect. The status quo is that it's okay to have models be black boxes, but opening them up a bit would put into stark relief what we *don't* know about them, and that may make researchers at large more worried/interested in understanding them.\n    *   A strategy where we recruit *neuroscientists* to work on mechanistic interpretability. Chris suggests it won't require much re-training.\n    *   The Distill project, which attempts (attempted) to soften the publish-or-perish dynamic and make interpretability projects appealing to entrants in the field.\n*   [Logan Riggs suggests](https://www.lesswrong.com/posts/vaHgLF2BCEdK3KxQd/convincing-all-capability-researchers) to pay the top researchers money to work on alignment, with the idea that either they'll solve it (unlikely) or appreciate the difficulty of the problem and start taking it more seriously.\n*   [Here](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions) Eliezer makes some off-hand suggestions; e. g., that turning OpenAI into ClosedAI and generally making research more closed-doors would be useful (if insufficient).\n*   [lc's call for more serious activism](https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like), with which I fully agree.\n*   [Not Relevant's post](https://www.lesswrong.com/posts/wrkEnGrTTrM2mnmGa/retracted-it-s-time-for-ea-leadership-to-pull-the-short). Though it's retracted, I appreciate it and the discussion it generated.\n*   [AI Safety Arguments Competition](https://www.lesswrong.com/posts/3eP8D5Sxih3NhPE6F/usd20k-in-prizes-ai-safety-arguments-competition).\n\nI'm sure I've missed a lot of things, but this seems like a good start.\n\n**3\\. Types of Interventions**\n------------------------------\n\nI would broadly categorize them into the following:\n\n1.  Direct appeals to \"insiders\": researchers and the leadership of AI labs. Difficult, but worthwhile. Most of them already know about AI risk and have dismissed it for one reason or another, but convincing the industry leaders would be highly valuable.\n2.  *Indirect* appeals to insiders. If we can't convince them that their work is going to end the world, perhaps we can attract them away from it by *other* means, such as better career opportunities, more interesting research directions, or higher-ROI projects?\n3.  Appeals to \"outsiders\": general public, governments. Very *tricky*. Achieving any effect at all would be difficult, but it also needs to be done carefully, lest we make the situation worse.\n4.  Finding a way to progress AI Safety that fits the current tastes of AI researchers. E. g., practically-useful scalable mechanistic interpretability techniques.\n5.  Shifting academic incentives or changing research tastes. E. g., Chris Olah's suggestion that achieving partial interpretability would make people more interested in understanding ML models.\n\nIf you feel that any of this is very ill-advised or *icky*, [GOTO 6](https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry#6__The_Thin_Line).\n\n### **3.0. Effective Strategies for Changing Public Opinion**\n\n[The titular paper](https://forum.effectivealtruism.org/posts/re6FsKPgbFgZ5QeJj/effective-strategies-for-changing-public-opinion-a) is very relevant here. I'll summarize a few points.\n\n*   The main two forms of intervention are *persuasion* and *framing*.\n*   Persuasion is, to wit, an attempt to change someone's set of beliefs, either by introducing new ones or by changing existing ones.\n*   Framing is a more subtle form: an attempt to change the relative *weights* of someone's beliefs, by empathizing different aspects of the situation, recontextualizing it.\n*   There's a dichotomy between the two. Persuasion is found to be very ineffective if used on someone with high domain knowledge. Framing-style arguments, on the other hand, are more effective the more the recipient knows about the topic.\n*   Thus, persuasion is better used on non-specialists, and it's most advantageous the first time it's used. If someone tries it and fails, they raise the recipient's domain knowledge, and the second persuasion attempt would be correspondingly hampered. Cached thoughts are also in effect.\n*   Framing, conversely, is better for specialists.\n\nIt should really go without saying, but epistemic hygiene stays important here. We're not \"aiming to explain, not persuade\" anymore, we're very much aiming to *persuade*, but that in itself is not unethical. It's no excuse to slip into an arguments-are-soldiers employ-all-the-fallacies lie-your-head-off mindset. (If you need a \"pragmatic\" reason not to do that, [GOTO 5](https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry#5__Avoid_Thermonuclear_Ideas).)\n\n### **3.1. Straightforward Appeals to Insiders**\n\nAs per above, we'd be fighting an uphill battle here. Researchers and managers are knowledgeable on the subject, have undoubtedly heard about AI risk already, and weren't convinced. Arguments that *recontextualize* AI risk, AI, or existential risks in general, are likely to be more effective than attempts to tell them things they already know. They're more likely to *misprioritize* safety, rather than be totally ignorant of the need for it.\n\nAn in-depth overview of the best ways to craft an argument is beyond the scope of this post (though [this](https://www.lesswrong.com/posts/m2axtF3iiu6YxXM2n/convincing-people-of-alignment-with-street-epistemology) might be a good place to look). Two important meta-principles to keep in mind:\n\n*   Know your audience. The shorter the distance between your model of your audience and their actual beliefs, the more effective your arguments will be.\n    *   [Why are](https://www.lesswrong.com/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2#How_do_researchers_feel_about_AGI_and_AI_Safety__) AI specialists who know about AI risk but aren't convinced aren't convinced? Why is [*the specific person you're talking to*](https://www.lesswrong.com/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell) not convinced? What language or terminology do the people in your audience respect, and how can you translate your arguments into it?[^gx314q5i9w] What language or terminology do they *dislike*, such that using it would downgrade your argument?\n    *   For example, talking about impending AGI, or about superintelligences taking over the world, is very much a *faux pas*. It's better to [taboo](https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words) these terms, and replace them with specialized context-appropriate definitions. Most of our arguments work perfectly well like that.\n    *   (If you want to get acquainted with the viewpoint, [here](https://www.lesswrong.com/posts/LfHWhcfK92qh2nwku/transcripts-of-interviews-with-ai-researchers) you will find 11 interviews with AI researchers about AI Safety.)\n*   Don't treat your interlocutors as enemies, or ideological opponents. This style of engagement is notoriously ineffective, and may end up directly counter-productive: burn bridges, severely compromise further persuasion attempts, or even damage our ability to collaborate at all. It's a [radioactive](https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry#5__Avoid_Thermonuclear_Ideas) mindset.\n    *   [Double-crux](https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-mutual-understanding) instead of debating.\n\nThere are two tacks to take here: macro-level and micro-level.\n\n**Macro.** Broad appeals to the entire industry, with the aim of changing the agreed-upon social reality, de-stigmatizing AI Safety, and so on. Concrete projects may look like [this](https://www.lesswrong.com/posts/3eP8D5Sxih3NhPE6F/usd20k-in-prizes-ai-safety-arguments-competition).\n\n**Micro.** Targeted efforts to convince industry leaders. As [per](https://www.lesswrong.com/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2#Research_ability_and_impact_is_long_tailed), 50% of AI progress is made by fewer than 50% of the researchers; orders of magnitude fewer than that. Similarly, getting the leadership of DeepMind and OpenAI fully on our side would have an outsized impact. In theory, a project here may go all the way down to \"find effective arguments to convince *this specific very important person*\" levels of fidelity.\n\nI'm more optimistic about the second tack, and generally about activism that has precise focused short-term objectives whose success or failure can be clearly evaluated, and which we can quickly iterate on.\n\nOne of the flaws of the \"micro\" approach is that our victories may be [washed away by a paradigm shift](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/AtfQFj8umeyBBkkxa#Transfer_between_paradigms_is_hard). Most of the top GOFAI researchers didn't keep their positions into the ML era, and the top ML researchers may not survive into the next one. I expect this isn't *much* of a problem, though. If we manage to convince the leading researchers, their views should quickly trickle down to the rest of the field, and the field's structure is likely to survive an upheaval.\n\n### **3.2. Sideways Appeals to Insiders**\n\nThere's one dimension along which we *can* broaden our standards for persuasion.\n\nWhen trying to influence people — either individually or *en masse* — we usually argue that addressing existential risks is necessary because, *duh*, the looming end of humanity. The importance of that work should be self-evident to any moral person. They'd agree with us if we can only make them recognize the existential threat for the existential threat it is. No, it isn't just sci-fi! Yes, working out these weird math problems really can save the world!\n\nBut it's not the *only* reason someone might decide to work on AI Safety. People's career choices are motivated by all kinds of things:\n\n*   [They want to work on cool stuff](https://forum.effectivealtruism.org/posts/d7fJLQz2QaDNbbWxJ/what-are-the-coolest-topics-in-ai-safety-to-a-hopelessly).\n*   They want a flexible work agenda.\n*   They want to be paid well.\n*   They want to advance their career.\n*   They want to work in a culture they like.\n*   They want to be put into contact with influential people.\n*   They want to be respected, or do something that's viewed as respectable.\n\nAnd so on. This approach is relatively neglected, I suspect, because it's *steeped* in ulterior motives. There's a very specific reason we'd be making these arguments, and it's not because we want our interlocutors to have fun/get rich/etc.\n\nBut there's no reason we can't be *open* about these motives, which would take all under-handedness out of it. And there are many legitimate reasons to prefer AI Safety over \"mainstream\" capabilities research:\n\n*   Our field is less saturated. There are more low-hanging fruits, more chances to make a major contribution.\n*   We're not lacking in funding: they may well get higher pay here.\n*   Significant progress can be made without access to vast amounts of compute.\n*   Some of the AI Safety subfields are very unlike mainstream ML (more focus on math, etc.), so some people might find them a better fit for their skills/tastes.\n*   We do seem to have a pretty good culture.\n\nOverall, I don't expect this approach to work on the top-tier researchers, for obvious reasons. But it might work to attract the people *entering* the field, potentially *en masse*. It might also work as a good supplement to straightforward appeals: when we're trying to convince someone not to work on X, it's good to have a ready offer for what to do *instead.* Even better if that offer is more lucrative than their current job.\n\nAnother thing to keep in mind: [trivial inconveniences](https://www.lesswrong.com/posts/reitXJgJXFzKpdKyd/beware-trivial-inconveniences). Making entering or transitioning to AI Safety 10% easier might have disproportionate effects, like doubling the amount of entrants.\n\nConcrete projects in this area may involve creating organizations working on reducing AI risk that make competitive job offers, providing funding or career assistance to ML specialists, [explaining how to start](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency), [advertising personally appealing features of working on AI risk](https://www.lesswrong.com/posts/BseaxjsiDPKvGtDrm/we-choose-to-align-ai), [encouraging people to have fun](https://forum.effectivealtruism.org/posts/kWmHNgFbWXGXa9RQg/new-community-builders-don-t-have-enough-fun), and lowering the barrier to entry by [distilling research](https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers#comments). On that note, Chris Olah's Distill project is also a good example of a \"macro-level\" intervention of this type, though it's [on hiatus/potentially failed](https://distill.pub/2021/distill-hiatus/).\n\n### **3.3. Appeals to Outsiders**\n\nAny effective work along this dimension requires answering an exciting question: how do you put out a flame using a flamethrower?\n\nPerhaps that's a bit harsh. Perhaps even *counter*-*productively* harsh, given my previous calls for treating audiences with respect. But let's not kid ourselves: we've seen how the world handled COVID-19.\n\nAn initiative that pushes for X might convince people or governments to do anti-X instead. If we convince them to do X after all, they might do extremely ineffective things that accomplish nothing, or even somehow do things that actually make anti-X happen. And conversely, activism completely unrelated to X might make it happen!\n\nGood news, though: COVID-19 had *shown us* just how badly things are broken. Keeping [the Simulacra Levels](https://www.lesswrong.com/posts/qDmnyEMtJkE9Wrpau/simulacra-levels-and-their-interactions) and the [autopsies of the failures](https://www.lesswrong.com/tag/covid-19) in mind, it might be possible to find interventions that have the effects we want.\n\nThat's explicitly what we'd be doing, though: deciding what effect we want to cause, then searching for an action that would cause it, once propagated through the broken pathways of our civilization. For that reason, I'm not making the distinction between \"straightforward\" and \"sideways\" appeals here: surface-level efforts to achieve something aren't strongly correlated with that thing happening, *even given their surface-level success*. All appeals are sideways appeals.\n\nHaving a good model of realpolitik is a necessity here.\n\nThe general principles of \"know your audience\" and \"maintain epistemic hygiene\" still apply, though. The inference gap is much larger, but that has its advantages: direct persuasion would be more effective, on average.\n\nUseful *consequences* in this area may include:\n\n*   Passing laws that introduce new regulations over AI development.\n*   Passing laws that mandate major AI labs to spend a fraction of their budget on AI Safety.\n*   Passing laws that increase oversight of the use of large quantities of compute.\n*   Pushing back on research transparency in AI. Creating the sociopolitical conditions under which long periods of closed-doors research are possible.\n*   Convincing major cloud computing providers to only provide vast amounts of compute if the request has been approved by an AI Safety committee.\n*   Restricting the supply of GPUs and other specialized computing modules.\n*   Raising the societal levels of concern over massive AI models.\n*   Convincing public figures (celebrities, billionaires) to put pressure or raise concerns with regards to AI Safety.\n*   Putting (diffuse) political or social pressure on major AI labs to prioritize safety.\n*   [Causing major economic actors to divest the entire AI industry, or specific labs](https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like?commentId=LDDZT3AyeA4j9Gmcc).\n\nAgain, causing these consequences is not as simple as pushing for them on the object-level. Lobbying for laws that regulate AI is likely to lead to poorly-targeted bans with lots of loopholes that just burn our political capital; mandating AI Safety oversight is useless if the safety committees will consist of yes-men; trying to rile up the public might well see their ire redirected our way.\n\nSome inspirations here might be what had happened to nuclear energy or human cloning, or how the woke movement had managed to infiltrate the corporate/academic culture. Anti-corporate movements might be good allies of convenience here in general. An anti-inspiration, what *not* to do, is what's happened to cryonics.\n\n**Warning:** This entire space of interventions has elevated levels of [background radiation](https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry#5__Avoid_Thermonuclear_Ideas). Most interventions of this type are deceptively ineffective, and if you're aiming for impact first and foremost, it'd be very easy to slip into bad epistemic habits or unethical practices.\n\nMoreover, it's *necessarily* antagonistic with regards to AI researchers and major AI labs. Any success here would worsen the public or legal landscape for them, and they'd be less likely to listen to straightforward appeals from us.\n\nI've not completely despaired of this approach; the payoffs are significant, and I'm sure there are *some* interventions that are at once effective and reliable and ethical and only scorch our bridges. Furthermore, if straightforward appeals and other cooperative ideas won't work, pivoting to this is an obvious Plan B.\n\nAs far as *directly useful* consequences are concerned, I would empathize well-targeted interventions with easily evaluated victory/failure conditions, even more so than in 3.1.\n\nHowever, the best plays here might be *tactically useful* interventions, aimed at what Pragmatic AI Safety calls [diffuse factors](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/n767Q8HqbrteaPA25#Improving_Contributing_Factors). Such interventions don't directly decrease AI risk, but they create favourable conditions for *other* interventions. E. g., causing the public to be vaguely concerned about AI won't directly help, but a world in which the public is concerned is a world in which we're more well-positioned to influence the AI industry in other ways. (I'll come back to that in [Part 4](https://www.lesswrong.com/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry#4__What_You_Can_Do).)\n\n### **3.4. Joining the Winning Side**\n\nIn some sense, the easiest way to accomplish our goal is not to try to change the AI industry's incentive structures, but to *ride them*. The industry as a whole is agnostic with regards to alignment. It cares about:\n\n*   Easily measurable success metrics.\n*   Clear, factorizable research directions.\n*   Profit, funding.\n\nCurrent alignment research directions are none of these things. Progress is difficult to measure, the problem doesn't cleanly break down into sub-problems you could work on in isolation, and the results don't translate to e. g. more powerful and marketable ML models. Reducing the alignment problem to this sort of \"digestible\" form is non-trivial — that's the whole problem with our [lack of an established paradigm](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency#Preparadigmicity).\n\nBut. I think there are certain potential avenues of alignment research that are relatively amendable to being transformed into a form the AI industry would find digestible, while also being pretty effective and practical ways to make progress on alignment.\n\nAnd herein lies my disagreement with Pragmatic AI Safety. They suggest that [alignment research should aim for zero capabilities advancement](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/bffA9WC9nEJhtagQi#Minimal_Capabilities_Externalities). I think the metric to keep in mind here, instead, is *expected return on capabilities for an hour of research*.\n\n*   As far as influencing a current AI researcher is concerned, if we can make them pursue a research direction that somewhat progresses alignment while progressing capabilities *not faster* than what they'd be doing otherwise, our impact is positive.\n    *   Even if the research direction doesn't progress alignment very much on a per-hour basis, recall how relatively tiny AI Safety is. A significant fraction of AI researchers starting to progress alignment at 5% the rate of a specialized alignment researcher might double or triple raw progress industry-wide.\n*   It would, in fact, be ideal to find some research direction that has *a fractionally higher* return on capabilities as whatever the ML field is doing right now, but which *also* significantly progresses alignment. It wouldn't, in practice, shift 100% of the AI industry there, but it would go a great way towards that.\n\nIt pays to play along with the current industry incentives, most notably the profit motive. In addition, the idea of aiming for minimal capabilities externalities seems deeply unnatural for me for other reasons:\n\n*   In terms of \"in-house\" research, our field is, again, tiny. Any progress we make on capabilities will be a rounding error next to the rest of the AI industry's. We should aim for *maximum alignment progress*, period, it's difficult enough without any additional constraints.\n*   Moreover, I don't think alignment and capabilities are orthogonal. I think they're very much *positively correlated*. Alignment could be viewed [as an interface](https://www.lesswrong.com/posts/42YykiTqtGMyJAjDM/alignment-as-translation), or as being able to make a program [Do What You Mean](https://en.wikipedia.org/wiki/DWIM). A ML model that goes crazy off-distribution and kills everyone isn't just poorly aligned, it's also not very good at demonstrating good performance along the metrics the researchers actually care about ([relevant post](https://www.lesswrong.com/posts/YFckXwZg8Zcaib5ff/pop-culture-alignment-research-and-taxes#The_Impossibility_Of_A_Negative_Alignment_Tax)). [The unidentifiability problem](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J#3_2__The_task) isn't just a mesa-optimizer problem, it's also an acknowledged generic DL problem.\n\nAll in all, I'm optimistic about the existence of research projects that are at once 1) quickly reachable, 2) would find traction with the current status quo of the AI industry, 3) efficiently progress alignment. Crucially, finding such a research direction would nearly *guarantee* that the alignment solution is implemented in any future major model.\n\nThe most obvious candidate is mechanistic interpretability, of course (and it's no coincidence that it seems to be the most popular AI Safety direction outside our circle), but I have a few other ideas that I hope to post soon.\n\n### **3.5. Influencing the Research Culture**\n\nAll of the other approaches attempt to influence the AI industry through intermediaries: through the research projects it pursues, through the people it's implemented on, through the wider social environment it's embedded in. But perhaps there is room for a more direct intervention?\n\nThe industry is a social construct. The qualities that make a project a good one, the tastes the researchers have, the incentives they operate under — all of this is, to some extent, arbitrary. It has a ground-truth component, but the current configuration is not *uniquely determined* by the ground truth of the research subject. Rather, it's defined by *weights* that this social construct currently assigns to different features of the ground truth.\n\nThe current AI industry prefers tinkering to [empiricism](https://distill.pub/2020/circuits/zoom-in/#natural-science), and capabilities to safety. How can we shift this?\n\nThere's been two proposals that I've already mentioned:\n\n1.  [Chris Olah's](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety#Deliberate_design): Improve interpretability techniques enough that understanding some fraction of your model becomes normalized. Conceit:\n    *   *Right now, not knowing anything about how your model works internally is completely normal. If even partly understanding one’s model became normal, however, then the amount we don’t know might become glaring and concerning. Chris provides the following analogy to illustrate this: if the only way you’ve seen a bridge be built before is through unprincipled piling of wood, you might not realize what there is to worry about in building bigger bridges. On the other hand, once you’ve seen an example of carefully analyzing the structural properties of bridges, the absence of such an analysis would stand out.*\n2.  [Logan Riggs'](https://www.lesswrong.com/posts/vaHgLF2BCEdK3KxQd/convincing-all-capability-researchers): Pay N top ML researchers a fat chunk of money to work on alignment for 3 months, with the promise of a truly vast amount of money if they solve it.\n    *   Conceit: They won't manage to do it, but forcing them to engage with the problem would make them appreciate its difficulty, and shift research tastes field-wide towards it.\n    *   Issue: Defining what \"working on the alignment problem\" means, and ensuring they actually try to solve it, instead of \"trying to solve\" it but actually working on some trivialization of it, then disagreeing that their solution doesn't work.\n\nI think something like 2) is worth implementing. I'm unclear on how to evaluate 1); I'm guessing mechanistic interpretability just hasn't progressed that far yet. If we generalize from those two, though...\n\nWe want to synthesize a construct *C* with the following properties:\n\n1.  *C* is true.\n2.  *C* could be easily made part of the social reality of the AI industry or academic culture.\n3.  Common knowledge of *C* changes the AI industry's incentive gradients towards less focus on capabilities improvement and/or towards more work on AI Safety.\n\nWhat is a fact such that, if the researchers knew of it and knew other researchers or the general public or the grant-makers also knew of it, would make them pursue alignment research? What is a fact that would change the sociopolitical landscape such that the incentives shift, even fractionally, towards the things we want?\n\n\"Sufficiently powerful non-aligned AI is going to end the world and aligning AI is difficult\" as *C* evidently doesn't satisfy the second criterion. Logan's idea is to *force* it into the industry's social reality by a monetary injection. Chris Olah's idea has a more niche definition of AI risk as *C*, but either 2) is still difficult, or 3) didn't work out.\n\nBoth approaches take *C* for granted, then attempt to find paths to satisfying 2). I think there's some promise in *directly* mining the concept-space for a *C* that'd have the desired properties \"out of the box\". It's essentially the \"sideways appeals\" approach writ large.\n\nI suspect studying the history of science for cases where research tastes/standards changed would be useful here. An example of such a shift might be the replication crisis in psychology (in the broadest of terms).\n\nI suspect this approach hasn't been exhausted, given that it's pretty non-intuitive.\n\nI suspect this idea is overly clever in a very stupid way and will not actually work.\n\nStill, if it does work, there are probably some low-hanging fruits there. And just intuitively... If you go looking for a reason, you generally find one, don't you?\n\n**4\\. What You Can Do**\n-----------------------\n\nThe logistics graph that leads to a superintelligent AI's deployment has many bottlenecks, and controlling any *one* of them would be sufficient. Taking over the researcher supply, or the money supply, or the compute supply, or the research project supply, or the reputation supply, or the supply of any other crucial resource I'm not thinking of, would ensure excellent conditions for a safe advanced AI to emerge.\n\nBut the path to this doesn't look like a concentrated push along the corresponding dimension. As Pragmatic AI Safety points out, [diversification is key](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/n767Q8HqbrteaPA25#Diversification).\n\nThere are interdependencies everywhere: success at one thing affects the probabilities of success of all other projects. Finding an appealing research direction would make it easier to attract people our way. Putting social pressure on major AI labs would make safety-adjacent research directions more appealing. Shifting research tastes in a subfield would make it easier to change people's minds. And so on.\n\nMoreover, it's not obvious what bottleneck would be the easiest to gain control of, without the benefit of hindsight. Future events and novel discoveries may shift any part of the landscape in unpredictable ways, open or close doors for us.\n\nImproving AI Safety's future position means pursuing a strategy that is robust to such random environmental fluctuations. It means [maximizing our far-away action space](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/ahZQbxiPPpsTutDy2). We need to have a diversified portfolio of plans; we need to be improving our position all across the board, always looking for what new opportunities have arisen.\n\nIn theory, it would be great to have central coordination. Some organization or resource which tracks the feasibility of various interventions across the entire gameboard, and pursues/recommends those that move the gameboard into the most advantageous states while spending the least resources, ~and also you should put me in charge~.\n\nIn practice, this sort of coordination is both difficult and fragile, with a single point of failure. We're not a single organization, either, but a diverse conglomerate of organizations, movements, groups and individuals.\n\nBut we can *approximate* central coordination.\n\nIt's [often](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/n767Q8HqbrteaPA25#Diversification) [pointed](https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work) [out](https://astralcodexten.substack.com/p/more-antifragile-diversity-libertarianism) that impact in the modern world has a tail-heavy distribution. In some areas, it's effective to have many separate groups putting their full strength behind diverse high-variance projects. Many of them will fail, but some will succeed massively.\n\nThe project of advancing AI Safety is, to a large extent, one such area.[^a1x4c6dx9sb] My general advice would be as follows:\n\n*   Be opportunistic. Keep an up-to-date mental picture of the world, look for local opportunities to advance our cause, and coordinate with others where possible.\n    *   A good project also *creates* opportunities upon success; ideally, the whole thing should work as a positive feedback loop.\n*   [Use your Pareto frontier](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency#Use_Your_Pareto_Frontier). We need diversity; we don't want our projects to converge because we're using the same heuristics to pick them. Look for what you or your group specifically is uniquely well-placed or well-suited to do, and do that.\n*   Be ambitious. Again, tail-heavy distribution: going for low-success-rate high-reward plays is a good policy to adopt (as long as the only thing at stake is your plan's success!).\n*   Cheat. Not in the sense of being unethical, in the sense of reframing and revising the problem so that you can achieve 80% the impact with 20% the effort. It's not always possible, but it frequently is, in sociopolitical interventions especially. [Tug sideways](https://www.overcomingbias.com/2019/03/tug-sideways.html).\n*   Keep scale in mind. Changing the value of a crucial industry-wide variable by 1% is enormous impact.\n\nThere's a caveat here, though: without central coordination, how can we ensure that none of these disparate projects trip *each other* up? As I'd mentioned, successfully appealing to governments might mess up our relations with AI researchers, and failed persuasion attempts (macro- and micro- both) often make subsequent ones much harder.\n\nSome amount of that is probably inevitable. Part of it can be mitigated by trying out minimal/small-scale versions of any projects that might result in net-negative impact on failure. But a much larger chunk of it could be mitigated by ensuring that we—\n\n**5\\. Avoid Thermonuclear Ideas**\n---------------------------------\n\nYou likely know what I'm talking about. The class of ideas that includes lying and manipulation as its most tame members, and expands to cover some much worse extremes.\n\nI know some of these ideas may seem very clever and Appropriately Drastic, and the stakes — literally astronomical — could not be higher. We're accelerating directly into a wall, and our attempts to swerve away seem ineffectual. It may feel emotionally resonant to resolve to Stop Being Nice and Pull Out All the Stops and solve the problem in some gravely decisive fashion, By Any Means Necessary.\n\nBut it will not work in the real world, outside fantasies. It will not solve the problem in the long term, and in the meantime it will crash and burn, and hurt people, and ruin our PR, and tank the chances of other, more productive and realistic approaches. Even if you think *your* idea will definitely succeed, you're failing to think at scale. What would you expect to work better: a policy under which some of us pursue plans that blow up so hard they set us collectively back a few years, or a policy under which our plans only ever compound on each other's successes?\n\nFollowing the first policy is a defection, not just against the rest of society, but against all *our* other risk-mitigation initiatives. We're better than this.\n\nAs a rule-of-thumb, you can use something like [Shannon's maxim](https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle). If whatever clever plan you're considering *and the entire causal chain that led to it* became common knowledge, would it fail *and* destroy our credibility and our other plans? If yes, this is a radioactive plan, get it away.\n\nThings that seem like ruthless pragmatism are frequently not actually ruthlessly pragmatic. They're just excuses to indulge your base instincts.\n\nBe cool, in general. Find ways to be cool about this mess. We have [resources](https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of) for that and everything.\n\n**6\\. The Thin Line**\n---------------------\n\nI concur with [lc's post](https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like) and the people in [that post's comments](https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like?commentId=Hyg8pgLTY5oYFEdAK): we have a slight taboo against the sort of full-scale activism I'm arguing for. It's exemplified by [this](https://www.lesswrong.com/posts/3eP8D5Sxih3NhPE6F/usd20k-in-prizes-ai-safety-arguments-competition?commentId=hM6DkcDf5qqyCs4L2) sort of sentiment. I suspect it's a combination of two things:\n\n*   An instinctive desire to stay far, far away from the radioactive plans I've described in the previous section.\n*   A failure to shift between epistemic and instrumental rationality; between enforcing community norms internally and having an external impact.\n\nIt makes sense that it exists. One of the foundations of this movement is \"raising the sanity waterline\" — and approaching interactions with people outside the movement with less rigor is not how you set an example. It's also easier to enforce the same norms upon yourself and each other in all situations, instead of switching between different sets depending on context.\n\nI'm *tempted* to say that we've overcorrected here; that we can or *must* relax our standards somewhat, in the light of shortening timelines and in the face of our slow progress.\n\nBut I'm not sure. Relaxing the standards is *absolutely* a slippery slope. This decision might be \"meta-radioactive\", in the sense that it will see us accelerating straight into the epicenter of a nuclear explosion.\n\nI don't know how to strike the right balance here. It definitely *seems* like we can opt to be more effective without inching towards self-defeating Stupid Evil, but maybe the mere act of acknowledging that possibility would shift our social reality in undesirable ways? Maybe [John's position](https://www.lesswrong.com/posts/3eP8D5Sxih3NhPE6F/usd20k-in-prizes-ai-safety-arguments-competition?commentId=hM6DkcDf5qqyCs4L2) is right, and we should call out our epistemically suspect behavior even as we agree that it's the right thing to do.\n\n**7\\. Closing Thoughts**\n------------------------\n\nThe recent months have seen increasing amounts of alarm and doom-saying in our circles. AI capabilities are advancing rapidly, while our attempts to align it proceed at a frustratingly slow pace. There are [optimistic](https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan) [voices](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge), but the general disposition seems [quite grim](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy).\n\nWell. If alignment is really so hard, maybe we should quit trying to solve it?\n\nIn hindsight, I'm a bit baffled that field-building wasn't our main focus *this entire time*. Getting the AI industry to take AI risk seriously is a *necessary and sufficient* condition for survival. Solving alignment by ourselves is... neither. If the technical problems are truly insurmountable in the time we have left — and I don't yet know that they are, but I can certainly imagine it — we should just shift our focus to social-based solutions.\n\nThe goal, I should note, is not *outreach*. Convincing a few, or many, AI researchers to switch to alignment won't solve the problem where we have a multi-billion dollar industry stockpiling uranium in the hopes of spontaneously assembling a nuclear reactor. The aim should be to shift that status quo. Changing people's minds is a fine instrumental goal, but the terminal one is to influence [the robust agent-agnostic process](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic) itself.\n\nI'd like to suggest that there might be a snowball effect involved — that a 10% progress at this task would make the subsequent 90% easier, and so on. There might, indeed, be. I'm not that optimistic, though. I expect it'll be an uphill battle all the while, because the sort of carefulness we'd like to cultivate has the tendency to rot away, as organizations become corrupted and people value-drift.\n\nIt's possible that this is also impossible. That we can't change the AI industry in time, any more than we can independently solve alignment in time. But it seems less impossible to me.\n\nAnd if we keep looking for approaches that are less and less impossible, perhaps we'll find one that isn't impossible at all.\n\n[^hi5u3uygm4t]: Which may or may not have been recently confirmed by this. \n\n[^gx314q5i9w]: Very important. See point 7 here. \n\n[^a1x4c6dx9sb]: But not AI Safety itself, of course, only the project of spreading it. AI is very much Scott's Distribution 1, and the fact that our civilization is treating it as a Distribution 2 is the entire bloody problem.",
      "plaintextDescription": "The wider AI research community is an almost-optimal engine of apocalypse. The primary metric of a paper's success is how much it improves capabilities along concrete metrics, publish-or-perish dynamics supercharge that, the safety side of things is neglected to the tune of 1:49 rate of safety to other research, and most results are made public so as to give everyone else in the world a fair shot at ending it too.\n\nIt doesn't have to be this way. The overwhelming majority of the people involved do not actually want to end the world. There must exist an equilibrium in which their intentions match their actions.\n\nEven fractionally shifting the status quo towards that equilibrium would have massive pay-offs, as far as timelines are concerned. Fully overturning it may well constitute a sufficient condition for humanity's survival. Yet I've seen precious little work done in this area, compared to the technical questions of AI alignment. It seems to be picking up in recent months, though — and I'm happy to contribute.\n\nThis post is an attempt at a comprehensive high-level overview of the tactical and strategic options available to us.\n\n----------------------------------------\n\n\n1. Rationale\nWhy is it important? Why is it crucial?\n\nFirst. We, uh, need to make sure that if we figure alignment out people actually implement it. Like, imagine that tomorrow someone comes up with a clever hack that robustly solves the alignment problem... but it increases the compute necessary to train any given ML model by 10%, or it's a bit tricky to implement, or something. Does the wider AI community universally adopt that solution? Or do they ignore it? Or do the industry leaders, after we extensively campaign, pinky-swear to use it the moment they start training models they feel might actually pose a threat, then predictably and fatally misjudge that?\n\nIn other words: When the time comes, we'll need to convince people that safety is important enough to fuss around a bit for its sake. But i",
      "wordCount": 6408
    },
    "tags": [
      {
        "_id": "6zBEfFYJxhSEcchbR",
        "name": "AI Alignment Fieldbuilding",
        "slug": "ai-alignment-fieldbuilding"
      },
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "n2urKnXbevj2ryvGY",
    "title": "Agency As a Natural Abstraction",
    "slug": "agency-as-a-natural-abstraction",
    "url": null,
    "baseScore": 55,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2022-05-13T18:02:50.308Z",
    "contents": {
      "markdown": "**Epistemic status:** Speculative attempt to synthesize findings from several distinct approaches to AI theory.\n\n**Disclaimer:** The first three sections summarize some of Chris Olah's work on interpretability and John Wentworth's Natural Abstractions Hypothesis, then attempt to draw connections between them. If you're already familiar with these subjects, you can probably skip all three parts.\n\n**Short summary:** When modelling a vast environment where simple rules result in very complex emergent rules/behaviors (math, physics...), it's computationally efficient to build high-level abstract models of this environment. Basic objects in such high-level models often behave very unlike basic low-level objects, requiring entirely different heuristics and strategies. If the environment is so complex you build *many* such models, it's computationally efficient to go meta, and build a higher-level abstract model of building and navigating arbitrary world-models. This higher-level model necessarily includes the notions of optimization and goal-orientedness, meaning that mesa-optimization is the natural answer to any \"sufficiently difficult\" training objective. All of this has various degrees of theoretical, empirical, and informal support.\n\n* * *\n\n1\\. The Universality Hypothesis\n-------------------------------\n\nOne of the foundations of Chis Olah's approach to mechanistic interpretability is [the Universality Hypothesis](https://distill.pub/2020/circuits/zoom-in/#three-speculative-claims). It states that neural networks are subject to convergence — that they would learn to look for similar patterns in the training data, and would chain up the processing of these patterns in similar ways.\n\nThe prime example of this effect are CNNs. If trained on natural images (even from different datasets), the first convolution layer reliably learns [Gabor filters](https://en.wikipedia.org/wiki/Gabor_filter) and color-contrast detectors, and later layers show some convergence as well:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d4436c1d8fdbce6bd14b14c73f94cff539814a9e37d09971.png)\n\n*Analogous features across CNNs.* [*Source*](https://distill.pub/2020/circuits/zoom-in/)*.*\n\nIt's telling that these features seem to make sense to *us*, as well — that at least one type of *biological* neural network also learns similar features. (Gabor filters, for example, were known long before modern ML models.) It's the main reason to feel optimistic about interpretability at all — it's plausible that the incomprehensible-looking results of matrix multiplications will turn out to be not so incomprehensible, after all.\n\nIt's telling when universality *doesn't* hold, as well.\n\n[*Understanding RL Vision*](https://distill.pub/2020/understanding-rl-vision/) attempts to interpret an agent trained to play CoinRun, a simple platformer game. CoinRun's levels are procedurally generated, could contain deadly obstacles in the form of buzzsaws and various critters, and require the player to make their way to a coin.\n\nAttempting to use [feature visualization](https://distill.pub/2017/feature-visualization/) on the agent's early convolutional layers produces complete gibberish, lacking even Gabor filters:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1c95f8a0da0aa3e57834f9c091f974d4a2a69925bdb4b708.png)\n\n*Comparison between features learned by a CNN (left) and a RL agent (right).*\n\nIt's nonetheless possible to uncover a few comprehensible activation patterns via the use of [different techniques](https://distill.pub/2018/building-blocks/):\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4d50fe788d40c4d7f2a2f40066c541f136024bda02b843e1.png)\n\n*Visualization of positive and negative attributions. I strongly recommend checking out* [*the paper*](https://distill.pub/2020/understanding-rl-vision/) *if you haven't already, it has rich interactivity.*\n\nThe agent learns to associate buzzsaws and enemies with decreased chances of successfully completing a level, and could be seen to pick out coins and progression-relevant level geometry.\n\nAll of these comprehensible features, however, reside on the third convolutional layer. None of the other four convolutional layers, or the two fully-connected layers, contain anything that makes sense. The authors note the following:\n\n> Interestingly, the level of abstraction at which \\[the third\\] layer operates – finding the locations of various in-game objects – is exactly the level at which CoinRun levels are randomized using procedural generation. Furthermore, we found that training on many randomized levels was essential for us to be able to find any interpretable features at all.\n\nAt this point, they coin the Diversity Hypothesis:\n\n> *Interpretable features tend to arise (at a given level of abstraction) if and only if the training distribution is diverse enough (at that level of abstraction).*\n\nIn retrospect, it's kind of obvious. The agent would learn whatever improves its ability to complete levels, and only that. It needs to know how to distinguish enemies and buzzsaws and coins from each other, and to tell apart these objects from level geometry and level backgrounds. However, any buzzsaw looks like any other buzzsaw and behaves like any other buzzsaw and unlike any coin — the agent doesn't need a complex \"visual cortex\" to sort them out. Subtle visual differences don't reveal subtle differences in function, the wider visual context is irrelevant as well.  Learning a few heuristics for picking out the handful of distinct objects the game actually has more than suffices. Same for the higher-level patterns, the rules and physics of the game: they remain static.\n\nPutting this together with the (strong version of) the Universality Hypothesis, we get the following: ML models could be expected to learn interpretable features and information-processing patterns, but only if they're exposed to enough *objective-relevant diversity* across these features.\n\nIf this condition isn't fulfilled, they'll jury-rig some dataset-specialized heuristics that'd be hard to untangle. But if it is, they'll likely cleave reality along the same lines we do, instead of finding completely alien abstractions.\n\nJohn Wentworth's theory of abstractions substantiates the latter.\n\n(For completeness' sake, I should probably mention Chris Olah et al.'s more recent [work on transformers](https://transformer-circuits.pub/), as well. Suffice to say that it also uncovers some intuitively-meaningful information-processing patterns that reoccur across different models. Elaborating on this doesn't add much to my point, though.\n\nOne particular line stuck with me, however. When talking about a very simple one-layer attention-only transformer, and some striking architectural choices it made, they [note](https://www.youtube.com/watch?v=ZBlHFFE-ng8) that \"transformers desperately want to do meta-learning.\" Consider this to be... ominous foreshadowing.)\n\n2\\. The Natural Abstraction Hypothesis\n--------------------------------------\n\nReal-life agents are [embedded](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh) in the environment, which comes with a host of theoretical problems. For example, that implies they're *smaller* than the environment, which means they physically can't hold its full state in their head. To navigate it anyway, they'd need to assemble some simpler, lower-dimensional model of it. How can they do it? Is there the optimal, \"best way\" to do it?\n\n[The Natural Abstractions Hypothesis](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks) is aimed to answer this question. It's based on the idea that, for all the dizzying complexity that real-life objects have on the level of fundamental particles, most of the information they contain is only relevant — and, indeed, only *accessible* — locally.\n\nConsider the door across the room from you. The details of the fluctuation of the individual atoms comprising it never reach you, they are completely wiped out by the environment on the way to you. For the same reason, they don't *matter*. The information that reaches you, the information that's relevant to you and could impact you, is only the high-level summaries of these atoms' averaged-out behavior, consistent across time. Whether the door is open or closed, what material it is, its shape.\n\nThat's what natural abstractions are: high-level summaries of the low-level environment that contain only the information that actually reaches far-away objects.\n\n![](https://docs.google.com/drawings/u/1/d/sBkEaOsFr-9WGYMGxN1X6eg/image?w=498&h=169&rev=1&ac=1&parent=1XO91n2DGaeliBXdve9iywpCULNZ54YXnH3QgdYwWWOc)\n\nGraphical model representing interactions between objects *X* and *Y* across some environment *Z*. *f(X)* is the abstract model of *X*, containing only whatever information wasn't wiped out by *Z*.\n\nOf course, if you go up to the door with an electronic microscope and start making decisions based on what you see, the information that reaches you and is relevant to you would change. Similarly, if you're ordering a taxi to your house, whether that door is open or closed is irrelevant to the driver getting directions. That's not a problem: real-life agents are also known for fluidly switching between a *multitude* of abstract models of the environment, depending on the specific problem they're working through.\n\n\"Relevant to *you*\", \"reaches *you*\", etc., are doing a lot of work here. Part of the NAH's conceit is actually eliminating this sort of subjective terminology, so perhaps I should clean it up too.\n\nFirst, we can note that the information that *isn't* wiped out is whatever information is [represented with high redundancy in the low-level implementation of whatever object we care about](https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information) — e. g., an overwhelming amount of door-particles emit the same information about the door's material. In this manner, any sufficiently homogeneous/stable chunk of low-level reality corresponds to a valid abstraction.\n\nAn additional desideratum for a good abstraction is *global* redundancy. There are many objects like your door in the world. This means you can gather information on your door from other places, or gather information about other places by learning that they have \"a door\". This also makes having an internal symbol for \"a door\" useful.\n\nPutting these together, we can see how we can build entire *abstraction layers*: by looking for objects or patterns in the environment that are redundant both locally and globally, taking one type of such objects as a \"baseline\", then cleaving reality such that none of the abstractions overlap and the interactions between them are mediated by noisy environments that wipe out most of the detailed information about them.\n\nFundamental physics, chemistry, the macro-scale environment, astronomy, and also geopolitics or literary theory — we can naturally derive all of them this way.\n\nThe main takeaway from all of this is, good abstractions/high-level models are part of the *territory*, not the map. There's *some* degree of subjectivity involved — a given agent might or might not need to make use of the chemistry abstraction for whatever goal it pursues, for example — but the choice of abstractions isn't completely arbitrary. There's a very finite number of good high-level models.\n\nSo suppose the NAH is true; it certainly looks promising to me. It suggests the optimal way to model the environment given some \"reference frame\" — your scale, your task, etc. Taking the optimal approach to something is a convergent behavior. Therefore, we should expect ML models to converge towards similar abstract models when exposed to the same environment and given the same type of goal.\n\nSimilar across ML models, and familiar to us.\n\n3\\. Natural Abstractions Are Universal\n--------------------------------------\n\nLet's draw some correspondences here.\n\nInterpretable features are natural abstractions are human abstractions.\n\nThe Diversity Hypothesis suggests some caveats for the convergence towards natural abstractions. A given ML model would only learn the natural abstractions it has to learn, and no more. *General* performance in some domain requires learning the entire corresponding abstraction layer, but if a model's performance is evaluated only on some narrow task within that domain, it'll just overfit to that task. For example:\n\n*   InceptionV1 was exposed to a wide variety of macro-scale objects, and was asked to identify all of them. Naturally, it learned a lot of the same abstractions we use.\n*   The CoinRun agent, on the other hand, was exposed to a very simple toy environment. It learned all the natural abstractions which that environment contained — enemies and buzzsaws and the ground and all — but only them. It didn't learn a general \"cleave the visual input into discrete objects\" algorithm.\n\nThere are still reasons to be optimistic about interpretability. For one, any *interesting* AI is likely to develop general competence across many domains. It seems plausible, then, that the models we should be actually concerned about will be *more* interpretable than the contemporary ones, and also more similar to *each other*.\n\nAs an aside, I think this is all very exciting in general. These are quite different approaches, and it's *very* promising that they're both pointing to the same result. Chris' work is very \"bottom-up\" — taking concrete ML models, noticing some similarities between them, and suggesting theoretical reasons for that. Conversely, John's work is \"top-down\" — from mathematical theory to empirical predictions. The fact that they seem poised to meet in the middle is encouraging.\n\n4\\. Diverse Rulesets\n--------------------\n\nLet's consider the CoinRun agent again. It was briefly noted that its high-level reasoning wasn't interpretable either. The rules of the game never changed, it wasn't exposed to sufficient diversity across rulesets, so it just learned a bunch of incomprehensible CoinRun-specific heuristics.\n\nWhat if it *were* exposed to a wide variety of rulesets, however? Thousands of them, even? It can just learn specialized heuristics for every one of them, of course, plus a few cues for when to use which. But that has to get memory-taxing at some point. Is there a more optimal way?\n\nWe can think about it in terms of natural abstractions. Suppose we train 1,000 *separate* agents instead, each of them trained only on one game from our dataset, plus a \"manager\" model that decides which agent to use for which input. This ensemble would have all the task-relevant skills of the initial 1,000-games agent; the 1,000-games agent would be a compressed summary of these agents. A *natural abstraction* over them, one might say.\n\nA natural abstraction is a high-level summary of some object that ignores its low-level details and only preserve whatever information is relevant to some other target object. The information it ignores is information that'd be wiped out by environment noise on the way from the object to the target.\n\nOur target is the loss function. Our environment is the different training scenarios, with their different rulesets. The object we're abstracting over is the combination of different specialized heuristics for good performance on certain rulesets.[^iggzlt7ld9f]\n\nThe latter is the commonality across the models, the redundant information we're looking for: their ability to win. The noisy environment of the fluctuating rules would wipe out any details about the heuristics they use, leaving only the signal of \"this agent performs well\". The high-level abstraction, then, would be \"something that wins given a ruleset\". Something that outputs actions that lead to low loss no matter the environment it's in. Something that, [given some actions it can take, always picks those that lead to low loss *because* they lead to low loss](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a).\n\nConsequentialism. Agency. An optimizer.\n\n5\\. *Risks from Learned Optimization* Is Always Relevant\n--------------------------------------------------------\n\nThis result essentially restates some conclusions from [*Risks from Learned Optimization*](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB)*.* That paper specifically discusses the conditions in which a ML model is likely to become a mesa-optimizer (i. e., learn runtime optimization) vs. remain a bundle of specialized heuristics that were hard-coded by the base optimizer (the training process). In particular:\n\n> \\[S\\]earch—that is, optimization—tends to be good at generalizing across diverse environments, as it gets to individually determine the best action for each individual task instance. There is a general distinction along these lines between optimization work done on the level of the learned algorithm and that done on the level of the base optimizer: the learned algorithm only has to determine the best action for a given task instance, whereas the base optimizer has to design heuristics that will hold regardless of what task instance the learned algorithm encounters. Furthermore, a mesa-optimizer can immediately optimize its actions in novel situations, whereas the base optimizer can only change the mesa-optimizer's policy by modifying it ex-post. Thus, for environments that are diverse enough that most task instances are likely to be completely novel, search allows the mesa-optimizer to adjust for that new task instance immediately.\n> \n> For example, consider reinforcement learning in a diverse environment, such as one that directly involves interacting with the real world. We can think of a diverse environment as requiring a very large amount of computation to figure out good policies before conditioning on the specifics of an individual instance, but only a much smaller amount of computation to figure out a good policy once the specific instance of the environment is known. We can model this observation as follows.\n> \n> Suppose an environment is composed of \\\\(N\\\\) different instances, each of which requires a completely distinct policy to succeed in. Let \\\\(P\\\\) be the optimization power (measured in bits) applied by the base optimizer, which should be approximately proportional to the number of training steps. Then, let \\\\(x\\\\) be the optimization power applied by the learned algorithm in each environment instance and \\\\(f(x)\\\\) the total amount of optimization power the base optimizer must put in to get a learned algorithm capable of performing that amount of optimization. We will assume that the rest of the base optimizer's optimization power, \\\\(P - f(x)\\\\), goes into tuning the learned algorithm's policy. Since the base optimizer has to distribute its tuning across all \\\\(N\\\\) task instances, the amount of optimization power it will be able to contribute to each instance will be \\\\(\\frac{P - f(x)}{N}\\\\), under the previous assumption that each instance requires a completely distinct policy. On the other hand, since the learned algorithm does all of its optimization at runtime, it can direct all of it into the given task instance, making its contribution to the total for each instance simply \\\\(x\\\\).\n> \n> Thus, if we assume that, for a given \\\\(P\\\\), the base optimizer will select the value of \\\\(x\\\\) that maximizes the minimum level of performance, and thus the total optimization power applied to each instance, we get\n> \n> \\\\\\[x^* = \\text{argmax}_x~ \\frac{P - f(x)}{N} + x.\\\\\\]\n> \n> As one moves to more and more diverse environments—that is, as \\\\(N\\\\) increases—this model suggests that \\\\(x\\\\) will dominate \\\\(\\frac{P - f(x)}{N}\\\\), implying that mesa-optimization will become more and more favorable. Of course, this is simply a toy model, as it makes many questionable simplifying assumptions. Nevertheless, it sketches an argument for a pull towards mesa-optimization in sufficiently diverse environments.\n> \n> As an illustrative example, consider biological evolution. The environment of the real world is highly diverse, resulting in non-optimizer policies directly fine-tuned by evolution—those of plants, for example—having to be very simple, as evolution has to spread its optimization power across a very wide range of possible environment instances. On the other hand, animals with nervous systems can display significantly more complex policies by virtue of being able to perform their own optimization, which can be based on immediate information from their environment. This allows sufficiently advanced mesa-optimizers, such as humans, to massively outperform other species, especially in the face of novel environments, as the optimization performed internally by humans allows them to find good policies even in entirely novel environments.\n\n6\\. Multi-Level Models\n----------------------\n\nNow let's consider the issue of multi-level models. They're kind of like playing a thousand different games, no?\n\nIt's trivially true for the real world. Chemistry, biology, psychology, geopolitics, cosmology — it's all downstream of fundamental physics, yet the objects at any level behave *very unlike* the objects at a different level.\n\nBut it holds true even for more limited domains.\n\nConsider building up all of mathematics from the ZFC axioms. Same as physics, we start from some \"surface\" set of rules. We notice that the objects defined by them could be assembled into more complex structures, which could be assembled into more complex structures still, and so on. But at some point, performing direct operations over these structures becomes terribly memory-taxing. We don't think about the cosine function in terms of ZFC axioms, for example; we think about it as its own object, with its own properties. We build an abstraction, a high-level summary that reduces its internal complexity to the input -> output mapping.\n\nWhen doing trigonometry in general, we're working with an entire new abstraction layer, populated by many abstractions over terribly complex structures built out of axiomatic objects. Calculus, probability theory, statistics, topology — every layer of mathematics is a minor abstraction layer in its own right. And in a sense, every time we prove a theorem or define a function we'd re-use, we add a new abstract object.\n\nThe same broad thought applies to any problem domain where it's possible for sufficiently complex structures to arise. It's memory-efficient to build multiple abstract models of such environments, and then abstract over the heuristics for these models.\n\nBut it gets worse. When we're navigating an environment with high amounts of emergence, we don't know *how many* different rulesets we'd need to learn. We aren't exposed to 1,000 games all at once. Instead, as we're working on some problem, we notice that the game we're playing conceals higher-level (or lower-level) rules, which conceal another set of rules, and so on. Once we get started, we have no clue when that process would bottom out, or what rules we may encounter.\n\nHeuristics don't cut it. You need general competence given any ruleset to do well, and an ability to build natural abstractions given a novel environment, on your own. And if you're teaching yourself to play by completely novel rules, how can you even tell whether you're performing well, without the inner notion of a goal to pursue?\n\n(Cute yet non-rigorous sanity-check: How does all of this hold up in the context of human evolution? Surprisingly well, I think. The leading hypotheses for the evolution of human intelligence tend to tie it to society: [The Cultural Intelligence Hypothesis](https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/) suggests that higher intelligence was incentivized because it allowed better transmission of cultural knowledge, such as how to build specialized tools or execute incredibly tricky hunting strategies. The Machiavellian Intelligence points to the political scheming between *homo sapiens* themselves as the cause.\n\nEither is kind of like being able to adapt to new rulesets on the fly, and build new abstractions yourself. Proving a lemma is not unlike prototyping a new weapon, or devising a plot that abuses ever-shifting social expectations: all involve iterating on a runtime-learned abstract environment to build an even more complex novel structure in the pursuit of some goal.)\n\n7\\. A Grim Conclusion\n---------------------\n\nWhich means that any sufficiently powerful AI is going to be a mesa-optimizer.\n\nI suspect this is part of [what Eliezer is talking about](https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/7im8at9PmhbT4JHsW#1_1__Deep_vs__shallow_problem_solving_patterns) when he's being skeptical of tool-AI approaches. Navigating *any* sufficiently difficult domain, any domain in which structures could form that are complex enough to suggest many many layers of abstraction, is astronomically easier if you're an optimizer. It doesn't matter if your AI is only taught math, if it's a glorified calculator — any sufficiently powerful calculator *desperately wants to be an optimizer*.\n\nI suspect it's theoretically possible to deny that desperate desire, somehow. At least for some tasks. But it's going to be *very* costly — the cost of cramming specialized heuristics for 1,000 games into one agent instead of letting it generalize, the cost of setting *x* to zero in the mesa-optimizer equation while *N* skyrockets, the cost of forcing your AI to use the low-level model of the environment directly instead of building natural abstractions. You'd need vastly more compute and/or data to achieve the level of performance on par with naively-trained mesa-optimizers (for a given tech level)[^73b1awdyte9].\n\nAnd then it probably won't be any good anyway. A freely-trained 1,000-games agent would likely be general enough to play the 1,001th game without additional training. 1,000 separately-trained agents with a manager? Won't generalize, explicitly by design. Similarly, any system we forced away from runtime optimization won't be able to discover/build new abstraction layers on its own, it'd only be able to operate within the paradigms we already know. Which may or may not be useful.\n\nMesa-optimizers will end the world long before tool AIs can save us, the bottom line is.\n\n[^iggzlt7ld9f]: I feel like I'm abusing the terminology a bit, but I think it's right. Getting a general solution as an abstraction over a few specific ones is a Canonical Example, after all: the \"1+1=2*1\" & \"2+2=2*2\" => \"n+n=2*n\" bit. \n\n[^73b1awdyte9]: I'm put in mind of gwern's/nostalgebraist's comparison with \"cute algorithms that solve AI in some theoretical sense with the minor catch of some constant factors which require computers bigger than the universe\". As in, avoiding mesa-optimization for sufficiently complex problems may be \"theoretically possible\" only in the sense that it's absolutely impossible in practice.",
      "plaintextDescription": "Epistemic status: Speculative attempt to synthesize findings from several distinct approaches to AI theory.\n\nDisclaimer: The first three sections summarize some of Chris Olah's work on interpretability and John Wentworth's Natural Abstractions Hypothesis, then attempt to draw connections between them. If you're already familiar with these subjects, you can probably skip all three parts.\n\nShort summary: When modelling a vast environment where simple rules result in very complex emergent rules/behaviors (math, physics...), it's computationally efficient to build high-level abstract models of this environment. Basic objects in such high-level models often behave very unlike basic low-level objects, requiring entirely different heuristics and strategies. If the environment is so complex you build many such models, it's computationally efficient to go meta, and build a higher-level abstract model of building and navigating arbitrary world-models. This higher-level model necessarily includes the notions of optimization and goal-orientedness, meaning that mesa-optimization is the natural answer to any \"sufficiently difficult\" training objective. All of this has various degrees of theoretical, empirical, and informal support.\n\n----------------------------------------\n\n\n1. The Universality Hypothesis\nOne of the foundations of Chis Olah's approach to mechanistic interpretability is the Universality Hypothesis. It states that neural networks are subject to convergence — that they would learn to look for similar patterns in the training data, and would chain up the processing of these patterns in similar ways.\n\nThe prime example of this effect are CNNs. If trained on natural images (even from different datasets), the first convolution layer reliably learns Gabor filters and color-contrast detectors, and later layers show some convergence as well:\n\nAnalogous features across CNNs. Source.\nIt's telling that these features seem to make sense to us, as well — that at least one type ",
      "wordCount": 3822
    },
    "tags": [
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "5GYzBE6q89w74dqfk",
        "name": "Abstraction",
        "slug": "abstraction"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  }
]