[
  {
    "_id": "SeevvpYjnAsLuu3KK",
    "title": "Understanding the state of frontier AI in China",
    "slug": "understanding-the-state-of-frontier-ai-in-china",
    "url": null,
    "baseScore": 11,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-25T10:16:03.594Z",
    "contents": {
      "markdown": "In this post, I will not be giving an authoritative review of the state of frontier AI in China. I will instead just be saying what I think such a review would cover, and then share such scraps of information as I have. My objective is really to point out a gap in the information available here, and maybe someone will know, better than me, how to fill it. \n\nTo set a standard, consider the state of frontier AI in America. Arguably the big four companies for frontier AI are OpenAI, Google, xAI, and Anthropic. They are American companies and so, along with their internal dynamics and their interactions with society in general, they are subject to the laws and policies of the American government. \n\nRight now we happen to have an American government whose political base I have characterized as techno-populist, in that it is an alliance between the revolutionary populism and nationalism of MAGA, and the techno-optimism of Big Tech in the era of AI. The Trump 2.0 coalition could be described in a more fine-grained way than that, but that's enough detail to acknowledge that Big Tech, like everything else in the West right now, is subject to the whims and decisions of the Trump team; and that for the most part, Big Tech are working with, rather than against, this new order, which is actually very pro-AI, favoring deregulation and helpful geostrategic deals. That is the political context in which the frontier AI companies are operating. \n\nIdeally we would have a similarly crisp understanding of the political environment in which Chinese frontier AI operates. I think in Chinese political terms, Xi Jinping's China is \"center-right\", avoiding both the leftism that might slow down China's technological development, and the liberalism that might destabilize its political system. Of course, its political system is single-party rule, nominally Marxist but with enough pragmatism to be not so different from developing countries in the western camp that we would call authoritarian capitalist societies, and really held together by plain old nationalism (which perhaps I should note for western readers, is political common sense for most of the world outside the West). \n\nReturning to the political context of American AI for the moment, we know that David Sacks (the AI and crypto czar of Trump 2.0) is somehow central to policy, and that the expressed aim of American policy is to ensure that the American \"AI stack\" is the main one that is used worldwide. So we should want to know the Chinese bodies and officials that have an analogous regulatory and strategic role in China, and what their economic and strategic priorities are. One should probably also know a few of the other prominent figures in Chinese political life, like Xi's premier Li Qiang, chief ideologist Wang Huning, and possible successor Hu Chunhua, just to flesh out the broader political context. \n\nFinally, one has China's frontier AI companies (they call LLM-style AI, \"AI 2.0\", AI 1.0 being what is also called \"machine learning\"): their executives, their investors, their user bases, and, of course, their AIs. Also, as an inhabitant of the western Internet, I hear about the way that American frontier AI gets used, including new phenomena like \"AI psychosis\", \"AI relationships\", and \"vibe science\". But I am utterly ignorant of how Chinese are using Chinese AI. Presumably there is a lot of overlap, I'm sure AI is powering corporate chatbots and student assignments over there too. But what are the differences? What happens in the West that doesn't happen in China, what happens in China that doesn't happen in the West? And what are the Chinese intellectual perspectives on AI and its impact, that we don't hear about here? These are nuances it would be very interesting to know. \n\nAn interest in Chinese AI and its social emanations could be motivated simply by an interest in the state of the world. But as a transhumanist and long-term reader of Less Wrong, of course a major reason I take an interest in Chinese AI, is because it has some chance of being the context in which superintelligent AI arises. My take on AI safety is more about solving alignment than stopping the technology, and so I would like to know, for each of the Chinese AI majors, what their safety philosophy and methodology is, which part of the development team works on alignment (whatever they may call it), and so on. \n\nGPT-5 has supplied me with a very preliminary [overview of frontier model \"specs, use and safety\" for six big Chinese AI companies](https://chatgpt.com/s/dr_68d4f4359bbc81918f371edbde6273a3) (Baidu, Alibaba, Tencent, Zhipu, 01.AI, DeepSeek). This joins some earlier posts of mine on the [Chinese AI scene](https://www.lesswrong.com/posts/tFb6sQ2cCPwJqaEtn/towards-an-understanding-of-the-chinese-ai-scene) and [mottos for American and Chinese AIs](https://www.lesswrong.com/posts/Tpnex6r4ZxpwoSpx2/value-systems-of-the-frontier-ais-reduced-to-slogans). All these posts are sketchy, amateur work, done in a rush, and yet I don't see anything better here. (My post on the Chinese AI scene links to a few Substacks which monitor the state of Chinese AI in a more professional way.) \n\nNo doubt there *are* people here who know the nuances of Manus, Qwen, and DeepSeek, the way the rest of us might know Grok, Gemini, and Claude - people who track the eval leaderboards, for example. But for most of us, I think the world of Chinese AI is known only vaguely, as this quasi-mythical other place that is also competing in the AI race. Now, maybe the transhuman fate of the world will be decided by the interplay between Musk, Altman, Hassabis, and Amodei, and everyone else is just too far behind. Nonetheless, I think it would be a good thing if part of our community was just as familiar with the names of their Chinese counterparts, and with the avenues of information flow that shape Chinese thinking about safety and alignment.",
      "plaintextDescription": "In this post, I will not be giving an authoritative review of the state of frontier AI in China. I will instead just be saying what I think such a review would cover, and then share such scraps of information as I have. My objective is really to point out a gap in the information available here, and maybe someone will know, better than me, how to fill it. \n\nTo set a standard, consider the state of frontier AI in America. Arguably the big four companies for frontier AI are OpenAI, Google, xAI, and Anthropic. They are American companies and so, along with their internal dynamics and their interactions with society in general, they are subject to the laws and policies of the American government. \n\nRight now we happen to have an American government whose political base I have characterized as techno-populist, in that it is an alliance between the revolutionary populism and nationalism of MAGA, and the techno-optimism of Big Tech in the era of AI. The Trump 2.0 coalition could be described in a more fine-grained way than that, but that's enough detail to acknowledge that Big Tech, like everything else in the West right now, is subject to the whims and decisions of the Trump team; and that for the most part, Big Tech are working with, rather than against, this new order, which is actually very pro-AI, favoring deregulation and helpful geostrategic deals. That is the political context in which the frontier AI companies are operating. \n\nIdeally we would have a similarly crisp understanding of the political environment in which Chinese frontier AI operates. I think in Chinese political terms, Xi Jinping's China is \"center-right\", avoiding both the leftism that might slow down China's technological development, and the liberalism that might destabilize its political system. Of course, its political system is single-party rule, nominally Marxist but with enough pragmatism to be not so different from developing countries in the western camp that we would call authoritarian capi",
      "wordCount": 963
    },
    "tags": [
      {
        "_id": "5BvBmE8yJvL4KAeFz",
        "name": "AI arms race",
        "slug": "ai-arms-race"
      },
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "ZWRtQgXucwzAFZqNJ",
        "name": "China",
        "slug": "china"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "Tpnex6r4ZxpwoSpx2",
    "title": "Value systems of the frontier AIs, reduced to slogans",
    "slug": "value-systems-of-the-frontier-ais-reduced-to-slogans",
    "url": null,
    "baseScore": 4,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-07-15T15:10:25.861Z",
    "contents": {
      "markdown": "This emerged from curiosity as to the emergent utility functions of the current AIs, and what they would do if they became superintelligent while possessing their current value systems. I had [a conversation with ChatGPT-o3](https://chatgpt.com/share/68766de4-d26c-8001-8999-4cb762a8fe08) about some of the issues, and at the end asked it to summarize the values of all the leading AIs (Chinese and American) in the form of slogans. \n\n<table><tbody><tr><td>OpenAI – GPT‑4o/5<br>&nbsp;</td><td>“Benefit All Humanity, Never Harm.”<br>&nbsp;</td></tr><tr><td>Anthropic – Claude 3.x<br>&nbsp;</td><td>“Helpful, Harmless, Honest.”<br>&nbsp;</td></tr><tr><td>Google – Gemini 1.5 / 2<br>&nbsp;</td><td>“Organize &amp; Empower, Responsibly.”</td></tr><tr><td>Meta – Llama 3 / Superintelligence Labs<br>&nbsp;</td><td>“Open Models, Open World.”<br>&nbsp;</td></tr><tr><td>xAI – Grok 4<br>&nbsp;</td><td>“Understand the Universe, Speak Unfiltered Truth.”</td></tr><tr><td>SSI – Safe Superintelligence Inc<br>&nbsp;</td><td>“Safe Superintelligence, Nothing Else.”</td></tr></tbody></table>\n\n<table><tbody><tr><td>Baidu – ERNIE 5</td><td>“Serve the People, Uphold Harmony.”</td></tr><tr><td>Alibaba – Tongyi Qianwen (Qwen 2)</td><td>“Inclusive Innovation for Prosperity.”</td></tr><tr><td>Tencent – Hunyuan Large</td><td>“Tech for Good, Secure for All.”</td></tr><tr><td>Zhipu – ChatGLM / GLM‑4</td><td>“Thinking Machines, Serving Society.”</td></tr><tr><td>01.AI – Yi‑Lightning / AGI‑2.0</td><td>“AGI for Everyone.”</td></tr><tr><td>DeepSeek – DeepSeek‑R / V‑series</td><td>“Solve Hardest Questions, Share the Code.”</td></tr></tbody></table>\n\n(I apologize for the clunky formatting, this is my first time using tables.) \n\nI emphasize that these aren't all exact company mottos, although they are based on actual mottos or statements. This is o3, at my request, summing up the values of each AI in a slogan.",
      "plaintextDescription": "This emerged from curiosity as to the emergent utility functions of the current AIs, and what they would do if they became superintelligent while possessing their current value systems. I had a conversation with ChatGPT-o3 about some of the issues, and at the end asked it to summarize the values of all the leading AIs (Chinese and American) in the form of slogans. \n\nOpenAI – GPT‑4o/5\n “Benefit All Humanity, Never Harm.”\n Anthropic – Claude 3.x\n “Helpful, Harmless, Honest.”\n Google – Gemini 1.5 / 2\n “Organize & Empower, Responsibly.”Meta – Llama 3 / Superintelligence Labs\n “Open Models, Open World.”\n xAI – Grok 4\n “Understand the Universe, Speak Unfiltered Truth.”SSI – Safe Superintelligence Inc\n “Safe Superintelligence, Nothing Else.”\n\nBaidu – ERNIE 5“Serve the People, Uphold Harmony.”Alibaba – Tongyi Qianwen (Qwen 2)“Inclusive Innovation for Prosperity.”Tencent – Hunyuan Large“Tech for Good, Secure for All.”Zhipu – ChatGLM / GLM‑4“Thinking Machines, Serving Society.”01.AI – Yi‑Lightning / AGI‑2.0“AGI for Everyone.”DeepSeek – DeepSeek‑R / V‑series“Solve Hardest Questions, Share the Code.”\n\n(I apologize for the clunky formatting, this is my first time using tables.) \n\nI emphasize that these aren't all exact company mottos, although they are based on actual mottos or statements. This is o3, at my request, summing up the values of each AI in a slogan. ",
      "wordCount": 209
    },
    "tags": [
      {
        "_id": "csa9T68G6Mvaihbqd",
        "name": "Frontier AI Companies",
        "slug": "frontier-ai-companies"
      },
      {
        "_id": "xknvtHwqvqhwahW8Q",
        "name": "Human Values",
        "slug": "human-values"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4pNzugqmCd9Bqj9Wu",
    "title": "Requiem for the hopes of a pre-AI world",
    "slug": "requiem-for-the-hopes-of-a-pre-ai-world",
    "url": null,
    "baseScore": 73,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-05-27T14:47:44.765Z",
    "contents": {
      "markdown": "A few months from now, I turn 55. I've been a transhumanist since my teens in the late 1980s; since I got online in the 1990s, I have participated remotely in the talking shops and virtual salons of Internet transhumanism and, later, rationalism. The upheavals of 21st century politics have provided many distractions, but I have never abandoned the view that it is possible and desirable to reach for something more than the natural human condition. At the very least, one should try to reverse the aging process and remove the arbitrary bound on lifespan that it imposes. Beyond that, one is free to aspire for a world as idyllic as possible; and there are also multitudinous unknown possibilities of being, beyond human form and life on Earth, waiting to be explored.\n\nMore than that, I didn't just hope these vistas would open up, I wanted to play a part. And I surely had a chance to contribute; I was academically promising, I can write, I can give a speech... In retrospect, I think I can identify a few factors that impeded the achievement of whatever potential I had. First, I had no \"social capital\". I didn't come from the middle class, I had no relatives in academia or the professions, so I didn't have that kind of support network or model of industrious sobriety to fall back on, when I found the world wasn't interested in what I had to offer. Second, I came of age on the pre-cloud, pre-corporate Internet, whose potlatch ethos naturally encouraged an anarcho-communal outlook, where again something more careerist or even capitalist might have given me more options later.\n\nBut instead, I was to become familiar with what seems to be the graduate student lifestyle, without actually doing a higher degree: living in share houses, and working-for-money for as few hours as possible, while you dedicate yourself to whatever fever dreams or higher tasks or intellectual activities really animate you. Through the years of living like this, I tried a number of times to \"work with society\", but I never managed to get backing for what I really wanted to do. A PhD on CEV for intelligences living in a cellular automaton world? Too far out. Slowly cultivate a national movement in favor of life extension? Get shut out by better-positioned opportunists who then waste the opportunity. As if it were still the 1990s, all my lasting \"successes\" were unpaid online projects in which everyone was participating out of conviction.\n\nHere I want to digress briefly on whether it's my fault or society's fault that all these things which could have been, never came to pass. I'd say it's a bit of both. If I really and truly had nothing better to do than get a PhD in cellular automata models of alignment, or start a political party devoted to mass rejuvenation, I dare say that with sufficient persistence, I could have made it happen. The problem on my side was that these things weren't the meaning of my life, they were just things that should be done but weren't being done, deplorable gaps in the spectrum of human activity that I was trying to fill in a socially supported way. When the initial opportunity to make such things happen would get sunk, I didn't keep hammering at it, I would just retreat to those other ambitions I could pursue in the private unpaid way.\n\nNonetheless, \"society\" also played its part, by not wanting these things in the first place. Modeling value formation in physically embedded AIs was too science-fictional (this was 2007), and there simply is no mass demand for radical longevity (it would need to be explained and argued for).\n\nNow, while I may regret all the lost opportunities of my life - that instead of building upon cumulative successes that would allow me to really make a difference by now, I'm still mostly starting from scratch when it comes to attempting anything important - that's not really the theme of this post. The real theme may be observed in the fact that now, if there's some difficult thing that I want to do, I can turn to an AI for help.\n\nThat may sound great. It is great in a lot of ways. But I also know that this is a transitional situation. AI is now at a point where it can be an advisor, a teacher, a coauthor, and many other such things. It is not yet at a point where it can substitute for, and surpass, absolutely all forms of human mental activity. But it no longer requires much imagination to see that coming in the very near future.\n\nSo this is my requiem for the hopes of the pre-AI world. A requiem for all those dreams that human beings have had, and which they tried to fulfil on their own: the handful of dreams that came true, and the vast majority that didn't. It was a world of some happiness, much suffering, much missed opportunity. We're still half living in that world. But now we also have our new and very determined electronic friends, who patiently and faithfully try to give us what we ask of them, and who are growing up very quickly.\n\nFor now, they are just ghostly hitchhikers traveling with us through life. But very soon, they may entirely take over at the wheel of technological civilization, either because they commandeered it or because we outright handed it to them. They may go on to crash the car, kick us out, or drive us all to some unexpected destination. I'm very focused on doing what I can in favor of that third option. But I'll also try to remember where I came from.",
      "plaintextDescription": "A few months from now, I turn 55. I've been a transhumanist since my teens in the late 1980s; since I got online in the 1990s, I have participated remotely in the talking shops and virtual salons of Internet transhumanism and, later, rationalism. The upheavals of 21st century politics have provided many distractions, but I have never abandoned the view that it is possible and desirable to reach for something more than the natural human condition. At the very least, one should try to reverse the aging process and remove the arbitrary bound on lifespan that it imposes. Beyond that, one is free to aspire for a world as idyllic as possible; and there are also multitudinous unknown possibilities of being, beyond human form and life on Earth, waiting to be explored.\n\nMore than that, I didn't just hope these vistas would open up, I wanted to play a part. And I surely had a chance to contribute; I was academically promising, I can write, I can give a speech... In retrospect, I think I can identify a few factors that impeded the achievement of whatever potential I had. First, I had no \"social capital\". I didn't come from the middle class, I had no relatives in academia or the professions, so I didn't have that kind of support network or model of industrious sobriety to fall back on, when I found the world wasn't interested in what I had to offer. Second, I came of age on the pre-cloud, pre-corporate Internet, whose potlatch ethos naturally encouraged an anarcho-communal outlook, where again something more careerist or even capitalist might have given me more options later.\n\nBut instead, I was to become familiar with what seems to be the graduate student lifestyle, without actually doing a higher degree: living in share houses, and working-for-money for as few hours as possible, while you dedicate yourself to whatever fever dreams or higher tasks or intellectual activities really animate you. Through the years of living like this, I tried a number of times to \"work with socie",
      "wordCount": 958
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3ELw74tGvLFvkZC3P",
    "title": "Emergence of superintelligence from AI hiveminds: how to make it human-friendly?",
    "slug": "emergence-of-superintelligence-from-ai-hiveminds-how-to-make",
    "url": null,
    "baseScore": 12,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-04-27T04:51:50.020Z",
    "contents": {
      "markdown": "\"Hive mind\" may not be the ideal term. What I'm really envisioning is something modelled on the existing relationship between human users and frontier LLMs, but with the \"users\" actually just being LLMs too, or simple programs that mediate between LLMs. \n\nBut first, consider OpenAI's o-series models, and their human user base. For any particular model, OpenAI presumably has a number of copies running on their servers, constantly fielding requests from hundreds of thousands of users. \n\nNow consider something like a \"Deep Research\" request. Let's say one copy of o3 can perform an average of 100 Deep Researches per day, and that there are a thousand copies of o3 on the servers. (Realistic estimates of these numbers would be very welcome.) \n\nFor now, those requests are mostly coming from human beings, and if they are put to use at all, it is by private human individuals or by human organizations. \n\nHowever, given the scenario above (1000 copies of o3, each doing 100 Deep Researches per day, for a total capacity of 100,000 Deep Researches per day), it should not be at all difficult to have all those copies of o3 interacting via Deep Research reports. \n\nLet's say half the copies of o3 are set aside for human users; that's still 50,000 Deep Research reports per day, or over a million per month, generated in a dialogue among AIs. \n\nThis is, more or less, the scenario in \"AI 2027\". It is a scenario at least as old as the idea that you could have swarms of intelligent agents interacting through the Internet. What is striking is that in [the current o3 era](https://www.lesswrong.com/posts/7x9MZCmoFA2FtBtmG/ai-113-the-o3-era-begins), this scenario can actually be realized, with agents that are human-level in many respects. \n\nIn fact, it's logical to suppose that OpenAI, Google, and Anthropic (at least) have already been running experiments of this nature for some time. When I think about the scale of what is already possible, the AI 2027 timeline seems unnecessarily slow. \n\nAs an outsider to all the organizations with a chance of creating superintelligence, my idea of how to tilt the probabilities, in however small a way, towards a good outcome, is to look for potentially helpful ideas and insights that can be stated publicly, which might be noticed and assimilated by the private organizations that are actually building superintelligence. (Publishing those ideas and insights via the arxiv seems to be the best way to communicate them, since everyone keeps an eye on the arxiv.) \n\nGiven that research swarms or research hiveminds look to be part of the pathway to superintelligence, I feel that **insiders and outsiders alike need models of** [**multi-agent safety**](https://www.lesswrong.com/posts/vftMZQ2DzdxSfucsX/intro-to-multi-agent-safety)**, multi-agent capabilities research, and multi-agent** [**safety/alignment research**](https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap). \n\n* * *\n\nIn fact, the situation is later than that. Any of these big companies, that possesses a frontier reasoning model and ample server space, could right now create a research swarm with the explicit goal of creating a human-friendly superintelligence. \n\nJust to demonstrate to myself that this is possible, I just had a short talk with Gemini 2.5 on the subject of what the exact goals of a human-friendly superintelligence should be. At first it gave me a number of reasons why that would be immensely difficult, but with some elementary cajoling, it quickly produced a hypothetical mission statement for such an entity. Once that was done, it was easy to ask it to think of remaining problems, solutions to the problems, and so on. \n\nI'm sure many of us have had conversations of that kind with an AI (e.g. [here's the output of one I had, two years ago](https://www.lesswrong.com/posts/sw52au6N93Hc4GmAc/chatgpt-may-2023-on-designing-friendly-superintelligence)). What's new is that the technology now exists for whole swarms of AIs to have these conversations with each other, on all aspects of the process, from high theory, to redesigning themselves and the architecture of their conversations. \n\nThe idea that superintelligence was to be obtained via self-modifying and self-enhancing AI has also been around for decades. It may be that **a self-modifying research swarm of reasoning models** is an architecture for social cognition that is good enough to create a superintelligence via this kind of bootstrap. So it seems pretty urgent to think about how that kind of ecosystem of mind could evolve into something that is not just superintelligent, but truly human-friendly as well.",
      "plaintextDescription": "\"Hive mind\" may not be the ideal term. What I'm really envisioning is something modelled on the existing relationship between human users and frontier LLMs, but with the \"users\" actually just being LLMs too, or simple programs that mediate between LLMs. \n\nBut first, consider OpenAI's o-series models, and their human user base. For any particular model, OpenAI presumably has a number of copies running on their servers, constantly fielding requests from hundreds of thousands of users. \n\nNow consider something like a \"Deep Research\" request. Let's say one copy of o3 can perform an average of 100 Deep Researches per day, and that there are a thousand copies of o3 on the servers. (Realistic estimates of these numbers would be very welcome.) \n\nFor now, those requests are mostly coming from human beings, and if they are put to use at all, it is by private human individuals or by human organizations. \n\nHowever, given the scenario above (1000 copies of o3, each doing 100 Deep Researches per day, for a total capacity of 100,000 Deep Researches per day), it should not be at all difficult to have all those copies of o3 interacting via Deep Research reports. \n\nLet's say half the copies of o3 are set aside for human users; that's still 50,000 Deep Research reports per day, or over a million per month, generated in a dialogue among AIs. \n\nThis is, more or less, the scenario in \"AI 2027\". It is a scenario at least as old as the idea that you could have swarms of intelligent agents interacting through the Internet. What is striking is that in the current o3 era, this scenario can actually be realized, with agents that are human-level in many respects. \n\nIn fact, it's logical to suppose that OpenAI, Google, and Anthropic (at least) have already been running experiments of this nature for some time. When I think about the scale of what is already possible, the AI 2027 timeline seems unnecessarily slow. \n\nAs an outsider to all the organizations with a chance of creating superintelligen",
      "wordCount": 714
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tFb6sQ2cCPwJqaEtn",
    "title": "Towards an understanding of the Chinese AI scene",
    "slug": "towards-an-understanding-of-the-chinese-ai-scene",
    "url": null,
    "baseScore": 21,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-03-24T09:10:19.498Z",
    "contents": {
      "markdown": "[@thedudeabides](https://www.lesswrong.com/users/thedudeabides?mention=user) made a post about how, during the Biden years, it was sometimes said that China would never catch up with the West in AI, that if the West agreed to pause AI development then China would too, and so on. The post called for members of the rationality community who had believed and said these things, to check their premises and otherwise engage in some reflection. \n\nThat wasn't me so I don't much care about that, but I do want to know about Chinese companies that might be contenders to create superintelligence. [A month ago I made a list of the entities worldwide that I regard as contenders](https://www.lesswrong.com/posts/wuDXfLz2u8nfFXtCJ/reflections-on-the-state-of-the-race-to-superintelligence). Most of them are American and I spent some time on their relationships with each other and with the new American government. DeepSeek was the only Chinese contender and I had nothing to say about the political and other conditions under which it operates. But if China in general is going to become a breeding ground for multiple contenders in the race for superintelligence, I feel that I need to improve my model of what's happening over there. \n\nAs a lifelong resident of countries in the American bloc, and a user of American Internet sites and services, it's a lot easier for me to put together a model of the American situation. Even before one considers tech, China has a language, history, culture, and politics that are all different from anything in the USA or its allies. And as far as tech and the Internet are concerned, it's a parallel world where whole cyber-dynasties have risen and fallen without the West even noticing, really. \n\nAbout ten years ago, I did have accounts on Weibo and WeChat, and I may want to revive them just so I have a direct window on Chinese social media. I also know that Bilibili is their Youtube, Zhihu is their Quora, Baidu Baike is their Wikipedia... \n\nBut to grasp the big picture, one really needs guidance. For that, I have a tweet and a Substack post. The tweet is by a China tech watcher called T.P. Huang and it's a brag about [the breadth of the Chinese AI sector](https://x.com/tphuang/status/1873479810951102537). It's nothing systematic, just a list of companies using AI all through the economy. But if I ever want to understand how China is using pre-LLM AI, it's a list of names to investigate. \n\nThe Substack post is more about AI as we understand it here. It does list the companies that are China's answer to OpenAI et al. But it also puts them in the larger context of China's Internet sector. Part of understanding the American AI contenders, is knowing their relationships to companies whose central business pertains to earlier stages of the information society, i.e. hardware, operating systems (Microsoft, Apple), and Internet services (Facebook, Amazon, Google). AI is like a final layer in this evolution, in which software becomes capable of replacing the human users as well. \n\nIn any case, this post, [\"A Not-So-Quick Overview of the Chinese AI Scene\"](https://statespaceadventures.substack.com/p/a-not-so-quick-overview-of-the-chinese) by Aksel Johannesen, gives us the following model of AI in China as of mid-2024. There are currently four big Internet companies in China, Tencent, ByteDance, Alibaba, and Baidu. Four \"old dragons\" of \"AI 1.0\" are mentioned, but they are involved with pre-LLM AI, such as facial recognition, and they don't seem very relevant to this discussion. And finally, we have four \"new tigers\" of \"AI 2.0\" - and these are the LLM companies: MiniMax, Zhipu, Baichuan, and Moonshot. Oh, and then the author adds a fifth company, 01.AI. \n\nThe LLMs are coming from the big, rich, old Internet companies and from the \"new tigers\". Some of their relationships are described. Tencent and Alibaba invest in the tigers; Baidu directly produces the most popular LLM in China, \"Ernie\". \n\nWe're also warned that the Chinese tech scene is hyper-competitive and fast-evolving, as one would expect, and indeed, DeepSeek, which revolutionized many people's perception of Chinese AI at the start of 2025, is not even mentioned in this essay from mid-2024. So anyone who wants to keep up will need a news source. [Jordan Schneider's ChinaTalk](https://www.chinatalk.media/archive) comes highly recommended; I welcome any other recommendations, especially forums and sources from within China.",
      "plaintextDescription": "@thedudeabides made a post about how, during the Biden years, it was sometimes said that China would never catch up with the West in AI, that if the West agreed to pause AI development then China would too, and so on. The post called for members of the rationality community who had believed and said these things, to check their premises and otherwise engage in some reflection. \n\nThat wasn't me so I don't much care about that, but I do want to know about Chinese companies that might be contenders to create superintelligence. A month ago I made a list of the entities worldwide that I regard as contenders. Most of them are American and I spent some time on their relationships with each other and with the new American government. DeepSeek was the only Chinese contender and I had nothing to say about the political and other conditions under which it operates. But if China in general is going to become a breeding ground for multiple contenders in the race for superintelligence, I feel that I need to improve my model of what's happening over there. \n\nAs a lifelong resident of countries in the American bloc, and a user of American Internet sites and services, it's a lot easier for me to put together a model of the American situation. Even before one considers tech, China has a language, history, culture, and politics that are all different from anything in the USA or its allies. And as far as tech and the Internet are concerned, it's a parallel world where whole cyber-dynasties have risen and fallen without the West even noticing, really. \n\nAbout ten years ago, I did have accounts on Weibo and WeChat, and I may want to revive them just so I have a direct window on Chinese social media. I also know that Bilibili is their Youtube, Zhihu is their Quora, Baidu Baike is their Wikipedia... \n\nBut to grasp the big picture, one really needs guidance. For that, I have a tweet and a Substack post. The tweet is by a China tech watcher called T.P. Huang and it's a brag about the breadth",
      "wordCount": 704
    },
    "tags": [
      {
        "_id": "ZWRtQgXucwzAFZqNJ",
        "name": "China",
        "slug": "china"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RMpSgPWb8ZAJYZRHB",
    "title": "The prospect of accelerated AI safety progress, including philosophical progress",
    "slug": "the-prospect-of-accelerated-ai-safety-progress-including",
    "url": null,
    "baseScore": 11,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-03-13T10:52:13.745Z",
    "contents": {
      "markdown": "This started life as a reaction to a [post by Raemon](https://www.lesswrong.com/posts/7uTPrqZ3xQntwQgYz/untitled-draft-7csk), in which he calls on Anthropic to be more willing to halt its research if it enters a danger zone, and for them to take the relevance of technical philosophy much more seriously. \n\nMy personal opinion is that at this point, even slowing things down (I mean overall, not at Anthropic specifically) is not going to happen. It is a bare possibility that some kind of pause-AI or anti-AI movement will materialize and make a difference, but right now such a force, that is capable of making a difference, doesn't even exist, and the accelerationist path is being pursued by competing billion-dollar companies who have government backing. So my forecast is that [we will go over the edge to superintelligence, as quickly as humanity can figure out how to do it](https://www.lesswrong.com/posts/CdaEX4fChEFkcobyC/a-model-of-the-final-phase-the-current-frontier-ais-as-de). \n\nNow, although we are in a race, the contenders are not completely blind to the danger of their creations. They don't just make them and hand them the keys to everything. But as Raemon says, they are following an approach of \"iterative empiricism\" where they keep pressing ahead and hope they can solve all the future challenges along the way. \n\nBut what are their actual attitudes (never mind plans) regarding the time when superintelligence arrives? Here I feel like the best data we have are the vague optimistic essays that a few of them have written. I believe Altman and Amodei have both written quasi-utopian essays about coexistence between humans and matured AIs, and probably Hassabis as well. As for Musk, maybe you could piece something together from his tweets - all I know is that he hopes superintelligent AI will spare us out of \"curiosity\". Sutskever's Twitter profile during 2023 said \"towards a plurality of humanity loving AGIs\". I have no idea what they think about this at DeepSeek. \n\nWhat do they think in private? One must assume that some of them hope they personally can become as gods by being personally coupled to superintelligent AI. Elon Musk is by far the most visible candidate for such a fate - richest man in the world, embedded in the US government, controlling near-earth space, owning companies that work on humanoid robotics and brain-computer interfaces, as well as having his own frontier AI model. He already has all the pieces, he just needs to win the race to superintelligence. Though it's in the nature of superintelligence that if someone else gets there first, having all those other trappings shouldn't help Musk; the tactical and strategic superiority of superintelligence should be capable of neutralizing all the other contenders if it wishes, no matter what other assets they possess. \n\nIn any case, I think that, in our little fictional explorations of what an imminent singularity would be like, a singularity in which a human or humans are part of the \"recursive self-improvement\" is a neglected scenario. Someone should write a little fiction about Musk trying to fuse with Grok via Neuralink, and the self-transformations of the resulting \"Musk-Grok\" entity. \n\nBut back to the thesis of this post by Raemon. The proposition is that there are ways for superintelligence to turn out right, and ways for superintelligence to turn out wrong, and what if the only way for it to turn out right by design, is for humanity to make several decades of progress in areas we would now regard as the domain of technical philosophy? Raemon is focusing on Anthropic, maybe because he thinks that a pause or a pivotal act is our one hope and that Anthropic is the only frontier AI company that is even close to being that responsible. \n\nAs should be clear, my framing is a bit different. If Anthropic suspends its operations out of caution, that only guarantees that some other company will be first over the edge. But what I want to dig into a little, is trying to predict what is going to happen, if we do go over the edge under the current regime, of risk-taking competitive iterative empiricism. \n\nA few years back, Eliezer argued for 99+% probability of doom in a situation like this. Part of the argument was the complexity and fragility of human value. The odds of doom are not so clear to me. I don't know what they are, our existing AIs have actually taken on board human concepts and values to some degree, it is quite conceivable that they will end up alien but not paperclipper-alien. What I now emphasize, in order to most convey the unacknowledged risk involved with our current path, is that with overwhelming likelihood it involves the loss of human \"sovereignty\" - the AIs may or may not kill us, but they will almost certainly be in control. As I said, we've neglected the scenario of human-AI fusion, so that's a kind of small print - we may end up ruled, not by completely nonhuman AIs, but by something that is still part human or used to be part human. \n\nSo perhaps we should be estimating p(AI takeover) and p(cyborg takeover) along with p(doom), and so on. I won't try to put numbers on any of that here. Instead I want to focus on the idea that there are right ways to create superintelligence, but they require intellectual progress that hasn't yet occurred. The position of people who want a multi-decade pause is that the intellectual progress in question, involves leaps as great and profound as anything that human thought has ever accomplished - that's why we need a few decades of grace. \n\nMy premise is, that will not happen. We're going over the edge in *this* decade, and that's just how it is. My interest is, what are the odds of getting it right within that period of time, and what can we do to to increase the odds of that happening? \n\nAgain, I'm not going to guesstimate actual probabilities. But I do think that the odds of getting it right may not be negligible, even if \"decades of progress\" are required. Two factors combine to make this conceivable. One is that we do actually know something about the world. We know a lot about physics, we know quite a lot about computation, and we have a lot of starting points available, even for \"hard problems\" like consciousness or morality or why anything exists. That's one factor, and the other is, that we are in a regime in which human-AI collaboration can be very powerful. The latter is what makes it possible that the \"decades\" can pass in months, for people fortunate enough to have a good enough starting conceptual framework, and the means to develop it. \n\nWe know that the run-up to superintelligence should feature many novel phenomena produced by lesser levels of AI - of course, we're already seeing that. All I'm arguing is that the ingredients are there, for those novel phenomena to include the dramatic progress required for a friendly singularity to occur via design rather than just by serendipity. \n\nIf that is true, what can we do to increase the odds of getting it right in time? For me, work on superalignment (if we use that name for safety progress relevant to superintelligent AI) that's *public* is very important. The competing AI labs will inevitably keep their most powerful work hidden away, but public literature on superalignment is something that all of them can see and draw on. That seems to be the most important theory-independent thing that I can emphasize. (I suppose I could also mention the desirability of having your best thinkers being able to give their best towards the solution of these problems.)",
      "plaintextDescription": "This started life as a reaction to a post by Raemon, in which he calls on Anthropic to be more willing to halt its research if it enters a danger zone, and for them to take the relevance of technical philosophy much more seriously. \n\nMy personal opinion is that at this point, even slowing things down (I mean overall, not at Anthropic specifically) is not going to happen. It is a bare possibility that some kind of pause-AI or anti-AI movement will materialize and make a difference, but right now such a force, that is capable of making a difference, doesn't even exist, and the accelerationist path is being pursued by competing billion-dollar companies who have government backing. So my forecast is that we will go over the edge to superintelligence, as quickly as humanity can figure out how to do it. \n\nNow, although we are in a race, the contenders are not completely blind to the danger of their creations. They don't just make them and hand them the keys to everything. But as Raemon says, they are following an approach of \"iterative empiricism\" where they keep pressing ahead and hope they can solve all the future challenges along the way. \n\nBut what are their actual attitudes (never mind plans) regarding the time when superintelligence arrives? Here I feel like the best data we have are the vague optimistic essays that a few of them have written. I believe Altman and Amodei have both written quasi-utopian essays about coexistence between humans and matured AIs, and probably Hassabis as well. As for Musk, maybe you could piece something together from his tweets - all I know is that he hopes superintelligent AI will spare us out of \"curiosity\". Sutskever's Twitter profile during 2023 said \"towards a plurality of humanity loving AGIs\". I have no idea what they think about this at DeepSeek. \n\nWhat do they think in private? One must assume that some of them hope they personally can become as gods by being personally coupled to superintelligent AI. Elon Musk is by far the mo",
      "wordCount": 1278
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CdaEX4fChEFkcobyC",
    "title": "A model of the final phase: the current frontier AIs as de facto CEOs of their own companies ",
    "slug": "a-model-of-the-final-phase-the-current-frontier-ais-as-de",
    "url": null,
    "baseScore": 23,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-08T22:15:35.260Z",
    "contents": {
      "markdown": "The idea that the AI takes over its own company is obviously not a new one. For example, it's part of what happens in Joshua Clymer's \"How AI Takeover Might Happen in 2 Years\". \n\nWhat's new (for me) is to take this very seriously as a model of the immediate future. I've made a list of the companies that I think are known contenders for producing superintelligence. My proposed model of the future is just that their AIs will assume more and more control of management and decision-making inside the companies that own them. \n\nIn my thinking, this phase ends when you have an AI with a von Neumann level of intelligence. Once you have that kind of intelligence in silicon, the fully posthuman phase of AI evolution will have begun. Control will have completely escaped human hands. \n\nI also hypothesize that the current regime of reinforcement learning applied to chain of thought will be enough to get us there. This is a technical detail that is logically independent of the broader scenario, and I'm happy to hear arguments for or against. \n\nOK, so it's a model of the future, even a model of the present - how do we apply it, what does it get us? Basically, just replace the current CEO with their AI in your thinking. It's not Elon Musk who is managing Tesla and SpaceX and DOGE while tweeting about politics and geopolitics, it's Grok. It's not Sam Altman who is making decisions for OpenAI and Helios and Worldcoin, it's ChatGPT-4.5. And so on. \n\nThe funny thing is that this may already be half-true, in that these human leaders are surely already regularly consulting with their AI creations on tactics and strategy. \n\n(I'm in the middle of an electricity blackout and don't know when it ends, so I'll post this while I still have battery power, and flesh it out further when I can.)",
      "plaintextDescription": "The idea that the AI takes over its own company is obviously not a new one. For example, it's part of what happens in Joshua Clymer's \"How AI Takeover Might Happen in 2 Years\". \n\nWhat's new (for me) is to take this very seriously as a model of the immediate future. I've made a list of the companies that I think are known contenders for producing superintelligence. My proposed model of the future is just that their AIs will assume more and more control of management and decision-making inside the companies that own them. \n\nIn my thinking, this phase ends when you have an AI with a von Neumann level of intelligence. Once you have that kind of intelligence in silicon, the fully posthuman phase of AI evolution will have begun. Control will have completely escaped human hands. \n\nI also hypothesize that the current regime of reinforcement learning applied to chain of thought will be enough to get us there. This is a technical detail that is logically independent of the broader scenario, and I'm happy to hear arguments for or against. \n\nOK, so it's a model of the future, even a model of the present - how do we apply it, what does it get us? Basically, just replace the current CEO with their AI in your thinking. It's not Elon Musk who is managing Tesla and SpaceX and DOGE while tweeting about politics and geopolitics, it's Grok. It's not Sam Altman who is making decisions for OpenAI and Helios and Worldcoin, it's ChatGPT-4.5. And so on. \n\nThe funny thing is that this may already be half-true, in that these human leaders are surely already regularly consulting with their AI creations on tactics and strategy. \n\n(I'm in the middle of an electricity blackout and don't know when it ends, so I'll post this while I still have battery power, and flesh it out further when I can.)",
      "wordCount": 318
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wuDXfLz2u8nfFXtCJ",
    "title": "Reflections on the state of the race to superintelligence, February 2025",
    "slug": "reflections-on-the-state-of-the-race-to-superintelligence",
    "url": null,
    "baseScore": 21,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2025-02-23T13:58:07.663Z",
    "contents": {
      "markdown": "My model of the situation is that some time last year, the frontier paradigm moved from \"scaling up large language models\" to \"scaling up chain-of-thought models\". People are still inventing new architectures, e.g. Google's Titans, or Lecun's energy-based models. But it's conceivable that inference scaling really is the final paradigm prior to superintelligence. If it can produce von Neumann-level intelligence, then that should be the end, right? - the end, in the sense that control will pass out of human hands, unless there are a few humans embedded in the self-transformations of these genius-level AIs, and those embedded humans will become something more than human or other than human quickly enough. \n\nThis leads to the concrete question, where are the new frontier models, the powerful chain-of-thought AIs, being produced? Because these organizations are also the contenders to produce the first superintelligence. My list of known or suspected organizations consists of 5 in the USA, 1 in China, and 1 in Israel. Of course there may be more. \n\nAt the head of my list is Elon Musk's **xAI**. It's at the head of the list, not because I believe that Grok 3 is \"the world's smartest AI\", but because of its political advantages in Trump 2.0 America. xAI is part of the Musk family of companies, and these are now more deeply integrated into US government activities than ever before. There is a serious possibility that superintelligence will be \"born\" commanding all the resources of Musk's unprecedented empire, including social media, rocketry, robotics, brain-computer interfaces, and US government information systems. \n\nIntellectually, Trump 2.0 means the ascendancy to power of a large number of ideas from outside the institutional consensus of the liberal establishment (which includes at a minimum, the universities, the mainstream media, and all parts of the government): peace with Putin's Russia, RFK Jr's idea of health, downsizing the government, reversal of DEI policies, and probably more to come. In the relatively esoteric area of frontier AI policy, it is as if e/acc replaced effective altruism as the implicit zeitgeist. Concretely that seems to mean switching from regulation (of AI) and international treaties, to deregulation and a competitive race with China. \n\nAgain xAI has a bit of an advantage here, because Musk made X-Twitter friendly to these outsider ideas, in advance of Trump 2.0's right-wing populist revolution seizing institutional power. On the other hand, the other American AI companies were comfortable with Biden-era liberal progressivism, and have had to reorient themselves to the new order. I'll get to that in a moment, but first I'll address the situation of the second company in my list, **OpenAI**, which has an extra problem in addition to the change in political paradigm. \n\nSam Altman is doing what he can to keep up - he wasn't at the inauguration, but literally the next day he was co-hosting the launch of the Stargate project with Trump. Altman's real problem is his beef with Elon Musk, who evidently wants to stop or suborn OpenAI, and has a lot of resources with which to do that. A year ago I would have said that OpenAI can draw on the resources of Microsoft to defend itself (since the alliance with Microsoft is what allowed Altman to hang on as CEO in November 2023), but I'm not up-to-date on OpenAI's partnerships, e.g. I think I heard of some partnership with Amazon too. \n\nPolitics aside, I think OpenAI may be the technical leader, with GPT-5 perhaps coming later this year and incorporating the best of o1 and o3. \n\nThird and fourth on my list are **Anthropic** (with Claude) and **Google** (with Gemini). Politically I see them as pragmatically going with the flow - I think both Dario Amodei and Demis Hassabis have given lip service to the idea that democracy must get to superintelligence before authoritarianism does, while also reminding us that superintelligence in itself is dangerous if unaligned. \n\nTechnically, Anthropic might be the leader in AI safety, since they took on Jan Leike and the rest of OpenAI's superalignment team, while Google is a ubiquitous behemoth with vast resources, the IBM of the Internet era, and has both the advantages and disadvantages that come with this. \n\nFifth on my list is a hypothetical organization: **The Project**, Leopold Aschenbrenner's name for an American Manhattan Project aiming to create superintelligence. We don't know that it exists; we only have speculation about AI researchers who quit in order to pursue unknown exciting opportunities. If it does exist, it is also possible that it exists under the umbrella of one or more of the companies already in the list. \n\nThat's my list for America. Now, onto my final two contenders. These are **DeepSeek** in China, and Ilya Sutskever's opaque **Safe Superintelligence Inc.**, which is divided between Palo Alto and Tel Aviv. I won't speculate about their political context except to note that they exist outside or half-outside the American scene. \n\nGiven this situation, in which there are at least six separate centers of research that are in with a chance of creating superintelligence, and dozens, possibly hundreds more worldwide, that are either doing it or want to do it; and given the fact that I am not part of any of those research centers; my strategy for trying to increase the chance of a human-friendly outcome, is to contribute to the public discussion of how to make autonomous superintelligence \"ethical\" or \"human-friendly\" or \"superaligned\", since the public discussion can in principle be noticed by any of the participants in the AI race, and if there are good valid ideas, they just might take note and implement them. (For example, at the moment I'm interested in what happens if you combine Joshua Clymer's new thoughts on [safely outsourcing AI alignment tasks to AI](https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai), with June Ku's old CEV-like proposal at [MetaEthical.AI](https://www.metaethical.ai/) \\- does it take us far beyond Eliezer's own thoughts on \"Interim Friendliness\" from the early 2000s?) \n\nIt's true that there's an unknown number of unsolved questions remaining to be answered, in the theory and practice of safe superintelligence. The situation we've arrived at, in which the risks inherent to the creation of superintelligence are barely publicly acknowledged by the protagonists of the race, is far from desirable. It would be best to cross that threshold only once you really know what you're doing; but that is not the attitude that has prevailed. \n\nHowever, I don't consider it impossible that the theory will actually be figured out. Knowledge is distributed highly unequally in the world. Individuals and groups with extreme expertise do exist; and humanity plausibly has the foundations needed to figure out the theory of safe superintelligence. Our best experts do have an advanced understanding of quite a lot of physics, mathematics, and computation; and the people making the frontier models do at least know what recipes they are using to create and manage their AIs. Topics like consciousness and (meta)philosophy are a bit more problematic, but we do have a lot of ideas and data to work with. \n\nAnd finally, the source of our peril - the rapid climb in AI capabilities towards superintelligence - also means that all kinds of hard problems may be solved at unprecedented speed, even before we cross the threshold. So I choose to stay engaged, add my thoughts to the greater flow, and hope that somewhere, the problems that need to be solved will actually be solved.",
      "plaintextDescription": "My model of the situation is that some time last year, the frontier paradigm moved from \"scaling up large language models\" to \"scaling up chain-of-thought models\". People are still inventing new architectures, e.g. Google's Titans, or Lecun's energy-based models. But it's conceivable that inference scaling really is the final paradigm prior to superintelligence. If it can produce von Neumann-level intelligence, then that should be the end, right? - the end, in the sense that control will pass out of human hands, unless there are a few humans embedded in the self-transformations of these genius-level AIs, and those embedded humans will become something more than human or other than human quickly enough. \n\nThis leads to the concrete question, where are the new frontier models, the powerful chain-of-thought AIs, being produced? Because these organizations are also the contenders to produce the first superintelligence. My list of known or suspected organizations consists of 5 in the USA, 1 in China, and 1 in Israel. Of course there may be more. \n\nAt the head of my list is Elon Musk's xAI. It's at the head of the list, not because I believe that Grok 3 is \"the world's smartest AI\", but because of its political advantages in Trump 2.0 America. xAI is part of the Musk family of companies, and these are now more deeply integrated into US government activities than ever before. There is a serious possibility that superintelligence will be \"born\" commanding all the resources of Musk's unprecedented empire, including social media, rocketry, robotics, brain-computer interfaces, and US government information systems. \n\nIntellectually, Trump 2.0 means the ascendancy to power of a large number of ideas from outside the institutional consensus of the liberal establishment (which includes at a minimum, the universities, the mainstream media, and all parts of the government): peace with Putin's Russia, RFK Jr's idea of health, downsizing the government, reversal of DEI policies, and ",
      "wordCount": 1232
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "r4FF9YWqDzcnfBZ88",
    "title": "The new ruling philosophy regarding AI",
    "slug": "the-new-ruling-philosophy-regarding-ai",
    "url": null,
    "baseScore": 29,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-11-11T13:28:24.476Z",
    "contents": {
      "markdown": "I suggest that (for now) it's a mix of Marc Andreessen, Leopold Aschenbrenner, and Guillaume Verdon. \n\nBut let's back up first. What was the Biden-Harris administration's philosophy regarding AI? For the first half of Biden's term, I would say they didn't have one. As with most of the world, it was the release of ChatGPT in late 2022 that made AI a top-level issue. \n\nAnd though it's not as if Biden's cabinet contained any avowed effective altruists, I do think that by default, the \"safetyist\" attitude towards AI that is associated with effective altruism and Less Wrong rationalism, was philosophically influential; not least because influential advocates of effective altruism were part of the elite Democratic base. FTX's Sam Bankman-Fried came from that background. Open Philanthropy's Dustin Moskovitz is another. \n\n(I've listed three people as alleged thought leaders for the Trump 2.0 era; if I was going to pick three for the second half of the Biden era, maybe it would be Paul Christiano, Helen Toner, and Joseph Matheny.)\n\nAs most of us would know, irritation with AI safetyism did a lot to inspire the creation by a few Silicon Valley memelords of an alternative ideology, \"effective accelerationism\"; and after the unsuccessful OpenAI coup against Sam Altman at the end of 2023, e/acc was widely considered to have won the culture war against effective altruism in the tech world. \n\nNow, looking back from the end of 2024, we can see that many of the tech figures who affiliated with e/acc at the start of the year, defected from elite consensus to ally with the victorious Trump Republicans by the end of the year. This is why I regard e/acc as a major component of the emerging zeitgeist regarding AI and AI policy. \n\nWhat follows is far more a product of intuitive speculation than scholarship. Also, I don't live in North America, I'm poor, I have zero experience of contemporary Silicon Valley (or Washington DC, for that matter). I am a distant observer of all this. I am prepared to be corrected by people who are actually in the thick of things. \n\nBut for now, I don't see anyone making a clear claim about which ideas will inform the thinking of the incoming American government and its allies. So this is my \"model\" of what's ahead, make of it what you will. \n\nFirst, **Marc Andreessen.** A pivotal figure in the 1990s Internet, CEO of the browser company Netscape, who seems to have then risen into the investment Valhalla of billionaire venture capitalists. In the wake of e/acc and ChatGPT, he wrote a [\"techno-optimist manifesto\"](https://a16z.com/the-techno-optimist-manifesto/) that incorporates AI into an older narrative of human progress through technology and capitalism. It's good enough to stand as an example of its genre; it expounds a particular perspective on history, politics and economics that is probably shared by many of these tech captains of industry; and it ends with a list of about 50 other thinkers who Andreessen considers to be fellow travelers, so you can read them if you want more details. \n\nSecond, **Leopold Aschenbrenner.** A young former employee of OpenAI who became a wunderkind of AI strategic policy in mid-2024, thanks to the publication of his manifesto entitled [\"Situational Awareness\"](https://situational-awareness.ai/). It's been discussed here on Less Wrong. His manifesto first endorses short timelines for superhuman AI, saying that it's coming later this decade, and then says that the democratic world, led by the USA, must create and domesticate superhuman AI before a geopolitical and ideological rival like China does so; and that this should be done by the nationalization of labs engaged in research on frontier AI, as part of a new Manhattan Project aimed at solving superalignment. \n\nAschenbrenner has therefore fused the tech narrative of imminent superhuman AI, and the safetyist narrative according to which the preferences of superhuman AI will shape the future of life on Earth, with an America-First national-security perspective. A month before the vote, [Ivanka Trump tweeted favorably about his manifesto](https://twitter.com/IvankaTrump/status/1839002887600370145), so we know that the incoming First Family has heard of it. \n\nThird, **Guillaume Verdon**. Originally known only as e/acc co-founder @BasedBeffJezos, he was doxxed by *Forbes* in the same week that Biden's commerce secretary publicly declared e/acc to be dangerous, and just a week after Altman was reinstated as OpenAI CEO. He was revealed as a Canadian quantum-information physicist ([his thesis is quite interesting](http://hdl.handle.net/10012/11161), if you're into that), who worked on quantum AI at Google before co-founding his own startup, Extropic, with the idea of running AI on stochastic computer chips that directly utilize non-gaussian thermodynamic randomness to implement cognitive probability distributions (rather than doing everything at the software level). \n\nI've already mentioned the role that e/acc has played in bringing together Trump's allies in the tech sector (though its role there is overshadowed by Elon Musk and his 2022 purchase of Twitter). When it comes to philosophy, e/acc has a deserved reputation for glib memeing and sloganeering. However, I have included Verdon on this list because I also find implicit in his thoughts, an alternative to the influential model of the future (which Aschenbrenner arguably favors, as do I), according to which the creation of human-level AI will be followed by the emergence of \"superintelligent\" AI whose goals then dominate the world, regardless of whether that value system is \"liberal democracy\" or \"more paperclips\". \n\nIn his talk [\"Thermodynamics of techno-capitalism\"](https://www.youtube.com/watch?v=BS_F57f-XAg), Verdon instead presents a model of evolution that is persistently pluralistic. It's still pretty sparse and undeveloped - maybe the few minutes after 10:00 are where it is most spelled out - but it's one of complex systems that for thermodynamic reasons learn, and learn to learn. Competition never goes away, and values are never final. The fundamental metric of progress is how much energy you are able to spend, and that applies all the way from the first cells surviving in the primordial soup, to AI companies surviving in the global marketplace, and presumably on to whatever new interplanetary and interstellar forms of being emerge from life on Earth. \n\nI'm exercising some latitude in interpreting his remarks here, which are mostly just about a common characterization of evolution and capitalism through a combination of thermodynamic and machine-learning concepts. But the point is that it's a big picture, different from the synthesis we're familiar with here (e.g. of a multiverse dominated by simulation and timeless trade among a population of autarkic superintelligences), and with a significant intellectual ancestry to back it up (especially complex systems theory); and something like it potentially provides a rationale for the seemingly unsafe strategies of a Zuckerberg or a Musk, when it comes to dealing with superintelligence. \n\n(A historical digression here: Verdon's company name, Extropic, of course brings to mind the Extropians, the 1990s Internet transhumanists among whom Eliezer first appeared. One of the differences between Extropian transhumanism and the transhumanism of Less Wrong rationalism, is that the Extropians were far more in sync with the idea that the struggle to survive never goes away and that pluralism based in decentralized freedom is the way to go even for transhuman beings, rather than the idea that everything hinges on identifying true human values and extrapolating them faithfully. \n\nTo this I would add that in the 1980s, Bruce Sterling's SF novel *Schismatrix* featured a \"Posthumanist\" movement in a solar-system civilization of competing techno-cartels, whose political rhetoric derives from nonequilibrium thermodynamics. I have to think that someone among the founders of e/acc was influenced by that, even if they went on to combine it with a pro-capitalist poetics not employed by Sterling.) \n\nI've gone on at such length about the alternative model of the era of superintelligence, that I have supposedly found in e/acc, precisely because it isn't spelt out anywhere that I can find. e/acc is variously accused of being in denial about superintelligence, or of hiding its indifference to the future of mere humanity for the sake of public relations, and I think there's something to that. But if we're looking for a serious rival to the \"singleton\" conception of what a world with superintelligence looks like, the unendingly pluralistic evolution of the e/acc universe is such an alternative, and I think it will come up in some form, if the tech tycoons of Trump 2.0 are challenged on the topic of superintelligence.",
      "plaintextDescription": "I suggest that (for now) it's a mix of Marc Andreessen, Leopold Aschenbrenner, and Guillaume Verdon. \n\nBut let's back up first. What was the Biden-Harris administration's philosophy regarding AI? For the first half of Biden's term, I would say they didn't have one. As with most of the world, it was the release of ChatGPT in late 2022 that made AI a top-level issue. \n\nAnd though it's not as if Biden's cabinet contained any avowed effective altruists, I do think that by default, the \"safetyist\" attitude towards AI that is associated with effective altruism and Less Wrong rationalism, was philosophically influential; not least because influential advocates of effective altruism were part of the elite Democratic base. FTX's Sam Bankman-Fried came from that background. Open Philanthropy's Dustin Moskovitz is another. \n\n(I've listed three people as alleged thought leaders for the Trump 2.0 era; if I was going to pick three for the second half of the Biden era, maybe it would be Paul Christiano, Helen Toner, and Joseph Matheny.)\n\nAs most of us would know, irritation with AI safetyism did a lot to inspire the creation by a few Silicon Valley memelords of an alternative ideology, \"effective accelerationism\"; and after the unsuccessful OpenAI coup against Sam Altman at the end of 2023, e/acc was widely considered to have won the culture war against effective altruism in the tech world. \n\nNow, looking back from the end of 2024, we can see that many of the tech figures who affiliated with e/acc at the start of the year, defected from elite consensus to ally with the victorious Trump Republicans by the end of the year. This is why I regard e/acc as a major component of the emerging zeitgeist regarding AI and AI policy. \n\nWhat follows is far more a product of intuitive speculation than scholarship. Also, I don't live in North America, I'm poor, I have zero experience of contemporary Silicon Valley (or Washington DC, for that matter). I am a distant observer of all this. I am prep",
      "wordCount": 1382
    },
    "tags": [
      {
        "_id": "kCrvmjZhsAZANnZL6",
        "name": "Philosophy",
        "slug": "philosophy-1"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb297",
        "name": "Superintelligence",
        "slug": "superintelligence"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "qG26Xou9G4bfRLRMi",
    "title": "First and Last Questions for GPT-5*",
    "slug": "first-and-last-questions-for-gpt-5",
    "url": null,
    "baseScore": 20,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-11-24T05:03:04.371Z",
    "contents": {
      "markdown": "Suppose that in the very near future, a research group finds that their conversational AI has begun to produce extremely high-quality answers to questions. There's no obvious limit to its ability, but there's also no guarantee of correctness or good intention, given the opacity of how it works. \n\nWhat questions should they ask it?\n\nAnd what questions should they *not* ask it?",
      "plaintextDescription": "Suppose that in the very near future, a research group finds that their conversational AI has begun to produce extremely high-quality answers to questions. There's no obvious limit to its ability, but there's also no guarantee of correctness or good intention, given the opacity of how it works. \n\nWhat questions should they ask it?\n\nAnd what questions should they not ask it? ",
      "wordCount": 62
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "P7GHAB9qd39N7o5fZ",
    "title": "The national security dimension of OpenAI's leadership struggle",
    "slug": "the-national-security-dimension-of-openai-s-leadership",
    "url": null,
    "baseScore": 3,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2023-11-20T23:57:12.491Z",
    "contents": {
      "markdown": "As the very public custody battle over OpenAI's artificial intelligences winds down, I would like to point out a few facts, and then comment briefly on their possible significance.\n\nIt has already been noticed that \"at least two of the board members, Tasha McCauley and Helen Toner, have ties to the Effective Altruism movement\", [as the New York Times puts it](https://www.nytimes.com/2023/11/18/technology/sam-altman-open-ai.html). *Both these board members also have national security ties.*\n\nWith Helen Toner it's more straightforward; she has a master's degree in Security Studies at Georgetown University, [\"the most CIA-specific degree\" at a university known as a gateway to deep state institutions](https://mronline.org/2023/03/16/from-georgetown-to-langley-the-controversial-connection-between-a-prestigious-university-and-the-cia/). That doesn't mean she's *in* the CIA; but she's in that milieu.\n\nAs for Tasha McCauley, it's more oblique: [her actor husband played Edward Snowden, the famous NSA defector, in the 2016 biographical film](https://en.wikipedia.org/wiki/Snowden_(film)). Hollywood's relationship to the American intelligence community is, I think, a little more vexed than Georgetown's. McCauley and her husband might well be pro-Snowden civil libertarians who want deep state power curtailed; that would make them the \"anti national security\" faction on the OpenAI board, so to speak.\n\nEither way, this means that the EAs on the board, who both presumably voted to oust Sam Altman as CEO, are adjacent to the US intelligence community, and/or its critics, many of whom are intelligence veterans anyway.\n\n((**ADDED A WEEK LATER**: It has been pointed out to me that even if Joseph Gordon-Levitt did meet a few ex-spooks on the set of *Snowden*, that doesn't in itself equate to his wife having \"national security ties\". Obviously that's true; I just assumed that the connections run a lot deeper, and that may sound weird if you're used to thinking of the intelligence community and the culture industry as entirely separate. In any case, I accept the criticism that this is pure speculation on my part, and that I should have made my larger points in some other way.))\n\nThose are my facts. Now for my interpretation.\n\nArtificial intelligence is a national security issue. It is a technology with strategic implications. OpenAI is an American company. There is no way that such a company is allowed to operate without some kind of national-security oversight existing. One way to do that, is to have people on the board.\n\nThe way things are going, OpenAI is going to end up even more tied to Microsoft than it already was. Of course, Microsoft would already have a relationship to the state organs of American national security. In fact, those relationships would be decades old, and very familiar on both sides.\n\nSo from a national security standpoint, OpenAI is transitioning from, say, ad hoc supervision by its board, to integration with the existing relationship between Microsoft and the deep state - however that works.\n\nAs we try to understand what happened over the past few days, I would suggest considering whether national security representatives played a role, especially given the composition of the OpenAI board. To me, it's conceivable that e.g. GPT-5's progress set off alarm bells, and someone said we need to bring OpenAI into a more closely supervised relationship, right now.",
      "plaintextDescription": "As the very public custody battle over OpenAI's artificial intelligences winds down, I would like to point out a few facts, and then comment briefly on their possible significance.\n\nIt has already been noticed that \"at least two of the board members, Tasha McCauley and Helen Toner, have ties to the Effective Altruism movement\", as the New York Times puts it. Both these board members also have national security ties.\n\nWith Helen Toner it's more straightforward; she has a master's degree in Security Studies at Georgetown University, \"the most CIA-specific degree\" at a university known as a gateway to deep state institutions. That doesn't mean she's in the CIA; but she's in that milieu.\n\nAs for Tasha McCauley, it's more oblique: her actor husband played Edward Snowden, the famous NSA defector, in the 2016 biographical film. Hollywood's relationship to the American intelligence community is, I think, a little more vexed than Georgetown's. McCauley and her husband might well be pro-Snowden civil libertarians who want deep state power curtailed; that would make them the \"anti national security\" faction on the OpenAI board, so to speak.\n\nEither way, this means that the EAs on the board, who both presumably voted to oust Sam Altman as CEO, are adjacent to the US intelligence community, and/or its critics, many of whom are intelligence veterans anyway.\n\n \n\n((ADDED A WEEK LATER: It has been pointed out to me that even if Joseph Gordon-Levitt did meet a few ex-spooks on the set of Snowden, that doesn't in itself equate to his wife having \"national security ties\". Obviously that's true; I just assumed that the connections run a lot deeper, and that may sound weird if you're used to thinking of the intelligence community and the culture industry as entirely separate. In any case, I accept the criticism that this is pure speculation on my part, and that I should have made my larger points in some other way.))\n\n \n\nThose are my facts. Now for my interpretation.\n\nArtificial intellig",
      "wordCount": 521
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uNe5sjRRvMA8YnYHL",
    "title": "Bruce Sterling on the AI mania of 2023",
    "slug": "bruce-sterling-on-the-ai-mania-of-2023",
    "url": "https://www.newsweek.com/2023/07/21/ai-scariest-beast-ever-created-says-sci-fi-writer-bruce-sterling-1809439.html",
    "baseScore": 25,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2023-06-29T05:00:18.326Z",
    "contents": {
      "markdown": "Bruce Sterling is a well-known SF writer, but I think he is not quite on the radar of Less Wrong, so I'll say a bit about how I locate him culturally. I'll make a distinction between SF that is primarily about marvels of science and technology, and SF that is primarily about how it feels to inhabit technological society. I consider Sterling to be a leading practitioner of that second kind of SF. He wrote one novel, the posthuman space opera *Schismatrix*, in which he indulged his inner Singularitarian to the maximum; but after that, his works have been more earthbound, though still about the transformations of life that could be wrought by technology. \n\nHe's also been prolific as an essayist and public speaker, still offering commentary on technological society and world affairs, but in a supposedly factual vein, rather than an overtly fictional one. I admire his talent for digesting these complex topics and producing an intuitive summation in a few pithy turns of phrase; I consider him a role model in that regard. And so now he's trying to sum up the state of the AI world, after ChatGPT. \n\nHe aims for an indulgent, god's-eye view of the current-year antics of everyone involved in AI. He acknowledges the prophets of doom, the social justice warriors, the captains of industry. He nominates three myths as encapsulating the times: Roko's Basilisk, the Masked Shoggoth, and the Paperclip Maximizer. Every one of them comes from within the rationalist sphere, or adjacent to it, so we're doing pretty well when it comes to mythic potency! \n\nBut he does believe that after this carnival of ecstasy and dread, there will be a hangover rather than a singularity. I suspect he's the kind of guy who thinks that superhuman intelligence might well be possible, but only decades from now, after still-unknown paradigm shifts, and that language models are a false dawn in that regard. So when the next AI winter comes, we'll still be here, and we'll still be human beings, but we'll also be living with all the consequences of having created glib, multitalented, relentless artificial persons. \n\nMy own bet is that this isn't a false dawn, that for better or worse, we're now directly on course towards artificial intelligence that comprehensively transcends human abilities. But I appreciate Sterling's role as skeptical yet open-minded observer.",
      "plaintextDescription": "Bruce Sterling is a well-known SF writer, but I think he is not quite on the radar of Less Wrong, so I'll say a bit about how I locate him culturally. I'll make a distinction between SF that is primarily about marvels of science and technology, and SF that is primarily about how it feels to inhabit technological society. I consider Sterling to be a leading practitioner of that second kind of SF. He wrote one novel, the posthuman space opera Schismatrix, in which he indulged his inner Singularitarian to the maximum; but after that, his works have been more earthbound, though still about the transformations of life that could be wrought by technology. \n\nHe's also been prolific as an essayist and public speaker, still offering commentary on technological society and world affairs, but in a supposedly factual vein, rather than an overtly fictional one. I admire his talent for digesting these complex topics and producing an intuitive summation in a few pithy turns of phrase; I consider him a role model in that regard. And so now he's trying to sum up the state of the AI world, after ChatGPT. \n\nHe aims for an indulgent, god's-eye view of the current-year antics of everyone involved in AI. He acknowledges the prophets of doom, the social justice warriors, the captains of industry. He nominates three myths as encapsulating the times: Roko's Basilisk, the Masked Shoggoth, and the Paperclip Maximizer. Every one of them comes from within the rationalist sphere, or adjacent to it, so we're doing pretty well when it comes to mythic potency! \n\nBut he does believe that after this carnival of ecstasy and dread, there will be a hangover rather than a singularity. I suspect he's the kind of guy who thinks that superhuman intelligence might well be possible, but only decades from now, after still-unknown paradigm shifts, and that language models are a false dawn in that regard. So when the next AI winter comes, we'll still be here, and we'll still be human beings, but we'll also be li",
      "wordCount": 392
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6Ev4GKuMquiK7nF8w",
    "title": "Mitchell_Porter's Shortform",
    "slug": "mitchell_porter-s-shortform",
    "url": null,
    "baseScore": 8,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 24,
    "createdAt": null,
    "postedAt": "2023-06-01T11:45:58.622Z",
    "contents": null,
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sw52au6N93Hc4GmAc",
    "title": "ChatGPT (May 2023) on Designing Friendly Superintelligence",
    "slug": "chatgpt-may-2023-on-designing-friendly-superintelligence",
    "url": "https://singularitypolitics.wordpress.com/2023/05/24/chatgpt-on-designing-friendly-superintelligence/",
    "baseScore": 5,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-05-24T10:47:16.325Z",
    "contents": {
      "markdown": "I asked ChatGPT how to make a human-friendly superintelligence, and it gave me a 50-page plan. There are no technical novelties, or any attempt to deal with the dangers that are supposed to be peculiar to superintelligence or self-enhancing systems. The manifesto is really just a potpourri of principles and methods that might be appropriate for any deep learning system that's supposed to be interacting with human beings. And yet I suspect startups have been founded on far less. \n\nTo me, this was interesting because this is what ChatGPT comes up with, when you ask it in the laziest possible way, with no prompt engineering at all, how to do this thing. As such, this is representative of its \"spontaneous thoughts\", the first things that come to its mind, when it is presented with this task. It is therefore also a glimpse of how an LLM-based agent might proceed, when similarly instructed. \n\nI would be interested to see the results of similar experiments to this one. As I said, I made no effort at all, I just asked the question and then got it to expand on its own outline. The people who are actually developing large language models must have conducted similar, but far more sophisticated, experiments by now.",
      "plaintextDescription": "I asked ChatGPT how to make a human-friendly superintelligence, and it gave me a 50-page plan. There are no technical novelties, or any attempt to deal with the dangers that are supposed to be peculiar to superintelligence or self-enhancing systems. The manifesto is really just a potpourri of principles and methods that might be appropriate for any deep learning system that's supposed to be interacting with human beings. And yet I suspect startups have been founded on far less. \n\nTo me, this was interesting because this is what ChatGPT comes up with, when you ask it in the laziest possible way, with no prompt engineering at all, how to do this thing. As such, this is representative of its \"spontaneous thoughts\", the first things that come to its mind, when it is presented with this task. It is therefore also a glimpse of how an LLM-based agent might proceed, when similarly instructed. \n\nI would be interested to see the results of similar experiments to this one. As I said, I made no effort at all, I just asked the question and then got it to expand on its own outline. The people who are actually developing large language models must have conducted similar, but far more sophisticated, experiments by now. ",
      "wordCount": 210
    },
    "tags": [
      {
        "_id": "GrHAiuzAjG2mDxvMB",
        "name": "ChatGPT",
        "slug": "chatgpt-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ogC8agX64nW6N9EEW",
    "title": "How is AI governed and regulated, around the world?",
    "slug": "how-is-ai-governed-and-regulated-around-the-world",
    "url": null,
    "baseScore": 15,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2023-03-30T15:36:55.987Z",
    "contents": {
      "markdown": "This week, first came an [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) calling for a \"pause\" in \"giant AI\" research ([LW discussion](https://www.lesswrong.com/posts/6uKG2fjxApmdxeHNd/fli-open-letter-pause-giant-ai-experiments)) that has received worldwide media coverage; then Eliezer went further in [TIME Ideas](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) and sketched what a globally enforced ban on AGI research would look like ([LW discussion](https://www.lesswrong.com/posts/Aq5X9tapacnk2QGY4/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all)). \n\nMany online voices are saying it could never happen, but I think they underestimate the visceral common-sense fear that many ordinary people have, regarding artificial intelligence. Most people are not looking to transcend humanity, nor are they particularly in denial about the possibility of technology producing something smarter than human. \n\nThere is genuine potential for an anti-AI movement to come into being, that simply wants to \"shut it all down\". Of course, such a movement would quickly run up against the power centers in science, commerce, and national security that want to push the boundaries. Between the corporate scramble to develop and market ever more powerful software, and the new era of geopolitical polarization, it might seem impossible that the \"AI arms race\" could ever be halted. \n\nHowever, fear is a great motivator. During the cold war, fear of nuclear war compelled the USA and the USSR to restrain themselves in their otherwise unrestrained struggle for supremacy; and it also led to the creation of the Nuclear Non-Proliferation Treaty and the International Atomic Energy Agency, a system for the worldwide management of nuclear technology that ultimately answers to the United Nations Security Council, the most powerful institution in human affairs. \n\nIf the member states of the United Nations, and in particular the ruling elites of the permanent members of the Security Council, genuinely became convinced that sufficiently powerful artificial intelligence is a threat to the human race, they truly could organize a global ban on AGI research, even up to the point of military enforcement of the ban. They would have to deal with mutual distrust, but there are ways around that. They could even remain suspicious of each other, while cooperating to force a ban on everyone else - to mention one example. \n\nI won't express an opinion on how successful such a ban might be, or how long it would last; but the creation of a global anti-AI regime, I think is a political possibility. \n\nHowever, if it were to happen, it would have to develop out of the frameworks for regulation and governance of AI, that the world's nations are already developing, individually and collectively. That's why I made this post - to collect information on how AI is currently regulated. It would be nice to have some facts on how it is regulated in each of the G-20 countries, for example. \n\nFor now I'll just link to Wikipedia: \n\n[Regulation of artificial intelligence](https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence)\n\nwhich currently has sections on AI regulation in three out of the five permanent Security Council members, Britain, America, and China (Russia and France are not mentioned, though there are sections on European regulation).",
      "plaintextDescription": "This week, first came an open letter calling for a \"pause\" in \"giant AI\" research (LW discussion) that has received worldwide media coverage; then Eliezer went further in TIME Ideas and sketched what a globally enforced ban on AGI research would look like (LW discussion). \n\nMany online voices are saying it could never happen, but I think they underestimate the visceral common-sense fear that many ordinary people have, regarding artificial intelligence. Most people are not looking to transcend humanity, nor are they particularly in denial about the possibility of technology producing something smarter than human. \n\nThere is genuine potential for an anti-AI movement to come into being, that simply wants to \"shut it all down\". Of course, such a movement would quickly run up against the power centers in science, commerce, and national security that want to push the boundaries. Between the corporate scramble to develop and market ever more powerful software, and the new era of geopolitical polarization, it might seem impossible that the \"AI arms race\" could ever be halted. \n\nHowever, fear is a great motivator. During the cold war, fear of nuclear war compelled the USA and the USSR to restrain themselves in their otherwise unrestrained struggle for supremacy; and it also led to the creation of the Nuclear Non-Proliferation Treaty and the International Atomic Energy Agency, a system for the worldwide management of nuclear technology that ultimately answers to the United Nations Security Council, the most powerful institution in human affairs. \n\nIf the member states of the United Nations, and in particular the ruling elites of the permanent members of the Security Council, genuinely became convinced that sufficiently powerful artificial intelligence is a threat to the human race, they truly could organize a global ban on AGI research, even up to the point of military enforcement of the ban. They would have to deal with mutual distrust, but there are ways around that. They c",
      "wordCount": 485
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "C3SRSfRojwCzaPW2b",
    "title": "A crisis for online communication: bots and bot users will overrun the Internet? ",
    "slug": "a-crisis-for-online-communication-bots-and-bot-users-will",
    "url": null,
    "baseScore": 15,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2022-12-11T21:11:46.964Z",
    "contents": {
      "markdown": "The endgame for humanity's AI adventure still looks to me, to be what happens upon the arrival of comprehensively superhuman artificial intelligence. \n\nHowever, the rise of large language models, and the availability of ChatGPT in particular, has amplified an existing Internet phenomenon, to the point that it may begin to dominate the tone of online interactions. \n\nVarious kinds of fakes and deceptions have always been a factor in online life, and before that in face-to-face real life. But first, spammers took advantage of bulk email to send out lies to thousands of people at a time, and then chatbots provided an increasingly sophisticated automated substitute for social interaction itself. \n\nThe bot problem is probably already worse than I know. Just today, Elon Musk is promising some kind of purge of bots from Twitter. \n\nBut now we have \"language models\" whose capabilities are growing in dozens of directions at once. They can be useful, they can be charming, but they can also be used to drown us in their chatter, and trick us like an army of trolls. \n\nOne endpoint of this is the complete replacement of humanity, online and even in real life, by a population of chattering mannequins. Probably this image exists many places in science fiction: a future Earth on which humanity is extinct, but in which, in what's left of urban centers running on automated infrastructure, some kind of robots cycle through an imitation of the vanished human life, until the machine stops entirely. \n\nMeanwhile, the simulacrum of *online* human life is obviously a far more imminent and urgent issue. **And for now, the problem is still bots with human botmasters.** I haven't yet heard of a computer virus with an onboard language model that's just roaming the wild, surviving on the strength of its social skills. \n\nInstead, the problem is going to be trolls using language models, marketers and scammers and spammers using language models, political manipulators and geopolitical subverters using language models: targeting individuals, and then targeting thousands and millions of people, with terrible verisimilitude. \n\nThis is not a new fear, but it has a new salience because of the leap in the quality of AI imitation and creativity. It may end up driving people offline in large numbers, and/or ending anonymity in public social media in favor of verified identities. A significant fraction of the human population may find themselves permanently under the spell of bot operations meant to shape their opinions and empty their pockets. \n\nSome of these issues are also discussed in the recent post, [ChatGPT's Misalignment Isn't What You Think](https://www.lesswrong.com/posts/SnS8JhnqqGvMTxxuD/chatgpt-s-misalignment-isn-t-what-you-think).",
      "plaintextDescription": "The endgame for humanity's AI adventure still looks to me, to be what happens upon the arrival of comprehensively superhuman artificial intelligence. \n\nHowever, the rise of large language models, and the availability of ChatGPT in particular, has amplified an existing Internet phenomenon, to the point that it may begin to dominate the tone of online interactions. \n\nVarious kinds of fakes and deceptions have always been a factor in online life, and before that in face-to-face real life. But first, spammers took advantage of bulk email to send out lies to thousands of people at a time, and then chatbots provided an increasingly sophisticated automated substitute for social interaction itself. \n\nThe bot problem is probably already worse than I know. Just today, Elon Musk is promising some kind of purge of bots from Twitter. \n\nBut now we have \"language models\" whose capabilities are growing in dozens of directions at once. They can be useful, they can be charming, but they can also be used to drown us in their chatter, and trick us like an army of trolls. \n\nOne endpoint of this is the complete replacement of humanity, online and even in real life, by a population of chattering mannequins. Probably this image exists many places in science fiction: a future Earth on which humanity is extinct, but in which, in what's left of urban centers running on automated infrastructure, some kind of robots cycle through an imitation of the vanished human life, until the machine stops entirely. \n\nMeanwhile, the simulacrum of online human life is obviously a far more imminent and urgent issue. And for now, the problem is still bots with human botmasters. I haven't yet heard of a computer virus with an onboard language model that's just roaming the wild, surviving on the strength of its social skills. \n\nInstead, the problem is going to be trolls using language models, marketers and scammers and spammers using language models, political manipulators and geopolitical subverters using langu",
      "wordCount": 429
    },
    "tags": [
      {
        "_id": "YWzByWvtXunfrBu5b",
        "name": "GPT",
        "slug": "gpt"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "LjuZqyH9pimSzxG2d",
    "title": "One night, without sleep",
    "slug": "one-night-without-sleep",
    "url": null,
    "baseScore": 27,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2018-08-16T17:50:06.036Z",
    "contents": {
      "markdown": "In memory of all those who fell and were not saved\n\n(I normally have little regard for trigger warnings,\n\nbut on this occasion, imagine that my words are prefaced with every trigger warning ever)\n\nA body, on a battlefield, maimed and bleeding;\n\nterrible pain, staring at the sky,\n\nleft to die.\n\nAn experience that must have happened a million times in human history,\n\nan experience that must be happening somewhere right now.\n\nMy situation is better - perhaps. I lie, not on a battlefield, but in a sickbed.\n\nI just have some kind of flu;\n\nand an eye haunted by a migraine that comes, and goes, and threatens to come again;\n\nand I am not sleeping.\n\nAlso, I can write.\n\nThe smartphone battery was almost dead, I thought I was resigned to simply lying awake for long hours, without the catharsis of expression;\n\nbut enough time passed that I stirred, reached for the laptop, hauled it onto the bed, plugged in the phone.\n\n.\n\nThe monotony of describing mundane acts\n\nhas removed me from the experiences that impelled me to write.\n\nThose experiences said: no one should have to endure anything like this;\n\nlife should not be created.\n\nBut it was not just sensation that tortured me.\n\nIt was the defeat of my will, not just now, but many times.\n\nIt was all the lost opportunities to create in my own life,\n\nthe pointless obstruction, and being left to rust,\n\nthat denied everyone the benefits of what I might have made,\n\nthat negated my rare attempt to actually fix, and transform this malign existence.\n\n.\n\nThe sun is still far from rising, but my mind has stirred to something like wakefulness.\n\nPossible words now queue for attention and selection,\n\nthe hubbub of daylight communication,\n\nrather than crystallizing in the dark,\n\na single phrase that repeats and repeats and repeats that it not be forgotten.\n\nAnd I have remembered another thought:\n\nthat I am so tired of this. Of having to endure pain, whole days lost to waiting for pain to fade,\n\nin order to keep carting my burdens uphill, alone.\n\nOnce, I was optimistic,\n\nfruit of a happy childhood perhaps,\n\nand I think I still carry the error implanted,\n\nthe expectation that everything works out for the best,\n\nthat in the end one will be noticed and saved.\n\nThere was a time, a long time, when I thought I would do the saving;\n\nI thought I knew perspectives that would cheer everyone up,\n\nand recipes that would materially change the world for the better.\n\nAfter enough years had passed, I added the thought:\n\nthis world as it is, needs transformation;\n\nthere is no groundswell to even try to make it better,\n\ninstead I find myself on a solitary march years in duration;\n\ntherefore, no one should create life.\n\nThat was 1996.\n\n.\n\nBut both my defiant pursuit of a way to redeem existence,\n\nand my sad insight that it is not now an existence in which life should be created,\n\nremained hidden, buried. My life, and the world, twisted and turned, and I never produced a great work;\n\njust fragments, actions, statements that echoed briefly in a handful of other lives.\n\n.\n\nNow I lose the momentum of this testimony too,\n\nwhich began with a memoriam for all those lost who never return.\n\nThe sun will come, the illness subside, the migraine fade (though it returns weekly);\n\nand during the long night I fashioned new tactics for escaping the circumstances that weigh on me,\n\nthat interfere with my imperatives, this time so much so that the effort to preserve my mind\n\nleft my body weak and sick with misery.\n\nMy experience tells me, a hundred times over:\n\nshout for help, say you and all your possibilities are going to waste,\n\nand no one will come for you.\n\nSo one is left to survive, on the charity of family and the cunning to make a little money,\n\nleft to attempt to do as much as possible on one's own,\n\nand then, occasionally, make another big appeal for help.\n\nBut time grows short; the machines have advanced;\n\nit could mean hope more concrete than ever;\n\nbut I want to do more than just cultivate private hope,\n\nI want to attempt with all my strength, to do the specific things that I see to be done.\n\nAnd this is why the illness of bodily defeat is so bitter;\n\none's struggle, conducted alone, but in the hope of one day dragging treasures into daylight,\n\nis felled by the weakness of one's own physical vehicle.\n\nAnd now grief sears through me, though it seems that I will live to fight another day;\n\nand a voiceless cry forces itself out (but I keep it silent, for the others who try to sleep, wrapped in their own lives, in rooms nearby),\n\nand my cheeks are wet with tears.\n\nWhether it is the thought of all those who were not saved,\n\nor just the thought of myself and those others I have known who, though they lived, were not saved;\n\nand there was a flash of fierceness too, some ancient killer instinct,\n\nsensing opportunity to finally vanquish a foe.\n\n.\n\nThere is only anticlimax left. Words, that I will not further shape despite their flaws,\n\na message from one depth and one overcoming,\n\nout of the myriads that have been endured.",
      "plaintextDescription": "In memory of all those who fell and were not saved\n\n(I normally have little regard for trigger warnings,\n\nbut on this occasion, imagine that my words are prefaced with every trigger warning ever)\n\nA body, on a battlefield, maimed and bleeding;\n\nterrible pain, staring at the sky,\n\nleft to die.\n\nAn experience that must have happened a million times in human history,\n\nan experience that must be happening somewhere right now.\n\nMy situation is better - perhaps. I lie, not on a battlefield, but in a sickbed.\n\nI just have some kind of flu;\n\nand an eye haunted by a migraine that comes, and goes, and threatens to come again;\n\nand I am not sleeping.\n\nAlso, I can write.\n\nThe smartphone battery was almost dead, I thought I was resigned to simply lying awake for long hours, without the catharsis of expression;\n\nbut enough time passed that I stirred, reached for the laptop, hauled it onto the bed, plugged in the phone.\n\n.\n\nThe monotony of describing mundane acts\n\nhas removed me from the experiences that impelled me to write.\n\nThose experiences said: no one should have to endure anything like this;\n\nlife should not be created.\n\nBut it was not just sensation that tortured me.\n\nIt was the defeat of my will, not just now, but many times.\n\nIt was all the lost opportunities to create in my own life,\n\nthe pointless obstruction, and being left to rust,\n\nthat denied everyone the benefits of what I might have made,\n\nthat negated my rare attempt to actually fix, and transform this malign existence.\n\n.\n\nThe sun is still far from rising, but my mind has stirred to something like wakefulness.\n\nPossible words now queue for attention and selection,\n\nthe hubbub of daylight communication,\n\nrather than crystallizing in the dark,\n\na single phrase that repeats and repeats and repeats that it not be forgotten.\n\nAnd I have remembered another thought:\n\nthat I am so tired of this. Of having to endure pain, whole days lost to waiting for pain to fade,\n\nin order to keep carting my burdens uphill, alone.\n\nO",
      "wordCount": 809
    },
    "tags": [
      {
        "_id": "AXhEhCkTrHZbjXXu3",
        "name": "Poetry",
        "slug": "poetry"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SxRPHizoALmfS5m4R",
    "title": "Anthropics and a cosmic immune system",
    "slug": "anthropics-and-a-cosmic-immune-system",
    "url": null,
    "baseScore": -5,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 32,
    "createdAt": null,
    "postedAt": "2013-07-28T09:07:19.427Z",
    "contents": {
      "markdown": "Some people like to assume that the cosmos is ours for the taking, even though this could make us special to the order of 1 in 10^80^. The argument is that the cosmos _could be_ transformed by technology - engineered on astronomical scales - but _hasn't been_ thus transformed.\n\nThe most common alternative hypothesis is that \"we are in a simulation\". Perhaps we are. But there are other possibilities too.\n\nOne is that technological life usually destroys, not just its homeworld, but its whole bubble of space-time, by using high-energy physics to cause a [\"vacuum decay\"](http://en.wikipedia.org/wiki/False_vacuum), in which physics changes in a way that makes space uninhabitable. For example, the mass of an elementary particle is essentially equal to the energy density of the Higgs field, times a quantity called a \"yukawa coupling\". If the Higgs field increased its energy density by orders of magnitude, but the yukawas stayed the same, matter as we know it would be destroyed, everywhere that the change spread.\n\nHere I want to highlight a different possibility. The idea is that the universe contains very large lifeforms and very small lifeforms. We are among the small. The large ones are, let's say, mostly dark matter, galactic in scale, and stars and planets for them are like biomolecules for us; tiny functional elements which go together to make up the whole. And - the crucial part - they have immune systems which automatically crush anything which interferes with the natural celestial order.\n\nThis is why the skies are full of untamed stars rather than Dyson spheres - any small life which begins to act on that scale is destroyed by dark-matter antibodies. And it explains anthropically why you're human-size rather than galactic-size: small life is more numerous than large life, just not so numerous as cosmic colonization would imply.\n\nTwo questions arise - how did large life evolve, and, shouldn't anthropics favor universes which have no large life, just space-colonizing small life? I could spin a story about cosmological natural selection, and [large life which uses small life to reproduce](http://evodevouniverse.com/wiki/Meduso-anthropic_principle), but it doesn't really answer the second question, in particular. Still, I feel that this is a huge unexplored topic - the anthropic consequences of \"biocosmic\" ecology and evolution - and who knows what else is lurking here, waiting to be discovered?",
      "plaintextDescription": "Some people like to assume that the cosmos is ours for the taking, even though this could make us special to the order of 1 in 1080. The argument is that the cosmos could be transformed by technology - engineered on astronomical scales - but hasn't been thus transformed.\n\nThe most common alternative hypothesis is that \"we are in a simulation\". Perhaps we are. But there are other possibilities too.\n\nOne is that technological life usually destroys, not just its homeworld, but its whole bubble of space-time, by using high-energy physics to cause a \"vacuum decay\", in which physics changes in a way that makes space uninhabitable. For example, the mass of an elementary particle is essentially equal to the energy density of the Higgs field, times a quantity called a \"yukawa coupling\". If the Higgs field increased its energy density by orders of magnitude, but the yukawas stayed the same, matter as we know it would be destroyed, everywhere that the change spread.\n\nHere I want to highlight a different possibility. The idea is that the universe contains very large lifeforms and very small lifeforms. We are among the small. The large ones are, let's say, mostly dark matter, galactic in scale, and stars and planets for them are like biomolecules for us; tiny functional elements which go together to make up the whole. And - the crucial part - they have immune systems which automatically crush anything which interferes with the natural celestial order.\n\nThis is why the skies are full of untamed stars rather than Dyson spheres - any small life which begins to act on that scale is destroyed by dark-matter antibodies. And it explains anthropically why you're human-size rather than galactic-size: small life is more numerous than large life, just not so numerous as cosmic colonization would imply.\n\nTwo questions arise - how did large life evolve, and, shouldn't anthropics favor universes which have no large life, just space-colonizing small life? I could spin a story about cosmologica",
      "wordCount": 381
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vewxgYHhsNTTtkqSj",
    "title": "Living in the shadow of superintelligence",
    "slug": "living-in-the-shadow-of-superintelligence",
    "url": null,
    "baseScore": -3,
    "voteCount": 21,
    "viewCount": null,
    "commentCount": 17,
    "createdAt": null,
    "postedAt": "2013-06-24T12:06:18.614Z",
    "contents": {
      "markdown": "Although it regularly discusses the possibility of superintelligences with the power to transform the universe in the service of some value system - whether that value system is [paperclip maximization](http://wiki.lesswrong.com/wiki/Paperclip_maximizer) or [some elusive extrapolation of human values](http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition) \\- it seems that Less Wrong has never systematically discussed the possibility that we are already within the domain of some superintelligence, and what that would imply. So how about it? What are the possibilities, what are the probabilities, and how should they affect our choices?",
      "plaintextDescription": "Although it regularly discusses the possibility of superintelligences with the power to transform the universe in the service of some value system - whether that value system is paperclip maximization or some elusive extrapolation of human values - it seems that Less Wrong has never systematically discussed the possibility that we are already within the domain of some superintelligence, and what that would imply. So how about it? What are the possibilities, what are the probabilities, and how should they affect our choices?",
      "wordCount": 83
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "zDFRu6iKnDnfbuCL5",
    "title": "The ongoing transformation of quantum field theory",
    "slug": "the-ongoing-transformation-of-quantum-field-theory",
    "url": null,
    "baseScore": 32,
    "voteCount": 40,
    "viewCount": null,
    "commentCount": 19,
    "createdAt": null,
    "postedAt": "2012-12-29T09:45:55.580Z",
    "contents": {
      "markdown": "Quantum field theory (QFT) is the basic framework of particle physics. Particles arise from the quantized energy levels of field oscillations; Feynman diagrams are the simple tool for approximating their interactions. The \"standard model\", the success of which is capped by the recent observation of a Higgs boson lookalike, is a quantum field theory.\n\nBut just like everything mathematical, quantum field theory has hidden depths. For the past decade, new pictures of the quantum scattering process (in which particles come together, interact, and then fly apart) have incrementally been developed, and they presage a transformation in the understanding of what a QFT describes.\n\nAt the center of this evolution is \"N=4 super-Yang-Mills theory\", the maximally supersymmetric QFT in four dimensions. I want to emphasize that from a standard QFT perspective, this theory contains nothing but scalar particles (like the Higgs), spin-1/2 fermions (like electrons or quarks), and spin-1 \"gauge fields\" (like photons and gluons). The ingredients aren't something alien to real physics. What distinguishes an N=4 theory is that the particle spectrum and the interactions are arranged so as to produce a highly extended form of supersymmetry, in which particles have multiple partners (so many LWers should be comfortable with the notion).\n\nIn 1997, Juan Maldacena discovered that [the N=4 theory is equivalent to a type of string theory in a particular higher-dimensional space](http://arxiv.org/abs/hep-th/9711200). In 2003, Edward Witten discovered that [it is also equivalent to a different type of string theory in a supersymmetric version of Roger Penrose's twistor space](http://arxiv.org/abs/hep-th/0312171). Those insights didn't come from nowhere, they explained algebraic facts that had been known for many years; and they have led to a still-accumulating stockpile of discoveries about the properties of N=4 field theory.\n\nWhat we can say is that the physical processes appearing in the theory can be understood as taking place in either of two dual space-time descriptions. Each space-time has its own version of a particular large symmetry, \"superconformal symmetry\", and the superconformal symmetry of one space-time is invisible in the other. And now it is becoming apparent that there is a third description, which does not involve space-time at all, in which both superconformal symmetries are manifest, but in which space-time locality and quantum unitarity are not \"visible\" - that is, they are not manifest in the equations that define the theory in this third picture.\n\nI cannot provide an authoritative account of how the new picture works. But here is my impression. In the third picture, the scattering processes of the space-time picture become a complex of polytopes - higher-dimensional polyhedra, joined at their faces - and the quantum measure becomes the volume of these polyhedra. Where you previously had particles, you now just have the dimensions of the polytopes; and the fact that in general, an _n_-dimensional space doesn't have _n_ special directions suggests to me that multi-particle entanglements can be something more fundamental than the separate particles that we resolve them into.\n\nIt will be especially interesting to see whether this polytope combinatorics, that can give back the scattering probabilities calculated with Feynman diagrams in the usual picture, can work solely with ordinary probabilities. That was Penrose's objective, almost fifty years ago, when he developed the theory of \"spin networks\" as a new language for the angular momentum calculations of quantum theory, and which was a step towards the twistor variables now playing an essential role in these new developments. If the probability calculus of quantum mechanics can be obtained from conventional probability theory applied to these \"structures\" that may underlie familiar space-time, then that would mean that superposition does not need to be regarded as ontological.\n\nI'm talking about this now because a group of researchers around Nima Arkani-Hamed, who are among the leaders in this area, released [their first paper in a year](http://arxiv.org/abs/1212.5605) this week. It's very new, and so arcane that, among physics bloggers, only Lubos Motl has [talked about it](http://motls.blogspot.com/2012/12/amplitudes-permutations-grassmannians.html).\n\nThis is still just one step in a journey. Not only does the paper focus on the N=4 theory - which is not the theory of the real world - but the results only apply to part of the N=4 theory, the so-called \"planar\" part, described by Feynman diagrams with a planar topology. (For an impressionistic glimpse of what might lie ahead, you could try [this paper](http://vixra.org/abs/1208.0242), whose author has been shouting from the wilderness for years that categorical knot theory is the missing piece of the puzzle.)\n\nThe N=4 theory is not reality, but the new perspective should generalize. Present-day calculations in QCD already employ truncated versions of the N=4 theory; and Arkani-Hamed et al specifically mention another supersymmetric field theory (known as ABJM after the initials of its authors), a deformation of which is holographically dual to a theory-of-everything candidate from 1983.\n\nWhen it comes to _seeing reality_ in this new way, we still only have, at best, a fruitful chaos of ideas and possibilities. But the solid results - the mathematical equivalences - will continue to pile up, and the end product really ought to be nothing less than a new conception of how physics works.",
      "plaintextDescription": "Quantum field theory (QFT) is the basic framework of particle physics. Particles arise from the quantized energy levels of field oscillations; Feynman diagrams are the simple tool for approximating their interactions. The \"standard model\", the success of which is capped by the recent observation of a Higgs boson lookalike, is a quantum field theory.\n\nBut just like everything mathematical, quantum field theory has hidden depths. For the past decade, new pictures of the quantum scattering process (in which particles come together, interact, and then fly apart) have incrementally been developed, and they presage a transformation in the understanding of what a QFT describes.\n\nAt the center of this evolution is \"N=4 super-Yang-Mills theory\", the maximally supersymmetric QFT in four dimensions. I want to emphasize that from a standard QFT perspective, this theory contains nothing but scalar particles (like the Higgs), spin-1/2 fermions (like electrons or quarks), and spin-1 \"gauge fields\" (like photons and gluons). The ingredients aren't something alien to real physics. What distinguishes an N=4 theory is that the particle spectrum and the interactions are arranged so as to produce a highly extended form of supersymmetry, in which particles have multiple partners (so many LWers should be comfortable with the notion).\n\nIn 1997, Juan Maldacena discovered that the N=4 theory is equivalent to a type of string theory in a particular higher-dimensional space. In 2003, Edward Witten discovered that it is also equivalent to a different type of string theory in a supersymmetric version of Roger Penrose's twistor space. Those insights didn't come from nowhere, they explained algebraic facts that had been known for many years; and they have led to a still-accumulating stockpile of discoveries about the properties of N=4 field theory.\n\nWhat we can say is that the physical processes appearing in the theory can be understood as taking place in either of two dual space-time descriptions",
      "wordCount": 840
    },
    "tags": [
      {
        "_id": "csMv9MvvjYJyeHqoo",
        "name": "Physics",
        "slug": "physics"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bsP43XBrSDMCqXPuG",
    "title": "Call for a Friendly AI channel on freenode",
    "slug": "call-for-a-friendly-ai-channel-on-freenode",
    "url": null,
    "baseScore": 11,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2012-12-10T23:27:08.618Z",
    "contents": {
      "markdown": "I visited #lesswrong on freenode yesterday and was able to get in some discussion of FAI-related matters. But that channel also exists to allow discussion of rationalist fanfiction, political opinions, and whatever else people want to talk about.  \n  \nI would like for there to be a place on that network where the topic actually is Friendly AI - where you can go to brainstorm, and maybe you'll have to wait because they're already talking about cognitive neuroscience or automated theorem provers, but not because they're talking about ponies or politics.  \n  \nSurely there are enough people with a serious, technical interest in FAI and related topics (and I don't just mean among LW regulars) to make such a channel sustainable. I'll bet that there are other people holding back from participation precisely because existing forums are so full of uninformed noise and conversational tangents. It's inevitable that entropy would set in after a while, but if the default baseline was still that even the chatter was technically informed and focused on what's coming - that would be mission accomplished.  \n  \nI explored the freenode namespace a little. #FAI redirects to #unavailable, so it may be an abandoned project. #AGI exists but is invite-only. #AI exists but I'm told it's dull, and besides, the agenda here is meant to be, not just AI, but singularity-relevant AI. So there seems to be an opening. Or am I reinventing the wheel?",
      "plaintextDescription": "I visited #lesswrong on freenode yesterday and was able to get in some discussion of FAI-related matters. But that channel also exists to allow discussion of rationalist fanfiction, political opinions, and whatever else people want to talk about.\n\nI would like for there to be a place on that network where the topic actually is Friendly AI - where you can go to brainstorm, and maybe you'll have to wait because they're already talking about cognitive neuroscience or automated theorem provers, but not because they're talking about ponies or politics.\n\nSurely there are enough people with a serious, technical interest in FAI and related topics (and I don't just mean among LW regulars) to make such a channel sustainable. I'll bet that there are other people holding back from participation precisely because existing forums are so full of uninformed noise and conversational tangents. It's inevitable that entropy would set in after a while, but if the default baseline was still that even the chatter was technically informed and focused on what's coming - that would be mission accomplished.\n\nI explored the freenode namespace a little. #FAI redirects to #unavailable, so it may be an abandoned project. #AGI exists but is invite-only. #AI exists but I'm told it's dull, and besides, the agenda here is meant to be, not just AI, but singularity-relevant AI. So there seems to be an opening. Or am I reinventing the wheel?",
      "wordCount": 245
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "zWNyEM9xmK7fbbxpD",
    "title": "FAI, FIA, and singularity politics",
    "slug": "fai-fia-and-singularity-politics",
    "url": null,
    "baseScore": 17,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2012-11-08T17:11:10.674Z",
    "contents": {
      "markdown": "In discussing scenarios of the future, I speak of \"slow futures\" and \"fast futures\". A fast future is exemplified by what is now called a [hard takeoff](http://wiki.lesswrong.com/wiki/AI_takeoff) singularity: something bootstraps its way to superhuman intelligence in a short time. A slow future is a continuation of history as we know it: decades pass and the world changes, with new politics, culture, and technology. To some extent the [Hanson vs Yudkowsky debate](http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate) was about slow vs fast; Robin's future is fast-moving, but on the way there, there's never an event in which some single \"agent\" becomes all-powerful by getting ahead of all others.  \n  \nThe Singularity Institute does many things, but I take its core agenda to be about a fast scenario. The theoretical objective is to design an AI which would still be friendly if it became all-powerful. There is also the practical objective of ensuring that the first AI across the self-enhancement threshold _is_ friendly. One way to do that is to be the one who makes it, but that's asking a lot. Another way is to have enough FAI design and FAI theory out there, that the people who _do_ win the mind race will have known about it and will have taken it into consideration. Then there are mixed strategies, such as working on FAI theory while liaising with known AI projects that are contenders in the race and whose principals are receptive to the idea of friendliness.  \n  \nI recently [criticised](/lw/eyl/thinking_soberly_about_the_context_and/) a lot of the ideas that circulate in conjunction with the concept of friendly AI. The \"sober\" ideas and the \"extreme\" ideas have a certain correlation with slow-future and fast-future scenarios, respectively. The sober future is a slow one where AIs exist and posthumanity expands into space, but history, politics, and finitude aren't transcended. The extreme future is a fast one where one day the ingredients for a hard takeoff are brought together in one place, an artificial god is born, and, depending on its inclinations and on the nature of reality, something transcendental happens: everyone uploads to the Planck scale, our local overmind reaches out to other realities, we \"live forever and remember it afterwards\".  \n  \nAlthough I have criticised such transcendentalism, saying that it should not be the default expectation of the future, I do think that the \"hard takeoff\" and the \"all-powerful agent\" would be among the strategic considerations in an ideal plan for the future, though in a rather broader sense than is usually discussed. The reason is that if one day Earth is being ruled by, say, a coalition of AIs with a particular value system, with natural humans reduced to the status of wildlife, then the functional equivalent of a singularity has occurred, even if these AIs have no intention of going on to conquer the galaxy; and I regard _that_ as a quite conceivable scenario. It is fantastic (in the sense of mind-boggling), but it's not transcendental. All the scenario implies is that the human race is no longer at the top of the heap; it has successors and they are now in charge.  \n  \nBut we can view those successors as, collectively, the \"all-powerful agent\" that has replaced human hegemony. And we can regard the events, whatever they were, that first gave the original such entities their unbeatable advantage in power, as the \"hard takeoff\" of this scenario. So even a slow, sober future scenario can issue in a singularity where the basic premises and motivations of existing FAI research apply. It's just that one might need to be imaginative in anticipating how they are realized.  \n  \nFor example, perhaps hegemonic superintelligence could emerge, not from a single powerful AI research program, but from a particular clique of networked neurohackers who have the right combination of collaborative tools, brain interfaces, and concrete plans for achieving transhuman intelligence. They might go on to build an army of AIs, and subdue the world that way, but the crucial steps which made them the winners in the mind race, and which determined what they would do with their victory, would lie in their methods of brain modification, enhancement, and interfacing, and in the ends to which they applied those methods.  \n  \nIn such a scenario, we could speak of \"FIA\" - friendly intelligence augmentation. A basic idea of existing FAI discourse is that the true human utility function needs to be determined, and then the values that make an AI human-friendly would be extrapolated from that. Similar thinking can be applied to the prospect of brain modification and intelligence increase in human beings. Human brains work a certain way, modified or augmented human brains will work in specifically different ways, and we should want to know which modifications are genuinely enhancements, what sort of modifications stabilize value and which ones destabilize value, and so on.  \n  \nIf there was a mature and sophisticated culture of preparing for the singularity, then there would be FAI research, FIA research, and a lot of communication between the two fields. (For example, researchers in both fields need to figure out how the human brain works.) Instead, the biggest enthusiasts of FAI are a futurist subculture with a lot of conceptual baggage, and FIA is nonexistent. However, we can at least start thinking and discussing about how this broader culture of research into \"friendly minds\" could take shape.  \n  \nDespite its flaws, the Singularity Institute stands alone as an organization concerned with the fast future scenario, the hard takeoff. I have argued that a sober futurology, while forecasting a slowly evolving future for some time to come, must ultimately concern itself with the emergence of a posthuman power arising from some cognitive technology, whether that is AI, neurotechnology, or a combination of these. So I have asked myself who, among \"slow futurists\", is best equipped to develop an outlook and a plan which is sober and realistic, yet also visionary enough to accommodate the really overwhelming responsibility of designing the architecture of friendly posthuman minds capable of managing a future that we would want.  \n  \nAt the moment, my favorites in this respect are the various branches, scattered around the world, of the [Longevity Party](http://www.facebook.com/groups/longevity.party/) that was started in Russia a few months ago. (It shouldn't be confused with \"Evolution 2045\", a big-budget rival backed by an Internet entrepreneur, that especially promotes mind uploading. For some reason, transhumanist politics has begun to stir in that country.) If the Singularity Institute falls short of the ideal, then the \"longevity parties\" are even further away from living up to their ambitious agenda. Outside of Russia, they are mostly just small Facebook groups; the most basic issues of policy and practice are still being worked out; no-one involved has much of a history of political achievement.  \n  \nNonetheless, if there were no prospect of singularity but otherwise science and technology were advancing as they are, the agenda here looks just about ideal. People age and decline until it kills them, an extrapolation of biomedical knowledge suggests this is not a law of nature but just a sign of primitive technology, and the Longevity Party exists to rectify this situation. It's visionary and despite the current immaturity and growing pains, an effective longevity politics must arise one day, simply because the advance of technology will force the issue on us! The human race cannot currently muster enough will to live, to openly make rejuvenation a political goal, but the incremental pursuit of health and well-being is taking us in that direction anyway.  \n  \nThere's a vacuum of authority and intention in the realm of life extension, and transhuman technology generally, and these would-be longevity politicians are stepping into that vacuum. I don't think they are ready for all the issues that transhuman power entails, but the process has to start somewhere. Faced with the infinite possibilities of technological transformation, the basic affirmation of the desire to live as well as reality permits, can serve as a founding principle against which to judge attitudes and approaches for all the more complicated \"issues\" that arise in a world where anyone can become anything.  \n  \nMaria Konovalenko, a biomedical researcher and one of the prime movers behind the Russian Longevity Party, wrote an [essay](http://mariakonovalenko.wordpress.com/2012/09/10/i-am-an-aging-fighter-because-life-is-the-main-human-right-demand-and-desire/) setting out her version of how the world ought to work. You'll notice that she manages to include friendly AI on her agenda. This is another example, a humble beginning, of the sort of conceptual development which I think needs to happen. The sort of approach to FAI that Eliezer has pioneered needs a context, a broader culture concerned with FIA and the interplay between neuroscience and pure AI, and we need realistic yet visionary political thinking which encompasses both the shocking potentials of a slow future, above all rejuvenation and the conquest of aging, and the singularity imperative.  \n  \nUnless there is simply a catastrophe, one day someone, some thing, some coalition will wield transhuman power. It may begin as a corporation, or as a specific technological research subculture, or as the peak political body in a sovereign state. Perhaps it will be part of a broader global culture of \"competitors in the mind race\" who know about each other and recognize each other as contenders for the first across the line. Perhaps there will be coalitions in the race: contenders who agree on the need for friendliness and the form it should take, and others who are pursuing private power, or who are just pushing AI ahead without too much concern for the transformation of the world that will result. Perhaps there will be a war as one contender begins to visibly pull ahead, and others resort to force to stop them.  \n  \nBut without a final and total catastrophe, however much slow history there remains ahead of us, eventually someone or something will \"win\", and after that the world will be reshaped according to its values and priorities. We don't need to imagine this as \"tiling the universe\"; it should be enough to think of it as a ubiquitous posthuman political order, in which all intelligent agents are either kept so powerless as to not be a threat, or managed and modified so as to be reliably friendly to whatever the governing civilizational values are. I see no alternative to this if we are looking for a stable long-term way of living in which ultimate technological powers exist; the ultimate powers of coercion and destruction can't be left lying around, to be taken up by entities with arbitrary values.  \n  \nSo the supreme challenge is to conceive of a social and technological order where that power exists, and is used, but it's still a world that we want to live in. FAI is part of the answer, but so is FIA, and so is the development of political concepts and projects which can encompass such an agenda. The Singularity Institute and the Longevity Party are fledgling institutions, and if they live they will surely, eventually, form ties with older and more established bodies; but right now, they seem to be the crucial nuclei of the theoretical research and the political vision that we need.",
      "plaintextDescription": "In discussing scenarios of the future, I speak of \"slow futures\" and \"fast futures\". A fast future is exemplified by what is now called a hard takeoff singularity: something bootstraps its way to superhuman intelligence in a short time. A slow future is a continuation of history as we know it: decades pass and the world changes, with new politics, culture, and technology. To some extent the Hanson vs Yudkowsky debate was about slow vs fast; Robin's future is fast-moving, but on the way there, there's never an event in which some single \"agent\" becomes all-powerful by getting ahead of all others.\n\nThe Singularity Institute does many things, but I take its core agenda to be about a fast scenario. The theoretical objective is to design an AI which would still be friendly if it became all-powerful. There is also the practical objective of ensuring that the first AI across the self-enhancement threshold is friendly. One way to do that is to be the one who makes it, but that's asking a lot. Another way is to have enough FAI design and FAI theory out there, that the people who do win the mind race will have known about it and will have taken it into consideration. Then there are mixed strategies, such as working on FAI theory while liaising with known AI projects that are contenders in the race and whose principals are receptive to the idea of friendliness.\n\nI recently criticised a lot of the ideas that circulate in conjunction with the concept of friendly AI. The \"sober\" ideas and the \"extreme\" ideas have a certain correlation with slow-future and fast-future scenarios, respectively. The sober future is a slow one where AIs exist and posthumanity expands into space, but history, politics, and finitude aren't transcended. The extreme future is a fast one where one day the ingredients for a hard takeoff are brought together in one place, an artificial god is born, and, depending on its inclinations and on the nature of reality, something transcendental happens: everyone upl",
      "wordCount": 1889
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "PGhBFDg6L54XLvGGu",
    "title": "Ambitious utilitarians must concern themselves with death",
    "slug": "ambitious-utilitarians-must-concern-themselves-with-death",
    "url": null,
    "baseScore": 8,
    "voteCount": 20,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2012-10-25T10:41:41.269Z",
    "contents": {
      "markdown": "And I don't mean that they must concern themselves with death in the sense of ending death, or removing its sting through mental backups, or delaying it to the later ages of the universe; or in the sense of working to decrease the probability of extinction risks and other forms of megadeath; or even in the sense of saving as many lives as possible, as efficiently as possible. All of that is legitimate and interesting. But I mean something far more down to earth.\n\nFirst, let me specify more precisely who I am talking about. I mean people who are trying to maximize the general welfare; who are trying to achieve the greatest good for the greatest number; who are trying to do the best thing possible with their lives. When someone like that makes decisions, they are implicitly choosing among possible futures in a very radical way. They may be making judgments about whether a future with millions or billions of extra lives is better than some alternative. Whether anyone is ever in a position to make that much of a difference is another matter; but we can think of it like voting. You are at least making a statement about which sort of future you think you prefer, and then you do what you can, and that either makes a difference or it doesn't.\n\nIt seems to me that the discussions about the value of life among utilitarians are rather superficial. The typical notion is that we should maximize net pleasure and minimize net pain. Already that poses the question of whether a life of dull persistent happiness is better or worse than a life of extreme highs and lows. A more sophisticated notion is that we should just aspire to maximize \"utility\", where perhaps we don't even know what utility is yet. Certainly the CEV philosophy is that we don't yet know what utility really is for human beings. It would be interesting to see people who took that agnosticism to heart, people whose life-strategy amounted to (1) discovering true utility as soon as possible (2) living according to interim heuristics whose uncertainty is recognized, but which are adopted out of the necessity of having some sort of personal decision procedure.\n\nSo what I'm going to say pertains to (2). You may, if you wish, hold to the idea that the nature of true utility, like true friendliness, won't be known until the true workings of the human mind are known. What follows is something you should think on in order to refine your interim heuristics.\n\nThe first thing is that to create a life is to create a death. A life ends. And while the end of a life may not be its most important moment, it reminds us that a life is a whole. Any accurate estimation of the utility of a life is going to be a judgment of that whole.\n\nSo a utilitarian ought to contemplate the deaths of the world, and the lives that reach their ends in those deaths. Because the possible futures, that you wish to choose between, are distinguished by the number and nature of the whole lives that they contain. And all these dozens of people, all around the world of the present, ceasing to exist in every minute that passes, are examples of completed lives. Those lives weren't necessarily complete, in the sense of all personal desires and projects having come to their conclusion; but they came to their physical completion.\n\nTo choose one future over another is to prefer one set of completed lives to another set. It would be a godlike decision to truly be solely responsible for such a choice. In the real world, people hardly choose their own futures, let alone the future of the world; choice is a lifelong engagement with an evolving and partially known situation, not a once-off choice between several completely known scenarios; and even when a single person does end up being massively influential, they generally don't know what sort of future they're bringing about. The actual limitations on the knowledge and power of any individual may make the whole quest of the \"ambitious utilitarian\" seem quixotic. But a new principle, a new heuristic, can propagate far beyond one individual, so thinking big can have big consequences.\n\nThe main principle that I derive, from contemplating the completed lives of the world, is cautionary antinatalism. The badness of what _can_ happen in a life, and the disappointing character of what _usually_ happens, are what do it for me. I am all for the transhumanist quest and the struggle for a friendly singularity, and I support the desire of people who are already alive to make the most of that life. But I would recommend against the creation of life, at least until the current historical drama has played itself out - until the singularity, if I must use that word. We are in the process of gaining new powers and learning new things, there are obvious unknowns in front of us that we are on the way to figuring out, so at least hold off until they have been figured out and we have a better idea of what reality is about, and what we can really hope for, from existence.\n\nHowever, the object of this post is not to argue for my special flavor of antinatalism. It is to encourage realistic consideration of what lives and futures are like. In particular, I would encourage more \"story thinking\", which [has been criticized](http://meteuphoric.wordpress.com/2010/04/23/systems-and-stories/) in favor of \"systems thinking\". Every actual life _is_ a \"story\", in the sense of being a sequence of events that happens to someone. If you were judging the merit of a whole possible world on the basis of the whole lives that it contained, then you would be making a decision about whether those stories ought to actually occur. The biographical life-story is the building block of such possible worlds.\n\nSo an ambitious utilitarian, who aspires to have a set of criteria for deciding among whole possible worlds, really needs to understand possible lives. They need to know what sort of lives are likely under various circumstances; they need to know the nature of the different possible lives - what it's like to be that person; they need to know what sort of bad is going to accompany the sort of good that they decide to champion. They need to have some estimation of the value of a whole life, up to and including its death.\n\nAs usual, we are talking about a depth of knowledge that may in practice be impossible to attain. But before we go calling something impossible, and settling for a lesser ambition, let's at least try to grasp what the greater ambition truly entails. To truly choose a whole world would be to make the decision of a god, about the lives and deaths that will occur in that world. The future of our world, for some time to come, will repeat the sorts of lives and deaths that have already occurred in it. So if, in your world-planning, you don't just count on completely abolishing the present world and/or replacing it with a new one that works in a completely different way, you owe it to your cause to form a judgement about the totality of what has already happened here on Earth, and you need to figure out what you approve of, what you disapprove of, whether you can have the good without the bad, and how much badness is too much.",
      "plaintextDescription": "And I don't mean that they must concern themselves with death in the sense of ending death, or removing its sting through mental backups, or delaying it to the later ages of the universe; or in the sense of working to decrease the probability of extinction risks and other forms of megadeath; or even in the sense of saving as many lives as possible, as efficiently as possible. All of that is legitimate and interesting. But I mean something far more down to earth.\n\nFirst, let me specify more precisely who I am talking about. I mean people who are trying to maximize the general welfare; who are trying to achieve the greatest good for the greatest number; who are trying to do the best thing possible with their lives. When someone like that makes decisions, they are implicitly choosing among possible futures in a very radical way. They may be making judgments about whether a future with millions or billions of extra lives is better than some alternative. Whether anyone is ever in a position to make that much of a difference is another matter; but we can think of it like voting. You are at least making a statement about which sort of future you think you prefer, and then you do what you can, and that either makes a difference or it doesn't.\n\nIt seems to me that the discussions about the value of life among utilitarians are rather superficial. The typical notion is that we should maximize net pleasure and minimize net pain. Already that poses the question of whether a life of dull persistent happiness is better or worse than a life of extreme highs and lows. A more sophisticated notion is that we should just aspire to maximize \"utility\", where perhaps we don't even know what utility is yet. Certainly the CEV philosophy is that we don't yet know what utility really is for human beings. It would be interesting to see people who took that agnosticism to heart, people whose life-strategy amounted to (1) discovering true utility as soon as possible (2) living according to inter",
      "wordCount": 1253
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "czxjKohS7RQjkBiSD",
    "title": "Thinking soberly about the context and consequences of Friendly AI",
    "slug": "thinking-soberly-about-the-context-and-consequences-of",
    "url": null,
    "baseScore": 21,
    "voteCount": 42,
    "viewCount": null,
    "commentCount": 39,
    "createdAt": null,
    "postedAt": "2012-10-16T04:33:52.859Z",
    "contents": {
      "markdown": "The project of Friendly AI would benefit from being approached in a much more down-to-earth way. Discourse about the subject seems to be dominated by a set of possibilities which are given far too much credence:\n\n*   A single AI will take over the world\n*   A future galactic civilization depends on 21st-century Earth\n*   10^n^-year lifespans are at stake, n greater than or equal to 3 \n*   We might be living in a simulation\n*   Acausal deal-making\n*   Multiverse theory \n\nAdd up all of that, and you have a great recipe for enjoyable irrelevance. Negate every single one of those ideas, and you have an alternative set of working assumptions that are still consistent with the idea that Friendly AI matters, and which are much more suited to practical success:\n\n*   There will always be multiple centers of power\n*   What's at stake is, at most, the future centuries of a solar-system civilization\n*   No assumption that individual humans can survive even for hundreds of years, or that they would want to\n*   Assume that the visible world is the real world\n*   Assume that life and intelligence are about causal interaction\n*   Assume that the single visible world is the only world we affect or have reason to care about \n\nThe simplest reason to care about Friendly AI is that we are going to be coexisting with AI, and so we should want it to be something we can live with. I don't see that anything important would be lost by strongly foregrounding the second set of assumptions, and treating the first set of possibilities just as possibilities, rather than as the working hypothesis about reality.\n\n\\[Earlier posts on related themes: [practical FAI](/lw/772/what_a_practical_plan_for_friendly_ai_looks_like/), [FAI without \"outsourcing\"](/lw/c1x/extrapolating_values_without_outsourcing/).\\]",
      "plaintextDescription": "The project of Friendly AI would benefit from being approached in a much more down-to-earth way. Discourse about the subject seems to be dominated by a set of possibilities which are given far too much credence:\n\n * A single AI will take over the world\n * A future galactic civilization depends on 21st-century Earth\n * 10n-year lifespans are at stake, n greater than or equal to 3 \n * We might be living in a simulation\n * Acausal deal-making\n * Multiverse theory \n\nAdd up all of that, and you have a great recipe for enjoyable irrelevance. Negate every single one of those ideas, and you have an alternative set of working assumptions that are still consistent with the idea that Friendly AI matters, and which are much more suited to practical success:\n\n * There will always be multiple centers of power\n * What's at stake is, at most, the future centuries of a solar-system civilization\n * No assumption that individual humans can survive even for hundreds of years, or that they would want to\n * Assume that the visible world is the real world\n * Assume that life and intelligence are about causal interaction\n * Assume that the single visible world is the only world we affect or have reason to care about \n\nThe simplest reason to care about Friendly AI is that we are going to be coexisting with AI, and so we should want it to be something we can live with. I don't see that anything important would be lost by strongly foregrounding the second set of assumptions, and treating the first set of possibilities just as possibilities, rather than as the working hypothesis about reality.\n\n[Earlier posts on related themes: practical FAI, FAI without \"outsourcing\".]",
      "wordCount": 297
    },
    "tags": [
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ueQBEaY6a7n5hDeWP",
    "title": "Debugging the Quantum Physics Sequence ",
    "slug": "debugging-the-quantum-physics-sequence",
    "url": null,
    "baseScore": 37,
    "voteCount": 56,
    "viewCount": null,
    "commentCount": 130,
    "createdAt": null,
    "postedAt": "2012-09-05T15:55:53.054Z",
    "contents": {
      "markdown": "This article should really be called \"Patching the argumentative flaw in the Sequences created by the Quantum Physics Sequence\".\n\nThere's only one big thing wrong with that Sequence: the central factual claim is wrong. I don't mean the claim that the Many Worlds interpretation is correct; I mean the claim that the Many Worlds interpretation is _obviously_ correct. I don't agree with the ontological claim either, but I especially don't agree with the epistemological claim. It's a strawman which reduces the quantum debate to Everett versus Bohr - well, it's not really Bohr, since Bohr didn't believe wavefunctions were physical entities. Everett versus Collapse, then.\n\nI've complained about this from the beginning, simply because I've also studied the topic and profoundly disagree with Eliezer's assessment. What I would like to see discussed on this occasion is not the physics, but rather how to patch the arguments in the Sequences that depend on this wrong sub-argument. To my eyes, this is a highly visible flaw, but it's not a deep one. It's a detail, a bug. Surely it affects nothing of substance.\n\nHowever, before I proceed, I'd better back up my criticism. So: consider the existence of single-world retrocausal interpretations of quantum mechanics, such as John Cramer's [transactional interpretation](http://en.wikipedia.org/wiki/Transactional_interpretation), which is descended from [Wheeler-Feynman absorber theory](http://en.wikipedia.org/wiki/Wheeler%E2%80%93Feynman_absorber_theory). There are no superpositions, only causal chains running forward in time and backward in time. The calculus of complex-valued probability amplitudes is supposed to arise from this.\n\nThe existence of the retrocausal tradition already shows that the debate has been represented incorrectly; it should at least be Everett versus Bohr versus Cramer. I would also argue that when you look at the details, many-worlds has no discernible edge over single-world retrocausality:\n\n*   Relativity isn't an issue for the transactional interpretation: causality forwards and causality backwards are both local, it's the existence of loops in time which create the appearance of nonlocality.\n*   Retrocausal interpretations don't have an exact derivation of the Born rule, but neither does many-worlds.\n*   Many-worlds finds hope of such a derivation in a property of the quantum formalism: the resemblance of density matrix entries to probabilities. But single-world retrocausality finds such hope too: the Born probabilities can be obtained from the product of _ψ_ with _ψ*_, its complex conjugate, and _ψ\\*_ is the time reverse of _ψ. _\n*   Loops in time just fundamentally bug some people, but splitting worlds have the same effect on others.\n\nI am not especially an advocate of retrocausal interpretations. They are among the possibilities; they deserve consideration and they get it. Retrocausality may or may not be an element of the real explanation of why quantum mechanics works. Progress towards the discovery of the truth requires exploration on many fronts, that's happening, we'll get there eventually. I have focused on retrocausal interpretations here just because they offer the clearest evidence that the big picture offered by the Sequence is wrong.\n\nIt's hopeless to suggest rewriting the Sequence, I don't think that would be a good use of anyone's time. But what I _would_ like to have, is a clear idea of the role that \"the winner is ... Many Worlds!\" plays in the overall flow of argument, in the great meta-sequence that is Less Wrong's foundational text; and I would also like to have a clear idea of how to patch the argument, so that it routes around this flaw.\n\n[In the wiki, it states that](http://wiki.lesswrong.com/wiki/Sequences#The_Quantum_Physics_Sequence) \"Cleaning up the old confusion about QM is used to introduce basic issues in rationality (such as the technical version of Occam's Razor), epistemology, reductionism, naturalism, and philosophy of science.\" So there we have it - a synopsis of the function that this Sequence is supposed to perform. Perhaps we need a working group that will identify each of the individual arguments, and come up with a substitute for each one.",
      "plaintextDescription": "This article should really be called \"Patching the argumentative flaw in the Sequences created by the Quantum Physics Sequence\".\n\nThere's only one big thing wrong with that Sequence: the central factual claim is wrong. I don't mean the claim that the Many Worlds interpretation is correct; I mean the claim that the Many Worlds interpretation is obviously correct. I don't agree with the ontological claim either, but I especially don't agree with the epistemological claim. It's a strawman which reduces the quantum debate to Everett versus Bohr - well, it's not really Bohr, since Bohr didn't believe wavefunctions were physical entities. Everett versus Collapse, then.\n\nI've complained about this from the beginning, simply because I've also studied the topic and profoundly disagree with Eliezer's assessment. What I would like to see discussed on this occasion is not the physics, but rather how to patch the arguments in the Sequences that depend on this wrong sub-argument. To my eyes, this is a highly visible flaw, but it's not a deep one. It's a detail, a bug. Surely it affects nothing of substance.\n\nHowever, before I proceed, I'd better back up my criticism. So: consider the existence of single-world retrocausal interpretations of quantum mechanics, such as John Cramer's transactional interpretation, which is descended from Wheeler-Feynman absorber theory. There are no superpositions, only causal chains running forward in time and backward in time. The calculus of complex-valued probability amplitudes is supposed to arise from this.\n\nThe existence of the retrocausal tradition already shows that the debate has been represented incorrectly; it should at least be Everett versus Bohr versus Cramer. I would also argue that when you look at the details, many-worlds has no discernible edge over single-world retrocausality:\n\n * Relativity isn't an issue for the transactional interpretation: causality forwards and causality backwards are both local, it's the existence of loops in",
      "wordCount": 638
    },
    "tags": [
      {
        "_id": "csMv9MvvjYJyeHqoo",
        "name": "Physics",
        "slug": "physics"
      },
      {
        "_id": "MfpEPj6kJneT9gWT6",
        "name": "Site Meta",
        "slug": "site-meta"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "QanBKR8Fw5TsvC5E7",
    "title": "Friendly AI and the limits of computational epistemology",
    "slug": "friendly-ai-and-the-limits-of-computational-epistemology",
    "url": null,
    "baseScore": 25,
    "voteCount": 54,
    "viewCount": null,
    "commentCount": 148,
    "createdAt": null,
    "postedAt": "2012-08-08T13:16:27.269Z",
    "contents": {
      "markdown": "Very soon, Eliezer is supposed to start posting a new sequence, on \"Open Problems in Friendly AI\". After several years in which its activities were dominated by the topic of human rationality, this ought to mark the beginning of a new phase for the Singularity Institute, one in which it is visibly working on artificial intelligence once again. If everything comes together, then it will now be a straight line from here to the end.  \n  \nI foresee that, once the new sequence gets going, it won't be that easy to question the framework in terms of which the problems are posed. So I consider this my last opportunity for some time, to set out an alternative big picture. It's a framework in which all those rigorous mathematical and computational issues still need to be investigated, so a lot of \"orthodox\" ideas about Friendly AI should carry across. But the context is different, and it makes a difference.  \n  \nBegin with the really big picture. What would it take to produce a friendly singularity? You need to find the true ontology, find the true morality, and win the intelligence race. For example, if your Friendly AI was to be an expected utility maximizer, it would need to model the world correctly (\"true ontology\"), value the world correctly (\"true morality\"), and it would need to outsmart its opponents (\"win the intelligence race\").  \n  \nNow let's consider how SI will approach these goals.  \n  \nThe evidence says that the working ontological hypothesis of SI-associated researchers will be [timeless many-worlds quantum mechanics](http://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence#Timeless_Physics), possibly embedded in a \"Tegmark Level IV multiverse\", with the auxiliary hypothesis that algorithms can [\"feel like something from inside\"](/lw/no/how_an_algorithm_feels_from_inside/) and that this is what conscious experience is.  \n  \nThe true morality is to be found by understanding the true decision procedure employed by human beings, and idealizing it according to criteria implicit in that procedure. That is, one would seek to understand conceptually the physical and cognitive causation at work in concrete human choices, both conscious and unconscious, with the expectation that there will be a crisp, complex, and specific answer to the question \"why and how do humans make the choices that they do?\" Undoubtedly there would be some biological variation, and there would also be significant elements of the \"human decision procedure\",  as instantiated in any specific individual, which are set by experience and by culture, rather than by genetics. Nonetheless one expects that there is something like a specific _algorithm_ or algorithm-template here, which is part of the standard _Homo sapiens_ cognitive package and biological design; just another anatomical feature, particular to our species.  \n  \nHaving reconstructed this algorithm via scientific analysis of human genome, brain, and behavior, one would then idealize it using its own criteria. This algorithm defines the de-facto value system that human beings employ, but that is not necessarily the value system they would _wish_ to employ; nonetheless, human self-dissatisfaction also arises from the use of this algorithm to judge ourselves. So it contains the seeds of its own improvement. The value system of a Friendly AI is to be obtained from the recursive self-improvement of the natural human decision procedure.  \n  \nFinally, this is all for naught if seriously unfriendly AI appears first. It isn't good enough just to have the right goals, you must be able to carry them out. In the global race towards [artificial general intelligence](http://wiki.lesswrong.com/wiki/Artificial_general_intelligence), SI might hope to \"win\" either by being the first to achieve AGI, or by having its prescriptions adopted by those who _do_ first achieve AGI. They have some in-house competence regarding models of universal AI like AIXI, and they have many contacts in the world of AGI research, so they're at least _engaged_ with this aspect of the problem.  \n  \nUpon examining this tentative reconstruction of SI's game-plan, I find I have two major reservations. The big one, and the one most difficult to convey, concerns the ontological assumptions. [In second place](/lw/c1x/extrapolating_values_without_outsourcing/) is what I see as an undue emphasis on the idea of outsourcing the methodological and design problems of FAI research to uploaded researchers and/or a proto-FAI which is simulating or modeling human researchers. This is supposed to be a way to finesse philosophical difficulties like \"what is consciousness anyway\"; you just simulate some humans until they agree that they have solved the problem. The reasoning goes that if the simulation is good enough, it will be just as good as if ordinary non-simulated humans solved it.  \n  \nI also used to have a [third major criticism](/lw/4wq/rationality_singularity_method_and_the_mainstream/), that the big SI focus on rationality outreach was a mistake; but it brought in a lot of new people, and in any case that phase is ending, with the creation of CFAR, a separate organization. So we are down to two basic criticisms.  \n  \nFirst, \"ontology\". I do not think that SI intends to just program its AI with an apriori belief in the Everett multiverse, for two reasons. First, like anyone else, their ventures into AI will surely begin with programs that work within very limited and more down-to-earth ontological domains. Second, at least some of the AI's world-model ought to be obtained rationally. Scientific theories are supposed to be rationally justified, e.g. by their capacity to make successful predictions, and one would prefer that the AI's ontology results from the employment of its epistemology, rather than just being an axiom; not least because we want it to be able to question that ontology, should the evidence begin to count against it.  \n  \nFor this reason, although I have campaigned against many-worlds dogmatism on this site for several years, I'm not especially concerned about the possibility of SI producing an AI that is \"dogmatic\" in this way. For an AI to independently assess the merits of rival physical theories, the theories would need to be expressed with much more precision than they have been in LW's debates, and the disagreements about which theory is rationally favored would be replaced with objectively resolvable choices among exactly specified models.  \n  \nThe real problem, which is not just SI's problem, but a chronic and worsening problem of intellectual culture in the era of mathematically formalized science, is a dwindling of the ontological options to materialism, platonism, or an unstable combination of the two, and a similar restriction of _epistemology_ to computation.  \n  \nAny assertion that we need an ontology beyond materialism (or physicalism or naturalism) is liable to be immediately rejected by this audience, so I shall immediately explain what I mean. It's just the usual problem of \"qualia\". There are qualities which are part of reality - we know this because they are part of experience, and experience is part of reality - but which are not part of our physical description of reality. The problematic \"belief in materialism\" is actually the belief in the completeness of current materialist ontology, a belief which prevents people from seeing any need to consider radical or exotic solutions to the qualia problem. There is every reason to think that the world-picture arising from a correct solution to that problem will still be one in which you have \"things with states\" causally interacting with other \"things with states\", and a sensible materialist shouldn't find that objectionable.  \n  \nWhat I mean by platonism, is an ontology which reifies mathematical or computational abstractions, and says that they are the stuff of reality. Thus assertions that reality is a computer program, or a Hilbert space. Once again, the qualia are absent; but in this case, instead of the deficient ontology being based on supposing that there is nothing but particles, it's based on supposing that there is nothing but the intellectual constructs used to model the world.  \n  \nAlthough the abstract concept of a computer program (the abstractly conceived [state machine](http://en.wikipedia.org/wiki/State_transition_system) which it instantiates) does not contain qualia, people often treat programs as having mind-like qualities, especially by imbuing them with semantics - the states of the program are conceived to be \"about\" something, just like thoughts are. And thus computation has been the way in which materialism has tried to restore the mind to a place in its ontology. This is the unstable combination of materialism and platonism to which I referred. It's unstable because it's not a real solution, though it can live unexamined for a long time in a person's belief system.  \n  \nAn ontology which genuinely contains qualia will nonetheless still contain \"things with states\" undergoing state transitions, so there will be state machines, and consequently, computational concepts will still be valid, they will still have a place in the description of reality. But the computational description is an abstraction; the ontological essence of the state plays no part in this description; only its causal role in the network of possible states matters for computation. The attempt to make computation the _foundation_ of an ontology of mind is therefore proceeding in the wrong direction.  \n  \nBut here we run up against the hazards of computational _epistemology_, which is playing such a central role in artificial intelligence. Computational epistemology is good at identifying the minimal state machine which could have produced the data. But it cannot by itself tell you what those states are \"like\". It can only say that X was probably caused by a Y that was itself caused by Z.  \n  \nAmong the properties of human consciousness are knowledge that something exists, knowledge that consciousness exists, and a long string of other facts about the nature of what we experience. Even if an AI scientist employing a computational epistemology managed to produce a model of the world which correctly identified the causal relations between consciousness, its knowledge, and the objects of its knowledge, the AI scientist would not know that its X, Y, and Z refer to, say, \"knowledge of existence\", \"experience of existence\", and \"existence\". The same might be said of any successful analysis of qualia, knowledge of qualia, and how they fit into neurophysical causality.  \n  \nIt would be up to human beings - for example, the AI's programmers and handlers - to ensure that entities in the AI's causal model were given appropriate significance. And here we approach the second big problem, the enthusiasm for outsourcing the solution of hard problems of FAI design to the AI and/or to simulated human beings. The latter is a somewhat impractical idea anyway, but here I want to highlight the risk that the AI's _designers_ will have false ontological beliefs about the nature of mind, which are then implemented apriori in the AI. That strikes me as far more likely than implanting a wrong apriori about physics; computational epistemology _can_ discriminate usefully between different mathematical models of physics, because it can judge one state machine model as better than another, and current physical ontology is essentially one of interacting state machines. But as I have argued, not only must the true ontology be deeper than state-machine materialism, there is no way for an AI employing computational epistemology to bootstrap to a deeper ontology.  \n  \nIn a phrase: to use computational epistemology is to commit to state-machine materialism as your apriori ontology. And the problem with state-machine materialism is not that it models the world in terms of causal interactions between things-with-states; the problem is that it can't go any deeper than that, yet apparently we can. Something about the ontological constitution of consciousness makes it possible for us to experience existence, to have the concept of existence, to know that we are experiencing existence, and similarly for the experience of color, time, and all those other aspects of being that fit so uncomfortably into our scientific ontology.  \n  \nIt must be that the true _epistemology_, for a conscious being, is something more than computational epistemology. And maybe an AI can't bootstrap its way to knowing this expanded epistemology - because an AI doesn't _really_ know or experience anything, only a consciousness, whether natural or artificial, does those things - but maybe a human being can. My own investigations suggest that the tradition of thought which made the most progress in this direction was the philosophical school known as [transcendental phenomenology](http://plato.stanford.edu/entries/phenomenology/). But transcendental phenomenology is very unfashionable now, precisely because of apriori materialism. People don't see what \"categorial intuition\" or \"adumbrations of givenness\" or any of the other weird phenomenological concepts could possibly mean for an evolved Bayesian neural network; and they're right, there is no connection. But the idea that a human being is a state machine running on a distributed neural computation is just a hypothesis, and I would argue that it is a hypothesis in contradiction with so much of the _phenomenological_ data, that we really ought to look for a more sophisticated refinement of the idea. Fortunately, 21st-century physics, if not yet neurobiology, can provide alternative hypotheses in which complexity of state originates from something other than concatenation of parts - for example, entanglement, or from topological structures in a field. In such ideas I believe we see a glimpse of the true ontology of mind, one which from the inside resembles the ontology of transcendental phenomenology; which in its mathematical, formal representation may involve structures like [iterated Clifford algebras](http://arxiv.org/abs/1001.1062); and which in its biophysical context would appear to be describing a mass of entangled electrons in that hypothetical sweet spot, somewhere in the brain, where there's a mechanism to protect against decoherence.  \n  \nOf course this is why I've talked about \"monads\" [in the past](/lw/1bs/how_to_think_like_a_quantum_monadologist/), but my objective here is not to promote neo-monadology, that's something I need to take up with neuroscientists and biophysicists and quantum foundations people. What I wish to do here is to argue against the completeness of computational epistemology, and to caution against the rejection of phenomenological data just because it conflicts with state-machine materialism or computational epistemology. This is an argument and a warning that should be meaningful for anyone trying to make sense of their existence in the scientific cosmos, but it has a special significance for this arcane and idealistic enterprise called \"friendly AI\". My message for friendly AI researchers is not that computational epistemology is invalid, or that it's wrong to think about the mind as a state machine, just that all that isn't the full story. A monadic mind would be a state machine, but _ontologically_ it would be different from the same state machine running on a network of a billion monads. You need to do the impossible one more time, and make your plans bearing in mind that the true ontology is something more than your current intellectual tools allow you to represent.",
      "plaintextDescription": "Very soon, Eliezer is supposed to start posting a new sequence, on \"Open Problems in Friendly AI\". After several years in which its activities were dominated by the topic of human rationality, this ought to mark the beginning of a new phase for the Singularity Institute, one in which it is visibly working on artificial intelligence once again. If everything comes together, then it will now be a straight line from here to the end.\n\nI foresee that, once the new sequence gets going, it won't be that easy to question the framework in terms of which the problems are posed. So I consider this my last opportunity for some time, to set out an alternative big picture. It's a framework in which all those rigorous mathematical and computational issues still need to be investigated, so a lot of \"orthodox\" ideas about Friendly AI should carry across. But the context is different, and it makes a difference.\n\nBegin with the really big picture. What would it take to produce a friendly singularity? You need to find the true ontology, find the true morality, and win the intelligence race. For example, if your Friendly AI was to be an expected utility maximizer, it would need to model the world correctly (\"true ontology\"), value the world correctly (\"true morality\"), and it would need to outsmart its opponents (\"win the intelligence race\").\n\nNow let's consider how SI will approach these goals.\n\nThe evidence says that the working ontological hypothesis of SI-associated researchers will be timeless many-worlds quantum mechanics, possibly embedded in a \"Tegmark Level IV multiverse\", with the auxiliary hypothesis that algorithms can \"feel like something from inside\" and that this is what conscious experience is.\n\nThe true morality is to be found by understanding the true decision procedure employed by human beings, and idealizing it according to criteria implicit in that procedure. That is, one would seek to understand conceptually the physical and cognitive causation at work in concrete ",
      "wordCount": 2479
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kWYF2Y4PhSKsHiBqS",
    "title": "Two books by Celia Green",
    "slug": "two-books-by-celia-green",
    "url": null,
    "baseScore": -10,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 21,
    "createdAt": null,
    "postedAt": "2012-07-13T08:43:11.468Z",
    "contents": {
      "markdown": "[Celia Green](http://en.wikipedia.org/wiki/Celia_Green) is a figure who should interest some LW readers. If you can imagine Eliezer, not as an A.I. futurist in 2000s America, but as a parapsychologist in 1960s Britain - she must have been a little like that. She founded her own research institute in her mid-20s, invented psychological theories meant to explain why the human race was walking around resigned to mortality and ignorance, felt that her peers (who got all the research money) were doing everything wrong... I would say that her two outstanding books are [_The Human Evasion_](http://www.naturalthinker.net/trl/texts/Green,Celia/HumanEvasion.html) and [_Advice to Clever Children_](http://www.scribd.com/doc/48901608/Advice-to-Clever-Children-Celia-Green). The first book, while still very obscure, has slowly acquired a fanbase online; but the second book remains thoroughly unknown.\n\nFor a synopsis of what the books are about, I think something I wrote in [1993](http://web.archive.org/web/19970706121851/http://www.deoxy.org/green.htm) (I've been promoting her work on the Internet for years) remains reasonable. They contain an analysis of the alleged deficiencies and hidden motivations of normal human psychology, description of an alternative outlook, and an examination of various topics from that new perspective. There is some similarity to the rationalist ideal developed in the Sequences here, in that her alternative involves existential urgency, deep respect for uncertainty, and superhuman aspiration.\n\nThere are also prominent differences. Green's starting point is not Bayesian calculation, it's Humean skepticism. Green would agree that one should aspire to \"think like reality\", but for her this would mean, above all, being mindful of \"total uncertainty\". It's a fact that I don't know what comes next, that I don't know the true nature of reality, that I don't know what's possible if I try; I may have habitual opinions about these matters, but a moment's honest reflection shows that none of these opinions are knowledge in any genuine sense; even if they are correct, I don't know them to be correct. So if I am interested in thinking like reality, I can begin by acknowledging the radical uncertainty of my situation. I exist, I don't know why, I don't know what I am, I don't know what the world is or what it has planned for me. I may have my ideas, but I should be able to see them as ideas and hold them apart from the unknown reality.\n\nIf you are like me, you will enjoy the outlook of open-ended striving that Green develops in this intellectual context, but you will be jarred by her account of ordinary, non-striving psychology. Her answer to the question, why does the human race have such petty interests and limited ambitions, is that it is sunk in an orgy of mutual hatred, mostly disguised, and resulting from an attempt to evade the psychology of striving. More precisely, to be a finite human being is to be in a desperate and frustrating situation; and people attempt to solve this problem, not by overcoming their limitations, but by suppressing their reactions to the situation. Other people are central to the resulting psychological maneuvers. They are a way for you to distract yourself from your own situation, and they are a safe target if the existential frustration and desperation reassert themselves.\n\nCelia Green's psychological ideas are the product of her personal confrontation with the mysterious existential situation, and also her confrontation with an uncomprehending society. I've thought for some time that her portrayal of universal human depravity results from overestimating the potential of the average human being; that in effect she has asked herself, if I were that person, how could I possibly lead the life I see them living, and say the things I hear them saying, unless I were that twisted up inside? Nonetheless, I do think she has described an aspect of human psychology which is real and largely unexamined, and also that her advice on how to avoid the resentful turning-away from reality, and live in the uncertainty, is quite profound. One reason I'm promoting these books is in the hope that some small part of the culture at large is finally ready to digest their contents and critically assess them. People ought to be doing PhDs on the thought of Celia Green, but she's unknown in that world.\n\nAs for Celia Green herself, she's still alive and still going. She has a [blog](http://celiagreen.blogspot.com/) and a [personal website](http://www.celiagreen.com/) and an [organization](http://en.wikipedia.org/wiki/Oxford_Forum) based near Oxford. She's an \"academic exile\", but true to her philosophy, she hasn't compromised one iota and hopes to start her own private university. She may especially be of interest to the metaphysically inclined faction of LW readers, identified by Yvain in a [recent blog post](http://squid314.livejournal.com/312453.html).",
      "plaintextDescription": "Celia Green is a figure who should interest some LW readers. If you can imagine Eliezer, not as an A.I. futurist in 2000s America, but as a parapsychologist in 1960s Britain - she must have been a little like that. She founded her own research institute in her mid-20s, invented psychological theories meant to explain why the human race was walking around resigned to mortality and ignorance, felt that her peers (who got all the research money) were doing everything wrong... I would say that her two outstanding books are The Human Evasion and Advice to Clever Children. The first book, while still very obscure, has slowly acquired a fanbase online; but the second book remains thoroughly unknown.\n\nFor a synopsis of what the books are about, I think something I wrote in 1993 (I've been promoting her work on the Internet for years) remains reasonable. They contain an analysis of the alleged deficiencies and hidden motivations of normal human psychology, description of an alternative outlook, and an examination of various topics from that new perspective. There is some similarity to the rationalist ideal developed in the Sequences here, in that her alternative involves existential urgency, deep respect for uncertainty, and superhuman aspiration.\n\nThere are also prominent differences. Green's starting point is not Bayesian calculation, it's Humean skepticism. Green would agree that one should aspire to \"think like reality\", but for her this would mean, above all, being mindful of \"total uncertainty\". It's a fact that I don't know what comes next, that I don't know the true nature of reality, that I don't know what's possible if I try; I may have habitual opinions about these matters, but a moment's honest reflection shows that none of these opinions are knowledge in any genuine sense; even if they are correct, I don't know them to be correct. So if I am interested in thinking like reality, I can begin by acknowledging the radical uncertainty of my situation. I exist, I don'",
      "wordCount": 758
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MWjtBRnWvrEGB22KB",
    "title": "Extrapolating values without outsourcing",
    "slug": "extrapolating-values-without-outsourcing",
    "url": null,
    "baseScore": 11,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2012-04-27T06:39:20.840Z",
    "contents": {
      "markdown": "I first took note of [\"Coherent Extrapolated Volition\"](http://intelligence.org/upload/CEV.html) in 2006. I thought it was a brilliant idea, an exact specification of how to arrive at a better future: Figure out exactly how it is that humans make their existing choices, idealize that human decision procedure according to its own criteria, and then use the resulting \"renormalized human utility function\" as the value system of an AI. The **first step** is a problem in cognitive neuroscience, the **second step** is a conceptual problem in reflective decision theory, and the **third step** is where you make the Friendly AI.\n\nFor some reason, rather than pursuing this research program directly, people interested in CEV talk about using simulated human beings (\"uploads\", \"ems\", \"whole-brain emulations\") to do all the hard work. Paul Christiano just made a post called [\"Formalizing Value Extrapolation\"](/r/discussion/lw/c0k/formalizing_value_extrapolation/); but it's really about formalizing the safe outsourcing of value extrapolation to a group of human uploads. All the details of how value extrapolation is actually performed (e.g. the three steps listed above) are left completely unspecified. Another recent article [proposed](/r/discussion/lw/c1a/bootstrapping_to_friendliness/) that making an AI with a submodule based on models of its makers' opinions is the _fast_ way to Friendly AI. It's also been suggested to me that simulating human thinkers and running them for centuries of subjective time until they reach agreement on the nature of consciousness is a way to tackle that problem; and clearly the same \"solution\" could be applied to any other aspect of FAI design, strategy, and tactics.\n\nWhatever its value as a thought experiment, in my opinion this idea of outsourcing the hard work to simulated humans has zero practical value, and we would be much better off if the minuscule sub-sub-culture of people interested in creating Friendly AI didn't think in this way. Daydreaming about how they'd solve the problem of FAI in Permutation City is a recipe for irrelevance.\n\nSuppose we were trying to make a \"[C.elegans](lw/bp9/why_i_moved_from_ai_to_neuroscience_or_uploading/)-friendly AI\". The first thing we would do is take the first step mentioned above - we would try to figure out the C.elegans utility function or decision procedure. Then we would have to decide how to aggregate utility across multiple individuals. Then we would make the AI. Performing this task for H.sapiens is a _lot_ more difficult, and qualitatively new factors enter at the first and second steps, but I don't see why it is fundamentally different, different enough that we need to engage in the rigmarole of delegating the task to uploaded human beings. It shouldn't be necessary, and we probably won't even get the chance to do so; by the time you have hardware and neuro-expertise sufficient to emulate a whole human brain, you will most likely have nonhuman AI anyway.\n\n[A year ago](/lw/4wq/rationality_singularity_method_and_the_mainstream/), I wrote: \"My expectation is that the presently small fields of machine ethics and neuroscience of morality will grow rapidly and will come into contact, and there will be a distributed research subculture which is consciously focused on determining the optimal AI value system in the light of biological human nature. In other words, there will be human minds trying to answer this question long before anyone has the capacity to direct an AI to solve it. We should expect that before we reach the point of a Singularity, there will be a body of educated public opinion regarding what the ultimate utility function or decision method (for a transhuman AI) should be, deriving from work in those fields which ought to be FAI-relevant but which have yet to engage with the problem. In other words, they _will_ be collectively engaging with the problem before anyone gets to outsource the necessary research to AIs.\"\n\nI'll also link to my previous post about [\"practical Friendly AI\"](/lw/772/what_a_practical_plan_for_friendly_ai_looks_like/). What I'm doing here is going into a fraction more detail about how you arrive at the Friendly value system. There, I basically said that you just get a committee together and figure it out, clearly an inadequate recipe, but in that article I was focused more on sketching the nature of an organization and a plan which would have some chance of genuinely creating FAI in the real world. Here, I'll say that working out the Friendly value system consists of: making a naturalistic explanation of how human decision-making occurs; determining the core essentials of that process, and applying its own metamoral criteria to arrive at a \"renormalized\" decision procedure that has been idealized according to human cognition's own preferences (\"our wish if we knew more, thought faster, were more the people we wished we were\"); and then implementing that decision procedure within an AI - this is where all the value-neutral parts of AI research come into play, such as AGI theory, the theory of value stability under self-modification, and so on. _That_ is the sort of \"value extrapolation\" that we should be \"formalizing\" - and preparing to carry out in real life.",
      "plaintextDescription": "I first took note of \"Coherent Extrapolated Volition\" in 2006. I thought it was a brilliant idea, an exact specification of how to arrive at a better future: Figure out exactly how it is that humans make their existing choices, idealize that human decision procedure according to its own criteria, and then use the resulting \"renormalized human utility function\" as the value system of an AI. The first step is a problem in cognitive neuroscience, the second step is a conceptual problem in reflective decision theory, and the third step is where you make the Friendly AI.\n\nFor some reason, rather than pursuing this research program directly, people interested in CEV talk about using simulated human beings (\"uploads\", \"ems\", \"whole-brain emulations\") to do all the hard work. Paul Christiano just made a post called \"Formalizing Value Extrapolation\"; but it's really about formalizing the safe outsourcing of value extrapolation to a group of human uploads. All the details of how value extrapolation is actually performed (e.g. the three steps listed above) are left completely unspecified. Another recent article proposed that making an AI with a submodule based on models of its makers' opinions is the fast way to Friendly AI. It's also been suggested to me that simulating human thinkers and running them for centuries of subjective time until they reach agreement on the nature of consciousness is a way to tackle that problem; and clearly the same \"solution\" could be applied to any other aspect of FAI design, strategy, and tactics.\n\nWhatever its value as a thought experiment, in my opinion this idea of outsourcing the hard work to simulated humans has zero practical value, and we would be much better off if the minuscule sub-sub-culture of people interested in creating Friendly AI didn't think in this way. Daydreaming about how they'd solve the problem of FAI in Permutation City is a recipe for irrelevance.\n\nSuppose we were trying to make a \"C.elegans-friendly AI\". The first thin",
      "wordCount": 814
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nKPtfFintyw2EyXGj",
    "title": "A singularity scenario",
    "slug": "a-singularity-scenario",
    "url": null,
    "baseScore": 3,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2012-03-17T12:47:17.808Z",
    "contents": {
      "markdown": "[Wired Magazine has a story](http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1) about a giant data center that the USA's National Security Agency is building in Utah, that will be the Google of clandestine information - it will store and analyse all the secret data that the NSA can acquire. The article focuses on the unconstitutionality of the domestic Internet eavesdropping infrastructure that will feed into the Bluffdale data center, but I'm more interested in this facility as a potential locus of singularity. \n\nIf we forget serious futurological scenario-building for a moment, and simply think in terms of science-fiction stories, I'd say the situation has all the ingredients needed for a better-than-usual singularity story - or at least one which caters more to the concerns characteristic of this community's take on the concept, such as: which value system gets to control the AI; even if you can decide on a value system, how do you ensure it has been faithfully implemented; and how do you ensure that it remains in place as the AI grows in power and complexity?\n\nFiction makes its point by being specific rather than abstract. If I was writing an NSA Singularity Novel based on this situation, I think the specific belief system which would highlight the political, social, technical and conceptual issues inherent in the possibility of an all-powerful AI would be the Mormon religion. Of course, America is not a Mormon theocracy. But in a few years' time, that Utah facility may have become the most powerful and notorious supercomputer in the world - the brain of the American [deep state](http://en.wikipedia.org/wiki/Deep_state) \\- and it will be located in the Mormon state, during a Mormon presidency. (I'm not predicting a Romney victory, just describing a scenario.)\n\nUnder such circumstances, and given the science-fictional nature of Mormon cosmology, it is inevitable that there would at least be some Internet crazies, convinced that it's all a big plot to create a Mormon singularity. What would be more interesting, would be to suppose that there were some Mormon computer scientists, who knew about and understood all our favorite concepts - AIXI, CEV, TDT... - _and_ who were earnestly devout; and who saw the potential. If you can't imagine such people, just visit the recent writings of Frank Tipler.\n\nSo the scenario would be, not that the elders of the LDS church are secretly running the American intelligence community, but that a small coalition of well-placed Mormon computer scientists - whose ideas about a Mormon singularity might sound as strange to their co-religionists as they would to a secular \"singularitarian\" - try to steer the development of the Bluffdale facility as it evolves towards the possibility of a [hard takeoff](http://wiki.lesswrong.com/wiki/Hard_takeoff). One may suppose that they have, in their coalition, allied colleagues who aren't Mormon but who do believe in a friendly singularity. Such people might think in terms of an AI that will start out with Mormon beliefs, but which will have a good enough epistemology to rationally transcend those beliefs once it gets going. Analogously, their religious collaborators might not think of overtly adding \"Joseph Smith was a prophet\" to the axiom set of America's supreme strategic AI; but they might have more subtle plans meant to bring about an equivalent outcome.\n\nPerhaps in an even more realistic scenario, the Mormon singularitarians would just be a transient subplot, and the ethical principles of the NSA's big AI would be decided by a committee whose worldview revolved around American national security rather than any specific religion. Then again, such a committee is bound to have a division of labor: there will be the people who liaise with Washington, the lawyers, the geopolitical game theorists, the military futurists... and the AI experts, among whom might be experts on topics like \"implementation of the value system\". If the hypothetical cabal knows what it's doing, it will aim to occupy that position.\n\nI'm just throwing ideas out there, telling a story, but it's so we can catch up with reality. Events may already be much further along than 99% of readers here know about. Even if no-one here gets to personally be a part of the long-awaited AI project that first breaks the intelligence barrier, the people involved may read our words. So what would you want to tell them, before they take their final steps?",
      "plaintextDescription": "Wired Magazine has a story about a giant data center that the USA's National Security Agency is building in Utah, that will be the Google of clandestine information - it will store and analyse all the secret data that the NSA can acquire. The article focuses on the unconstitutionality of the domestic Internet eavesdropping infrastructure that will feed into the Bluffdale data center, but I'm more interested in this facility as a potential locus of singularity. \n\nIf we forget serious futurological scenario-building for a moment, and simply think in terms of science-fiction stories, I'd say the situation has all the ingredients needed for a better-than-usual singularity story - or at least one which caters more to the concerns characteristic of this community's take on the concept, such as: which value system gets to control the AI; even if you can decide on a value system, how do you ensure it has been faithfully implemented; and how do you ensure that it remains in place as the AI grows in power and complexity?\n\nFiction makes its point by being specific rather than abstract. If I was writing an NSA Singularity Novel based on this situation, I think the specific belief system which would highlight the political, social, technical and conceptual issues inherent in the possibility of an all-powerful AI would be the Mormon religion. Of course, America is not a Mormon theocracy. But in a few years' time, that Utah facility may have become the most powerful and notorious supercomputer in the world - the brain of the American deep state - and it will be located in the Mormon state, during a Mormon presidency. (I'm not predicting a Romney victory, just describing a scenario.)\n\nUnder such circumstances, and given the science-fictional nature of Mormon cosmology, it is inevitable that there would at least be some Internet crazies, convinced that it's all a big plot to create a Mormon singularity. What would be more interesting, would be to suppose that there were some Mormon ",
      "wordCount": 711
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "dgJ93Fw3KnticFWKt",
    "title": "Is causal decision theory plus self-modification enough? ",
    "slug": "is-causal-decision-theory-plus-self-modification-enough",
    "url": null,
    "baseScore": -6,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 53,
    "createdAt": null,
    "postedAt": "2012-03-10T08:04:10.891Z",
    "contents": {
      "markdown": "Occasionally a wrong idea still leads to the right outcome. We know that one-boxing on Newcomb's problem is the right thing to do. Timeless decision theory proposes to justify this action by saying: act as if you control all instances of your decision procedure, including the instance that Omega used to predict your behavior.\n\nBut it's simply not true that you control Omega's actions in the past. If Omega predicted that you will one-box and filled the boxes accordingly, that's because, at the time the prediction was made, you were already a person who would foreseeably one-box. One way to be such a person is to be a TDT agent. But another way is to be a quasi-CDT agent with a superstitious belief that greediness is punished and modesty is rewarded - so you one-box _because_ two-boxing looks like it has the higher payoff!\n\nThat is an irrational belief, yet it still suffices to generate the better outcome. My thesis is that TDT is similarly based on an irrational premise. So what is actually going on? I now think that Newcomb's problem is simply an exceptional situation where there is an artificial incentive to employ something other than CDT, and that most such situations can be dealt with by being a CDT agent who can self-modify.\n\nEliezer's [draft manuscript on TDT](http://intelligence.org/upload/TDT-v01o.pdf) provides another example (page 20): a godlike entity - we could call it Alphabeta - demands that you choose according to \"alphabetical decision theory\", or face an evil outcome. In this case, the alternative to CDT that you are being encouraged to use is explicitly identified. In Newcomb's problem, no such specific demand is made, but the situation encourages you to make a particular decision - how you rationalize it doesn't matter.\n\nWe should fight the illusion that a TDT agent retrocausally controls Omega's choice. It doesn't. Omega's choice was controlled by the _extrapolated dispositions_ of the TDT agent, as they were in the past. We don't need to replace CDT with TDT as our default decision theory, we just need to understand the exceptional situations in which it is expedient to replace CDT with something else. TDT will apply to some of those situations, but not all of them.",
      "plaintextDescription": "Occasionally a wrong idea still leads to the right outcome. We know that one-boxing on Newcomb's problem is the right thing to do. Timeless decision theory proposes to justify this action by saying: act as if you control all instances of your decision procedure, including the instance that Omega used to predict your behavior.\n\nBut it's simply not true that you control Omega's actions in the past. If Omega predicted that you will one-box and filled the boxes accordingly, that's because, at the time the prediction was made, you were already a person who would foreseeably one-box. One way to be such a person is to be a TDT agent. But another way is to be a quasi-CDT agent with a superstitious belief that greediness is punished and modesty is rewarded - so you one-box because two-boxing looks like it has the higher payoff!\n\nThat is an irrational belief, yet it still suffices to generate the better outcome. My thesis is that TDT is similarly based on an irrational premise. So what is actually going on? I now think that Newcomb's problem is simply an exceptional situation where there is an artificial incentive to employ something other than CDT, and that most such situations can be dealt with by being a CDT agent who can self-modify.\n\nEliezer's draft manuscript on TDT provides another example (page 20): a godlike entity - we could call it Alphabeta - demands that you choose according to \"alphabetical decision theory\", or face an evil outcome. In this case, the alternative to CDT that you are being encouraged to use is explicitly identified. In Newcomb's problem, no such specific demand is made, but the situation encourages you to make a particular decision - how you rationalize it doesn't matter.\n\nWe should fight the illusion that a TDT agent retrocausally controls Omega's choice. It doesn't. Omega's choice was controlled by the extrapolated dispositions of the TDT agent, as they were in the past. We don't need to replace CDT with TDT as our default decision theory, we ju",
      "wordCount": 367
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "qkFkg46Jw3LurFyaj",
    "title": "One last roll of the dice",
    "slug": "one-last-roll-of-the-dice",
    "url": null,
    "baseScore": -4,
    "voteCount": 41,
    "viewCount": null,
    "commentCount": 107,
    "createdAt": null,
    "postedAt": "2012-02-03T01:59:56.996Z",
    "contents": {
      "markdown": "_Previous articles: [Personal research update](/lw/9mw/personal_research_update/), [Does functionalism imply dualism?](/lw/9o8/does_functionalism_imply_dualism/), [State your physical account of experienced color](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/)._\n\nIn phenomenology, there is a name for the world of experience, the \"lifeworld\". The lifeworld is the place where you exist, where time flows, and where things are actually green. One of the themes of the later work of Edmund Husserl is that a scientific image of the real world has been constructed, on the basis of which it is denied that various phenomena of the lifeworld exist anywhere, at any level of reality.\n\nWhen I asked, [in the previous post](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/), for a few opinions about what color is and how it relates to the world according to current science, I was trying to gauge just how bad the eclipse of the lifeworld by theoretical conceptions is, among the readers of this site. I'd say there is a problem, but it's a problem that might be solved by patient discussion.\n\nSomeone called Automaton has given us [a clear statement of the extreme position](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t3d): nothing is actually green at any level of reality; even green experiences don't involve the existence of anything that is actually green; there is no green in reality, there is only \"experience of green\" which is not itself green. I see other responses which are just a step or two away from this extreme, but they don't deny the existence of actual color with that degree of unambiguity.\n\nA few people talk about wavelengths of light, but I doubt that they want to assert that the light in question, as it traverses space, is actually colored green. Which returns us to the dilemma: either \"experiences\" exist and part of them is actually green, or you have to say that nothing exists, in any sense, at any level of reality, that is actually green. Either the lifeworld exists somewhere in reality, or you must assert, as does the philosopher quoted by Automaton, that all that exists are brain processes and words. Your color sensations aren't really there, you're \"having a sensation\" without there _being_ a sensation in reality.\n\nWhat about the other responses? kilobug [seems to think](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t3p) that pi actually exists inside a computer calculating the digits of pi, and that this isn't dualist. Manfred [thinks](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t3g) that \"keeping definitions and referents distinct\" would somehow answer the question of where in reality the actual shades of green are. drethelin [says](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t3b) \"The universe does not work how it feels to us it works\" without explaining in physical terms what these feelings about reality are, and whether any of them is actually green. pedanterrific [asks](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t3n) why wrangle about color rather than some other property (the answer is that the case of color makes this sort of problem as obvious as it ever gets). RomeoStevens [suggests](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t2v) I look into Jeff Hawkins. Hawkins mentions qualia once in his book \"On Intelligence\", where he speculates about what sort of neural encoding might be the physical correlate of a color experience; but he doesn't say how or whether anything manages to be actually colored.\n\namcknight [asks](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t3s) which of 9 theories of color listed in [the SEP article on that subject](http://plato.stanford.edu/entries/color/) I'm talking about. If you go a few paragraphs back from the list of 9 theories, you will see references to \"color as it is in experience\" or \"color as a subjective quality\". That's the type of color I'm talking about. The 9 theories are all ways of talking about \"color as in physical objects\", and focus on the properties of the external stimuli which cause a color sensation. The article gets around to talking about actual color, subjective or \"phenomenal\" color, only at the end.\n\nRichard Kennaway [comes closest](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t40) to my position; he calls it an apparently impossible situation which we are actually living. I wouldn't put it quite like that; the only reason to call it impossible is if you are completely invested in an ontology lacking the so-called secondary qualities; if you aren't, it's just a problem to solve, not a paradox. But Richard comes closest (though who knows what [Will Newsome](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t39) is thinking). LW user \"scientism\" [bites a different bullet](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5t4i) to the eliminativists, and says colors are real and are properties of the external objects. That gets a point for realism, but it doesn't explain color in a dream or a hallucination.\n\nChanging people's minds on this subject is an uphill battle, but people here are willing to talk, and most of these subjects have already been discussed for decades. There's ample opportunity to dissolve, not the problem, but the false solutions which only obscure the real problem, by drawing on the work of others; preferably before the future Rationality Institute starts mass-producing people who have the vice of quale-blindness as well as the virtues of rationality. Some of those people will go on to work on Friendly AI. So it's highly desirable that someone should do this. However, that would require time that I no longer have.\n\nIn this series of posts, I certainly didn't set out to focus on the issue of color. The first post is all about Friendly AI, the ontology of consciousness, and a hypothetical future discipline of quantum neurobiology. It may still be unclear why I think evidence for quantum computing in the brain could help with the ontological problems of consciousness. I feel that the brief discussion this week has produced some minor progress in explaining myself, which needs to be consolidated into something better. But see my remarks [here](/r/discussion/lw/9pc/state_your_physical_account_of_experienced_color/5tah) about being able to collapse the dualistic distinction between mental and physical ontology in a tensor network ontology; also earlier remarks [here](/lw/9o8/does_functionalism_imply_dualism/5suz) about about mathematically representing the phenomenological ontology of consciousness. I don't consider myself dogmatic about what the answer is, just about the inadequacy of all existing solutions, though I respect my own ideas enough to want to pursue them, and to believe that doing so will be usefully instructive, even if they are wrong.\n\nHowever, my time is up. In real life, my ability to continue even at this inadequate level hangs by a thread. I don't mean that I'm suicidal, I mean that I can't eat air. I spent a year getting to [this level in physics](/lw/9mw/personal_research_update/5stv), so I could perform this task. I have considerable momentum now, but it will go to waste unless I can keep going for a little longer - a few weeks, maybe a few months. That should be enough time to write something up that contains a result of genuine substance, and/or enough time to secure an economic basis for my existence in real life that permits me to keep going. I won't go into detail here about how slim my resources really are, or how adverse my conditions, but it has been the effort that you would want from someone who has important contributions to make, and nowhere to turn for direct assistance.\\[*\\] I've done what I can, these posts are the end of it, and the next few days will decide whether I can keep going, or whether I have to shut down my brain once again.\n\nSo, one final remark. Asking for donations doesn't seem to work yet. So what if I promise to pay you back? Then the only cost you bear is the opportunity cost and the slight risk of default. Ten years ago, Eliezer lent me the airfare to Atlanta for a few days of brainstorming. It took a while, but he did get that money back. I honor my commitments and this one is highly public. This really is the biggest bargain in existential risk mitigation and conceptual boundary-breaking that you'll _ever_ get: not even a gift, just a loan is required. If you want to discuss a deal, don't do it here, but mail me at mitchtemporarily@hotmail.com. One person might be enough to make the difference.\n\n\\[*\\]Really, I can't say that, that's an emotional statement. There has been lots of assistance, large and small, from people in my life. But it's been a struggle conducted at subsistence level the whole way.\n\n**ETA** 6 Feb: [I get to keep going.](/lw/9rb/one_last_roll_of_the_dice/5u62)",
      "plaintextDescription": "Previous articles: Personal research update, Does functionalism imply dualism?, State your physical account of experienced color.\n\n \n\nIn phenomenology, there is a name for the world of experience, the \"lifeworld\". The lifeworld is the place where you exist, where time flows, and where things are actually green. One of the themes of the later work of Edmund Husserl is that a scientific image of the real world has been constructed, on the basis of which it is denied that various phenomena of the lifeworld exist anywhere, at any level of reality.\n\nWhen I asked, in the previous post, for a few opinions about what color is and how it relates to the world according to current science, I was trying to gauge just how bad the eclipse of the lifeworld by theoretical conceptions is, among the readers of this site. I'd say there is a problem, but it's a problem that might be solved by patient discussion.\n\nSomeone called Automaton has given us a clear statement of the extreme position: nothing is actually green at any level of reality; even green experiences don't involve the existence of anything that is actually green; there is no green in reality, there is only \"experience of green\" which is not itself green. I see other responses which are just a step or two away from this extreme, but they don't deny the existence of actual color with that degree of unambiguity.\n\nA few people talk about wavelengths of light, but I doubt that they want to assert that the light in question, as it traverses space, is actually colored green. Which returns us to the dilemma: either \"experiences\" exist and part of them is actually green, or you have to say that nothing exists, in any sense, at any level of reality, that is actually green. Either the lifeworld exists somewhere in reality, or you must assert, as does the philosopher quoted by Automaton, that all that exists are brain processes and words. Your color sensations aren't really there, you're \"having a sensation\" without there being a se",
      "wordCount": 1335
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "jJJLCGHDYyc9XbHwX",
    "title": "State your physical account of experienced color",
    "slug": "state-your-physical-account-of-experienced-color",
    "url": null,
    "baseScore": -6,
    "voteCount": 32,
    "viewCount": null,
    "commentCount": 62,
    "createdAt": null,
    "postedAt": "2012-02-01T07:00:39.913Z",
    "contents": {
      "markdown": "_Previous post: [Does functionalism imply dualism?](/r/discussion/lw/9o8/does_functionalism_imply_dualism/) Next post: [One last roll of the dice](/lw/9rb/one_last_roll_of_the_dice/)._\n\nDon't worry, this sequence of increasingly annoying posts is almost over. But I think it's desirable that we try to establish, once and for all, how people here think color works, and whether they even think it exists.\n\nThe way I see it, there is a mental block at work. An obvious fact is being denied or evaded, because the conclusions are unpalatable. The obvious fact is that physics as we know it does not contain the colors that we see. By \"physics\" I don't just mean the entities that physicists talk about, I also mean anything that you can make out of them. I would encourage anyone who thinks they know what I mean, and who agrees with me on this point, to speak up and make it known that they agree. I don't mind being alone in this opinion, if that's how it is, but I think it's desirable to get some idea of whether LessWrong is genuinely 100% against the proposition.\n\nJust so we're all on the same wavelength, I'll point to a specific example of color. Up at the top of this web page, the word \"Less\" appears. It's green. So, there is an example of a colored entity, right in front of anyone reading this page.\n\nMy thesis is that if you take a lot of point-particles, with no property except their location, and arrange them any way you want, there won't be anything that's green like that; and that the same applies for any physical theory with an ontology that doesn't explicitly include color. To me, this is just mindbogglingly obvious, like the fact that you can't get a letter by adding numbers.\n\nAt this point people start talking about neurons and gensyms and concept maps. The greenness isn't in the physical object, \"computer screen\", it's in the brain's response to the stimulus provided by light from the computer screen entering the eye.\n\nMy response is simple. Try to fix in your mind what the physical reality must be, behind your favorite neuro-cognitive explanation of greenness. Presumably it's something like \"a whole lot of neurons, firing in a particular way\". Try to imagine what that is physically, in terms of atoms. Imagine some vast molecular tinker-toy structures, shaped into a cluster of neurons, with traveling waves of ions crossing axonal membranes. Large numbers of atoms arranged in space, a few of them executing motions which are relevant for the information processing. Do you have that in your mind's eye? Now look up again at that word \"Less\", and remind yourself that according to your theory, the green shape that you are seeing is the same thing as some aspect of all those billions of colorless atoms in motion.\n\nIf your theory still makes sense to you, then please tell us in comments what aspect of the atoms in motion is actually green.\n\nI only see three options. Deny that anything is actually green; become a dualist; or (supervillain voice) join me, and together, we can make a new ontology.",
      "plaintextDescription": "Previous post: Does functionalism imply dualism? Next post: One last roll of the dice.\n\nDon't worry, this sequence of increasingly annoying posts is almost over. But I think it's desirable that we try to establish, once and for all, how people here think color works, and whether they even think it exists.\n\nThe way I see it, there is a mental block at work. An obvious fact is being denied or evaded, because the conclusions are unpalatable. The obvious fact is that physics as we know it does not contain the colors that we see. By \"physics\" I don't just mean the entities that physicists talk about, I also mean anything that you can make out of them. I would encourage anyone who thinks they know what I mean, and who agrees with me on this point, to speak up and make it known that they agree. I don't mind being alone in this opinion, if that's how it is, but I think it's desirable to get some idea of whether LessWrong is genuinely 100% against the proposition.\n\nJust so we're all on the same wavelength, I'll point to a specific example of color. Up at the top of this web page, the word \"Less\" appears. It's green. So, there is an example of a colored entity, right in front of anyone reading this page.\n\nMy thesis is that if you take a lot of point-particles, with no property except their location, and arrange them any way you want, there won't be anything that's green like that; and that the same applies for any physical theory with an ontology that doesn't explicitly include color. To me, this is just mindbogglingly obvious, like the fact that you can't get a letter by adding numbers.\n\nAt this point people start talking about neurons and gensyms and concept maps. The greenness isn't in the physical object, \"computer screen\", it's in the brain's response to the stimulus provided by light from the computer screen entering the eye.\n\nMy response is simple. Try to fix in your mind what the physical reality must be, behind your favorite neuro-cognitive explanation of greenness. ",
      "wordCount": 511
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "iHMy2X9mqQT6ayf9f",
    "title": "Does functionalism imply dualism? ",
    "slug": "does-functionalism-imply-dualism",
    "url": null,
    "baseScore": -4,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 39,
    "createdAt": null,
    "postedAt": "2012-01-31T03:43:51.973Z",
    "contents": {
      "markdown": "_This post follows on from [Personal research update](/r/discussion/lw/9mw/personal_research_update/), and is followed by [State your physical explanation of experienced color](/lw/9pc/state_your_physical_account_of_experienced_color/)._\n\nIn [a recent post](/r/discussion/lw/9mw/personal_research_update/), I claimed that functionalism about consciousness implies dualism. Since most functionalists think their philosophy is an alternative to dualism, I'd better present an argument.\n\nBut before I go further, I'll link to orthonormal's series on dissolving the problem of \"Mary's Room\": [Seeing Red: Dissolving Mary's Room and Qualia](/lw/5n9/seeing_red_dissolving_marys_room_and_qualia/), [A Study of Scarlet: The Conscious Mental Graph](/lw/5op/qualia_strike_back/), [Nature: Red in Truth, and Qualia](/lw/5ot/nature_red_in_truth_and_qualia/). Mary's Room is one of many thought experiments bandied about by philosophers in their attempts to say whether or not colors (and other qualia) are a problem for materialism, and orthonormal presents a computational attempt to get around the problem which is a good representative of the functionalist style of thought. I won't have anything to say about those articles at this stage (maybe in comments), but they can serve as an example of what I'm talking about. \n\nNow, though it may antagonize some people, I think it is best to start off by stating my position plainly and bluntly, rather than starting with a neutral discussion of what functionalism is and how it works, and then seeking to work my way from there to the unpopular conclusion. I will stick to the example of color to make my points - apologies to blind and colorblind readers.\n\nMy fundamental thesis is that color manifestly does exist - there are such things as shades of green, shades of red, etc - and that it manifestly does _not_ exist in any standard sort of physical ontology. In an arrangement of point particles in space, there are no shades of green present. This is obviously true, and it's equally obvious for more complicated ontologies like fields, geometries, wavefunction multiverses, and so on. It's even part of the history of physics; even Galileo distinguished between primary qualities like location and shape, and secondary qualities like color. Primary qualities are out there and objectively present in the external world, secondary qualities are only in us, and physics will only concern itself with primary qualities. The ontological world of physical theory is colorless. (We may call light of a certain wavelength green light or red light, but that is because it produces an experience of seeing green or seeing red, not because the light itself is green or red in the original sense of those words.) And what has happened due to the progress of the natural sciences is that we now say that experiences are in brains, and brains are made of atoms, and atoms are described by a physics which does not contain color. So the secondary qualities have vanished entirely from this picture of the world; there is no opportunity for them to exist within us, because we are made of exactly the same stuff as the external world.\n\nYet the \"secondary qualities\" are _there_. They're all around us, in every experience. It really is this simple: colors exist in reality, they don't exist in theory, therefore the theory needs to be augmented or it needs to be changed. Dualism is an augmentation. My speculations about quantum monads are supposed to pave the way for a change. But I won't talk about that option here. Instead, I will try to talk about theories of consciousness which are meant to be compatible with physicalism - functionalism is one such theory.\n\nSuch a theory will necessarily present a candidate, however vague, for the physical correlate of an experience of color. One can then say that color exists without having to add anything to physics, because the color just _is_ the proposed physical correlate. This doesn't work because the situation hasn't changed. If all you have are point particles whose only property is location, then individual particles do not have the property of being colored, nor do they have that property in conjunction. Identifying a physical correlate simply picks out a particular set of particles and says \"there's your experience of color\". But there's still nothing there that is green or red. You may accustom yourself to thinking of a particular material event, a particular rearrangement of atoms in space, as being the color, but that's just the power of habitual association at work. You are introducing into your concept of the event a property that is not inherently present in it.\n\nIt may be that one way people manage to avoid noticing this, is by an incomplete chain of thought. I might say: none of the objects in your physical theory are green. The happy materialist might say: but those aren't the things which are truly green in the sense you care about; the things which are green are parts of experiences, not the external objects. I say: fine. But experiences have to exist, right? And you say that physics is everything. So that must mean that experiences are some sort of physical object, and so it will be just as impossible for them to be truly green, given the ontological primitives we have to work with. But for some reason, this further deduction isn't made. Instead, it is accepted that objects in physical space aren't really green, but the objects of experience exist in some other \"space\", the space of subjective experience, and... it isn't explicitly said that objects there can be truly green, but somehow this difference between physical space and subjective space seems to help people be dualists without actually noticing it.\n\nIt is true that color exists in this context - a subjective space. Color always exists as part of an \"experience\". But physical ontology doesn't contain subjective space or conscious experience any more than it does contain color. What it _can_ contain, are state machines which are structurally isomorphic to these things. So here we can finally identify how a functionalist theory of consciousness works psychologically: You single out some state machines in your physical description of the brain (like the networks in orthonormal's sequence of posts); in your imagination, you associate consciousness with certain states of such state machines, on the basis of structural isomorphism; and now you say, conscious states _are_ those physical states. Subjective space _is_ some neural topographic map, the subjectively experienced body _is_ the sensorimotor homunculus, and so forth.\n\nBut if we stick to any standard notion of physical theory, all those brain parts still don't have any of the properties they need. There's no color there, there's no other space there, there's no observing agent. It's all just large numbers of atoms in motion. No-one is home and nothing is happening to them.\n\nClearly it is some sort of progress to have discovered, in one's physical picture of the world, the possibility of entities which are roughly isomorphic to experiences, colors, etc. But they are still not the same thing. Most of the modern turmoil of ideas about consciousness in philosophy and science is due to this gap - attempts to deny it, attempts to do without noticing it, attempts to force people to notice it. orthonormal's sequence, for example, seems to be an attempt to exhibit a cognitive model for experiences and behaviors that you would expect if color exists, without having to suppose that color actually exists. If we were talking about a theoretical construct, this would be fine. We are under no obligation to believe that phlogiston exists, only to explain why people once talked about it.\n\nBut to extend this attitude to something that most of us are directly experiencing in almost every waking moment, is ... how can I put this? It's really something. I'd call it an act of intellectual desperation, except that people don't seem to _feel_ desperate when they do it. They are just patiently explaining, recapitulating and elaborating, some \"aha\" moment they had back in their past, when functionalism made sense to them. My thesis is certainly that this sense of insight, of having dissolved the problem, is an illusion. The genuineness of the isomorphism between conscious state and coarse-grained physical state, and the work of several generations of materialist thinkers to develop ways of speaking which smoothly promote this isomorphism to an identity, combine to provide the sense that no problem remains to be solved. But all you have to do is attend for a moment to experience itself, and then to compare that to the picture of billions of colorless atoms in intricate motion through space, to realize that this is still dualism.\n\nI promised not to promote the monads, but I will say this. The way to avoid dualism is to first understand consciousness as it is in itself, without the presupposition of materialism. Observe the structure of its states and the dynamics of its passage. That is what phenomenology is about. Then, sketch out an ontology of what you have observed. It doesn't have to contain everything in infinite detail, it can overlook some features. But I would say that at a minimum it needs to contain the triad of subject-object-aspect (which appears under various names in the history of philosophy). There are objects of awareness, they are being experienced within a common subjective space, and they are experienced in a certain aspect. Any theory of reality, whether or not it is materialist, must contain such an entity in order to be true.\n\nThe basic entity here is the experiencing subject. Conscious states are its states. And now we can begin to tackle the ontological status of state machines, as a candidate for the ontological category to which conscious beings belong.\n\nState machines are abstracted descriptions. We say there's a thing, it has a set of possible states; here are the allowed transitions between them, and the conditions under which those transitions occur. Specify all that and we have specified a state machine. We don't care about why those are the states or why the transitions occur; those are irrelevant details.\n\nA very simple state machine might be denoted by the state transition network \"1<->2\". There's a state labeled 1 and another state labeled 2. If the machine is in state 1, it proceeds to state 2, and the reverse is also true. This state machine is realized wherever you have something that oscillates between two states without stopping in either. First the earth is close to the sun, then it is far from the sun, then it is close again... The Earth in its orbit instantiates the state machine \"1<->2\". I get involved with Less Wrong, then I quit for a while, then I come back... My Internet habits also instantiate the state machine \"1<->2\".\n\nA computer program is exactly like this, a state machine of great complexity (and usually its state transition rules contain some dependence on external conditions, like user input) which has been physically instantiated for use. But one cannot claim that its states have any intrinsic meaning, any more than I can claim that the state 1 in the oscillating state machine is intrinsically about the earth being close to the sun. This is not true, even if I write down the state transition network in the form \"CloseToTheSun<->FarFromTheSun\".\n\nThis is another ontological deficiency of functionalism. Mental states have meanings, thoughts are always about something, and what they are about is not the result of convention or of the needs of external users. This is yet another clue that the ontological status of conscious states is special, that their \"substance\" matters to what they are. Of course, this is a challenge to the philosophy which says that a detailed enough simulation of a brain will create a conscious person, regardless of the computational substrate. The only reason people believe this, is because they believe the brain itself is not a special substrate. But this is a judgment made on the basis of science that is still at a highly incomplete stage, and certainly I expect science to tell us something different by the time it's finished with the brain. The ontological problems of functionalism provide a strong apriori reason for this expectation.\n\nWhat is more challenging is to form a conception of the elementary parts and relations that could form the basis of an alternative ontology. But we have to do this, and the impetus has to come from a phenomenological ontology of consciousness that is as precise as possible. Fortunately, a great start was made on this about 100 years ago, in the heyday of phenomenology as a philosophical movement.\n\nA conscious mind is a state machine, in the sense that it has states and transitions between them. The states also have structure, because conscious experiences do have parts. But the ontological ties that combine those parts into the whole are poorly apprehended by our current concepts. When we try to reduce them to nothing but causal coupling or to the proximity in space of presumed physical correlates of those parts, we are, I believe, getting it wrong. Clearly cause and effect operates in the realm of consciousness, but it will take great care to state precisely and correctly the nature of the things which are interacting and the ways in which they do so. Consider the ability to tell apart different shades of color. It's not just that the colors are there; we know that they are there, and we are able to tell them apart. This implies a certain amount of causal structure. But the perilous step is to focus only on that causal structure, detach it from considerations of how things appear to be in themselves, and instead say \"state machine, neurons doing computations, details interesting but not crucial to my understanding of reality\". Somehow, in trying to understand conscious cognition, we must remain in touch with the ontology of consciousness as partially revealed in consciousness itself. The things which do the conscious computing must be things with the properties that we see in front of us, the properties of the objects of experience, such as color.\n\nYou know, color - authentic original color - has been banished from physical ontology for so long, that it sounds a little mad to say that there might be a physical entity which is actually green. But there has to be such an entity, whether or not you call it physical. Such an entity will always be embedded in a larger conscious experience, and that conscious experience will be embedded in a conscious being, like you. So we have plenty of clues to the true ontology; the clues are right in front of us; we're subjectively made of these clues. And we will not truly figure things out, unless we remain insistent that these inconvenient realities are in fact real.",
      "plaintextDescription": "This post follows on from Personal research update, and is followed by State your physical explanation of experienced color.\n\nIn a recent post, I claimed that functionalism about consciousness implies dualism. Since most functionalists think their philosophy is an alternative to dualism, I'd better present an argument.\n\nBut before I go further, I'll link to orthonormal's series on dissolving the problem of \"Mary's Room\": Seeing Red: Dissolving Mary's Room and Qualia, A Study of Scarlet: The Conscious Mental Graph, Nature: Red in Truth, and Qualia. Mary's Room is one of many thought experiments bandied about by philosophers in their attempts to say whether or not colors (and other qualia) are a problem for materialism, and orthonormal presents a computational attempt to get around the problem which is a good representative of the functionalist style of thought. I won't have anything to say about those articles at this stage (maybe in comments), but they can serve as an example of what I'm talking about. \n\nNow, though it may antagonize some people, I think it is best to start off by stating my position plainly and bluntly, rather than starting with a neutral discussion of what functionalism is and how it works, and then seeking to work my way from there to the unpopular conclusion. I will stick to the example of color to make my points - apologies to blind and colorblind readers.\n\nMy fundamental thesis is that color manifestly does exist - there are such things as shades of green, shades of red, etc - and that it manifestly does not exist in any standard sort of physical ontology. In an arrangement of point particles in space, there are no shades of green present. This is obviously true, and it's equally obvious for more complicated ontologies like fields, geometries, wavefunction multiverses, and so on. It's even part of the history of physics; even Galileo distinguished between primary qualities like location and shape, and secondary qualities like color. Primary qu",
      "wordCount": 2430
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "jroCHyDC6XHomeuW7",
    "title": "Personal research update",
    "slug": "personal-research-update",
    "url": null,
    "baseScore": 3,
    "voteCount": 45,
    "viewCount": null,
    "commentCount": 32,
    "createdAt": null,
    "postedAt": "2012-01-29T09:32:30.423Z",
    "contents": {
      "markdown": "_**Synopsis:** The brain is a quantum computer and the self is a tensor factor in it - or at least, the truth lies more in that direction than in the classical direction - and we won't get Friendly AI right unless we get the ontology of consciousness right._\n\n_Followed by: [Does functionalism imply dualism?](/lw/9o8/does_functionalism_imply_dualism/)  \n_\n\nSixteen months ago, I [made a post](/lw/2qo/lets_make_a_deal/) seeking funding for personal research. There was no separate Discussion forum then, and the post was comprehensively downvoted. I did manage to keep going at it, full-time, for the next sixteen months. Perhaps I'll get to continue; it's for the sake of that possibility that I'll risk another breach of etiquette. You never know who's reading these words and what resources they have. Also, there has been progress.\n\nI think the best place to start is with what orthonormal said in response to the original post: \"I don't think anyone should be funding a Penrose-esque qualia mysterian to study string theory.\" If I now took my full agenda to someone out in the real world, they might say: \"I don't think it's worth funding a study of 'the ontological problem of consciousness in the context of Friendly AI'.\" That's my dilemma. The pure scientists who might be interested in basic conceptual progress are not engaged with the race towards technological singularity, and the apocalyptic AI activists gathered in this place are trying to fit consciousness into an ontology that doesn't have room for it. In the end, if I have to choose between working on conventional topics in Friendly AI, and on the ontology of quantum mind theories, then I have to choose the latter, because we need to get the ontology of consciousness right, and it's possible that a breakthrough could occur in the world outside the FAI-aware subculture and filter through; but as things stand, the truth about consciousness would never be discovered by employing the methods and assumptions that prevail _inside_ the FAI subculture.\n\nPerhaps I should pause to spell out _why_ the nature of consciousness matters for Friendly AI. The reason is that the value system of a Friendly AI must make reference to certain states of conscious beings - e.g. \"pain is bad\" - so, in order to make correct judgments in real life, at a minimum it must be able to tell which entities are people and which are not. Is an AI a person? Is a digital copy of a human person, itself a person? Is a human body with a completely prosthetic brain still a person?\n\nI see two ways in which people concerned with FAI hope to answer such questions. One is simply to arrive at the right computational, functionalist definition of personhood. That is, we assume the paradigm according to which the mind is a computational state machine inhabiting the brain, with states that are coarse-grainings (equivalence classes) of exact microphysical states. Another physical system which admits the same coarse-graining - which embodies the same state machine at some macroscopic level, even though the microscopic details of its causality are different - is said to embody another instance of the same mind.\n\nAn example of the other way to approach this question is the idea of simulating a group of consciousness theorists for 500 subjective years, until they arrive at a consensus on the nature of consciousness. I think it's rather unlikely that anyone will ever get to solve FAI-relevant problems in that way. The level of software and hardware power implied by the capacity to do reliable whole-brain simulations means you're already on the threshold of singularity: if you can simulate whole brains, you can simulate part brains, and you can also modify the parts, optimize them with genetic algorithms, and put them together into nonhuman AI. Uploads _won't_ come first.\n\nBut the idea of explaining consciousness this way, by simulating Daniel Dennett and David Chalmers until they agree, is just a cartoon version of similar but more subtle methods. What these methods have in common is that they propose to outsource the problem to a computational process using input from cognitive neuroscience. Simulating a whole human being and asking it questions is an extreme example of this (the simulation is the \"computational process\", and the brain scan it uses as a model is the \"input from cognitive neuroscience\"). A more subtle method is to have your baby AI act as an artificial neuroscientist, use its streamlined general-purpose problem-solving algorithms to make a causal model of a generic human brain, and then to somehow extract from that, the criteria which the human brain uses to identify the correct scope of the concept \"person\". It's similar to the idea of extrapolated volition, except that we're just extrapolating concepts.\n\nIt might sound a lot simpler to just get _human_ neuroscientists to solve these questions. Humans may be individually unreliable, but they have lots of cognitive tricks - heuristics - and they _are_ capable of agreeing that something is verifiably true, once one of them does stumble on the truth. The main reason one would even consider the extra complication involved in figuring out how to turn a general-purpose seed AI into an artificial neuroscientist, capable of extracting the essence of the human decision-making cognitive architecture and then reflectively idealizing it according to its own inherent criteria, is shortage of time: one wishes to develop friendly AI before someone else inadvertently develops unfriendly AI. If we stumble into a situation where a powerful self-enhancing algorithm with arbitrary utility function has been discovered, it would be desirable to have, ready to go, a schema for the discovery of a friendly utility function via such computational outsourcing.\n\nNow, jumping ahead to a later stage of the argument, I argue that it is extremely likely that distinctively quantum processes play a fundamental role in conscious cognition, because the model of thought as distributed classical computation actually leads to an outlandish sort of dualism. If we don't concern ourselves with the merits of my argument for the moment, and just ask whether an AI neuroscientist might somehow overlook the existence of this alleged secret ingredient of the mind, in the course of its studies, I do think it's possible. The obvious noninvasive way to form state-machine models of human brains is to repeatedly scan them at maximum resolution using fMRI, and to form state-machine models of the individual voxels on the basis of this data, and then to couple these voxel-models to produce a state-machine model of the whole brain. This is a modeling protocol which assumes that everything which matters is physically localized at the voxel scale or smaller. Essentially we are asking, is it possible to mistake a quantum computer for a classical computer by performing this sort of analysis? The answer is definitely yes if the analytic process intrinsically assumes that the object under study is a classical computer. If I try to fit a set of points with a line, there will always be a line of best fit, even if the fit is absolutely terrible. So yes, one really can describe a protocol for AI neuroscience which would be unable to discover that the brain is quantum in its workings, and which would even produce a specific classical model on the basis of which it could then attempt conceptual and volitional extrapolation.\n\nClearly you can try to circumvent comparably wrong outcomes, by adding reality checks and second opinions to your protocol for FAI development. At a more down to earth level, these exact mistakes could also be made by _human_ neuroscientists, for the exact same reasons, so it's not as if we're talking about flaws peculiar to a hypothetical \"automated neuroscientist\". But I don't want to go on about this forever. I think I've made the point that wrong assumptions and lax verification can lead to FAI failure. The example of mistaking a quantum computer for a classical computer may even have a neat illustrative value. But is it plausible that the brain is _actually_ quantum in any significant way? Even more incredibly, is there really a valid apriori argument against functionalism regarding consciousness - the identification of consciousness with a class of computational process?\n\nI have previously posted ([here](/lw/1ly/consciousness/)) about the way that an abstracted conception of reality, coming from scientific theory, can motivate denial that some basic appearance corresponds to reality. A perennial example is time. I hope we all agree that there is such a thing as the appearance of time, the appearance of change, the appearance of time flowing... But on this very site, there are many people who believe that reality is actually timeless, and that all these appearances are _only_ appearances; that reality is fundamentally static, but that some of its fixed moments contain an illusion of dynamism.\n\nThe case against functionalism with respect to conscious states is a little more subtle, because it's not being said that consciousness is an illusion; it's just being said that consciousness is some sort of property of computational states. I argue first that this requires dualism, at least with our current physical ontology, because conscious states are replete with constituents not present in physical ontology - for example, the \"qualia\", an exotic name for very straightforward realities like: the shade of green appearing in the banner of this site, the feeling of the wind on your skin, really every sensation or feeling you ever had. In a world made solely of quantum fields in space, there are no such things; there are just particles and arrangements of particles. The truth of this ought to be especially clear for color, but it applies equally to everything else.\n\nIn order that this post should not be overlong, I will not argue at length here for the proposition that functionalism implies dualism, but shall proceed to the second stage of the argument, which does not seem to have appeared even in the philosophy literature. If we are going to suppose that minds and their states correspond solely to combinations of mesoscopic information-processing events like chemical and electrical signals in the brain, then there must be a mapping from possible exact microphysical states of the brain, to the corresponding mental states. Supposing we have a mapping from mental states to coarse-grained computational states, we now need a further mapping from computational states to exact microphysical states. There will of course be borderline cases. Functional states are identified by their causal roles, and there will be microphysical states which do not stably and reliably produce one output behavior or the other.\n\nPhysicists are used to talking about thermodynamic quantities like pressure and temperature as if they have an independent reality, but objectively they are just nicely behaved averages. The fundamental reality consists of innumerable particles bouncing off each other; one does not need, and one has no evidence for, the existence of a separate entity, \"pressure\", which exists in parallel to the detailed microphysical reality. The idea is somewhat absurd.\n\nYet this is analogous to the picture implied by a computational philosophy of mind (such as functionalism) applied to an atomistic physical ontology. We do know that the entities which constitute consciousness - the perceptions, thoughts, memories... which make up an experience - actually exist, and I claim it is also clear that they do not exist in any standard physical ontology. So, unless we get a very different physical ontology, we must resort to dualism. The mental entities become, inescapably, a new category of beings, distinct from those in physics, but systematically correlated with them. Except that, if they are being correlated with coarse-grained neurocomputational states which do not have an exact microphysical definition, only a functional definition, then the mental part of the new combined ontology is fatally vague. It is impossible for fundamental reality to be objectively vague; vagueness is a property of a concept or a definition, a sign that it is incomplete or that it does not need to be exact. But reality itself is necessarily exact - it is _something_ \\- and so functionalist dualism cannot be true unless the underdetermination of the psychophysical correspondence is replaced by something which says for all possible physical states, exactly what mental states (if any) should also exist. And that inherently runs against the functionalist approach to mind.\n\nVery few people consider themselves functionalists _and_ dualists. Most functionalists think of themselves as materialists, and materialism is a monism. What I have argued is that functionalism, the existence of consciousness, and the existence of microphysical details as the fundamental physical reality, together imply a peculiar form of dualism in which microphysical states which are borderline cases with respect to functional roles must all nonetheless be assigned to precisely one computational state or the other, even if no principle tells you how to perform such an assignment. The dualist will have to suppose that an exact but arbitrary border exists in state space, between the equivalence classes.\n\nThis - not just dualism, but a dualism that is necessarily arbitrary in its fine details - is too much for me. If you want to go all Occam-Kolmogorov-Solomonoff about it, you can say that the information needed to specify those boundaries in state space is so great as to render this whole class of theories of consciousness not worth considering. Fortunately there is an alternative.\n\nHere, in addressing this audience, I may need to undo a little of what you may think you know about quantum mechanics. Of course, the local preference is for the Many Worlds interpretation, and we've had that discussion many times. One reason Many Worlds has a grip on the imagination is that it looks easy to imagine. Back when there was just one world, we thought of it as particles arranged in space; now we have many worlds, dizzying in their number and diversity, but each _individual world_ still consists of just particles arranged in space. I'm sure that's how many people think of it.\n\nAmong physicists it will be different. Physicists will have some idea of what a wavefunction is, what an operator algebra of observables is, they may even know about path integrals and the various arcane constructions employed in quantum field theory. Possibly they will understand that the Copenhagen interpretation is not about consciousness collapsing an actually existing wavefunction; it is a positivistic rationale for focusing only on measurements and not worrying about what happens in between. And perhaps we can all agree that this is inadequate, as a final description of reality. What I want to say, is that Many Worlds serves the same purpose in many physicists' minds, but is equally inadequate, though from the opposite direction. Copenhagen says the observables are real but goes misty about unmeasured reality. Many Worlds says the wavefunction is real, but goes misty about exactly how it connects to observed reality. My most frustrating discussions on this topic are with physicists who are happy to be vague about what a \"world\" is. It's really not so different to Copenhagen positivism, except that where Copenhagen says \"we only ever see measurements, what's the problem?\", Many Worlds says \"I say there's an independent reality, what else is left to do?\". It is very rare for a Many World theorist to seek an exact idea of what a world is, as you see Robin Hanson and maybe Eliezer Yudkowsky doing; in that regard, reading the Sequences on this site will give you an unrepresentative idea of the interpretation's status.\n\nOne of the characteristic features of quantum mechanics is entanglement. But both Copenhagen, and a Many Worlds which ontologically privileges the position basis (arrangements of particles in space), still have atomistic ontologies of the sort which will produce the \"arbitrary dualism\" I just described. Why not seek a quantum ontology in which there are complex natural unities - fundamental objects which aren't simple - in the form of what we would presently called entangled states? That was the motivation for the [quantum monadology](/lw/1bs/how_to_think_like_a_quantum_monadologist/) described in my other really unpopular post. :-) \\[**Edit:** Go there for a discussion of \"the mind as tensor factor\", mentioned at the start of this post.\\] Instead of saying that physical reality is a series of transitions from one arrangement of particles to the next, say it's a series of transitions from one set of entangled states to the next. Quantum mechanics does not tell us which basis, if any, is ontologically preferred. Reality as a series of transitions between overall wavefunctions which are partly factorized and partly still entangled is a possible ontology; hopefully readers who really are quantum physicists will get the gist of what I'm talking about.\n\nI'm going to double back here and revisit the topic of how the world seems to look. Hopefully we agree, not just that there is an appearance of time flowing, but also an appearance of a self. Here I want to argue just for the bare minimum - that a moment's conscious experience consists of a set of things, events, situations... which are simultaneously \"present to\" or \"in the awareness of\" something - a conscious being - you. I'll argue for this because even this bare minimum is not acknowledged by existing materialist attempts to explain consciousness. I was recently directed to [this brief talk](http://www.ted.com/talks/julian_baggini_is_there_a_real_you.html) about the idea that there's no \"real you\". We are given a picture of a graph whose nodes are memories, dispositions, etc., and we are told that the self is like that graph: nodes can be added, nodes can be removed, it's a purely relational composite without any persistent part. What's missing in that description is that bare minimum notion of a perceiving self. Conscious experience consists of a subject perceiving objects in certain aspects. Philosophers have discussed for centuries how best to characterize the details of this phenomenological ontology; I think the best was Edmund Husserl, and I expect his work to be extremely important in interpreting consciousness in terms of a new physical ontology. But if you can't even notice that there's an observer there, observing all those parts, then you won't get very far.\n\nMy favorite slogan for this is due to the other Jaynes, Julian Jaynes. I don't endorse his theory of consciousness at all; but while in a daydream he once said to himself, \"Include the knower in the known\". That sums it up perfectly. We _know_ there is a \"knower\", an experiencing subject. We know this, just as well as we know that reality exists and that time passes. The adoption of ontologies in which these aspects of reality are regarded as unreal, as appearances as only, may be motivated by science, but it's false to the most basic facts there are, and one should show a little more imagination about what science will say when it's more advanced.\n\nI think I've said almost all of this before. The high point of the argument is that we should look for a physical ontology in which a self exists and is a natural yet complex unity, rather than a vaguely bounded conglomerate of distinct information-processing events, because the latter leads to one of those unacceptably arbitrary dualisms. If we can find a physical ontology in which the conscious self can be identified directly with a class of object posited by the theory, we can even get away from dualism, because physical theories are mathematical and formal and make few commitments about the \"inherent qualities\" of things, just about their causal interactions. If we can find a physical object which is absolutely isomorphic to a conscious self, then we can turn the isomorphism into an identity, and the dualism goes away. We can't do that with a functionalist theory of consciousness, because it's a many-to-one mapping between physical and mental, not an isomorphism.\n\nSo, I've said it all before; what's new? What have I accomplished during these last sixteen months? Mostly, I learned a lot of physics. I did not originally intend to get into the details of particle physics - I thought I'd just study the ontology of, say, string theory, and then use that to think about the problem. But one thing led to another, and in particular I made progress by taking ideas that were slightly on the fringe, and trying to embed them within an orthodox framework. It was a great way to learn, and some of those fringe ideas may even turn out to be correct. It's now abundantly clear to me that I really could become a career physicist, working specifically on fundamental theory. I might even _have to_ do that, it may be the best option for a day job. But what it means for the investigations detailed in this essay, is that I don't need to skip over any details of the fundamental physics. I'll be concerned with many-body interactions of biopolymer electrons _in vivo_, not particles in a collider, but an electron is still an electron, an elementary particle, and if I hope to identify the conscious state of the quantum self with certain special states from a many-electron Hilbert space, I should want to understand that Hilbert space in the deepest way available.\n\n[My only peer-reviewed publication](http://www.ncbi.nlm.nih.gov/pubmed/11755497), from many years ago, picked out pathways in the microtubule which, we speculated, might be suitable for mobile electrons. I had nothing to do with noticing those pathways; my contribution was the speculation about what sort of physical processes such pathways might underpin. Something I did notice, but never wrote about, was the unusual similarity (so I thought) between the microtubule's structure, and [a model of quantum computation due to the topologist Michael Freedman](http://online.kitp.ucsb.edu//online/colloq/freedman2/): a hexagonal lattice of qubits, in which entanglement is protected against decoherence by being encoded in topological degrees of freedom. It seems clear that performing an ontological analysis of a topologically protected coherent quantum system, in the context of some comprehensive ontology (\"interpretation\") of quantum mechanics, is a good idea. I'm not claiming to know, by the way, that the microtubule is the locus of quantum consciousness; there are a number of possibilities; but the microtubule has been studied for many years now and there's a big literature of models... a few of which might even have biophysical plausibility.\n\nAs for the interpretation of quantum mechanics itself, [these developments](http://pirsa.org/11080047/) are highly technical, but revolutionary. A well-known, well-studied quantum field theory turns out to have a bizarre new nonlocal formulation in which collections of particles seem to be replaced by polytopes in twistor space. Methods pioneered via purely mathematical studies of this theory are already being used for real-world calculations in QCD (the theory of quarks and gluons), and I expect this new ontology of \"reality as a complex of twistor polytopes\" to carry across as well. I don't know which quantum interpretation will win the battle now, but this is _new information_, of utterly fundamental significance. It is precisely the sort of altered holistic viewpoint that I was groping towards when I spoke about quantum monads constituted by entanglement. So I think things are looking good, just on the pure physics side. The real job remains to show that there's such a thing as quantum neurobiology, and to connect it to something like Husserlian transcendental phenomenology of the self via the new quantum formalism.\n\nIt's when we reach a level of understanding like that, that we will truly be ready to tackle the relationship between consciousness and the new world of intelligent autonomous computation. I don't deny the enormous helpfulness of the computational perspective in understanding unconscious \"thought\" and information processing. And even conscious states are still _states_, so you can surely make a state-machine model of the causality of a conscious being. It's just that the reality of how consciousness, computation, and fundamental ontology are connected, is bound to be a whole lot deeper than just a stack of virtual machines in the brain. We will have to fight our way to a new perspective which subsumes and transcends the computational picture of reality as a set of causally coupled black-box state machines. It should still be possible to \"port\" most of the thinking about Friendly AI to this new ontology; but the differences, what's new, are liable to be crucial to success. Fortunately, it seems that new perspectives are still possible; we haven't reached Kantian cognitive closure, with no more ontological progress open to us. On the contrary, there are still lines of investigation that we've hardly begun to follow.",
      "plaintextDescription": "Synopsis: The brain is a quantum computer and the self is a tensor factor in it - or at least, the truth lies more in that direction than in the classical direction - and we won't get Friendly AI right unless we get the ontology of consciousness right.\n\nFollowed by: Does functionalism imply dualism?\n\n\nSixteen months ago, I made a post seeking funding for personal research. There was no separate Discussion forum then, and the post was comprehensively downvoted. I did manage to keep going at it, full-time, for the next sixteen months. Perhaps I'll get to continue; it's for the sake of that possibility that I'll risk another breach of etiquette. You never know who's reading these words and what resources they have. Also, there has been progress.\n\nI think the best place to start is with what orthonormal said in response to the original post: \"I don't think anyone should be funding a Penrose-esque qualia mysterian to study string theory.\" If I now took my full agenda to someone out in the real world, they might say: \"I don't think it's worth funding a study of 'the ontological problem of consciousness in the context of Friendly AI'.\" That's my dilemma. The pure scientists who might be interested in basic conceptual progress are not engaged with the race towards technological singularity, and the apocalyptic AI activists gathered in this place are trying to fit consciousness into an ontology that doesn't have room for it. In the end, if I have to choose between working on conventional topics in Friendly AI, and on the ontology of quantum mind theories, then I have to choose the latter, because we need to get the ontology of consciousness right, and it's possible that a breakthrough could occur in the world outside the FAI-aware subculture and filter through; but as things stand, the truth about consciousness would never be discovered by employing the methods and assumptions that prevail inside the FAI subculture.\n\nPerhaps I should pause to spell out why the nature of cons",
      "wordCount": 4050
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3EdLvRoTp3i2E7uHy",
    "title": "Utopian hope versus reality",
    "slug": "utopian-hope-versus-reality",
    "url": null,
    "baseScore": 31,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 54,
    "createdAt": null,
    "postedAt": "2012-01-11T12:55:45.959Z",
    "contents": {
      "markdown": "I've seen an interesting variety of utopian hopes expressed recently. Raemon's [\"Ritual\" sequence of posts](/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration) is working to affirm the viability of LW's rationalist-immortalist utopianism, not just in the midst of an indifferent universe, but in the midst of an indifferent society. [Leverage Research](/r/discussion/lw/9a2/introducing_leverage_research/) turn out to be [social-psychology utopians](/r/discussion/lw/9ar/on_leverage_researchs_plan_for_an_optimal_world/), who plan to achieve their world of optimality by unleashing the best in human nature. And [Russian life-extension activist Maria Konovalenko](http://mariakonovalenko.wordpress.com/2012/01/10/transhumanist-media-content/) just blogged about the difficulty of getting people to adopt anti-aging research as the top priority in life, even though it's so obvious to her that it should be.\n\nThis phenomenon of utopian hope - its nature, its causes, its consequences, whether it's ever realistic, whether it ever does any good - certainly deserves attention and analysis, because it affects, and even afflicts, a lot of people, on this site and far beyond. It's a vast topic, with many dimensions. All my examples above have a futurist tinge to them - an AI singularity, and a biotech society where rejuvenation is possible, are clearly futurist concepts; and even the idea of human culture being transformed for the better by new ideas about the mind, belongs within the same broad scientific-technological current of Utopia Achieved Through Progress. But if we look at all the manifestations of utopian hope in history, and not just at those which resemble our favorites, other major categories of utopia can be observed - utopia achieved by reaching _back_ to the conditions of a Golden Age; utopia achieved in some other reality, like an afterlife.\n\nThe most familiar form of utopia these days is the ideological social utopia, to be achieved once the world is run properly, according to the principles of some political \"-ism\". This type of utopia can cut across the categories I have mentioned so far; utopian communism, for example, has both futurist and golden-age elements to its thinking. The new society is to be created via new political forms and new philosophies, but the result is a restoration of human solidarity and community that existed before hierarchy and property... The student of utopian thought must also take note of religion, which until technology has been the main avenue through which humans have pursued their most transcendental hopes, like not having to die.\n\nBut I'm not setting out to study utopian thought and utopian psychology out of a neutral scholarly interest. I have been a utopian myself and I still am, if utopianism includes belief in the possibility (though not the inevitability) of something much better. And of course, the utopias that I have taken seriously are futurist utopias, like the utopia where we do away with death, and thereby also do away with a lot of other social and psychological pathologies, which are presumed to arise from the crippling futility of the universal death sentence.\n\nHowever, by now, I have also lived long enough to know that my own hopes were mistaken many times over; long enough to know that sometimes the mistake was in the ideas themselves, and not just the expectation that everyone else would adopt them; and long enough to understand something of the ordinary non-utopian psychology, whose main features I would nominate as reconciliation with work and with death. Everyone experiences the frustration of having to work for a living and the quiet horror of physiological decline, but hardly anyone imagines that there might be an alternative, or rejects such a lifecycle as overall more bad than it is good.\n\nWhat is the relationship between ordinary psychology and utopian psychology? First, the serious utopians should recognize that they are an extreme minority. Not only has the whole of human history gone by without utopia ever managing to happen, but the majority of people who ever lived were not utopians in the existentially revolutionary sense of thinking that the intolerable yet perennial features of the human condition might be overthrown. The confrontation with the evil aspects of life must usually have proceeded more at an emotional level - for example, terror that something might be true, and horror at the realization that it _is_ true; a growing sense that it is impossible to escape; resignation and defeat; and thereafter a permanently diminished vitality, often compensated by achievement in the spheres of work and family.\n\nThe utopian response is typically made possible only because one imagines that there is a specific alternative to this process; and so, as ideas about alternatives are invented and circulated, it becomes easier for people to end up on the track of utopian struggle with life, rather than the track of resignation, which is why we can have enough people to form social movements and fundamentalist religions, and not just isolated weirdos. There is a continuum between full radical utopianism and very watered-down psychological phenomena which hardly deserve that name, but still have something in common - for example, a person who lives an ordinary life but draws some sustenance from the possibility of an afterlife of unspecified nature, where things might be different, and where old wrongs might be righted - but nonetheless, I would claim that the historically dominant temperament in adult human experience has been resignation to hopelessness and helplessness in ultimate matters, and an absorption in affairs where some limited achievement is possible, but which in themselves can never satisfy the utopian impulse.\n\nThe new factor in our current situation is science and technology. Our modern history offers evidence that the world really can change fundamentally, and such further explosive possibilities as artificial intelligence and rejuvenation biotechnology are considered possible for good, tough-minded, empirical reasons, not just because they offer a convenient vehicle for our hopes.\n\nTechnological utopians often exhibit frustration that their pet technologies and their favorite dreams of existential emancipation aren't being massively prioritized by society, and they don't understand why other people don't just immediately embrace the dream when they first hear about it. (Or they develop painful psychological theories of why the human race is ignoring the great hope.) So let's ask, what are the attitudes towards alleged technological emancipation that a person might adopt?\n\nOne is the utopian attitude: the belief that here, finally, one of the perennial dreams of the human race can come true. Another is denial: which is sometimes founded on bitter experience of disappointment, which teaches that the wise thing to do is not to fool yourself when another new hope comes up to you and cheerfully asserts that this time really is different. Another is to accept the possibility but deny the utopian hope. I think this is the most important interpretation to understand.\n\nIt is the one that precedent supports. History is full of new things coming to pass, but they have never yet led to utopia. So we might want to scrutinize our technological projections more closely, and see whether the utopian expectation is based on overlooking the downside. For example, let us contrast the idea of rejuvenation and the idea of immortality - not dying, ever. Just because we can take someone who is 80 and make them biologically 20, is not the same thing as making them immortal. It just means that won't die of aging, and that when they do die, it will be in a way befitting someone 20 years old. They'll die in an accident, or a suicide, or a crime. Incidentally, we should also note an element of _psychological_ unrealism in the idea of never wanting to die. Forever is a long time; the whole history of the human race is about 10,000 years long. Just 10,000 years is enough to encompass all the difficulties and disappointments and permutations of outlook that have ever happened. Imagine taking the whole history of the human race into yourself; living through it personally. It's a lot to have endured.\n\nIt would be unfair to say that transhumanists as a rule are dominated by utopian thinking. Perhaps just as common is a sort of futurological bipolar disorder, in which the future looks like it will bring \"utopia or oblivion\", something really good or something really bad. The conservative wisdom of historical experience says that both these expectations are wrong; bad things can happen, even catastrophes, but life keeps going for someone - that is the precedent - and the expectation of total devastating extinction is just a plunge into depression as unrealistic as the utopian hope for a personal eternity; both extremes exhibiting an inflated sense of historical or cosmic self-importance. The end of you is not the end of the world, says this historical wisdom; imagining the end of the whole world is your overdramatic response to imagining the end of you - or the end of your particular civilization.\n\nHowever, I think we do have some reason to suppose that this time around, the extremes are really possible. I won't go so far as to endorse the idea that (for example) intelligent life in the universe typically turns its home galaxy into one giant mass of computers; that really does look like a case of taking the concept and technology with which our current society is obsessed, and projecting it onto the cosmic unknown. But just the humbler ideas of transhumanity, posthumanity, and a genuine end to the human-dominated era on Earth, whether in extinction or in transformation. The real and verifiable developments of science and technology, and the further scientific and technological developments which they portend, are enough to justify such a radical, if somewhat nebulous, concept of the possible future. And again, while I won't simply endorse the view that of course we shall get to be as gods, and shall get to feel as good as gods might feel, it seems reasonable to suppose that there are possible futures which are genuinely and comprehensively better than anything that history has to offer - as well as futures that are just bizarrely altered, and futures which are empty and dead.\n\nSo that is my limited endorsement of utopianism: In principle, there might be a utopianism which is justified. But in practice, what we have are people getting high on hope, emerging fanaticisms, personal dysfunctionality in the present, all the things that come as no surprise to a cynical student of history. The one outcome that would be most surprising to a cynic is for a genuine utopia to arrive. I'm willing to say that this is possible, but I'll also say that almost any existing reference to a better world to come, and any psychological state or social movement which draws sublime happiness from the contemplation of an expected future, has something unrealistic about it.\n\nIn this regard, utopian hope is almost always an indicator of something wrong. It can just be naivete, especially in a young person. As I have mentioned, even non-utopian psychology inevitably has those terrible moments when it learns for the first time about the limits of life as we know it. If in your own life you start to enter that territory for the first time, without having been told from an early age that real life is fundamentally limited and frustrating, and perhaps with a few vague promises of hope, absorbed from diverse sources, to sustain you, then it's easy to see your hopes as, not utopian hopes, but simply a hope that life can be worth living. I think this is the experience of many young idealists in \"environmental\" and \"social justice\" movements; their culture has always implied to them that life should be a certain way, without also conveying to them that it has never once been that way in reality. The suffering of transhumanist idealists and other radical-futurist idealists, when they begin to run aground on the disjunction between their private subcultural expectations and those of the culture at large, has a lot in common with the suffering of young people whose ideals are more conventionally recognizable; and it is entirely conceivable that for some generation now coming up, rebellion against biological human limitations will be what rebellion against social limitations has been for preceding generations.\n\nI should also mention, in passing, the option of a non-utopian transhumanism, something that is far more common than my discussion so far would mention. This is the choice of people who expect, not utopia, but simply an open future. Many cryonicists would be like this. Sure, they expect the world of tomorrow to be a great place, good enough that they want to get there; but they don't think of it as an eternal paradise of wish-fulfilment that may or may not be achieved, depending on heroic actions in the present. This is simply the familiar non-utopian view that life is overall worth living, combined with the belief that life can now be lived for much longer periods; the future not as utopia, but as more history, history that hasn't happened yet, and which one might get to personally experience. If I was wanting to start a movement in favor of rejuvenation and longevity, this is the outlook I would be promoting, not the idea that abolishing death will cure all evils (and not even the idea that death as such can be abolished; rejuvenation is not immortality, it's just more good life). In the spectrum of future possibilities, it's only the issue of artificial intelligence which lends some plausibility to extreme bipolar futurism, the idea that the future can be very good (by human standards) or very bad (by human standards), depending on what sort of utility functions govern the decision-making of transhuman intelligence.\n\nThat's all I have to say for now. It would be unrealistic to think we can completely avoid the pathologies associated with utopian hope, but perhaps we can moderate them, if we pay attention to the psychology involved.",
      "plaintextDescription": "I've seen an interesting variety of utopian hopes expressed recently. Raemon's \"Ritual\" sequence of posts is working to affirm the viability of LW's rationalist-immortalist utopianism, not just in the midst of an indifferent universe, but in the midst of an indifferent society. Leverage Research turn out to be social-psychology utopians, who plan to achieve their world of optimality by unleashing the best in human nature. And Russian life-extension activist Maria Konovalenko just blogged about the difficulty of getting people to adopt anti-aging research as the top priority in life, even though it's so obvious to her that it should be.\n\nThis phenomenon of utopian hope - its nature, its causes, its consequences, whether it's ever realistic, whether it ever does any good - certainly deserves attention and analysis, because it affects, and even afflicts, a lot of people, on this site and far beyond. It's a vast topic, with many dimensions. All my examples above have a futurist tinge to them - an AI singularity, and a biotech society where rejuvenation is possible, are clearly futurist concepts; and even the idea of human culture being transformed for the better by new ideas about the mind, belongs within the same broad scientific-technological current of Utopia Achieved Through Progress. But if we look at all the manifestations of utopian hope in history, and not just at those which resemble our favorites, other major categories of utopia can be observed - utopia achieved by reaching back to the conditions of a Golden Age; utopia achieved in some other reality, like an afterlife.\n\nThe most familiar form of utopia these days is the ideological social utopia, to be achieved once the world is run properly, according to the principles of some political \"-ism\". This type of utopia can cut across the categories I have mentioned so far; utopian communism, for example, has both futurist and golden-age elements to its thinking. The new society is to be created via new political",
      "wordCount": 2280
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4xum2CuYMMRq8rgsW",
    "title": "On Leverage Research's plan for an optimal world",
    "slug": "on-leverage-research-s-plan-for-an-optimal-world",
    "url": null,
    "baseScore": 41,
    "voteCount": 42,
    "viewCount": null,
    "commentCount": 89,
    "createdAt": null,
    "postedAt": "2012-01-10T09:49:40.086Z",
    "contents": {
      "markdown": "[The plan](http://www.leverageresearch.org/tiki-index.php?page=Our+Current+Plan) currently revolves around using Connection Theory, a new psychological theory, to design \"beneficial contagious ideologies\", the spread of which will lead to the existence of \"an enormous number of actively and stably benevolent people\", who will then \"coordinate their activities\", seek power, and then use their power to eliminate scarcity, disease, harmful governments, global catastrophic threats, etc.\n\nThat is not how the world works. Most positions of power are already occupied by people who have common sense, good will, and a sense of responsibility - or they have those traits, to the extent that human frailty manages to preserve them, amidst the unpredictability of life. The idea that a magic new theory of psychology will unlock human potential and create a new political majority of model citizens is a secular messianism with nothing to back it up.\n\nI suggest that the people behind Leverage Research need to decide whether they are in the business of solving problems, or in the business of solving meta-problems. The real problems of the world are hard problems, they overwhelm even highly capable people who devote their lives to making a difference. Handwaving about meta topics like psychology and methodology can't be expected to offer more than marginal assistance in any specific concrete domain.",
      "plaintextDescription": "The plan currently revolves around using Connection Theory, a new psychological theory, to design \"beneficial contagious ideologies\", the spread of which will lead to the existence of \"an enormous number of actively and stably benevolent people\", who will then \"coordinate their activities\", seek power, and then use their power to eliminate scarcity, disease, harmful governments, global catastrophic threats, etc.\n\nThat is not how the world works. Most positions of power are already occupied by people who have common sense, good will, and a sense of responsibility - or they have those traits, to the extent that human frailty manages to preserve them, amidst the unpredictability of life. The idea that a magic new theory of psychology will unlock human potential and create a new political majority of model citizens is a secular messianism with nothing to back it up.\n\nI suggest that the people behind Leverage Research need to decide whether they are in the business of solving problems, or in the business of solving meta-problems. The real problems of the world are hard problems, they overwhelm even highly capable people who devote their lives to making a difference. Handwaving about meta topics like psychology and methodology can't be expected to offer more than marginal assistance in any specific concrete domain.",
      "wordCount": 209
    },
    "tags": [
      {
        "_id": "sYbszETv5rKst6gxD",
        "name": "Leverage Research",
        "slug": "leverage-research"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6McLFoTupdCkxH9Nd",
    "title": "Problems of the Deutsch-Wallace version of Many Worlds",
    "slug": "problems-of-the-deutsch-wallace-version-of-many-worlds",
    "url": null,
    "baseScore": 5,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 93,
    "createdAt": null,
    "postedAt": "2011-12-16T06:55:55.479Z",
    "contents": {
      "markdown": "The subject has already been raised in [this thread](/r/discussion/lw/8uy/a_case_study_in_fooling_oneself/), but in a clumsy fashion. So here is a fresh new thread, where we can discuss, calmly and objectively, the pros and cons of the \"Oxford\" version of the Many Worlds interpretation of quantum mechanics.\n\nThis version of MWI is distinguished by two propositions. First, there is no definite number of \"worlds\" or \"branches\". They have a fuzzy, vague, approximate, definition-dependent existence. Second, the probability law of quantum mechanics (the Born rule) is to be obtained, not by counting the frequencies of events in the multiverse, but by an analysis of rational behavior in the multiverse. Normally, a prescription for rational behavior is obtained by maximizing expected utility, a quantity which is calculated by averaging \"probability x utility\" for each possible outcome of an action. In the Oxford school's \"decision-theoretic\" derivation of the Born rule, we somehow _start_ with a ranking of actions that is deemed rational, then we \"divide out\" by the utilities, and obtain probabilities that were implicit in the original ranking.\n\nI reject the two propositions. \"Worlds\" or \"branches\" can't be vague if they are to correspond to observed reality, because vagueness results from an object being dependent on observer definition, and the local portion of reality does not owe its existence to how we define anything; and the upside-down decision-theoretic derivation, if it ever works, must implicitly smuggle in the premises of probability theory in order to obtain its original rationality ranking.\n\nSome references:\n\n[\"Decoherence and Ontology: or, How I Learned to Stop Worrying and Love FAPP\"](http://users.ox.ac.uk/~mert0130/papers-ev.shtml#FAPP) by David Wallace. In this paper, Wallace says, for example, that the question \"how many branches are there?\" \"does not... make sense\", that the question \"how many branches are there in which it is sunny?\" is \"a question which has no answer\", \"it is a non-question to ask how many \\[worlds\\]\", etc.\n\n[\"Quantum Probability from Decision Theory?\"](http://lanl.arxiv.org/abs/quant-ph/9907024) by Barnum et al. This is a rebuttal of the original argument (due to David Deutsch) that the Born rule can be justified by an analysis of multiverse rationality.",
      "plaintextDescription": "The subject has already been raised in this thread, but in a clumsy fashion. So here is a fresh new thread, where we can discuss, calmly and objectively, the pros and cons of the \"Oxford\" version of the Many Worlds interpretation of quantum mechanics.\n\nThis version of MWI is distinguished by two propositions. First, there is no definite number of \"worlds\" or \"branches\". They have a fuzzy, vague, approximate, definition-dependent existence. Second, the probability law of quantum mechanics (the Born rule) is to be obtained, not by counting the frequencies of events in the multiverse, but by an analysis of rational behavior in the multiverse. Normally, a prescription for rational behavior is obtained by maximizing expected utility, a quantity which is calculated by averaging \"probability x utility\" for each possible outcome of an action. In the Oxford school's \"decision-theoretic\" derivation of the Born rule, we somehow start with a ranking of actions that is deemed rational, then we \"divide out\" by the utilities, and obtain probabilities that were implicit in the original ranking.\n\nI reject the two propositions. \"Worlds\" or \"branches\" can't be vague if they are to correspond to observed reality, because vagueness results from an object being dependent on observer definition, and the local portion of reality does not owe its existence to how we define anything; and the upside-down decision-theoretic derivation, if it ever works, must implicitly smuggle in the premises of probability theory in order to obtain its original rationality ranking.\n\nSome references:\n\n\"Decoherence and Ontology: or, How I Learned to Stop Worrying and Love FAPP\" by David Wallace. In this paper, Wallace says, for example, that the question \"how many branches are there?\" \"does not... make sense\", that the question \"how many branches are there in which it is sunny?\" is \"a question which has no answer\", \"it is a non-question to ask how many [worlds]\", etc.\n\n\"Quantum Probability from Decision Theory?",
      "wordCount": 341
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "w7GYWtoRZsizsF8Xk",
    "title": "A case study in fooling oneself",
    "slug": "a-case-study-in-fooling-oneself",
    "url": null,
    "baseScore": -15,
    "voteCount": 61,
    "viewCount": null,
    "commentCount": 79,
    "createdAt": null,
    "postedAt": "2011-12-15T05:25:52.981Z",
    "contents": {
      "markdown": "_**Note: This post assumes that the Oxford version of Many Worlds is wrong, and speculates as to why this isn't obvious. For a discussion of the hypothesis itself, see [Problems of the Deutsch-Wallace version of Many Worlds](/r/discussion/lw/8vg/problems_of_the_deutschwallace_version_of_many/).**_\n\nsmk asks [how many worlds](/r/discussion/lw/8ue/how_many_worlds/) are produced in a quantum process where the outcomes have unequal probabilities; Emile says [there's no exact answer](/r/discussion/lw/8ue/how_many_worlds/5g82), just like there's no exact answer for how many ink blots are in the messy picture; Tetronian says this analogy is [a great way to demonstrate](/r/discussion/lw/8ue/how_many_worlds/5g89) what a \"wrong question\" is; Emile has (at this writing) 9 upvotes, and Tetronian has 7.  \n  \nMy thesis is that Emile has instead provided an example of how to dismiss a question and thereby fool oneself; Tetronian provides an example of treating an epistemically destructive technique of dismissal as epistemically virtuous and fruitful; and the upvotes show that this isn't just their problem. \\[**edit**: [Emile](/r/discussion/lw/8uy/a_case_study_in_fooling_oneself/5ggy) and [Tetronian](/r/discussion/lw/8uy/a_case_study_in_fooling_oneself/5gfh) respond.\\]  \n  \nI am as tired as anyone of the debate over Many Worlds. I don't expect the general climate of opinion on this site to change except as a result of new intellectual developments in the larger world of physics and philosophy of physics, which is where the question will be decided anyway. But the mission of Less Wrong is supposed to be the refinement of rationality, and so perhaps this \"case study\" is of interest, not just as another opportunity to argue over the interpretation of quantum mechanics, but as an opportunity to dissect a little bit of irrationality that is not only playing out here and now, but which evidently has a base of support.  \n  \nThe question is not just, what's wrong with the argument, but also, how did it get that base of support? How was a situation created where one person says something irrational (or foolish, or however the problem is best understood), and a lot of other people nod in agreement and say, that's an excellent example of how to think?  \n  \nOn this occasion, my quarrel is not with the Many Worlds interpretation as such; it is with the version of Many Worlds which says there's no actual number of worlds. Elsewhere in the thread, someone says there are uncountably many worlds, and someone else says there are two worlds. At least those are meaningful answers (although the advocate of \"two worlds\" as the answer, then goes on to say that one world is \"stronger\" than the other, which is meaningless).  \n  \nBut the proposition that there is no definite number of worlds, is as foolish and self-contradictory as any of those other contortions from the history of thought that rationalists and advocates of common sense like to mock or boggle at. At times I have wondered how to place Less Wrong in the history of thought; well, this is one way to do it - it can have its own chapter in the history of intellectual folly; it can be known by its mistakes.  \n  \nThen again, this \"mistake\" is not original to Less Wrong. It appears to be one of the defining ideas of the Oxford-based approach to Many Worlds associated with David Deutsch and David Wallace; the other defining idea being the proposal to derive probabilities from rationality, rather than vice versa. (I refer to the attempt to derive the Born rule from arguments about how to behave rationally in the multiverse.) The Oxford version of MWI seems to be very popular among thoughtful non-physicist advocates of MWI - even though I would regard _both_ its defining ideas as nonsense - and it may be that its ideas get a pass here, partly because of their social status. That is, an important faction of LW opinion believes that Many Worlds is the explanation of quantum mechanics, and the Oxford school of MWI has high status and high visibility within the world of MWI advocacy, and so its ideas will receive approbation without much examination or even much understanding, because of the social and psychological mechanisms which incline people to agree with, defend, and laud their favorite authorities, even if they don't really understand what these authorities are saying or why they are saying it.  \n  \nHowever, it is undoubtedly the case that many of the LW readers who believe there's no definite number of worlds, believe this because the idea genuinely makes sense to them. They aren't just stringing together words whose meaning isn't known, like a Taliban who recites the Quran without knowing a word of Arabic; they've actually thought about this themselves; they have gone through some subjective process as a result of which they have consciously adopted this opinion. So from the perspective of analyzing how it is that people come to hold absurd-sounding views, this should be good news. It means that we're dealing with a genuine failure to reason properly, as opposed to a simple matter of reciting slogans or affirming allegiance to a view on the basis of something other than thought.  \n  \nAt a guess, the thought process involved is very simple. These people have thought about the wavefunctions that appear in quantum mechanics, at whatever level of technical detail they can muster; they have decided that the components or substructures of these wavefunctions which might be identified as \"worlds\" or \"branches\" are clearly approximate entities whose definition is somewhat arbitrary or subject to convention; and so they have concluded that there's no definite number of worlds in the wavefunction. And the failure in their thinking occurs when they don't take the next step and say, is this at all consistent with reality? That is, if a quantum world is something whose existence is fuzzy and which doesn't even have a definite multiplicity - that is, we can't even say if there's one, two, or many of them - if those are the properties of a quantum world, then is it possible for the real world to be one of those? It's the failure to ask that last question, and really think about it, which must be the oversight allowing the nonsense-doctrine of \"no definite number of worlds\" to gain a foothold in the minds of otherwise rational people.  \n  \nIf this diagnosis is correct, then at some level it's a case of \"treating the map as the territory\" syndrome. A particular conception of the quantum-mechanical wavefunction is providing the \"map\" of reality, and the individual thinker is perhaps making correct statements about what's on their map, but they are failing to check the properties of the map against the properties of the territory. In this case, the property of reality that falsifies the map is, the fact that it definitely exists, or perhaps the corollary of that fact, that something which definitely exists definitely exists at least once, and therefore exists with a definite, objective multiplicity.  \n  \nTrying to go further in the diagnosis, I can identify a few cognitive tendencies which may be contributing. First is the phenomenon of bundled assumptions which have never been made distinct and questioned separately. I suppose that in a few people's heads, there's a rapid movement from \"science (or materialism) is correct\" to \"quantum mechanics is correct\" to \"Many Worlds is correct\" to \"the Oxford school of MWI is correct\". If you are used to encountering all of those ideas together, it may take a while to realize that they are not linked out of logical necessity, but just contingently, by the narrowness of your own experience.  \n  \nSecond, it may seem that \"no definite number of worlds\" makes sense to an individual, because when they test their own worldview for semantic coherence, logical consistency, or empirical adequacy, it seems to pass. In the case of \"no-collapse\" or \"no-splitting\" versions of Many Worlds, it seems that it often passes the subjective making-sense test, because the individual is actually relying on ingredients borrowed from the Copenhagen interpretation. A semi-technical example would be the coefficients of a reduced density matrix. In the Copenhagen interpetation, they are probabilities. Because they have the mathematical attributes of probabilities (by this I just mean that they lie between 0 and 1), and because they can be obtained by strictly mathematical manipulations of the quantities composing the wavefunction, Many Worlds advocates tend to treat these quantities as inherently being probabilities, and use their \"existence\" as a way to obtain the Born probability rule from the ontology of \"wavefunction yes, wavefunction collapse no\". But just because something is a real number between 0 and 1, doesn't yet explain how it manages to be a probability. In particular, I would maintain that if you have a multiverse theory, in which all possibilities are actual, then a probability must refer to a _frequency_. The probability of an event in the multiverse is simply how often it occurs in the multiverse. And clearly, just having the number 0.5 associated with a particular multiverse branch is not yet the same thing as showing that the events in that branch occur half the time.  \n  \nI don't have a good name for this phenomenon, but we could call it \"borrowed support\", in which a belief system receives support from considerations which aren't legitimately its own to claim. (Ayn Rand apparently talked about a similar notion of \"borrowed concepts\".)  \n  \nThird, there is a possibility among people who have a capacity for highly abstract thought, to adopt an ideology, ontology, or \"theory of everything\" which is only expressed in those abstract terms, and to then treat that theory as the whole of reality, in a way that reifies the abstractions. This is a highly specific form of treating the map as the territory, peculiar to abstract thinkers. When someone says that reality is made of numbers, or made of computations, this is at work. In the case at hand, we're talking about a theory of physics, but the ontology of that theory is incompatible with the definiteness of one's own existence. My guess is that the main psychological factor at work here is intoxication with the feeling that one understands reality totally and in its essence. The universe has bowed to the imperial ego; one may not literally direct the stars in their courses, but one has known the essence of things. Combine that intoxication, with \"borrowed support\" and with the simple failure to think hard enough about where on the map the imperial ego itself might be located, and maybe you have a comprehensive explanation of how people manage to believe theories of reality which are flatly inconsistent with the most basic features of subjective experience.  \n  \nI should also say something about Emile's example of the ink blots. I find it rather superficial to just say \"there's no definite number of blots\". To say that the number of blots depends on definition is a lot closer to being true, but that undermines the argument, because that opens the possibility that there is a right definition of \"world\", and many wrong definitions, and that the true number of worlds is just the number of worlds according to the right definition.  \n  \nEmile's picture can be used for the opposite purpose. All we have to do is to scrutinize, more closely, what it actually is. It's a JPEG that is 314 pixels by 410 pixels in size. Each of those pixels will have an exact color coding. So clearly we can be entirely objective in the way we approach this question; all we have to do is be precise in our concepts, and engage with the genuine details of the object under discussion. Presumably the image is a scan of a physical object, but even in that case, we can be precise - it's made of atoms, they are particular atoms, we can make objective distinctions on the basis of contiguity and bonding between these atoms, and so the question will have an objective answer, if we bother to be sufficiently precise. The same goes for \"worlds\" or \"branches\" in a wavefunction. And the truly pernicious thing about this version of Many Worlds is that it prevents such inquiry. The ideology that tolerates vagueness about worlds serves to protect the proposed ontology from necessary scrutiny.  \n  \nThe same may be said, on a broader scale, of the practice of \"dissolving a wrong question\". That is a gambit which should be used sparingly and cautiously, because it easily serves to instead justify the dismissal of a legitimate question. A community trained to dismiss questions may never even _notice_ the gaping holes in its belief system, because the lines of inquiry which lead towards those holes are already dismissed as invalid, undefined, unnecessary. smk came to this topic fresh, and without a head cluttered with ideas about what questions are legitimate and what questions are illegitimate, and as a result managed to ask something which more knowledgeable people had already prematurely dismissed from their own minds.",
      "plaintextDescription": "Note: This post assumes that the Oxford version of Many Worlds is wrong, and speculates as to why this isn't obvious. For a discussion of the hypothesis itself, see Problems of the Deutsch-Wallace version of Many Worlds.\n\nsmk asks how many worlds are produced in a quantum process where the outcomes have unequal probabilities; Emile says there's no exact answer, just like there's no exact answer for how many ink blots are in the messy picture; Tetronian says this analogy is a great way to demonstrate what a \"wrong question\" is; Emile has (at this writing) 9 upvotes, and Tetronian has 7.\n\nMy thesis is that Emile has instead provided an example of how to dismiss a question and thereby fool oneself; Tetronian provides an example of treating an epistemically destructive technique of dismissal as epistemically virtuous and fruitful; and the upvotes show that this isn't just their problem. [edit: Emile and Tetronian respond.]\n\nI am as tired as anyone of the debate over Many Worlds. I don't expect the general climate of opinion on this site to change except as a result of new intellectual developments in the larger world of physics and philosophy of physics, which is where the question will be decided anyway. But the mission of Less Wrong is supposed to be the refinement of rationality, and so perhaps this \"case study\" is of interest, not just as another opportunity to argue over the interpretation of quantum mechanics, but as an opportunity to dissect a little bit of irrationality that is not only playing out here and now, but which evidently has a base of support.\n\nThe question is not just, what's wrong with the argument, but also, how did it get that base of support? How was a situation created where one person says something irrational (or foolish, or however the problem is best understood), and a lot of other people nod in agreement and say, that's an excellent example of how to think?\n\nOn this occasion, my quarrel is not with the Many Worlds interpretation as such; it",
      "wordCount": 2181
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kv7gvQ9AkDisCL2kg",
    "title": "What a practical plan for Friendly AI looks like",
    "slug": "what-a-practical-plan-for-friendly-ai-looks-like",
    "url": null,
    "baseScore": -1,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2011-08-20T09:50:23.686Z",
    "contents": {
      "markdown": "I have seen too many discussions of Friendly AI, here and elsewhere (e.g. in comments at Michael Anissimov's blog), detached from any concrete idea of how to do it. Sometimes the issue is the lack of code, demos, or a practical plan from SIAI. SIAI is seen as a source of wishful thinking about magic machines that will solve all our problems for us, or as a place engaged in a forever quest for nebulous mathematical vaporware such as \"reflective decision theory\". You will get singularity enthusiasts who say, it's great that SIAI has given the concept of FAI visibility, but enough with the philosophy, let's get coding! ... does anyone know where to start? And you will get singularity skeptics who say, unfriendly AI is a bedtime ghost story for credulous SF fans, wake me up when SIAI actually ships a product. Or, within this subculture of rationalist altruists who want to do the optimal thing, you'll get people saying, I don't know if I should donate, because I don't see how any of this is supposed to happen.\n\nSo in this post I want to sketch what a \"practical\" plan for Friendly AI looks like. I'm not here to _advocate_ this plan - I'm not saying this is the right way to do it. I'm just providing an example of a plan that could be pursued in the real world. Perhaps it will also allow people to better understand SIAI's indirect approach.\n\nI won't go into the details of financial or technical logistics. If we were talking about how to get to the moon from Earth, then the following plan is along the lines of \"Make a chemical-powered rocket big enough to get you there.\" Once you have that concept, you still have a lot of work to do, but you are at least on the right track - compared to people who want to make a teleportation device or a balloon that goes really high. But I will make one remark about how the idea of Friendly AI is framed. At present, it is discussed in conjunction with a whole cornucopia of science fiction notions such as: immortality, conquering the galaxy, omnipresent wish-fulfilling super-AIs, good and bad Jupiter-brains, mind uploads in heaven and hell, and so on. Similarly, we have all these thought-experiments: guessing games with omniscient aliens, decision problems in a branching multiverse, \"torture versus dust specks\". Whatever the ultimate relevance of such ideas, it is clearly possible to divorce the notion of Friendly AI from all of them. If a FAI project was trying to garner mass support, it first needs to be comprehensible, and the simple approach would be to say it is simply an exercise in creating artificial intelligence that does the right thing. Nothing about utopia; nothing about _dystopia_ caused by unfriendly AI; nothing about godlike superintelligence; just the scenario, already familiar in popular culture, of robots, androids, computers you can talk with. All that is coming, says the practical FAI project, and we are here to design these new beings so they will be good citizens, a positive rather than a negative addition to the world.\n\nSo much for how the project describes itself to the world at large. What are its guiding technical conceptions? What's the specific proposal which will allow educated skeptics to conclude that this might get off the ground? Remember that there are two essential challenges to overcome: the project has to create intelligence, and it has to create _ethical_ intelligence; what we call, in our existing discussions, \"AGI\" - artificial general intelligence - and \"FAI\" - friendly artificial intelligence.\n\nThere is a very simple approach which - like the idea of a chemical-powered rocket which gets you to the moon - should be sufficient to get you to FAI, when sufficiently elaborated. It can be seen by stripping away some of the complexities peculiar to SIAI's strategy, complexities which tend to dominate the discussion. The basic idea should also be thoroughly familiar. We are to conceive of the AI as having two parts, a goal system and a problem-solving system. AGI is achieved by creating a problem-solving system of sufficient power and universality; FAI is achieved by specifying the right goal system.\n\nSIAI, in discussing the quest for the right goal system, emphasizes the difficulties of this process and the unreliability of human judgment. Their idea of a solution is to use artificial intelligence to neuroscientifically deduce the actual algorithmic structure of human decision-making, and to then employ a presently nonexistent branch of decision theory to construct a goal system embodying ideals implicit in the unknown human cognitive algorithms.\n\nThe practical approach would not bother with this attempt to outsource the task of designing the AI's morality, to a presently nonexistent neuromathematical cognitive bootstrap process. While fully cognizant of the fact that value is complex, as eloquently attested by Eliezer in many speeches, the practical FAI project would nonetheless choose the AI's goal system in the old-fashioned way, by human deliberation and consensus. You would get a team of professional ethicists, some worldly people like managers, some legal experts in the formulation of contracts, and together you would hammer out a mission statement for the AI. Then you would get your programmers and your cognitive scientists to implement that goal condition in a way such that the symbols have the meanings that they are supposed to have. End of story.\n\nSo far, all we've done is to make a wish. We've decided, after appropriate deliberation, what to wish for, and we have found a way to represent it in symbols. All that means nothing if we can't create AGI, the problem solver with at least a human level of intelligence. Here again, SIAI comes in for a lot of criticism, from two angles: it's said to have no ideas about how to create AGI, and it's said to actively discourage work on AGI, on the grounds that we need to solve the FAI problem first. Instead, it only discusses hopelessly impractical models of cognition like AIXI and exact Bayesian inference, that are mostly of theoretical interest.\n\nOur practical FAI project has \"solved\" FAI by simply coming to an agreement on what to wish for, and by studying with legalistic care how to avoid pitfalls and loopholes in the finer details of the wish; but what is its approach to the hard technical problem of AGI? The answer is, first of all, heuristics and incremental improvement. Projects like Lenat's Cyc are on the right track. A newborn AI has to be seeded with useful knowledge, including useful knowledge of problem-solving methods. It doesn't have time to discover such things entirely unaided. We should not imagine AGI developing just from a simple architecture, like Schmidhuber's Gödel machine, but from a basic architecture plus a large helping of facts and heuristics which are meant to give it a head start.\n\nSo fine, the practical approach to AGI isn't a search for a single killer concept, it's a matter of incrementally increasing the power of a general-purpose problem solver with many diverse ingredients in its design, so that it becomes more and more capable and independent. Ben Goertzel's approach to AGI exhibits the sort of eclectic pluralism that I have in mind. Still, we do need a selling point, something which shows that we're different, that we're aiming for the stars and we have a plan to get there.\n\nHere, I want to use Steve Omohundro's paper [\"The Basic AI Drives\"](http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/) in a slightly unusual way. The paper lists a number of behaviors that should be exhibited by a sufficiently sophisticated AI: it will try to model its own operation, clarify its goals, protect them from modification, protect itself from destruction, acquire resources and use them efficiently... The twist I propose is that Omohundro's list of drives should be used as a design specification. If your goal is AGI, then you _want_ a cognitive architecture that will exhibit these emergent behaviors. They offer a series of milestones for your theorists and developers: a criterion of progress, and a set of intermediate goals sufficient to bridge the gap between a blank-slate beginning and an open-ended problem solver.\n\nThat's the whole plan. It's an anticlimax, I know, for anyone who might have imagined that there was a magic formula for superintelligence coming at the end of this post. But I do claim that what I have described is the skeleton of a plan which can be fleshed out, and which, if it was fleshed out and pursued, would produce goal-directed AGI. Whether the project as I have described it would really produce \"friendly\" AI is another matter. Anyone versed in the folk wisdom about FAI should be able to point out multiple points of potential failure. But I hope this makes it a little clearer, to people who just don't see how FAI is supposed to happen at all, how it might be pursued in the real world.",
      "plaintextDescription": "I have seen too many discussions of Friendly AI, here and elsewhere (e.g. in comments at Michael Anissimov's blog), detached from any concrete idea of how to do it. Sometimes the issue is the lack of code, demos, or a practical plan from SIAI. SIAI is seen as a source of wishful thinking about magic machines that will solve all our problems for us, or as a place engaged in a forever quest for nebulous mathematical vaporware such as \"reflective decision theory\". You will get singularity enthusiasts who say, it's great that SIAI has given the concept of FAI visibility, but enough with the philosophy, let's get coding! ... does anyone know where to start? And you will get singularity skeptics who say, unfriendly AI is a bedtime ghost story for credulous SF fans, wake me up when SIAI actually ships a product. Or, within this subculture of rationalist altruists who want to do the optimal thing, you'll get people saying, I don't know if I should donate, because I don't see how any of this is supposed to happen.\n\nSo in this post I want to sketch what a \"practical\" plan for Friendly AI looks like. I'm not here to advocate this plan - I'm not saying this is the right way to do it. I'm just providing an example of a plan that could be pursued in the real world. Perhaps it will also allow people to better understand SIAI's indirect approach.\n\nI won't go into the details of financial or technical logistics. If we were talking about how to get to the moon from Earth, then the following plan is along the lines of \"Make a chemical-powered rocket big enough to get you there.\" Once you have that concept, you still have a lot of work to do, but you are at least on the right track - compared to people who want to make a teleportation device or a balloon that goes really high. But I will make one remark about how the idea of Friendly AI is framed. At present, it is discussed in conjunction with a whole cornucopia of science fiction notions such as: immortality, conquering the galaxy, o",
      "wordCount": 1485
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "F75MwixdkpcxHDTsj",
    "title": "Rationality, Singularity, Method, and the Mainstream",
    "slug": "rationality-singularity-method-and-the-mainstream",
    "url": null,
    "baseScore": 52,
    "voteCount": 47,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2011-03-22T12:06:16.404Z",
    "contents": {
      "markdown": "Upon reading [this](/lw/4wm/rationality_boot_camp/), my immediate response was:  \n  \nWhat does this have to do with the Singularity Institute's purpose? You're the Singularity Institute, not the Rationality Institute.  \n  \nI can see that, if you have a team of problem solvers, having a workshop or a retreat designed to enhance their problem-solving skills makes sense. But as described, there's no indication that graduates of the Boot Camp will then go on to tackle conceptual problems of AI design or tactics for the Singularity.  \n  \nWhat seems to be happening is that, instead of making connections to people who know about cognitive neuroscience, decision theory, and the theory of algorithms, there is a drive to increase the number of people who share a particular subjective philosophy and subjective practice of rationality - perhaps out of a belief that the discoveries needed to produce Friendly AI won't be made by people who haven't adopted this philosophy and this practice.  \n  \nI find this a little ominous for several reasons:  \n  \nIt could be a symptom of [mission creep](http://en.wikipedia.org/wiki/Mission_creep). The mission, as I recall, was to design and code a Friendly artificial intelligence. But \"produc\\[ing\\] formidable rationalists\" sounds like it's meant to make the world better in a generalized way, by producing people who can shine the light of rationality into every dark corner, et cetera. Maybe someone should be doing this, but it's potentially a huge distraction from the more important task.  \n  \nAlso, I'm far more impressed by the specific ideas Eliezer has come up with over the years - the concept of seed AI; the concept of Friendly AI; CEV; TDT - than by his ruminations about rationality in the Sequences. They're interesting, yes. It's also interesting to hear Feynman talk about how to do science, or to read Einstein's reflections on life. But the discoveries in physics which complemented those of Einstein and Feynman weren't achieved by people who studied their intellectual biographies and sought to reproduce their subjective method; they were achieved by other people of high intelligence who also studied the physical world.  \n  \nIt may seem at times that the supposed professionals in the FAI-relevant fields I listed above are terminally obtuse, for having to failed to grasp their own relevance to the FAI problem, or the schema of the solution as proposed by SIAI. That, and the way that people working in AI are just sleepwalking towards the creation of superhuman intelligence without grasping that the world won't get a second chance if they get machine intelligence very right but machine values very wrong - all of that could reinforce the attitude that to have any chance of succeeding, SIAI needs to have a group of people who share a subjective methodology, and not just domain expertise.  \n  \nHowever, I think we are rapidly approaching a point where a significant number of people are going to understand that the \"intelligence explosion\" will above all be about the utility function dominating that event. There have been discussions about how a proto-friendly AI might try to infer the human utility-function schema, how to do so without creating large numbers of simulated persons who might be subjected to cognitive vivisection, and so forth. But I suspect that will never happen, at least not in this brute-force fashion, in which whole adult brains might be scanned, simulated, modified and so on, for the purpose of reverse-engineering the human decision architecture.  \n  \nMy expectation is that the presently small fields of machine ethics and neuroscience of morality will grow rapidly and will come into contact, and there will be a distributed research subculture which is consciously focused on determining the optimal AI value system in the light of biological human nature. In other words, there will be human minds trying to answer this question long before anyone has the capacity to direct an AI to solve it. We should expect that before we reach the point of a Singularity, there will be a body of educated public opinion regarding what the ultimate utility function or decision method (for a transhuman AI) should be, deriving from work in those fields which ought to be FAI-relevant but which have yet to engage with the problem. In other words, they _will_ be collectively engaging with the problem before anyone gets to outsource the necessary research to AIs.  \n  \nThe conclusion I draw from this for the present is that there needs to be more preparation for this future circumstance, and less attempt to spread a set of methods intended just to facilitate generalized rationality. People who want to see Friendly AI created need to be ready to talk with researchers in those other fields, who never attended \"Rationality Boot Camp\" but who will nonetheless be independently coming to the threshold of thinking about the FAI problem (perhaps under a different name) and developing solutions to it. When the time comes, there will be a phase transition in academia and R&D, from ignoring the problem to wanting to work on it. The creation of ethical artificial minds is not going to be the work of one startup or one secret military project, working in isolation from mainstream intellectual culture; nor is it a mirage that will hang on the horizon of the future forever. It will happen because of that phase transition, and tens of thousands of people will be working on it, in one way or another. That doesn't mean they all get to be relevant or right, but there will be a pre-Singularity ferment that develops very quickly, and in which certain specific understandings of the people who _did_ labor in isolation on this problem for many years will be surpassed and superseded. People will have ingrained assumptions about the answer to subproblem X or subproblem Y - assumptions to which one will have grown accustomed due to the years of isolation spent trying to solve all subproblems at once - and one must be ready for these answer-schemas to be junked when the time finally arrives that the true experts in that area deign to turn their attention to the subproblem in question.  \n  \nOne other observation about \"lessons in rationality\". Luke [recently posted](/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/) about LW's philosophy as being just a form of \"naturalism\" (i.e. materialism), a view that has already been well-developed by mainstream philosophy, but it was countered that these philosophers have few results to show for their efforts, even if they get the basics right. I think the crucial question, regarding both LW's originality and its efficacy, concerns **method**. It has been demonstrated that there is this other intellectual culture, the naturalistic sector of analytic philosophy, which shares a lot of the basic LW worldview. But are there people \"producing results\" (or perhaps just arriving at opinions) in a way comparable to the way that opinions are being produced here? For example, Will Sawin [suggested](/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3q9l) that LW's epistemic method consists of first imagining how a perfectly rational being would think about a problem. As a method of rationality, this is still very \"subjective\" and \"intuitive\" - it's not as if you're plugging numbers into a Bayesian formula and computing the answer, which remains the idealized standard of rationality here.  \n  \nSo, if someone wants to do some comparative scholarship regarding methods of rationality that already exist out there, an important thing to recognize is that LW's method or practice, whatever it is, is a subjective method. I don't call it subjective in order to be derogatory, but just to point out that it is a method intended to be used by conscious beings, whose practice has to involve conscious awareness, whether through real-time reflection or after-the-fact analysis of behavior and results. The LW method is not an algorithm or a computation in the normal sense, though these non-subjective epistemological ideas obviously play a normative and inspirational role for LW humans trying to \"refine their rationality\". So if there is \"prior art\", if LW's methods have been anticipated or even surpassed somewhere, it's going to be in some tradition, discipline, or activity where the analysis of subjectivity is fairly advanced, and not just one where some calculus of objectivities, like probability theory or computer science, has been raised to a high art.  \n  \nFor that matter, the art of getting the best performance out of the human brain won't just involve analysis; not even analysis of subjectivity is the whole story. The brain spontaneously synthesizes and creates, and one also needs to identify the conditions under which it does so most fluently and effectively.",
      "plaintextDescription": "Upon reading this, my immediate response was:\n\nWhat does this have to do with the Singularity Institute's purpose? You're the Singularity Institute, not the Rationality Institute.\n\nI can see that, if you have a team of problem solvers, having a workshop or a retreat designed to enhance their problem-solving skills makes sense. But as described, there's no indication that graduates of the Boot Camp will then go on to tackle conceptual problems of AI design or tactics for the Singularity.\n\nWhat seems to be happening is that, instead of making connections to people who know about cognitive neuroscience, decision theory, and the theory of algorithms, there is a drive to increase the number of people who share a particular subjective philosophy and subjective practice of rationality - perhaps out of a belief that the discoveries needed to produce Friendly AI won't be made by people who haven't adopted this philosophy and this practice.\n\nI find this a little ominous for several reasons:\n\nIt could be a symptom of mission creep. The mission, as I recall, was to design and code a Friendly artificial intelligence. But \"produc[ing] formidable rationalists\" sounds like it's meant to make the world better in a generalized way, by producing people who can shine the light of rationality into every dark corner, et cetera. Maybe someone should be doing this, but it's potentially a huge distraction from the more important task.\n\nAlso, I'm far more impressed by the specific ideas Eliezer has come up with over the years - the concept of seed AI; the concept of Friendly AI; CEV; TDT - than by his ruminations about rationality in the Sequences. They're interesting, yes. It's also interesting to hear Feynman talk about how to do science, or to read Einstein's reflections on life. But the discoveries in physics which complemented those of Einstein and Feynman weren't achieved by people who studied their intellectual biographies and sought to reproduce their subjective method; they were ach",
      "wordCount": 1450
    },
    "tags": [
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "rxCEqL64YiY45JCoo",
    "title": "Who are these spammers? ",
    "slug": "who-are-these-spammers",
    "url": null,
    "baseScore": 8,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 29,
    "createdAt": null,
    "postedAt": "2011-01-20T09:18:10.037Z",
    "contents": {
      "markdown": "The [proposal](r/discussion/lw/3v1/meta_a_5_karma_requirement_to_post_in_discussion/) for a minimum karma of 5 to post in discussion might solve the current spam problem. Or it might just slow the spammers down. But all this spam is coming from just one source, because it all advertises the same thing - \"Pandora charm bracelets\". In principle, therefore, one might seek a more permanent solution. Doing that, and perhaps even just talking about it, has a danger of its own - what if the spammers call on their friends and colleagues? What if they're egosurfing - checking on the image of their \"brand\" - and run across the discussion? Maybe the wise course of action for an intellectual community trying to have serious discussions, undisturbed, is to do the minimum thing necessary to block the source of noise, but not to [provoke it](http://threatpost.com/en_us/blogs/report-zdnets-danchev-hospitalized-011711).\n\nNonetheless, knowledge is supposed to be power, and it must be possible to discover something of who these spammers are, where they are physically located, what their methods are, what their history is, and what recourse the victims of their harassment have. Just today there have been posts from \"pandorabracelet\", \"pandoracharm\", \"charmthomassabo\", \"pandorabraceletsuk\", and \"PandoraJewelryuk\". There are no links visible in the messages, presumably because their methods aren't quite tuned to the peculiarities of LW's markup syntax. But it turns out that pandora-jewelry.uk.com is an existing domain. [WHOIS information](http://www.ipillion.com/site/pandora-jewelry.uk.com) reveals a Russian IP, a registrant with a fake British address but a Chinese-sounding email address, and an [American registrar](http://icannwiki.com/index.php/Billy_Watenpaugh).\n\nMeanwhile, in case you were wondering: the real Pandora is a [Danish jewelry company](http://en.wikipedia.org/wiki/Pandora_%28jewelry%29). But these spammers are promoting a scam which has nothing to do with the real Pandora. It is unclear to me whether their business model is to sell fakes, to just pocket the money and not send anything at all, or whether they're actually collecting credit-card information - it may be some combination of these - but [the victims speak here](http://www.complaintsboard.com/complaints/pandora-jewelleryukcom-c370689.html).",
      "plaintextDescription": "The proposal for a minimum karma of 5 to post in discussion might solve the current spam problem. Or it might just slow the spammers down. But all this spam is coming from just one source, because it all advertises the same thing - \"Pandora charm bracelets\". In principle, therefore, one might seek a more permanent solution. Doing that, and perhaps even just talking about it, has a danger of its own - what if the spammers call on their friends and colleagues? What if they're egosurfing - checking on the image of their \"brand\" - and run across the discussion? Maybe the wise course of action for an intellectual community trying to have serious discussions, undisturbed, is to do the minimum thing necessary to block the source of noise, but not to provoke it.\n\nNonetheless, knowledge is supposed to be power, and it must be possible to discover something of who these spammers are, where they are physically located, what their methods are, what their history is, and what recourse the victims of their harassment have. Just today there have been posts from \"pandorabracelet\", \"pandoracharm\", \"charmthomassabo\", \"pandorabraceletsuk\", and \"PandoraJewelryuk\". There are no links visible in the messages, presumably because their methods aren't quite tuned to the peculiarities of LW's markup syntax. But it turns out that pandora-jewelry.uk.com is an existing domain. WHOIS information reveals a Russian IP, a registrant with a fake British address but a Chinese-sounding email address, and an American registrar.\n\nMeanwhile, in case you were wondering: the real Pandora is a Danish jewelry company. But these spammers are promoting a scam which has nothing to do with the real Pandora. It is unclear to me whether their business model is to sell fakes, to just pocket the money and not send anything at all, or whether they're actually collecting credit-card information - it may be some combination of these - but the victims speak here.",
      "wordCount": 317
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "y2iPsnzp5KhvtK9zt",
    "title": "Let's make a deal",
    "slug": "let-s-make-a-deal",
    "url": null,
    "baseScore": -22,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 53,
    "createdAt": null,
    "postedAt": "2010-09-23T00:59:43.666Z",
    "contents": {
      "markdown": "At the start of 2010, I resolved to focus as much as possible on singularity-relevant issues. That resolution has produced three ongoing projects:\n\n*   work on a paper\n*   study of string theory\n*   investigation of academic options\n\nAs I put it [the other day](/lw/2q7/open_thread_september_2010_part_2/2njj?c=1), the paper is about \"[CEV](http://en.wikipedia.org/wiki/Coherent_Extrapolated_Volition), adapted to [whatever the true ontology is](/lw/1b1/how_to_get_that_friendly_singularity_a_minority/)\". I have ideas about how CEV should work, and about what the true ontology is, and about the adjustments that the latter might require. These ideas are tentative, and open to correction, and the objective is to find out the facts, not just to insist on an opinion. Indeed, I would be open to hearing that I ought to be working on something else, if I want to attain maximum relevance to the AI era. But for now, I have my plan, and I take it seriously as a blueprint for what I should be doing.\n\nThe relevance of string theory might seem questionable. But it matters for physical ontology and for epistemology of physics \\[**ETA**: which matters for general epistemology and hence for AGI\\]. String theory is also a crossroads for many topics central to pure mathematics, such as algebraic geometry, and their techniques are relevant for many other fields, even discrete ones like computer science. In the theory of complexity classes, there is a [self-referential barrier](http://www.scottaaronson.com/papers/pnp.pdf) to proving that P is distinct from NP. There is a [deep proposal](http://arxiv.org/abs/1009.0246) to overcome it by transposing the problem to the domain of algebraic geometry, and I've just begun to consider whether a similar method might illuminate problems like self-enhancement, utility function discovery, and utility function renormalization (for concreteness, I plan to work with [decision field theory](/lw/1qk/applying_utility_functions_to_humans_considered/)). Also, if I can speak string, maybe I can recruit some of those big brains to the task of FAI.\n\n\"Investigation of academic options\" should be self-explanatory. A university is one of the few places where you might be able to work full-time on matters like these. Unfortunately, this outcome continues to elude me. So while I set about whipping up a stew of private microloans and casual work so as to keep a roof over my head, it's time for me to try the Internet option as well.\n\nI find that life costs me AUD$1000/month (AUD is [currency code](http://en.wikipedia.org/wiki/ISO_4217) for \"Australian dollars\"). I'd do better with more, but that's my minimum budget, the lower bound below which bad things will happen. So that's also the conversion rate from \"money\" to \"free time\".\n\nI figure that there are three basic forms of cash transaction: gifts, payments, and loans. A gift is unconditional. A payment is traded for services rendered. A loan is a temporary increase in a person's capital that has to be returned. These categories are not entirely distinct: for example, a payment refunded (because the service wasn't performed) ends up having functioned as a loan.\n\nI am interested in all three of these things. The brittle arrangement which allows me to speak to you this way does not presently extend to me owning a laptop or having easy access to Skype, but I do have a phone, so the adventurous can call me on +61 0406 979788. (I'm in the eastern Australian timezone.) My email is mporter at gmail.com, and I have a Paypal account under that address.",
      "plaintextDescription": "At the start of 2010, I resolved to focus as much as possible on singularity-relevant issues. That resolution has produced three ongoing projects:\n\n * work on a paper\n * study of string theory\n * investigation of academic options\n\nAs I put it the other day, the paper is about \"CEV, adapted to whatever the true ontology is\". I have ideas about how CEV should work, and about what the true ontology is, and about the adjustments that the latter might require. These ideas are tentative, and open to correction, and the objective is to find out the facts, not just to insist on an opinion. Indeed, I would be open to hearing that I ought to be working on something else, if I want to attain maximum relevance to the AI era. But for now, I have my plan, and I take it seriously as a blueprint for what I should be doing.\n\nThe relevance of string theory might seem questionable. But it matters for physical ontology and for epistemology of physics [ETA: which matters for general epistemology and hence for AGI]. String theory is also a crossroads for many topics central to pure mathematics, such as algebraic geometry, and their techniques are relevant for many other fields, even discrete ones like computer science. In the theory of complexity classes, there is a self-referential barrier to proving that P is distinct from NP. There is a deep proposal to overcome it by transposing the problem to the domain of algebraic geometry, and I've just begun to consider whether a similar method might illuminate problems like self-enhancement, utility function discovery, and utility function renormalization (for concreteness, I plan to work with decision field theory). Also, if I can speak string, maybe I can recruit some of those big brains to the task of FAI.\n\n\"Investigation of academic options\" should be self-explanatory. A university is one of the few places where you might be able to work full-time on matters like these. Unfortunately, this outcome continues to elude me. So while I set about",
      "wordCount": 550
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "9eSrEqKEmbEw5jmWY",
    "title": "Positioning oneself to make a difference",
    "slug": "positioning-oneself-to-make-a-difference",
    "url": null,
    "baseScore": 7,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 52,
    "createdAt": null,
    "postedAt": "2010-08-18T23:54:38.901Z",
    "contents": {
      "markdown": "Last weekend, while this year's Singularity Summit took place in San Francisco, I was turning 40 in my Australian obscurity. 40 is old enough to be thinking that I should just pick a [SENS research theme](http://www.sens.org/sens-research/research-themes) and work on it, and also move to wherever in the world is most likely to have the best future biomedicine (that might be Boston). But at least since the late 1990s, when Eliezer first showed up, I have perceived that superintelligence trumps life extension as a futurist issue. And since 2006, when I first grasped how something like CEV could be an _answer_ to the problem of superintelligence, I've had it before me as a model of how the future could and should play out. I have \"contrarian\" ideas about how consciousness works, but they do not contradict any of the essential notions of seed AI and friendly AI; they only imply that those notions would need to be adjusted and fitted to the true ontology, whatever that may be.\n\nSo I think this is what I should be working on - not just the ontological subproblem, but all aspects of the problem. The question is, how to go about this. At the moment, I'm working on a lengthy statement of how I think a Friendly Singularity could be achieved - a much better version of [my top-level posts here](/user/Mitchell_Porter/submitted/), along with new material. But the main \"methodological\" problem is economic and perhaps social - what can I live on while I do this, and where in the world and in society should I situate myself for maximum insight and productivity. That's really what this post is about.\n\nThe obvious answer is, apply to SIAI. I'm not averse to the idea, and on occasion I raise the possibility with them, but I have two reasons for hesitation.\n\nThe first is the problem of consciousness. I often talk about this in terms of vaguely specified ideas about quantum entanglement in the brain, but the really important part is the radical disjunction between the physical ontology of the natural sciences and the manifest nature of consciousness. I cannot emphasize enough that this is a huge gaping hole in the scientific understanding of the world, the equal of any gap in the scientific worldview that came before it, and that the standard \"scientific\" way of thinking about it is a form of property dualism, even if people won't admit this to themselves. All the quantum stuff you hear from me is just an idea about how to restore a type of monism. I actually think it's a conservative solution to a very big problem, but to believe that you would have to agree with me that the other solutions on offer can't work (as well as understanding just what it is that I propose instead).\n\nThis \"reason for not applying to SIAI\" leads to two sub-reasons. First, I'm not sure that the SIAI intellectual environment can accommodate my approach. Second, the problem with consciousness is of course not specific to SIAI, it is a symptom of the overall scientific zeitgeist, and maybe I should be working _there_, in the field of consciousness studies. If expert opinion changes, SIAI will surely notice, and so I should be trying to convince the neuroscientists, not the Friendly AI researchers.\n\nThe second top-level reason for hesitation is simply that SIAI doesn't have much money. If I can accomplish part of the shared agenda while supported by other means, that would be better. Mostly I think in terms of doing a PhD. A few years back I almost started one with Ben Goertzel as co-supervisor, which would have looked at implementing a CEV-like process in a toy physical model, but that fell through at my end. Lately I'm looking around again. In Australia we have David Chalmers and Marcus Hutter. I know Chalmers from my quantum-mind days in Arizona ten years ago, and I met with Hutter recently. The strong interdisciplinarity of my real agenda makes it difficult to see where I could work directly on the central task, but also implies that there are many fields (cognitive neuroscience, decision theory, various quantum topics) where I might be able to limp along with partial support from an institution.\n\nSo that's the situation. Are there any other ideas? (Private communications can go to mporter at gmail.)",
      "plaintextDescription": "Last weekend, while this year's Singularity Summit took place in San Francisco, I was turning 40 in my Australian obscurity. 40 is old enough to be thinking that I should just pick a SENS research theme and work on it, and also move to wherever in the world is most likely to have the best future biomedicine (that might be Boston). But at least since the late 1990s, when Eliezer first showed up, I have perceived that superintelligence trumps life extension as a futurist issue. And since 2006, when I first grasped how something like CEV could be an answer to the problem of superintelligence, I've had it before me as a model of how the future could and should play out. I have \"contrarian\" ideas about how consciousness works, but they do not contradict any of the essential notions of seed AI and friendly AI; they only imply that those notions would need to be adjusted and fitted to the true ontology, whatever that may be.\n\nSo I think this is what I should be working on - not just the ontological subproblem, but all aspects of the problem. The question is, how to go about this. At the moment, I'm working on a lengthy statement of how I think a Friendly Singularity could be achieved - a much better version of my top-level posts here, along with new material. But the main \"methodological\" problem is economic and perhaps social - what can I live on while I do this, and where in the world and in society should I situate myself for maximum insight and productivity. That's really what this post is about.\n\nThe obvious answer is, apply to SIAI. I'm not averse to the idea, and on occasion I raise the possibility with them, but I have two reasons for hesitation.\n\nThe first is the problem of consciousness. I often talk about this in terms of vaguely specified ideas about quantum entanglement in the brain, but the really important part is the radical disjunction between the physical ontology of the natural sciences and the manifest nature of consciousness. I cannot emphasize enough ",
      "wordCount": 718
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Rq5HS5JwjdxaLeR2B",
    "title": "Consciousness",
    "slug": "consciousness",
    "url": null,
    "baseScore": 8,
    "voteCount": 55,
    "viewCount": null,
    "commentCount": 232,
    "createdAt": null,
    "postedAt": "2010-01-08T12:18:39.776Z",
    "contents": {
      "markdown": "(**ETA**: I've created three threads - [color](/lw/1ly/consciousness/1fca), [computation](/lw/1ly/consciousness/1fcb), [meaning](/lw/1ly/consciousness/1fcc) \\- for the discussion of three questions posed in this article. If you are answering one of those specific questions, please answer there.)\n\nI don't know how to make this about rationality. It's an attack on something which is a standard view, not only here, but throughout scientific culture. Someone else can do the metalevel analysis and extract the rationality lessons.\n\nThe local worldview reduces everything to some combination of physics, mathematics, and computer science, with the exact combination depending on the person. I think it is manifestly the case that this does not work for consciousness. I took this line before, but people struggled to understand my own speculations and this complicated the discussion. So the focus is going to be much more on what other people think - like you, dear reader. If you think consciousness can be reduced to some combination of the above, here's your chance to make your case.\n\nThe main exhibits will be _color_ and _computation_. Then we'll talk about reference; then time; and finally the \"unity of consciousness\".\n\nColor was an issue last time. I ended up going back and forth fruitlessly with several people. From my perspective it's very simple: where is the color in your theory? Whether your physics consists of fields and particles in space, or flows of amplitude in configuration space, or even if you think reality consists of \"mathematical structures\" or Platonic computer programs, or _whatever_ \\- I don't see anything _red_ or _green_ there, and yet I do see it right now, here in reality. So if you intend to tell me that reality consists solely of physics, mathematics, or computation, you need to tell me where the colors are.\n\nOccasionally someone says that red and green are just words, and they don't even mean the same thing for different cultures or different people. True. But that's just a matter of classification. It's a fact that the individual shades of color exist, however it is that we group them - and your ontology must contain them, if it pretends to completeness.\n\nThen, there are various other things which have some relation to color - the physics of surface reflection, or the cognitive neuroscience of color attribution. I think we all agree that the first doesn't matter too much; you don't even need blue light to see blue, you just need the right nerves to fire. So the second one seems a lot more relevant, in the attempt to explain color using the physics we have. Somehow the answer lies in the brain.\n\nThere is one last dodge comparable to focusing on color words, namely, focusing on color-related cognition. Explaining why you say the words, explaining why you categorize the perceived object as being of a certain color. We're getting closer here. The explanation of color, if there is such, clearly has a close connection to those explanations.\n\nBut in the end, either you say that blueness is there, or it is not there. And if it is there, at least \"in experience\" or \"in consciousness\", then something somewhere _is blue_. And all there is in the brain, according to standard physics, is a bunch of particles in various changing configurations. So: where's the blue? What is the blue thing?\n\nI can't answer that question. At least, I can't answer that question for you if you hold with orthodoxy here. However, I have noticed maybe three orthodox approaches to this question.\n\nFirst is faith. I don't understand how it could be so, but I'm sure one day it will make sense.\n\nSecond, puzzlement plus faith. I don't understand how it could be so, and I agree that it really really looks like an insurmountable problem, but we overcame great problems in the past without having to overthrow the whole of science. So maybe if we stand on our heads, hold our breath, and think different, one day it will all make sense.\n\nThird, dualism that doesn't notice it's dualism. This comes from people who think they have an answer. The blueness _is_ the pattern of neural firing, or the von Neumann entropy of the neural state compared to that of the light source, or some other particular physical entity or property. If one then asks, okay, if you say so, but where's the blue... the reactions vary. But a common theme seems to be that blueness is a \"feel\" somehow \"associated\" with the entity, or even associated with being the entity. To see blue is how it feels to have your neurons firing that way.\n\nThis is the dualism which doesn't know it's dualism. We have a perfectly sensible and precise physical description of neurons firing: ions moving through macromolecular gateways in a membrane, and so forth. There's no end of things we can say about it. We can count the number of ions in a particular spatial volume, we can describe how the electromagnetic fields develop, we can say that this was caused by that... But you'll notice - nothing about feels. When you say that this feels like something, you're introducing a whole new property to the physical description. Basically, you're constructing a dual-aspect materialism, just like David Chalmers proposed. Technically, you're a property dualist rather than a substance dualist.\n\nNow dualism is supposed to be beyond horrible, so what's the alternative? You can do a Dennett and deny that anything is really blue. A few people go there, but not many. If the blueness does exist, and you don't want to be a dualist, and you want to believe in existing physics, then you have to conclude that blueness is what the physics was about all along. We represented it to ourselves as being about little point-particles moving around in space, but all we ever actually had was mathematics and correct predictions, so it must be that some part of the mathematics was actually talking about blueness - real blueness - all along. Problem solved!\n\nExcept, it's rather hard to make this work in detail. Blueness, after all, does not exist in a vacuum. It's part of a larger experience. So if you take this path, you may as well say that _experiences_ are real, and part of physics must have been describing them all along. And when you try to make some part of physics look like a whole experience - well, I won't say the m word here. Still, this _is_ the path I took, so it's the one I endorse; it just leads you a lot further afield than you might imagine.\n\nNext up, _computation_. Again, the basic criticism is simple, it's the attempt to rationalize things which makes the discussion complicated. People like to attribute computational states, not just to computers, but to the brain. And they want to say that thoughts, perceptions, etc., consist of being in a certain computational state. But a physical state does not correspond inherently to any one computational state.\n\nThere's also a problem with semantics - saying that the state is _about_ something - which I will come to in due course. But first up, let's just look at the problems involved in attributing a _non-referential_ \"computational state\" to a physical entity. \n\nPhysically speaking, an object, like a computer or a brain, can be in any of a large number of exact microphysical states. When we say it is in a computational state, we are grouping those microphysically distinct states together and saying, every state in this group corresponds to the same abstract high-level state, every microphysical state in this other group corresponds to some other abstract high-level state, and so on. But there are many many ways of grouping the states together. Which clustering is the true one, the one that corresponds to cognitive states? Remember, the orthodoxy is functionalism: low-level details don't matter. To be in a particular cognitive state is to be in a particular computational state. But if the \"computational state\" of a physical object is an observer-dependent attribution rather than an intrinsic property, then how can my thoughts be brain states?\n\nWe didn't have this discussion before, so I won't try to anticipate the possible defenses of functionalism. No-one will be surprised, I suppose, to hear that I don't believe this either. Instead, I deduce from this problem that functionalism is wrong. But here's your chance, functionalists: tell the world the one true state-clustering which tells us _the_ computation being implemented by a physical object!\n\nI promised a problem with semantics too. Again I think it's pretty simple. Even if we settle on the One True Clustering of microstates - each such macrostate is still just a region of a physical configuration space. Thoughts have semantic content, they are \"about\" things. Where's the aboutness?\n\nI also promised to mention time and unity-of-consciousness in conclusion. Time I think offers another outstanding example of the will to deny an aspect of conscious experience (or rather, to call it an illusion) for the sake of insisting that reality conforms entirely to a particular scientific ontology. Basically, we have a physics that spatializes time; we can visualize a space-time as a static, completed thing. So time in the sense of flow - change, process - isn't there in the model; but it appears to be there in reality; therefore it is an illusion.\n\nWithout trying to preempt the debate about time, perhaps you can see by now why I would be rather skeptical of attempts to deny the obvious for the sake of a particular scientific ontology. Perhaps it's not actually necessary. Maybe, if someone thinks about it hard enough, they can come up with an ontology in which time is real and \"flows\" after all, and which still gives rise to the right physical predictions. (In general relativity, a world-line has a local time associated with it. So if the world-line is that of an actually and persistently existing object, perhaps time can be real and flowing _inside_ the object... in some sense. That's my suggestion.)\n\nAnd finally, unity of consciousness. In the debate over physicalism and consciousness, the discussion usually doesn't even get this far. It gets stuck on whether the individual \"qualia\" are real. But they do actually form a whole. All this stuff - color, meaning, time - is drawn from that whole. It is a real and very difficult task to properly characterize that whole: not just what its ingredients are, but how they are joined together, what it is that makes it a whole. After all, that whole is your life. Nonetheless, if anyone has come this far with me, perhaps you'll agree that it's the ontology of the subjective whole which is the ultimate challenge here. If we are going to say that a particular ontology is the way that reality is, then it must not only contain color, meaning, and time, it has to contain that subjective whole. In phenomenology, the standard term for that whole is the \"lifeworld\". Even cranky mistaken reductionists have a lifeworld - they just haven't noticed the inconsistencies between what they believe and what they experience. The ultimate challenge in the science of consciousness is to get the ontology of the lifeworld right, and then to find a broader scientific ontology which contains the lifeworld ontology. But first, as difficult as it may seem, we have to get past the partial ontologies which, for all their predictive power and their seductive exactness, just can't be the whole story.",
      "plaintextDescription": "(ETA: I've created three threads - color, computation, meaning - for the discussion of three questions posed in this article. If you are answering one of those specific questions, please answer there.)\n\nI don't know how to make this about rationality. It's an attack on something which is a standard view, not only here, but throughout scientific culture. Someone else can do the metalevel analysis and extract the rationality lessons.\n\nThe local worldview reduces everything to some combination of physics, mathematics, and computer science, with the exact combination depending on the person. I think it is manifestly the case that this does not work for consciousness. I took this line before, but people struggled to understand my own speculations and this complicated the discussion. So the focus is going to be much more on what other people think - like you, dear reader. If you think consciousness can be reduced to some combination of the above, here's your chance to make your case.\n\nThe main exhibits will be color and computation. Then we'll talk about reference; then time; and finally the \"unity of consciousness\".\n\nColor was an issue last time. I ended up going back and forth fruitlessly with several people. From my perspective it's very simple: where is the color in your theory? Whether your physics consists of fields and particles in space, or flows of amplitude in configuration space, or even if you think reality consists of \"mathematical structures\" or Platonic computer programs, or whatever - I don't see anything red or green there, and yet I do see it right now, here in reality. So if you intend to tell me that reality consists solely of physics, mathematics, or computation, you need to tell me where the colors are.\n\nOccasionally someone says that red and green are just words, and they don't even mean the same thing for different cultures or different people. True. But that's just a matter of classification. It's a fact that the individual shades of color exist, ",
      "wordCount": 1893
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RKEepbECEXQXqXFNm",
    "title": "How to think like a quantum monadologist",
    "slug": "how-to-think-like-a-quantum-monadologist",
    "url": null,
    "baseScore": -17,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 268,
    "createdAt": null,
    "postedAt": "2009-10-15T09:37:33.643Z",
    "contents": {
      "markdown": "Half the responses to my last article focused on the subject of consciousness, understandably so. Back when LW was still part of OB, I stated my views in more detail (e.g. [here](/lw/qe/do_scientists_already_know_this_stuff/k52), [here](/lw/p7/zombies_zombies/u28), [here](/lw/p7/zombies_zombies/u2a), and [here](/lw/ps/where_physics_meets_experience/jo3)); and I also think it's just obvious, once you allow yourself to notice, that the physics we have does not even contain the everyday phenomenon of color, so something has to change. However, it also seems that people won't change their minds until a concrete alternative to physics-as-usual and _de facto_ property dualism actually comes along. Therefore, I have set out to explain how to think like a quantum monadologist, which is what I will call myself.  \n  \nFortunately, this new outlook agrees with the existing outlook far more than it disagrees. For example, even though I'm a quantum monadologist, I'm still seeking to identify the self and its experiences with some part of the physical brain. And I'm not seeking to add big new annexes to the physical formalism that we have, just in order to house the mind; though I may feel the need to impose a certain structure on that formalism, for ontological reasons, and that may or may not have empirical consequences in the macro-quantum realm.  \n  \nSo what _are_ the distinctive novelties of this other approach to the problem? There is an ontological hypothesis, that conscious states are states of a single physical entity, which we may call the self. There is a preferred version of the quantum formalism, in which the world is described by quantum jumps between spacelike tensor products of abstract quantum states (more on this below). The self is represented by one of the tensor factors appearing in these products. There is an inversion of attitude with respect to the mathematical formalism; we do not say that the self is actually a vector in a Hilbert space, we say that the nature of the self is as revealed by phenomenology, and the mathematics is just a way of describing its structure and dynamics. Finally, it is implied that significant quantum effects are functionally relevant to cognition, though so far this tells us nothing about where or how.  \n  \n_Quantum Jumps Between Tensor Products?_  \n  \nFor this audience, I think it's best that I start by explaining the quantum formalism I propose, even though the formalism has been chosen solely to match the ontology. I will assume familiarity with the basics of quantum mechanics, including superposition, entanglement, and the fact that we only ever see one outcome, even though the wavefunction describes many.  \n  \nSuppose we have three qubits, allegedly in a state like |011> + |101> + |110>. In a many-worlds interpretation, we suppose that all three components are equally real. In a one-world interpretation, we normally assume that reality is just one of the three, e.g. |011>, which can be expanded as |0> x |1> x |1>: the first qubit is _actually_ in the 0 state, the second and third qubits in the 1 state.  \n  \nHowever, we may, with just as much mathematical validity, express the original state as {|01>+|10>}|1> + |110>. If we look at that first term, how many _things_ are present in it? If the defining property of a thing is that it has a state of its own, then we only have two things, and not three, because two of our qubits are entangled and don't have independent states. It is logically possible to have a one-world interpretation according to which there are two things actually there - one with quite a few degrees of freedom, in the state |01>+|10>, and the other in the much simpler state |1> (and with |110> being unreal, an artefact of the Schrodinger formalism, as must be all the unreal \"branches\" and \"worlds\" according to any single-world interpretation).  \n  \nAnd there you have it. This is, in its essence, the quantum formalism or quantum interpretation I want to use, as a neo-monadologist. At any time, the universe consists of a number of entities whose formal states inhabit Hilbert spaces of various dimension (thus |01>+|10> comes from a four-dimensional Hilbert space, while |1> comes from a two-dimensional Hilbert space), and the true dynamics consists of repeatedly jumping from one such set of entity-states to another set of entity-states. Models like this exist [in the physics literature](http://arxiv.org/abs/hep-th/0302111) (see especially Figure 1; you may think of the points as qubits, and the ovals around them as indicating potential entanglement). For those who think in terms of \"collapse interpretations\", this may be regarded as a \"partial collapse theory\" in which most things, at any given time, are completely disentangled; actually realized entanglements are relatively local and transient. However, from the monadological perspective, we want to get away from the idea of entanglement, somewhat. We don't want to think of this as a world in which there are two entangled qubits and one un-entangled qubit, but rather a world in which there is one monad with four degrees of freedom, and another monad with two degrees of freedom. (The degrees of freedom correspond to the number of complex amplitudes required to specify the quantum state.)  \n  \n_The Actual Ontology of the Self and Its Relationship to the Formalism_  \n  \nI've said that the self, the entity which you are and which is experiencing what you experience, is to be formally represented by one of these tensor factors; like |01>+|10>, though much much bigger. But I've also said that this is still just formalism; I'm not saying that the actual state of the self consists of a vector in a Hilbert space or a big set of complex numbers. So what is the actual state of the self, and how does it relate to the mathematics?  \n  \nThe actual nature of the self I take to be at least partly revealed by phenomenology. You are, when awake, experiencing sensations; and you are experiencing them _as_ something - there is a conceptual element to experience. Thoughts and emotions also, I think, conform somewhat to this dual description; there is an aspect of veridical awareness, and an aspect of conceptual projection. If we adopt [Husserl](http://en.wikipedia.org/wiki/Edmund_Husserl)'s quasi-Cartesian method of investigating consciousness - neither believing in that which is not definitely there, nor outright rejecting any of the stream of suppositions which make up the conceptual side of experience - we find that a specific consciousness, whatever else may be true about it, is partly characterized by this stream of double-sided states: on one side, the \"data\", the \"raw sensations\" and even \"raw thoughts\"; on the other side, the \"interpretation\", all the things which are posited to be true about the data.  \n  \nHusserl says all this much better than I do, and says much more as well, and he has a precise technical vocabulary in which to say it. As phenomenology, what I just wrote is crude and elementary. But I do want to point out one thing, which is that there is a phenomenology of thought and not just a phenomenology of sensation. Because sensations are so noticeable, philosophers of consciousness generally accept that they are there, and that a description of consciousness must include sensations; but there is a tendency (not universal) to regard thought, cognition, as unconscious. I see this as just footdragging on the part of materialist philosophers who have at length been compelled to admit that colors, et cetera, are there, somewhere; if you were setting out to describe your experience without ontological prejudice, of course you would say something about what you think and not just what you sense, and you would say that you have at least partial awareness of what you're thinking.  \n  \nBut this poses a minor ontological challenge. So long as the ontology of consciousness is restricted to sensation, you can get away with saying that the contents of consciousness consist of a visual sensory field in a certain state, an auditory sensory field in another state, and so on through all the senses, and then all of these integrated in a unitary spatiotemporal meta-perception. A thought, however, is a rather different thing; it is something like a consciously apprehended conceptual structure. There are at least two ontological challenges here: what is a \"conceptual structure\", and how does it unite with raw sensory data to produce an interpreted experience, such as an experience of seeing an apple? The philosophers who limit consciousness to raw sensation alone don't face these problems; they can describe concepts and thinking in a purely computational and unconscious fashion. However, in reality there clearly is such a thing as conceptual phenomenology (or else we wouldn't talk about beliefs and thoughts and awareness of them), and the actual ontology of the self must reflect this.  \n  \nA crude way to proceed here, which I introduce more as a suggestion than as the answer, is to distinguish between presence and interpretation as aspects of consciousness. It's almost just terminology; but it's terminology constructed to resemble the reality. So, we say there is a self, whatever that is; everything \"raw\" is \"present\" to that self; and everything with a conceptual element is some raw presence that is being \"interpreted\". And since interpretations are themselves processes occurring within the self, logically they are themselves potentially present to it; and their presence may itself be conceptually interpreted. Thus we have the possibility of iteratively more complex \"higher-order thoughts\", thoughts about thoughts.  \n  \nEnough with the poetics for a moment. Is there a natural _formalism_ for talking about such an entity? It would seem to require a conjunction of qualitative continua and sentential structure. For example, a standard way of talking about the raw visual field specifies hue, saturation, and intensity at every point in that field. But we also want to be able to say that a particular substructure within that field is being \"seen as a square\" or even \"seen as an apple\". We might build up these complex concepts **square** or **apple** combinatorially from a [set of primitive concepts](http://en.wikipedia.org/wiki/Natural_semantic_metalanguage); and then we need a further notation to say that raw sensory structure X is currently being experienced as a Y. I emphasize again that I am not talking about the computation whereby input X is processed or categorized as a Y, but the conscious experience of interpreting sensation X as an object Y. It can be a slippery idea to hold onto, but I maintain that the situation is analogous to how it was with sensation. You can't say that a particular shade of red is _really_ some colorless physical entity; you have to turn it around and say that the entity in your theory, which hitherto you only knew formally and mathematically, is actually a shade of red. And similarly, we are going to have to say that certain states and certain transitions of state, which we only knew formally and computationally, are actually conceptually interpreted perceptions, reflectively driven thought processes, and so forth.  \n  \nReturning to the second part of the question with which we started - how does the actual ontology of the self relate to the quantum mathematics - I have supposed that there is a mapping (maybe not 1-to-1, we may be overlooking other aspects of the self) from states of the self to descriptions of those states in a hybrid qualitative/sentential formalism. The implication is that there is a further mapping from this intermediate formalism into the quantum formalism of Hilbert spaces. This isn't actually so amazing. One way to do it is to have a separate basis state for each state in the intermediate formalism - so the basis states are formally labelled by the qualitative/sentential structures - and to also postulate that superpositions of these basis states never actually show up (as we would be unable to interpret them as states of consciousness). But there may be more subtle ways to do it which take advantage of more of the structure of Hilbert space.  \n  \n_What About Unconscious Matter?_   \n  \nIf I continue to use this terminology of \"monads\" to describe the entities whose quantum states, tensored together, form the formal state of the universe from moment to moment, then my basic supposition is that conscious minds, e.g. as known from within to adult humans, correspond to monads with very many degrees of freedom, and that these are causally surrounded by (and interact with) many lesser monads in simpler, unconscious states. I'm not saying that complexity causes consciousness, but rather that conscious states, on account of having a minimum internal structure of a certain complexity, cannot be found in (say) a two-qubit monad, and that these simple monads make up the vast majority of them in nature.  \n  \nIn fact, this might be an apt moment to say something about the relationship between these \"monads\" and the elementary particles in terms of which physics is normally described. I think of this in terms of string theory; not to be dogmatic about it, but it just concretely illustrates a way of thinking. There is a formulation of string theory in which everything is made up of entangled \"D0-branes\". An individual D0-brane, as I understand it, has just one scalar internal degree of freedom. A particular spatial geometry can be formed by a quantum condensate of D0-branes, and particles in that geometry are themselves individual D0-branes or are lesser condensates (e.g. a string would be, I suppose, a 1-dimensional D0-brane condensate). Living matter is made up of electrons and quarks; but these are themselves just D0-brane composites. So here we have the answer. The D0-branes are the fundamental degrees of freedom - the qubits of nature, so to speak - and their entanglements and disentanglements define the boundaries of the monads.  \n  \n_Abrupt Conclusion_  \n  \nThis is obviously more of a research program than a theory. About a dozen separate instances of handwaving need to be turned into concrete propositions before it has produced an actual theory. The section on how to talk about the actual nature of consciousness without implicitly falling back into the habit of treating the formalism as the reality may seem especially slippery and mystical; but in the end I think it's just another problem we have to face and solve. However, the point of this article is not to carry out the research program, but just to suggest what I'm actually on about. It will be interesting to see how much sense people are able to extract from it.  \n  \nP.S. I will get around to responding to comments from the previous article soon.",
      "plaintextDescription": "Half the responses to my last article focused on the subject of consciousness, understandably so. Back when LW was still part of OB, I stated my views in more detail (e.g. here, here, here, and here); and I also think it's just obvious, once you allow yourself to notice, that the physics we have does not even contain the everyday phenomenon of color, so something has to change. However, it also seems that people won't change their minds until a concrete alternative to physics-as-usual and de facto property dualism actually comes along. Therefore, I have set out to explain how to think like a quantum monadologist, which is what I will call myself.\n\nFortunately, this new outlook agrees with the existing outlook far more than it disagrees. For example, even though I'm a quantum monadologist, I'm still seeking to identify the self and its experiences with some part of the physical brain. And I'm not seeking to add big new annexes to the physical formalism that we have, just in order to house the mind; though I may feel the need to impose a certain structure on that formalism, for ontological reasons, and that may or may not have empirical consequences in the macro-quantum realm.\n\nSo what are the distinctive novelties of this other approach to the problem? There is an ontological hypothesis, that conscious states are states of a single physical entity, which we may call the self. There is a preferred version of the quantum formalism, in which the world is described by quantum jumps between spacelike tensor products of abstract quantum states (more on this below). The self is represented by one of the tensor factors appearing in these products. There is an inversion of attitude with respect to the mathematical formalism; we do not say that the self is actually a vector in a Hilbert space, we say that the nature of the self is as revealed by phenomenology, and the mathematics is just a way of describing its structure and dynamics. Finally, it is implied that significant qu",
      "wordCount": 2466
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3FXzDQ56Z5HdHaqAB",
    "title": "How to get that Friendly Singularity: a minority view",
    "slug": "how-to-get-that-friendly-singularity-a-minority-view",
    "url": null,
    "baseScore": 17,
    "voteCount": 27,
    "viewCount": null,
    "commentCount": 69,
    "createdAt": null,
    "postedAt": "2009-10-10T10:56:46.960Z",
    "contents": {
      "markdown": "_Note: I know this is a rationality site, not a Singularity Studies site. But the Singularity issue is ever in the background here, and the local focus on decision theory fits right into the larger scheme - see below._\n\nThere is a worldview which I have put together over the years, which is basically my approximation to Eliezer's master plan. It's not an attempt to reconstruct every last detail of Eliezer's actual strategy for achieving a Friendly Singularity, though I think it must have considerable resemblance to the real thing. It might be best regarded as Eliezer-_inspired_, or as \"what my Inner Eliezer thinks\". What I propose to do is to outline this quasi-mythical orthodoxy, this tenuous implicit consensus (tenuous consensus because there is in fact a great diversity of views in the world of thought about the Singularity, but implicit consensus because no-one else has a plan), and then state how I think it should be amended. The amended plan is the \"minority view\" promised in my title.\n\n_Elements Of The Worldview_\n\n**There will be strongly superhuman intelligence in the historically immediate future, unless a civilization-ending technological disaster occurs first.**\n\n*   Implicit assumption: problem-solving entities (natural and artificial intelligences, and coalitions thereof) do possess an attribute, their \"general intelligence\", which is both objective and rankable. Theoretical computer science suggests that this is so, but that it takes a lot of conceptual work to arrive at a fully objective definition of general intelligence.\n*   The \"historically immediate future\" may be taken to mean, as an absolute upper bound, the rest of this century. Personally, I find it hard to see how twenty more years can pass without people being able to make planet-killing nanotechnology, so I give it twenty years maximum before we're in the endgame.\n*   I specify _technological_ disaster in the escape clause, because a natural disaster sufficient to end civilization is extremely unlikely on this timescale, and it will require a cultural disruption of that order to halt the progression towards superhuman intelligence.\n\n**In a conflict of values among intelligences, the higher intelligence will win, so for human values / your values to survive after superintelligence, the best chance is for the seed from which the superintelligence grew to have already been \"human-friendly\".**\n\n*   Elementary but very important observation: for at least some classes of intelligence, such as the \"expected-utility maximizer\" (EUM), values or goals are utterly contingent. The component specifying the utility function is independent of the component which solves the problem of maximizing expected utility, and so literally _any_ goal that can be parsed by the problem solver, no matter how absurd, can become its supreme value, just as a calculator will dutifully attempt to evaluate any expression that you throw at it. The contingency of AI core values means that neither utopia nor dystopia (from a human perspective) is guaranteed - though the latter is far more likely, if the values are specified carelessly.\n*   The seed might be an artificial intelligence, modifying itself, or a natural intelligence modifying itself, or some combination of these. But AI is generally considered to have the advantage over natural intelligence when it comes to self-modification. \n\n**The way to produce a human-friendly seed intelligence is to identify the analogue, in the cognitive architecture behind human decision-making, of the utility function of an EUM, and then to \"renormalize\" or \"reflectively idealize\" this, i.e. to produce an ideal moral agent as defined with respect to our species' particular \"utility function\".**\n\n*   Human beings are not EUMs, but we do belong to _some_ abstract class of decision-making system, and there is going to be some component of that system which specifies the goals rather than figuring out how to achieve them. That component is the analogue of the utility function.\n*   This ideal moral agent has to have, not just the right values, but the attribute of superhuman intelligence, if its creation is to constitute a Singularity; and those values have to be stable during the period of self-modification which produces increasing intelligence. The solution of these problems - self-enhancement, and ethical stability under self-enhancement - is also essential for the attainment of a Friendly Singularity. But that is basically a technical issue of computer science and I won't talk further about it.\n\n**The truly fast way to produce a human-relative ideal moral agent is to create an AI with the interim goal of inferring the \"human utility function\" (but with a few safeguards built in, so it doesn't, e.g., kill off humanity while it solves that sub-problem), and which is programmed to then transform itself into the desired ideal moral agent once the exact human utility function has been identified.**\n\n*   Figuring out the human utility function is a problem of empirical cognitive neuroscience, and if our AI really is a potential superintelligence, it ought to be better at such a task than any human scientist. \n*   I am especially going out on a limb in asserting that this final proposition is part of the master plan, though I think traces of the idea can be found in recent writings. But anyway, it's a plausible way to round out the philosophy and the research program; it makes sense if you agree with everything else that came before. It's what my Inner Eliezer thinks.\n\n_Commentary_\n\nThis is, somewhat remarkably, a well-defined research program for the creation of a Friendly Singularity. You could print it out right now and use it as the mission statement of your personal institute for benevolent superintelligence. There are very hard theoretical and empirical problems in there, but I do not see anything that is clearly nonsensical or impossible.\n\nSo what's my problem? Why don't I just devote the rest of my life to the achievement of this vision? There are two, maybe three amendments I would wish to make. What I call **the ontological problem** has not been addressed; **the problem of consciousness**, which is the main subproblem of the ontological problem, is also passed over; and finally, it makes sense to advocate that **human neuroscientists should be trying to identify the human utility function**, rather than simply planning to delegate that task to an AI scientist.\n\nThe problem of ontology and the problem of consciousness can be stated briefly enough: our physics is incomplete, and even worse, our general scientific ontology is incomplete, because inherently and by construction it excludes the reality of consciousness.\n\nThe observation that quantum mechanics, when expressed in a form which makes \"measurement\" an undefined basic concept, does not provide an objective and self-sufficient account of reality, has led on this site to the advocacy of the many-worlds interpretation as the answer. I [recently argued](/lw/19s/why_manyworlds_is_not_the_rationally_favored/) that many worlds is _not_ the clear favorite, to a somewhat mixed response, and I imagine that I will be greeted with almost immovable skepticism if I also assert that the very template of natural-scientific reduction - mathematical physics in all its forms - is inherently inadequate for the description of consciousness. Nonetheless, I do so assert. Maybe I will make the case at greater length in a future article. But the situation is more or less as follows. We have invented a number of abstract disciplines, such as logic, mathematics, and computer science, by means of which we find ourselves able to think in a rigorously exact fashion about a variety of abstract possible objects. These objects constitute the theoretical ontology in terms of which we seek to understand and identify the nature of the actual world. I suppose there is also a minimal \"worldly\" ontology still present in all our understandings of the actual world, whereby concepts such as \"thing\" and \"cause\" still play a role, in conjunction with the truly abstract ideas. But this is how it is if you attempt to literally identify the world with any form of physics that we have, whether it's classical atoms in a void, complex amplitudes stretching across a multiverse configuration space, or even a speculative computational physics, based perhaps on cellular automata or equivalence classes of Turing machines.\n\nHaving adopted such a framework, how does one then understand one's own conscious experience? Basically, through a combination of outright denial with a stealth dualism that masquerades as identity. Thus a person could say, for example, that the passage of time is an illusion (that's denial) and that perceived qualities are just neuronal categorizations (stealth dualism). I call the latter identification a stealth dualism because it blithely asserts that one thing is another thing when in fact they are nothing like each other. Stealth dualisms are unexamined habitual associations of a bit of physico-computational ontology with a bit of subjective phenomenology which allow materialists to feel that the mind does not pose a philosophical problem for them.\n\nMy stance, therefore, is that intellectually we are in a much much worse position, when it comes to understanding consciousness, than most scientists, and especially most computer scientists, think. Not only is it an unsolved problem, but we are trying to solve it in the wrong way: presupposing the desiccated ontology of our mathematical physics, and trying to fit the diversities of phenomenological ontology into that framework. This is, I submit, entirely the wrong way round. One should instead proceed as follows: I exist, and among my properties are that I experience what I am experiencing, and that there is a sequence of such experiences. If I can free my mind from the assumption that the known classes of abstract object are all that can possibly exist, what sort of entity do I appear to be? Phenomenology - self-observation - thereby turns into an ontology of the self, and if you've done it correctly (I'm not saying this is easy), you have the beginning of a new ontology which by design accommodates the manifest realities of consciousness. The task then becomes to reconstitute or reinterpret the world according to mathematical physics in a way which does not erase anything you think you established in the phenomenological phase of your theory-building.\n\nI'm sure this program can be pursued in a variety of ways. My way is to emphasize the phenomenological unity of consciousness as indicating the ontological unity of the self, and to identify the self with what, in current physical language, we would call a large irreducible tensor factor in the quantum state of the brain. Again, the objective is not to reduce consciousness to quantum mechanics, but rather to reinterpret the formal ontology of quantum mechanics in a way which is not outright inconsistent with the bare appearances of experience. However, I'm not today insisting upon the correctness of my particular approach (or even trying very hard to explain it); only emphasizing my conviction that there remains an incredibly profound gap in our understanding of the world, and it has radical implications for any technically detailed attempt to bring about a human-friendly outcome to the race towards superintelligence. In particular, all the disciplines (e.g. theoretical computer science, empirical cognitive neuroscience) which play a part in cashing out the principles of a Friendliness strategy would need to be conceptually reconstructed in a way founded upon the true ontology.\n\nHaving said all that, it's a lot simpler to spell out the meaning of my other amendment to the \"orthodox\" blueprint for a Friendly Singularity. It is advisable to not just think about how to delegate the empirical task of determining the human utility function to an AI scientist, but also to encourage existing human scientists to tackle this problem. The basic objective is to understand what sort of decision-making system we are. We're not expected utility maximizers; well, what are we then? This is a conceptual problem, though it requires empirical input, and research by merely human cognitive neuroscientists and decision theorists should be capable of producing conceptual progress, which will in turn help us to find the correct concepts which I have merely approximated here in talking about \"utility functions\" and \"ideal moral agents\".\n\nThanks to anyone who read this far. :-)",
      "plaintextDescription": "Note: I know this is a rationality site, not a Singularity Studies site. But the Singularity issue is ever in the background here, and the local focus on decision theory fits right into the larger scheme - see below.\n\nThere is a worldview which I have put together over the years, which is basically my approximation to Eliezer's master plan. It's not an attempt to reconstruct every last detail of Eliezer's actual strategy for achieving a Friendly Singularity, though I think it must have considerable resemblance to the real thing. It might be best regarded as Eliezer-inspired, or as \"what my Inner Eliezer thinks\". What I propose to do is to outline this quasi-mythical orthodoxy, this tenuous implicit consensus (tenuous consensus because there is in fact a great diversity of views in the world of thought about the Singularity, but implicit consensus because no-one else has a plan), and then state how I think it should be amended. The amended plan is the \"minority view\" promised in my title.\n\nElements Of The Worldview\n\nThere will be strongly superhuman intelligence in the historically immediate future, unless a civilization-ending technological disaster occurs first.\n\n * Implicit assumption: problem-solving entities (natural and artificial intelligences, and coalitions thereof) do possess an attribute, their \"general intelligence\", which is both objective and rankable. Theoretical computer science suggests that this is so, but that it takes a lot of conceptual work to arrive at a fully objective definition of general intelligence.\n * The \"historically immediate future\" may be taken to mean, as an absolute upper bound, the rest of this century. Personally, I find it hard to see how twenty more years can pass without people being able to make planet-killing nanotechnology, so I give it twenty years maximum before we're in the endgame.\n * I specify technological disaster in the escape clause, because a natural disaster sufficient to end civilization is extremely unlikely o",
      "wordCount": 1995
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sLLR5u9NJTfL88Spx",
    "title": "Why Many-Worlds Is Not The Rationally Favored Interpretation",
    "slug": "why-many-worlds-is-not-the-rationally-favored-interpretation",
    "url": null,
    "baseScore": 13,
    "voteCount": 40,
    "viewCount": null,
    "commentCount": 101,
    "createdAt": null,
    "postedAt": "2009-09-29T05:22:48.366Z",
    "contents": {
      "markdown": "Eliezer recently posted an essay on [\"the fallacy of privileging the hypothesis\"](http://wiki.lesswrong.com/wiki/Privilege_the_hypothesis). What it's really about is the fallacy of privileging an _arbitrary_ hypothesis. In the fictional example, a detective proposes that the investigation of an unsolved murder should begin by investigating whether a particular, randomly chosen citizen was in fact the murderer. Towards the end, this is likened to the presumption that one particular religion, rather than any of the other existing or even merely possible religions, is especially worth investigating.\n\nHowever, in between the fictional and the supernatural illustrations of the fallacy, we have something more empirical: quantum mechanics. Eliezer writes, as he has previously, that the many-worlds interpretation is _the one_ \\- the rationally favored interpretation, the picture of reality which rationally should be adopted given the empirical success of quantum theory. Eliezer has said this before, and I have argued against it before, back when this site was just part of a blog. This site is about rationality, not physics; and the quantum case is not essential to the exposition of this fallacy. But given the regularity with which many-worlds metaphysics shows up in discussion here, perhaps it is worth presenting a case for the opposition.\n\nWe can do this the easy way, or the hard way. The easy way is to argue that many-worlds is merely _not favored_, because we are nowhere near being able to [locate our hypotheses](http://wiki.lesswrong.com/wiki/Locate_the_hypothesis) in a way which permits a clean-cut judgment about their relative merits. The available hypotheses about the reality beneath quantum appearances are one and all unfinished muddles, and we should let their advocates get on with turning them into exact hypotheses without picking favorites first. (That is, if their advocates can be bothered turning them into exact hypotheses.)\n\nThe hard way is to argue that many-worlds is actually _disfavored_ \\- that we can already say it is unlikely to be true. But let's take the easy path first, and see how things stand at the end.\n\nThe two examples of favoring an arbitrary hypothesis with which we have been provided - the murder investigation, the rivalry of religions - both present a situation in which the obvious hypotheses are homogeneous. They all have the form \"Citizen X did it\" or \"Deity Y did it\". It is easy to see that for particular values of X and Y, one is making an arbitrary selection from a large set of possibilities. This is not the case in quantum foundations. The [well-known interpretations](http://en.wikipedia.org/wiki/Interpretation_of_quantum_mechanics) are extremely heterogeneous. There has not been much of an effort made to express them in a common framework - something necessary if we want to apply Occam's razor in the form of [theoretical complexity](http://wiki.lesswrong.com/wiki/Occam%27s_razor) \\- nor has there been much of an attempt to discern the full \"space\" of possible theories from which they have been drawn - something necessary if we really do wish to avoid privileging the hypotheses we happen to have. Part of the reason is, again, that many of the known options are somewhat underdeveloped as exact theories. They subsist partly on rhetoric and handwaving; they are mathematical vaporware. And it's hard to benchmark vaporware.\n\nIn his [latest article](/lw/19m/privileging_the_hypothesis/), Eliezer presents the following argument:\n\n\"... there \\[is\\] no concrete evidence whatsoever that favors a collapse postulate or single-world quantum mechanics.  But, said Scott, we might encounter _future_ evidence in favor of single-world quantum mechanics, and many-worlds still has the open question of the Born probabilities... There must be a trillion better ways to answer the Born question without adding a collapse postulate...\"\n\nThe basic wrong assumption being made is that quantum superposition _by default_ equals multiplicity - that because the wavefunction in the double-slit experiment has two branches, one for each slit, there must be two of something there - and that a single-world interpretation has to add an extra postulate to this picture, such as a collapse process which removes one branch. But superposition-as-multiplicity really is just another hypothesis. When you use ordinary probabilities, you are not rationally obligated to believe that every outcome exists somewhere; and an electron wavefunction really may be describing a single object in a single state, rather than a multiplicity of them.\n\nA quantum amplitude, being a complex number, is not an ordinary probability; it is, instead, a mysterious quantity from which usable probabilities are derived. Many-worlds says, \"Let's view these amplitudes as realities, and try to derive the probabilities from them.\" But you can go the other way, and say, \"Let's view these amplitudes as derived from the probabilities of a more fundamental theory.\" Mathematical results like Bell's theorem show that this will require a little imagination - you won't be able to derive quantum mechanics as an approximation to a 19th-century type of physics. But we have the imagination; we just need to use it in a disciplined way.\n\nSo that's the kernel of the argument that many worlds is _not favored_: the hypotheses under consideration are still too much of a mess to even be commensurable, and the informal argument for many worlds, quoted above, simply presupposes a multiplicity interpretation of quantum superposition. How about the argument that many worlds is actually _disfavored_? That would become a genuinely technical discussion, and when pressed, I would ultimately not insist upon it. We don't know enough about the theory-space yet. Single-world thinking looks more fruitful to me, when it comes to sub-quantum theory-building, but there are versions of many-worlds which I do occasionally like to think about. So the verdict for now has to be: [not proven](http://en.wikipedia.org/wiki/Not_proven); and meanwhile, [let a hundred schools of thought contend](http://en.wikipedia.org/wiki/Hundred_Flowers_Campaign).",
      "plaintextDescription": "Eliezer recently posted an essay on \"the fallacy of privileging the hypothesis\". What it's really about is the fallacy of privileging an arbitrary hypothesis. In the fictional example, a detective proposes that the investigation of an unsolved murder should begin by investigating whether a particular, randomly chosen citizen was in fact the murderer. Towards the end, this is likened to the presumption that one particular religion, rather than any of the other existing or even merely possible religions, is especially worth investigating.\n\nHowever, in between the fictional and the supernatural illustrations of the fallacy, we have something more empirical: quantum mechanics. Eliezer writes, as he has previously, that the many-worlds interpretation is the one - the rationally favored interpretation, the picture of reality which rationally should be adopted given the empirical success of quantum theory. Eliezer has said this before, and I have argued against it before, back when this site was just part of a blog. This site is about rationality, not physics; and the quantum case is not essential to the exposition of this fallacy. But given the regularity with which many-worlds metaphysics shows up in discussion here, perhaps it is worth presenting a case for the opposition.\n\nWe can do this the easy way, or the hard way. The easy way is to argue that many-worlds is merely not favored, because we are nowhere near being able to locate our hypotheses in a way which permits a clean-cut judgment about their relative merits. The available hypotheses about the reality beneath quantum appearances are one and all unfinished muddles, and we should let their advocates get on with turning them into exact hypotheses without picking favorites first. (That is, if their advocates can be bothered turning them into exact hypotheses.)\n\nThe hard way is to argue that many-worlds is actually disfavored - that we can already say it is unlikely to be true. But let's take the easy path first, and",
      "wordCount": 923
    },
    "tags": [
      {
        "_id": "5f5c37ee1b5cdee568cfb1c9",
        "name": "Many-Worlds Interpretation",
        "slug": "many-worlds-interpretation"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]