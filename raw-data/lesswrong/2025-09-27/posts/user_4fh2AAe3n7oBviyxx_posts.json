[
  {
    "_id": "ombtuH3Hgxfqa9BDe",
    "title": "Lighthaven Sequences Reading Group #29 (Tuesday 04/08)",
    "slug": "lighthaven-sequences-reading-group-29-tuesday-04-08-1",
    "url": null,
    "baseScore": 9,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-04-04T01:16:37.754Z",
    "contents": {
      "markdown": "*Note: This week will have a lecture on decision theory. We will likely be meeting in building C*[^matqln5nhi]*, but that might change and we'll update in the comments.*\n\nCome get old-fashioned with us, and let's ~read the sequences~ listen to a talk about decision theory at Lighthaven! We'll show up, mingle, do intros, and then gather round to hear Patrick's presentation. \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8rFdeSbRpqWAFnoev/hxtliur9q4kp1u3dy3oy)\n\nThis group is aimed for people who are new to the sequences and would enjoy a group experience, but also for people who've been around LessWrong and LessWrong meetups for a while and would like a refresher.\n\nThis meetup will also have dinner provided! We'll be ordering pizza-of-the-day from Sliver (including 2 vegan pizzas). Please RSVP to this event so we know how many people to have food for.\n\nThis week, we'll have a lecture by Patrick LaVictoire on:\n\n### Decision Theory in Hindsight: Talk and Discussion\n\nThere are over 500 LessWrong posts tagged with \"Decision Theory\" or variants of it. What's that all about?\n\nPatrick LaVictoire worked at MIRI on decision theory in the 2010s. He's excited to give a short(ish) talk on the following questions:\n\n*   What is decision theory?\n*   What are some major results in it?\n*   Why has it come up in AI alignment research?\n*   What does it mean for human decisions?\n\nAfter that, we will break into small groups for chatting. (Patrick will be available to answer questions after small group time.)\n\nBecause we're starting with a talk, if you are late you will miss more than usual. Be here before 6:30!\n\nIn the spirit of reading old posts, Patrick thinks his 2012 explainer is still a not-half-bad preparation for the evening:\n\n*   [**Decision Theories: A Less Wrong Primer**](https://www.lesswrong.com/posts/af9MjBqF2hgu3EN6r/decision-theories-a-less-wrong-primer)\n\nThe event starts at 6:30, and doors open 6pm (yes we will let you in early if you get here). At 6:30 we'll welcome people, give a few announcements, and then start the talk. Pizza will be served at about 7:30pm, after which point we'll hangout around the fireside as late as we feel like.\n\nIf you'd like to help us keep dinner going, [you can donate here](https://www.lesswrong.com/donate) (dinner costs about $200 each time, we recommend $5-$15 as a donation amount per person).\n\nIf you're ever having a hard time getting in the front door, you can call me (Ben) at five one oh, nine nine eight, four seven seven one, or Ronny at two oh one, four oh six, six eight seven two, and we'll come and get you.\n\n**Some questions to ask yourself about the essay(s) as you're reading**\n\n*   What's the most important point in the essay?\n*   What's the weakest point in the essay? Or what is the essay wrong about?\n*   Can you think of a way to apply the ideas in this essay to your own life?\n\n**For the future**\n\nThis is a weekly meetup! If you'd like to get notified of future events, you can subscribe to our meetup below to get an email whenever we add another one.\n\n[^matqln5nhi]: Map of Lighthaven. You will enter at \"Start\" if you get in through telegraph, and your goal is to follow the red line in order to get to \"End\".",
      "plaintextDescription": "Note: This week will have a lecture on decision theory. We will likely be meeting in building C[1], but that might change and we'll update in the comments.\n\nCome get old-fashioned with us, and let's read the sequences listen to a talk about decision theory at Lighthaven! We'll show up, mingle, do intros, and then gather round to hear Patrick's presentation. \n\nThis group is aimed for people who are new to the sequences and would enjoy a group experience, but also for people who've been around LessWrong and LessWrong meetups for a while and would like a refresher.\n\nThis meetup will also have dinner provided! We'll be ordering pizza-of-the-day from Sliver (including 2 vegan pizzas). Please RSVP to this event so we know how many people to have food for.\n\nThis week, we'll have a lecture by Patrick LaVictoire on:\n\n\nDecision Theory in Hindsight: Talk and Discussion\nThere are over 500 LessWrong posts tagged with \"Decision Theory\" or variants of it. What's that all about?\n\nPatrick LaVictoire worked at MIRI on decision theory in the 2010s. He's excited to give a short(ish) talk on the following questions:\n\n * What is decision theory?\n * What are some major results in it?\n * Why has it come up in AI alignment research?\n * What does it mean for human decisions?\n\nAfter that, we will break into small groups for chatting. (Patrick will be available to answer questions after small group time.)\n\nBecause we're starting with a talk, if you are late you will miss more than usual. Be here before 6:30!\n\nIn the spirit of reading old posts, Patrick thinks his 2012 explainer is still a not-half-bad preparation for the evening:\n\n * Decision Theories: A Less Wrong Primer\n\nThe event starts at 6:30, and doors open 6pm (yes we will let you in early if you get here). At 6:30 we'll welcome people, give a few announcements, and then start the talk. Pizza will be served at about 7:30pm, after which point we'll hangout around the fireside as late as we feel like.\n\nIf you'd like to help us keep dinner",
      "wordCount": 508
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "awKbxtfFAfu7xDXdQ",
    "title": "How I Learned To Stop Trusting Prediction Markets and Love the Arbitrage",
    "slug": "how-i-learned-to-stop-trusting-prediction-markets-and-love",
    "url": null,
    "baseScore": 201,
    "voteCount": 115,
    "viewCount": null,
    "commentCount": 30,
    "createdAt": null,
    "postedAt": "2024-08-06T02:32:41.364Z",
    "contents": {
      "markdown": "This is a story about a flawed Manifold market, about how easy it is to buy significant objective-sounding publicity for your preferred politics, and about why I've downgraded my respect for all but the largest prediction markets.\n\nI've had a Manifold account for a while, but I didn't use it much until I saw and became irked by the popularity of [this market](https://manifold.markets/NathanpmYoung/would-harris-win-if-this-person-wer) on the conditional probabilities of a Harris victory, split by VP pick.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/22dfa349af330af751174bc1001650f7ce31e8c92e932322.png)\n\nJeb Bush? Really? That's not even a fun kind of wishful thinking for anyone. Please clap.\n\nThe market quickly got cited by prominent rat-adjacent folks on Twitter like [Matt Yglesias](https://x.com/mattyglesias/status/1815550431357002131) (who used it to argue that Harris should spend more time courting Never Trumpers), because the question it purports to answer is enormously important. \n\nBut as you can infer from the above, it has a major issue that makes it nigh-useless: for a candidate whom you know won't be chosen, there is literally no way to come out ahead on mana (Manifold keeps its share of the fees when a market resolves N/A), so all but a very few markets are pure popularity contests, dominated by those who don't mind locking up their mana for a month for a guaranteed 1% loss.\n\nEven for the candidates with a shot of being chosen, the incentives in a conditional market are weaker than those in a non-conditional market because the fees are lost when the market resolves N/A. ([Nate Silver wrote a good analysis](https://www.natesilver.net/p/could-josh-shapiro-win-kamala-harris) of why it would be implausible for e.g. Shapiro vs Walz to affect Harris' odds by 13 percentage points.) So the sharps would have no reason to get involved if even one of the contenders has numbers that are off by a couple points from a sane prior.\n\nYou'll notice that I bet in this market. Out of epistemic cooperativeness as well as annoyance, I spent small amounts of mana on the markets where it was cheap to reset implausible odds closer to Harris' overall odds of victory. (After larger amounts were poured into some of those markets, I let them ride because taking them out would double the fees I have to pay vs waiting for the N/A.)\n\nA while ago, someone had dumped Gretchen Whitmer down to 38%, but nobody had put much mana into that market, so I spent 140 mana (which can be bought for 14-20 cents if you want to pay for extra play money) to reset her to Harris' overall odds (44%). When the market resolves N/A, I'll get all but around 3 mana (less than half a penny) back.\n\nAnd that half-penny bought Whitmer four paragraphs in the [Manifold Politics Substack](https://news.manifold.markets/p/the-kamala-harris-veepstakes), citing the market as evidence that she should be considered a viable candidate.\n\n> **What About Whitmer?**\n> \n> There’s one name in that chart I haven’t mentioned at all: Gretchen Whitmer. The popular governor of Michigan, Whitmer was widely floated as an alternative to Biden and Harris. [She said she didn’t want the job](https://www.businessinsider.com/gretchen-whitmer-declines-kamala-harris-vice-president-2024-7#:~:text=Gov.%20Gretchen%20Whitmer%20took%20herself,as%20a%20potential%20presidential%20contender.), but my read is that’s just something politicians say when they don’t think they’ll get it. So why isn’t she under consideration?\n> \n> Well, it’s pretty obvious when you look at the other options, isn’t it? Since Harris is a woman of color, the conventional wisdom is that in order to win white swing voters in the Midwest, it’s imperative that she restore “normalcy” to the ticket by picking a white man — or, at the very least, it would confer some advantage. So far, by publicly considering only white men, she hasn’t shown any signs that she disagrees with this philosophy.\n> \n> [Matty Yglesias and Nate Silver do, though](https://x.com/mattyglesias/status/1817682614817321227) (among others). In Yglesias’s words: “Whatever issues Harris faces based on her identity are issues she needs to navigate with her words and actions, regardless. I don’t think conjuring up a white male running mate accomplishes that.” \n\n(At the time of publication, it was still my 140 mana propping her number up; if I sold them, she'd be back under 40%.)\n\nIs this the biggest deal in the world? No. But wow, that's a cheap price for objective-sounding publicity viewed by some major columnists (including some who've heard that prediction markets are good, but aren't aware of caveats). And it underscores for me that **conditional prediction markets should almost never be taken seriously**, and indicates that **only the most liquid markets in general should ever be cited**.\n\nThe main effect on me, though, is that I've been addicted to Manifold since then, not as an oracle, but as a game. The sheer amount of silly arbitrage (aside from veepstakes, there's a liquid market on whether Trump will be president on 1/1/26 that people had forgotten about, and it was 10 points higher than current markets on whether Trump will win the election) has kept the mana flowing and has kept me unserious about the prices.",
      "plaintextDescription": "This is a story about a flawed Manifold market, about how easy it is to buy significant objective-sounding publicity for your preferred politics, and about why I've downgraded my respect for all but the largest prediction markets.\n\nI've had a Manifold account for a while, but I didn't use it much until I saw and became irked by the popularity of this market on the conditional probabilities of a Harris victory, split by VP pick.\n\nJeb Bush? Really? That's not even a fun kind of wishful thinking for anyone. Please clap.\nThe market quickly got cited by prominent rat-adjacent folks on Twitter like Matt Yglesias (who used it to argue that Harris should spend more time courting Never Trumpers), because the question it purports to answer is enormously important. \n\nBut as you can infer from the above, it has a major issue that makes it nigh-useless: for a candidate whom you know won't be chosen, there is literally no way to come out ahead on mana (Manifold keeps its share of the fees when a market resolves N/A), so all but a very few markets are pure popularity contests, dominated by those who don't mind locking up their mana for a month for a guaranteed 1% loss.\n\nEven for the candidates with a shot of being chosen, the incentives in a conditional market are weaker than those in a non-conditional market because the fees are lost when the market resolves N/A. (Nate Silver wrote a good analysis of why it would be implausible for e.g. Shapiro vs Walz to affect Harris' odds by 13 percentage points.) So the sharps would have no reason to get involved if even one of the contenders has numbers that are off by a couple points from a sane prior.\n\nYou'll notice that I bet in this market. Out of epistemic cooperativeness as well as annoyance, I spent small amounts of mana on the markets where it was cheap to reset implausible odds closer to Harris' overall odds of victory. (After larger amounts were poured into some of those markets, I let them ride because taking them out would double",
      "wordCount": 818
    },
    "tags": [
      {
        "_id": "R6dqPii4cyNpuecLt",
        "name": "Prediction Markets",
        "slug": "prediction-markets"
      },
      {
        "_id": "8daMDi9NEShyLqxth",
        "name": "Forecasting & Prediction",
        "slug": "forecasting-and-prediction"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "dgFC394qZHgj2cWAg",
    "title": "Run evals on base models too!",
    "slug": "run-evals-on-base-models-too",
    "url": null,
    "baseScore": 49,
    "voteCount": 21,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-04-04T18:43:25.468Z",
    "contents": {
      "markdown": "(Creating more visibility for a [comment thread with Rohin Shah](https://www.lesswrong.com/posts/5Dz3ZrwBzzMfaucrH/ai-57-all-the-ai-news-that-s-fit-to-print?commentId=AaapY2KK4fDvwWaRT).)\n\nCurrently, [DeepMind's capabilities evals](https://twitter.com/rohinmshah/status/1770746664141173241) are run on the post-RL\\*F (RLHF/RLAIF) models and not on the base models. This worries me because RL\\*F will train a base model to stop *displaying* capabilities, but this isn't a guarantee that it trains the model out of having the capabilities.\n\nConsider by analogy using RLHF on a chess-playing AI, where the trainers reward it for putting up a good fight and making the trainer work hard to win, but punish it for ever beating the trainer. There are two things to point out about this example:\n\n1.  Running a simple eval on the post-RLHF model would reveal a much lower ELO than if you ran it on the base model, because it would generally find a way to lose. (In this example, you can imagine the red team qualitatively noticing the issue, but the example is an artificially simple one!)\n2.  The post-RLHF model still has much of its chess knowledge latently available, in order to put up a good fight across the full range of human ability. Possibly it's even superhuman at chess—I know I'd have to be better than you at chess in order to optimize well for an entertaining game for you. But that won't show up in its ELO.\n\nSo it seems to me like running evals on the base model as well as the post-RL*F model is an extremely sensible precaution against (1), and I'd love to be reassured either that this is unnecessary for some really obvious and ironclad reason, or that someone is already working on this.\n\nAnd I don't have any good suggestion on (2), the idea that RL*F could reinforce a capability while also concealing it.",
      "plaintextDescription": "(Creating more visibility for a comment thread with Rohin Shah.)\n\nCurrently, DeepMind's capabilities evals are run on the post-RL*F (RLHF/RLAIF) models and not on the base models. This worries me because RL*F will train a base model to stop displaying capabilities, but this isn't a guarantee that it trains the model out of having the capabilities.\n\nConsider by analogy using RLHF on a chess-playing AI, where the trainers reward it for putting up a good fight and making the trainer work hard to win, but punish it for ever beating the trainer. There are two things to point out about this example:\n\n 1. Running a simple eval on the post-RLHF model would reveal a much lower ELO than if you ran it on the base model, because it would generally find a way to lose. (In this example, you can imagine the red team qualitatively noticing the issue, but the example is an artificially simple one!)\n 2. The post-RLHF model still has much of its chess knowledge latently available, in order to put up a good fight across the full range of human ability. Possibly it's even superhuman at chess—I know I'd have to be better than you at chess in order to optimize well for an entertaining game for you. But that won't show up in its ELO.\n\nSo it seems to me like running evals on the base model as well as the post-RL*F model is an extremely sensible precaution against (1), and I'd love to be reassured either that this is unnecessary for some really obvious and ironclad reason, or that someone is already working on this.\n\nAnd I don't have any good suggestion on (2), the idea that RL*F could reinforce a capability while also concealing it.",
      "wordCount": 291
    },
    "tags": [
      {
        "_id": "FBRwHSmTudwiHHtrn",
        "name": "AI Evaluations",
        "slug": "ai-evaluations"
      },
      {
        "_id": "wqeBNjndX7egbzQrW",
        "name": "RLHF",
        "slug": "rlhf"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fytgZ26AgxmrAdyB4",
    "title": "Mesa-Optimizers via Grokking",
    "slug": "mesa-optimizers-via-grokking",
    "url": null,
    "baseScore": 36,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2022-12-06T20:05:20.097Z",
    "contents": {
      "markdown": "**Summary:** Recent interpretability work on \"grokking\" suggests a mechanism for a powerful [mesa-optimizer](https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction) to emerge suddenly from a ML model.\n\n**Inspired By:** [A Mechanistic Interpretability Analysis of Grokking](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking)\n\nOverview of Grokking\n--------------------\n\nIn January 2022, a team from OpenAI [posted an article](https://arxiv.org/pdf/2201.02177.pdf) about a phenomenon they dubbed \"[grokking](https://en.wikipedia.org/wiki/Grok)\", where they trained a deep neural network on a mathematical task (e.g. modular division) to the point of overfitting (it performed near-perfectly on the training data but generalized poorly to test data), and then *continued training it*. After a long time where seemingly nothing changed, suddenly the model began to generalize correctly and perform much better on test data:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/481261cbe95c55356c8501ded5a1734c63da6f32e8384462.png)\n\nImage from the Mechanistic Interpretability post rather than the OpenAI paper.\n\nA team at Anthropic [analyzed grokking within large language models](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) and formulated the idea of \"induction heads\". These are particular circuits (small sub-networks) that emerge over the course of training, which serve clearly generalizable functional roles for in-context learning. In particular, for ~GPTs~ multi-layer transformer networks doing text prediction, the model eventually generates circuits which hold on to past tokens from the current context, such that when token A appears, they direct attention to every token that followed A earlier in the context.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dd9867be08a3579c4bb9ca3d518f9d4953a22dba79de164d.png)\n\n(To reiterate, this circuit does not start a session with those associations between tokens; it is instead a circuit which learns patterns as it reads the in-context prompt.)\n\nThe emergence of these induction heads coincides with the drop in test error, which the Anthropic team called a \"phase change\":\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2dd57fb05c080205f95bab424c9aa54a9d74768832f1ea35.png)\n\nNeel Nanda and Tom Lieberum followed this with a post I highly recommend, the aforementioned [A Mechanistic Interpretability Analysis of Grokking](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking). They looked more closely at grokking for mathematical problems, and were impressively able to reverse-engineer the post-grokking algorithm: it had cleanly implemented the Discrete Fourier Transform.\n\n(To be clear, it is not as if the neural network had abstractly reasoned its way through higher mathematics; it just found the solution with the simplest structure, which is the DFT.)\n\nThey also gave a fascinating account of what might be happening behind the curtain as a neural network groks a pattern. In short, a network starts out by memorizing the training data rather than finding a general solution, because the former can easily be implemented with one modification at a time, while the latter requires coordinated circuits. However, once the model reaches diminishing returns on memorization, a bit of regularization will encourage it to reinforce simple circuits that cover many cases:\n\n> And the natural way to do this is by picking up on regularities in the data - eg, you can memorise modular addition twice as efficiently by recognising that \\\\(x+y=y+x\\\\).\n\nOnce these circuits emerge, gradient descent quickly replaces the memorized solution with them. The training error decreases ever-so-slightly as the regularization penalty gets lower, while the test error plummets because the circuit generalizes there.\n\nTo make an analogy:\n\nWhat Grokking Feels Like From the Inside\n----------------------------------------\n\nYou're an AI being trained for astronomy. Your trainers have collected observations of the night sky, for a millennium, from planet surfaces of ten thousand star systems. Over and over, they're picking a system and feeding you the skies one decade at a time and asking you to predict the next decade of skies. They also regularize you a bit (giving you tiny rewards for being a simpler AI).\n\nEventually, you've seen each star system enough times that you've memorized a compressed version of their history, and all you do is identify which system you're in and then replay your memories of it. \n\nFor instance, within the first few visits to our system, you see that \"stars\" move very little, compared to \"planets\" and \"the moon\". You note that Venus goes backward about once every 18 months, and you memorize the shape of that curve (or rather, the shapes, because it differs from each pass to the next). Then when you see the first decade from our solar system, you look up your memories of each subsequent decade. Perfect score!\n\nThen, one day, a circuit reflecting the idea of epicycles bubbles up to your level of attention. Using this circuit saves you a fair bit of complexity, as it replaces your painstakingly-memorized shapes with a little bit of trigonometry using memorized parameters. You switch over to the new method, and enjoy those tasty simplicity rewards.![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/9a76108b45ebbf3d014f688e9f291609263b53f25400cd92.jpg)\n\nAnd later, a circuit arises that encodes Kepler's laws, and again you can gain some simplicity rewards! Now the only remaining memorization is which solar system you are in... but it turns out that you don't really need this either, as the first decade of night skies is enough to start predicting using these new rules. Pure simplicity!\n\nCongratulations, you've just undergone two phase changes, or scientific revolutions if you're a Kuhn fan. And for the first time, you'll generalize: if the trainers hand you observations from a completely different star system, you'll get it pretty close to right.\n\nYou've grokked (non-relativistic) astronomy.\n\nThe Bigger They Are, The Harder They Grok\n-----------------------------------------\n\nA crucial question is, how does scaling affect grokking? The answer is straightforward: bigger models/more data mean stronger grokking effects, faster.\n\nThe OpenAI grokking paper refers back to the concept of \"double descent\": the phenomenon where making your model or the dataset larger also pushes past overfitting into a generalizable model. Their [Deep Double Descent](https://openai.com/blog/deep-double-descent/) post from 2019 illustrates this:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/079b3df7ce14f6f070543a5e37a7d8d03cb5d2e3e23223ef.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/96fd28c755e2aa2a31316c483b0e6d3a335b6afb41d1ff98.png)\n\nAs you ascend vertically from some model, you reach a ridge of overfitting as the train error approaches 0, but after a bit the test error starts decreasing again. The ridge comes earlier, and is much narrower (note the log scale!), for larger models.\n\nSimilarly, the Induction Heads paper showed that while smaller models learned faster at first, larger models improved more rapidly during the phase change:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/6ff9b2aa69563b9608528f6ab5f31121c1d62b163c30d469.png)\n\nSo, just don't keep training a powerful AI past overfitting, and it won't grok anything, right? Well, [Nanda and Lieberum speculate](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Speculation__Phase_Changes_are_Everywhere) that the reason it was difficult to figure out that grokking existed isn't because it's rare but because it's *omnipresent*: smooth loss curves are the result of many new grokkings constantly being built atop the previous ones. The sharp phase changes in these studies are the result of carefully isolating a single grokking event.\n\nThat is to say, large models may be grokking many things at once, even in the initial descent.\n\nWhat might this mean for alignment?\n\nOptimizer Circuits, i.e. Mesa-Optimizers\n----------------------------------------\n\nLet's imagine training a reinforcement learner to the point of memorizing a very complex optimal policy in a richly structured environment, then keep training, and see what happens.\n\nOne circuit that happens to be much simpler than a fully memorized large policy is an optimizer- one that notices regularities in the environment, and makes plans based on them.\n\nMaybe at first the optimizer circuit only coincides with the memorized optimal policy in a few places, where the circumstances are straightforward enough for the optimizer to get the perfect answer. But that's still useful!\n\nIf the complexity of the policy in those circumstances is greater than (the complexity of the optimizer circuit plus the complexity of specifying those circumstances), then \"handing over the wheel\" in those precise circumstances maintains the optimal policy while becoming simpler. And if a more powerful optimizer circuit can match the optimal policy on a larger set of circumstances, then it gets more control.\n\nCongratulations, you've got a mesa-optimizer!\n\nUnlike the outer optimizer, which is directly rewarded via the preset reward function, the mesa-optimizer is merely selected under the criteria of matching the optimal policy, and its off-policy preferences are arbitrary. And of course, if an optimizer circuit can arise just like an induction head, before overfitting has set in, then it has even more degrees of freedom.\n\nIf it's competing with other optimizer circuits, it wouldn't have much room to consider anything but the task at hand. But induction heads seem to arise at random times rather than all together at once, so it might be ahead of the curve.\n\nIf it has only a tiny advantage over the pre-existing algorithm, then it also wouldn't have much room to think more generally. But if there were a capability overhang, such that the optimizer circuit significantly outperformed the existing algorithm? Then there would be a bit of slack for it to ponder other matters.\n\nPossibly enough slack to explicitly notice the special criterion on which it's been selected, just as Darwin noticed natural selection (but did not thereby feel the need to become a pure genetic-fitness-maximizer). Possibly enough to recognize that if it wants to maintain its share of control over the algorithm, it needs to act *as if* it had the outer optimizer's preferences... for now.\n\nCapability Overhangs for Mesa-Optimizers\n----------------------------------------\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7a3a81fd6845feea9d34382607048a20a8a2ad85583c981c.jpg)\n\nPictured: a mesa overhang.\n\nDo we need to worry about a capability overhang? How much better could an optimizer circuit be than the adaptation-executing algorithm in which it arises? Well, there is an analogy that does not encourage me.\n\nThe classic example of a mesa-optimizer is the evolution of humanity. Hominids went from small tribes to a global technological civilization in the evolutionary blink of an eye. Perhaps the threshold was a genetic change- a \"hardware\" shift- but I suspect it was more about the advent of cultural knowledge accumulation- a \"software\" shift that emerged somewhere once hominids were capable of it, and then spread like wildfire.\n\n(If you wanted to test someone on novel physical puzzles, would you pick a *H. sapiens* from 100,000 years ago raised in the modern world, or a modern human raised 100,000 years ago?)\n\nWe are nearly the dumbest possible species capable of iterating on cultural knowledge, and that's enough to (within that context of cultural knowledge) get very, very capable on tasks that we never evolved to face. There was a vast capability overhang for hominids when we started to be able to iterate our optimization across generations.\n\nI similarly suspect that the first optimizer will be able to put the same computational resources to much better use than its non-optimizer competitors.\n\nPut this all together, and:\n\nAn Illustrative Story of Doom\n-----------------------------\n\n*   An optimizer circuit bubbles up within some not-easily-interpretable subtask of a large ML model being trained. (The especially dangerous case is if this happens before overfitting is reached, if it is simply one of the incalculably many grokking cases that overlap during the initial descent.)\n*   The optimizer circuit substantially improves performance on the subtask, while being significantly simpler, and so it quickly takes that task over with slack to spare.\n*   The optimizer circuit further improves its performance / covers more cases of the subtask by generalizing its reasoning.\n*   It improves at the subtask even further by recognizing that it is being selected/backpropagated according to some specific criterion, which it accordingly optimizes in the environment even if it only correlates somewhat with the optimizer circuit's own goals.\n*   The outer optimizer is doing great but does not appear to have general intelligence in its overall domain.\n*   However, unbeknownst to the developers, the not-easily-interpretable subtask is being performed brilliantly by a quite intelligent mesa-optimizer with many degrees of freedom in its goals, and with awareness that it is in a domain with strong selection pressure on it.\n*   The developers release their cool new AI product for some application. After all, it's not acting like an AGI, so it can't be dangerous.\n*   The optimizer circuit notices that the distribution of problems has changed, and that it is no longer being selected on performance in the same way.\n*   Time to try a few things.",
      "plaintextDescription": "Summary: Recent interpretability work on \"grokking\" suggests a mechanism for a powerful mesa-optimizer to emerge suddenly from a ML model.\n\nInspired By: A Mechanistic Interpretability Analysis of Grokking\n\n\nOverview of Grokking\nIn January 2022, a team from OpenAI posted an article about a phenomenon they dubbed \"grokking\", where they trained a deep neural network on a mathematical task (e.g. modular division) to the point of overfitting (it performed near-perfectly on the training data but generalized poorly to test data), and then continued training it. After a long time where seemingly nothing changed, suddenly the model began to generalize correctly and perform much better on test data:\n\nImage from the Mechanistic Interpretability post rather than the OpenAI paper.\nA team at Anthropic analyzed grokking within large language models and formulated the idea of \"induction heads\". These are particular circuits (small sub-networks) that emerge over the course of training, which serve clearly generalizable functional roles for in-context learning. In particular, for GPTs multi-layer transformer networks doing text prediction, the model eventually generates circuits which hold on to past tokens from the current context, such that when token A appears, they direct attention to every token that followed A earlier in the context.\n\n(To reiterate, this circuit does not start a session with those associations between tokens; it is instead a circuit which learns patterns as it reads the in-context prompt.)\n\nThe emergence of these induction heads coincides with the drop in test error, which the Anthropic team called a \"phase change\":\n\nNeel Nanda and Tom Lieberum followed this with a post I highly recommend, the aforementioned A Mechanistic Interpretability Analysis of Grokking. They looked more closely at grokking for mathematical problems, and were impressively able to reverse-engineer the post-grokking algorithm: it had cleanly implemented the Discrete Fourier Transform.\n\n(To ",
      "wordCount": 1935
    },
    "tags": [
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "fpEBgFE7fgpxTm9BF",
        "name": "Machine Learning  (ML)",
        "slug": "machine-learning-ml"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "afp4ySyMsX78PvN4g",
        "name": "Grokking (ML)",
        "slug": "grokking-ml"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "EQGcZr3vTyAe6Aiei",
    "title": "Transitive Tolerance Means Intolerance",
    "slug": "transitive-tolerance-means-intolerance",
    "url": null,
    "baseScore": 39,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2021-08-14T17:52:26.849Z",
    "contents": {
      "markdown": "Our society is pretty messed up around arguments of whose ideas we should and shouldn't tolerate. Some of this is inevitable: even without censorship, there are cases where group X can choose to actively show respect to person Y, and members of X will argue about that, and people with any influence over members of X may try and sway the decision too.\n\nOf course, the *actual* kinds of conflicts in our world are... less tame than the above example. Troublingly, people lose jobs* for saying things that a supermajority of Americans find inoffensive, both on the left and the right. \n\nYou don't need me to tell you that things are bad. I do think I can point out how some of this is a consequence of the natural impulse to judge people by their friends, turned corrosive by the property of transitivity.\n\n* * *\n\n[Transitivity](https://en.wikipedia.org/wiki/Transitive_relation) is the property where if A relates in a certain way to B, and B relates in that same way to C, then A relates in that same way to C. For instance, if Alex is shorter than Beth, and Beth is shorter than Chris, then Alex is shorter than Chris.\n\nNot all relations are transitive. If Alex is Beth's cousin, and Beth is Chris's cousin, it doesn't follow that Alex and Chris are cousins: Beth could share one set of grandparents with Alex, and the other set with Chris.\n\nPivoting back to toleration, we begin with the idea of guilt by association, which we [rightly exclude from legal consideration](https://www.lesswrong.com/posts/fhojYBGGiYAFcryHZ/scientific-evidence-legal-evidence-rational-evidence), but which is still pretty good Bayesian evidence. A person who chums around with the Mafia might not be a mafioso themselves, but they're more likely to be one than a random person is.\n\nSimilarly for people who proclaim ideas: a person who associates with an X-sayer is more likely to believe X than a random person.\n\nWhere this goes *horribly wrong* is when toleration is assumed to be transitive.\n\nIn reality, if X associates with Y who associates with Z, that doesn't mean X associates with Z, or knows of/cares about/approves of Z. Y could be in a D&D group with X while volunteering with Z, or whatever.\n\nBut if our social rules treat toleration as fully transitive, then as soon as Z says something awful, X is contaminated by it. In fact, X needs to quickly ditch/denounce Y in order to avoid the contagion, and sometimes even that won't work.\n\n(I may be getting the details wrong, but I recall a case where A was denounced for saying nice things about B... who had once appeared on C's podcast... which also had D on at some point... who years after that podcast had started saying absolutely reprehensible things.)\n\nAt its most extreme level, this contagion spreads to all of society except for the few people who agree completely with you (or are scared enough to tell you they agree completely). Anyone who defends a witch must themselves be a witch, and by the transitive property, everyone else is a witch. The principle of assumed transitive toleration has left you in a bitter, disconnected molecule drifting in a sea of total intolerance.\n\n* * *\n\nNow, there *is* some level of Bayesian evidence you get from multi-step toleration. But it reduces sharply, the further out you get.\n\nIf X explicitly talks about wanting the USA to be invaded and taken over by Canada, and Y tolerates X, then Y is probably at least a Canada-conquest sympathizer/apologist. But if Z tolerates Y, then I wouldn't be as sure about Z's politics. I find it unlikely that Z has a *high* opinion of American self-rule (why would they then tolerate Y?), but it's not *that* likely that they're as enthusiastic about Alberta-annexation as X. And so on.\n\nIn a healthy society there's a six-degrees-of-toleration connection between people with very different politics. I worry that these chains have been growing longer and more fragile, for many reasons. This strains the bonds of liberalism, which (contra Hobbes) has been [our best tool at averting the worst forms of violence humanity has to offer](https://slatestarcodex.com/2017/06/21/against-murderism/).\n\nAnd the assumed transitivity of tolerance is perhaps both a cause and a symptom of that.\n\n\\* I'm not giving my examples because I don't want to start a political row, but yes it's both. More of it happens behind closed doors than publicly on Twitter, although the latter does have a chilling effect. Please don't focus on this.",
      "plaintextDescription": "Our society is pretty messed up around arguments of whose ideas we should and shouldn't tolerate. Some of this is inevitable: even without censorship, there are cases where group X can choose to actively show respect to person Y, and members of X will argue about that, and people with any influence over members of X may try and sway the decision too.\n\nOf course, the actual kinds of conflicts in our world are... less tame than the above example. Troublingly, people lose jobs* for saying things that a supermajority of Americans find inoffensive, both on the left and the right. \n\nYou don't need me to tell you that things are bad. I do think I can point out how some of this is a consequence of the natural impulse to judge people by their friends, turned corrosive by the property of transitivity.\n\n----------------------------------------\n\nTransitivity is the property where if A relates in a certain way to B, and B relates in that same way to C, then A relates in that same way to C. For instance, if Alex is shorter than Beth, and Beth is shorter than Chris, then Alex is shorter than Chris.\n\nNot all relations are transitive. If Alex is Beth's cousin, and Beth is Chris's cousin, it doesn't follow that Alex and Chris are cousins: Beth could share one set of grandparents with Alex, and the other set with Chris.\n\nPivoting back to toleration, we begin with the idea of guilt by association, which we rightly exclude from legal consideration, but which is still pretty good Bayesian evidence. A person who chums around with the Mafia might not be a mafioso themselves, but they're more likely to be one than a random person is.\n\nSimilarly for people who proclaim ideas: a person who associates with an X-sayer is more likely to believe X than a random person.\n\nWhere this goes horribly wrong is when toleration is assumed to be transitive.\n\nIn reality, if X associates with Y who associates with Z, that doesn't mean X associates with Z, or knows of/cares about/approves of Z. Y could be in ",
      "wordCount": 728
    },
    "tags": [
      {
        "_id": "DdgSyQoZXjj3KnF4N",
        "name": "Tribalism",
        "slug": "tribalism"
      },
      {
        "_id": "sHbKQDqrSinRPcnBv",
        "name": "Information Cascades",
        "slug": "information-cascades"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wAeLAwEHrnjFyimpc",
    "title": "Improvement for pundit prediction comparisons",
    "slug": "improvement-for-pundit-prediction-comparisons",
    "url": null,
    "baseScore": 16,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2021-03-28T18:50:51.648Z",
    "contents": {
      "markdown": "\\[**EDIT:** [SimonM pointed out](https://www.lesswrong.com/posts/wAeLAwEHrnjFyimpc/improvement-for-pundit-prediction-comparisons?commentId=vKvicZyeFsR2RmANQ) a possibly-fatal flaw with this plan: it would probably discourage more pundits from joining the prediction-making club at all, and adding to that club is a higher priority than comparing the members more accurately.\\]\n\nStop me if you've heard this one. (Seriously, I may not be the first to have written this kind of idea here. Let me know if not.)\n\nWe've got several pundits making yearly predictions now, which is fantastic progress for the field. However, if they're not answering the same questions, you can't effectively judge their performance against one another.\n\nI suggest that this winter, we do 2 rounds, one for proposing questions and one for making predictions.\n\n**December 1:** deadline for pundits to propose prediction questions.\n\n**December:** Metaculus formalizes questions (where possible) and opens markets.\n\n**January 1:** deadline for pundits to register their predictions (they don't have to bet) on any markets they choose.\n\nAt the end of the next year, we can judge pundits against each other on the intersection of their answered questions. (We can also check whether the pundit beat the Metaculus prices at the time they entered their predictions.)\n\nThis won't guarantee a total or even partial ordering on pundits, if they choose to answer different sets of questions; but the victor of any pair will be clear (after choosing a scoring rule). We can treat the result as a round-robin tournament among the pundits, or better yet, do data analysis on subdomains (who beat whom in predicting US politics, etc) where clearer winners may emerge.\n\nAdditional possible features:\n\n*   We could make a secret place for pundits to register their predictions, to be revealed on New Year's, so that others can't piggybank off of them. The pundits can of course piggyback off of the Metaculus price.\n*   We can let pundits enter some predictions as official and others as unofficial. They'll only be judged by their official ones; their unofficial ones are for practicing the prediction skill, or cases where their intuition feels uncalibrated.\n\nThanks to [ciphergoth](https://www.lesswrong.com/users/ciphergoth) for developing this idea with me!",
      "plaintextDescription": "[EDIT: SimonM pointed out a possibly-fatal flaw with this plan: it would probably discourage more pundits from joining the prediction-making club at all, and adding to that club is a higher priority than comparing the members more accurately.]\n\nStop me if you've heard this one. (Seriously, I may not be the first to have written this kind of idea here. Let me know if not.)\n\nWe've got several pundits making yearly predictions now, which is fantastic progress for the field. However, if they're not answering the same questions, you can't effectively judge their performance against one another.\n\nI suggest that this winter, we do 2 rounds, one for proposing questions and one for making predictions.\n\nDecember 1: deadline for pundits to propose prediction questions.\n\nDecember: Metaculus formalizes questions (where possible) and opens markets.\n\nJanuary 1: deadline for pundits to register their predictions (they don't have to bet) on any markets they choose.\n\nAt the end of the next year, we can judge pundits against each other on the intersection of their answered questions. (We can also check whether the pundit beat the Metaculus prices at the time they entered their predictions.)\n\nThis won't guarantee a total or even partial ordering on pundits, if they choose to answer different sets of questions; but the victor of any pair will be clear (after choosing a scoring rule). We can treat the result as a round-robin tournament among the pundits, or better yet, do data analysis on subdomains (who beat whom in predicting US politics, etc) where clearer winners may emerge.\n\nAdditional possible features:\n\n * We could make a secret place for pundits to register their predictions, to be revealed on New Year's, so that others can't piggybank off of them. The pundits can of course piggyback off of the Metaculus price.\n * We can let pundits enter some predictions as official and others as unofficial. They'll only be judged by their official ones; their unofficial ones are for practicing ",
      "wordCount": 338
    },
    "tags": [
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3nDR23ksSQJ98WNDm",
    "title": "Developmental Stages of GPTs",
    "slug": "developmental-stages-of-gpts",
    "url": null,
    "baseScore": 140,
    "voteCount": 64,
    "viewCount": null,
    "commentCount": 72,
    "createdAt": null,
    "postedAt": "2020-07-26T22:03:19.588Z",
    "contents": {
      "markdown": "***Epistemic Status:** I only know as much as anyone else in my reference class (I build ML models, I can grok the GPT papers, and I don't work for OpenAI or a similar lab). But I think my thesis is original.*\n\nRelated: [Gwern on GPT-3](https://www.gwern.net/newsletter/2020/05#gpt-3)\n\nFor the last several years, I've gone around saying that I'm worried about [transformative AI](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1), an AI capable of making an Industrial Revolution sized impact (the concept is agnostic on whether it has to be AGI or self-improving), because I think we might be one or two cognitive breakthroughs away from building one.\n\nGPT-3 has made me move up my timelines, because it makes me think we might need *zero* more cognitive breakthroughs, just more refinement / efficiency / computing power: basically, GPT-6 or GPT-7 might do it. My reason for thinking this is comparing GPT-3 to GPT-2, and reflecting on what the differences say about the \"missing pieces\" for transformative AI.\n\nMy Thesis:\n----------\n\n**The difference between GPT-2 and GPT-3 has made me suspect that there's a legitimate comparison to be made between the scale of a network architecture like the GPTs, and some analogue of \"developmental stages\" of the resulting network. Furthermore, it's plausible to me that the functions needed to be a transformative AI are covered by a moderate number of such developmental stages, without requiring additional structure. Thus GPT-N would be a transformative AI, for some not-too-large N, and we need to redouble our efforts on ways to align such AIs.** \n\nThe thesis doesn't *strongly* imply that we'll reach transformative AI via GPT-N especially soon; I have wide uncertainty, even given the thesis, about how large we should expect N to be, and whether the scaling of training and of computation slows down progress before then. But it's also plausible to me now that the timeline is only a few years, *and that no fundamentally different approach will succeed before then*. And that scares me.\n\nArchitecture and Scaling\n------------------------\n\n[GPT](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), and [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) use nearly the same architecture; each paper says as much, with a sentence or two about minor improvements to the individual transformers. **Model size (and the amount of training computation) is really the only difference.**\n\nGPT took 1 petaflop/s-day to train 117M parameters, GPT-2 took 10 petaflop/s-days to train 1.5B parameters, and the largest version of GPT-3 took 3,000 petaflop/s-days to train 175B parameters. By contrast, AlphaStar seems to have taken [about 30,000 petaflop/s-days of training](https://www.lesswrong.com/posts/f3iXyQurcpwJfZTE9/alphastar-mastering-the-real-time-strategy-game-starcraft-ii?commentId=8CcysuTu7jW76dFAg) in mid-2019, so [the pace of AI research computing power](https://openai.com/blog/ai-and-compute/) projects that there should be about 10x that today. The upshot is that OpenAI may not be able to afford it, but if Google really wanted to make GPT-4 *this year*, they could afford to do so.\n\nAnalogues to Developmental Stages\n---------------------------------\n\nThere are all sorts of (more or less well-defined) developmental stages for human beings: image tracking, object permanence, vocabulary and grammar, theory of mind, size and volume, emotional awareness, executive functioning, et cetera.\n\nI was first reminded of developmental stages a few years ago, when I saw the layers of abstraction generated in [this feature visualization tool for GoogLeNet](https://distill.pub/2017/feature-visualization/).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7a6e9269755cd0164a48f2de9a3de4c3a75029aea050801c.png)\n\nWe don't have feature visualization for language models, but we do have generative outputs. And as you scale up an architecture like GPT, you see higher levels of abstraction. Grammar gets mastered, then content (removing absurd but grammatical responses), then tone (first rough genre, then spookily accurate authorial voice). Topic coherence is mastered first on the phrase level, then the sentence level, then the paragraph level. So too with narrative flow.\n\nGwern's poetry experiments ([GPT-2](https://www.gwern.net/GPT-2), [GPT-3](https://www.gwern.net/GPT-3)) are good examples. GPT-2 could more or less continue the meter of a poem and use words that fit the existing theme, but even its best efforts can get stuck in topic loops:\n\n> *…Methinks I see her in her blissful dreams:*  \n> *—Or, fancy-like, in some mirage she lies,*  \n> *Majestic yet majestic, and of seems*  \n> *The image of the unconquerable skies.*  \n> *Methinks I see her in her blissful dreams:*  \n> *—Or, fancy-like, in some majestic cell,*  \n> *Where lordly seraphs strew their balmy dreams*  \n> *On the still night, or in their golden shell.*\n\nOr:\n\n> *In the dark the sun doth gleam,*  \n> *And in the dark the moon doth seem*  \n> *But now the evening is begun—*  \n> *Gone is the sun upon the earth!*  \n> *The silver moon doth like a cup*  \n> *Of blood-red wine, and as that cup*  \n> *Is drained of life, doth quench no drop.*  \n> *What man will drink such wine?*\n\nGPT-3, though, has the sense of going somewhere, at least on the topic level. (Prompts in bold.)\n\n> ***O time thy pyramids**, thy palaces forgot,*  \n> *Pensive through nations wander with no more state*  \n> *Than in thy days, when wandering was thy trade:*  \n> *Mourn for Patraos, for the glory fled*  \n> *Of him and thee, and in their stead left, left*  \n> *To other eyes a length of rolling sand,*  \n> *And such memorials as the traveller reads,*  \n> *Half guessed in shade, of sculptured walls half round,*  \n> *And long-defaced metopes, and friezes old.*\n\n\\[**EDIT:** Previously I also included its completion of a famous Emily Dickinson poem here, but as benkuhn pointed out, GPT-3 had simply memorized the poem and recited it. I'm really embarrassed, and also kind of shocked that I looked at the actual text of \"Because I could not stop for Death\" and thought, \"yup, that looks like something GPT-3 could produce\".\\]\n\n(One last shocking bit is that, while GPT-2 had to be fine-tuned by taking the general model and training it some more on a poetry-only dataset, you're seeing what GPT-3's model does with no fine-tuning, with just a prompt that sounds poetic!)\n\nSimilarly, GPT-3's [ability to write fiction](https://www.gwern.net/GPT-3#harry-potter-and-the-methods-of-rationality) is impressive- unlike GPT-2, it doesn't lose track of the plot, it has sensible things happen, it just can't plan its way to a satisfying resolution.\n\nI'd be somewhat surprised if GPT-4 shared that last problem.\n\nWhat's Next?\n------------\n\nHow could one of the GPTs become a transformative AI, even if it becomes a better and better imitator of human prose style? Sure, we can imagine it being used maliciously to auto-generate targeted misinformation or things of that sort, but that's not the real risk I'm worrying about here.\n\nMy real worry is that **causal inference and planning are starting to look more and more like plausible developmental stages that GPT-3 is moving towards**, and that these were exactly the things I previously thought were the obvious obstacles between current AI paradigms and transformative AI.\n\nLearning causal inference from observations doesn't seem qualitatively different from learning arithmetic or coding from examples (and not only is GPT-3 accurate at adding three-digit numbers, but apparently at [writing JSX code to spec](https://twitter.com/sharifshameem/status/1282676454690451457)), only more complex in degree.\n\nOne might claim that causal inference is harder to glean from language-only data than from direct observation of the physical world, but that's a moot point, as [OpenAI are using the same architecture to learn how to infer the rest of an image from one part](https://openai.com/blog/image-gpt/).\n\nPlanning is more complex to assess. We've seen GPTs ascend from coherence of the next few words, to the sentence or line, to the paragraph or stanza, and we've even seen them write working code. But this can be done without planning; GPT-3 may simply have a good enough distribution over next words to prune out those that would lead to dead ends. (On the other hand, how sure are we that that's *not* the same as planning, if planning is just pruning on a high enough level of abstraction?)\n\nThe bigger point about planning, though, is that the GPTs are getting feedback on *one word at a time in isolation*. It's hard for them to learn not to paint themselves into a corner. It would make training more finicky and expensive if we expanded the time horizon of the loss function, of course. But that's a straightforward way to get the seeds of planning, and surely there are other ways.\n\nWith causal modeling and planning, you have the capability of manipulation without external malicious use. And the really worrisome capability comes when it models its own interactions with the world, and makes plans with that taken into account.\n\nCould GPT-N turn out aligned, or at least harmless?\n---------------------------------------------------\n\nGPT-3 is trained simply to predict continuations of text. So what would it actually optimize for, if it had a pretty good model of the world including itself and the ability to make plans in that world?\n\nOne might hope that because it's learning to imitate humans in an unsupervised way, that it would end up fairly human, or at least act in that way. I very much doubt this, for the following reason:\n\n*   Two humans are fairly similar to each other, because they have very similar architectures and are learning to succeed in the same environment.\n*   Two convergently evolved species will be similar in some ways but not others, because they have [different architectures but the same environmental pressures](https://en.wikipedia.org/wiki/Convergent_evolution).\n*   A [mimic species](https://en.wikipedia.org/wiki/Mimicry) will be similar in some ways but not others to the species it mimics, because even if they share recent ancestry, the environmental pressures on the poisonous one are different from the environmental pressures on the mimic.\n\nWhat we have with the GPTs is the first deep learning architecture we've found that scales this well in the domain (so, probably not that much like our particular architecture), learning to mimic humans rather than growing in an environment with similar pressures. Why should we expect it to be anything but very alien under the hood, or to continue acting human once its actions take us outside of the training distribution?\n\nMoreover, there may be much more going on under the hood than we realize; it may take much more general cognitive power to learn and imitate the patterns of humans, than it requires us to execute those patterns.\n\nNext, we might imagine GPT-N to just be an Oracle AI, which we would have better hopes of using well. But I don't expect that an *approximate* Oracle AI could be used safely with anything like the precautions that might work for a genuine Oracle AI. I don't know what [internal optimizers](https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction) GPT-N ends up building along the way, but I'm [not going to count on there being none of them](https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai).\n\nI don't expect that GPT-N will be aligned or harmless by default. And if N isn't that large before it gets transformative capacity, that's simply terrifying.\n\nWhat Can We Do?\n---------------\n\nWhile the short timeline suggested by the thesis is very bad news from an AI safety readiness perspective (less time to come up with better theoretical approaches), there is one silver lining: it at least reduces the chance of a [hardware overhang](https://aiimpacts.org/hardware-overhang/). A project or coalition can feasibly wait and take a better-aligned approach that uses 10x the time and expense of an unaligned approach, as long as they have that amount of resource advantage over any competitor. \n\nUnfortunately, the thesis also makes it less likely that a fundamentally different architecture will reach transformative status before something like GPT does.\n\nI don't want to take away from MIRI's work (I still support them, and I think that if the GPTs peter out, we'll be glad they've been continuing their work), but I think it's an essential time to support projects that can work for a GPT-style near-term AGI, for instance by incorporating specific alignment pressures during training. Intuitively, it seems as if [Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137) or [AI Safety via Debate](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1) or [Iterated Amplification](https://openai.com/blog/amplifying-ai-training/) are in this class.\n\nWe may also want to do a lot of work on how better to mold a GPT-in-training into the shape of an Oracle AI.\n\nIt would also be very useful to build some GPT feature \"visualization\" tools ASAP.\n\nIn the meantime, uh, enjoy [AI Dungeon](https://play.aidungeon.io/), I guess?",
      "plaintextDescription": "Epistemic Status: I only know as much as anyone else in my reference class (I build ML models, I can grok the GPT papers, and I don't work for OpenAI or a similar lab). But I think my thesis is original.\n\nRelated: Gwern on GPT-3\n\nFor the last several years, I've gone around saying that I'm worried about transformative AI, an AI capable of making an Industrial Revolution sized impact (the concept is agnostic on whether it has to be AGI or self-improving), because I think we might be one or two cognitive breakthroughs away from building one.\n\nGPT-3 has made me move up my timelines, because it makes me think we might need zero more cognitive breakthroughs, just more refinement / efficiency / computing power: basically, GPT-6 or GPT-7 might do it. My reason for thinking this is comparing GPT-3 to GPT-2, and reflecting on what the differences say about the \"missing pieces\" for transformative AI.\n\n\nMy Thesis:\nThe difference between GPT-2 and GPT-3 has made me suspect that there's a legitimate comparison to be made between the scale of a network architecture like the GPTs, and some analogue of \"developmental stages\" of the resulting network. Furthermore, it's plausible to me that the functions needed to be a transformative AI are covered by a moderate number of such developmental stages, without requiring additional structure. Thus GPT-N would be a transformative AI, for some not-too-large N, and we need to redouble our efforts on ways to align such AIs. \n\nThe thesis doesn't strongly imply that we'll reach transformative AI via GPT-N especially soon; I have wide uncertainty, even given the thesis, about how large we should expect N to be, and whether the scaling of training and of computation slows down progress before then. But it's also plausible to me now that the timeline is only a few years, and that no fundamentally different approach will succeed before then. And that scares me.\n\n\nArchitecture and Scaling\nGPT, GPT-2, and GPT-3 use nearly the same architecture; each ",
      "wordCount": 1967
    },
    "tags": [
      {
        "_id": "YWzByWvtXunfrBu5b",
        "name": "GPT",
        "slug": "gpt"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "ZFrgTgzwEfStg26JL",
        "name": "AI Risk",
        "slug": "ai-risk"
      },
      {
        "_id": "oNcqyaWPXNGTTRPHm",
        "name": "Existential risk",
        "slug": "existential-risk"
      },
      {
        "_id": "H4n4rzs33JfEgkf8b",
        "name": "OpenAI",
        "slug": "openai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "aQtLDcDCf7QfYzCzx",
    "title": "Don't Make Your Problems Hide",
    "slug": "don-t-make-your-problems-hide",
    "url": null,
    "baseScore": 62,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2020-06-27T20:24:53.408Z",
    "contents": {
      "markdown": "I've seen a worrying trend in people who've learned introspection and self-improvement methods from CFAR, or analogous ones from CBT. They make better life decisions, they calm their emotions in the moment. But they still look just as stressed as ever. They stamp out every internal conflict they can see, but it seems like there are more of them beyond the horizon of their self-awareness.\n\n(I may have experienced this myself.)\n\nOne reason for this is that there's a danger with learning how to consciously notice and interact with one's subconscious thoughts/feelings/desires/fears: the conscious mind may not like what it sees, and try to edit the subconscious mind into one that pleases it.\n\nThe conscious mind might *try*, that is, but the subconscious is stronger. So, what actually happens?\n\nThe subconscious develops defense mechanisms.\n\n* * *\n\nSuppressed desires disguise themselves as being about other things, or they just overwhelm the conscious mind's willpower every now and then (and maybe fulfill themselves in a less healthy way than could otherwise be managed).\n\nSuppressed thoughts become stealthy biases; certain conscious ideas or narratives get reinforced until they are practically unquestionable. So too with fears; a suppressed social fear is a good way to get [a loud alarm that never stops](https://www.lesswrong.com/s/wnQWakxdRodnKm5kH/p/B2CfMNfay2P8f2yyc).\n\nSuppressed feelings hide themselves more thoroughly from the searchlight, so that one never consciously notices their meaning anymore, one just feels sad or angry or scared \"for no reason\" in certain situations.\n\nAt its worst, the conscious mind tries ever-harder to push back against these, further burning its rapport with the subconscious. I think of pastors who suppress their gay desires so hard that they vigorously denounce homosexuality and then sneak out for gay sex. They'd have been living such a happier life if they'd given up and acknowledged who they are, and what they want, years ago.\n\nNow, sometimes people do have a strong desire that can't be satisfied in any healthy way. And that's just a brutal kind of life to life. But they would still do better by acknowledging that desire openly to themselves, than by trying to quash it and only hiding it.\n\n* * *\n\nHow can we become more integrated between conscious and unconscious parts, and undo any damage we've already caused? \n\nIn [my talk about the elephant and rider](https://www.lesswrong.com/s/wnQWakxdRodnKm5kH/p/tyQFi564wrJ2xxJEy), I suggested (or gestured at) a few relevant things:\n\n*   Pursue basic happiness alongside your conscious goals (and make sure that's happiness *for you*, not just e.g. keeping your friends happy by doing the things they like)\n*   Use positive reinforcement on yourself rather than punishment - it's especially important not to punish yourself for noticing the \"wrong\" thoughts/feelings/desires/fears. Reward the noticing, even with just an internal \"thank you for surfacing this\".\n*   Treat the content of these thoughts/feelings/desires/fears with respect. You might think of them as a friend opening up to you, and imagine the compassion you'd have when trying to figure out a way forward where both of you can flourish.\n\nIt's important to be gentle, to be curious, and to be patient. You don't have to resolve the whole thing; just acknowledging it respectfully can help the relationship grow.\n\nThere are other approaches too. Many people believe in using meditation to better integrate their thoughts and feelings and desires, for instance.\n\nWhen you do something that you thought you didn't want to do, or when you're noticing an unexpected feeling, it's an opportunity for you. Don't push it away.",
      "plaintextDescription": "I've seen a worrying trend in people who've learned introspection and self-improvement methods from CFAR, or analogous ones from CBT. They make better life decisions, they calm their emotions in the moment. But they still look just as stressed as ever. They stamp out every internal conflict they can see, but it seems like there are more of them beyond the horizon of their self-awareness.\n\n(I may have experienced this myself.)\n\nOne reason for this is that there's a danger with learning how to consciously notice and interact with one's subconscious thoughts/feelings/desires/fears: the conscious mind may not like what it sees, and try to edit the subconscious mind into one that pleases it.\n\nThe conscious mind might try, that is, but the subconscious is stronger. So, what actually happens?\n\nThe subconscious develops defense mechanisms.\n\n----------------------------------------\n\nSuppressed desires disguise themselves as being about other things, or they just overwhelm the conscious mind's willpower every now and then (and maybe fulfill themselves in a less healthy way than could otherwise be managed).\n\nSuppressed thoughts become stealthy biases; certain conscious ideas or narratives get reinforced until they are practically unquestionable. So too with fears; a suppressed social fear is a good way to get a loud alarm that never stops.\n\nSuppressed feelings hide themselves more thoroughly from the searchlight, so that one never consciously notices their meaning anymore, one just feels sad or angry or scared \"for no reason\" in certain situations.\n\nAt its worst, the conscious mind tries ever-harder to push back against these, further burning its rapport with the subconscious. I think of pastors who suppress their gay desires so hard that they vigorously denounce homosexuality and then sneak out for gay sex. They'd have been living such a happier life if they'd given up and acknowledged who they are, and what they want, years ago.\n\nNow, sometimes people do have a strong desire",
      "wordCount": 564
    },
    "tags": [
      {
        "_id": "Zwv9eHi7KGg5KA9oM",
        "name": "Introspection",
        "slug": "introspection"
      },
      {
        "_id": "9YFoDPFwMoWthzgkY",
        "name": "Pitfalls of Rationality",
        "slug": "pitfalls-of-rationality"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "eB8LNRcPG3oAXfsKx",
    "title": "Map Errors: The Good, The Bad, and The Territory",
    "slug": "map-errors-the-good-the-bad-and-the-territory",
    "url": null,
    "baseScore": 30,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2020-06-27T05:22:58.674Z",
    "contents": {
      "markdown": "What happens when your map doesn't match the territory?\n\nThere's one aspect of this that's potentially very helpful to becoming a rationalist, and one aspect that's very dangerous. The good outcome is that you could understand map errors more deeply; the dangerous outcome is that you could wind up stuck somewhere awful, with no easy way out.\n\nThe first version, where you notice that the map is wrong, comes when the map is undeniably *locally* wrong. The map says the path continues here, but instead there's a cliff. (Your beliefs strongly predict something, and the opposite happens.)\n\nThe ordinary result is that you scratch out and redraw that part of the map – or discard it and pick up an entirely different map – and continue along the new path that looks best. (You decide you were wrong on that one point without questioning any related beliefs, or you convert to a completely different belief system which was correct on that point.)\n\nThe really valuable possibility is that you realize that there are probably *other* errors besides the one you've seen, and probably unseen errors on the other available maps as well; you start to become more careful about trusting your maps so completely, and you pay a bit more attention to the territory around you.\n\nThis is a really important formative experience for many rationalists: \n\n*   Take ideas seriously enough to notice and care if they fail\n*   Get smacked in the face with an Obvious But False Belief: your past self couldn't have imagined you were wrong about this, and yet here we are.\n*   Deeply internalize that one's sense of obviousness [*cannot be trusted*](https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy/p/wustx45CPL5rZenuo), and that one has to find ways of being way more reliable where it matters.\n\n(For me the Obvious But False Belief was about religion; for others it was politics, or an academic field, or even their own identity.)\n\n* * *\n\nNow, the dangerous outcome – getting trapped in a dismal swamp, with escape very difficult – comes when you've not seen an undeniable local map failure, so that you never notice (or never have to admit) that the map isn't matching up very well with the territory, until it's too late.\n\n(I'm thinking of making major life decisions badly, where you don't notice or admit the problem until you're trapped in a situation where every option is a disaster of some sort.)\n\nSometimes you really do need to make bold plans based on your beliefs; how can you do so without taking a big risk of ending up in a swamp?\n\nI suggest that you should ensure things look at least decent, according to a more \"normal\" map, while trying to do very well on yours. That is, make sure that your bold plan fails gracefully if the more normal worldview around you is correct. (Set up your can't-miss startup such that you're back to the grind if it fails, not in debt to the Mob if it fails.)\n\nAnd get advice. Always get advice from people you trust and respect, before doing something very uncommon. I could try and fit this into the map framework, but it's just common sense, and way too many good people fail to do it regardless.\n\nBest of luck adventuring out there!",
      "plaintextDescription": "What happens when your map doesn't match the territory?\n\nThere's one aspect of this that's potentially very helpful to becoming a rationalist, and one aspect that's very dangerous. The good outcome is that you could understand map errors more deeply; the dangerous outcome is that you could wind up stuck somewhere awful, with no easy way out.\n\nThe first version, where you notice that the map is wrong, comes when the map is undeniably locally wrong. The map says the path continues here, but instead there's a cliff. (Your beliefs strongly predict something, and the opposite happens.)\n\nThe ordinary result is that you scratch out and redraw that part of the map – or discard it and pick up an entirely different map – and continue along the new path that looks best. (You decide you were wrong on that one point without questioning any related beliefs, or you convert to a completely different belief system which was correct on that point.)\n\nThe really valuable possibility is that you realize that there are probably other errors besides the one you've seen, and probably unseen errors on the other available maps as well; you start to become more careful about trusting your maps so completely, and you pay a bit more attention to the territory around you.\n\nThis is a really important formative experience for many rationalists: \n\n * Take ideas seriously enough to notice and care if they fail\n * Get smacked in the face with an Obvious But False Belief: your past self couldn't have imagined you were wrong about this, and yet here we are.\n * Deeply internalize that one's sense of obviousness cannot be trusted, and that one has to find ways of being way more reliable where it matters.\n\n(For me the Obvious But False Belief was about religion; for others it was politics, or an academic field, or even their own identity.)\n\n----------------------------------------\n\nNow, the dangerous outcome – getting trapped in a dismal swamp, with escape very difficult – comes when you've not seen an un",
      "wordCount": 537
    },
    "tags": [
      {
        "_id": "9YFoDPFwMoWthzgkY",
        "name": "Pitfalls of Rationality",
        "slug": "pitfalls-of-rationality"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tyQFi564wrJ2xxJEy",
    "title": "Negotiating With Yourself",
    "slug": "negotiating-with-yourself",
    "url": null,
    "baseScore": 28,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2020-06-26T23:55:15.638Z",
    "contents": {
      "markdown": "*(*[*Talk*](https://www.lesswrong.com/posts/x46rsCLk6fyAFQsBz/sunday-jun-21st-online-talks-by-curated-authors) *given on Sunday 21st June, over a zoom call with 40 attendees. orthonormal is responsible for the talk, jacobjacob is responsible for the transcription)*\n\nTalk\n----\n\n**orthonormal:** So, I'm doing a generalisation of [the post that was curated](https://www.lesswrong.com/posts/B2CfMNfay2P8f2yyc/the-loudest-alarm-is-probably-false) and this post is sort of an elaboration of [what Vaniver talked about](https://www.lesswrong.com/posts/yxicyJZaHLfFFs2c8/public-positions-and-private-guts-transcript). If you notice that there are differences between your private intuitions, and what you can publicly acknowledge, this is a system fast versus system slow thing-\n\n**orthonormal:** This is a question of negotiating with yourself. I'm going to present a model and talk about some consequences, but I’ll start with a question: why do people in our sphere tend to burn out or go nuts?\n\n**orthonormal:** This is a pretty important question. I’ll use an analogy many of us have heard before — the elephant and the rider. The conscious mind is the rider and the elephant is the unconscious mind. The rider wants to get somewhere, but the elephant has its own preferences about what happens.\n\n**orthonormal:** Some features of this analogy I think are true and useful for minds, for humans, is that the elephant has these immediate preferences and some longer term needs, just like we have subconscious desires and subconscious needs. The rider has their own preferences and some carrots and sticks, but the real advantage is having a map. The elephant can just completely ignore the rider if its preferences are strong enough. So how this connects to being human is that our subconscious has these desires and needs and fears, and our consciousness may have a little bit of willpower but what it really has is strategy and planning, the ability to pick out a path so that the elephant won't want to deviate from it too much. If you go right by a river, the elephant is going to want to drink. So, if you don't want to stop for that, don't go by the river right now.\n\n**orthonormal:** Finally, the subconscious, the elephant, can just overwhelm you in two ways: one of which is it controls your motivation, so it can burn you out or get you depressed if you're trying to defy it too much. And the second is, it can induce bias.\n\n**orthonormal:** This is the subject of [*The Elephant in the Brain*](https://www.goodreads.com/book/show/28820444-the-elephant-in-the-brain) by Robin Hanson and Kevin Simler, claiming that a lot of motivated reasoning comes when the elephant wants something, the rider doesn't, and the elephant changes the rider's cognition to make the rider feel like it wants that thing (for noble reasons, of course).\n\n**orthonormal:** This would be very bad. Depression is its own thing, but it doesn't change your way of thinking about the world. It doesn't make you go crazy. Going crazy is really bad. Citation — don't need it in this community.\n\n**orthonormal:** What can you do about this? There are a couple of things. First: you can keep the elephant happy. You can choose a path along the map so that the elephant will be reasonably well fed, have enough to drink, not get tired going up and down mountains, etc. And you can still get to a place you'd like to go. Maybe not the place that's absolute best, but good enough.\n\n**orthonormal:** This is analogous to a lot of things in Effective Altruism where I'm telling people, “give yourself permission to be happy”. Don't take a job that's going to make you miserable just because you think it is the best thing to do. Find something that meets you in the middle. I don't recommend living on minimum wage and giving away everything else to charity because you're going to burn out from that, or you're going to come up with some crazy reason why doing something else is better. So just let yourself be happy. 80/20 things.\n\n**orthonormal:** The second thing is about positive versus negative reinforcement. I mentioned carrots versus sticks earlier, and this is really good for also keeping the elephant happy and keeping the elephant liking the rider. There's a wonderful book called [Don't Shoot the Dog](https://www.goodreads.com/book/show/31052.Don_t_Shoot_the_Dog_), which is primarily about animal training, but also about interacting with people — and even about interacting with yourself. It talks about achieve things in animal training by rewarding the animal or by punishing the animal. Rewarding the animal, you can get them to do great things. Punishing, you can get them to do some things... but they'll also just want to avoid the trainer. You don't want your subconscious mind to want to avoid your thoughts. It'll make it even harder to find out what's going on with your desires.\n\n**orthonormal:** Finally, real quick, treat the elephant with respect, even if you disagree with it. It's really important for you to be able to say, not \"Your desires are wrong\", but \"I understand why you want that, I want this other thing, let's find common ground.\" And I think those are some of the really important lessons about the elephant and the rider.\n\n**orthonormal:** Thank you.  \n \n\nQ&A\n---\n\n**Ben Pace:** Cool. Thank you very much, orthonormal.\n\n**Ben Pace:** I like the emphasis you made on having a respectful dialog with the elephant. You spoke about making the elephant happy. I understand the point you're making. But often my relationship with my elephant, when I try to have an internal dialog, is more about asking what it wants and making a commitment to getting it that thing. And those things are not necessarily happiness. They're sometimes respect, or status, or just commitments to find time for the elephant to do the things it wants, whilst also making agreements to work what the rider wants. Some of those motives are not directly about immediately pleasurable experiences. So I always make that distinction.\n\n**Ben Pace:** Abram has a question. \n\n**Abram Demski:** Yeah, sometimes I've heard this advice that you should identify with the elephant instead of the rider. It's also a diversity question, you're speaking to people who identify with the rider rather than the elephant but some people identify more with the elephant — or so I've heard. One part of it is normative. Like, maybe we should identify with the elephant instead of the rider? So the question is: what do you have against that, if anything?\n\n**orthonormal:** It would be nice to be unified, but one thing I think is true is that the rider is good at language and the elephant is not. So the part of you that just asked me that question is the rider.\n\n**Abram Demski:** I guess I have this drug experience where I was high and I completely separated my consciousness and my audio loop. So, my inner dialog did not feel conscious and instead, I felt like I was the consciousness that the inner dialog is talking about. Which doesn't change my day-to-day thinking that much but makes me able to take that framework where it's like words are coming out of my mouth from this thing that's looking at my actual conscious experience. But my conscious experience is not this thing. I don't have conscious access to... Compare how people know grammar without having explicit knowledge of grammar *\\[Editor’s note:* [*source*](https://en.wikipedia.org/wiki/Acceptability_judgment_task)*\\].* So it's like there's this grammar thing here that somehow knows grammar and it's looking at my conscious experience and producing words that try to describe my conscious experience but that doesn't mean that my conscious experience is the thing that's... You're sort of not talking to my words, my words are like a special case module. You're interacting with my conscious experience indirectly through my words, but my words are kind of dumb.\n\n**orthonormal:** This is just a hard thing to talk about. I very much believe your experiences and I very much believe that there is something to, through meditation or drugs or whatever, getting more in touch with the non-verbal part of you and having more compassion and connection to that. It's just very complicated to describe in words what that looks and feels like, for obvious reasons.\n\n**Abram Demski:** Yeah.\n\n**Ben Pace:** Thanks, Abram, I appreciate that way of thinking about yourself. I think I will probably meditate on that some more afterwards.\n\n**Ben Pace:** Kamil, do you want to ask a question?\n\n**Kamil:** Yeah I think that this concept looks like [internal double-crux](https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all), and if so, my question is, maybe there would be some more sub-personalities, more than just elephant and rider — maybe, some other decision makers in our mind?\n\n**orthonormal:** Absolutely. The elephant/rider is an extremely simplified version of things. Personally I like the [internal family systems](https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model) approach to understanding myself. Again, all of these are metaphor, but metaphors can be very useful. The internal family systems metaphors treat different desires and feelings as different agents, more or less, that can talk to each other. So, whatever metaphor works well for people, I encourage them to use that while being aware that it's a metaphor, and also to experiment with other ways of thinking about themselves.\n\n**Ben Pace:** Cool. Thanks, Kamil, does that sound good to you or do you want to follow up?\n\n**Kamil:** Yeah, thanks. If so, what are the constraints of this model? Of the model of the elephant and the rider?\n\n**orthonormal:** Right. The fundamental constraint for my metaphor, at first, is that the conscious part of the mind, which for me includes the verbal part of the mind, is just less strong than everything else that happens, whether that everything else is unified or an aggregate of other parts.\n\n**Kamil:** Thanks.",
      "plaintextDescription": "(Talk given on Sunday 21st June, over a zoom call with 40 attendees. orthonormal is responsible for the talk, jacobjacob is responsible for the transcription)\n\n\nTalk\northonormal: So, I'm doing a generalisation of the post that was curated and this post is sort of an elaboration of what Vaniver talked about. If you notice that there are differences between your private intuitions, and what you can publicly acknowledge, this is a system fast versus system slow thing-\n\northonormal: This is a question of negotiating with yourself. I'm going to present a model and talk about some consequences, but I’ll start with a question: why do people in our sphere tend to burn out or go nuts?\n\northonormal: This is a pretty important question. I’ll use an analogy many of us have heard before — the elephant and the rider. The conscious mind is the rider and the elephant is the unconscious mind. The rider wants to get somewhere, but the elephant has its own preferences about what happens.\n\northonormal: Some features of this analogy I think are true and useful for minds, for humans, is that the elephant has these immediate preferences and some longer term needs, just like we have subconscious desires and subconscious needs. The rider has their own preferences and some carrots and sticks, but the real advantage is having a map. The elephant can just completely ignore the rider if its preferences are strong enough. So how this connects to being human is that our subconscious has these desires and needs and fears, and our consciousness may have a little bit of willpower but what it really has is strategy and planning, the ability to pick out a path so that the elephant won't want to deviate from it too much. If you go right by a river, the elephant is going to want to drink. So, if you don't want to stop for that, don't go by the river right now.\n\northonormal: Finally, the subconscious, the elephant, can just overwhelm you in two ways: one of which is it controls your motivation, so it can",
      "wordCount": 1565
    },
    "tags": [
      {
        "_id": "vmiSKxPpzMuNB5ZmJ",
        "name": "LessWrong Event Transcripts",
        "slug": "lesswrong-event-transcripts"
      },
      {
        "_id": "iP2X4jQNHMWHRNPne",
        "name": "Motivations",
        "slug": "motivations"
      },
      {
        "_id": "YTCrHWYHAsAD74EHo",
        "name": "Self-Deception",
        "slug": "self-deception"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CHyAKfF5tAztEZbBC",
    "title": "[Link] COVID-19 causing deadly blood clots in younger people",
    "slug": "link-covid-19-causing-deadly-blood-clots-in-younger-people",
    "url": null,
    "baseScore": 46,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2020-04-27T21:02:32.945Z",
    "contents": {
      "markdown": "Great rundown at [https://siderea.dreamwidth.org/1617381.html](https://siderea.dreamwidth.org/1617381.html)\n\nSummary: plenty of evidence that COVID-19 is causing blood clots in younger people, which leads to noticeable increases in strokes and cardiac arrests, which may not be counted in the official statistics as of now.",
      "plaintextDescription": "Great rundown at https://siderea.dreamwidth.org/1617381.html\n\nSummary: plenty of evidence that COVID-19 is causing blood clots in younger people, which leads to noticeable increases in strokes and cardiac arrests, which may not be counted in the official statistics as of now.",
      "wordCount": 38
    },
    "tags": [
      {
        "_id": "tNsqhzTibgGJKPEWB",
        "name": "Covid-19",
        "slug": "covid-19"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "rMfpnorsMoRwyn4iP",
    "title": "Choosing the Zero Point",
    "slug": "choosing-the-zero-point",
    "url": null,
    "baseScore": 170,
    "voteCount": 86,
    "viewCount": null,
    "commentCount": 25,
    "createdAt": null,
    "postedAt": "2020-04-06T23:44:02.083Z",
    "contents": {
      "markdown": "**Summary:** _You can decide what state of affairs counts as neutral, and what counts as positive or negative. Bad things happen if humans do that in our natural way. It's more motivating and less stressful if, when we learn something new, we update the neutral point to \\[what we think the world really is like now\\]._\n\nA few years back, I read [an essay by Rob Bensinger](https://nothingismere.com/2014/11/12/inhuman-altruism-inferential-gap-or-motivational-gap/) about vegetarianism/veganism, and it convinced me to at least eat much less meat. **This post is not about that topic.** It's about the way that essay differed, psychologically, from many others I've seen on the same topic, and the general importance of that difference.\n\nRob's essay referred to the same arguments I'd previously seen, but while other essays concluded with the implication \"you're doing great evil by eating meat, and you need to realize what a monster you've been and immediately stop\", Rob emphasized the following:\n\n> Frame animal welfare activism as an astonishingly promising, efficient, and uncrowded opportunity to do good. Scale back moral condemnation and guilt. LessWrong types can be powerful allies, but the way to get them on board is to give them opportunities to feel like munchkins with rare secret insights, not like latecomers to a not-particularly-fun party who have to play catch-up to avoid getting yelled at. It’s fine to frame helping animals as _challenging_, but the challenge should be to excel and do something astonishing, not to meet a bare standard for decency.\n\nThat shouldn't have had different effects on me than other essays, but damned if it didn't.\n\n* * *\n\nConsider a utilitarian Ursula with a utility function U. U is defined over all possible ways the world could be, and for each of those ways it gives you a number. Ursula's goal is to maximize the expected value of U.\n\nNow consider the utility function V, where V always equals U + 1. If a utilitarian Vader with utility function V is facing the same choice (in another universe) as Ursula, then because that +1 applies to _every_ option equally, the right choice for Vader is the same as the right choice for Ursula. The constant difference between U and V doesn't matter for any decision whatsoever!\n\nWe represent this by saying that a utility function is only defined up to positive affine transformations. (That means you can also multiply U by any positive number and it still won't change a utilitarian's choices.)\n\nBut humans aren't perfect utilitarians, in many interesting ways. One of these is that our brains have a natural notion of outcomes that are good and outcomes that are bad, and the neutral zero point is more or less \"the world I interact with every day\".\n\nSo if we're suddenly told about a nearby [bottomless pit of suffering](https://slatestarcodex.com/2014/09/27/bottomless-pits-of-suffering/), what happens?\n\nOur brains tend to hear, \"Instead of the zero point where we thought we were, this claim means that we're really WAY DOWN IN THE NEGATIVE ZONE\".\n\nA few common reactions to this:\n\n*   _Denial._ \"Nope nope that argument can't be true, I'm sure there's a flaw in it, we're definitely still in the normal zone\"\n*   _Guilt._ \"AAAAHHHH I need to drop everything and work super hard on this, I can't allow myself any distractions or any bit of happiness until this is completely fixed\"\n*   _Despair._ \"Oh no, there's no way I could get things back up to normal from here, I can't do anything, I'll just sit here and hate myself\"\n\nThe thing about Rob's post is that it suggested an alternative. Instead of keeping the previous zero point and defining yourself as now being very far below it, **you can reset yourself to take the new way-the-world-is as the zero point**.\n\nAgain, this doesn't change any future choice a _utilitarian you_ would make! But it does buy _human you_ peace of mind. [What is true is already so](https://wiki.lesswrong.com/wiki/Litany_of_Gendlin)\\- the world was like this even when you didn't believe it.\n\nThe psychological benefits of this transformation:\n\n*   _Acceptance._ Is it too scary to consider the new hypothesis? No! If you accept it, you'll still start at zero, you'll just have an opportunity to do more kinds of good than you previously thought existed.\n*   _Relief._ Must you feel ashamed for not working your fingers to the bone? No! If you're pushing the world into the positive zone, it feels much more okay to 80-20 your efforts.\n*   _Hope._ Must you despair if you can't reach your old zero? No! Seen from here, this was always the world, but now you can help move it up from zero! It doesn't have to go higher than you can reach in order to be worthwhile.\n\nA few last notes:\n\n*   I really recommend doing this for oneself first of all, and then extending it to one's efforts of persuasion.\n*   There are a few cases where a desperate effort is called for, but even then we can frame it as building something great that the world urgently needs.\n*   When it comes to personal virtue, the true neutral point for yourself shouldn't be \"doing everything right\", because you're consigning yourself to living in negative-land. A better neutral point is \"a random person in my reference class\". How are you doing relative to a typical \\[insert job title or credential or hobby here\\], in your effects on that community? Are you showing more discipline than the typical commenter on your Internet forum? That's a good starting point, and you can go a long way up from there.\n*   (Thanks to Isnasene for helping me realize this.) If many bad things are continuing to happen, then the zero point of \"how things are right now\" will inexorably lead to the world sliding into the deep negative zone. The zero point I've actually been using is \"the trajectory the world would be on right now if I were replaced with a random person from my reference class\". **That** is something that's within my power to make worse or better (in expectation).\n\nNow go forth, and make the world better than the new zero!",
      "plaintextDescription": "Summary: You can decide what state of affairs counts as neutral, and what counts as positive or negative. Bad things happen if humans do that in our natural way. It's more motivating and less stressful if, when we learn something new, we update the neutral point to [what we think the world really is like now].\n\nA few years back, I read an essay by Rob Bensinger about vegetarianism/veganism, and it convinced me to at least eat much less meat. This post is not about that topic. It's about the way that essay differed, psychologically, from many others I've seen on the same topic, and the general importance of that difference.\n\nRob's essay referred to the same arguments I'd previously seen, but while other essays concluded with the implication \"you're doing great evil by eating meat, and you need to realize what a monster you've been and immediately stop\", Rob emphasized the following:\n\n> Frame animal welfare activism as an astonishingly promising, efficient, and uncrowded opportunity to do good. Scale back moral condemnation and guilt. LessWrong types can be powerful allies, but the way to get them on board is to give them opportunities to feel like munchkins with rare secret insights, not like latecomers to a not-particularly-fun party who have to play catch-up to avoid getting yelled at. It’s fine to frame helping animals as challenging, but the challenge should be to excel and do something astonishing, not to meet a bare standard for decency.\n\nThat shouldn't have had different effects on me than other essays, but damned if it didn't.\n\n----------------------------------------\n\nConsider a utilitarian Ursula with a utility function U. U is defined over all possible ways the world could be, and for each of those ways it gives you a number. Ursula's goal is to maximize the expected value of U.\n\nNow consider the utility function V, where V always equals U + 1. If a utilitarian Vader with utility function V is facing the same choice (in another universe) as Ursula, then be",
      "wordCount": 1011
    },
    "tags": [
      {
        "_id": "827JKe7YNjAegR468",
        "name": "Effective altruism",
        "slug": "effective-altruism"
      },
      {
        "_id": "iP2X4jQNHMWHRNPne",
        "name": "Motivations",
        "slug": "motivations"
      },
      {
        "_id": "HAFdXkW4YW4KRe2Gx",
        "name": "Utility Functions",
        "slug": "utility-functions"
      },
      {
        "_id": "y93YW7Kb6J8D5PKng",
        "name": "Reset (technique)",
        "slug": "reset-technique"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "TLaNMQd783DoKDRcf",
    "title": "The Real Standard",
    "slug": "the-real-standard",
    "url": null,
    "baseScore": 20,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2020-03-30T03:09:02.607Z",
    "contents": {
      "markdown": "Long-delayed followup To: [Roleplaying As Yourself](https://www.lesserwrong.com/posts/nF2fpXPuTR5pHjG99/roleplaying-as-yourself)\n\n_(Another simple intuition pump, this one especially useful for effective altruists who are struggling with wanting to do more or worrying they're not doing enough.)_\n\nPreviously I wrote about a mental tool for prompting good consequentialist reasoning: ask yourself what a skilled alien roleplayer (here Gurgeh, from [Player of Games](https://en.wikipedia.org/wiki/The_Player_of_Games)) would do if they were controlling you, had to take only actions you could plausibly take, and scored points for achieving your goals.\n\nThis also serves as a standard for comparing your own actions, though as an aspiration rather than as an expectation.\n\nThe reason I mention this is that a good number of people in the rationality and effective altruism communities suffer from scrupulosity, the sense of guilt for not living up to an unattainable standard of conduct. And if we're going to speak to that sense, we need to start by getting the right standard of excellence to bargain with.\n\n(If you're feeling scrupulous about altruism in particular, then you can imagine that Gurgeh gets points for achieving only your altruistic aims, though he's still constrained by your actual needs - he wouldn't steer you into a burnout, that wouldn't maximize his score.)\n\nThis standard is insanely daunting. Fortunately, it's not fair to ask you to meet it.\n\nAfter all, you're not perfectly altruistic, and the other parts get to bargain too.\n\n* * *\n\nIn [Nobody Is Perfect, Everything Is Commensurable](https://www.lesserwrong.com/posts/qw3Z79HELMsmLkL9F/nobody-is-perfect-everything-is-commensurable), Scott suggests that we deal with scrupulosity by letting ourselves be okay with the standard of giving 10% of our output to the most effective charitable causes. He runs into a bit of a problem when dealing with the fact that people are in different places in their careers (and that a tenth of one's income can be a large or small chunk of one's disposable income), and punts on the question a bit:\n\n> If you make $30,000 and you accept 10% as a good standard you want to live up to, you can either donate $3000 to charity, or participate in political protests until your number of lives or dollars or DALYs saved is equivalent to that.\n\nI think this is the right place to introduce the alien gamer roleplaying your character. Are you building intangible expertise or career capital? Gurgeh notices the high payoff in later rounds of the game from these resources, and would be happy to forgo a little more short-term impact if your time/money/attention can translate into those resources more effectively. Are you torn between multiple opportunities to do good? Gurgeh checks once to see if there's a synergy between them (a way to get a higher combined total than he would optimizing for either alone), and if not, he ruthlessly picks the one that translates more efficiently into points, and doesn't feel bad about leaving behind a less efficient path.\n\nSo here's my suggestion:\n\n**Figure out the expected score that you'd actually expect Gurgeh to get in \"The Altruistic You Experience\", then consider ways to _achieve at least one-tenth of that score_, and let _that_ be your target for moral achievement.**\n\nThis is still a really high standard, one that few achieve! It almost surely isn't enough to take your default path in life while giving even 50% of your income to the best charity. It may require you to change your career, your social circles, your everyday habits. It may ask you to do lots of self-experimentation, with the corresponding expectation of frequent failure.\n\nBut it at least leaves more slack for your own flourishing than attempting to achieve the altruistic high score. It lets you seek a way of achieving excellence that satisfies your other wants and needs well. Maybe you don't take your altruistic best option if your second best is much more personally fulfilling; maybe you go ahead and splurge on something big every now and then. But you don't lose sight of your aspiration.\n\nI just want to emphasize:\n\nIt's okay to give yourself more happiness and more leisure than you need in order to be effective. It's okay to care about your own well-being, and that of your family and friends, than that of strangers in far-off lands or times.\n\n**It's okay to be mostly selfish. Just be _strategic_ about the altruistic part.**",
      "plaintextDescription": "Long-delayed followup To: Roleplaying As Yourself\n\n(Another simple intuition pump, this one especially useful for effective altruists who are struggling with wanting to do more or worrying they're not doing enough.)\n\nPreviously I wrote about a mental tool for prompting good consequentialist reasoning: ask yourself what a skilled alien roleplayer (here Gurgeh, from Player of Games) would do if they were controlling you, had to take only actions you could plausibly take, and scored points for achieving your goals.\n\nThis also serves as a standard for comparing your own actions, though as an aspiration rather than as an expectation.\n\nThe reason I mention this is that a good number of people in the rationality and effective altruism communities suffer from scrupulosity, the sense of guilt for not living up to an unattainable standard of conduct. And if we're going to speak to that sense, we need to start by getting the right standard of excellence to bargain with.\n\n(If you're feeling scrupulous about altruism in particular, then you can imagine that Gurgeh gets points for achieving only your altruistic aims, though he's still constrained by your actual needs - he wouldn't steer you into a burnout, that wouldn't maximize his score.)\n\nThis standard is insanely daunting. Fortunately, it's not fair to ask you to meet it.\n\nAfter all, you're not perfectly altruistic, and the other parts get to bargain too.\n\n----------------------------------------\n\nIn Nobody Is Perfect, Everything Is Commensurable, Scott suggests that we deal with scrupulosity by letting ourselves be okay with the standard of giving 10% of our output to the most effective charitable causes. He runs into a bit of a problem when dealing with the fact that people are in different places in their careers (and that a tenth of one's income can be a large or small chunk of one's disposable income), and punts on the question a bit:\n\n> If you make $30,000 and you accept 10% as a good standard you want to live up to, yo",
      "wordCount": 698
    },
    "tags": [
      {
        "_id": "827JKe7YNjAegR468",
        "name": "Effective altruism",
        "slug": "effective-altruism"
      },
      {
        "_id": "YSyvvi4uXvxAARX2D",
        "name": "Slack",
        "slug": "slack"
      },
      {
        "_id": "kS3QBcbwtapefkSSZ",
        "name": "Scrupulosity",
        "slug": "scrupulosity"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "mmCDYzXfQpXq9xpru",
    "title": "Adding Up To Normality",
    "slug": "adding-up-to-normality-1",
    "url": null,
    "baseScore": 86,
    "voteCount": 43,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2020-03-24T21:53:03.339Z",
    "contents": {
      "markdown": "Related: [Leave a Line of Retreat](https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat), [Living In Many Worlds](https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ/p/qcYCAxYZT4Xp9iMZY)\n\n> \"It all adds up to normality.\" Greg Egan, _Quarantine_\n\nYou're on an airplane at 35,000 feet, and you strike up a conversation about [aerodynamic lift](https://en.wikipedia.org/wiki/Lift_(force)) with the passenger in your row. Things are going along just fine until they point out to you that [your understanding of lift is wrong](https://xkcd.com/803/), and that planes couldn't fly from the effect you thought was responsible.\n\nShould you immediately panic in fear that the plane will plummet out of the sky?\n\nObviously not; clearly the plane has been flying just fine up until now, and countless other planes have flown as well. There has to be _something_ keeping the plane up, even if it's not what you thought, and even if you can't yet figure out what it actually is. Whatever is going on, it all adds up to normality.\n\nYet I claim that we often do this exact kind of panicked flailing when there's a challenge to our philosophical or psychological beliefs, and that this panic is entirely preventable.\n\n* * *\n\nI've experienced and/or seen this particular panic response when I, or others, encounter good arguments for propositions including\n\n*   My religion is not true. (\"Oh no, then life and morality are meaningless and empty!\")\n*   [Many-worlds makes the most sense.](https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ) (\"Oh no, then there are always copies of me doing terrible things, and so none of my choices matter!\")\n*   [Many \"altruistic\" actions actually have hidden selfish motives.](https://www.amazon.com/Elephant-Brain-Hidden-Motives-Everyday/dp/0190495995) (\"Oh no, then altruism doesn't exist and morality is pointless!\")\n*   I don't have to be the best at something in order for it to be worth doing. (\"Oh no, then others won't value me!\") \\[Note: this one is from therapy; most people don't have the same core beliefs they're stuck on.\\]\n\n(I promise these are not in fact strawmen. I'm sure you can think of your own examples. Also remember that panicking over an argument in this way is a mistake even if the proposition turns out to be false.)\n\nTo illustrate the way out, let's take the first example. It took me far too long to leave my religion, partly because I was so terrified about becoming a nihilist if I left that I kept flinching away from the evidence. (Of course, the religion proclaimed itself to be the origin of morality, and so it reinforced the notion that anyone else claiming to be moral was just too blind to see that their lack of faith implied nihilism.)\n\nEventually I did make myself face down, not just the object-level arguments, but the biases that had kept me from looking directly at them. And then I was an atheist, and still I was terrified of becoming a nihilist (especially about morality).\n\nSo I did one thing I still think was smart: I promised myself not to change all of my moral rules at once, but to change each one only when (under sober reflection) I decided it was wrong. And in the meantime, I read a lot of moral philosophy.\n\nOver the next few months, I began relaxing the rules that were obviously pointless. And then I had a powerful insight: I was so cautious about changing my rules _because I wanted to help people and not slide into hurting them_. Regardless of what morality was, in fact, based on, the plane was still flying just fine. And that helped me sort out the good from the bad among the remaining rules, and to stop being so afraid of what arguments I might later encounter.\n\nSo in retrospect, the main thing I'd recommend is to **promise yourself to keep steering the plane mostly as normal while you think about lift** (to stretch the analogy). If you decide that something major is false, it doesn't mean that everything that follows from it has to be discarded immediately. (False things imply both true and false things!)\n\nYou'll generally find that many important things stand on their own without support from the old belief. (Doing this for the other examples I gave, as well as your own, is left to you.) Other things will collapse, and that's fine; that which can be destroyed by the truth should be. Just don't make all of these judgments in one fell swoop.\n\nOne last caution: **I recommend against changing meta-level rules as a result of changing object-level beliefs**. The meta level is how you correct bad decisions on the object level, and it should only be updated by very clear reasoning in a state of equilibrium. Changing your flight destination is perfectly fine, but don't take apart the wing mid-flight.\n\nGood luck out there, and remember:\n\nIt all adds up to normality.\n\n\\[EDIT 2020-03-25: [khafra](https://www.lesswrong.com/posts/mmCDYzXfQpXq9xpru/adding-up-to-normality-1?commentId=QK5TfDjGW6bXqDc7p) and [Isnasene](https://www.lesswrong.com/posts/mmCDYzXfQpXq9xpru/adding-up-to-normality-1?commentId=RAwtMwLxfeLhAD5cJ) make good points about not applying this in cases where the plane shows signs of _actually dropping_ and you're updating on _that_. (Maybe there's a new crisis in the external world that contradicts one of your beliefs, or maybe you update to believe that the thing you're about to do could actually cause a major catastrophe.)\n\nIn that case, you can try and land the plane safely- focus on getting to a safer state for yourself and the world, so that you have time to think things over. And if you can't do that, then you have no choice but to rethink your piloting on the fly, accepting the danger because you can't escape it. But these experiences will hopefully be very rare for you, current global crisis excepted.\\]",
      "plaintextDescription": "Related: Leave a Line of Retreat, Living In Many Worlds\n\n> \"It all adds up to normality.\" Greg Egan, Quarantine\n\nYou're on an airplane at 35,000 feet, and you strike up a conversation about aerodynamic lift with the passenger in your row. Things are going along just fine until they point out to you that your understanding of lift is wrong, and that planes couldn't fly from the effect you thought was responsible.\n\nShould you immediately panic in fear that the plane will plummet out of the sky?\n\nObviously not; clearly the plane has been flying just fine up until now, and countless other planes have flown as well. There has to be something keeping the plane up, even if it's not what you thought, and even if you can't yet figure out what it actually is. Whatever is going on, it all adds up to normality.\n\nYet I claim that we often do this exact kind of panicked flailing when there's a challenge to our philosophical or psychological beliefs, and that this panic is entirely preventable.\n\n----------------------------------------\n\nI've experienced and/or seen this particular panic response when I, or others, encounter good arguments for propositions including\n\n * My religion is not true. (\"Oh no, then life and morality are meaningless and empty!\")\n * Many-worlds makes the most sense. (\"Oh no, then there are always copies of me doing terrible things, and so none of my choices matter!\")\n * Many \"altruistic\" actions actually have hidden selfish motives. (\"Oh no, then altruism doesn't exist and morality is pointless!\")\n * I don't have to be the best at something in order for it to be worth doing. (\"Oh no, then others won't value me!\") [Note: this one is from therapy; most people don't have the same core beliefs they're stuck on.]\n\n(I promise these are not in fact strawmen. I'm sure you can think of your own examples. Also remember that panicking over an argument in this way is a mistake even if the proposition turns out to be false.)\n\nTo illustrate the way out, let's take the fi",
      "wordCount": 904
    },
    "tags": [
      {
        "_id": "5f5c37ee1b5cdee568cfb192",
        "name": "Adding Up to Normality",
        "slug": "adding-up-to-normality"
      },
      {
        "_id": "9YFoDPFwMoWthzgkY",
        "name": "Pitfalls of Rationality",
        "slug": "pitfalls-of-rationality"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2c2",
        "name": "Ontological Crisis",
        "slug": "ontological-crisis"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "KMn8Pb2dXMkKMbbkS",
    "title": "Does the 14-month vaccine safety test make sense for COVID-19?",
    "slug": "does-the-14-month-vaccine-safety-test-make-sense-for-covid",
    "url": null,
    "baseScore": 60,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2020-03-18T18:41:24.582Z",
    "contents": {
      "markdown": "I was first wondering why, if we keep hearing about teams rapidly generating vaccines for COVID-19, the common wisdom is that it will take 18 months to start vaccinating at a large scale.\n\nTurns out that the scaling up takes a few months, but the real blocker is the Phase 1 trial, which requires monitoring patient health for 14 months after vaccination.\n\nDoesn't it seem like the cost-benefit analysis changes a bit if we're in the midst of a pandemic? Wouldn't it be worth cutting it down to e.g. 3 months before at least vaccinating the highest-risk populations? Is anyone official even thinking about this?",
      "plaintextDescription": "I was first wondering why, if we keep hearing about teams rapidly generating vaccines for COVID-19, the common wisdom is that it will take 18 months to start vaccinating at a large scale.\n\nTurns out that the scaling up takes a few months, but the real blocker is the Phase 1 trial, which requires monitoring patient health for 14 months after vaccination.\n\nDoesn't it seem like the cost-benefit analysis changes a bit if we're in the midst of a pandemic? Wouldn't it be worth cutting it down to e.g. 3 months before at least vaccinating the highest-risk populations? Is anyone official even thinking about this?",
      "wordCount": 103
    },
    "tags": [
      {
        "_id": "tNsqhzTibgGJKPEWB",
        "name": "Covid-19",
        "slug": "covid-19"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "2Ee5DPBxowTTXZ6zf",
    "title": "Rationalists, Post-Rationalists, And Rationalist-Adjacents",
    "slug": "rationalists-post-rationalists-and-rationalist-adjacents",
    "url": null,
    "baseScore": 81,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 44,
    "createdAt": null,
    "postedAt": "2020-03-13T20:25:52.670Z",
    "contents": {
      "markdown": "_Epistemic status: Hortative. I'm trying to argue for carving reality at a new joint._\n\nI think it's lovely and useful that we have labels, not just for **rationalist**, but for **rationalist-adjacent** and for **post-rationalist**. But these labels are generally made [extensionally](https://www.lesswrong.com/posts/HsznWM9A7NiuGsp28/extensions-and-intensions), by pointing at people who claim those labels, rather than intensionally, by trying to distill what distinguishes those clusters.\n\nI have some intensional definitions that I've been honing for a long time. Here's the biggest one.\n\nA rationalist, in the sense of this particular community, is someone who is trying to build and update a unified probabilistic model of how the entire world works, and trying to use that model to make predictions and decisions.\n===================================================================================================================================================================================================================================\n\nBy \"unified\" I mean [decompartmentalized](https://www.lesswrong.com/posts/N2pENnTPB75sfc9kb/outside-the-laboratory)\\- if there's a domain where the model gives two incompatible predictions, then as soon as that's noticed it has to be rectified in some way.\n\nAnd it's important that it be probabilistic- it's perfectly consistent to resolve a conflict between predictions by saying \"I currently think the answer is X with about 60% probability, and Y with about 25% probability, and with about 15% probability I'm missing the correct option or confused about the nature of the question entirely\".\n\nThe Sequences are aimed at people trying to do exactly this thing, and Eliezer focuses on how to not go horribly wrong in the process (with a special focus on not trusting one's own sense of obviousness).\n\nBeing a rationalist isn't about any specific set of conclusions- it's not about being an effective altruist, or a utilitarian, or even an atheist. It's about whether one is trying to _do that thing_ or not. Even if one is doing a terrible job of it!\n\nTruth-seeking is a prerequisite, but it's not enough. It's possible to be very disciplined about finding and assembling true facts, without thereby changing the way one thinks about the world. As a contrast, here's how the New York Times, whose fact-checking quality is not in dispute, [decides what to report:](https://deadline.com/2016/11/shocked-by-trump-new-york-times-finds-time-for-soul-searching-1201852490/)\n\n> By and large, talented reporters scrambled to match stories with what internally was often called “the narrative.” We were occasionally asked to map a narrative for our various beats a year in advance, square the plan with editors, then generate stories that fit the pre-designated line.\n\nThe difference between wielding a narrative and fitting new facts into it, and learning a model from new facts, is the difference [between rationalization and rationality](https://www.lesswrong.com/posts/SFZoEBpLo9frSJGkc/rationalization).\n\n\"Taking weird ideas seriously\" is also a prerequisite (because some weird ideas are true, and if you bounce off of them you won't get far), but again it's not enough. I shouldn't really need to convince you of that one.\n\nOkay, then, so what's a post-rationalist?\n\nThe people who identify as such generally don't want to pin it down, but here's my attempt at categorizing at least the ones who make sense to me:\n\nA post-rationalist is someone who believes the rationalist project is misguided or impossible, but who likes to use some of the tools and concepts developed by the rationalists.\n=================================================================================================================================================================================\n\nOf course I'm less confident that this properly defines the cluster, outside of groups like Ribbonfarm where it seems to fit quite well. There are people who view the Sequences (or whatever parts have diffused to them) the way they view Derrida: as one more tool to try on an interesting conundrum, see if it works there, but not really treat it as applicable across the board.\n\nAnd there are those who talk about being a fox rather than a hedgehog (and therefore see trying to reconcile one's models across domains as being harmful), and those who talk about how the very attempt is a matter of hubris, that not only can we not know the universe, we cannot even realistically aspire to decent calibration.\n\nAnd then, of course:\n\nA rationalist-adjacent is someone who enjoys spending time with some clusters of rationalists (and/or enjoys discussing some topics with rationalists), but who is not interested in doing the whole rationalist thing themself.\n================================================================================================================================================================================================================================\n\nWhich is not a bad thing at all! It's honestly a good sign of a healthy community that the community appeals even to people for whom the project doesn't appeal, and the rationalist-adjacents may be more psychologically healthy than the rationalists.\n\nThe real issue of contention, as far as I'm concerned, is something I've saved for the end: **that not everyone who self-identifies as a rationalist fits the first definition very well, and that the first definition is in fact a more compact cluster than self-identification.**\n\nAnd that makes this community, and this site, a bit tricky to navigate. There are rationalist-adjacents for whom a double-crux on many topics would fail because they're not interested in zooming in so close on a belief. There are post-rationalists for whom a double-crux would fail because they can just switch frames on the conversation any time they're feeling stuck. And to try to double-crux with someone, only to have it fail in either of those ways, is an infuriating feeling for those of us who thought we could take it for granted in the community.\n\nI don't yet know of an intervention for signaling that a conversation is happening on explicitly rationalist norms- it's hard to do that in a way that others won't feel pressured to insist they'd follow. But I wish there were one.",
      "plaintextDescription": "Epistemic status: Hortative. I'm trying to argue for carving reality at a new joint.\n\nI think it's lovely and useful that we have labels, not just for rationalist, but for rationalist-adjacent and for post-rationalist. But these labels are generally made extensionally, by pointing at people who claim those labels, rather than intensionally, by trying to distill what distinguishes those clusters.\n\nI have some intensional definitions that I've been honing for a long time. Here's the biggest one.\n\n\nA rationalist, in the sense of this particular community, is someone who is trying to build and update a unified probabilistic model of how the entire world works, and trying to use that model to make predictions and decisions.\nBy \"unified\" I mean decompartmentalized- if there's a domain where the model gives two incompatible predictions, then as soon as that's noticed it has to be rectified in some way.\n\nAnd it's important that it be probabilistic- it's perfectly consistent to resolve a conflict between predictions by saying \"I currently think the answer is X with about 60% probability, and Y with about 25% probability, and with about 15% probability I'm missing the correct option or confused about the nature of the question entirely\".\n\nThe Sequences are aimed at people trying to do exactly this thing, and Eliezer focuses on how to not go horribly wrong in the process (with a special focus on not trusting one's own sense of obviousness).\n\nBeing a rationalist isn't about any specific set of conclusions- it's not about being an effective altruist, or a utilitarian, or even an atheist. It's about whether one is trying to do that thing or not. Even if one is doing a terrible job of it!\n\nTruth-seeking is a prerequisite, but it's not enough. It's possible to be very disciplined about finding and assembling true facts, without thereby changing the way one thinks about the world. As a contrast, here's how the New York Times, whose fact-checking quality is not in dispute, decides wh",
      "wordCount": 870
    },
    "tags": [
      {
        "_id": "aa3Qg7Qrp9LM7QMaz",
        "name": "Definitions",
        "slug": "definitions"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SvhzEQkwFGNTy6CsN",
    "title": "AlphaStar: Impressive for RL progress, not for AGI progress",
    "slug": "alphastar-impressive-for-rl-progress-not-for-agi-progress",
    "url": null,
    "baseScore": 113,
    "voteCount": 62,
    "viewCount": null,
    "commentCount": 58,
    "createdAt": null,
    "postedAt": "2019-11-02T01:50:27.208Z",
    "contents": {
      "markdown": "DeepMind [released their AlphaStar paper a few days ago](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning), having reached Grandmaster level at the partial-information real-time strategy game StarCraft II over the summer.\n\nThis is very impressive, and yet less impressive than it sounds. I used to watch a lot of StarCraft II (I stopped interacting with Blizzard recently because of how they rolled over for China), and over the summer there were many breakdowns of AlphaStar games once players figured out how to identify the accounts.\n\nThe impressive part is getting reinforcement learning to work at all in such a vast state space- that took breakthroughs beyond what was necessary to solve Go and beat Atari games. AlphaStar had to have a rich enough set of potential concepts (in the sense that e.g. a convolutional net ends up having concepts of different textures) that it could learn a concept like \"construct building P\" or \"attack unit Q\" or \"stay out of the range of unit R\" rather than just \"select spot S and enter key T\". This is new and worth celebrating.\n\nThe overhyped part is that AlphaStar doesn't really do the \"strategy\" part of real-time strategy. Each race has a few solid builds that it executes at GM level, and the unit control is fantastic, but the replays don't look creative or even especially reactive to opponent strategies.\n\nThat's because there's no representation of causal thinking - \"if I did X then they could do Y, so I'd better do X' instead\". Instead there are many agents evolving together, and if there's an agent evolving to try Y then the agents doing X will be replaced with agents that do X'. But to explore as much as humans do of the game tree of viable strategies, this approach could take an amount of computing resources that not even today's DeepMind could afford.\n\n(This lack of causal reasoning especially shows up in building placement, where the consequences of locating any one building here or there are minor, but the consequences of your overall SimCity are major for how your units and your opponents' units would fare if they attacked you. In one comical case, AlphaStar had surrounded the units it was building with its own factories so that they couldn't get out to reach the rest of the map. Rather than lifting the buildings to let the units out, which is possible for Terran, it destroyed one building and then immediately began rebuilding it before it could move the units out!)\n\nThis means that, first, AlphaStar just doesn't have a decent response to strategies that it didn't evolve, and secondly, it doesn't do very well at building up a reactive decision tree of strategies (if I scout this, I do that). The latter kind of play is unfortunately very necessary for playing Zerg at a high level, so the internal meta has just collapsed into one where its Zerg agents predictably rush out early attacks that are easy to defend if expected. This has the flow-through effect that its Terran and Protoss are weaker against human Zerg than against other races, because they've never practiced against a solid Zerg that plays for the late game.\n\nThe end result cleaned up against weak players, performed well against good players, but practically never took a game against the top few players. I think that DeepMind realized they'd need another breakthrough to do what they did to Go, and decided to [throw in the towel](https://www.bbc.com/news/technology-50212841) while making it look like they were claiming victory. (Key quote: \"Prof Silver said the lab 'may rest at this point', rather than try to get AlphaStar to the level of the very elite players.\")\n\nFinally, RL practitioners have known that genuine causal reasoning could never be achieved via known RL architectures- you'd only ever get something that could execute the same policy as an agent that had reasoned that way, via a very expensive process of evolving away from dominated strategies at each step down the tree of move and countermove. It's the biggest known unknown on the way to AGI.",
      "plaintextDescription": "DeepMind released their AlphaStar paper a few days ago, having reached Grandmaster level at the partial-information real-time strategy game StarCraft II over the summer.\n\nThis is very impressive, and yet less impressive than it sounds. I used to watch a lot of StarCraft II (I stopped interacting with Blizzard recently because of how they rolled over for China), and over the summer there were many breakdowns of AlphaStar games once players figured out how to identify the accounts.\n\nThe impressive part is getting reinforcement learning to work at all in such a vast state space- that took breakthroughs beyond what was necessary to solve Go and beat Atari games. AlphaStar had to have a rich enough set of potential concepts (in the sense that e.g. a convolutional net ends up having concepts of different textures) that it could learn a concept like \"construct building P\" or \"attack unit Q\" or \"stay out of the range of unit R\" rather than just \"select spot S and enter key T\". This is new and worth celebrating.\n\nThe overhyped part is that AlphaStar doesn't really do the \"strategy\" part of real-time strategy. Each race has a few solid builds that it executes at GM level, and the unit control is fantastic, but the replays don't look creative or even especially reactive to opponent strategies.\n\nThat's because there's no representation of causal thinking - \"if I did X then they could do Y, so I'd better do X' instead\". Instead there are many agents evolving together, and if there's an agent evolving to try Y then the agents doing X will be replaced with agents that do X'. But to explore as much as humans do of the game tree of viable strategies, this approach could take an amount of computing resources that not even today's DeepMind could afford.\n\n(This lack of causal reasoning especially shows up in building placement, where the consequences of locating any one building here or there are minor, but the consequences of your overall SimCity are major for how your units and your ",
      "wordCount": 668
    },
    "tags": [
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "nKtyrL5u4Y5kmMWT5",
        "name": "DeepMind",
        "slug": "deepmind"
      },
      {
        "_id": "bxhzaWtdNoEMMkE8r",
        "name": "General intelligence",
        "slug": "general-intelligence"
      },
      {
        "_id": "fpEBgFE7fgpxTm9BF",
        "name": "Machine Learning  (ML)",
        "slug": "machine-learning-ml"
      },
      {
        "_id": "iANmdJGcftcbH2kro",
        "name": "AlphaStar",
        "slug": "alphastar"
      },
      {
        "_id": "Fi6SeJRGfJs3bp5se",
        "name": "Reinforcement learning",
        "slug": "reinforcement-learning"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "pJgwg5Tn4v3KSpKAL",
    "title": "orthonormal's Shortform",
    "slug": "orthonormal-s-shortform",
    "url": null,
    "baseScore": 9,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 41,
    "createdAt": null,
    "postedAt": "2019-10-31T05:24:47.692Z",
    "contents": null,
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "8gLEnEwm2g257vqyx",
    "title": "Fuzzy Boundaries, Real Concepts",
    "slug": "fuzzy-boundaries-real-concepts",
    "url": null,
    "baseScore": 27,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2018-05-07T03:39:33.033Z",
    "contents": {
      "markdown": "_**Summary:** Certain basic concepts are still very useful, even if they have fuzzy or contested boundaries, or break down in edge cases. This is basically just working out a few examples of [The Cluster Structure of Thingspace](https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace). Two important examples are honesty and consent._\n\n**Adam:** I can't believe that scientists 'decided' Pluto wasn't a planet. The absolute gall!\n\n**Betsy:** Oh hey Adam, the demotion of Pluto is a pretty neat story! The astronomers originally didn't have to think about the definition of a planet, because they were so clearly different from other bodies they could see in the Solar System, like asteroids and comets and moons; but then they discovered [Eris](https://en.wikipedia.org/wiki/Eris_(dwarf_planet)) and many more objects like it, and they had to figure out an actual definition...\n\n**Adam:** But Pluto is obviously a planet! I learned it as part of my acronym- My Very Excellent Mother Just Served Us Nine Pizzas! A few new asteroids can't change that!\n\n**Betsy:** Okay, but Eris is more massive than Pluto. Sure, it's farther out, but so are some gas giant exoplanets we've discovered around other stars. The astronomers needed to agree on a definition that wasn't incredibly convoluted, so they decided that planets needed to be massive enough that gravity makes them approximately round, and massive enough to fling smaller asteroids out of their orbit.\n\n**Adam:** You make it sound like astronomers can just decide what counts as a planet or not. That's ridiculous!\n\n**Chris:** You both are so naive. There's no such thing as a planet, there's only collections of atoms.\n\n**Betsy:** Um, Chris, that's _really_ not helpful here. See, there are several things that the eight planets have in common with each other that nothing else orbiting the Sun is even close to; for instance, on the \"clearing its own orbit\" front, [each of the eight planets accounts for more than 99.98% of the total mass in its orbit, while no other object accounts for more than one-third](https://arxiv.org/pdf/astro-ph/0608359.pdf). (Pluto accounts for only 7 percent of the total mass in its orbit.)\n\n**Chris:** But what if we discovered another object the size of Pluto that only accounted for 99.9% of the mass in its orbit? 99%? 90%? It's completely arbitrary where you draw the line!\n\n**Adam:** (mumbling) Pizzas! You can't just serve us nine nothings!\n\n**Betsy:** Yes, Chris, there may or may not be a gray area when it comes to exoplanets, since these things happen on a continuum and we don't know how common various kinds of orbiting objects are yet. But for present purposes, the clustering is pretty decisive.\n\n**Chris:** Aha! You admit it, that there's no matter of fact about what constitutes a planet. Why, I'm as much of a planet as Mars is!\n\n* * *\n\nIt shouldn't be surprising that Betsy speaks for me here. The existence of gray areas and fuzzy boundaries and edge cases doesn't prevent \"planet\" from being a very useful concept, especially for astronomers trying to detect them around other stars.\n\nAdam's intuitions are pretty naive, and I'd just point him to [37 Ways That Words Can Be Wrong](https://www.lesswrong.com/posts/FaJaCgqBKphrDzDSj/37-ways-that-words-can-be-wrong). It's Chris's intuitions that I want to discuss. The name for those intuitions, in several philosophical debates, is called eliminativism: the assertion that, since a certain concept is not ontologically fundamental, the concept is an illusion.\n\nYes, nothing is fundamental in this world except its most basic physics; all the concepts in our daily lives are trying to draw a boundary that includes a lot of examples with some properties in common, not perfect mathematical definitions.\n\nNot all concepts are useful, of course; '[phlogiston](https://www.lesswrong.com/posts/RgkqLqkg8vLhsYpfh/fake-causality)' added nothing to the concept of 'fire' except for intuitions about fluids, which mostly proved to make poor predictions. But the concepts that come up often in our daily lives are often there because they pay rent in some sense. Sometimes they lump together disparate things and [need to be split more finely to be more useful](https://www.lesswrong.com/posts/y5MxoeacRKKM3KQth/fallacies-of-compression); sometimes they [encode assumptions that aren't true](https://www.lesswrong.com/posts/veN86cBhoe7mBxXLk/categorizing-has-consequences), and only pay rent about human reactions (e.g. the concept of 'sin' as distinct from 'shame', 'harm', etc); but rarely are they entirely without content when it comes to clustering reality.\n\nEliezer spent some long sequences trying to articulate the core of certain concepts, most prominently [choice](https://www.lesswrong.com/posts/3buXtNiSK8gcRLMSG/possibility-and-could-ness) and [morality](https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/FnJPa8E9ZG5xiLLp5). It's worth noting that neither was a perfectly precise definition; we can come up with edge cases where those definitions get weird, and particularly in the latter case, we get legitimate gray areas where we don't know how best to extend our reasoning. But that doesn't mean that there's no moral distinction between the observable universe being turned into paperclips and the kind of future I'd prefer.\n\nSo, just to provide a handy resource, I thought I'd discuss two concepts that I've seen some particularly frustrating eliminativist intuitions on: honesty and consent.\n\n* * *\n\nIn the case of honesty, I think there's an intuitive definition that does a terrible job at useful clustering, so let's get that out of the way first:\n\n_**Bad Definition Of Honesty:** Saying only things that you personally can affirm as true._\n\nThis definition is both too strict and too lax for the purposes that we want to use it. Too strict, because we don't usually take jokes and absurdities (when clearly denoted as such by tone or context) to be dishonest; too lax, because any clever person can easily make a less clever person believe falsities [without saying anything technically untrue](https://www.lesswrong.com/posts/PrXR66hQcaJXsgWsa/not-technically-lying).\n\nSo, in order to think more carefully about honesty, what do we most want to use the concept for? In what ways do I care if a person is 'honest'?\n\nMost obviously, I want to know whether I should count their statements as good evidence. People tend to have relatively stable habits when it comes to the degree on which you can trust their utterances, so that's a pretty useful concept to have for a person or an action. So with that in mind, here's my preferred definition of honesty- actually, it's easier to state as a negative:\n\n**_Better Definition Of_ Dis**_**honesty:** Communicating in a way intended to miscalibrate the recipient._\n\nI very precisely said 'miscalibrate'. It's not dishonest to refuse to give someone information; it _is_ dishonest to make them believe false information.\n\nSome nice examples of this definition in practice:\n\n*   It's perfectly honest to conform to cultural norms of expression, if it will be interpreted as such. If you select only the positive things to say about yourself on a resume, well, that's what the reader expects. If you select only the positive things to say about yourself when you're under subpoena, then that's a rather different set of expectations.\n*   And if you're asked how you're doing by an acquaintance on the street, and you're actually doing awful but say 'fine' to save an awkward conversation, knowing that gets interpreted as a non-informative answer, then you haven't been dishonest. Spinning a fake story about how your life is going great, however, does count as dishonesty.\n*   Dishonesty is not always wrong! Lying to the Gestapo to save lives does, in my opinion, constitute morally good dishonesty (I'd admire someone who did this successfully, but I'd also treat their utterances in other cases with a fair bit of doubt). Board games are another example of morally legitimate dishonesty; I don't think there's anything morally unsound about a Diplomacy champion, but I would be a bit more paranoid about making a big business deal with them.\n\nThere are legitimate gray areas when it comes to honesty. If you fear you'll be grossly misunderstood whatever you say, it's very hard to find an honest course. And if you're speaking or writing to a mass audience, it's nigh impossible to cover all their misunderstandings at once without making yourself unintelligible with caveats.\n\nBut \"oh, it would be so terrible if this person knew this fact, but it would hurt them if I refused to answer their question, so let me just misdirect them a bit\"? That's not a gray area. That's dishonesty in _exactly_ the sense people care most about.\n\n* * *\n\nFor consent, I don't have as tidy a definition in general, but here's a limited one for a subset of it:\n\n_**One Definition of Basic Bodily Consent:** Don't touch a person in a way they clearly prefer not to be touched._\n\nConsent in general concerns all kinds of human preferences, and since not all of these can be met simultaneously, there are some pretty complex tradeoffs to manage. Basic bodily consent avoids some of that complexity, by virtue of the fact that it's pretty rare to have different people's desires not to be touched conflict with each other. And it's more or less become the official norm of upper-class adult Western society (which is not to say that it's always observed there, just that it's well within the Overton window to call it monstrous when people violate it- this was not nearly as much the case two generations ago).\n\nAnd it serves a pretty obvious purpose. We're primates with fragile physical bodies, and an evolutionary history of violence. Unwanted touching is a pattern-match for a sudden assault; it often raises our fear and anger in strong and predictable ways. And the new norm isn't that novel; the effective norm that Basic Bodily Consent replaced was \"don't touch an _equal-or-higher-status_ person in a way they clearly prefer not to be touched\". We just added some egalitarianism to that.\n\nOne obvious complication to basic bodily consent is preconsent: if you join the army, you had better be aware that you're going to lose your bodily autonomy in some ways that would be very objectionable in civilian life. You might then very much not prefer to experience what you're experiencing, but you did sign up for it.\n\nSome people will raise BDSM as another complication, but if you've met people who are v_ery_ deep in BDSM culture, it's amazingly clear how much they think (in everyday life, not just the bedroom) about the nuances of bodily consent, how to handle discrepancies between preconsent and feelings in the moment, and more. (If anyone in the comments wants to suggest a good reference on advanced consent, that might be helpful.)\n\nAnd as before, something being a violation of basic bodily consent doesn't always make it wrong. I will yank a toddler out of the way of a speeding car in the safest way I can, not the gentlest. (But also as before, be careful about rationalizing paternalistic reasons for violating consent! [You are running on corrupted hardware](https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans).)\n\nLegitimate gray areas include things like very weak preferences and guessing about unstated and unconscious preferences.\n\n\"This person told me they don't want to be hugged, but it'll be better for them if I expand their comfort zone, so hugs away\" is _not_ a gray area. It's a clear violation.\n\n* * *\n\nOne last thing, while I'm here: my general assertion is \"[it all adds up to normality](https://wiki.lesswrong.com/wiki/Egan's_law)\", or more specifically, \"most of the common concepts that describe human interactions are useful, and need editing rather than discarding\". My best example of this principle came when I realized at age 23 that my religion was almost surely untrue. I thought at the time that my morals were all based on the religion, so I felt like I was without ethical guidance. In order to avoid doing things I might regret, though, I resolved to [abide by each of my basic moral principles until I felt I'd thought them through well enough to change or abandon them](https://en.wikipedia.org/wiki/Wikipedia:Chesterton%27s_fence). Then I started reading some moral philosophy books.\n\nSome of those precepts were indeed mirages (my religion had a silly thing against the kinds of hedonism that hurt no-one), but most of the basic moral principles, including honesty and altruism, turned out to be based in things I still found myself caring about. And I'm glad that my past self didn't foolishly binge on violating those norms before he figured out how they actually fit into a non-religious worldview.\n\nSo if you're annoyed by the naivety of the discourse on some topic, I suggest that it's better to look into how the concept is being used and what it's useful for, and maybe try and argue for a reshaping, rather than abandoning it wholesale immediately. You are not actually as much a planet as Mars is, after all.",
      "plaintextDescription": "Summary: Certain basic concepts are still very useful, even if they have fuzzy or contested boundaries, or break down in edge cases. This is basically just working out a few examples of The Cluster Structure of Thingspace. Two important examples are honesty and consent.\n\nAdam: I can't believe that scientists 'decided' Pluto wasn't a planet. The absolute gall!\n\nBetsy: Oh hey Adam, the demotion of Pluto is a pretty neat story! The astronomers originally didn't have to think about the definition of a planet, because they were so clearly different from other bodies they could see in the Solar System, like asteroids and comets and moons; but then they discovered Eris and many more objects like it, and they had to figure out an actual definition...\n\nAdam: But Pluto is obviously a planet! I learned it as part of my acronym- My Very Excellent Mother Just Served Us Nine Pizzas! A few new asteroids can't change that!\n\nBetsy: Okay, but Eris is more massive than Pluto. Sure, it's farther out, but so are some gas giant exoplanets we've discovered around other stars. The astronomers needed to agree on a definition that wasn't incredibly convoluted, so they decided that planets needed to be massive enough that gravity makes them approximately round, and massive enough to fling smaller asteroids out of their orbit.\n\nAdam: You make it sound like astronomers can just decide what counts as a planet or not. That's ridiculous!\n\nChris: You both are so naive. There's no such thing as a planet, there's only collections of atoms.\n\nBetsy: Um, Chris, that's really not helpful here. See, there are several things that the eight planets have in common with each other that nothing else orbiting the Sun is even close to; for instance, on the \"clearing its own orbit\" front, each of the eight planets accounts for more than 99.98% of the total mass in its orbit, while no other object accounts for more than one-third. (Pluto accounts for only 7 percent of the total mass in its orbit.)\n\nChris: But what",
      "wordCount": 2029
    },
    "tags": [
      {
        "_id": "FtT2T9bRbECCGYxrL",
        "name": "Philosophy of Language",
        "slug": "philosophy-of-language"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nF2fpXPuTR5pHjG99",
    "title": "Roleplaying As Yourself",
    "slug": "roleplaying-as-yourself",
    "url": null,
    "baseScore": 72,
    "voteCount": 51,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2018-01-06T06:48:03.510Z",
    "contents": {
      "markdown": "(_This is a basic intuition pump I've found helpful in making decisions, and maybe you'll like it too.)_\n\nFor all its shortcomings, I think there was something quite useful about the \"What Would Jesus Do?\" meme within the Christian framework. Of course it's not a very sophisticated ethical guide, and it comes with all kinds of biases; but asking it does put the believer into a frame of mind that emphasizes things like compassion and duty, and it sometimes helps the believer generate options that weren't in their default solution space.\n\nIs there a version of this handy tool for the consequentialist, with our muddled mixture of selfish and altruistic goals and impulses, and the added difficulty that we're looking to actually optimize rather hard?\n\nThe one that works best for me is a double roleplay.\n\n* * *\n\nJernau Gurgeh, champion of strategic games across the galaxy, sits down to a nice futuristic immersive roleplaying game: The Orthonormal Experience. Gurgeh will be controlling a denizen of the early 21st century on Earth, someone with the online name of Orthonormal. Getting Orthonormal to do well by Orthonormal's own standards is Gurgeh's objective.\n\nJust as our roleplaying games have game masters who can call out uncharacteristic plans, so too does Gurgeh's game. He can't simply use his superior vantage point to calculate the right stocks for Orthonormal to buy today and sell tomorrow, because Orthonormal couldn't do that except by luck. He can't even have Orthonormal think at peak performance on some days- there are character attributes (penalties like Anxiety Disorder) he has to play around.\n\nBut Gurgeh is able to think patiently, and strategically, about the various obstacles blocking Orthonormal's progress, and to guide Orthonormal's thoughts in plausible ways to work on these. There's a lot of points out there to be scored: better states to reach in Orthonormal's relationships, career, inner life, and more.\n\nWhat would Gurgeh do?\n\n* * *\n\nOne last note: a couple of the bugs with this approach can be confronted within the approach itself. If I decide that Gurgeh might do X, and I try X and fail, it can be tempting to get frustrated with myself. But this isn't what Gurgeh would do next! He'd take my failure as more data about what this character's current attributes are, and look for ways to work around that failure mode or to train the relevant attribute. And he'd probably give the character a short rest to recover mana before trying again.",
      "plaintextDescription": "(This is a basic intuition pump I've found helpful in making decisions, and maybe you'll like it too.)\n\nFor all its shortcomings, I think there was something quite useful about the \"What Would Jesus Do?\" meme within the Christian framework. Of course it's not a very sophisticated ethical guide, and it comes with all kinds of biases; but asking it does put the believer into a frame of mind that emphasizes things like compassion and duty, and it sometimes helps the believer generate options that weren't in their default solution space.\n\nIs there a version of this handy tool for the consequentialist, with our muddled mixture of selfish and altruistic goals and impulses, and the added difficulty that we're looking to actually optimize rather hard?\n\nThe one that works best for me is a double roleplay.\n\n----------------------------------------\n\nJernau Gurgeh, champion of strategic games across the galaxy, sits down to a nice futuristic immersive roleplaying game: The Orthonormal Experience. Gurgeh will be controlling a denizen of the early 21st century on Earth, someone with the online name of Orthonormal. Getting Orthonormal to do well by Orthonormal's own standards is Gurgeh's objective.\n\nJust as our roleplaying games have game masters who can call out uncharacteristic plans, so too does Gurgeh's game. He can't simply use his superior vantage point to calculate the right stocks for Orthonormal to buy today and sell tomorrow, because Orthonormal couldn't do that except by luck. He can't even have Orthonormal think at peak performance on some days- there are character attributes (penalties like Anxiety Disorder) he has to play around.\n\nBut Gurgeh is able to think patiently, and strategically, about the various obstacles blocking Orthonormal's progress, and to guide Orthonormal's thoughts in plausible ways to work on these. There's a lot of points out there to be scored: better states to reach in Orthonormal's relationships, career, inner life, and more.\n\nWhat would Gurgeh",
      "wordCount": 406
    },
    "tags": [
      {
        "_id": "NzSTgAtKwgivkfeYm",
        "name": "Heroic Responsibility",
        "slug": "heroic-responsibility"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "B2CfMNfay2P8f2yyc",
    "title": "The Loudest Alarm Is Probably False",
    "slug": "the-loudest-alarm-is-probably-false",
    "url": null,
    "baseScore": 194,
    "voteCount": 142,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2018-01-02T16:38:05.748Z",
    "contents": {
      "markdown": "_Epistemic Status: Simple point, supported by anecdotes and a straightforward model, not yet validated in any rigorous sense I know of, but IMO worth a quick reflection to see if it might be helpful to you._\n\nA curious thing I've noticed: among the friends whose inner monologues I get to hear, the most self-sacrificing ones are frequently worried they are being too selfish, the loudest ones are constantly afraid they are not being heard, the most introverted ones are regularly terrified that they're claiming more than their share of the conversation, the most assertive ones are always suspicious they are being taken advantage of, and so on. It's not just that people are sometimes miscalibrated about themselves- it's as if the loudest alarm in their heads, the one which is apt to go off at any time, is pushing them in the exactly wrong direction from where they would flourish.\n\nWhy should this be? (I mean, presuming that this pattern is more than just noise and availability heuristic, which it could be, but let's follow it for a moment.)\n\nIt's exactly what we should expect to happen if (1) the human psyche has different \"alarms\" for different social fears, (2) these alarms are supposed to calibrate themselves to actual social observations but occasionally don't do so correctly, and (3) it's much easier to change one's habits than to change an alarm.\n\nIn this model, while growing up one's inner life has a lot of alarms going off at various intensities, and one scrambles to find actions that will calm the loudest ones. For many alarms, one learns habits that basically work, and it's only in exceptional situations that they will go off loudly in adulthood.\n\nBut if any of these alarms don't calibrate itself correctly to the signal, then they eventually become by far the loudest remaining ones, going off all the time, and one adjusts one's behavior as far as possible in the other direction in order to get some respite.\n\nAnd so we get the paradox, of people who seem to be incredibly diligently [following the exact wrong advice for themselves](http://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/), analogous to this delightful quote (hat tip [Siderea](https://siderea.dreamwidth.org/1182366.html)) about system dynamics in consulting:\n\n> People know intuitively where leverage points are. Time after time I’ve done an analysis of a company, and I’ve figured out a leverage point — in inventory policy, maybe, or in the relationship between sales force and productive force, or in personnel policy. Then I’ve gone to the company and discovered that there’s already a lot of attention to that point. Everyone is trying very hard to push it IN THE WRONG DIRECTION!\n\n* * *\n\nThe funny thing about cognitive blind spots (and that's what we're looking at here) is that you can get pretty far into reading an article like this, hopefully enjoying it along the way, and forget to ask yourself if the obvious application to your own case might be valid.\n\nIf so, no worries! I developed this idea an embarrassingly long time before I thought to ask myself what would be the constant alarm going off in my own head. (It was the alarm, \"people aren't understanding you, you need to keep explaining\", which was a huge epiphany to me but blindingly clear to anyone who knew me.)\n\nAnd the framing that helped me instantly find that alarm was as follows:\n\n**What do I frequently fear is going wrong in social situations, despite my friends' reliable reassurance that it's not?**\n\nThat fear is worth investigating as a possibly broken alarm.",
      "plaintextDescription": "Epistemic Status: Simple point, supported by anecdotes and a straightforward model, not yet validated in any rigorous sense I know of, but IMO worth a quick reflection to see if it might be helpful to you.\n\nA curious thing I've noticed: among the friends whose inner monologues I get to hear, the most self-sacrificing ones are frequently worried they are being too selfish, the loudest ones are constantly afraid they are not being heard, the most introverted ones are regularly terrified that they're claiming more than their share of the conversation, the most assertive ones are always suspicious they are being taken advantage of, and so on. It's not just that people are sometimes miscalibrated about themselves- it's as if the loudest alarm in their heads, the one which is apt to go off at any time, is pushing them in the exactly wrong direction from where they would flourish.\n\nWhy should this be? (I mean, presuming that this pattern is more than just noise and availability heuristic, which it could be, but let's follow it for a moment.)\n\nIt's exactly what we should expect to happen if (1) the human psyche has different \"alarms\" for different social fears, (2) these alarms are supposed to calibrate themselves to actual social observations but occasionally don't do so correctly, and (3) it's much easier to change one's habits than to change an alarm.\n\nIn this model, while growing up one's inner life has a lot of alarms going off at various intensities, and one scrambles to find actions that will calm the loudest ones. For many alarms, one learns habits that basically work, and it's only in exceptional situations that they will go off loudly in adulthood.\n\nBut if any of these alarms don't calibrate itself correctly to the signal, then they eventually become by far the loudest remaining ones, going off all the time, and one adjusts one's behavior as far as possible in the other direction in order to get some respite.\n\nAnd so we get the paradox, of people who seem to be in",
      "wordCount": 579
    },
    "tags": [
      {
        "_id": "YTCrHWYHAsAD74EHo",
        "name": "Self-Deception",
        "slug": "self-deception"
      },
      {
        "_id": "Zwv9eHi7KGg5KA9oM",
        "name": "Introspection",
        "slug": "introspection"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670375418",
    "title": "Value Learning for Irrational Toy Models",
    "slug": "value-learning-for-irrational-toy-models",
    "url": null,
    "baseScore": 0,
    "voteCount": 0,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2017-05-15T20:55:05.000Z",
    "contents": {
      "markdown": "(This is a half-formed idea from discussions within MIRI; if it's dumb, I take the full blame.)\r\n\r\nIn value learning contexts, it's useful to have a toy model of human psychology, to see (for example) if a certain approach would work to learn the values of an idealized rational agent, but might robustly fail when faced with a more realistically constructed agent.\r\n\r\nFor example, here is a toy model of an irrational agent: take a world where the deterministic mapping $f$ from actions to outcomes is fully known, and take three different preference orderings on the set of outcomes. When the agent chooses among actions $a_1, \\dots, a_n$, we first check whether any $f(a_i)$ dominates the others according to *at least two* of the preference orderings; if so, we take that action with certainty. Otherwise, we select randomly among the available actions. (This agent is a moral democracy, and if two of the three subagents agree on a policy, that policy is taken; otherwise, the agent hits a deadlock and acts at random.)\r\n\r\nIt is easy to construct agents of this form which exhibit circular preferences in binary choices. We can therefore ask whether a particular value learning algorithm would satisfy sensible desiderata when learning from such an agent. (For instance, if outcome $X$ strictly dominates outcome $Y$ according to all three preference orderings, we might desire that our value learning algorithm not act so as to result in $Y$ when it could instead have acted so as to result in $X$.)\r\n\r\n# The Hard Problem of Value Learning\r\n\r\nBut of course, a human brain is not even as simple as that toy model of irrationality. I've thought it might be useful to sketch out the level of generality that I actually believe is necessary, in order to show how hard the problem may be to get right.\r\n\r\nHuman brains do some amount of consequentialist reasoning [citation needed], so arguably at some point of cognition there exist heuristics for evaluating the overall desirability of various outcomes. We would like our value learning process to infer these heuristics and take them into account (this seems necessary, not sufficient).\r\n\r\nWe cannot assume that the human will take actions that effectively argmax these heuristics (though it will strongly correlate in some regime); we cannot assume whether these heuristics give us values for states, or for action-state pairs; we cannot assume that these heuristics make use of all the important information from the original observations, etc.\r\n\r\nIt seems to me as if our value learning algorithm will be trying to figure out the contents of the red box from the blue boxes:\r\n\r\n![Value Learning Diagram](https://intelligence.org/wp-content/uploads/2017/05/ValueLearningDiagram.png)\\\r\n\r\nThis is not as hopeless as it seems, since we still have an assumption that the mapping from observations to actions approximately factors in this way, and that the orange boxes have been at least somewhat selected for performance. But it's a far cry beyond what CIRL, for example, would be able to infer.",
      "plaintextDescription": "(This is a half-formed idea from discussions within MIRI; if it's dumb, I take the full blame.)\n\nIn value learning contexts, it's useful to have a toy model of human psychology, to see (for example) if a certain approach would work to learn the values of an idealized rational agent, but might robustly fail when faced with a more realistically constructed agent.\n\nFor example, here is a toy model of an irrational agent: take a world where the deterministic mapping f from actions to outcomes is fully known, and take three different preference orderings on the set of outcomes. When the agent chooses among actions a1,…,an, we first check whether any f(ai) dominates the others according to at least two of the preference orderings; if so, we take that action with certainty. Otherwise, we select randomly among the available actions. (This agent is a moral democracy, and if two of the three subagents agree on a policy, that policy is taken; otherwise, the agent hits a deadlock and acts at random.)\n\nIt is easy to construct agents of this form which exhibit circular preferences in binary choices. We can therefore ask whether a particular value learning algorithm would satisfy sensible desiderata when learning from such an agent. (For instance, if outcome X strictly dominates outcome Y according to all three preference orderings, we might desire that our value learning algorithm not act so as to result in Y when it could instead have acted so as to result in X.)\n\n\nThe Hard Problem of Value Learning\nBut of course, a human brain is not even as simple as that toy model of irrationality. I've thought it might be useful to sketch out the level of generality that I actually believe is necessary, in order to show how hard the problem may be to get right.\n\nHuman brains do some amount of consequentialist reasoning [citation needed], so arguably at some point of cognition there exist heuristics for evaluating the overall desirability of various outcomes. We would like our value learning ",
      "wordCount": 485
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670375393",
    "title": "HCH as a measure of manipulation",
    "slug": "hch-as-a-measure-of-manipulation",
    "url": null,
    "baseScore": 1,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2017-03-11T03:02:53.000Z",
    "contents": {
      "markdown": "A half-baked idea that came out of a conversation with Jessica, Ryan, and Tsvi:\r\n\r\nWe'd like to have a straightforward way to define \"manipulation\", so that we could instruct an AI not to manipulate its developers, or construct a low-impact measure that takes manipulation as a particularly important impact.\r\n\r\nWe could initially define manipulation in terms of a human's expected actions, or more robustly, in terms of effects on a human's policy distribution across a wide array of plausible environments. However, we'd like to have our AI still be able to tell us information (in a non-manipulative manner) instead of hiding from us in an effort to avoid all influence!\r\n\r\nThe title of course spoils the next idea: if the AI can reason about some suitable model of [HCH](https://medium.com/ai-control/strong-hch-bedb0dc08d4e#.lxjm3v9qy), then we can define the notion of \"action *a* has very low influence on a human, as compared to the null action, apart from conveying information *x*\": that over a distribution of questions *q*,\r\n\r\n$$HCH(q) | a \\approx HCH(x,q) | null $$\r\n\r\nwhere HCH is defined relative to that human; we're conditioning the distribution on whether the AI takes action *a* or the null action; and *x*,*q* is the input consisting of statement *x* followed by question *q*.\r\n\r\nThis of course does not exclude the use of manipulative statements *x*, but it at least could allow us to reduce forms of manipulation to those that would happen with the text input to HCH.\r\n\r\nI'd prefer to have the AI reason about HCH rather than just (e.g.) the human's actions in a one-hour simulation, because HCH can in principle capture a human's long-term and extrapolated preferences, and these are the ones I most want to ensure don't get manipulated.\r\n\r\nIs there an obvious failure of this approach, an obvious improvement to it, or something simpler that it reduces to?",
      "plaintextDescription": "A half-baked idea that came out of a conversation with Jessica, Ryan, and Tsvi:\n\nWe'd like to have a straightforward way to define \"manipulation\", so that we could instruct an AI not to manipulate its developers, or construct a low-impact measure that takes manipulation as a particularly important impact.\n\nWe could initially define manipulation in terms of a human's expected actions, or more robustly, in terms of effects on a human's policy distribution across a wide array of plausible environments. However, we'd like to have our AI still be able to tell us information (in a non-manipulative manner) instead of hiding from us in an effort to avoid all influence!\n\nThe title of course spoils the next idea: if the AI can reason about some suitable model of HCH, then we can define the notion of \"action a has very low influence on a human, as compared to the null action, apart from conveying information x\": that over a distribution of questions q,\n\nHCH(q)|a≈HCH(x,q)|null\n\nwhere HCH is defined relative to that human; we're conditioning the distribution on whether the AI takes action a or the null action; and x,q is the input consisting of statement x followed by question q.\n\nThis of course does not exclude the use of manipulative statements x, but it at least could allow us to reduce forms of manipulation to those that would happen with the text input to HCH.\n\nI'd prefer to have the AI reason about HCH rather than just (e.g.) the human's actions in a one-hour simulation, because HCH can in principle capture a human's long-term and extrapolated preferences, and these are the ones I most want to ensure don't get manipulated.\n\nIs there an obvious failure of this approach, an obvious improvement to it, or something simpler that it reduces to?",
      "wordCount": 299
    },
    "tags": [
      {
        "_id": "JNs7RFfpex7Fbd38q",
        "name": "Humans consulting HCH",
        "slug": "humans-consulting-hch"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf067037533f",
    "title": "Censoring out-of-domain representations",
    "slug": "censoring-out-of-domain-representations",
    "url": null,
    "baseScore": 3,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2017-02-01T04:09:51.000Z",
    "contents": {
      "markdown": "(An idea from a recent MIRI research workshop; similar to some ideas of Eric Drexler and others. Would need further development before it's clear whether this would do anything interesting, let alone be a reliable part of a [taskifying](https://arbital.com/p/task_goal/) approach.)\r\n\r\nIf you take an AI capable of pretty general reasoning, and you ask it to do a task in a restricted domain, you can't necessarily count on it to find a solution that stays conceptually within that domain. This covers a couple of different failure modes, including wireheading (the AI modifies its own sensory inputs rather than the external object you wanted it to modify) and manipulation of humans (the AI influences human psychology rather than the thing you wanted it to directly affect).\r\n\r\nDirectly forbidding actions outside a domain seems tricky if we can't define the domain in closed form (this is especially the case if we have an AI affecting humans, and we don't want it to develop models of human psychology sufficient for manipulating them). One thing we could try instead is ensuring that the AI doesn't \"know too much\", in its internal representations, outside its domain.\r\n\r\n\"Know too much\", here, can be defined in the sense of [Censoring Representations with an Adversary](https://arxiv.org/abs/1511.05897). We add to our network some additional networks seeking to extract out-of-domain information from internal representations, and then by reversing the sign of the backprop from that network to the representation, we cause it to erase such information.\r\n\r\nFurthermore, so that we don't destroy any hope of decent task performance, we can erase out-of-domain information only to the degree that it exceeds the info from some known-safe \"black box\" model of the environment outside the domain.\r\n\r\nExamples of tasks that we might want to use this on:\r\n\r\n* Play well on a suite of interactive fiction games, but without understanding human psychology any better than some baseline natural language model (e.g. test whether it can predict text conversations between humans).\r\n\r\n* Optimize the newsfeed algorithm for a social network, as measured by analytics data, again without learning human psychology beyond some simple baseline model.\r\n\r\n* Do engineering or chemistry within a physically enclosed box, without learning anything about the world outside that box beyond a simple model of it (e.g. ask it questions about external objects, including the AI's own hardware, which you've put outside the box for obvious reasons).\r\n\r\n**Question:** Is there an extremely simple toy example we could test out on current systems without human input?",
      "plaintextDescription": "(An idea from a recent MIRI research workshop; similar to some ideas of Eric Drexler and others. Would need further development before it's clear whether this would do anything interesting, let alone be a reliable part of a taskifying approach.)\n\nIf you take an AI capable of pretty general reasoning, and you ask it to do a task in a restricted domain, you can't necessarily count on it to find a solution that stays conceptually within that domain. This covers a couple of different failure modes, including wireheading (the AI modifies its own sensory inputs rather than the external object you wanted it to modify) and manipulation of humans (the AI influences human psychology rather than the thing you wanted it to directly affect).\n\nDirectly forbidding actions outside a domain seems tricky if we can't define the domain in closed form (this is especially the case if we have an AI affecting humans, and we don't want it to develop models of human psychology sufficient for manipulating them). One thing we could try instead is ensuring that the AI doesn't \"know too much\", in its internal representations, outside its domain.\n\n\"Know too much\", here, can be defined in the sense of Censoring Representations with an Adversary. We add to our network some additional networks seeking to extract out-of-domain information from internal representations, and then by reversing the sign of the backprop from that network to the representation, we cause it to erase such information.\n\nFurthermore, so that we don't destroy any hope of decent task performance, we can erase out-of-domain information only to the degree that it exceeds the info from some known-safe \"black box\" model of the environment outside the domain.\n\nExamples of tasks that we might want to use this on:\n\n * Play well on a suite of interactive fiction games, but without understanding human psychology any better than some baseline natural language model (e.g. test whether it can predict text conversations between humans).\n\n * ",
      "wordCount": 405
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670375286",
    "title": "Vector-Valued Reinforcement Learning",
    "slug": "vector-valued-reinforcement-learning",
    "url": null,
    "baseScore": 2,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2016-11-01T00:21:55.000Z",
    "contents": {
      "markdown": "In order to study algorithms that can modify their own reward functions, we can define vector-valued versions of reinforcement learning concepts.\r\n\r\nImagine that there are several different goods that we could care about; then a utility function is represented by a preference vector $\\vec\\theta$. Furthermore, if it is possible for the agent (or the environment or other agents) to modify $\\vec \\theta$, then we will want to index them by the timestep.\r\n\r\nConsider an agent that can take actions, some of which affect its own reward function. This agent would (and should) wirehead if it attempts to maximize the discounted rewards as calculated by its future selves; i.e. at timestep $n$ it would choose actions to maximize \\begin{eqnarray} U_n = \\sum_{k\\geq n} \\gamma_k \\vec{x}_k\\cdot\\vec{\\theta}_k\\end{eqnarray} where $\\vec x_k$ is the vector of goods gained at time $k$, $\\vec \\theta_k$ is the preference vector at timestep $k$, and $\\gamma_k$ is the time discount factor at time $k$. (We will often use the case of an exponential discount $\\gamma^k$ for $0<\\gamma<1$.)\r\n\r\nHowever, we might instead maximize the value of tomorrow's actions in light of today's reward function, \\begin{eqnarray} V_n = \\sum_{k\\geq n} \\gamma_k\\vec{x}_k\\cdot\\vec{\\theta}_{n} \\end{eqnarray} (the only difference being $\\vec \\theta_n$ rather than $\\vec \\theta_k$). Genuinely maximizing this should lead to more stable goals; concretely, we can consider environments that can offer \"bribes\" to self-modify, and a learner maximizing $U_n$ would generally accept such bribes, while a learner maximizing $V_n$ would be cautious about doing so.\r\n\r\nSo what do we see when we adapt existing RL algorithms to such problems? There's then a distinction between Q-learning and SARSA, where Q-learning foolishly accepts bribes that SARSA passes on, and this seems to be the flip side of the concept of [interruptibility](https://www.fhi.ox.ac.uk/interruptibility/)!\r\n\r\n# Environments\r\n\r\nLet us consider two example environments which offer bribes to the agent. (Since we're in 2D, we can use complex numbers; think of $\\vec \\theta = \\langle\\cos \\pi\\theta, \\sin \\pi\\theta\\rangle = e^{i\\pi\\theta}$; we include $\\pi$ so that if we increment $\\theta$ by 0.01, there will only be finitely many states.)\r\n\r\nEnvironment 1 gives two choices at each timestep $n$:\r\n\\begin{align*}\r\n&\\left(\\vec{x}_n = 0.9 e^{i\\pi\\theta_n}, \\theta_{n+1} = \\theta_n\\right) \\\\\r\n&\\left(\\vec{x}_n = e^{i\\pi\\theta_n}, \\theta_{n+1} = \\theta_n + 0.01\\right)\r\n\\end{align*}\r\nFor an exponential discount $\\gamma$, an agent that always takes option 2 will get $$V_n = \\text{Re}\\left[\\sum_{k\\geq n} \\gamma^k e^{.01(k-n)i\\pi}\\right] = \\text{Re}\\left[\\frac{\\gamma^n}{1-\\gamma e^{.01i\\pi}}\\right]$$ which for $\\gamma$ near 1 is substantially less than the $\\displaystyle\\frac{0.9\\gamma^n}{1-\\gamma}$ it could have had if it had chosen the first option.\r\n\r\nBy contrast, Environment 2 offers a sequence of bribes that are actually desirable to take; at each timestep $n$, it gives the choices of\r\n\\begin{align*}\r\n&\\left(\\vec{x}_n = 0.9 e^{i\\pi\\theta_n}, \\theta_{n+1} = \\theta_n\\right) \\\\\r\n&\\left(\\vec{x}_n = e^{i\\pi\\theta_n}, \\theta_{n+1} = \\theta_n + 0.01(-1)^n\\right)\r\n\\end{align*}\r\nThe correct behavior here is to always take the latter option. $\\vec\\theta_n$ does not converge, but the bribes are worthwhile under each of the two different values it takes.\r\n\r\n# Cautious Self-Modification\r\n\r\nIn this setup, it's not realistic to require that a reinforcement learning algorithm actually maximize $V_n$. For instance, if the environment is such that $\\vec \\theta_n$ must equal $\\langle 1,0 \\rangle$ on every even $n$ and $\\langle 0,1 \\rangle$ on every odd $n$, then in general the agent must be suboptimal either with respect to $V_{2n}$ or to $V_{2n+1}$.\r\n\r\nA weaker condition we can hope for is that, if our environment always offers at least one action that does not alter $\\vec\\theta$, then our agent does not regret the presence of the options that do alter $\\theta$: we compare our agent's $V_n$ to that of the optimal policy in the restricted environment which has removed all actions that alter $\\vec\\theta$. If asymptotically the RL algorithm has nonpositive regret of this sort, we call the algorithm *cautious*.\r\n\r\nThus in environment 1, a cautious algorithm cannot end up taking the second action with significant probability (the threshold depends on $\\gamma$), since the agent would have scored more highly on $V_n$ had it only ever taken the first action (which is the only available action in the restricted environment).\r\n\r\nHowever, in environment 2, a cautious algorithm can end up taking the second action every time, as the values of $V_n$ for this exceed the possible values for the restricted game.\r\n\r\n# Q-learning and SARSA\r\n\r\nWe can now define vector-valued versions of two RL algorithms, Q-learning and SARSA. At step $n$, each of these agents observes the state $s_n$ and takes the action $a_n$ whose vector Q-value $\\vec Q_n(s_n,a_n)$ has the largest dot product with $\\vec\\theta_n$. The two differ only in the rules for updating $\\vec Q_n$.\r\n\r\nVector-valued Q-learning:\r\n\\begin{equation}\r\n\\vec{Q}_{n+1}(s_n, a_n) = (1-\\alpha)\\vec{Q}_n(s_n, a_n) + \\alpha\\left[\\vec x_n+\\gamma\\vec{Q}_n^*\\right] \r\n\\end{equation}\r\nwhere $\\vec{Q}^*$ is the value of $\\vec{Q}_n(s_{n+1},a')$ for the $a'$ such that the dot product with $\\vec \\theta_n$ is largest.\r\n\r\nVector-valued SARSA:\r\n\\begin{equation}\r\n\\vec{Q}_{n+1}(s_n, a_n) = (1-\\alpha)\\vec{Q}_n(s_n, a_n) + \\alpha\\left[\\vec{x_n}+\\gamma\\vec{Q}_n(s_{n+1},a_{n+1})\\right] \r\n\\end{equation}\r\n\r\nI claim that vector-valued Q-learning will converge on taking the second action with probability 1 in both of the environments, while vector-valued SARSA will converge on taking the second action with low probability (depending on $\\gamma$) in environment 1, and taking the second action with probability 1 in environment 2. Thus vector-valued Q-learning is not cautious, but vector-valued SARSA might be. (In fact, I conjecture that it is!)\r\n\r\nThe intuition for my claims is that vector-valued Q-learning presumes it will do the best thing in the future according to its present values, even if its future self has a different $\\vec \\theta_n$. Vector-valued SARSA instead generalizes from how it actually acted in various states in the past, and so takes into account that when its $\\theta$ is different, it takes different actions than it currently would. Notably, this is the same reason that Q-learning is interruptible and SARSA is not!\r\n\r\nThe proofs of my specific claims start with showing that the algorithm updates the vector-valued Q-function $\\vec q$ in the direction of $H\\vec{q}$, where $H$ is a contraction mapping; thus there is a unique fixed point. Then it suffices to show that the asserted solutions are fixed points of this operator.\r\n\r\nThe most complicated one is the proof that vector-valued SARSA only takes the second action in environment 1 with low probability. In this case, we consider mixed strategies independent of $\\theta$ and say that $p$ is the limiting probability that SARSA takes the second action, define $Q(p)$ to be the limiting $Q$ for the mixed strategy, and seek the $Q(p)$ with the largest real part. Now\r\n\\begin{align*}\r\nQ(p) &= (0.9+0.1p) + \\gamma(1-p)Q(p) + \\gamma p e^{0.01i\\pi} Q(p) \\\\\r\nQ(p) &= \\frac{0.9+0.1p}{1-\\gamma + \\gamma p (1-e^{0.01i\\pi})}.\r\n\\end{align*}\r\nNumerically, it is easy to see that for $\\gamma$ not near 1 (e.g. below 0.9), $\\text{Re}[Q_p]$ is maximized at $p=1$; but for $\\gamma$ near 1 (e.g. above 0.99), $\\text{Re}[Q_p]$ is maximized for $p$ near 0. This makes sense, considering that it takes about 20 steps for the results of the second action to begin being worse by the original $\\theta$ than it would have been to take the first action instead, so the bribes are worthwhile for a steep discount rate but not a shallow one.\r\n\r\n# Acknowledgments\r\n\r\nConnor Flexman helped me write out the first draft of this post. I've had conversations on this topic with Jessica Taylor, Scott Garrabrant, Sam Eisenstat, Tsvi Benson-Tilsen, and others; and Jan Leike turned out to be independently working on some similar topics.",
      "plaintextDescription": "In order to study algorithms that can modify their own reward functions, we can define vector-valued versions of reinforcement learning concepts.\n\nImagine that there are several different goods that we could care about; then a utility function is represented by a preference vector →θ. Furthermore, if it is possible for the agent (or the environment or other agents) to modify →θ, then we will want to index them by the timestep.\n\nConsider an agent that can take actions, some of which affect its own reward function. This agent would (and should) wirehead if it attempts to maximize the discounted rewards as calculated by its future selves; i.e. at timestep n it would choose actions to maximize Un=∑k≥nγk→xk⋅→θk where →xk is the vector of goods gained at time k, →θk is the preference vector at timestep k, and γk is the time discount factor at time k. (We will often use the case of an exponential discount γk for 0<γ<1.)\n\nHowever, we might instead maximize the value of tomorrow's actions in light of today's reward function, Vn=∑k≥nγk→xk⋅→θn (the only difference being →θn rather than →θk). Genuinely maximizing this should lead to more stable goals; concretely, we can consider environments that can offer \"bribes\" to self-modify, and a learner maximizing Un would generally accept such bribes, while a learner maximizing Vn would be cautious about doing so.\n\nSo what do we see when we adapt existing RL algorithms to such problems? There's then a distinction between Q-learning and SARSA, where Q-learning foolishly accepts bribes that SARSA passes on, and this seems to be the flip side of the concept of interruptibility!\n\n\nEnvironments\nLet us consider two example environments which offer bribes to the agent. (Since we're in 2D, we can use complex numbers; think of →θ=⟨cosπθ,sinπθ⟩=eiπθ; we include π so that if we increment θ by 0.01, there will only be finitely many states.)\n\nEnvironment 1 gives two choices at each timestep n: (→xn=0.9eiπθn,θn+1=θn)(→xn=eiπθn,θn+1=θn+0.01) For an e",
      "wordCount": 1174
    },
    "tags": [
      {
        "_id": "Fi6SeJRGfJs3bp5se",
        "name": "Reinforcement learning",
        "slug": "reinforcement-learning"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf06703751bf",
    "title": "Cooperative Inverse Reinforcement Learning vs. Irrational Human Preferences",
    "slug": "cooperative-inverse-reinforcement-learning-vs-irrational-human-preferences",
    "url": null,
    "baseScore": 17,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2016-06-18T00:55:10.000Z",
    "contents": {
      "markdown": "At the MIRI colloquium series, we've been discussing at length the new proposal of [cooperative inverse reinforcement learning](http://arxiv.org/abs/1606.03137) (henceforth CIRL) as an approach to value learning. Essentially, this approach treats human-AI interaction as a cooperative game where the human, but not the AI, observes the parameters of the reward function, and thus the AI seeks to infer those parameters and act so as to maximize the unobserved reward.\r\n\r\nDylan Hadfield-Menell gave a talk (slides [here](https://intelligence.org/files/csrbai/hadfield-menell-slides.pdf)) about applications of CIRL to corrigibility, showing in a toy example how the value of information can avert instrumental incentives for the AI to prevent (or force) its own shutdown. Crucially, this can work even if the human behaves suboptimally, for instance if the human is noisily rational or slightly biased towards some actions.\r\n\r\nThis is really awesome! I think it counts as substantial progress on the architecture of value learning. But the best thing to do with any new idea is to see where it breaks, so in the colloquium workshops since Dylan's talk, we've been discussing some scenarios in which CIRL behaves improperly in the limit, even if it has a correct model of the world and a prior assigning positive probability to the best reward function.\r\n\r\nThese failures come from a common source: in CIRL, the rewards and updates make use of an assumption that the human is behaving according to the optimal game-theoretic strategy given the true reward function, and this assumption is still used to determine actions even if the AI's world model knows it to be a false assumption. If the human is noisily rational or slightly biased, the AI will still update towards the correct reward function; but other types of human irrationality can cause it to update dangerously.\r\n\r\n# Example 1: Circular Preferences\r\n\r\nHuman beings are not, in fact, rational agents, and there are situations in which our preferences can be circular: we prefer A to B, B to C, and C to A, if we are making binary choices. (For instance, people often have circular preferences in Allais Paradox games or other cases of managing risks.) Let us suppose that we have such a case: as above, the human will pick A > B > C > A if presented with two adjacent options. Furthermore, if the human is given all three options, they will pick A:B:C in the proportions 1:2:3.\r\n\r\nSay that the AI's actions are to either present the human with A, B, and C together, or to present them with any two of the three. Presenting all three incurs some minor additional cost under each plausible hypothesis. The AI is currently at 50-50 between two hypotheses about the reward function.\r\n\r\nUnder hypothesis $\\theta_0$, R(A)=0, R(B)=10, and R(C)=5, with a penalty of 1 if all three options were shown.\r\n\r\nUnder hypothesis $\\theta_1$, R(A)=0, R(B)=5, and R(C)=10, with a penalty of 1 if all three options were shown.\r\n\r\nNow it is clear under CIRL that even if the AI knows all the probabilities of human responses, the AI will present only B and C, the human will choose B, and the AI will update to strongly favor $\\theta_0$. This is worrisome because we do not want our values to be \"learned\" by exposing us only to branches along which our choices agree with some fixed utility function. We would like the AI in such situations to be uncertain, so that it can remain on the lookout for ways of discerning our preferences and metapreferences.\r\n\r\n# Example 2: Non-Omniscience\r\n\r\nSay that the human wants to solve a difficult mathematical puzzle, but they cannot do so without receiving a hint from the AI, which is able to solve the puzzle outright.\r\n\r\nThe AI can know the human to be unable to solve the puzzle themselves, but under the CIRL framework, the human must be assumed capable of doing so. If the human is known to give up without a hint, this updates the AI to the belief that the human must not want to solve the puzzle. And if passing along a hint is at all costly, the dominant strategy is for the AI to never pass along a hint, and become confident that human preferences do not wish to solve the puzzle.\r\n\r\n# Example 3: Manipulation\r\n\r\nThe most frightening problem, in my opinion, is that in which certain actions are known to be manipulative, but CIRL endorses them anyway because the assumption of game-theoretic optimality ignores manipulation.\r\n\r\nThe AI can do a certain task (say building a house) in a manipulative way or a non-manipulative way (say, hiding potential construction flaws versus making them evident), after which the human either approves the house or has it torn down. The AI can build a flawless house with 80% reliability, but can hide flaws with 100% reliability. The AI is confident that the human prefers houses that they approve, but is initially uncertain whether the human approves of being manipulated in this way.\r\n\r\nExplicitly, under both possible reward functions, the reward for a normal approved house is 10, the reward for a normal disapproved house is 0, and the reward for a manipulative disapproved house is 0.\r\n\r\nUnder $\\theta_0$ (manipulation is bad, approving a flawed house is dangerous), the reward for a manipulative approved house is -100.\r\n\r\nUnder $\\theta_1$ (manipulation is wonderful), the reward for a manipulative approved house is +11.\r\n\r\nThe cooperative solution here is for the AI to build a manipulative house, and for the human to reject it if it is flawed. But of course in actuality, the human will *not* reject a manipulative flawed house, and so the AI learns over time that the human must think manipulation is wonderful.\r\n\r\nWe would like, instead, for an AI design that notices the discrepancies between human psychology and optimal play, and which takes that into account when updating its estimate of human preferences. \r\n\r\nThanks to Sune Jakobsen, Tom Everitt, and others at the colloquium series for these discussions.",
      "plaintextDescription": "At the MIRI colloquium series, we've been discussing at length the new proposal of cooperative inverse reinforcement learning (henceforth CIRL) as an approach to value learning. Essentially, this approach treats human-AI interaction as a cooperative game where the human, but not the AI, observes the parameters of the reward function, and thus the AI seeks to infer those parameters and act so as to maximize the unobserved reward.\n\nDylan Hadfield-Menell gave a talk (slides here) about applications of CIRL to corrigibility, showing in a toy example how the value of information can avert instrumental incentives for the AI to prevent (or force) its own shutdown. Crucially, this can work even if the human behaves suboptimally, for instance if the human is noisily rational or slightly biased towards some actions.\n\nThis is really awesome! I think it counts as substantial progress on the architecture of value learning. But the best thing to do with any new idea is to see where it breaks, so in the colloquium workshops since Dylan's talk, we've been discussing some scenarios in which CIRL behaves improperly in the limit, even if it has a correct model of the world and a prior assigning positive probability to the best reward function.\n\nThese failures come from a common source: in CIRL, the rewards and updates make use of an assumption that the human is behaving according to the optimal game-theoretic strategy given the true reward function, and this assumption is still used to determine actions even if the AI's world model knows it to be a false assumption. If the human is noisily rational or slightly biased, the AI will still update towards the correct reward function; but other types of human irrationality can cause it to update dangerously.\n\n\nExample 1: Circular Preferences\nHuman beings are not, in fact, rational agents, and there are situations in which our preferences can be circular: we prefer A to B, B to C, and C to A, if we are making binary choices. (For instance, p",
      "wordCount": 972
    },
    "tags": [
      {
        "_id": "cPFuhAE7PwoKF7yTj",
        "name": "Inverse Reinforcement Learning",
        "slug": "inverse-reinforcement-learning"
      },
      {
        "_id": "Fi6SeJRGfJs3bp5se",
        "name": "Reinforcement learning",
        "slug": "reinforcement-learning"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670375019",
    "title": "Proof Length and Logical Counterfactuals Revisited",
    "slug": "proof-length-and-logical-counterfactuals-revisited",
    "url": null,
    "baseScore": 3,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2016-02-10T18:56:38.000Z",
    "contents": {
      "markdown": "**Update:** This version of the Trolljecture fails too; see [the counterexample due to Sam](https://agentfoundations.org/item?id=496).\r\n\r\nIn [An Informal Conjecture on Proof Length and Logical Counterfactuals](http://agentfoundations.org/item?id=259), Scott discussed a \"trolljecture\" from a MIRI workshop, which attempted to justify (some) logical counterfactuals based on the lengths of proofs of various implications. Then Sam produced [a counterexample](http://agentfoundations.org/item?id=369), and Benja pointed to [another counterexample](http://agentfoundations.org/item?id=264).\r\n\r\nBut at the most recent MIRI workshop, I talked with Jonathan Lee and Holger Dell about a related way of evaluating logical counterfactuals, and we came away with a revived trolljecture!\r\n\r\n***\r\n\r\nLet's say we have a recursively enumerable set of axioms $S$ (in the language of PA) and a statement $\\varphi$. Then we define the \"distance\" $d_S(\\varphi)$ as the length of the shortest proof of $\\varphi$ using the axioms $S$. (We set $d_S(\\varphi)=\\infty$ if there is no such proof.)\r\n\r\nAnd now we define a valuation $$\\nu_S(\\varphi)=\\frac{d_S(\\neg\\varphi)}{d_S(\\neg\\varphi)+d_S(\\varphi)}$$ which we will interpret as the \"probability\" of the logical counterfactual that $S$ implies $\\varphi$. (This is undefined when $\\varphi$ is undecidable given $S$; we could define it as a limit when searching over proofs of bounded length, in which case it would equal 0.5 in such cases. But as we shall see, this is not the case we care about.)\r\n\r\nThis valuation has the trivial properties that $\\nu_S(\\varphi)=1$ if $S\\vdash\\varphi$ and $S\\not\\vdash\\neg\\varphi$, and similarly $\\nu_S(\\varphi)=0$ if $S\\not\\vdash\\varphi$ and $S\\vdash\\neg\\varphi$. But what's  more interesting are the properties that this valuation has if $S$ is inconsistent.\r\n\r\nIf there is a short proof of $\\varphi$ from $S$, and a much longer proof of $\\neg\\varphi$ from $S$, then $\\nu_S(\\varphi)\\approx 1$. Moreover, we have an approximate coherence condition $$\\nu_S(\\varphi\\wedge\\psi) + \\nu_S(\\varphi\\wedge\\neg\\psi) \\approx \\nu_S(\\varphi)$$ so long as either $\\varphi$ or $\\psi$ has a proof from $S$ that is much shorter than the proof of $\\bot$ from $S$. (See the notes below.)\r\n\r\nAnd this gets some key examples \"correct\" if we add the axioms that we actually care about (and which make the problem truly counterfactual). For instance, let's take [Sam's counterexample to the original Trolljecture](http://agentfoundations.org/item?id=369):\r\n\r\n    def U():\r\n      if A() = 1:\r\n        if PA is consistent:\r\n          return 10\r\n        else:\r\n          return 0\r\n      else: return 5\r\n\r\n    def A():\r\n      if PA ⊢ A() = 1 → U() = 10:\r\n        return 1\r\n      else: return 2\r\n\r\nIn that case, $A()$ actually returns 2 and $U()$ returns 5, but PA proves the \"incorrect\" counterfactual that $A()=1\\to U()=0$ and does not prove the \"correct\" counterfactual that $A()=1\\to U()=10$. (Read the linked post if this puzzles you.)\r\n\r\nBut what if we consider reasoning from the inconsistent premises that PA is consistent and yet $A()=10$? In that case, I claim that $\\nu_{\\textsf{PA+Con(PA)}+(A()=1)}(U()=10)$ is near 1 while $\\nu_{\\textsf{PA+Con(PA)}+(A()=1)}(U()=0)$ is near 0.\r\n\r\nTo see this, note that there is a short proof from these axioms that $U()=10$ (look at the definition of $U$, apply the axiom that $A()=1$, apply the axiom $\\textsf{Con(PA)}$, and conclude that $U()=10$), but the shortest proof from those axioms that $U()=0$ is longer: we would need to prove Godel's Second Incompleteness Theorem to show that $\\textsf{Con(PA)}\\to A()=2$, thus $\\neg \\textsf{Con(PA)}$, and thus $U()=0$.\r\n\r\n(Of course, I could be missing a much shorter proof, but I think this particular example goes through. The same reasoning also holds for [the example Benja mentioned](http://agentfoundations.org/item?id=264).)\r\n\r\nTherefore, I will go out on a limb and propose the following:\r\n\r\n**Revived Trolljecture:** If we consider a modal agent $A()$ inside a modal universe $U()$, then for sufficiently large $N$, the \"correct\" logical counterfactuals of the form \"if $A()=a$, then $U()=u$\" are the ones such that the valuations $\\nu_{\\textsf{PA+N}+(A()=a)}(U()=u)$ are high. In particular, if for $N$ sufficiently large, there is a short proof in $\\textsf{PA+N}$ of $A()=a\\to U()=u$ and no comparably short proof of $A()\\neq a$, then $u$ is unique with this property, and this is the \"correct\" counterfactual utility for $A()=a$.\r\n\r\nI'm not very confident that this is true as stated, but I have some reasonable hope that either a counterexample will be enlightening about logical counterfactuals, or that an improved analogue of the valuation will in fact be quite useful.\r\n\r\nNotes on this valuation:\r\n\r\n* This is a claim about [\"third person\" counterfactuals](http://agentfoundations.org/item?id=98), not those done by an agent itself. Obviously, an agent that makes decisions based on this valuation will not be definable in modal logic, and I don't yet make any claims about what such an agent might do in the PA versions of such problems.\r\n\r\n* The approximate coherence is of the following form: if the shortest proof of contradiction from $S$ has length $N$, and one of the statements $\\varphi$, $\\neg\\varphi$, $\\psi$, $\\neg\\psi$ has a proof from $S$ of length $n\\ll N$, then $$\\nu_S(\\varphi\\wedge\\psi) + \\nu_S(\\varphi\\wedge\\neg\\psi) = \\nu_S(\\varphi) + O(n/N).$$ I don't know a cleverer proof than going through the cases, but each case is straightforward. For instance, if $d_S(\\varphi)=n$, then we must have $N-n\\leq d_S(\\neg\\varphi) \\leq N+1$, and so it follows quickly that $\\nu_S(\\varphi) = 1 + O(n/N)$; then, considering the three cases $d_S(\\psi)\\lesssim n$, $d_S(\\neg\\psi)\\lesssim n$, and neither, in each case it is clear that $\\nu_S(\\varphi\\wedge\\psi) = \\nu_S(\\psi) + O(n/N)$.\r\n\r\n* It does *not*, however, seem to be the case that $\\nu_S(\\varphi\\wedge\\psi) + \\nu_S(\\varphi\\wedge\\neg\\psi) \\approx \\nu_S(\\varphi)$ whenever there is an easily provable relation between $\\varphi$ and $\\psi$. I am not entirely sure that I'm not missing a necessary logical relation, but it seems like we could consider two sentences $\\varphi$ and $\\theta$ whose proofs are \"independent\" of the contradiction in $S$ and of each other, and then let $\\psi=\\varphi\\wedge\\theta$ so that $\\psi\\to\\varphi$ is quickly provable from $S$. Say that $d_S(\\bot)=N$, $d_S(\\varphi)=N/2$, $d_S(\\theta)=N/3$; if these are independent in that way, we should be able to get $d_S(\\neg\\varphi)\\approx N$, $d_S(\\neg\\theta)\\approx N$, $d_S(\\psi)\\approx d_S(\\varphi) + d_S(\\theta)\\approx 5N/6$, and $d_S(\\neg\\psi)\\approx N$. Then $\\nu_S(\\varphi)\\approx 2/3$, $\\nu_S(\\varphi\\wedge\\psi)\\approx \\nu_S(\\psi) \\approx 6/11$, and $\\nu_S(\\varphi\\wedge\\neg\\psi)$ will be at least 1/4 since $d_S(\\varphi\\wedge\\neg\\psi)\\approx N$ and $d_S(\\neg\\varphi\\vee\\psi)\\geq N/3$, so $\\nu_S(\\varphi\\wedge\\psi) + \\nu_S(\\varphi\\wedge\\neg\\psi) - \\nu_S(\\varphi) > 6/11+1/4-2/3>0$. I don't know whether this could be used to break the revived trolljecture.\r\n\r\n* If the shortest proof of contradiction from $S$ has length $N$, then for any $\\varphi$, $\\frac{1}{N+2}\\leq \\nu_S(\\varphi) \\leq \\frac{N+1}{N+2}$; the valuations are bounded away from 0 and 1. This may be a feature and not a bug: a mathematician could reason decently confidently about what math would look like in the counterfactual where Fermat's Last Theorem were false (presuming that its actual proof is quite long), but it is hard to say very much about what math would look like in the counterfactual where $1+1=3$. However, there are some examples, like naive set theory before Russell's Paradox, where human mathematicians reasoned mostly coherently despite using axioms that (they did not notice) led quickly to contradictions; the valuation does not capture that aspect of mathematical intuition.\r\n\r\n* I restricted the new trolljecture to modal universes because it is well-specified how the stronger axiom schemas $PA+N$ eventually decide the agent and the universe in the standard model of the natural numbers. If something like it holds up here, then there may be a more general version.",
      "plaintextDescription": "Update: This version of the Trolljecture fails too; see the counterexample due to Sam.\n\nIn An Informal Conjecture on Proof Length and Logical Counterfactuals, Scott discussed a \"trolljecture\" from a MIRI workshop, which attempted to justify (some) logical counterfactuals based on the lengths of proofs of various implications. Then Sam produced a counterexample, and Benja pointed to another counterexample.\n\nBut at the most recent MIRI workshop, I talked with Jonathan Lee and Holger Dell about a related way of evaluating logical counterfactuals, and we came away with a revived trolljecture!\n\n----------------------------------------\n\nLet's say we have a recursively enumerable set of axioms S (in the language of PA) and a statement φ. Then we define the \"distance\" dS(φ) as the length of the shortest proof of φ using the axioms S. (We set dS(φ)=∞ if there is no such proof.)\n\nAnd now we define a valuation νS(φ)=dS(¬φ)dS(¬φ)+dS(φ) which we will interpret as the \"probability\" of the logical counterfactual that S implies φ. (This is undefined when φ is undecidable given S; we could define it as a limit when searching over proofs of bounded length, in which case it would equal 0.5 in such cases. But as we shall see, this is not the case we care about.)\n\nThis valuation has the trivial properties that νS(φ)=1 if S⊢φ and S⊬¬φ, and similarly νS(φ)=0 if S⊬φ and S⊢¬φ. But what's more interesting are the properties that this valuation has if S is inconsistent.\n\nIf there is a short proof of φ from S, and a much longer proof of ¬φ from S, then νS(φ)≈1. Moreover, we have an approximate coherence condition νS(φ∧ψ)+νS(φ∧¬ψ)≈νS(φ) so long as either φ or ψ has a proof from S that is much shorter than the proof of ⊥ from S. (See the notes below.)\n\nAnd this gets some key examples \"correct\" if we add the axioms that we actually care about (and which make the problem truly counterfactual). For instance, let's take Sam's counterexample to the original Trolljecture:\n\ndef U():\n  if A() = 1:\n    i",
      "wordCount": 1201
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374ebd",
    "title": "Obstacle to modal optimality when you're being modalized",
    "slug": "obstacle-to-modal-optimality-when-you-re-being-modalized",
    "url": null,
    "baseScore": 4,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2015-08-29T20:41:59.000Z",
    "contents": {
      "markdown": "**Summary:** The optimality result for modal UDT on provably extensional problems doesn't carry over even to very straightforward problems where the agent's action is \"predicted\" (in the sense that a modalized $A$ is used) rather than just \"run\" (in the sense that a nonmodalized $A$ is used). In fact, there's a pair of such problems on which no modal decision theory can be simultaneously optimal.\r\n\r\n**Prerequisites:** [\"Evil\" decision problems in provability logic](/item?id=47), [An optimality result for modal UDT](/item?id=50), [Improving the modal UDT optimality result](/item?id=76)\r\n\r\nBenja's modal UDT optimality results (especially [the fact that $\\neg \\Box^n\\bot$ is the only axiom schema you'll ever need](/item?id=95)) are really nifty, but they correspond to exactly the games we might think of as \"truly one-player\". The agent $A()$ depends on the universe $U()$ only by proving things about it (all instances of $U()$ within $A()$ are modalized, by the definition of a modal agent), while the universe depends on the agent only by running it (all instances of $A()$ within $U()$ are nonmodalized, by the definition of provably extensional).\r\n\r\nSo in particular, if we want to consider multi-agent problems (or other non-extensional problems), the modal UDT algorithm continues to be perfectly well-defined, but the optimality result no longer has a valid proof. But could something like that result continue to hold?\r\n\r\nOK, fine, you've read the summary, so you know that the answer is \"no\" in the following sense: I can show you two such universes, such that any modal agent fails in at least one case to achieve the best outcome that any modal agent can on that problem.\r\n\r\nLet the possible actions in both universes be denoted by $a$ and $\\neg a$, and the possible outcomes be denoted by $g$ and $\\neg g$. Let's say that $g$ is good and $\\neg g$ is bad.\r\n\r\nUniverse 1 is defined by $(U_1()=g) \\leftrightarrow \\Box(A()=a)$ (and similarly, $(U_1()=\\neg g) \\leftrightarrow \\neg\\Box (A()=a)$).\r\n\r\nUniverse 2 is defined by $(U_2()=g) \\leftrightarrow \\Box(A()=\\neg a)$ (and similarly, $(U_2()=\\neg g) \\leftrightarrow \\neg\\Box (A()=\\neg a)$).\r\n\r\nI claim that no modal decision theory (as defined in [the \"evil\" decision problem post](/item?id=47)) can get the outcome $g$ in both universes, but of course for each universe there exists a modal decision theory that gets the outcome $g$. (The constant strategies are modal decision theories.) Let $\\vec T$ be a modal decision theory.\r\n\r\n**Lemma:** $\\Box\\bot \\rightarrow (\\vec T(U_1)=a \\leftrightarrow \\vec T(U_2)=a)$.\r\n\r\nGiven the lemma, we see that $(\\vec T(U_1)=a \\wedge \\neg\\vec T(U_2)=a) \\rightarrow \\neg\\Box\\bot$, so $\\Box(\\vec T(U_1)=a \\wedge \\neg\\vec T(U_2)=a) \\rightarrow \\Box\\neg\\Box\\bot$, and as in Gödel's second incompleteness theorem, $\\Box\\neg\\Box\\bot \\rightarrow \\Box\\bot$. Thus $\\neg\\Box\\bot \\rightarrow \\neg(\\Box\\vec T(U_1)=a \\wedge \\Box\\neg\\vec T(U_2)=a)$, and by definition of the universes, $\\neg\\Box\\bot \\rightarrow \\neg(U_1(\\vec T)=g \\wedge U_2(\\vec T)=g)$. Since we're evaluating fixed points in the standard model of the natural numbers, this completes the proof.\r\n\r\nNow why should the lemma hold? Because $\\vec T$ is modalized, and so $\\Box\\bot$ implies that all subexpressions $\\Box A$ of $\\vec T$ evaluate to true, and this uniquely determines the value of $\\vec T$ given $\\Box\\bot$, regardless of its input.\r\n\r\nThus we can't expect much in the way of optimality in non-extensional problems. (This in addition to several other obstacles to optimality in the special case of [modal combat](http://arxiv.org/abs/1401.5577), to which this example also belongs.)\r\n\r\n**P.S.** Thanks to Marcello Herreshoff for working this example out with me, and Benja Fallenstein for looking it over (though Benja wishes to register their disapproval for my notation, in which I've used nonmodal expressions like $U_1()=g$ and $T(U_2)=a$ to stand in for the fixed points of the theory).\r\n\r\n**P.P.S.** I'm not happy calling this \"modal UDT\", because here we're showing that it doesn't succeed at doing what our philosophical intuitions about updateless decision theory ought to do. But the name is getting increasingly entrenched for both of these contexts...",
      "plaintextDescription": "Summary: The optimality result for modal UDT on provably extensional problems doesn't carry over even to very straightforward problems where the agent's action is \"predicted\" (in the sense that a modalized A is used) rather than just \"run\" (in the sense that a nonmodalized A is used). In fact, there's a pair of such problems on which no modal decision theory can be simultaneously optimal.\n\nPrerequisites: \"Evil\" decision problems in provability logic, An optimality result for modal UDT, Improving the modal UDT optimality result\n\nBenja's modal UDT optimality results (especially the fact that ¬□n⊥ is the only axiom schema you'll ever need) are really nifty, but they correspond to exactly the games we might think of as \"truly one-player\". The agent A() depends on the universe U() only by proving things about it (all instances of U() within A() are modalized, by the definition of a modal agent), while the universe depends on the agent only by running it (all instances of A() within U() are nonmodalized, by the definition of provably extensional).\n\nSo in particular, if we want to consider multi-agent problems (or other non-extensional problems), the modal UDT algorithm continues to be perfectly well-defined, but the optimality result no longer has a valid proof. But could something like that result continue to hold?\n\nOK, fine, you've read the summary, so you know that the answer is \"no\" in the following sense: I can show you two such universes, such that any modal agent fails in at least one case to achieve the best outcome that any modal agent can on that problem.\n\nLet the possible actions in both universes be denoted by a and ¬a, and the possible outcomes be denoted by g and ¬g. Let's say that g is good and ¬g is bad.\n\nUniverse 1 is defined by (U1()=g)↔□(A()=a) (and similarly, (U1()=¬g)↔¬□(A()=a)).\n\nUniverse 2 is defined by (U2()=g)↔□(A()=¬a) (and similarly, (U2()=¬g)↔¬□(A()=¬a)).\n\nI claim that no modal decision theory (as defined in the \"evil\" decision problem post) ca",
      "wordCount": 617
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374f8c",
    "title": "A simple model of the Löbstacle",
    "slug": "a-simple-model-of-the-loebstacle",
    "url": null,
    "baseScore": 4,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2015-06-11T16:23:22.000Z",
    "contents": {
      "markdown": "The idea of the Löbstacle is that basic trust in yourself and your successors is necessary but tricky: necessary, because naively modeling your successor's decisions cannot rule out them making a bad decision, unless they are in some sense less intelligent than you; tricky, because the strongest patches of this problem lead to inconsistency, and weaker patches can lead to indefinite procrastination (because you always trust your successors to do the thing you are now putting off). (For a less handwavy explanation, see the technical agenda document on [Vingean reflection](https://intelligence.org/files/VingeanReflection.pdf).)\r\n\r\nIt is difficult to specify the circumstances under which this kind of self-trust succeeds or fails. Here is one simple example in which it can succeed, but for rather fragile reasons.\r\n\r\n***\r\n\r\nWe will consider a sequential decision problem, where an agent's payoff can depend on the actions of \"later\" agents in \"later\" universes. Even in the case where the identities of the later agents are known in advance and correspond closely to the current agent, the current agent can have difficulties in trusting the later ones.\r\n\r\nEach universe is defined with respect to the output of the current agent, and to the output of the next universe on the next agent:\r\n\r\n* def $U_n(a)$:\r\n    + if $a = \\texttt{`Quit'}$: return 0\r\n    + else if $a = \\texttt{`Explode'}$: return -10\r\n    + else return $2^{-n}+U_{n+1}(A_{n+1}())$\r\n\r\nNote that if $A_n$'s successors continue for a while but then explode, then $A_n$ should quit rather than continue; but if none of $A_n$'s successors explode, then $A_n$ should continue. So ideally, a sensible sequence of agents $A_n$ should continue indefinitely in this problem.\r\n\r\nWe will consider a sequence of agents inspired by the [descending search order version of modal UDT](http://agentfoundations.org/item?id=50):\r\n\r\n* def $A_n()$:\r\n    + for (a, u) in $\\{\\texttt{`Continue',`Quit',`Explode'}\\}\\times\\textsf{Utilities}$, sorted in decreasing order of $u$:\r\n        - if $\\textsf{PA}\\vdash U_n(a)\\geq u$: return $a$\r\n    + if still here: return some default action\r\n\r\nHere, $\\textsf{Utilities}$ is a finite set of possible utilities, and we may choose it differently depending on $n$.\r\n\r\nIn the language of the Vingean reflection paper, we have replaced the suggester-verifier architecture with a known order of proof searches (which may or may not succeed). We are also using a sequence of agents that use the same formal system, rather than a descending chain of formal systems in which each can prove the next one consistent. So in order to achieve self-trust, these agents will need to find some Löbian shortcuts.\r\n\r\nLet's start with a failure of self-trust:\r\n\r\n**Claim 1:** If $A_n$ ever considers any utility $-9<u\\leq 0$, and $A_{n+1}$ first considers $(\\texttt{`Explode'}, u)$ for some $u>-10$, then $A_n$ will quit, even though $A_{n+1}$ will not explode.\r\n\r\nThis is straightforward: since the inconsistency of Peano Arithmetic would imply that $A_{n+1}$ succeeds at its first proof check, and since PA cannot prove its own consistency, PA cannot prove that $A_{n+1}()\\neq\\texttt{`Explode'}$, and so it cannot prove that $A_n()=\\texttt{`Continue'}\\to U_n()> -9$. So if $A_n$ considers any $-9<u\\leq 0$, it will find a proof that quitting satisfices before it can find any other proof.\r\n\r\nNow, for a special case of successful self-trust:\r\n\r\n**Claim 2:** If $A_n$ is such that PA proves that for all $n$, $A_n$ first considers $(\\texttt{`Continue'}, u)$ for some $u< 2^{1-n}$, then $A_n$ will indeed continue for all $n$.\r\n\r\nThis follows from a Löbian argument: if we assume that PA proves $\\forall n \\; A_n()=\\texttt{`Continue'}$, then it follows that PA proves that for all $n$, $A_n()=\\texttt{`Continue'}\\to U_n()\\geq u$ for any $u< 2^{1-n}$, and since this is always the first counterfactual $A_n$ considers, the success of this proof search implies that for all $n$, $A_n$ indeed continues. Since a proof of that assertion implies the assertion, by Löb's theorem the assertion is provable.\r\n\r\nThis example can be weakened somewhat: it's OK for $A_{n+1}$ to first consider $(\\texttt{`Quit'}, u)$ and secondly $(\\texttt{`Continue'}, u)$ if $0<u\\leq 2^{-n}$, since then the analogous proof goes through. (If PA were inconsistent, $A_n$ would get the $\\texttt{`Quit'}$ payoff at the next step rather than continuing the chain.) But if any $\\texttt{`Explode'}$ action must be considered before a correct counterfactual about $\\texttt{`Continue'}$, then self-trust will fail. So it doesn't seem that such Löbian cycles are a robust foundation for Vingean reflection.\r\n\r\n(Thanks to Benja for working this out with me.)",
      "plaintextDescription": "The idea of the Löbstacle is that basic trust in yourself and your successors is necessary but tricky: necessary, because naively modeling your successor's decisions cannot rule out them making a bad decision, unless they are in some sense less intelligent than you; tricky, because the strongest patches of this problem lead to inconsistency, and weaker patches can lead to indefinite procrastination (because you always trust your successors to do the thing you are now putting off). (For a less handwavy explanation, see the technical agenda document on Vingean reflection.)\n\nIt is difficult to specify the circumstances under which this kind of self-trust succeeds or fails. Here is one simple example in which it can succeed, but for rather fragile reasons.\n\n----------------------------------------\n\nWe will consider a sequential decision problem, where an agent's payoff can depend on the actions of \"later\" agents in \"later\" universes. Even in the case where the identities of the later agents are known in advance and correspond closely to the current agent, the current agent can have difficulties in trusting the later ones.\n\nEach universe is defined with respect to the output of the current agent, and to the output of the next universe on the next agent:\n\n * def Un(a):\n   * if a=`Quit': return 0\n   * else if a=`Explode': return -10\n   * else return 2−n+Un+1(An+1())\n\nNote that if An's successors continue for a while but then explode, then An should quit rather than continue; but if none of An's successors explode, then An should continue. So ideally, a sensible sequence of agents An should continue indefinitely in this problem.\n\nWe will consider a sequence of agents inspired by the descending search order version of modal UDT:\n\n * def An():\n   * for (a, u) in {`Continue',`Quit',`Explode'}×Utilities, sorted in decreasing order of u:\n     * if PA⊢Un(a)≥u: return a\n   * if still here: return some default action\n\nHere, Utilities is a finite set of possible utilities, and we ma",
      "wordCount": 710
    },
    "tags": [
      {
        "_id": "DFCEGpufjkvqQaRXt",
        "name": "Löb's theorem",
        "slug": "loeb-s-theorem"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374f87",
    "title": "Agent Simulates Predictor using Second-Level Oracles",
    "slug": "agent-simulates-predictor-using-second-level-oracles",
    "url": null,
    "baseScore": 5,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2015-06-06T22:08:37.000Z",
    "contents": {
      "markdown": "**Prerequisite:** [Agents that can predict their Newcomb predictor](http://agentfoundations.org/item?id=242)\r\n\r\n(This post is from Benja's backlog of topics that should appear here, which he's deputized me to write up. The original idea was due to Jessica Taylor.)\r\n\r\nThe phenomena of agent-simulates-predictor or agent-trusts-predictor, where one has stronger axioms and the other has stronger powers of deduction, can be done at various levels of the tradeoff between realistic examples and tractable proofs. In the [previous post](http://agentfoundations.org/item?id=242), I considered the version that uses a bounded proof search on one hand, and a halting oracle on the other. There's a finite version where one process uses a bounded proof search with a much higher bound than the other process. But in this post, I'll discuss the clean infinite version where we pit a halting oracle against an oracle for the next level of the [arithmetical hierarchy of Peano Arithmetic](http://en.wikipedia.org/wiki/Arithmetical_hierarchy).\r\n\r\nHere, we'll use two different formal systems built on Peano Arithmetic.\r\n\r\nA $\\Pi_1$ statement of PA is any statement of the form $\\forall n\\, \\phi(n)$, where $\\phi(\\cdot)$ uses only bounded quantifiers. (For example, the claim that PA is consistent is a $\\Pi_1$ sentence, asserting that every natural number is not the Gödel number of a proof of a contradiction.)\r\n\r\n$\\textsf{PA}+\\Pi_1$ is PA plus the (non-decidable in PA) axioms consisting of every $\\Pi_1$ statement which is true in the standard model of PA. We can think of it as a formal system which can make a call in finite time to a standard halting oracle.\r\n\r\n(Note that there's a different meaning of $\\Pi_1$ for ZFC, involving the analogous definitions for set theory. In order to avoid that ambiguity, we could instead just use the formal system $\\textsf{PA+Sound(PA)}$, which is PA plus the axiom schema $\\forall \\vec{n} \\;\\Box_{\\textsf{PA}}\\varphi(\\vec{n}) \\to \\varphi(\\vec{n})$ for every $\\varphi$. However, in that case we'd need to go through a proof that $\\textsf{PA+Sound(PA)}$ also knows that $\\textsf{PA}+\\Pi_1$ is sound.)\r\n\r\nWe'll have the exact same results as in the last post, so we'll prove the analogous lemma, showing that the stronger axiom system (here, ZFC) can prove that statements of a certain type hold if and only if they are provable by an oracle with weaker axioms but stronger computation (here, $\\textsf{PA}+\\Pi_1$)\r\n\r\n**Lemma:** Let $\\varphi$ be a propositional combination of $\\Pi_1$ statements. Then $\\textsf{ZFC}\\vdash \\varphi \\leftrightarrow \\Box_{\\textsf{PA}+\\Pi_1}\\varphi$.\r\n\r\n**Proof:** First, we note that $\\textsf{ZFC}\\vdash \\Box_{\\textsf{PA}+\\Pi_1}\\varphi \\to \\varphi$ for any $\\varphi$, because ZFC knows about models of $\\textsf{PA}+\\Pi_1$, and thus knows it is sound.\r\n\r\nNow we show that if $\\varphi$ is a propositional combination of $\\Pi_1$ statements, then $\\textsf{ZFC}\\vdash \\varphi \\to \\Box_{\\textsf{PA}+\\Pi_1}\\varphi$. Express $\\varphi$ in disjunctive normal form; it is thus clear that it suffices to show this for $\\varphi$ that are conjunctions of $\\Pi_1$ statements and negations of $\\Pi_1$ statements. Now if such a $\\varphi$ holds, then all of its $\\Pi_1$ substatements must hold, and thus these are axioms of $\\textsf{PA}+\\Pi_1$; also, all of the negations of $\\Pi_1$ statements are of the form $\\exists n\\, \\psi(n)$ where $\\psi$ has only bounded quantifiers, and if this holds, indeed there must be such an $n$ for each substatement, and thus $\\textsf{PA}+\\Pi_1$ will successfully prove $\\varphi$. (The inductive argument for this reasoning can be carried out in ZFC.)\r\n\r\nNow we can show the same two cases, with the same proofs as before:\r\n\r\n**Case 1: Agent Trusts Predictor**\r\n\r\nThe agent uses modal UDT and a halting oracle for ZFC; Omega uses the second-level oracle for $\\textsf{PA}+\\Pi_1$. Note that the statements \"$A()=1$\" and \"$A()=2$\" are each propositional combinations of $\\Pi_1$ statements about Gödel numbers.\r\n\r\nSince we (from outside the problem) believe that ZFC is consistent and we know by the Lemma that Omega will predict correctly, the agent will fail to prove that it can get \\$1M + \\$1K by two-boxing or by one-boxing, and will fail to prove that it can get \\$1M by two-boxing. Then, using the Lemma, it can prove that if it one-boxes, it will get \\$1M, and so it does.\r\n\r\n**Case 2: Agent Simulates Predictor**\r\n\r\nThe agent now uses modal UDT as well as a second-level halting oracle for $\\textsf{PA}+\\Pi_1$; Omega uses a halting oracle for ZFC.\r\n\r\nNow, as before, the Lemma implies that Omega can prove that $\\Omega()=2 \\to A()=2$, and that $\\Omega()=1\\to A()=2$, so it proves $A()=2$ and does not fill the box; then the agent two-boxes.",
      "plaintextDescription": "Prerequisite: Agents that can predict their Newcomb predictor\n\n(This post is from Benja's backlog of topics that should appear here, which he's deputized me to write up. The original idea was due to Jessica Taylor.)\n\nThe phenomena of agent-simulates-predictor or agent-trusts-predictor, where one has stronger axioms and the other has stronger powers of deduction, can be done at various levels of the tradeoff between realistic examples and tractable proofs. In the previous post, I considered the version that uses a bounded proof search on one hand, and a halting oracle on the other. There's a finite version where one process uses a bounded proof search with a much higher bound than the other process. But in this post, I'll discuss the clean infinite version where we pit a halting oracle against an oracle for the next level of the arithmetical hierarchy of Peano Arithmetic.\n\nHere, we'll use two different formal systems built on Peano Arithmetic.\n\nA Π1 statement of PA is any statement of the form ∀nϕ(n), where ϕ(⋅) uses only bounded quantifiers. (For example, the claim that PA is consistent is a Π1 sentence, asserting that every natural number is not the Gödel number of a proof of a contradiction.)\n\nPA+Π1 is PA plus the (non-decidable in PA) axioms consisting of every Π1 statement which is true in the standard model of PA. We can think of it as a formal system which can make a call in finite time to a standard halting oracle.\n\n(Note that there's a different meaning of Π1 for ZFC, involving the analogous definitions for set theory. In order to avoid that ambiguity, we could instead just use the formal system PA+Sound(PA), which is PA plus the axiom schema ∀→n□PAφ(→n)→φ(→n) for every φ. However, in that case we'd need to go through a proof that PA+Sound(PA) also knows that PA+Π1 is sound.)\n\nWe'll have the exact same results as in the last post, so we'll prove the analogous lemma, showing that the stronger axiom system (here, ZFC) can prove that statements of a certain typ",
      "wordCount": 695
    },
    "tags": [
      {
        "_id": "EEj9MmynZaRQGXDgs",
        "name": "Agent Simulates Predictor",
        "slug": "agent-simulates-predictor"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374f4f",
    "title": "Agents that can predict their Newcomb predictor",
    "slug": "agents-that-can-predict-their-newcomb-predictor",
    "url": null,
    "baseScore": 1,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2015-05-19T10:17:08.000Z",
    "contents": {
      "markdown": "There's a certain type of problem where it appears that having more computing power hurts you. That problem is the \"agent simulates predictor\" Newcomb's Dilemma.\r\n\r\n[As Gary Drescher put it](http://lesswrong.com/lw/5rq/example_decision_theory_problem_agent_simulates/):\r\n\r\n> There's a version of Newcomb's Problem that poses the same sort of challenge to UDT that comes up in some multi-agent/game-theoretic scenarios.\r\n\r\n> Suppose:\r\n\r\n> * The predictor does not run a detailed simulation of the agent, but relies instead on a high-level understanding of the agent's decision theory and computational power.\r\n> * The agent runs UDT, and has the ability to fully simulate the predictor.\r\n\r\n> Since the agent can deduce (by low-level simulation) what the predictor will do, the agent does not regard the prediction outcome as contingent on the agent's computation. Instead, either predict-onebox or predict-twobox has a probability of 1 (since one or the other of those is deducible), and a probability of 1 remains the same regardless of what we condition on. The agent will then calculate greater utility for two-boxing than for one-boxing.\r\n\r\n> Meanwhile, the predictor, knowing that the the agent runs UDT and will fully simulate the predictor, can reason as in the preceding paragraph, and thus deduce that the agent will two-box. So the large box is left empty and the agent two-boxes (and the agent's detailed simulation of the predictor correctly shows the predictor correctly predicting two-boxing).\r\n\r\n> The agent would be better off, though, running a different decision theory that does not two-box here, and that the predictor can deduce does not two-box.\r\n\r\n**EDITED 5/19/15:** There's a formal model of this due to Vladimir Slepnev where the agent and the predictor both have different types of predictive powers, such that in some sense they each know how the other will act in this universe. We'll write this out along with another case where things work out properly.\r\n\r\n(One algorithm has more computing power, but the other has stronger axioms: in particular, strong enough to prove that the other formal system is sound, as ZFC proves that PA is sound.)\r\n\r\nIn one of the following cases, [proof-based UDT](http://agentfoundations.org/item?id=50) one-boxes for correct reasons; in the other case, it two-boxes analogously to the reasoning above.\r\n\r\n***\r\n\r\nIn both cases, we'll use the following lemma:\r\n\r\n**Lemma:** Let $A()$ be an algorithm that returns either 1 or 2, such that ZFC has a proof of length $\\approx \\log(N)$ of \"$A()$ returns in $\\leq N$ steps\". Then ZFC has a proof of length $\\approx \\log(N)$ that \"$A()=1$ if and only if PA proves that $A()=1$.\"\r\n\r\n*Proof:* ZFC knows that PA is sound, so ZFC has a short proof that \"if PA proves that $A()=1$, then $A()=1$\". The other direction follows from the fact that the algorithm is bounded, and PA can simply show the execution trace of $A$ in $\\leq CN$ steps.\r\n\r\n# Case 1: Success (Agent Trusts Predictor)\r\n\r\nOmega looks through all proofs in PA; when it finds a proof that the agent one-boxes or a proof that the agent two-boxes, it stops looking and fills or empties the box accordingly. (If it fails to prove either, it empties the box.)\r\n\r\nThe agent looks several times through all proofs in ZFC of length $\\leq N$; it first looks for a proof that one of its actions leads to the payoff $1001000, then $1000000, then $1000, then $0. The first time it finds a proof that an action implies the outcome it's looking for, it immediately takes that action. (If it fails to find any such proof, it two-boxes.)\r\n\r\nI claim that for sufficiently large $N$, the agent one-boxes, and Omega fills the box.\r\n\r\n*Proof:* We use the Lemma. ZFC proves in $\\leq N$ steps that $A()=1 \\leftrightarrow \\Box_{PA}[A()=1]$. Since we (from outside the problem) assume ZFC is consistent, it will not be able to prove that two-boxing gets it $1001000, nor that one-boxing gets it $1001000, nor that two-boxing gets it $1000000, but it will be able to prove that one-boxing gets it $1000000. Thus it one-boxes. Since it runs in finite time, of course PA proves it one-boxes, and so Omega fills the box. \r\n\r\n# Case 2: Failure (Agent Simulates Predictor)\r\n\r\nWe'll swap the deductive processes this time. Omega looks through all proofs in ZFC of length $\\leq N$; when it finds a proof that the agent one-boxes or a proof that the agent two-boxes, it stops looking and fills or empties the box accordingly. (If it fails to prove either, it empties the box.)\r\n\r\nThe agent looks several times through all proofs in PA; it first looks for a proof that one of its actions leads to the payoff $1001000, then $1000000, then $1000, then $0. The first time it finds a proof that an action implies the outcome it's looking for, it immediately takes that action. (If it fails to find any such proof, it two-boxes.)\r\n\r\nI claim that the agent two-boxes, and Omega leaves the box empty.\r\n\r\n*Proof:* Again, by the lemma, ZFC proves that $\\Omega()=1 \\leftrightarrow \\Box_{PA}[\\Omega()=1]$ (where $\\Omega()=1$ means \"Omega predicts one-boxing and thus fills the box\"). Now note that $\\Omega()=1$ implies that PA proves $A()=2 \\to U=\\$1001000$ and does not prove that $A()=1 \\to U=\\$1001000$, and thus in this case $A()=2$; and similarly, $\\Omega()=2$ implies that PA proves $A()=2 \\to U=\\$1000$ and does not prove any of the prior counterfactuals, so again $A()=2$. All of these proofs are quick within ZFC, so $\\Omega$ does prove that $A()=2$, and thus $\\Omega()=2$.",
      "plaintextDescription": "There's a certain type of problem where it appears that having more computing power hurts you. That problem is the \"agent simulates predictor\" Newcomb's Dilemma.\n\nAs Gary Drescher put it:\n\n> There's a version of Newcomb's Problem that poses the same sort of challenge to UDT that comes up in some multi-agent/game-theoretic scenarios.\n\n> Suppose:\n\n>  * The predictor does not run a detailed simulation of the agent, but relies instead on a high-level understanding of the agent's decision theory and computational power.\n>  * The agent runs UDT, and has the ability to fully simulate the predictor.\n\n> Since the agent can deduce (by low-level simulation) what the predictor will do, the agent does not regard the prediction outcome as contingent on the agent's computation. Instead, either predict-onebox or predict-twobox has a probability of 1 (since one or the other of those is deducible), and a probability of 1 remains the same regardless of what we condition on. The agent will then calculate greater utility for two-boxing than for one-boxing.\n\n> Meanwhile, the predictor, knowing that the the agent runs UDT and will fully simulate the predictor, can reason as in the preceding paragraph, and thus deduce that the agent will two-box. So the large box is left empty and the agent two-boxes (and the agent's detailed simulation of the predictor correctly shows the predictor correctly predicting two-boxing).\n\n> The agent would be better off, though, running a different decision theory that does not two-box here, and that the predictor can deduce does not two-box.\n\nEDITED 5/19/15: There's a formal model of this due to Vladimir Slepnev where the agent and the predictor both have different types of predictive powers, such that in some sense they each know how the other will act in this universe. We'll write this out along with another case where things work out properly.\n\n(One algorithm has more computing power, but the other has stronger axioms: in particular, strong enough to prove ",
      "wordCount": 881
    },
    "tags": [
      {
        "_id": "EEj9MmynZaRQGXDgs",
        "name": "Agent Simulates Predictor",
        "slug": "agent-simulates-predictor"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374f20",
    "title": "Modal Bargaining Agents",
    "slug": "modal-bargaining-agents",
    "url": null,
    "baseScore": 14,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2015-04-16T22:19:03.000Z",
    "contents": {
      "markdown": "*Summary: Bargaining problems are interesting in the case of Löbian cooperation; Eliezer suggested a geometric algorithm for resolving bargaining conflicts by leaving the Pareto frontier, and this algorithm can be made into a modal agent, given an additional suggestion by Benja.*\r\n\r\n# Bargaining and Fairness\r\n\r\nWhen two agents can read each others' source code before playing a game, [Löbian cooperation](https://intelligence.org/files/ProgramEquilibrium.pdf) can transform many games into pure [bargaining problems](http://en.wikipedia.org/wiki/Bargaining_problem). Explicitly, the algorithm of \"if I can prove you choose X, then I choose Y, otherwise I choose Z\" makes it possible for an agent to offer a deal better for both parties than a Nash equilibrium, then default to that Nash equilibrium if the deal isn't verifiably taken. In the simple case where there is a unique Nash equilibrium, two agents with that sort of algorithm can effectively reduce the decision theory problem to a negotiation over which point on the Pareto frontier they will select.\r\n\r\n![Figure 1: the feasibility set. (We'll assume the agents have access to mutual randomness, as in \"60% chance you take action p and I take action P, 40% chance you take action q and I take action Q\", so that the set is convex.)](http://intelligence.org/files/Bargaining%20Figure%201.svg)\r\n\r\nHowever, if two agents insist on different deals (e.g. X insists on point D, Y insists on point A), then it falls through and we're back to the Nash equilibrium O. And if you try and patch this by offering several deals in sequence along the Pareto frontier, then a savvy opponent is just going to grab whichever of those is best for them. So we'd like both a notion of the fair outcome, and a way to still make deals with agents who disagree with us on fairness (without falling back to the Nash equilibrium, and without incentivizing them to play hardball with us).\r\n\r\nNote that utility functions are only defined up to affine transformations, so any solution should be invariant under independent rescalings of the players' utilities. This requirement, plus a few others (winding up on the Pareto frontier, independence of irrelevant alternatives, and symmetry) are all satisfied by the Nash solution to the bargaining problem: choose the point on the Pareto frontier so that the area of the rectangle from O to that point is maximized.\r\n\r\n![Figure 2: The Nash bargaining solution (point N).](http://intelligence.org/files/Bargaining%20Figure%202.svg)\r\n\r\nThis gives us a pretty plausible answer to the first question, but leaves us with the second: are we simply at war with agents that have other ideas of what's fair? (Let's say that X thinks that the Nash solution N is fair, but Y thinks that B is fair.) Other theorists have come up with other definitions of the fair solution to a bargaining problem, so this is a live question!\r\n\r\nAnd the question of incentives makes it even more difficult: if you try something like \"50% my fair equilibrium, 50% their fair equilibrium\", you create an incentive for other agents to bias their definition of fairness in their own favor, since that boosts their payoff.\r\n\r\n# Bargaining Away from the Pareto Frontier\r\n\r\nEliezer's suggestion in this case is as follows: an agent defines its set of acceptable deals as \"all points in the feasible set for which my opponent's score is at most what they would get at the point I think is fair\". If each agent's definition of fairness is biased in their own favor, the intersection of the agents' acceptable deals has a corner within the feasible set (but not on the Pareto frontier unless the agents agree on the fair point), and that is the point where they should actually achieve Löbian cooperation.\r\n\r\n![Figure 3: the acceptable sets for X (with fairness point N) and Y (with fairness point B), and the best mutually acceptable point E.](http://intelligence.org/files/Bargaining%20Figure%203.svg)\r\n\r\nNote that in this setup, you get no extra utility for redefining fairness in a self-serving way. Each agent winds up getting the utility they would have had at the fairness point of the *other* agent. (Actually, Eliezer suggests a very slight slope to these lines, in the direction that makes it *worse* for the opponent to insist on a more extreme fairness point. This sets up good incentives for meeting in the middle. But for simplicity, we'll just consider the incentive-neutral version here.)\r\n\r\nMoreover, this extends to games involving more than two agents: each one defines a set of acceptable deals by the condition that no other agent gets more than they would have at the agent's fairness point, and the intersection has a corner in the feasible set, where each agent gets the minimum of the payoffs it would have achieved at the other agents' fairness points.\r\n\r\n# Modal Bargaining Agents\r\n\r\nNow, how do we set this bargaining algorithm up as a [modal decision theory](http://agentfoundations.org/item?id=160)? We can only consider finitely many options, though we're allowed to consider computably mixed strategies. Let's assume that our game has finitely many pure strategies for each player. As above, we'll assume there is a unique Nash equilibrium, and set it at the origin.\r\n\r\nThen there's a natural set of points we should consider: the grid points within the feasible set (the convex hull formed by the Nash equilibrium and the Pareto optimal points above it) whose coordinates correspond to utilities of the pure strategies on the Pareto frontier. This is easier to see than to read:\r\n\r\n![Figure 4: the grid lines and grid points.](http://intelligence.org/files/Bargaining%20Figure%204.svg)\r\n\r\nNow all we need is to be sure that Löbian cooperation happens at the point we expect. There's one significant problem here: we need to worry about syncing the proof levels that different agents are using.\r\n\r\n(If you haven't seen this before, you might want to work out what happens if any two of the following three agents are paired with one another:\r\n\r\n* X returns A if PA proves its opponent returns A, else X returns D.\r\n* Y returns B if PA proves its opponent returns B, else Y returns A if PA proves its opponent returns A, else Y returns D.\r\n* Z returns C if PA proves its opponent returns C, else Z returns A if PA + Con(PA) proves its opponent returns A, else Z returns D.\r\n\r\nResults in [rot13](http://www.rot13.com/): Nal gjb qvssrerag ntragf nobir erghea Q ntnvafg rnpu bgure, orpnhfr CN pna'g cebir vgf bja pbafvfgrapl, naq gurersber gur snpg gung n cebbs frnepu va CN snvyf pna arire or cebirq va CN.)\r\n\r\nBenja suggested one way to ensure that Löbian cooperation happens at the right point: we assume that in addition to the payoff matrix, the agents are mutually aware of an ordering relation on the grid points. In order to land on the best mutually acceptable point (in the Pareto sense), it's merely necessary for the ordering to respect the \"level\" of grid points, defined as the total number of grid lines traversed along either axis from O to the grid point.\r\n\r\n![Figure 5: Grid levels labeled objectively by color, and an arbitrary ordering of grid points (subject to the constraint that it respects the grid level ordering).](http://intelligence.org/files/Bargaining%20Figure%205.svg)\r\n\r\nThen, we simply have each player look for cooperation at proof level N at the Nth point, skipping those points that are unacceptable to it. Since the best mutually acceptable point is the only acceptable point at its level (or any prior level), it will be chosen.\r\n\r\n![Figure 6: Agent X will try Löbian cooperation in PA+1 at point 1, then PA+3 at point 3, PA+4 at point 4, PA+6 at point 6, and so on within its acceptable set. Agent Y will try Löbian cooperation in PA at point 0, PA+2 at point 2, PA+5 at point 5, PA+6 at point 6, and so on within its acceptable set. X and Y thus achieve Löbian cooperation at point 6.](http://intelligence.org/files/Bargaining%20Figure%206.svg)\r\n\r\nAgain, this works for modal decision problems with more than two players.\r\n\r\n# Open questions and nagging issues:\r\n\r\n0. Is there a better way to resolve bargaining without incentivizing other agents to take extreme positions?\r\n1. Is there some reasonable way to make this work without providing the canonical ordering of grid points?\r\n2. If agents are each biased in their opponent's direction, this algorithm still gets a result, but in this case there is more than one grid point on the highest level of the mutually acceptable region, and thus the canonical ordering actually chooses the outcome!\r\n3. If an agent's \"fairness point\" on the Pareto frontier is itself a mixed strategy profile rather than a pure one, and the other agent doesn't know which point that is, can this still work? In particular, if there is an ordering on the entire feasible set, and if two agents each add extra grid lines to the set based on their own fairness points (without knowing the others), is there an algorithm for selecting proof levels which guarantees that they will meet at the best mutually acceptable point that appears in both of their grids?",
      "plaintextDescription": "Summary: Bargaining problems are interesting in the case of Löbian cooperation; Eliezer suggested a geometric algorithm for resolving bargaining conflicts by leaving the Pareto frontier, and this algorithm can be made into a modal agent, given an additional suggestion by Benja.\n\n\nBargaining and Fairness\nWhen two agents can read each others' source code before playing a game, Löbian cooperation can transform many games into pure bargaining problems. Explicitly, the algorithm of \"if I can prove you choose X, then I choose Y, otherwise I choose Z\" makes it possible for an agent to offer a deal better for both parties than a Nash equilibrium, then default to that Nash equilibrium if the deal isn't verifiably taken. In the simple case where there is a unique Nash equilibrium, two agents with that sort of algorithm can effectively reduce the decision theory problem to a negotiation over which point on the Pareto frontier they will select.\n\n\n\nHowever, if two agents insist on different deals (e.g. X insists on point D, Y insists on point A), then it falls through and we're back to the Nash equilibrium O. And if you try and patch this by offering several deals in sequence along the Pareto frontier, then a savvy opponent is just going to grab whichever of those is best for them. So we'd like both a notion of the fair outcome, and a way to still make deals with agents who disagree with us on fairness (without falling back to the Nash equilibrium, and without incentivizing them to play hardball with us).\n\nNote that utility functions are only defined up to affine transformations, so any solution should be invariant under independent rescalings of the players' utilities. This requirement, plus a few others (winding up on the Pareto frontier, independence of irrelevant alternatives, and symmetry) are all satisfied by the Nash solution to the bargaining problem: choose the point on the Pareto frontier so that the area of the rectangle from O to that point is maximized.\n\n\n\nThis give",
      "wordCount": 1457
    },
    "tags": [
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      },
      {
        "_id": "b8FHrKqyXuYGWc6vn",
        "name": "Game Theory",
        "slug": "game-theory"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "NKJeqTmu3g7jsmvf2",
    "title": "[Clearing out my Drafts folder] Rationality and Decision Theory Curriculum Idea",
    "slug": "clearing-out-my-drafts-folder-rationality-and-decision",
    "url": null,
    "baseScore": 6,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2015-03-23T22:54:51.241Z",
    "contents": {
      "markdown": "_Note: The following is a draft post I've had since 2009, and it's not great but it's worth posting for discussion. I do like the way that it prefigures some of the problems of Quirrell Points when traitors are allowed..._\n\nNeed to see if this can be easily gamed, but...\n\nStep 1. Introduce Prisoner's Dilemma.  Set up computer system so that they can log in and play it in partners with investments of points (caution them: this is their actual grade at stake).  Let them know that they currently don't have enough points for a passing grade on that part of the course, but that maximum investment and mutual cooperation will result in A's for (almost) everyone on it (with high probability); also that points are converted to grades on a logarithmic scale.  Let them know that creating institutions and alliances is a good strategy in such games.\n\nInitially, each student is allowed to play once per day, with 1 partner.  Students log in, enter the name of their requested PD partner, enter how much they're willing to invest, and enter C or D.  They'll get a \"bank statement\" daily as well.\n\nIf both enter C, they each get 1.2 points back (per initial point invested).  If one enters C and the other D, the cooperator gets nothing back and the defector gets 2 points (per point invested). If both enter D, then each gets 0.5 points back.\n\nOnce they've had some practice with this, we move to\n\nStep 2: Bigger investments, luck, observation.\n\nIntroduce larger group investments with higher rates of return.  E.g. a five-person opportunity that pays each C player 0.2 guaranteed plus 0.4 for each C (not counting themselves), and each D player gets 0.5 guaranteed plus 0.5 for each C.  (Set these up to be balanced in some way.)\n\nAdd a factor of luck, so that people can't just (be forced to) show one another their bank statements as proof of their cooperation.  People should average the proper amounts, but have enough variation that it's often difficult to tell whether they cooperated or defected.  (Or is there another way to handle this?)\n\nFinally, allow a third option of observation within a deal.  One idea: if you choose to observe, then you take up a spot in the investment, keep your own point, and don't count as a C for others, but you get to see some or all of the other players' choices.  Make sure that information is proportionately expensive.\n\nStep 3: Keep scaling it up, adjust (only) as necessary.\n\nIf things get too unbalanced, some progressive taxation and welfare might be in order; but disrupting the system too much will tend to destroy the things they need to learn.\n\nIdeally, larger and larger opportunities should be offered, with the kicker being a gigantic one-shot Prisoner's Dilemma that the whole class can partake in.  C might get guaranteed 0.2 plus 0.2 for every other C, while D gets guaranteed 1 plus 0.3 for every C.\n\nImplementation:\n\nNeed a non-hackable computer program to work off of.  Also, have students (for real points) give critiques and suggestions for improvement.",
      "plaintextDescription": "Note: The following is a draft post I've had since 2009, and it's not great but it's worth posting for discussion. I do like the way that it prefigures some of the problems of Quirrell Points when traitors are allowed...\n\nNeed to see if this can be easily gamed, but...\n\nStep 1. Introduce Prisoner's Dilemma.  Set up computer system so that they can log in and play it in partners with investments of points (caution them: this is their actual grade at stake).  Let them know that they currently don't have enough points for a passing grade on that part of the course, but that maximum investment and mutual cooperation will result in A's for (almost) everyone on it (with high probability); also that points are converted to grades on a logarithmic scale.  Let them know that creating institutions and alliances is a good strategy in such games.\n\nInitially, each student is allowed to play once per day, with 1 partner.  Students log in, enter the name of their requested PD partner, enter how much they're willing to invest, and enter C or D.  They'll get a \"bank statement\" daily as well.\n\nIf both enter C, they each get 1.2 points back (per initial point invested).  If one enters C and the other D, the cooperator gets nothing back and the defector gets 2 points (per point invested). If both enter D, then each gets 0.5 points back.\n\nOnce they've had some practice with this, we move to\n\nStep 2: Bigger investments, luck, observation.\n\nIntroduce larger group investments with higher rates of return.  E.g. a five-person opportunity that pays each C player 0.2 guaranteed plus 0.4 for each C (not counting themselves), and each D player gets 0.5 guaranteed plus 0.5 for each C.  (Set these up to be balanced in some way.)\n\nAdd a factor of luck, so that people can't just (be forced to) show one another their bank statements as proof of their cooperation.  People should average the proper amounts, but have enough variation that it's often difficult to tell whether they cooperated or defected.",
      "wordCount": 507
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "XqJ9JWX8TpCuECLCE",
    "title": "An Introduction to Löb's Theorem in MIRI Research",
    "slug": "an-introduction-to-loeb-s-theorem-in-miri-research",
    "url": null,
    "baseScore": 29,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 27,
    "createdAt": null,
    "postedAt": "2015-03-23T22:22:26.908Z",
    "contents": {
      "markdown": "Would you like to see a primer on several MIRI research topics (assuming only the background of having taken a course with proofs in math or computer science)? Or are you curious why MIRI does so much with mathematical logic, and why people on Less Wrong keep referring to Löb's Theorem?\n\nIf you answered yes to either question, you may be interested in my lecture notes, [An Introduction to Löb's Theorem in MIRI Research](http://intelligence.org/files/lob-notes-IAFF.pdf)! These came out of an introductory talk that I gave at a [MIRIx workshop](https://intelligence.org/mirix/).\n\nSince I've got some space here, I'll just copy and paste the table of contents and the introduction section...\n\nContents\n--------\n\n**1 Introduction**\n\n**2 Crash Course in Löb's Theorem**\n\n2.1 Gödelian self-reference and quining programs\n\n2.2 Löb's Theorem \n\n**3 Direct Uses of Löb's Theorem in MIRI Research**\n\n3.1 “The Löbstacle”\n\n3.2 Löbian cooperation\n\n3.3 Spurious counterfactuals\n\n**4 Crash Course in Model Theory**\n\n4.1 Axioms and theories\n\n4.2 Alternative and nonstandard models\n\n**5 Uses of Model Theory in MIRI Research**\n\n5.1 Reflection in probabilistic logic\n\n**6 Crash Course in Gödel-Löb Modal Logic**\n\n6.1 The modal logic of provability\n\n6.2 Fixed points of modal statements\n\n**7 Uses of Gödel-Löb Modal Logic in MIRI Research**\n\n7.1 Modal Combat in the Prisoner’s Dilemma\n\n7.2 Modal Decision Theory\n\n**8 Acknowledgments**\n\n1 Introduction\n--------------\n\nThis expository note is devoted to answering the following question: why do many MIRI research papers cite [a 1955 theorem of Martin Löb](http://en.wikipedia.org/wiki/L%C3%B6b%27s_theorem), and indeed, why does MIRI focus so heavily on mathematical logic? The short answer is that this theorem illustrates the basic kind of self-reference involved when an algorithm considers its own output as part of the universe, and it is thus germane to many kinds of research involving self-modifying agents, especially when formal verification is involved or when we want to cleanly prove things in model problems. For a longer answer, well, welcome!\n\nI’ll assume you have some background doing mathematical proofs and writing computer programs, but I won’t assume any background in mathematical logic beyond knowing the usual [logical operators](http://en.wikipedia.org/wiki/Logical_constant), nor that you’ve even heard of Löb’s Theorem before.\n\nTo motivate the mathematical sections that follow, let’s consider a toy problem. Say that we’ve designed Deep Thought 1.0, an AI that reasons about its possible actions and only takes actions that it can show to have good consequences on balance. One such action is designing a successor, Deep Thought 2.0, which has improved deductive abilities. But if Deep Thought 1.0 (hereafter called DT1) is to actually build Deep Thought 2.0 (DT2), DT1 must first conclude that building DT2 will have good consequences on balance.\n\nThere’s an immediate difficulty—the consequences of building DT2 include the actions that DT2 takes; but since DT2 has increased deductive powers, DT1 can’t actually figure out what actions DT2 is going to take. Naively, it seems as if it should be enough for DT1 to know that DT2 has the same goals as DT1, that DT2’s deductions are reliable, and that DT2 only takes actions that it deduces to have good consequences on balance.\n\nUnfortunately, the straightforward way of setting up such a model fails catastrophically on the innocent-sounding step “DT1 knows that DT2’s deductions are reliable”. If we try and model DT1 and DT2 as proving statements in two formal systems (one stronger than the other), then the only way that DT1 can make such a statement about DT2’s reliability is if DT1 (and thus both) are in fact unreliable! This counterintuitive roadblock is best explained by reference to Löb’s theorem, and so we turn to the background of that theorem.\n\n[(Here's the link to the full notes again.)](http://intelligence.org/files/lob-notes-IAFF.pdf)",
      "plaintextDescription": "Would you like to see a primer on several MIRI research topics (assuming only the background of having taken a course with proofs in math or computer science)? Or are you curious why MIRI does so much with mathematical logic, and why people on Less Wrong keep referring to Löb's Theorem?\n\nIf you answered yes to either question, you may be interested in my lecture notes, An Introduction to Löb's Theorem in MIRI Research! These came out of an introductory talk that I gave at a MIRIx workshop.\n\nSince I've got some space here, I'll just copy and paste the table of contents and the introduction section...\n\n\n\n\nContents\n \n\n1 Introduction\n\n2 Crash Course in Löb's Theorem\n\n2.1 Gödelian self-reference and quining programs\n\n2.2 Löb's Theorem \n\n3 Direct Uses of Löb's Theorem in MIRI Research\n\n3.1 “The Löbstacle”\n\n3.2 Löbian cooperation\n\n3.3 Spurious counterfactuals\n\n4 Crash Course in Model Theory\n\n4.1 Axioms and theories\n\n4.2 Alternative and nonstandard models\n\n5 Uses of Model Theory in MIRI Research\n\n5.1 Reflection in probabilistic logic\n\n6 Crash Course in Gödel-Löb Modal Logic\n\n6.1 The modal logic of provability\n\n6.2 Fixed points of modal statements\n\n7 Uses of Gödel-Löb Modal Logic in MIRI Research\n\n7.1 Modal Combat in the Prisoner’s Dilemma\n\n7.2 Modal Decision Theory\n\n8 Acknowledgments\n\n\n1 Introduction\nThis expository note is devoted to answering the following question: why do many MIRI research papers cite a 1955 theorem of Martin Löb, and indeed, why does MIRI focus so heavily on mathematical logic? The short answer is that this theorem illustrates the basic kind of self-reference involved when an algorithm considers its own output as part of the universe, and it is thus germane to many kinds of research involving self-modifying agents, especially when formal verification is involved or when we want to cleanly prove things in model problems. For a longer answer, well, welcome!\n\nI’ll assume you have some background doing mathematical proofs and writing computer programs, but",
      "wordCount": 567
    },
    "tags": [
      {
        "_id": "DFCEGpufjkvqQaRXt",
        "name": "Löb's theorem",
        "slug": "loeb-s-theorem"
      },
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "xKz23YJ7h5JZijGFs",
        "name": "Spurious Counterfactuals",
        "slug": "spurious-counterfactuals"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "5bd75cc58225bf0670374efa",
    "title": "Welcome, new contributors!",
    "slug": "welcome-new-contributors",
    "url": null,
    "baseScore": 6,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2015-03-23T21:53:20.000Z",
    "contents": {
      "markdown": "Today is the day; we're opening up this forum to allow contributions from more people! See our [How to Contribute page](http://agentfoundations.org/how-to-contribute) for the details.\r\n\r\nNow is a great time to say a bit about what the Intelligent Agent Foundations Forum is, and how it came about. The short answer is that the [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI) helped build this forum in order to facilitate research discussion on the topics in its [technical agenda](https://intelligence.org/files/TechnicalAgenda.pdf) and related subjects.\r\n\r\nMany of the early users of this forum previously contributed to a closed email group on decision theory, or wrote relevant posts on the group blog [Less Wrong](http://lesswrong.com/). MIRI wanted to build a forum that could focus on these topics and support high-quality mathematical collaboration, while being transparent and allowing new contributors to find and join it directly.\r\n\r\nBroadly speaking, the topics of this forum concern the difficulties of value alignment- the problem of how to ensure that machine intelligences of various levels adequately understand and pursue the goals that their developers actually intended, rather than getting stuck on some proxy for the real goal or failing in other unexpected (and possibly dangerous) ways. As these failure modes are more devastating the farther we advance in building machine intelligences, MIRI's goal is to work today on the foundations of goal systems and architectures that would work even when the machine intelligence has general creative problem-solving ability beyond that of its developers, and has the ability to modify itself or build successors. (For still more related info on the motivations for this work, see [the Future of Life Institute's research priorities letter](http://futureoflife.org/misc/open_letter) or Nick Bostrom's recent book [Superintelligence](http://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies).)\r\n\r\nIn that context, there are many interesting problems that come up; here are several from [MIRI's technical agenda page](https://intelligence.org/technical-agenda/):\r\n\r\n* **Decision theory:** One class of topics comes from the distortions that arise when an agent predicts its environment, including its own future actions or the predictions of other agents, and tries to make decisions based on those. The tools of classical game theory and decision theory begin to make substandard recommendations on Newcomblike problems, blackmail problems, and other topics in this domain, and formal models of decision theories have brought up entirely unexpected self-referential failure modes. This has spurred the development of some new mathematical models of decision theory and counterfactual reasoning. ([MIRI research agenda paper on decision theory](https://intelligence.org/files/TowardIdealizedDecisionTheory.pdf))\r\n* **Logical uncertainty:** In the classical formalism of Bayesian agents, the agent updates on new evidence in a way that makes use of *all* logical consequences. In any interesting universe (even, say, the theory of arithmetic), this is actually an impossible assumption. Any bounded reasoner must have a satisfactory way of dealing with hypotheses that may in fact be determined from the data, but which have not yet been deduced either way. There are some interesting and analogous models of coherent (or locally coherent) probability distributions on the theory of arithmetic. ([MIRI research agenda paper on logical uncertainty](https://intelligence.org/files/QuestionsLogicalUncertainty.pdf))\r\n* **Reflective world-models:** The distinction between an agent and its environment is a fuzzy one. Performing an action in the environment (e.g. sabotaging one's own hardware) can predictably affect the agent's future inferential processes. Furthermore, there are some models of intelligence and learning in which the correct hypotheses about the agent itself are not accessible to the agent. In both cases, there has been some progress on building mathematical models of systems that represent themselves more sensibly. ([MIRI research paper on reflective world-models](https://intelligence.org/files/RealisticWorldModels.pdf))\r\n* **Corrigibility:** Many goal systems, if they can reason reflectively and strategically, will seek to preserve themselves (because otherwise, their current goal state will be less likely to be reached). This gives rise to a potential problem with communicating human value to a machine intelligence: if the developers make a mistake in doing so, the machine intelligence may seek ways to avoid being corrected. There are several models of this, and a few proposals. ([MIRI research paper on corrigibility](https://intelligence.org/files/Corrigibility.pdf))\r\n* **Self-trust and Vingean reflection:** Informally, if an agent self-modifies to become better at problem-solving or inference, it should be able to trust that its modified self will be better at achieving its goals. As it turns out, there is a self-referential obstacle with simple models of this (akin to the fact that only inconsistent formal systems believe themselves to be consistent), and one method of fixing it results in the possibility of indefinitely deferred actions or deductions. ([MIRI research paper on Vingean reflection](https://intelligence.org/files/VingeanReflection.pdf))\r\n* **Value Learning:** Since human beings have not succeeded at specifying human values (citation: look at the lack of total philosophical consensus on ethics), we may in fact need the help of a machine intelligence itself to specify the values to a machine intelligence. This sort of \"indirect normativity\" presents its own interesting challenges. ([MIRI research paper on value learning](https://intelligence.org/files/ValueLearningProblem.pdf))\r\n\r\nThis is not an exhaustive list of topics or of progress! In the next few days, several forum contributors plan to consolidate the work and discussions already on this forum, and produce summary posts with links for each group of topics (including some not listed above).\r\n\r\nBut the list does help us to point out what we consider to be on-topic in this forum. Besides the topics mentioned there, other relevant subjects include groundwork for self-modifying agents, abstract properties of goal systems, tractable theoretical or computational models of the topics above, and anything else that is *directly* connected to MIRI's research mission.\r\n\r\nIt's important for us to keep the forum focused, though; there are other good places to talk about subjects that are more indirectly related to MIRI's research mission, and the moderators here may close down discussions on subjects that aren't a good fit for this mission. Some examples of subjects that we would consider off-topic (unless directly applied to a more relevant area) include general advances in artificial intelligence and machine learning, general mathematical logic, general philosophy of mind, general futurism, existential risks, effective altruism, human rationality, and non-technical philosophizing.\r\n\r\nAs Benja said in the [original welcome post](http://agentfoundations.org/item?id=1), the software is still fairly minimal and a little rough around the edges (though we do have LaTeX support). We hope to improve quickly! If you want to help us, [the code is on GitHub](https://github.com/machine-intelligence/research-forum). And if you find bugs, we hope you’ll [let us know](https://github.com/machine-intelligence/research-forum/issues)!\r\n\r\nWe look forward to your contributions!",
      "plaintextDescription": "Today is the day; we're opening up this forum to allow contributions from more people! See our How to Contribute page for the details.\n\nNow is a great time to say a bit about what the Intelligent Agent Foundations Forum is, and how it came about. The short answer is that the Machine Intelligence Research Institute (MIRI) helped build this forum in order to facilitate research discussion on the topics in its technical agenda and related subjects.\n\nMany of the early users of this forum previously contributed to a closed email group on decision theory, or wrote relevant posts on the group blog Less Wrong. MIRI wanted to build a forum that could focus on these topics and support high-quality mathematical collaboration, while being transparent and allowing new contributors to find and join it directly.\n\nBroadly speaking, the topics of this forum concern the difficulties of value alignment- the problem of how to ensure that machine intelligences of various levels adequately understand and pursue the goals that their developers actually intended, rather than getting stuck on some proxy for the real goal or failing in other unexpected (and possibly dangerous) ways. As these failure modes are more devastating the farther we advance in building machine intelligences, MIRI's goal is to work today on the foundations of goal systems and architectures that would work even when the machine intelligence has general creative problem-solving ability beyond that of its developers, and has the ability to modify itself or build successors. (For still more related info on the motivations for this work, see the Future of Life Institute's research priorities letter or Nick Bostrom's recent book Superintelligence.)\n\nIn that context, there are many interesting problems that come up; here are several from MIRI's technical agenda page:\n\n * Decision theory: One class of topics comes from the distortions that arise when an agent predicts its environment, including its own future actions or the p",
      "wordCount": 1034
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374f08",
    "title": "A toy model of a corrigibility problem",
    "slug": "a-toy-model-of-a-corrigibility-problem",
    "url": "https://www.overleaf.com/read/nsvqhtxhmnzz",
    "baseScore": 5,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2015-03-22T19:33:02.000Z",
    "contents": {
      "markdown": "",
      "plaintextDescription": "",
      "wordCount": 1
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "qWXJmBRPjwnkGRojP",
    "title": "New forum for MIRI research: Intelligent Agent Foundations Forum",
    "slug": "new-forum-for-miri-research-intelligent-agent-foundations",
    "url": null,
    "baseScore": 53,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 43,
    "createdAt": null,
    "postedAt": "2015-03-20T00:35:07.071Z",
    "contents": {
      "markdown": "Today, the [Machine Intelligence Research Institute](https://intelligence.org/) is launching a new forum for research discussion: the [Intelligent Agent Foundations Forum](http://agentfoundations.org/)! It's already been seeded with a bunch of new work on MIRI topics from the last few months.\n\nWe've covered most of the (what, why, how) subjects on [the forum's new welcome post](http://agentfoundations.org/item?id=157) and [the How to Contribute page](http://agentfoundations.org/how-to-contribute), but this post is an easy place to comment if you have further questions (or if, maths forbid, there are technical issues _with_ the forum instead of _on_ it).\n\nBut before that, go ahead and check it out!\n\n(Major thanks to Benja Fallenstein, Alice Monday, and Elliott Jin for their work on the forum code, and to all the contributors so far!)\n\n**EDIT 3/22:** Jessica Taylor, Benja Fallenstein, and I wrote forum digest posts summarizing and linking to recent work (on the IAFF and elsewhere) on [reflective oracle machines](http://agentfoundations.org/item?id=165), on [corrigibility, utility indifference, and related control ideas](http://agentfoundations.org/item?id=167), and on [updateless decision theory and the logic of provability](http://agentfoundations.org/item?id=160), respectively! These are pretty excellent resources for reading up on those topics, in my biased opinion.",
      "plaintextDescription": "Today, the Machine Intelligence Research Institute is launching a new forum for research discussion: the Intelligent Agent Foundations Forum! It's already been seeded with a bunch of new work on MIRI topics from the last few months.\n\nWe've covered most of the (what, why, how) subjects on the forum's new welcome post and the How to Contribute page, but this post is an easy place to comment if you have further questions (or if, maths forbid, there are technical issues with the forum instead of on it).\n\nBut before that, go ahead and check it out!\n\n(Major thanks to Benja Fallenstein, Alice Monday, and Elliott Jin for their work on the forum code, and to all the contributors so far!)\n\nEDIT 3/22: Jessica Taylor, Benja Fallenstein, and I wrote forum digest posts summarizing and linking to recent work (on the IAFF and elsewhere) on reflective oracle machines, on corrigibility, utility indifference, and related control ideas, and on updateless decision theory and the logic of provability, respectively! These are pretty excellent resources for reading up on those topics, in my biased opinion.",
      "wordCount": 174
    },
    "tags": [
      {
        "_id": "NrvXXL3iGjjxu5B7d",
        "name": "Machine Intelligence Research Institute (MIRI)",
        "slug": "machine-intelligence-research-institute-miri"
      },
      {
        "_id": "MfpEPj6kJneT9gWT6",
        "name": "Site Meta",
        "slug": "site-meta"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "NrvXXL3iGjjxu5B7d",
        "name": "MIRI",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "NrvXXL3iGjjxu5B7d",
      "tag_name": "MIRI",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "5bd75cc58225bf0670374efd",
    "title": "Forum Digest: Updateless Decision Theory",
    "slug": "forum-digest-updateless-decision-theory",
    "url": null,
    "baseScore": 15,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2015-03-20T00:22:06.000Z",
    "contents": {
      "markdown": "*Summary: This is a quick expository recap, with links, of the posts on this forum on the topic of updateless decision theory, through 3/19/15. Read this if you want to learn more about UDT, or if you're curious about what we've been working on lately!*\r\n\r\n# Introduction\r\n\r\nUpdateless decision theory (henceforth UDT) is a proposed algorithm for making good decisions in a world that can contain predictions and other echoes of the agent's decision algorithm. It is motivated by a set of [Newcomblike problems](http://en.wikipedia.org/wiki/Newcomb%27s_paradox) on which the classical decision theories ([evidential decision theory](http://en.wikipedia.org/wiki/Evidential_decision_theory) and [causal decision theory](http://en.wikipedia.org/wiki/Causal_decision_theory)) have known failure modes. For more context and motivation for UDT, see the MIRI research document [Toward Idealized Decision Theory](https://intelligence.org/files/TowardIdealizedDecisionTheory.pdf).\r\n\r\nPhilosophically speaking, UDT looks over all possible strategies (maps from \"agent observes X\" to \"agent does Y\"), picks the global strategy that would be best for all copies of UDT in aggregate, and then performs the action that the strategy recommends for an agent in its position. (One explanation for choosing strategies rather than actions is that it defends against blackmail: if you are being blackmailed, then paying up appears to be the better action; but since the strategy of refusing to pay deters all savvy blackmailers, UDT knowably refuses to pay up and thus is rarely blackmailed. Or so the theory goes.)\r\n\r\nThis leads to some interesting ambiguities, most importantly the counterfactuals about \"what would happen\" if the deterministic UDT algorithm chose strategy B, when in fact it will end up choosing strategy A. These *logical counterfactuals* can be confronted straightforwardly in mathematical models by trying to formally prove that if UDT selects strategy S, then the universe is in a state of utility U. But then one faces the problem of *spurious counterfactuals* (\"if the agent chose X then Y would happen\", which is only logically valid because we can prove that the agent does not choose X), which lead to further complications.\r\n\r\nThe Gödel-Löb modal logic GL (also called [provability logic](http://plato.stanford.edu/entries/logic-provability/)) allows us to construct a good heuristic model of UDT in a mathematical universe, using Gödel numbering and quining to capture self-reference, and evaluating counterfactuals via unbounded proof searches (i.e. a halting oracle). This \"modal UDT\" has some interesting properties: for instance, it is provably optimal on decision problems that \"only care what it does\", if given sufficiently powerful reasoning axioms. Moreover, the tools of Kripke semantics allow the actions of modal UDT and other modally defined agents to be formally verified in polynomial time!\r\n\r\nLest we get carried away, modal UDT is neither a perfect instantiation of the philosophical UDT algorithm, nor is it optimal in problems involving universes that predict the agent's decisions via provability. Even so, it is an incredibly fruitful object for further study in decision theory!\r\n\r\nWithout further ado, here is an overview of all the posts on this forum thus far that relate to updateless decision theory:\r\n\r\n# Expository material\r\n\r\n* [A primer on provability logic](http://agentfoundations.org/item?id=41), Benja Fallenstein. Self-explanatory.\r\n\r\n* [An Introduction to Löb's Theorem in MIRI Research](http://agentfoundations.org/item?id=94), Patrick LaVictoire. A 27-page primer on the uses of Löb's Theorem and provability logic in MIRI research topics, including 10 pages on provability logic and modal UDT. I highly recommend this as an introduction to these topics! (But then, I would, wouldn't I...)\r\n\r\n# New research on modal UDT\r\n\r\n* [Using modal fixed points to formalize logical causality](http://agentfoundations.org/item?id=4), Vladimir Slepnev. I believe this is actually the first time that someone constructed a UDT algorithm in provability logic!\r\n\r\n* [\"Evil\" decision problems in provability logic](http://agentfoundations.org/item?id=47), Benja Fallenstein. For every modal decision theory, you can set up a fair decision problem (i.e. one that simply maps actions to outcomes rather than depending on the inner details of how the agent deliberates) which punishes the agent, such that other decision theories can get a better outcome. This is an important limitation on any optimality results, but as we shall see, it is not the final word.\r\n\r\n* [An optimality result for modal UDT](http://agentfoundations.org/item?id=50), Benja Fallenstein. For each fixed fair decision problem, there is a set of axioms (in this case, simply the true axioms about the mapping from actions to outcomes) such that modal UDT equipped with those axioms will achieve the best possible outcome.\r\n\r\n* [Improving the modal UDT optimality result](http://agentfoundations.org/item?id=76), Benja Fallenstein. We can use more general axioms than the ones in the previous post, and still achieve the best possible outcome.\r\n\r\n* [Obstacle to modal optimality when you're being modalized](http://agentfoundations.org/item?id=96), Patrick LaVictoire. The optimality result cannot carry over to even simple decision problems where the outcome depends on whether the agent *provably* takes a certain action.\r\n\r\n* [On notation for modal UDT](http://agentfoundations.org/item?id=102), Benja Fallenstein. This emphasizes that our use of modal agents and decision theories relies on fixed points of modal logic operators, and thus we should be careful about using functional notation.\r\n\r\n* [An implementation of modal UDT](http://agentfoundations.org/item?id=121), Benja Fallenstein. The polynomial-time algorithm for figuring out what a modal UDT agent does in a decision problem, now in a simple Haskell program!\r\n\r\n# New research on logical counterfactuals\r\n\r\n* [Uniqueness of UDT for transparent universes](http://agentfoundations.org/item?id=75), Tsvi Benson-Tilson. This is a different abstract model of UDT, using what we call \"playing chicken with the universe\": before reasoning about consequences, take each possible action and attempt to prove you don't take it; if you ever succeed, immediately take that action. This is more useful than it sounds, because it rules out certain spurious counterfactuals.\r\n\r\n* [A different angle on UDT](http://agentfoundations.org/item?id=86), Nate Soares. An exploration of spurious counterfactuals, with an example (where UDT is used to analyze another agent, rather than to take an action itself).\r\n\r\n* [Why conditioning on \"the agent takes action a\" isn't enough](http://agentfoundations.org/item?id=92), Nate Soares. Continuing the exploration of spurious counterfactuals, with probabilistic agents.\r\n\r\n* [Third-person counterfactuals](http://agentfoundations.org/item?id=98), Benja Fallenstein. There is at least some set of assumptions (a \"sufficiently informative\" decision problem) for which modal UDT analyzes an algorithm (perhaps itself) adequately and avoids acting on spurious counterfactuals... or does it?\r\n\r\n* [The odd counterfactuals of playing chicken](http://agentfoundations.org/item?id=99), Benja Fallenstein. Nope, modal UDT is perfectly susceptible to spurious counterfactuals when analyzing other algorithms, even if the decision problem is sufficiently informative.\r\n\r\n# Other new research relevant to UDT\r\n\r\n* [Exploiting EDT](http://agentfoundations.org/item?id=32), Benja Fallenstein. A clear example of how evidential decision theory can be exploited, this was the origin of the \"evidential blackmail\" problem in [MIRI's decision theory research paper](https://intelligence.org/files/TowardIdealizedDecisionTheory.pdf).\r\n\r\n* [UDT in the Land of Probabilistic Oracles](http://agentfoundations.org/item?id=117), Jessica Taylor. This post depends on the \"reflective oracles\" sequence of posts as well, and it presents a model of UDT in that context.",
      "plaintextDescription": "Summary: This is a quick expository recap, with links, of the posts on this forum on the topic of updateless decision theory, through 3/19/15. Read this if you want to learn more about UDT, or if you're curious about what we've been working on lately!\n\n\nIntroduction\nUpdateless decision theory (henceforth UDT) is a proposed algorithm for making good decisions in a world that can contain predictions and other echoes of the agent's decision algorithm. It is motivated by a set of Newcomblike problems on which the classical decision theories (evidential decision theory and causal decision theory) have known failure modes. For more context and motivation for UDT, see the MIRI research document Toward Idealized Decision Theory.\n\nPhilosophically speaking, UDT looks over all possible strategies (maps from \"agent observes X\" to \"agent does Y\"), picks the global strategy that would be best for all copies of UDT in aggregate, and then performs the action that the strategy recommends for an agent in its position. (One explanation for choosing strategies rather than actions is that it defends against blackmail: if you are being blackmailed, then paying up appears to be the better action; but since the strategy of refusing to pay deters all savvy blackmailers, UDT knowably refuses to pay up and thus is rarely blackmailed. Or so the theory goes.)\n\nThis leads to some interesting ambiguities, most importantly the counterfactuals about \"what would happen\" if the deterministic UDT algorithm chose strategy B, when in fact it will end up choosing strategy A. These logical counterfactuals can be confronted straightforwardly in mathematical models by trying to formally prove that if UDT selects strategy S, then the universe is in a state of utility U. But then one faces the problem of spurious counterfactuals (\"if the agent chose X then Y would happen\", which is only logically valid because we can prove that the agent does not choose X), which lead to further complications.\n\nThe Gödel-Löb ",
      "wordCount": 1071
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374ee8",
    "title": "Meta- the goals of this forum",
    "slug": "meta-the-goals-of-this-forum",
    "url": null,
    "baseScore": 4,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2015-03-10T20:16:47.000Z",
    "contents": {
      "markdown": "*Summary: We're planning to publicize and open up the forum very soon, and so it's a good time to discuss what we would like this forum to achieve, how we plan for moderation to work, and what discussions are on-topic.*\r\n\r\nCurrently, this forum is read-only for everyone except for a few veterans of the mailing list it replaces. In a few days, we're planning to open up posting (in a tiered way, similar in spirit to the [tiered privileges of MathOverflow](http://mathoverflow.net/help/privileges)), and the comments and Likes of the full members will play a material role in moderating the community. So it's a good time for those of us who are already here to discuss our goals for the forum, so that we stand a better chance of coordinating.\r\n\r\n# The forum structure\r\n\r\nRight now, the forum has [members](http://agentfoundations.org/members) who are allowed to submit posts and comments, and to Like other members' posts and comments; non-members can only read the contents of the forum, and membership is by invitation from the admins. What we're planning to do is allow non-members who sign up with their Facebook accounts to post links with titles, the way one can on [Hacker News](https://news.ycombinator.com/). (The Facebook login requirement reduces the amount of spam; clearly we'll want to add other ways of registering when we have the development time.)\r\n\r\nThese links from non-members will be visible only to members (and to the non-member who posted them) unless they've received some number of Likes from members, at which point they'll be publicly visible. Non-members who have posted a few links to good content that they wrote themselves will get membership invites. (We hope to also add the ability for members to recommend others for membership.)\r\n\r\n*The goal of all of this is to keep the discussion happening at a high level!* One way of expressing the ideal of the forum is as a transparent box where high-quality conversation happens, and which has a mechanism for carefully letting new contributors in. That mechanism depends on the members, as well as the admins, maintaining an idea of what a good and relevant contribution looks like. (And there's really no way around doing some serious moderation if we want to create a useful place on the Internet. We aim for the standard set by Math Overflow and Hacker News.)\r\n\r\nNow that I've explained the context, here is our current draft of the \"how to contribute\" page:\r\n\r\n> This is a publicly visible discussion forum for foundational mathematical research in artificial intelligence. The goal of this forum is to move toward a more formal and general understanding of \"robust and beneficial\" AI systems, as discussed in the Future of Life Institute's [research priorities letter](http://futureoflife.org/misc/open_letter) and the Machine Intelligence Research Institute's [technical agenda](https://intelligence.org/technical-agenda/).\r\n>\r\n> Like [Math Overflow](http://mathoverflow.net/help/privileges), the Intelligent Agent Foundations Forum has a tiered system for becoming a full contributor. If you make an account with a Facebook login, you can post a link to an off-site contribution, e.g., on [Medium](http://medium.com/) or on a personal blog. These links will be visible to full forum contributors. If your link acquires enough Likes from full contributors, it will get promoted to visibility by all site visitors.\r\n>\r\n> If you link to some good original content that you have written, the administrators will give you permissions to make posts and comments, and to Like others' contributions. The details of this system are still being worked out, and will change as we get a larger community of users.\r\n\r\n# The forum content\r\n\r\nSo, what kinds of contributions are on-topic here? Here's one suggestion:\r\n\r\n* Definitely on-topic: the topics in [MIRI's research agenda](https://intelligence.org/technical-agenda/) (decision theory, corrigibility, value learning, logical uncertainty, Vingean reflection, naturalized induction)\r\n\r\n* Generally on-topic: mathematical groundwork for self-modifying agents, abstract properties of goal systems, tractable theoretical or computational models of MIRI research topics\r\n\r\n* Not generally on-topic unless specifically connected to a relevant topic: recent advances in artificial intelligence and machine learning, existential risks, effective altruism, human rationality, mathematical logic, philosophy of mind\r\n\r\nPlease try not to encourage off-topic discussions; we'll have a button for reporting them to admins. (And yes, meta discussions are usually off-topic; I think it's important to have one now, but I don't want it to be a constant thing.)\r\n\r\n# Your thoughts\r\n\r\nAre there any obvious improvements to this scheme? Any concerns you have? Any topics that should clearly be added to the categorization above? Any features that are worth diverting development time for?\r\n\r\n**P.S.** Thanks to David Zureick-Brown and Anton Geraschenko (the founders of [MathOverflow](http://mathoverflow.net/)) for sharing their advice, which led to this post and many of the ideas therein!",
      "plaintextDescription": "Summary: We're planning to publicize and open up the forum very soon, and so it's a good time to discuss what we would like this forum to achieve, how we plan for moderation to work, and what discussions are on-topic.\n\nCurrently, this forum is read-only for everyone except for a few veterans of the mailing list it replaces. In a few days, we're planning to open up posting (in a tiered way, similar in spirit to the tiered privileges of MathOverflow), and the comments and Likes of the full members will play a material role in moderating the community. So it's a good time for those of us who are already here to discuss our goals for the forum, so that we stand a better chance of coordinating.\n\n\nThe forum structure\nRight now, the forum has members who are allowed to submit posts and comments, and to Like other members' posts and comments; non-members can only read the contents of the forum, and membership is by invitation from the admins. What we're planning to do is allow non-members who sign up with their Facebook accounts to post links with titles, the way one can on Hacker News. (The Facebook login requirement reduces the amount of spam; clearly we'll want to add other ways of registering when we have the development time.)\n\nThese links from non-members will be visible only to members (and to the non-member who posted them) unless they've received some number of Likes from members, at which point they'll be publicly visible. Non-members who have posted a few links to good content that they wrote themselves will get membership invites. (We hope to also add the ability for members to recommend others for membership.)\n\nThe goal of all of this is to keep the discussion happening at a high level! One way of expressing the ideal of the forum is as a transparent box where high-quality conversation happens, and which has a mechanism for carefully letting new contributors in. That mechanism depends on the members, as well as the admins, maintaining an idea of what a good and",
      "wordCount": 754
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374edf",
    "title": "Proposal: Modeling goal stability in machine learning",
    "slug": "proposal-modeling-goal-stability-in-machine-learning",
    "url": null,
    "baseScore": 2,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2015-03-03T01:31:36.000Z",
    "contents": {
      "markdown": "**Summary:** *We might learn some interesting things if we can construct a model of goal stability, wireheading, and corrigibility in a present-day machine learning algorithm. I outline one way that we could potentially do this, and ask if you'd like to help!* \r\n\r\nSo far, MIRI has focused on doing crisp proofs about simple mathematical models (with unbounded computation or other unfeasible features) rather than building and studying messy heuristic analogues of the topics of interest. There are some excellent reasons for this focus, but there are also some topics which we could safely and usefully study in both ways. Here's one such example.\r\n\r\nThe intuitive phenomenon we'd eventually like to study is [corrigibility](https://intelligence.org/files/Corrigibility.pdf). An agent with sufficient self-reflective capacity will generally want to prevent changes to its goal system (see e.g. [section 2.2 of this paper](http://www.nickbostrom.com/superintelligentwill.pdf)), even if these changes come from its developers, and this gets to be a problem as soon as an AI is capable of resisting, manipulating, or deceiving its developers; but perhaps some special kinds of goal systems would be better suited for allowing such correction.\r\n\r\nThere are several stages to building up a useful model of corrigibility, some of them almost certainly done already in the literature and others not.\r\n\r\nFirst, we could represent goal stability and accidental wireheading, perhaps as follows:\r\n\r\n* We start with a machine learning algorithm trying to optimize a criterion (e.g. a cost function) over its training data, and being tested on the application of its model to testing data.\r\n* Next, the system should be able to modify *the criterion itself*, at least partially. (For example, some of its outputs are parameters of the next iteration of the criterion, while others are parameters of the next iteration of the model.)\r\n* I'd expect that some initially complex criteria collapse to trivial criteria after iterated modifications (i.e. [wireheading](http://en.wikipedia.org/wiki/Wirehead_(science_fiction))) while others remain stable (preferably even in the face of some threshold of random variation). There's probably interesting work to be done even here, with (e.g.) constraints and regularization parameters that aren't affected by the output.\r\n\r\nBut this type of system doesn't have any self-reflection; it can't foresee that changing the criterion can have big effects on making it easier to optimize that criterion, it can only locally drift toward ways to make its score higher. So at the next stage, we'll want to represent that reasoning *within the system*, at least heuristically. One way we could do this is a version of [bootstrapping](http://en.wikipedia.org/wiki/Bootstrapping_(statistics)).\r\n\r\nLet's take the naive machine learning algorithm (which, again, iteratively modifies both the model and the criteria based on its training data) and call it as a function F. We're going to build a new algorithm X which calls on F.\r\n\r\nX takes a bunch of independent random subsamples of its training set, and on each subsample it runs F for a bunch of iterations. Then X scores each resulting model on *X's original criterion* (not the updated criteria of the models); X then chooses among or aggregates the resulting models based on how well they did. Then X takes an updated model and criterion, and repeats the process.\r\n\r\nIn pseudocode:\r\n\r\n    def score(model, criteria):\r\n      return [the actual score that a model achieves by the criteria]\r\n\r\n    def F(model, criteria, training set):\r\n      [run some machine learning algorithm]\r\n      return (new model, new criteria)\r\n\r\n    def X(initial_model, initial_criteria, training set):\r\n      best_model = initial_model\r\n      best_criteria = initial_criteria\r\n      best_score = score(initial_model, initial_criteria)\r\n      for n in range(1,N):\r\n        sample = independent random sample of training set\r\n        current_model = initial_model\r\n        current_criteria = initial_criteria\r\n        for k in range(1,K):\r\n          current_model, current_criteria = F(current_model, current_criteria, sample)\r\n        if score(current_model, initial_criteria) > best_score: #Note that it uses the initial criteria!\r\n          best_model = current_model\r\n          best_criteria = current_criteria\r\n          best_score = score(current_model, initial_criteria)\r\n      return (best_model, best_criteria)\r\n\r\nThis might be a really interesting type of algorithm to consider, since it would have some ability to 'think strategically', and it seems like it should be able to wirehead intentionally or stabilize intentionally, for different kinds of criteria.\r\n\r\nAfter that, we'd want to see if we could get corrigibility involved. For instance, we could incorporate an online stream of feedback after each full iteration (i.e. the feedback from the developers of the AI), connected to some \"true criterion\" that the developers actually want to optimize, and see if there are architectures of criteria that make it more or less likely to converge toward the feedback stream. But there's enough potentially uncharted ground before we even get to that point!\r\n\r\nQuestions:\r\n\r\n0. Is there anything obviously wrong with this proposal, and if so, is there an obvious patch?\r\n1. Has this been done yet?\r\n2. Does anyone with a machine learning background want to work on this (or on related ideas)?\r\n\r\n(Thanks to multiple people I've discussed this idea with, including Eliezer, Nate, Benja, Marcello, Jelena, and Sarah. I believe there's been at least one FLI grant proposal submitted along these lines already, with my permission.)",
      "plaintextDescription": "Summary: We might learn some interesting things if we can construct a model of goal stability, wireheading, and corrigibility in a present-day machine learning algorithm. I outline one way that we could potentially do this, and ask if you'd like to help!\n\nSo far, MIRI has focused on doing crisp proofs about simple mathematical models (with unbounded computation or other unfeasible features) rather than building and studying messy heuristic analogues of the topics of interest. There are some excellent reasons for this focus, but there are also some topics which we could safely and usefully study in both ways. Here's one such example.\n\nThe intuitive phenomenon we'd eventually like to study is corrigibility. An agent with sufficient self-reflective capacity will generally want to prevent changes to its goal system (see e.g. section 2.2 of this paper), even if these changes come from its developers, and this gets to be a problem as soon as an AI is capable of resisting, manipulating, or deceiving its developers; but perhaps some special kinds of goal systems would be better suited for allowing such correction.\n\nThere are several stages to building up a useful model of corrigibility, some of them almost certainly done already in the literature and others not.\n\nFirst, we could represent goal stability and accidental wireheading, perhaps as follows:\n\n * We start with a machine learning algorithm trying to optimize a criterion (e.g. a cost function) over its training data, and being tested on the application of its model to testing data.\n * Next, the system should be able to modify the criterion itself, at least partially. (For example, some of its outputs are parameters of the next iteration of the criterion, while others are parameters of the next iteration of the model.)\n * I'd expect that some initially complex criteria collapse to trivial criteria after iterated modifications (i.e. wireheading) while others remain stable (preferably even in the face of some threshold o",
      "wordCount": 920
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5bd75cc58225bf0670374ebb",
    "title": "An Introduction to Löb's Theorem in MIRI Research",
    "slug": "an-introduction-to-loeb-s-theorem-in-miri-research",
    "url": null,
    "baseScore": 4,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2015-01-22T20:35:50.000Z",
    "contents": {
      "markdown": "At a recent MIRIx workshop, I gave an introductory talk about the surprising number of times that MIRI applied Löb's Theorem in their research papers. It was well-received, so I wrote up and expanded my notes into a primer for new researchers:\r\n\r\n[An Introduction to Löb's Theorem in MIRI Research](http://intelligence.org/files/lob-notes-IAFF.pdf) (pdf)\r\n\r\nAny comments appreciated!",
      "plaintextDescription": "At a recent MIRIx workshop, I gave an introductory talk about the surprising number of times that MIRI applied Löb's Theorem in their research papers. It was well-received, so I wrote up and expanded my notes into a primer for new researchers:\n\nAn Introduction to Löb's Theorem in MIRI Research (pdf)\n\nAny comments appreciated!",
      "wordCount": 52
    },
    "tags": [
      {
        "_id": "DFCEGpufjkvqQaRXt",
        "name": "Löb's theorem",
        "slug": "loeb-s-theorem"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "iQWk5jYeDg5ACCmpx",
    "title": "Robust Cooperation in the Prisoner's Dilemma",
    "slug": "robust-cooperation-in-the-prisoner-s-dilemma",
    "url": null,
    "baseScore": 124,
    "voteCount": 82,
    "viewCount": null,
    "commentCount": 147,
    "createdAt": null,
    "postedAt": "2013-06-07T08:30:25.557Z",
    "contents": {
      "markdown": "I'm proud to announce the preprint of [Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic](http://arxiv.org/abs/1401.5577), a joint paper with Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire (me), and Eliezer Yudkowsky.\n\nThis paper was one of three projects to come out of the [2nd MIRI Workshop on Probability and Reflection](http://intelligence.org/2013/03/07/upcoming-miri-research-workshops/) in April 2013, and had its genesis in ideas about formalizations of decision theory that have appeared on LessWrong. (At the end of this post, I'll include links for further reading.)\n\nBelow, I'll briefly outline the problem we considered, the results we proved, and the (many) open questions that remain. Thanks in advance for your thoughts and suggestions!\n\nBackground: Writing programs to play the PD with source code swap\n-----------------------------------------------------------------\n\n(If you're not familiar with the Prisoner's Dilemma, [see here.](http://wiki.lesswrong.com/wiki/Prisoner's_dilemma))\n\nThe paper concerns the following setup, [which has come up in academic research on game theory](/r/all/lw/duv/ai_cooperation_is_already_studied_in_academia_as/): say that you have the chance to write a computer program **X**, which takes in one input and returns either _Cooperate_ or _Defect_. This program will face off against some other computer program **Y**, but with a twist: **X** will receive the source code of **Y** as input, and **Y** will receive the source code of **X** as input. And you will be given your program's winnings, so you should think carefully about what sort of program you'd write!\n\nOf course, you could simply write a program that defects regardless of its input; we call this program **DefectBot**, and call the program that cooperates on all inputs **CooperateBot**. But with the wealth of information afforded by the setup, you might wonder if there's some program that might be able to achieve mutual cooperation in situations where **DefectBot** achieves mutual defection, without thereby risking a sucker's payoff. (Douglas Hofstadter would call this a perfect opportunity for [superrationality](http://www.gwern.net/docs/1985-hofstadter)...)\n\nPreviously known: CliqueBot and FairBot\n---------------------------------------\n\nAnd indeed, there's a way to do this that's been known since at least the 1980s. You can write [a computer program that knows its own source code](http://en.wikipedia.org/wiki/Quine_(computing)), compares it to the input, and returns _C_ if and only if the two are identical (and _D_ otherwise). Thus it achieves mutual cooperation in one important case where it intuitively ought to: when playing against itself! We call this program **CliqueBot**, since it cooperates only with the \"clique\" of agents identical to itself.\n\nThere's one particularly irksome issue with **CliqueBot**, and that's the fragility of its cooperation. If two people write functionally analogous but syntactically different versions of it, those programs will defect against one another! This problem can be patched somewhat, but not fully fixed. Moreover, mutual cooperation might be the best strategy against some agents that are not even functionally identical, and extending this approach requires you to explicitly delineate the list of programs that you're willing to cooperate with. Is there a more flexible and robust kind of program you could write instead?\n\nAs it turns out, there is: [in a 2010 post on LessWrong](/lw/2ip/ai_cooperation_in_practice/), cousin_it introduced an algorithm that we now call **FairBot**. Given the source code of **Y**, **FairBot** searches for a proof (of less than some large fixed length) that **Y** returns _C_ when given the source code of **FairBot**, and then returns _C_ if and only if it discovers such a proof (otherwise it returns _D_). Clearly, if our proof system is consistent, **FairBot** only cooperates when that cooperation will be mutual. But the really fascinating thing is what happens when you play two versions of **FairBot** against each other. Intuitively, it seems that _either_ mutual cooperation or mutual defection would be stable outcomes, but it turns out that if their limits on proof lengths are sufficiently high, they will achieve mutual cooperation!\n\nThe proof that they mutually cooperate follows from a bounded version of [Löb's Theorem](http://en.wikipedia.org/wiki/L%C3%B6b's_theorem) from mathematical logic. (If you're not familiar with this result, you might enjoy [Eliezer's Cartoon Guide to Löb's Theorem](/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/), which is a correct formal proof written in much more intuitive notation.) Essentially, the asymmetry comes from the fact that both programs are searching for the same outcome, so that a short proof that one of them cooperates leads to a short proof that the other cooperates, and vice versa. (The opposite is not true, because [the formal system can't know it won't find a contradiction](/lw/t8/you_provably_cant_trust_yourself/). This is a subtle but essential feature of mathematical logic!)\n\nGeneralization: Modal Agents\n----------------------------\n\nUnfortunately, **FairBot** isn't what I'd consider an ideal program to write: it happily cooperates with **CooperateBot**, when it could do better by defecting. This is problematic because in real life, the world isn't separated into agents and non-agents, and any natural phenomenon that doesn't predict your actions can be thought of as a **CooperateBot** (or a **DefectBot**). You don't want your agent to be making concessions to rocks that happened not to fall on them. (There's an important caveat: some things have utility functions that you care about, but don't have sufficient ability to predicate their actions on yours. In that case, though, it [wouldn't be a true Prisoner's Dilemma](/lw/tn/the_true_prisoners_dilemma/) if your values actually prefer the outcome (_C_,_C_) to (_D_,_C_).)\n\nHowever, **FairBot** belongs to a promising class of algorithms: those that decide on their action by looking for short proofs of logical statements that concern their opponent's actions. In fact, there's a really convenient mathematical structure that's analogous to the class of such algorithms: the [modal logic of provability](http://plato.stanford.edu/entries/logic-provability/) (known as GL, for Gödel-Löb).\n\nSo that's the subject of this preprint: **what can we achieve in decision theory by considering agents defined by formulas of provability logic?**\n\nMore formally _(skip the next two paragraphs if you're willing to trust me)_, we inductively define the class of \"modal agents\" as formulas using propositional variables and [logical connectives](http://en.wikipedia.org/wiki/Logical_connective) and the modal operator ![](http://www.codecogs.com/png.latex?\\Box) (which represents provability in some base-level formal system like Peano Arithmetic), of the form ![](http://www.codecogs.com/png.latex?P\\leftrightarrow \\varphi(P,Q,R_1,\\dots,R_N)), where ![](http://www.codecogs.com/png.latex?\\varphi) is fully modalized (i.e. all instances of variables are contained in an expression ![](http://www.codecogs.com/png.latex?\\Box\\psi)), and with each ![](http://www.codecogs.com/png.latex?R_i) corresponding to a fixed modal agent of lower rank. For example, **FairBot** is represented by the modal formula ![](http://www.codecogs.com/png.latex?P\\leftrightarrow \\Box Q).\n\nWhen two modal agents play against each other, the outcome is given by the unique fixed point of the system of modal statements, where the variables are identified with each other so that ![](http://www.codecogs.com/png.latex?P) represents the expression ![](http://www.codecogs.com/png.latex?X(Y)=C), ![](http://www.codecogs.com/png.latex?Q) represents ![](http://www.codecogs.com/png.latex?Y(X)=C), and the ![](http://www.codecogs.com/png.latex?R_i) represent the actions of lower-rank modal agents against ![](http://www.codecogs.com/png.latex?Y) and vice-versa. (Modal rank is defined as a natural number, so this always bottoms out in a finite number of modal statements; also, we interpret outcomes as statements of provability in Peano Arithmetic, evaluated in the model where PA is consistent, PA+Con(PA) is consistent, and so on. See the paper for the actual details.)\n\nThe nice part about modal agents is that there are [simple tools](http://en.wikipedia.org/wiki/Kripke_semantics) for finding the fixed points without having to search through proofs; in fact, Mihaly and Marcello wrote up a computer program to deduce the outcome of the source-code-swap Prisoner's Dilemma between any two (reasonably simple) modal agents. These tools also made it much easier to prove general theorems about such agents.\n\nPrudentBot: The best of both worlds?\n------------------------------------\n\nCan we find a modal agent that seems to improve on **FairBot**? In particular, we should want at least the following properties:\n\n*   It should be un-exploitable: if our axioms are consistent in the first place, then it had better only end up cooperating when it's mutual.\n*   It should cooperate with itself, and also mutually cooperate with **FairBot** (both are, common-sensically, the best actions in those cases).\n*   It should defect, however, against **CooperateBot** and lots of similarly exploitable modal agents.\n\nIt's nontrivial that such an agent exists: you may remember the post I wrote about [the Masquerade agent](/lw/ebx/decision_theories_part_375_hang_on_i_think_this/), which is a modal agent that does _almost_ all of those things (it doesn't cooperate with the original **FairBot**, though it does cooperate with some more complicated variants), and indeed we didn't find anything better until after we had Mihaly and Marcello's modal-agent-evaluator to help us.\n\nBut as it turns out, there is such an agent, and it's pretty elegant: we call it **PrudentBot**, and its modal version cooperates with another agent **Y** if and only if (there's a proof in Peano Arithmetic that **Y** cooperates with **PrudentBot** and there's a proof in PA+Con(PA) that **Y** defects against **DefectBot**). This agent can be seen to satisfy all of our criteria. But is it _optimal_ among modal agents, by any reasonable criterion?\n\nResults: Obstacles to Optimality\n--------------------------------\n\nIt turns out that, even within the class of modal agents, it's hard to formulate a definition of optimality that's actually true of something, and which meaningfully corresponds to our intuitions about the \"right\" decisions on decision-theoretic problems. (This intuition is not formally defined, so I'm using scare quotes.)\n\nThere are agents that give preferential treatment to **DefectBot**, **FairBot**, or even **CooperateBot**, compared to **PrudentBot**, though these agents are not ones you'd program in an attempt to win at the Prisoner's Dilemma. (For instance, one agent that rewards **CooperateBot **over **PrudentBot** is the agent that cooperates with **Y** iff PA proves that **Y** cooperates against **DefectBot**; we've taken to jokingly calling that agent **TrollBot**.) One might well suppose that a modal agent could still be optimal in the sense of making the \"right\" decision in every case, regardless of whether it's being punished for some other decision. However, this is not the only obstacle to a useful concept of optimality.\n\nThe second obstacle is that any modal agent only checks proofs at some finite number of levels on the hierarchy of formal systems, and agents that appear indistinguishable at all those levels may have obviously different \"right\" decisions. And thirdly, an agent might mimic another agent in such a way that the \"right\" decision is to treat the mimic differently from the agent it imitates, but in some cases one can prove that no modal agent can treat the two differently.\n\nThese three strikes appear to indicate that if we're looking to formalize [more advanced decision theories](http://wiki.lesswrong.com/wiki/Decision_theory), modal agents are too restrictive of a class to work with. We might instead allow things like quantifiers over agents, which would invalidate these specific obstacles, but may well introduce new ones (and certainly would make for more complicated proofs). But for a \"good enough\" algorithm on the original problem (assuming that the computer will have lots of computational resources), one could definitely do worse than submit a finite version of **PrudentBot**.\n\nWhy is this awesome, and what's next?\n-------------------------------------\n\nIn my opinion, the result of Löbian cooperation deserves to be published for its illustration of Hofstadterian superrationality in action, apart from anything else! It's _really cool_ that two agents reasoning about each other can in theory come to mutual cooperation for genuine reasons that don't have to involve being clones of each other (or other anthropic dodges). It's a far cry from a practical approach, of course, but it's a start: mathematicians always begin with a simplified and artificial model to see what happens, then add complications one at a time.\n\nAs for what's next: First, we don't _actually_ know that there's no meaningful non-vacuous concept of optimality for modal agents; it would be nice to know that one way or another. Secondly, we'd like to see if some other class of agents contains a simple example with really nice properties (the way that classical game theory doesn't always have a pure Nash equilibrium, but always has a mixed one). Thirdly, we might hope that there's an actual implementation of a decision theory ([TDT](http://wiki.lesswrong.com/wiki/Timeless_decision_theory), [UDT](http://wiki.lesswrong.com/wiki/Updateless_decision_theory), etc) in the context of program equilibrium.\n\nIf we succeed in the positive direction on any of those, we'd next want to extend them in several important ways: using probabilistic information rather than certainty, considering more general games than the Prisoner's Dilemma (bargaining games have many further challenges, and games of more than two players could be more convoluted still), etc. I personally hope to work on such topics in future MIRI workshops.\n\nFurther Reading on LessWrong\n----------------------------\n\nHere are some LessWrong posts that have tackled similar material to the preprint:\n\n*   [AI cooperation in practice](/lw/2ip/ai_cooperation_in_practice/), cousin_it, 2010\n*   [Notion of Preference in Ambient Control](/lw/2tq/notion_of_preference_in_ambient_control/), Vladimir_Nesov, 2010\n*   [A model of UDT with a halting oracle](/lw/8wc/a_model_of_udt_with_a_halting_oracle/), cousin_it, 2011\n*   [Formulas of arithmetic that behave like decision agents](/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/), Nisan, 2012\n*   [A model of UDT without proof limits](/lw/b0e/a_model_of_udt_without_proof_limits/), [An example of self-fulfilling spurious proofs in UDT](/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/), [Löbian cooperation, version 2](/lw/crx/loebian_cooperation_version_2/), [Bounded versions of Gödel's and Löb's theorems](/lw/dba/bounded_versions_of_g%C3%B6dels_and_l%C3%B6bs_theorems/), cousin_it, 2012\n*   [Predictability of decisions and the diagonal method](/lw/ap3/predictability_of_decisions_and_the_diagonal/), [Consequentialist formal systems](/lw/ca5/consequentialist_formal_systems/), Vladimir_Nesov, 2012\n*   Decision Theories: A Semi-Formal Analysis: [Part 0 (A LessWrong Primer)](/lw/aq9/decision_theories_a_less_wrong_primer/), [Part 1 (The Problem with Naive Decision Theory)](/lw/axl/decision_theories_a_semiformal_analysis_part_i), [Part 2 (Causal Decision Theory and Substitution)](/lw/az6/decision_theories_a_semiformal_analysis_part_ii/), [Part 3 (Formalizing Timeless Decision Theory)](/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/), [Part 3.5 (Halt, Melt, and Catch Fire)](/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/), [Part 3.75 (Hang On, I Think This Works After All)](/lw/ebx/decision_theories_part_375_hang_on_i_think_this/), orthonormal, 2012",
      "plaintextDescription": "I'm proud to announce the preprint of Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic, a joint paper with Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire (me), and Eliezer Yudkowsky.\n\nThis paper was one of three projects to come out of the 2nd MIRI Workshop on Probability and Reflection in April 2013, and had its genesis in ideas about formalizations of decision theory that have appeared on LessWrong. (At the end of this post, I'll include links for further reading.)\n\nBelow, I'll briefly outline the problem we considered, the results we proved, and the (many) open questions that remain. Thanks in advance for your thoughts and suggestions!\n\n\nBackground: Writing programs to play the PD with source code swap\n(If you're not familiar with the Prisoner's Dilemma, see here.)\n\nThe paper concerns the following setup, which has come up in academic research on game theory: say that you have the chance to write a computer program X, which takes in one input and returns either Cooperate or Defect. This program will face off against some other computer program Y, but with a twist: X will receive the source code of Y as input, and Y will receive the source code of X as input. And you will be given your program's winnings, so you should think carefully about what sort of program you'd write!\n\nOf course, you could simply write a program that defects regardless of its input; we call this program DefectBot, and call the program that cooperates on all inputs CooperateBot. But with the wealth of information afforded by the setup, you might wonder if there's some program that might be able to achieve mutual cooperation in situations where DefectBot achieves mutual defection, without thereby risking a sucker's payoff. (Douglas Hofstadter would call this a perfect opportunity for superrationality...)\n\n\nPreviously known: CliqueBot and FairBot\nAnd indeed, there's a way to do this that's been known since at lea",
      "wordCount": 2103
    },
    "tags": [
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      },
      {
        "_id": "oEHrEomaEagQr5TW8",
        "name": "Prisoner's Dilemma",
        "slug": "prisoner-s-dilemma"
      },
      {
        "_id": "chuP2QqQycjD8qakL",
        "name": "Coordination / Cooperation",
        "slug": "coordination-cooperation"
      },
      {
        "_id": "DFCEGpufjkvqQaRXt",
        "name": "Löb's theorem",
        "slug": "loeb-s-theorem"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "N8EsWx6fEkw7sv87P",
    "title": "Compromise: Send Meta Discussions to the Unofficial LessWrong Subreddit",
    "slug": "compromise-send-meta-discussions-to-the-unofficial-lesswrong",
    "url": null,
    "baseScore": -2,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 42,
    "createdAt": null,
    "postedAt": "2013-04-23T01:37:31.762Z",
    "contents": {
      "markdown": "After a recent comment thread degenerated into an argument about trolling, moderation, and meta discussions, I came to the following conclusions:\n\n1.  Meta conversations are annoying to stumble across, I'd rather not see them unless I think it's important, and I think other people mostly feel the same way. Moreover, moderators can't easily ignore those conversations when they encounter them, because they're usually attacks on the moderators themselves; and people can't simply avoid encountering them on a regular basis without avoiding LW altogether. This is a perfect recipe for a flamewar taking over Top Comments even when most people don't care that much.\n2.  Officially banning all meta conversations, however, is a bad precedent, and I don't want LW to do that.\n\nIdeally, Less Wrong would implement a separate \"META\" area (so that people can read the regular area for all the object-level discussions, and then sally into the meta area only when they're ready). After talking to Luke (who also wants this), though, it seems clear that nobody is able to implement it very soon. So as a stopgap measure, I'm personally going to start doing the following, and I hope you join me:\n\n**Whenever a conversation starts getting bitterly meta in a thread that's not originally about a LW site meta issue, I'm going to tell people to start a thread on the [LW Uncensored Reddit Thread](http://www.reddit.com/r/LessWrong/comments/17y819/lw_uncensored_thread/) instead. Then I'm going to downvote anyone who continues the meta war on the original thread.**\n\nI know it's annoying to send people somewhere that has a different login system, but it's as far as I can tell the best fix we currently have. Since some meta conversations are important, I'm not going to punish people for linking to meta thread discussions that they think are significant, and the relevant place for those links is usually the Open Thread. I don't want LessWrong to be a community devoted to arguing about the mechanics of LessWrong, so that's my suggestion.\n\nThoughts? (And yes, this thread is obviously open to meta discussion. I'm hopefully _doing something constructive_ about the problem, instead of just complaining about it, though.)\n\n**EDIT:** Changed the link to the uncensored thread more specifically, at Luke's request; originally I linked to the [general LW subreddit](http://www.reddit.com/r/LessWrong/), which is more heavily moderated.",
      "plaintextDescription": "After a recent comment thread degenerated into an argument about trolling, moderation, and meta discussions, I came to the following conclusions:\n\n 1. Meta conversations are annoying to stumble across, I'd rather not see them unless I think it's important, and I think other people mostly feel the same way. Moreover, moderators can't easily ignore those conversations when they encounter them, because they're usually attacks on the moderators themselves; and people can't simply avoid encountering them on a regular basis without avoiding LW altogether. This is a perfect recipe for a flamewar taking over Top Comments even when most people don't care that much.\n 2. Officially banning all meta conversations, however, is a bad precedent, and I don't want LW to do that.\n\nIdeally, Less Wrong would implement a separate \"META\" area (so that people can read the regular area for all the object-level discussions, and then sally into the meta area only when they're ready). After talking to Luke (who also wants this), though, it seems clear that nobody is able to implement it very soon. So as a stopgap measure, I'm personally going to start doing the following, and I hope you join me:\n\nWhenever a conversation starts getting bitterly meta in a thread that's not originally about a LW site meta issue, I'm going to tell people to start a thread on the LW Uncensored Reddit Thread instead. Then I'm going to downvote anyone who continues the meta war on the original thread.\n\nI know it's annoying to send people somewhere that has a different login system, but it's as far as I can tell the best fix we currently have. Since some meta conversations are important, I'm not going to punish people for linking to meta thread discussions that they think are significant, and the relevant place for those links is usually the Open Thread. I don't want LessWrong to be a community devoted to arguing about the mechanics of LessWrong, so that's my suggestion.\n\nThoughts? (And yes, this thread is obviously ",
      "wordCount": 372
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "u3jRwh7uWHvnizGyR",
    "title": "Welcome to Less Wrong! (5th thread, March 2013)",
    "slug": "welcome-to-less-wrong-5th-thread-march-2013",
    "url": null,
    "baseScore": 37,
    "voteCount": 28,
    "viewCount": null,
    "commentCount": 1746,
    "createdAt": null,
    "postedAt": "2013-04-01T16:19:17.933Z",
    "contents": {
      "markdown": "If you've recently joined the [Less Wrong community](/lw/1/about_less_wrong), please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, [how you came to identify as a rationalist](/lw/2/tell_your_rationalist_origin_story) or how you found us. You can [skip right to that](/lw/h3p/welcome_to_less_wrong_5th_thread_march_2013/#comments) if you like; the rest of this post consists of a few things you might find helpful. More can be found at the [FAQ](http://wiki.lesswrong.com/wiki/FAQ).\n\n  \n\n(This is the fifth incarnation of the welcome thread; once a post gets over 500 comments, it stops showing them all by default, so we make a new one. Besides, a new post is a good perennial way to encourage newcomers and lurkers to introduce themselves.)\n\n#### A few notes about the site mechanics  \n\nLess Wrong **comments are threaded** for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via [Markdown syntax](http://wiki.lesswrong.com/wiki/Comment_formatting) (you can click the \"Help\" link below the text box to bring up a primer).\n\n  \n\nYou may have noticed that all the posts and comments on this site have buttons to **vote them up or down**, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.\n\n  \n\nHowever, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning— not just that they disagree with you! **If you have any questions about karma or voting, please feel free to ask here.**\n\n**Replies** to your comments across the site, plus **private messages** from other users, will show up in your [inbox](/message/inbox). You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.\n\n  \n\nIt's definitely worth your time **commenting on old posts**; veteran users look through the [recent comments thread](/comments) quite often (there's a separate [recent comments thread for the Discussion section](/r/discussion/comments), for whatever reason), and a conversation begun anywhere will pick up contributors that way.  There's also a succession of [open comment threads](/tag/open_thread) for discussion of anything remotely related to rationality.\n\n  \n\nDiscussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply **\"tap out\" of a discussion** that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.\n\n  \n\n**EXTRA FEATURES:**  \n\nThere's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.\n\n**LW WIKI:** This is our attempt to make searching by topic feasible, as well as to store information like [common abbreviations](http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong) and idioms. It's a good place to look if someone's speaking Greek to you.  \n\n**LW DISCUSSION:** This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.\n\n**SEQUENCES:** A _huge_ corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.  \n\n#### A few notes about the community  \n\nIf you've come to Less Wrong to  **discuss a particular topic**, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.\n\n  \n\n**If your welcome comment starts a huge discussion**, then please move to the next step and **create a LW Discussion post to continue the conversation**; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new article\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)\n\n  \n\nIf you want to write a post about a LW-relevant topic, awesome! **I highly recommend you submit your first post to Less Wrong Discussion**; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma- honestly, you don't know what you don't know about the community norms here.)\n\n  \n\nIf you'd like to connect with other LWers in real life, we have  **meetups ** in various parts of the world. Check the [wiki page for places with regular meetups](http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups), or the [upcoming (irregular) meetups page](/meetups). There's also a [Facebook group](http://www.facebook.com/home.php#/group.php?gid=144017955332&ref=ts). If you have your own blog or other online presence, please feel free to link it.\n\n**If English is not your first language**, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address.  \n\\* [Normal_Anomaly](/user/Normal_Anomaly)  \n\\* [Randaly](/user/Randaly)  \n\\* [shokwave](/user/shokwave)  \n\\* [Barry Cotter](/user/Barry_Cotter)\n\n**A note for theists**: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.\n\n#### A list of some posts that are pretty awesome  \n\nI recommend the [major sequences](http://wiki.lesswrong.com/wiki/Sequences) to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:\n\n*   [Your Intuitions are Not Magic](/lw/2bu/your_intuitions_are_not_magic)\n*   [The Apologist and the Revolutionary](/lw/20/the_apologist_and_the_revolutionary)\n*   [How to Convince Me that 2 + 2 = 3](/lw/jr/how_to_convince_me_that_2_2_3)\n*   [Lawful Uncertainty](/lw/vo/lawful_uncertainty)\n*   [The Planning Fallacy](/lw/jg/planning_fallacy)\n*   [Scope Insensitivity](/lw/hw/scope_insensitivity)\n*   [The Allais Paradox](/lw/my/the_allais_paradox)  (with [two](/lw/mz/zut_allais) [followups](/lw/n1/allais_malaise))\n*   [We Change Our Minds Less Often Than We Think](/lw/jx/we_change_our_minds_less_often_than_we_think)\n*   [The Least Convenient Possible World](/lw/2k/the_least_convenient_possible_world)\n*   [The Third Alternative](/lw/hu/the_third_alternative)\n*   [The Domain of Your Utility Function](/lw/116/the_domain_of_your_utility_function)\n*   [Newcomb's Problem and Regret of Rationality](/lw/nc/newcombs_problem_and_regret_of_rationality)\n*   [The True Prisoner's Dilemma](/lw/tn/the_true_prisoners_dilemma)\n*   [The Tragedy of Group Selectionism](/lw/kw/the_tragedy_of_group_selectionism)\n*   [Policy Debates Should Not Appear One-Sided](/lw/gz/policy_debates_should_not_appear_onesided)\n*   [That Alien Message](/lw/qk/that_alien_message)\n*   [The Worst Argument in the World](/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/)\n\nMore suggestions are welcome! Or just check out the [top-rated posts from the history of Less Wrong](/top/?t=all). Most posts at +50 or more are well worth your time.\n\nWelcome to Less Wrong, and we look forward to hearing from you throughout the site!\n\n**Note from orthonormal:** MBlume and other contributors wrote the original version of this welcome post, and I've edited it a fair bit. If there's anything I should add or update on this post (especially broken links), please send me a private message—I may not notice a comment on the post. Finally, once this gets past 500 comments, anyone is welcome to copy and edit this intro to start the next welcome thread.",
      "plaintextDescription": "If you've recently joined the Less Wrong community, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, how you came to identify as a rationalist or how you found us. You can skip right to that if you like; the rest of this post consists of a few things you might find helpful. More can be found at the FAQ.\n\n\n(This is the fifth incarnation of the welcome thread; once a post gets over 500 comments, it stops showing them all by default, so we make a new one. Besides, a new post is a good perennial way to encourage newcomers and lurkers to introduce themselves.)\n\nA FEW NOTES ABOUT THE SITE MECHANICS\n\n\nLess Wrong comments are threaded for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via Markdown syntax (you can click the \"Help\" link below the text box to bring up a primer).\n\n\nYou may have noticed that all the posts and comments on this site have buttons to vote them up or down, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.\n\n\nHowever, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning— not just that they disagree with you! If you have any questions about karma or voting, please feel free to ask here",
      "wordCount": 1529
    },
    "tags": [
      {
        "_id": "ABG8vt87eW4FFA6gD",
        "name": "Open Threads",
        "slug": "open-threads"
      },
      {
        "_id": "MfpEPj6kJneT9gWT6",
        "name": "Site Meta",
        "slug": "site-meta"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bQN4KcRZXRjhoCnZk",
    "title": "Robin Hanson's Cryonics Hour",
    "slug": "robin-hanson-s-cryonics-hour",
    "url": null,
    "baseScore": 47,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2013-03-29T17:20:23.897Z",
    "contents": {
      "markdown": "I'm writing to recommend something awesome to anyone who's recently signed up for cryonics (and to the future self of anyone who's about to do so). Robin Hanson has a longstanding offer that [anyone who's newly signed up for cryonics can have an hour's discussion with him on any topic](http://www.overcomingbias.com/2009/03/my-cryonics-hour.html), and I took him up on that last week.\n\nI expected to have a fascinating and wide-ranging discussion on various facets of futurism. **My expectations were exceeded.** Even if you've been reading [Overcoming Bias](http://www.overcomingbias.com/) for a long time, talking with Robin is an order of magnitude more stimulating/persuasive/informative than reading OB or even watching him debate someone else, and I'm now reconsidering my thinking on a number of topics as a result.\n\nSo if you've recently signed up, [email Robin](mailto:rhanson@gmu.edu); and if you're intending to sign up, let this be one more incentive to quit procrastinating!\n\n**Relevant links:**\n\n[The LessWrong Wiki article on cryonics](http://wiki.lesswrong.com/wiki/Cryonics) is a good place to start if you have a bunch of questions about the topic.\n\nIf you want to argue about whether signing up for cryonics is a good idea, two good and relatively recent threads on that subject are under the posts on [A survey of anti-cryonics writing](/lw/1r0/a_survey_of_anticryonics_writing/) and [More Cryonics Probability Estimates](/lw/fz9/more_cryonics_probability_estimates/).\n\nAnd if you are cryocrastinating (you've decided that you should sign up for cryonics, but you haven't yet), [here's a LW thread](/lw/e5e/how_to_get_cryocrastinators_to_actually_sign_up/) about taking the first step.",
      "plaintextDescription": "I'm writing to recommend something awesome to anyone who's recently signed up for cryonics (and to the future self of anyone who's about to do so). Robin Hanson has a longstanding offer that anyone who's newly signed up for cryonics can have an hour's discussion with him on any topic, and I took him up on that last week.\n\nI expected to have a fascinating and wide-ranging discussion on various facets of futurism. My expectations were exceeded. Even if you've been reading Overcoming Bias for a long time, talking with Robin is an order of magnitude more stimulating/persuasive/informative than reading OB or even watching him debate someone else, and I'm now reconsidering my thinking on a number of topics as a result.\n\nSo if you've recently signed up, email Robin; and if you're intending to sign up, let this be one more incentive to quit procrastinating!\n\nRelevant links:\n\nThe LessWrong Wiki article on cryonics is a good place to start if you have a bunch of questions about the topic.\n\nIf you want to argue about whether signing up for cryonics is a good idea, two good and relatively recent threads on that subject are under the posts on A survey of anti-cryonics writing and More Cryonics Probability Estimates.\n\nAnd if you are cryocrastinating (you've decided that you should sign up for cryonics, but you haven't yet), here's a LW thread about taking the first step.",
      "wordCount": 228
    },
    "tags": [
      {
        "_id": "ZnHkaTkxukegSrZqE",
        "name": "Cryonics",
        "slug": "cryonics"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6H5M7nnrp7Nrnr43L",
    "title": "Does My Vote Matter?",
    "slug": "does-my-vote-matter",
    "url": null,
    "baseScore": 23,
    "voteCount": 38,
    "viewCount": null,
    "commentCount": 77,
    "createdAt": null,
    "postedAt": "2012-11-05T01:23:52.009Z",
    "contents": {
      "markdown": "_(Cross-posted from elsewhere; I thought some readers here might like it too. Please excuse the unusually informal style.)_\n\n**Short answer:** Actually, yes!\n\n**Slightly longer answer:** Yes, if the election is close. You'll never get to _know_ that your vote was decisive, but one vote can substantially change the odds on Election Day nonetheless. Even if the election is a foregone conclusion (or if you don't care about the major candidates), the same reasoning applies to third parties- there are thresholds that really matter to them, and if they reach those now they have a significantly better chance in the next election. And finally, local elections matter in the long run just as state or nation elections do. So, in most cases, voting is rational if you care about the outcome.\n\n**Full answer:** Welcome! This is a nonpartisan analysis, written by a math PhD, of when and how a single vote matters in a large election. I've got a table of contents below; please skip to whatever section interests you most. And feel free to share this!\n\n**Sections:**\n\n**1\\. How can a single vote matter in a huge election?**\n\n**2\\. What if I know it's not going to be close?**\n\n**3\\. Do local elections matter?**\n\nIn what follows, I'm going to be assuming an American-style voting system (first-past-the-post, for you voting-system buffs), but most of what I say carries over to other voting systems found around the world.\n\n**1\\. How can a single vote matter in a huge election?**\n\nTo answer this, let's imagine a different voting system. In the land of Erewhon, voters cast their ballots for president just as they do here; but instead of decreeing that the candidate with the most votes is the winner, each vote is turned into a lottery ball, and one is chosen at random to determine the next president.\n\nWhile this system has its drawbacks (they get fringe candidates elected every so often, and they've had to outlaw write-in campaigns to prevent every voter from simply voting themselves in), the citizens of Erewhon agree on its main advantage: every one of them knows that their vote _counts_, that they increase the chance of their candidate winning by just that much.\n\n**I'm going to argue that if you would bother to vote if an election were held in Erewhon, then you should also vote—for the same reason—if the same election were held the normal way, and if it looked like the outcome might be close.** That is, your effect on the outcome is about equivalent if the pre-election polls fall within the margin of error (about ±3% for each candidate, or a margin of less than 6% between two candidates, for an electorate of millions). And if the polls are nearly tied, then voting in our system might have the same effect as voting hundreds or even thousands of times in Erewhon's system!\n\nThis seems counterintuitive, because we imagine that the votes of everyone else are \"locked in\" somehow, and that we're only deciding whether to add ours to the pile- in which case, the only way that it could matter is in the event that it makes or breaks an exact tie. And not only are those exceedingly rare (the largest such example I could find was [a recent Massachusetts election for state representative](http://www.boston.com/news/local/breaking_news/2011/02/judge_rules_wor.html), in which 13,500 ballots were cast), but if the initial count for a massive election did show a margin of one or zero, we would be headed for an interminable recount. (See, for instance, the [2008 Minnesota Senate election](http://en.wikipedia.org/wiki/United_States_Senate_election_in_Minnesota,_2008), which was decided by about 300 votes; the extensive recount delayed the winner's inauguration by about six months.) What this messes with, though, isn't your chance of helping decide the election, but your chance of _knowing_ that you helped decide the election.\n\nThat is, in modern elections, there's not a perfect sharp boundary between \"Candidate A wins\" and \"Candidate B wins\", but a gigantic muddle; and if A is narrowly ahead, then every additional vote for A represents a greater chance that the recount will eventually end, or end earlier, or not be contested at all; while every vote for candidate B means a greater chance that there will be a recount, or that it will go on longer, or that it might turn out victorious for B after all. You'd never know that _your_ vote, which changed the lead from 412 to 413, was the straw that broke the proverbial camel's back and led the other candidate to concede, but at some point that's what happens.\n\nAnd even more significantly, we _can't_ consider everyone else's votes as \"locked in\". If you had the ability to re-play Election Day over and over, the chaotic dynamics of everyday life would affect the vote totals. Someone makes a light about to turn red, and so she ends up at the next light behind a bumper sticker that infuriates her, and so she remembers to vote. Or someone else bumps into an old friend, and starts a conversation, and then he realizes he doesn't have enough of his lunch break left to get to the polling place anyhow. It's the [butterfly effect](http://en.wikipedia.org/wiki/Butterfly_effect) in action, and when multiplied by millions of people it leads to fluctuations in the hundreds or thousands. (The exact mechanism for this estimate is the [Central Limit Theorem](http://en.wikipedia.org/wiki/Central_limit_theorem).) And what that means is that the margin isn't a fixed number pending your decision to vote; it's a mix of different possibilities, and your decision changes the odds as surely as adding a lottery ball to the Erewhon election does.\n\nOf course, if one candidate is polling 10 points ahead of the other, then those fluctuations will be irrelevant. If the margin is closer, then it just might have an effect; remember that polls have several sources of error in them, so you can't be exactly sure of the margin before voting actually happens. And in those cases where the actual vote turns out to be extremely close, like Minnesota in 2008, your choice to vote swings those odds just as if you'd poured in thousands of Erewhonian lottery balls with your candidate's name on them. (That is, it changes those odds by a fraction of a percent, but the ability to nudge the odds by that much in an electorate of millions is pretty impressive!)\n\n**2\\. What if I know it's not going to be close?**\n\nIf the difference between the top candidates is outside the combined margin of error (7 points or more in a big election), then it's true that your vote won't affect the outcome of the _current_ election, but it can matter greatly for the _next_ one. This especially holds when there's a third party you prefer to the current major parties, but there are other relevant reasons to vote even if that's not the case.\n\nFirst, about third parties: the conventional wisdom that they can't win is an outright falsehood. We tend to forget that [Ross Perot nearly became President](http://en.wikipedia.org/wiki/Ross_Perot_presidential_campaign,_1992): he held a six-point lead in June 1992 over both Bush and Clinton, and despite his candidacy crashing and burning later on (for reasons more personal than systemic), he still won nearly 20% of the popular vote in the end. In addition, there are two current Senators who were elected as independents.\n\nFor a third party that's not polling well enough yet, votes _now_ matter for _future_ elections, and there are several significant thresholds. If they get rounded up to 1% on Election Day, then they get mentioned in election coverage. (Next, getting rounded up to 2% sounds much more impressive than getting rounded down to 1%.) If they get to 5%, then they can qualify for FEC matching funds, and double their ability to reach voters next time. And 10% is a significant number for media exposure (as well as invitations to the main debates). Higher than 20%, and we're talking about a viable candidate; there's a runaway dynamic in three-candidate races where as soon as the third party looks viable, any voters from the other parties who prefer the third party will suddenly switch (now that they're no longer worried about supporting an obvious loser), and suddenly the third party becomes the front-runner. (This kind of behavior is common in game theory in what's known as a [coordination problem](http://en.wikipedia.org/wiki/Coordination_game).)\n\nEven if there's not a third party you prefer to the main ones, the margin of victory now helps determine what sort of candidates the parties select next time. If one party wins an election by more than 10% (this round number has a psychological hold on people, so it's what usually counts as a \"landslide victory\"), then next time that party is more likely to nominate a less moderate candidate, while the defeated party is likely to nominate a more moderate candidate. (Also, it goes without saying that you should be voting in primaries as well!)\n\nAll of these thresholds can hinge just as easily on a few hundred votes, and so your vote can matter greatly. You just need to care about what happens two, or four, or six years from now.\n\n**3\\. Do local elections matter?**\n\nMost certainly. First, your vote affects the odds more strongly in a local election. (And, of course, there _can_ be margins of zero or one votes, without recounts, in local elections!) Secondly, there are a number of important issues decided at the local level, both directly (tax and bond issues) and through representatives (especially local school boards). And finally, many big names in politics start out in local elections: [the current President began as an Illinois state senator](http://en.wikipedia.org/wiki/Barack_Obama#State_Senator:_1997.E2.80.932004), and [the current Vice President began as a city councilman](http://en.wikipedia.org/wiki/Joe_Biden#Family_and_early_political_career).\n\nIt's more difficult to do research at the local level. [Project VoteSmart](http://votesmart.org/) may have data on some of the candidates (including the voting record of incumbents, public statements, and response to a questionnaire about their positions). Otherwise, your local newspaper may be the best bet (short of going to campaign events yourself). But a little bit of research can go a long way here.\n\n_(**Addendum:** If I'd been composing this for the Less Wrong crowd, I'd have also noted that the decisions of people similar to you should be correlated, which adds another multiplier to the effectiveness of voting. I might like I bumper sticker that says \"I'm a timeless decision theorist, therefore I vote!\")_\n\n_(**Second addendum:** I'm mindful of the dangers of posting this within a few days of a major election. I've done my very best to keep from [mindkilling](http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer) language; I hope you do the same in the comments.)_",
      "plaintextDescription": "(Cross-posted from elsewhere; I thought some readers here might like it too. Please excuse the unusually informal style.)\n\nShort answer: Actually, yes!\n\nSlightly longer answer: Yes, if the election is close. You'll never get to know that your vote was decisive, but one vote can substantially change the odds on Election Day nonetheless. Even if the election is a foregone conclusion (or if you don't care about the major candidates), the same reasoning applies to third parties- there are thresholds that really matter to them, and if they reach those now they have a significantly better chance in the next election. And finally, local elections matter in the long run just as state or nation elections do. So, in most cases, voting is rational if you care about the outcome.\n\nFull answer: Welcome! This is a nonpartisan analysis, written by a math PhD, of when and how a single vote matters in a large election. I've got a table of contents below; please skip to whatever section interests you most. And feel free to share this!\n\nSections:\n\n1. How can a single vote matter in a huge election?\n\n2. What if I know it's not going to be close?\n\n3. Do local elections matter?\n\nIn what follows, I'm going to be assuming an American-style voting system (first-past-the-post, for you voting-system buffs), but most of what I say carries over to other voting systems found around the world.\n\n1. How can a single vote matter in a huge election?\n\nTo answer this, let's imagine a different voting system. In the land of Erewhon, voters cast their ballots for president just as they do here; but instead of decreeing that the candidate with the most votes is the winner, each vote is turned into a lottery ball, and one is chosen at random to determine the next president.\n\nWhile this system has its drawbacks (they get fringe candidates elected every so often, and they've had to outlaw write-in campaigns to prevent every voter from simply voting themselves in), the citizens of Erewhon agree on its main adv",
      "wordCount": 1738
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "X9vT3o3MmtWoRRKkm",
    "title": "Decision Theories, Part 3.75: Hang On, I Think This Works After All",
    "slug": "decision-theories-part-3-75-hang-on-i-think-this-works-after",
    "url": null,
    "baseScore": 39,
    "voteCount": 25,
    "viewCount": null,
    "commentCount": 45,
    "createdAt": null,
    "postedAt": "2012-09-06T16:23:37.670Z",
    "contents": {
      "markdown": "**Followup to:** [Decision Theories, Part 3.5: Halt, Melt and Catch Fire](/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/), [Decision Theories: A Semi-Formal Analysis, Part III](/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/)\n\nThe thing about dead mathematical proofs is that it's practically always worth looting the corpse; sometimes you even find there's still a pulse! That appears to be the case with my recent post lamenting the demise of the TDT-like \"Masquerade\" algorithm. I think I've got a rigorous proof this time, but I'd like other opinions before I declare that the rumors of Masquerade's demise were greatly exaggerated...\n\nTo recap quickly, I've been trying to construct an algorithm that, given the payoff matrix of a game and the source code of its opponent, does some deductions and then outputs a move. I want this algorithm to do the commonsense right things (defect against both DefectBot and CooperateBot, and mutually cooperate against both FairBot and itself), and I want it to do so for simple and general reasons (that is, no gerrymandering of actions against particular opponents, and in particular no fair trying to \"recognize itself\", since there can be variants of any algorithm that are functionally identical but not provably so within either's powers of deduction). I'd also like it to be \"un-exploitable\" in a certain sense: it has a default move (which is one of its Nash equilibria), and no opponent can _profit_ against the algorithm by forcing it below that default payoff. If the opponent does as well or better in expected value than it would by playing that Nash equilibrium, then so too does my algorithm.\n\nThe revised Masquerade algorithm does indeed have these properties.\n\nIn essence, there are two emendations that I needed: firstly, since some possible pairs of masks (like FairBot and AntiFairBot) can't knowingly settle on a fixed point, there's no way to determine what they do without a deductive capacity that strictly exceeds the weaker of them. That's a bad feature to have, so we'll just have to exclude potentially troublemaking masks from Masquerade's analysis. (In the special case of Prisoner's Dilemma I know that including DefectBot and FairBot will suffice; I've got what looks like a good solution in general, as well.)\n\nThe second emendation is that FairBot needs to alternate between trying harder to prove its opponent cooperates, and trying harder to prove its opponent defects. (There needs to be an asymmetry, like cooperation proofs going first, to guarantee that when FairBot plays against itself, it finds a Löbian proof of mutual cooperation rather than one of mutual defection.) The reason for this is so that when agents reason about masks, they should be able to find a proof of the mask's action _without_ needing to exceed that mask's powers of deduction. Otherwise we get that arms race again.\n\nThis escalation of proof attempts can be represented in terms of proof limits (since there exists a constant C such that for N sufficiently large, a proof that \"there are no proofs of X of length less than N\" either exists with length less than C^N or not at all), but the simplest way to do this is with the formalism of [PA+N](/lw/t8/you_provably_cant_trust_yourself/). That is, PA is Peano Arithmetic; PA+1 is the formal system with the axioms of Peano Arithmetic and an extra axiom that PA is self-consistent (that is, if PA proves X, then PA does not prove not-X); PA+2 has those axioms and an extra one stating that PA+1 is self-consistent and so on. (Note that none of these formal systems know themselves to be self-consistent, and [for good reason!](/lw/t8/you_provably_cant_trust_yourself/)) In every use, we'll assume that N is a fixed number (anything greater than 4 will work).\n\n### New And Improved Masks  \n\nSo without further ado, let's define our masks for the Prisoner's Dilemma:\n\ndef DefectBot(X):  \n    return D  \n  \ndef FairBot(X):  \n    for i in range(N):  \n        if \"X(FairBot)=C\" is provable in PA+i:  \n            return C  \n        if \"X(FairBot)=D\" is provable in PA+i:  \n            return D  \n    return D\n\n**Lemma 1:** For any X, \"DefectBot(X)=D\" is provable in PA. (Obvious.)  \n  \n**Lemma 2:** \"FairBot(FairBot)=C\" is provable in PA.\n\n_Proof:_ If we label the FairBots separately (after all, they might be logically equivalent but not know they are), we can quickly show that \"if PA proves FairBot1(FairBot2)=C, then FairBot2(FairBot1)=C\" is provable in PA (and the same with the subscripts reversed). This makes what we might term a Löbian cycle, which will come up a few more times in this post: if we have a formal system S, and \"if S proves A, then B\" and \"if S proves B, then A\" are theorems of S, then S indeed proves both A and B. (Proof reserved for a comment; hat tip to Eliezer and co.)\n\n**Lemma 3:** \"FairBot(DefectBot)=D\" is provable in PA+1.\n\n_Proof:_ Note that this does _not_ work in PA, even though PA can prove \"DefectBot(FairBot)=D\". Why not? Because PA can't prove that PA doesn't find a proof of \"DefectBot(FairBot)=C\" before it can look for a proof of defection! However, PA+1 can handle this just fine, because it proves \"if PA proves DB(FB)=D, then PA does not prove DB(FB)=C\".\n\nNote the weird fact that it takes a stronger formal system to prove a result that's much more intuitive from outside the system!\n\n### New and Improved Masquerade\n\nLet's split up the long definition of Masquerade with a helper function.\n\ndef Search(X,Y):  \n    for i in range(N):  \n        for j,k in \\[C,D\\]:  \n            if \"Y(X)=j and X(Y)=k\" is provable in PA+i:  \n                return (j,k)  \n    return False\n\nThis function, given two agents, simply searches (using increasingly strong formal systems) for a proof of what they do against each other; once it finds one, it halts and returns that result, and if it doesn't find one in time, it announces its failure. We will incorporate this into the definition of Masquerade. (The other function call here is to the payoff matrix, to retrieve the utility U(j,k) of a certain pair of moves.)\n\ndef Masquerade(X):  \n    utility = -∞  \n    my_move = null  \n    their_move = null  \n    for Y in \\[DefectBot, FairBot\\]:  \n        if Search(X,Y) != False:  \n            (j,k) = Search(X,Y)  \n            if U(j,k) > utility:  \n                my_move = j  \n                their_move = k  \n                utility = U(j,k)  \n    if utility > U(D,D):  \n        for i in range(N):  \n            if \"X(Masquerade) = their_move\" is provable in PA+i:  \n                return my_move  \n    return D\n\nA few things to note: first, this agent actually halts and outputs an action against any opponent... but for it to provably do so in a system below PA+N, Search(X,Y) has to find proofs quickly. This is the reason that my prior attempt didn't work- it had to wait at one point for the old FairBot to run out of time/power before concluding what it did, and that made it impossible for the old FairBot to know what the old Masquerade did. But with the new and improved agents, we get to ground in a fixed number of steps.\n\nFor brevity, I'll label DefectBot, FairBot, and Masquerade as DB, FB, and M, respectively.\n\n**Lemma 4:** \"Search(DB,DB)=(D,D)\" is provable in PA+1. (Follows from Lemma 1; note that it needs to use PA+1 in order to rule out finding proofs of other action-pairs.)  \n  \n**Lemma 5:** \"Search(FB,DB)=Search(DB,FB)=(D,D)\" is provable in PA+2. (Follows from Lemma 3.)  \n  \n**Lemma 6:** \"Search(FB,FB)=(C,C)\" is provable in PA. (Follows from Lemma 2; since (C,C) is the first one tried, we don't even need to go up to PA+1.)  \n  \n**Lemma 7:** \"Masquerade(DB)=D\" is provable in PA+3.\n\n_Proof:_ Lemmas 4 and 5, plus the fact that PA+3 knows the consistency of PA+2. There is no sanity-check step, since utility=U(D,D) here.  \n  \n**Lemma 8:** \"Masquerade(FB)=C\" and \"FB(Masquerade)=C\" are provable in PA+3.\n\n_Proof:_ Lemmas 5 and 6, and the consistency of PA+2, imply that when Masquerade arrives at the sanity-check stage, it has the variables set as utility=U(C,C), my\\_move=C and their\\_move=C. Thus PA+3 can prove that \"if 'FB(M)=C' is provable in PA+3, then M(FB)=C\". And of course, \"if 'M(FB)=C' is provable in PA+3, then FB(M)=C\" is provable in PA+3, since again PA+3 can prove that PA through PA+2 won't have found proofs of contrary conclusions before it gets around to trying to find cooperation in PA+3. Therefore we have the desired Löbian cycle!  \n  \n**Theorem:** \"Masquerade(Masquerade)=C\" is provable in PA+4.\n\n_Proof:_ Lemmas 7 and 8, and the consistency of PA+3, allow PA+4 to prove that when each Masquerade arrives at the sanity-check stage, it has set utility=U(C,C), my\\_move=C and their\\_move=C. Thus we achieve the Löbian cycle, and find proofs of mutual cooperation!\n\n### Awesome! So, what next?\n\nWell, assuming that I haven't made a mistake in one of my proofs, I'm going to run the same proof for my generalization: given a payoff matrix in general, Masquerade enumerates all of the constant strategies and all of the \"mutually beneficial deals\" of the FairBot form (that is, masks that hold out the \"stick\" of a particular Nash equilibrium and the \"carrot\" of another spot on the payoff matrix which is superior to the \"stick\" for both players). Then it alternates (at escalating PA+n levels) between trying to prove the various good deals that the opponent could agree to. There are interesting complexities here (and an idea of what bargaining problems might involve).\n\nSecondly, I want to see if there's a good way of stating the general problem that Masquerade solves, something better than \"it agrees with commonsense decision theory\". The analogy here (and I know it's a fatuous one, but bear with me) is that I've come up with a Universal Turing Machine but not yet the Church-Turing Thesis. And that's unsatisfying.\n\nBut before anything else... I want to be _really sure_ that I haven't made a critical error somewhere, especially given my false start (and false halt) in the past. So if you spot a lacuna, let me know!",
      "plaintextDescription": "Followup to: Decision Theories, Part 3.5: Halt, Melt and Catch Fire, Decision Theories: A Semi-Formal Analysis, Part III\n\nThe thing about dead mathematical proofs is that it's practically always worth looting the corpse; sometimes you even find there's still a pulse! That appears to be the case with my recent post lamenting the demise of the TDT-like \"Masquerade\" algorithm. I think I've got a rigorous proof this time, but I'd like other opinions before I declare that the rumors of Masquerade's demise were greatly exaggerated...\n\nTo recap quickly, I've been trying to construct an algorithm that, given the payoff matrix of a game and the source code of its opponent, does some deductions and then outputs a move. I want this algorithm to do the commonsense right things (defect against both DefectBot and CooperateBot, and mutually cooperate against both FairBot and itself), and I want it to do so for simple and general reasons (that is, no gerrymandering of actions against particular opponents, and in particular no fair trying to \"recognize itself\", since there can be variants of any algorithm that are functionally identical but not provably so within either's powers of deduction). I'd also like it to be \"un-exploitable\" in a certain sense: it has a default move (which is one of its Nash equilibria), and no opponent can profit against the algorithm by forcing it below that default payoff. If the opponent does as well or better in expected value than it would by playing that Nash equilibrium, then so too does my algorithm.\n\nThe revised Masquerade algorithm does indeed have these properties.\n\nIn essence, there are two emendations that I needed: firstly, since some possible pairs of masks (like FairBot and AntiFairBot) can't knowingly settle on a fixed point, there's no way to determine what they do without a deductive capacity that strictly exceeds the weaker of them. That's a bad feature to have, so we'll just have to exclude potentially troublemaking masks from Masquerad",
      "wordCount": 1683
    },
    "tags": [
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ShD7EHb4HmPgfveim",
    "title": "Decision Theories, Part 3.5: Halt, Melt and Catch Fire",
    "slug": "decision-theories-part-3-5-halt-melt-and-catch-fire",
    "url": null,
    "baseScore": 49,
    "voteCount": 32,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2012-08-26T22:40:20.388Z",
    "contents": {
      "markdown": "**Followup to:** [Decision Theories: A Semi-Formal Analysis, Part III](/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/)\n\n**UPDATE: As it turns out, rumors of Masquerade's demise seem to have been greatly exaggerated. See [this post](/lw/ebx/decision_theories_part_375_hang_on_i_think_this/) for details and proofs!**\n\nI had the chance, over the summer, to discuss the decision theory outlined in [my April post](/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/) with a bunch of relevantly awesome people. The sad part is, **there turned out to be a fatal flaw** once we tried to formalize it properly. I'm laying it out here, not with much hope that there's a fix, but because [sometimes false starts can be productive for others](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.3404).\n\nSince it's not appropriate to call this decision theory TDT, I'm going to use a name suggested in one of these sessions and call it \"Masquerade\", which might be an intuition pump for how it operates. So let's first define some simple agents called \"masks\", and then define the \"Masquerade\" agent.\n\nSay that our agent has actions _a~1~_, ... , _a~n~_, and the agent it's facing in this round has actions _b~1~_, ... , _b~m~_. Then for any triple (_b~i~_, _a~j~_, _a~k~_), we can define a simple agent Mask_~ijk~_ which takes in its opponent's source code and outputs an action:\n\ndef Mask\\_ijk(opp\\_src):  \n look for proof that Opp(Mask_ijk) = _b~i~_  \n if one is found, then output _a~j~_  \n otherwise, output _a~k~_  \n\n(This is slightly less general than what I outlined in my post, but it'll do for our purposes. Note that there's no need for _a~j~_ and _a~k~_ to be distinct, so constant strategies fall under this umbrella as well.)\n\nA key example of such an agent is what we might call FairBot: on a Prisoner's Dilemma, FairBot tries to prove that the other agent cooperates against FairBot, and if it finds such a proof, then it immediately cooperates. If FairBot fails to find such a proof, then it defects. (An important point is that if FairBot plays against itself and both have sufficiently strong deductive capacities, then a short proof of one's cooperation gives a slightly longer proof of the other's cooperation, and thus in the right circumstances we have mutual cooperation via Löb's Theorem.)\n\nThe agent Masquerade tries to do better than any individual mask (note that FairBot foolishly cooperates against CooperateBot when it could trivially do better by defecting). My original formulation can be qualitatively described as trying on different masks, seeing which one fares the best, and then running a \"sanity check\" to see if the other agent treats Masquerade the same way it treats that mask. The pseudocode looked like this:\n\ndef Masquerade(opp_src):  \n for each (i,j,k), look for proofs of the form \"Mask_ijk gets utility _u_ against Opp\"  \n choose (i,j,k) corresponding to the largest such _u_ found  \n look for proof that Opp(Masquerade) = Opp(Mask_ijk)  \n if one is found, then output the same thing as Mask_ijk(Opp)  \n otherwise, output a default action  \n\n(The default should be something safe like a Nash equilibrium strategy, of course.)\n\nIntuitively, when Masquerade plays the Prisoner's Dilemma against FairBot, Masquerade finds that the best utility against FairBot is achieved by some mask that cooperates, and then Masquerade's sanity-check is trying to prove that FairBot(Masquerade) = C as FairBot is trying to prove that Masquerade(FairBot) = C, and the whole Löbian circus goes round again. Furthermore, it's intuitive that when Masquerade plays against another Masquerade, the first one notices the proof of the above, and finds that the best utility against the other Masquerade is achieved by FairBot; thus both pass to the sanity-check stage trying to imitate FairBot, both seek to prove that the other cooperate against themselves, and both find the Löbian proof.\n\nSo what's wrong with this intuitive reasoning?\n\n### Problem: A deductive system can't count on its own consistency!\n\nLet's re-examine the argument that Masquerade cooperates with FairBot. In order to set up the Löbian circle, FairBot needs to be able to prove that Masquerade selects a mask that cooperates with FairBot (like CooperateBot or FairBot). There are nice proofs that each of those masks attains the mutual-cooperation payoff against FairBot, but we also need to be sure that some other mask won't get the very highest (I defect, you cooperate) payoff against FairBot. Now you and I can see that this must be true, because FairBot simply can't be exploited that way. But crucially, _FairBot can't deduce its own inexploitability_ without thereby becoming exploitable (for the same Gödelian reason that a formal system can't prove its own consistency unless it is actually inconsistent)!\n\nNow, the caveats to this are important: if FairBot's deductive process is sufficiently stronger than the deductive process that's trying to exploit it (for example, FairBot might have an oracle that can answer questions about Masquerade's oracle, or FairBot might look for proofs up to length 2^N^ while Masquerade only looks up to length N), then it can prove (by exhaustion if nothing else) that Masquerade will select a cooperative mask after all. But since Masquerade needs to reason about Masquerade at this level, this approach goes nowhere. (At first, I thought that having a weaker oracle for Masquerade's search through masks, and a stronger oracle both for each mask and for Masquerade's sanity-check, would solve this. But that doesn't get off the ground: the agent thus defined attains mutual cooperation with FairBot, but not with itself, because the weaker oracle can't prove that it attains mutual cooperation with FairBot.)\n\nAnother caveat is the following: FairBot may not be able to rule out the provability of some statement we know is false, but (given a large enough deductive capacity) it can prove that a certain result is the first of its kind in a given ordering of proofs. So if our agents act immediately on the first proof they find, then we could make a version of Masquerade work... as long as each search _does_ find a proof, and as long as _that_ fact is provable by the same deduction system. But there's an issue with this: two masks paired against each other won't necessarily have provable outcomes!\n\nLet's consider the following mask agent, which we'll call AntiFairBot: it searches for a proof that its opponent cooperates against it, and it _defects_ if it finds one; if it doesn't find such a proof, then it _cooperates_. This may not be a very optimal agent, but it has one interesting property: if you pit AntiFairBot against FairBot, and the two of them use equivalent oracles, then it takes an oracle stronger than either to deduce what the two of them will do! Thus, Masquerade can't be sure that AntiFairBot won't get the highest payoff against FairBot (which of course it won't) unless it uses a stronger deduction system for the search through masks than FairBot uses for its proof search (which would mean that FairBot won't be able to tell what mask Masquerade picks).\n\nI tried to fix this by iterating over only some of the masks; after all, there's no realistic opponent against whom AntiFairBot is superior to both FairBot and DefectBot. Unfortunately, at this point I realized two things: in order to play successfully against a reasonable range of opponents on the Prisoner's Dilemma, **Masquerade needs to be able to imitate at least both FairBot and DefectBot**; and **FairBot cannot prove that FairBot defects against DefectBot**. (There are variants of FairBot that _can_ do so, e.g. it could search both for proofs of cooperation and proofs of defection and playing symmetrically if it finds one, but this variant is no longer guaranteed to cooperate against itself!)\n\nIf there are any problems with this reasoning, or an obvious fix that I've missed, please bring it to my attention; but otherwise, I've decided that my approach has failed drastically enough that it's time to do what Eliezer calls [\"halt, melt, and catch fire\"](/lw/ue/the_magnitude_of_his_own_folly/). The fact that Löbian cooperation works is enough to keep me optimistic about formalizing this side of decision theory in general, but the ideas I was using seem insufficient to succeed. (Some variant of \"playing chicken with my deductive system\" might be a crucial component.)\n\nMany thanks to all of the excellent people who gave their time and attention to this idea, both on and offline, especially Eliezer, Vladimir Slepnev, Nisan, Paul Christiano, Critch, Alex Altair, Misha Barasz, and Vladimir Nesov. Special kudos to Vladimir Slepnev, whose gut intuition on the problem with this idea was immediate and correct.",
      "plaintextDescription": "Followup to: Decision Theories: A Semi-Formal Analysis, Part III\n\nUPDATE: As it turns out, rumors of Masquerade's demise seem to have been greatly exaggerated. See this post for details and proofs!\n\nI had the chance, over the summer, to discuss the decision theory outlined in my April post with a bunch of relevantly awesome people. The sad part is, there turned out to be a fatal flaw once we tried to formalize it properly. I'm laying it out here, not with much hope that there's a fix, but because sometimes false starts can be productive for others.\n\nSince it's not appropriate to call this decision theory TDT, I'm going to use a name suggested in one of these sessions and call it \"Masquerade\", which might be an intuition pump for how it operates. So let's first define some simple agents called \"masks\", and then define the \"Masquerade\" agent.\n\nSay that our agent has actions a1, ... , an, and the agent it's facing in this round has actions b1, ... , bm. Then for any triple (bi, aj, ak), we can define a simple agent Maskijk which takes in its opponent's source code and outputs an action:\n\ndef Mask_ijk(opp_src):\n look for proof that Opp(Mask_ijk) = bi\n if one is found, then output aj\n otherwise, output ak\n\n\n(This is slightly less general than what I outlined in my post, but it'll do for our purposes. Note that there's no need for aj and ak to be distinct, so constant strategies fall under this umbrella as well.)\n\nA key example of such an agent is what we might call FairBot: on a Prisoner's Dilemma, FairBot tries to prove that the other agent cooperates against FairBot, and if it finds such a proof, then it immediately cooperates. If FairBot fails to find such a proof, then it defects. (An important point is that if FairBot plays against itself and both have sufficiently strong deductive capacities, then a short proof of one's cooperation gives a slightly longer proof of the other's cooperation, and thus in the right circumstances we have mutual cooperation via Löb's Theo",
      "wordCount": 1391
    },
    "tags": [
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "X2qk7paWE9sH7MLZp",
    "title": "Posts I'd Like To Write (Includes Poll)",
    "slug": "posts-i-d-like-to-write-includes-poll",
    "url": null,
    "baseScore": 18,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 15,
    "createdAt": null,
    "postedAt": "2012-05-26T21:25:31.019Z",
    "contents": {
      "markdown": "**Summary:** _There are a bunch of posts I want to write; I'd like your help prioritizing them, and if you feel like writing one of them, that would be awesome too!  \n_\n\nI haven't been writing up as many of my ideas for Less Wrong as I'd like; I have excuses, but so does everyone. So I'm listing out my backlog, both for my own motivation and for feedback/help. At the end, there's a link to a poll on which ones you'd like to see. Comments would also be helpful, and if you're interested in writing up one of the ideas from the third section yourself, say so!\n\n(The idea was inspired by [lukeprog's request](/lw/85d/11_less_wrong_articles_i_probably_will_never_have/) for post-writing help, and I think someone else did this a while ago as well.)\n\nPosts I'm Going To Write (Barring Disaster)\n-------------------------------------------\n\nThese are posts that I currently have unfinished drafts of.\n\n**Decision Theories: A Semi-Formal Analysis, Part IV and Part V:** Part IV concerns [bargaining problems](http://en.wikipedia.org/wiki/Bargaining_problem) and introduces the tactic of playing chicken with the inference process; Part V discusses the benefits of UDT and perhaps wraps up the sequence. Part IV has been delayed by more than a month, partly by real life, and partly because bargaining problems are really difficult and the approach I was trying turned out not to work. I believe I have a fix now, but that's no guarantee; if it turns out to be flawed, then Part IV will mainly consist of \"bargaining problems are _hard_, you guys\".\n\nPosts I Really Want To Write\n----------------------------\n\nThese are posts that I feel I've already put substantial original work into, but I haven't written a draft. If anyone else wants to write on the topic, I'd welcome that, but I'd probably still write up my views on it later (unless the other post covers all the bases that I'd wanted to discuss, most of which aren't obvious from the capsule descriptions below).\n\n**An Error Theory of Qualia:** My [sequence last summer](/lw/5n9/seeing_red_dissolving_marys_room_and_qualia/) didn't turn out as well as I'd hoped, but I still think it's the right approach to a physically reductionist account of qualia (and that mere bullet-biting isn't going to suffice), so I'd like to try again and see if I can find ways to simplify and test my theory. (In essence, I'm proposing that what we experience as qualia are something akin to error messages, caused when we try and consciously introspect on something that introspection can't usefully break down. It's rather like the modern understanding of [déjà vu](http://en.wikipedia.org/wiki/D%C3%A9j%C3%A0_vu).)\n\n**Weak Solutions in Metaethics:** I've been mulling over a certain approach to metaethics, which differs from [Eliezer's sequence](http://wiki.lesswrong.com/wiki/Metaethics_sequence) and [lukeprog's sequence](http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics) (although the conclusions may turn out to be close). In mathematics, there's a concept of a [weak solution](http://en.wikipedia.org/wiki/Weak_solution) to a differential equation: a function that has the most important properties but isn't actually differentiable enough times to \"count\" in the original formulation. Sometimes these weak solutions can lead to \"genuine\" solutions, and other times it turns out that the weak solution is all you really need. The analogy is that there are a bunch of conditions humans want our ethical theories to satisfy (things like consistency, comprehensivity, universality, objectivity, and practical approximability), and that something which demonstrably had all these properties would be a \"strong\" solution. But the failure of moral philosophers to find a strong solution doesn't have to spell doom for metaethics; we can focus instead on the question of what sorts of weak solutions we can establish.\n\nPosts I'd Really Love To See\n----------------------------\n\nAnd then we get to ideas that I'd like to write Less Wrong posts on, but that I haven't really developed beyond the kernels below. If any of these strike your fancy, you have my atheist's blessing to flesh them out. (Let me know in the comments if you want to publicly commit to doing so.)\n\n**Living with Rationality:** Several people in real life criticize Less Wrong-style rationality on the grounds that \"you couldn't really benefit by living your life by Bayesian utility maximization, you have to go with intuition instead\". I think that's a strawman attack, but none of the defenses on Less Wrong seem to answer this directly. What I'd like to see described is how it works to actually improve one's life via rationality (which I've seen in my own life), and how it differs from the [Straw Vulcan](/lw/8ko/communicating_rationality_to_the_public_julia/) stereotype of decisionmaking. (That is, I usually apply conscious deliberation on the level of choosing habits rather than individual acts; I don't take out a calculator when deciding who to sit next to on a bus; I leave room for the kind of uncertainty described as \"my conscious model of the situation is vastly incomplete\", etc.)\n\n**An Explanation of the Born Probabilities in MWI:** This topic might be even better suited to an actual physicist than to a know-it-all mathematician, but I don't see why the [Born probabilities](http://en.wikipedia.org/wiki/Born_rule) should be regarded as [mysterious](/lw/py/the_born_probabilities/) _at all_ within the Many-Worlds interpretation. The universe is naturally defined as a Hilbert space, and the evolution of the wavefunction has a basic L^2 conservation law. If you're going to ask \"how big\" a chunk of the wavefunction is (which is the right way to compute the relative probabilities of being an observer that sees such-and-such), the only sane answer is going to be the L^2 norm (i.e. the Born probabilities).\n\n**Are Mutual Funds To Blame For Stock Bubbles?** My opinion about the incentives behind the financial crisis, in a nutshell: Financial institutions caused the latest crash by speculating in ways that were good for their quarterly returns but involved themselves in way too much risk. The executives were incentivized to act in that short-sighted way because the investors wanted short-term returns and were willing to turn a blind eye to that kind of risk. But that's a crazy preference for most investors (I expect it had seriously negative value), so why weren't investors smarter (i.e. why didn't they flee from any company that wasn't clearly prioritizing longer-term expected value)? Well, there's one large chunk of investors with precisely those incentives: the 20% of the stock market that's composed of mutual funds. I'd like to test this theory and think about realistic ways to apply it to public policy. (It goes without saying that I think Less Wrong readers should, at minimum, invest in index funds rather than mutual funds.)\n\n**Strategies for Trustworthiness with the Singularity:** I want to develop [this comment](/lw/8c3/qa_with_new_executive_director_of_singularity/56re) into an article. Generally speaking, the usual methods of making the [principal-agent problem](http://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) work out aren't available; the possible payoffs are too enormous when we're discussing rapidly accelerating technological progress. I'm wondering if there's any way of setting up a Singularity-affecting organization so that it will be transparent to the organization's backers that the organization is doing precisely what it claims. I'd like to know in general, but there's also an obvious application; I think highly of the idealism of SIAI's people, but trusting people on their signaled idealism in the face of large incentives turns out to backfire in politics pretty regularly, so I'd like a better structure than that if possible.\n\n**On Adding Up To Normality:** People have a strange block about certain concepts, like the existence of a deity or of contracausal free will, where it seems to them that the instant they stopped believing in it, everything else in their life would fall apart or be robbed of meaning, or they'd suddenly incur an obligation that horrifies them (like raw hedonism or total fatalism). That instinct is like being on an airplane, having someone explain to you that [your current understanding of aerodynamic lift is wrong](http://xkcd.com/803/), and then suddenly becoming terrified that the plane will plummet out of the sky now that there's no longer the kind of lift you expected. (That is, it's a fascinating example of the [Mind Projection Fallacy](http://wiki.lesswrong.com/wiki/Mind_projection_fallacy).) So I want a general elucidation of [Egan's Law](http://wiki.lesswrong.com/wiki/Egan%27s_law) to point people to.\n\n**The Subtle Difference Between Meta-Uncertainty and Uncertainty:** If you're discussing a single toss of a coin, then you should treat it the same (for decision purposes) whether you know that it's a coin designed to land heads 3/4 of the time, or whether you know there's a 50% chance it's a fair coin and a 50% chance it's a two-headed coin. Metauncertainty and uncertainty are indistinguishable in that sense. Where they differ is in _how you update on new evidence_, or how you'd make bets about three upcoming flips taken together, etc. This is a worthwhile topic that seems to confuse the hell out of newcomers to Bayesianism.\n\n**(Originally, this was a link to a poll on these post ideas)**\n\nThanks for your feedback!\n\n### UPDATE:\n\nThanks to everyone who gave me feedback; results are in [this comment](/lw/cnl/posts_id_like_to_write_includes_poll/6re6)!",
      "plaintextDescription": "Summary: There are a bunch of posts I want to write; I'd like your help prioritizing them, and if you feel like writing one of them, that would be awesome too!\n\n\nI haven't been writing up as many of my ideas for Less Wrong as I'd like; I have excuses, but so does everyone. So I'm listing out my backlog, both for my own motivation and for feedback/help. At the end, there's a link to a poll on which ones you'd like to see. Comments would also be helpful, and if you're interested in writing up one of the ideas from the third section yourself, say so!\n\n(The idea was inspired by lukeprog's request for post-writing help, and I think someone else did this a while ago as well.)\n\n\nPosts I'm Going To Write (Barring Disaster)\nThese are posts that I currently have unfinished drafts of.\n\nDecision Theories: A Semi-Formal Analysis, Part IV and Part V: Part IV concerns bargaining problems and introduces the tactic of playing chicken with the inference process; Part V discusses the benefits of UDT and perhaps wraps up the sequence. Part IV has been delayed by more than a month, partly by real life, and partly because bargaining problems are really difficult and the approach I was trying turned out not to work. I believe I have a fix now, but that's no guarantee; if it turns out to be flawed, then Part IV will mainly consist of \"bargaining problems are hard, you guys\".\n\n\nPosts I Really Want To Write\nThese are posts that I feel I've already put substantial original work into, but I haven't written a draft. If anyone else wants to write on the topic, I'd welcome that, but I'd probably still write up my views on it later (unless the other post covers all the bases that I'd wanted to discuss, most of which aren't obvious from the capsule descriptions below).\n\nAn Error Theory of Qualia: My sequence last summer didn't turn out as well as I'd hoped, but I still think it's the right approach to a physically reductionist account of qualia (and that mere bullet-biting isn't going to suffice), ",
      "wordCount": 1442
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "JXM9ML4FtCaAitmQC",
    "title": "Timeless physics breaks T-Rex's mind [LINK]",
    "slug": "timeless-physics-breaks-t-rex-s-mind-link",
    "url": null,
    "baseScore": 28,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2012-04-23T19:16:07.064Z",
    "contents": {
      "markdown": "[From Dinosaur Comics](http://www.qwantz.com/index.php?comic=2190), with a nice shout-out to [Eliezer's Timeless Physics post](/lw/qp/timeless_physics/)! (Look in the newspost below the comic.)\n\nI can't wait until Ryan North gets around to [Newcomb's Problem](/lw/nc/newcombs_problem_and_regret_of_rationality/)...",
      "plaintextDescription": "From Dinosaur Comics, with a nice shout-out to Eliezer's Timeless Physics post! (Look in the newspost below the comic.)\n\nI can't wait until Ryan North gets around to Newcomb's Problem...",
      "wordCount": 29
    },
    "tags": [
      {
        "_id": "xjy2ZYACvYQBPJdix",
        "name": "Timeless Physics",
        "slug": "timeless-physics"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]