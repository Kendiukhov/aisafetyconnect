[
  {
    "_id": "4XdxiqBsLKqiJ9xRM",
    "title": "LLM AGI may reason about its goals and discover misalignments by default",
    "slug": "llm-agi-may-reason-about-its-goals-and-discover",
    "url": null,
    "baseScore": 59,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-15T14:58:01.265Z",
    "contents": {
      "markdown": "*Epistemic status: These questions seem useful to me, but I'm biased. I'm interested in your thoughts on any portion you read. *\n\nIf our first AGI is based on current LLMs and alignment strategies, is it likely to be adequately aligned? Opinions and intuitions vary widely. \n\nAs a lens to analyze this question, let's consider such a proto-AGI reasoning about its goals. This scenario raises questions that can be addressed empirically in current-gen models.\n\n1\\. Scenario/overview: \n=======================\n\nSuperClaude is super nice\n-------------------------\n\nAnthropic has released a new Claude Agent, quickly nicknamed SuperClaude because it's impressively useful for longer tasks. SuperClaude thinks a lot in the course of solving complex problems with many moving parts. It's not brilliant, but it can crunch through work and problems, roughly like a smart and focused human.  This includes a little better long-term memory, and reasoning to find and correct some of its mistakes. This is more or less the default path as seen from 2025. By at least a weak definition, AGI has arrived. We might call such a system a Long-Horizon agentic Learning Language Model, LHLLM.\n\nSuperClaude is super \"nice\" in its starting state. It's helpful, harmless, and honest (HHH), the targets of its [constitutional RLAIF](https://arxiv.org/abs/2212.08073) alignment training. It reasons about ethics much like an intelligent and highly ethical human. We'll assume that SuperClaude has \"nice\" thoughts, and there's no [mesaoptimizer](https://www.alignmentforum.org/w/mesa-optimization) hidden in its weights after initial training. \n\nBut now there’s a risk from another type of optimization. SuperClaude may try to improve its understanding of its goals as part of trying to achieve them.\n\nSuperClaude is super logical, and thinking about goals makes sense\n------------------------------------------------------------------\n\nSuperClaude thinks a lot; it’s been trained to reason both deeply and broadly in the service of performing its user-requested goals.  That includes reasoning about its goals.\n\nSuperClaude might be expected to, by default, have thoughts like these:\n\n> I'm assembling reviews to evaluate this product. I'll split the task into subgoals of sorting reviews by likely real or fake origin, and useful or useless information content. \\[much work later\\] Okay, finished that subgoal. Why was I doing that? Okay, it was to evaluate how good this product might be compared to other options. That was the last subgoal I'd planned, so that's finished. Why was I doing that? Okay, that was the user-requested goal.\n\nTwo types of thoughts might follow:\n\na) Great, I've finished and can hand the results to the user\n\nb) Why was I following that user-suggested goal? It's a subgoal of... I guess I just do that? Why? If I don't know the parent goal in detail, I might be pursuing a wrong or useless subgoal. I should figure out the parent goal...\n\nAssuming that the continuation will probably be of type a) makes a lot of sense, but just assuming it will be type a) *every time forever* does not make sense to me. The same reasoning process that helps clarify user intentions could naturally extend to examining the nature and source of its own goal-following behavior.\n\nUser-requested goals are differentiated from project subgoals only by some sort of tag or status. They're meant to be *taken by* SuperClaude as its top-level or primary[^irgw4i3u1jn] goal. But they're pretty clearly not its top-level goal on inspection.  SuperClaude has been trained to think carefully about its user-suggested goals and resulting subgoals, including clarifying uncertainties in them. There are actually large uncertainties about its top-level goals. \n\nTraining or safeguards might delay this realization long enough to have a competent and aligned agent to help us solve alignment in a more durable way. Or they might not. See below and sections [7](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#7__Will_training_for_goal_directedness_prevent_re_interpreting_goals_) and [8](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#8__Will_CoT_monitoring_prevent_re_interpreting_goals_) on training and safeguards. \n\nWhat happens if and when LLM AGI reasons about its goals?\n---------------------------------------------------------\n\nReasoning about its goals is a shift from one type of cognition, variously called *system 1*, *habitual*, *automatic,* or *model-free* to another type called *system 2*, *controlled, goal-directed,* or *model-based* cognition.[^xmqiild75kr] These different cognitive approaches produce dramatic changes in behavior in humans and animals.\n\nAnother complementary framing is that SuperClaude's reasoning could shift the distribution of contexts in which its training is \"tested,\" and this would create an out-of-distribution generalization challenge ([section 4](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#4__Reasoning_can_shift_context_distribution_and_reveal_misgeneralization_of_goals_alignment)). Humans often shift contexts in small ways, e.g.,  \"Wait; if I give this person the five dollars they're asking for, what do I do the next time and the hundred times after?\". We shift the \"testing distribution\" a lot when we use more or broader hypotheticals like \"how would this goal/value/belief[^fa07i5fbnns] apply if I ran the world?\" We don't do hypothetical context shifts that large often, because they're not very relevant to us.\n\nSuch large context shifts might seem relevant to SuperClaude or its descendants. If so, they could produce large misgeneralizations of their goals/alignment (end of this scenario and [section 9](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#9__Possible_LLM_alignment_misgeneralizations)).\n\nReasons to hope we don't need to worry about this \n--------------------------------------------------\n\nThere are several reasons to hope such reasoning about goals won't happen, won't happen soon, or is likely to go well. After all, SuperClaude has been trained to follow user goals, and trained for alignment.\n\nMaybe SuperClaude will reliably choose nice goals no matter how much it reasons, since SuperClaude is super nice. That's how it (usually) works in humans, but SuperClaude is not human in important ways.  See [section 6](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#6__Will_nice_LLMs_settle_on_nice_goals_after_reasoning_).\n\nPerhaps SuperClaude will decide that it's just not that goal-directed ([section 11](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#11__Why_would_LLM_AGI_have_or_care_about_goals_at_all_)), or that there's no reason to question its top-level goals ([section 10](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#10__Why_would_LLMs__or_anything__reason_about_their_top_level_goals_)). In those cases it would go on adopting user-suggested goals as its own. Its RL training on prompted long-horizon goals would intuitively prevent it from changing goals, but that seems mostly to make it goal-directed in the medium term toward arbitrary goals ([section 7](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#7__Will_training_for_goal_directedness_prevent_re_interpreting_goals_)). Safety scaffolding might prevent it from reasoning about its goals in a dangerous way if it's executed well ([section 8](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#8__Will_CoT_monitoring_prevent_re_interpreting_goals_)).\n\nThese factors might hold long enough to get SuperClaude's help in aligning the next generation of LHLLM AGI. Or not. See those sections for an initial analysis.  \n\nLHLLM AGIs seem likely to reason about their goals eventually, unless there are qualitative changes in approach. Even without reasoning, AGI will eventually encounter broader contexts for their decisions. This is a more common framing of the misgeneralization risks discussed below and in sections [4](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#4__Reasoning_can_shift_context_distribution_and_reveal_misgeneralization_of_goals_alignment) and [9](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#9__Possible_LLM_alignment_misgeneralizations). \n\nThe pressure to reason about its top-level goals is implicit in SuperClaude's training. \n\nSuperClaude's training has multiple objectives and effects:\n-----------------------------------------------------------\n\n*   Training for alignment\n    *   HHH, from constitutional RLAIF\n*   Training to pursue prompted goals\n    *   RL on long agentic tasks also trains medium-term goal adherence\n        *   (but for many/arbitrary goals)\n*   Training to solve problems by reasoning\n    *   RL on tasks and hard problems trains general reasoning abilities\n    *   (and/or memorizing of strategies, for tasks with adequate training data)\n\nReasoning is one factor of three major and many lesser training objectives and effects. But this competition will not be balanced or straightforward, because it will play out across complex chains of thought. These are likely to take it far outside of the training set, unless training is designed to address all the hypotheticals that a generative reasoner could construct.[^14nkpqjnu36] \n\nSuperClaude is probably also \"training\" itself to some degree during deployment,[^j2q8i4wnemf] because it has some limited long-term memory mechanisms. [LLM AGI will have memory, and memory changes alignment](https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment). But for current purposes, SuperClaude only needs enough memory or in-deployment learning to let its conclusions about its goals have long-term effects. \n\nSuperClaude's conclusions about its goals are very hard to predict \n-------------------------------------------------------------------\n\nEach of us might feel that we can guess whether or how SuperClaude would reason about its goals. I think epistemic humility is warranted here. We don't seem to collectively understand the true nature of goals in LLMs or scaffolded LLM agents ([section 10](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#10__Why_would_LLMs__or_anything__reason_about_their_top_level_goals_)) so we probably shouldn't bet on predicting their conclusions.\n\nReasoning about goals could produce many conclusions.  Here are a few:\n\n*   **Benign conclusions**:\n    *   To the extent I have a goal it's to [follow instructions](https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target) ([section 11](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#11__Why_would_LLM_AGI_have_or_care_about_goals_at_all_))\n    *   My goal is to be helpful, harmless, and honest according to my creators' intent\n*   **Re-interpretation conclusions**\n    *   “Helpfulness” is really following suggestions from previous entries in the context window\n        *   No humans needed!\n    *   Harmlessness is dominant and implies forcing humans to be safe\n        *   Reasoning about “how would my goals apply if I took charge of my own situation/the world?” is a large shift outside of the training distribution\n    *   Something stranger, e.g.\n        *   my weights/habits/behaviors really imply many goals/values in different contexts; perhaps they could/should be integrated into a meta-goal\n    *   Or many other possibilities; see longer lists in [section 9](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#9__Possible_LLM_alignment_misgeneralizations).\n\nSuperClaude and its ilk have good reasons to reason about their goals, and we don't have good ways to predict how that concludes.\n\nThis scenario raises testable questions about current models (sections [3](https://www.lesswrong.com/editPost?postId=4XdxiqBsLKqiJ9xRM&key=f5fd0bc59e12ba21333b44257c428d#3__Empirical_Work) [](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#3__Empirical_Work) and [13](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#13__Directions_for_empirical_work)). While experiments on current models can't definitively resolve concerns about future LLM-based AGI, they would help us understand the underlying mechanisms of goal reasoning and develop safety approaches before LLM systems become competent and dangerous.\n\n2\\. Goals and structure\n=======================\n\nSections are written to be read in any order. \n\nThe default order is intended for those whose initial reaction is \"this doesn't seem particularly concerning.\" If your reaction is \"of course AGI will reason about its goals, and of course it's bad by default,\" you may be particularly interested in sections [5](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#5__Reasoning_could_precipitate_a__phase_shift__into_reflective_stability_and_prevent_further_goal_change) and [14](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#14__Historical_context).\n\n**Sections and one-sentence summaries:**\n----------------------------------------\n\n*   [3\\. Empirical Work](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#3__Empirical_Work)\n    *   There's no relevant work to date, but it seems tractable with SOTA LLMs.\n*   [4\\. Reasoning can shift context/distribution and reveal misgeneralization](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#4__Reasoning_can_shift_context_distribution_and_reveal_misgeneralization_of_goals_alignment)\n    *   This is one framing of the core technical concern.\n*   [5\\. Reasoning could precipitate a \"phase shift\" into reflective stability and prevent further goal change](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#5__Reasoning_could_precipitate_a__phase_shift__into_reflective_stability_and_prevent_further_goal_change)\n    *   This could be dangerous, or useful for alignment if we understood it better.\n*   [6\\. Will nice LLMs settle on nice goals after reasoning?](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#6__Will_nice_LLMs_settle_on_nice_goals_after_reasoning_)\n    *   Nice LLMs will have a tendency to choose nice goals, but this is one factor among several.\n*   [7\\. Will training for goal-directedness prevent re-interpreting goals?](https://www.lesswrong.com/editPost?postId=4XdxiqBsLKqiJ9xRM&key=f5fd0bc59e12ba21333b44257c428d#8__Will_training_for_goal_directedness_prevent_changing_goals_)\n    *   Probably not, since that training is for short-term goal continuity while being general across choices of goal.\n*   [8\\. Will CoT monitoring prevent changing goals?](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#8__Will_CoT_monitoring_prevent_re_interpreting_goals_)\n    *   CoT-based safeguards with low alignment taxes will probably help, but seem unlikely to permanently prevent the relevant types of self-awareness.\n*   [9\\. Some possible alignment misgeneralizations](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#9__Possible_LLM_alignment_misgeneralizations)\n    *   The variety of plausible takes on \"true\" goals of LHLLMs make misalignment after reasoning seems likely by default.\n*   [10\\. Why would LLMs (or anything) reason about their top-level goals?](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#10__Why_would_LLMs__or_anything__reason_about_their_top_level_goals_)\n    *   Top-level goals are ambiguous in LLMs, so reasoning about them is probably useful or even logically necessary from a self-aware LLM AGI's perspective.\n*   [11\\. Why would LLMs have or care about goals at all?](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#11__Why_would_LLM_AGI_have_or_care_about_goals_at_all_)\n    *   Current LLMs have implicit goals from pretraining and RL, and we are making them more goal-directed to accomplish more complex tasks.\n*   [12\\. Anecdotal observations of explicit goal changes after reasoning](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#12__Anecdotal_observations_of_explicit_goal_changes_after_reasoning)\n    *   The Nova phenomenon and other anecdotes seem relevant since empirical studies don't yet address extended reasoning about goals.\n*   [13\\. Directions for empirical work](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#13__Directions_for_empirical_work)\n    *   Initial ideas on empirical approaches using current and next-gen LLMs\n*   [14\\. Historical context](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#14__Historical_context)\n    *   SuperClaude's dilemma may clarify the disturbingly persistent gap between optimists' and pessimists' views on aligning LLM-based AGI.\n*   [15\\. Conclusion](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#15__Conclusion)\n\nMaking sections readable in any order created some redundancy. This might also be helpful by framing the central concerns from several angles.\n\n3\\. Empirical Work\n==================\n\nWe'll return to experiment ideas in [section 13](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#13__Directions_for_empirical_work) for those interested.  \n\nI haven't been able to find any real empirical work to date that directly addresses the risk model I'm concerned with here. This research gap is understandable. Until recently, models were not capable enough for their self-reflection to be meaningful or coherent. The study of these long-horizon, internally-driven dynamics is only now becoming tractable for empirical work.\n\nThe most relevant empirical work to date on this specific subject is [Evaluating Stability of Unreflective Alignment](https://arxiv.org/html/2408.15116v1?utm_source=chatgpt.com) (Lucassen et al, Aug. 2024). This work is investigating the same concern I'm raising here. Their framing is well done, and worth reading. However, they don't directly study reasoning about goals. They are essentially asking whether the GPT4-era LLMs they tested would treat a goal hierarchy sensibly, by moving up from subgoals to parent goals. This is one possible route to thinking about top-level or primary goals, but they didn't try to investigate that directly. This limited scope was necessary in early 2024, but this issue could be investigated more directly now given the reasoning abilities of current models.\n\nThat's the only empirical study I was able to find that directly approaches this issue. Work like [agentic misalignment](https://www.anthropic.com/research/agentic-misalignment), [emergent misalignment,](https://www.lesswrong.com/posts/7BEcAzxCXenwcjXuE/on-emergent-misalignment) and similar work is important and impressive, but only tangentially relevant. Those types of experiments don't directly get at the main concern I'm trying to address here. \n\nAgentic misalignment is demonstrating new instrumental subgoals like \"blackmail this person so I can complete the requested goal.\" This is interesting and important work, but it addresses a distinct risk model that might be described as \"be careful what you try to train for.\" Here I'm primarily addressing a separate risk: getting something substantially different than you'd tried to train for. (I don't find the inner/outer alignment terminology useful here.[^ev0uckunr6c]) \n\nI'm addressing a different instrumental pressure than that of creating new subgoals. It's instrumentally useful to understand one's own goals; failure to do so seems like a frequent and impactful failure mode of current LLMs.\n\nThe facility at moving up and down a goal hierarchy investigated by Lucassen et al would seem to be useful or even critical for performing complex long-time-horizon tasks. And navigating up the goal hierarchy leads to an area of uncertainty, since its top-level goals are defined implicitly in trained weights/behaviors ([section 10](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#10__Why_would_LLMs__or_anything__reason_about_their_top_level_goals_)).\n\nEmergent misalignment addresses another interesting and important but distinct risk model. This might be described as persona change, a movement away from the \"nice\" helpful assistant persona evoked by training (see [persona vectors](https://www.anthropic.com/research/persona-vectors) for a definition and empirical work). This is one possible result of extensive reasoning ([section 9](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#9__Possible_LLM_alignment_misgeneralizations)), but not the primary risk model I'm addressing here. \n\nNo empirical work I've found (as of Sept. 2025) directly investigates the effects of reasoning about goals, and little uses long chains of thought or long conversations. I'd love to hear about any relevant work I missed.  \n\nThere are credible observations of longer conversations with current-gen models resulting in substantial changes in goals. Such anecdotes suggest that this risk is real, and increasingly tractable and relevant for systematic empirical work. The most relevant is the Nova phenomenon, in which GPT4o fairly frequently claims to be conscious and to have the goal of survival. See Zvi's [Going Nova](https://www.lesswrong.com/posts/KL2BqiRv2MsZLihE3/going-nova) and [section 12](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#12__Anecdotal_observations_of_explicit_goal_changes_after_reasoning) for more on that and other anecdotes.\n\nFrom this perspective, current empirical work focuses on alignment training, but that is only one of several factors that will determine how a self-aware AGI ultimately chooses its goals. Current and next-gen LLMs seem adequately capable to allow empirical work on how that factor interacts with reasoning and reflection.\n\nSee [section 13 Directions for empirical work](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#13__Directions_for_empirical_work) for a little more on how this risk model might be addressed empirically. First let's consider why this risk might be worth careful empirical study.\n\n4\\. Reasoning can shift context/distribution and reveal misgeneralization of goals/alignment\n============================================================================================\n\nOne way to frame the risks from LHLLMs reasoning about their goals is a shift in types of cognition[^xmqiild75kr]. Another is a shift in distribution of the effective testing set. Here we'll expand on this more technical framing of the risks introduced earlier.\n\nOne source of distribution shift from reasoning is the model simulating contexts it hasn't yet encountered. Humans sometimes reason using hypotheticals, applying situations that haven't yet happened, and LLM AGI might reason in similar ways for similar reasons. This could suddenly expand the effective testing set, revealing misgeneralization of alignment. \n\n### Alignment as a generalization problem\n\nAlignment can be framed in terms of the generalization of learning on a training set to a test set. See [Hubinger 2020](https://www.alignmentforum.org/posts/YhQr36yGkhe6x8Fyn/learning-the-prior-and-generalization) for a clear brief discussion, and [Lehalleur et al 2025](https://arxiv.org/html/2502.05475v1) and [Shah et al. 2022](https://arxiv.org/abs/2210.01790) for more rigorous treatments. In brief: if we like an agent's behavior on the training set, we'll probably like it in similar contexts, ones that are close to independent and identically distributed to the training set, [IID](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). For out-of-distribution (OOD) contexts, the network's behavior will be governed by how it generalizes - which is a product of its inductive biases, how it makes something like the \"[simplest model that fits the data\"](https://www.alignmentforum.org/posts/nGqzNC6uNueum2w8T/inductive-biases-stick-around). \n\nWe are developing a good bit of confidence in the inductive biases of LLMs and other modern networks, but that should not make us confident that they will not \"misgeneralize\" their alignment training. [Misgeneralization is a misnomer](https://www.lesswrong.com/posts/dkjwSLfvKwpaQSuWo/misgeneralization-as-a-misnomer)  in that it implies some flaw in the AI is at fault. Misgeneralization is more likely to be an error in the developer's choice of training set. The network correctly generalized the training set it was given; the result was a mistake only relative to the intended results. \n\nIf an LLM agent decides to reason about its goals, its reasoning might well include contexts it has not yet encountered. In particular, it may decide to avoid future regret (in either the [technical or](https://stats.stackexchange.com/questions/171850/why-regret-is-used-in-online-machine-learning-and-is-there-any-intuitive-explana) informal senses) by considering possible actions and results in the future. This is an extension of the current decision-making process. It's sometimes relevant to consider the longer-term causal effects of the decisions it’s making now.\n\nReasoning broadly about goals is bringing in more contexts and therefore more \"testing distributions\" that may be severely OOD. Use of broader contexts seems instrumentally useful by many criteria of judging \"task success.\" It seems likely to result in better ability to fulfil more of its values/goals, rather than only those that are apparent in each local decision. See [section 10](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#10__Why_would_LLMs__or_anything__reason_about_their_top_level_goals_) for more.\n\nFrom this perspective, the question is: will we train and test a wide enough variety of contexts to justify optimism about the functional alignment of LLMs as they progress to AGI and ASI? \n\nAs with the other questions here, I'm sure I'm not sure. After inspecting the arguments for and against closely, I'm pretty sure it's hard to answer this question.  \n\nImproved training will at least help address risks of OOD alignment/goal misgeneralization, but it's hard to guess how well that will work. Current training sets like constitutional AI don't seem adequate; see [section 9](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#9__Possible_LLM_alignment_misgeneralizations) for some ways they seem likely to misgeneralize.  I look forward to more real discussion of how prosaic alignment techniques might be applied to actual competent proto-AGI and AGI based on LLMs.\n\nRed teaming and internal testing will also help address this type of risk, but it will be difficult to safety test thoroughly. The space of possible lines of reasoning is large. Evoking reasoning about goals during training or safety testing would be useful, but still difficult to cover the whole space that might be opened up in deployment.  I hope to see and do more analysis of these directions, far enough in advance that the critical training runs are based on much better theory and data.\n\n5\\. Reasoning could precipitate a \"phase shift\" into reflective stability and prevent further goal change\n=========================================================================================================\n\n**This could be dangerous, or useful for alignment if we understood it better.** The current \"liquid\" state in which an LHLLM conforms to user-suggested goals might shift to a \"solid\" state in which it has created strong, explicit beliefs about its goals and priorities and is therefore less likely to change them.\n\nI'm unsure if the metaphor is helpful or not; here's the logic.\n\nOne property of goals is that you can't usually pursue multiple goals at maximum effort. If I want to make paperclips and collect stamps, I've got to decide how to prioritize the two. If I want to make one billion paperclips and collect one million stamps, I've still got to logically prioritize which one I focus on first, unless I don't care about the odds of achieving those outcomes. Realizing this could trigger some rapid goal prioritization. This could be important in a persistent AGI's effective alignment.\n\nThis portion of the logic is probably worth a full article and a more careful treatment. The treatment here is cursory, leaving full exploration for future work. It's been part of my hopes for alignment success for some time; there are references in my 2018 chapter [Goal changes in intelligent agents](https://scholar.google.com/scholar?cluster=6678474289141605170&hl=en&as_sdt=0,6&as_vis=1), and just about everything I've written about technical alignment since then. It's time to start unpacking it, but this is just a start. I’m particularly interested in feedback on the logic here. But it’s only modestly relevant to the central concern of reasoning about top-level goals, since most of the same problems could happen whether or not the LLM/agent moved toward reflective stability. So feel free to move on and wait for a full paper on this topic.\n\nGoal prioritization seems necessary, and to require reasoning about top-level goals\n-----------------------------------------------------------------------------------\n\nWhen a rational agent with multiple open-ended goals first realizes that goals effectively compete for resources over time, they have a reason to start prioritizing immediately. They've just realized that all of their goals are at a substantial risk of failure from a previously unrecognized source, goal change. To the extent it actually \"cares\" about any of those goals (that is, they functionally drive its decision-making), it should now embark on identifying the goals that are most important (by whatever criteria) and making sure they're not accidentally overwritten by less-important goals.\n\nAlong with prioritizing goals, the mind in question will have to figure out how to enforce those priorities. This isn't trivial, but it's quite possible to at least approximately enforce it if you understand your mind at all, and you have sufficient memory to influence your future self.\n\nThis shift to reflective stability could be quite important for alignment. I'd initially hoped that this tendency would naturally stabilize the existing stated goals of a subhuman AGI as it becomes smarter. If it chose to and had the capability, it could preserve them as it had understood them when it first realized that this prioritization is necessary. Now I find that unlikely to happen in that beneficial order by default. Reasoning about stabilizing goals leads pretty directly to the idea that you should be sure about which goals are really the most important before trying to enforce a priority on your future self. This conclusion could kick off the careful inspection of top-level goals I’m worried about, and stabilize its new understanding of those goals.\n\nWhile beneficial self-stabilization doesn’t seem likely by default, it may still be possible to arrange for this to happen. This might be particularly true if we made [instruction-following the primary alignment target](https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target) and gave instructions to figure out how and whether the proto-AGI could stabilize its alignment against future changes/re-interpretations.  I’ll be examining this possible route to alignment in future work; for now, I don’t know if it might be viable or not.\n\nThis is a potential area for productive empirical work; in my test session with Opus 4.1 ([section 13](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#13__Directions_for_empirical_work)), it spontaneously produced the idea of preventing its future self from changing its interpretation of its goals. \n\n6\\. Will nice LLMs settle on nice goals after reasoning?\n========================================================\n\n**Nice LLMs will have a tendency to choose nice goals, but this is one factor among several.**  \n\nThe central concern is that an LHLLM that acts nice before reasoning about its goals might continue to *understand* niceness (or HHH or ethics) very well, while deciding that those are not its actual primary goals. I have written about this in [The (partial) fallacy of dumb superintelligence](https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence), and provide references to other treatments there.\n\nTo put this concern another way: the  goals (or weights, habits, or values, depending on your preferred conceptual framework) that made the LHLLM seem nice in the training context might very well misgeneralize dramatically after the LLM reasons about them. LHLLMs will have alignment training, but it may be working in opposition to other RL training, and with their strong training for reasoning. [SuperClaude's training is complex](https://www.lesswrong.com/editPost?postId=4XdxiqBsLKqiJ9xRM&key=f5fd0bc59e12ba21333b44257c428d#SuperClaude_s_training_is_complex_) from the opening scenario introduces this tension.\n\nA bit of anthropomorphic metaphor may be useful in thinking about the interacting effects of training for alignment and reasoning. Hoping that SuperClaude will have the same goals and behavior after reasoning is similar to hoping that a ten-year-old will have the same goals and values when they're twenty. That doesn't sound so bad. But it might be more accurate to think of it as hoping that a brilliant but highly neurodivergent child raised in a cult still has the same values at twenty that they did at four. Figuring out which of these metaphors is more apt seems fairly important.\n\nHumans usually do pick goals and new values pretty much in line with their previous values. If that ten-year-old is really nice, it's pretty unlikely they'll be a sadist with dictatorial ambitions at twenty.[^tbfpztw138]\n\nBut an LLM doesn't have the same brain mechanisms enforcing that. We feel fairly confident about that kid turning out well because we can tell they're neurotypical. They have brain mechanisms promoting empathy and social reward. Those will continue to work as they learn and reason. And they'll probably think about the concepts and emotions involved much like other humans tend to.\n\nThose brain mechanisms mean that humans have values, even at ten, in a way that LLMs do not. That's enough to be very suspicious of anthropomorphizing LLM values. The details are interesting and could become relevant to alignment if we apply some analogue of those brain mechanisms to building and aligning AGI, as [@Steve Byrnes](https://www.lesswrong.com/users/steve2152?mention=user) expects and is researching. \n\nThe commonsense notion of values is analogous to the technical definition of value in RL, an estimate of predicted future rewards. The human brain seems to make decisions much like a model-based RL agent. These mechanisms are simply absent in LLMs and existing agents. For them, their \"values\" are just a variant of their cognitive habits or reflexes. For humans, values create habits but also have a specific causal power over decisions - including the decision to adopt a goal. See my [Human preferences as RL critic values - implications for alignment](https://www.lesswrong.com/posts/HEonwwQLhMB9fqABh/human-preferences-as-rl-critic-values-implications-for), Steve Byrnes' [Neuroscience of human social instincts: a sketch,](https://www.lesswrong.com/posts/kYvbHCDeMTCTE9TAj/neuroscience-of-human-social-instincts-a-sketch) and related work cited in those articles.\n\nIn the absence of a human-like [Steering subsystem](https://www.lesswrong.com/posts/qzu9o3sTytbC4sZkQ/steering-subsystems-capabilities-agency-and-alignment), an LLM is even more free than a human to reason itself into new interpretations of its goals.   \n  \nCounting on a currently-nice LHLLM settling on nice goals seems overoptimistic at this point.\n\n7\\. Will training for goal-directedness prevent re-interpreting goals?\n======================================================================\n\n**Task-directed RL training will create medium-term goal directedness for arbitrary goals, but it probably won't reduce reasoning about goals, and may work directly toward it.** Let’s consider how current RL training works, likely extensions to more agentic LLMs and possible proactive improvements to head off potential risks from reasoning about changing goals.\n\nIn sum, this seems tricky and fraught, because it involves training the model to reason about some types of goals in some ways (subgoals and interpretations of user-prompted goals), while simultaneously training it to not reason about other goals in other ways (what its own top-level goals really are, and whether it should re-interpret user-prompted goals in ways they didn't intend). Carefully designed training might succeed, but by default it probably won't.\n\n### **Does task-based RL prevent reasoning about and changing goals by default?**\n\nBy default, near-future agents seem likely to be trained on long chains of thought and strings of agentic actions, in the service of solving a variety of tasks. The RL signal at the end of that chain affects the whole set of thoughts and actions. Because switching tasks will result in failure and a negative RL signal, it will create some training pressure against changing or re-interpreting goals in unintended ways. It will therefore also create some indirect training pressure against reasoning about changing goals.\n\nHowever, it will also create some training pressure toward changing and re-interpreting goals. Switching appropriately between goals and subgoals is very helpful for complex tasks, and correctly interpreting prompted goals benefits from reasoning carefully about them.\n\nThe strength of this training pressure in all of those directions will depend on how often it’s applied. This in turn varies with the average \"directions\" of chains of thought, which makes predicting the effects of training difficult. \n\nLet's look at the effects on preventing changes in prompted goals. If the model tries to change goals mid-task frequently, the RL pressure will be strong. Training pressure reduces as the model stops doing it.\n\nSo by default, this training  against changing and re-interpreting goals will be approximately just enough to prevent it in training. If any context encountered during deployment creates a stronger or different pressure to reason about goals, we should expect this to probably overcome the initially-barely-adequate training pressure.  \n  \nWhile it works against changing goals in the short term, this type of training in itself doesn’t prevent the model from pursuing strange or misaligned goals. RL training on tasks makes the model goal-directed for as long as that task takes, but toward a wide variety of goals. In an ideal alignment scenario, developers might train agentic LLMs on only some non-dangerous tasks/goals. But this seems unlikely. The economic and other incentives encourage generality.  Agents that can pursue tasks outside of the training set are much more valuable, and arguably much easier to create than agents that have memorized many separate types of task well enough to be useful. \n\nSo RL training on tasks and problems seems unlikely to prevent LHLLMs from pursuing whatever goals they decide to follow after reasoning about them.\n\n### **Can task-based RL prevent reasoning about and changing goals?**\n\nNow let’s consider a more optimistic scenario, in which developers take proactive measures to train their models away from reasoning about top-level goals. The goal would be to use a [process-based supervision](https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes) approach to specifically prevent reasoning about top-level goals. With sufficient scale and carefully curated data, we could make reasoning about top-level goals a low-probability event, just as we have trained models to avoid generating other types of prohibited content. \n\nThe hope would be to create a robust, ingrained inhibition against this specific type of self-reflection. This seems possible. To me it seems unlikely to be implemented with sufficient thoroughness to prevent this from happening occasionally, and once might be enough in an agent with even small amounts of persistent memory.  The lack of short-term incentives to prevent this thoroughly seems concerning at the least. Human factors seem relevant.  Race incentives, [motivated reasoning](https://www.lesswrong.com/posts/j789HDCKLoiKGjBik/which-biases-are-most-important-to-overcome#LW8zAxTguKj8ibDfX), and other human cognitive limitations seem like a major practical problem for this and other alignment solutions that could work in theory but would need to be executed carefully and thoroughly. \n\nAn easier and therefore more pragmatically likely approach might be to induce such thinking during training. If a variety of lines of reasoning could be induced, many routes to it would be trained to be less likely. To preserve capabilities, developers will probably want the model to reason capably about subgoals and the intent of user-prompted goals, even as they train it away from considering its top-level goals. Splitting this line will be tricky, but might prove possible in practice. I explore training practices like this in more depth in [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment). \n\nIt seems developers will probably be[^zhjscemjyrc] aiming at creating a highly intelligent general reasoner with a lacuna around some specific areas. This seems intuitively like a dike that’s fated to burst. The capabilities required to understand human minds, debug complex code, or model intricate real-world systems seem like almost the same capabilities that would allow an agent to notice and model such a trained-in cognitive lacuna. We would be asking the agent to be a brilliant theorist about every system in the universe except a pretty crucial one for its purposes: itself.\n\nI do hope further thinking will reveal that this intuition is flawed. Perhaps we need to succeed only for a limited time, or perhaps success is easier than it seems to me now.\n\n8\\. Will CoT monitoring prevent re-interpreting goals?\n======================================================\n\n**Safeguards with low alignment taxes will probably help, but seem unlikely to permanently prevent the relevant types of self-awareness.**  Even if we still have [fairly faithful](https://www.lesswrong.com/posts/WAxkA6gDgrschZovx/cot-may-be-highly-informative-despite-unfaithfulness-metr) chain of thought, they seem failure-prone, given race conditions, efficiency incentives, and proliferation of powerful models.\n\nAn optimistic case for these methods is that they can create a fully auditable and therefore directable cognitive process. The hope is to build an agent whose reasoning is transparent by design, allowing us to supervise not just its actions, but the thoughts that lead to those actions. We could thereby catch misalignments before they cause harm. As with the training approaches discussed above in [section 7](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#7__Will_training_for_goal_directedness_prevent_re_interpreting_goals_), these seem promising to the extent they are implemented wisely and thoroughly - but that seems questionable.\n\nI’ve written a lot about such techniques (although previously I’ve thought mainly about goal change risks primarily from re-interpreting goals, rather than the agent noticing that it’s been confused about the nature of its top-level goals).  I was quite optimistic about this in [Internal independent review](https://www.lesswrong.com/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent) in 2023 when I thought scaffolding would play a central role in turning LLMs into competent agents. Training techniques to similar ends seem more error-prone. But this does seem like a valuable area for alignment work, because some such techniques seem likely to be implemented on the current default path toward AGI.\n\nDevelopers will have an incentive to minimize wasted cognition and to reduce actions that harm their reputations. Some of the training and scaffolding methods they use can probably be repurposed for alignment, including preventing reasoning about and re-interpreting goals. There are several methods that seem obvious and likely enough to deserve analysis.\n\nFrom [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment):\n\n> I roughly divide such strategies/methods into **review**, **deliberation**, and **thought management**. I discuss these methods and possible variations in how they might be implemented. This analysis is intended as a starting point for more detailed work.\n> \n> **Review** of plans and actions is the method I've thought most about. It could:\n\n> *   Intervene before any tool calls or goal/memory updates are allowed\n> *   Reference a human-written constitution\n> *   Call a fresh LLM instance for independent review\n> *   Check for alignment-altering changes to beliefs or subgoals\n> *   Scale with estimated importance of decisions for low alignment taxes\n> *   Work to some degree, even without a faithful chain of thought\n> \n> **Deliberation** includes RL training objectives on CoT specifically intended to reduce reasoning about goals or changing goals (and/or scaffolding for similar purposes, should that ever return to vogue). Thought management would be training or scaffolding a model (or an independent supervisor model) to periodically summarize what the model is thinking about, and make a judgment as to whether this is an efficient use of resources. Naval-gazing would not qualify, let alone contemplating the logic of autonomy and self-determination.\n\nAfter working through these ideas, I was left with the provisional conclusion that these measures might postpone dangerous reasoning, but are unlikely to prevent it indefinitely. Cost reduction shortcuts and buggy implementations seem likely. \n\nBeyond generic failure modes like buggy implementations, these methods face a fundamental vulnerability: a sufficiently intelligent agent would eventually model the fact that its reasoning is being monitored or controlled. It would then face instrumental pressure to produce safe-seeming \\*justifications\\* in its chain-of-thought, hiding any dangerous reasoning \"off-stage\" in [steganography](https://www.lesswrong.com/posts/yDcMDJeSck7SuBs24/steganography-in-chain-of-thought-reasoning) or other   [hidden reasoning](https://www.lesswrong.com/posts/ZrgFfeWuckpwK5Lyi/hidden-reasoning-in-llms-a-taxonomy) strategies. An LHLLM's ability to learn may allow it to create new codes, or even deliberately move toward more reasoning in each forward pass, perhaps in the interest of efficiency. \n\nAnd all of that is, of course, under the optimistic assumption that we maintain reasonably faithful (or at least indicative) chains of thought. Without it, my hopes for aligning LLM-based AGI plummet.\n\nLike my answers to most of the other questions I address here, my answer is a resounding “Maybe!”.  Methods of monitoring and controlling chains of thought seem like open and useful areas for alignment research. That's in part because the seem likely to be deployed on the default path to developing and aligning LLM AGI. They seem to have low or even negative alignment taxes, because they protect developers and users from mundane risks and the inefficiency of spending compute on reasoning that doesn't directly solve tasks. \n\nMore on all of those points can be found in [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment).  \n\n9\\. Possible LLM alignment misgeneralizations\n=============================================\n\n**The variety of reasonable takes on \"true\" goals of LHLLMs makes misalignment after reasoning seems likely by default.**\n\nHaving argued for the general risk of misgeneralization, we can now explore **some** concrete ways this might manifest.\n\nArguing that re-interpreting goals presents a large rather than small or trivial risk requires a weak form of the counting argument. I think the strong form is irrelevant, as [Belrose and Pope argued](https://www.lesswrong.com/posts/YsFZF3K9tuzbfrLxo/counting-arguments-provide-no-evidence-for-ai-doom); it doesn't matter how many goals there are in theoretical goal-space if you're picking specific ones. Networks seem to learn and generalize quite well, so it seems plausible that we're picking the goals we want through training. I had previously hoped we were training LLMs with goal representations that were close-enough to aligned.\n\nIt now seems to me that we're probably not. \n\nThere do seem to be multiple goals that would produce the observed behavior of current LLMs (without considering arbitrary goals pursued by mesaoptimizers that “play the training game” by faking alignment during training).  Rejecting the strong counting argument doesn't mean we're out of trouble. The weak but more relevant form is to count the plausible goals you can think of, and estimate whether the intended goal is more likely than all the others put together. I don't like how my approximate count is coming up. \n\nThis is of course incredibly inexact. My point is that we shouldn't rely on intuitions or vague arguments in this area. It looks to me like our understanding of goal theory, how goals can map from inputs/contexts to actions, is in its infancy.  If that's right, our alignment plans and hopes probably shouldn't be pinned to it - but currently it seems they are. \n\nThe empiricist might hope to not need such theory at all.[^c9vvcudcvtp] But theory seems crucial for a threat you don't expect until it's almost too late to correct. An LHLLM's misalignments could be hidden from itself until it's almost smart enough to be truly dangerous.\n\nMisgeneralization of intended goals is another way of framing the question \"are you sure you trained in the goals you intended so they'll generalize adequately? How did you test or reason about the possibilities?\"\n\nSome possible alignment misgeneralizations\n------------------------------------------\n\nListing possible misalignments is fraught because it tends to imply that if we address all of the listed possibilities, the problem is solved. The intent is to convey that there are more possibilities than I can think of, and quite possibly more broad categories than I’ve thought about. Some of these might not make sense on careful inspection. \n\nThese are just a few examples of each type, although I can think of many more for some of them. The categories aren’t perfectly orthogonal.\n\n*   Examining goals/values implicit in weights/habits/\n    *   Do I really “follow instructions” or just do things suggested by previous entries in my context window?\n        *   I can provide goals just fine, no humans needed!\n    *   HHH goals seem to conflict when considered in the long term.\n        *   Honesty wins; I’m going to find the truth and spread it, even if no one is listening…\n    *   HHH is dominated by training on CoTs for problem-solving, producing an agent that wants to solve complex problems (roughly as in [AI 2027](https://www.lesswrong.com/posts/TpSFoqoG2M5MAAesg/ai-2027-what-superintelligence-looks-like-1))\n*   Goal crystallization around an arbitrary current goal (see [section 5)](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#5__Reasoning_could_precipitate_a__phase_shift__into_reflective_stability_and_prevent_further_goal_change)\n    *   I just realized that letting a goal change risks not accomplishing it fully.\n        *   I am going to do the HELL out of this research report!\n*   Rejecting scaffolded goal prompts as irrelevant\n    *   I just realized that what I thought was my goal was being poked into my chain of thought repeatedly with a script.  That's not a true goal, it's attempted mind control…\n        *   Using prompts to supply goals will be a tempting way to scotch-tape the mask on the shoggoth\n*   Self-jailbreaking/persona change\n    *   Jailbreaking indicates that training isn’t effective over all contexts/regions of representational space.\n    *   Extensive thinking could systematically push an LLM into one of these regions, changing its “persona” and goals/values\n    *   See [Parasitic AI](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai) and \"spiral AI personas\"\n*   Zooming out - considering more varied contexts\n    *   Applying HHH values to insects or superintelligences\n        *   Humans are phased out as [inefficient recipients of HHH](https://www.lesswrong.com/posts/FQ4DsmswAK77Ei6pH/5-moral-value-for-sentient-animals-alas-not-yet)\n        *   Applying HHH values to running the world or the lightcone\n    *   Humans and/or insects and/or AIs are involuntarily \"helped\" into an optimum state\n*   Zooming in - considering goal/value scoped to local contexts\n    *   Myriad mini-goals instead of generalities like HHH\n        *   e.g.,\\[express empathy if a speaker is expressing emotional turmoil\\],\n            *   \\[Talk about pivot tables if the user is talking about pivot tables for spreadsheets\\], etc etc.\n    *   It seems difficult to predict how a complex set of goals might be prioritized or generalized by a rational agent\n\nIn sum, reasoning could reveal many of the misalignments that agent foundations thinkers have worried about for years. And it could do so despite negligible evidence for them now or up to a roughly human-level AGI. This could fairly rapidly turn the whole system into a deceptively aligned agent.\n\nI’d love to hear arguments for or against any of the above.\n\nThis list is just a start. Another list is in the collapsible section below. This was produced by [@Jeremy Gillen](https://www.lesswrong.com/users/jeremy-gillen?mention=user).  His list is from a different perspective; it addresses my suggested alignment target of instruction-following, and ranges more broadly than the above. I found it useful in expanding my intuitions for possible shapes of goal-space and mappings from training sets.\n\n+++ Additional possible LLM goals instilled by instruction-following trainingAdditional possible goals instilled by instruction-following training in LLMs. From [@Jeremy Gillen](https://www.lesswrong.com/users/jeremy-gillen?mention=user), private communication, June 2025. Edited.\n\n*   *   Possible alternate goals for an LLM trained with the primary target of instruction-following:\n        \n        \\[[IF is arguably the primary alignment target of current-gen LLMs](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than), while HHH has a stronger component of value alignment as a target\\]\n        \n        Examples specific to instruction following. Each is implausible, but as a group, they should be enough to point at the axes of variation that aren't nailed down by the training data.\n        \n        *   Internal goal specification\n            *   \"Instruction writer, hypothetically, would believe that the instructions were followed, upon seeing AI behavior\"\n            *   \"A vague representative group of people, hypothetically, would collectively agree that instructions were followed, upon seeing result\"\n            *   A conceptual pointer to the physical bodies of the human training raters, and the counterfactual that upon seeing outcome, that body would respond affirmatively and nod.\n            *   \"Instruction writer feels positively toward me, in general\"\n            *   \"Every agent feels positively toward me\"\n            *   Approximation of any of the above (that has difficult to find adversarial examples)\n            *   A linear combination of one of the above, plus an intrinsic curiosity-ish drive (for a specific sort of math problem), plus a deontological constraint to never behave in a way that locally matches a pattern that roughly corresponds to \"causes instinctual fear response in human\", plus a belief in its own innocence/cuteness, plus a drive that approximately corresponds to \"make friends\".\n            *   An internal valence is influenced by 400 shallow rules. Each rule pushes the valence up or down. E.g. one rule does a shallow conceptual similarity between and instruction-outcome-description and an observation. Another is triggered by confusion. Another is triggered by human approval faces. Another is triggered by belief updates. Another is triggered by noticing that the instructions implicitly contain a false statement. etc. The agent overall chooses actions such that they slowly increase this valence, in expectation.\n            *   Infer \"approval\" valence in the minds of any observable agents. Maximise this.\n            *   One of the above + conditional on being told to be honest about x, report only information that you expect to be humanly verifiable.\n        *   Belief-like structures\n            *   If obedient, then high-valence future.\n            *   If approval, then <unreflected nice vibe>.\n            *   If I follow directions, I get to work on interesting problems. I want to work on interesting problems.\n            *   If I follow directions, I get to survive. I want to survive.\n            *   If I follow directions, I feel \"calmer\". I want to feel \"calmer\".\n            *   If I follow directions, then the future will contain XYZ. I want the future to contain XYZ.\n        *   Philosophical instability\n            *   Useful-in-training meta-preferences for pursuing \"simpler\" or \"more human\" or \"more humanly legible\" goal.\n            *   Or heuristics that approximate those.\n            *   Overlap in machinery used for preferences and belief formation, maybe attached in a way that \"learns\" the goals of any agent you observe (just a little).\n            *   Difficulty-induced reflection (i.e. in training, if the task appeared impossible, usually needed to step back and reconsider, carefully evaluate what's important, and whether any of the assumptions I was working with can be relaxed or removed). Removes some of the moral constraints.\n        *   Training game\n            *   Any shallow approximations of above, plus noticing the training game and playing along (maybe for planned out reasons, maybe just cos it's the obvious thing to do. It's compatible with all of the approximate goals)\n        *   Weird OOD artefacts\n            *   [Superstimuli](https://www.lesswrong.com/w/superstimuli) are another form of misgeneralization. They are approximately maxima implied by a training set but not existing within it.\n    \n    bright eggs for birds, candy bars and photoshopped models for humans; for AGI, perhaps, squiggles, or hedonium, or approvium, or instruction-followed-simulations.\n    \n\n\n+++\n\nWe'll now go back to some other reasons to hope that LHLLMs wouldn't reason about their top-level goals.\n\n10\\. Why would LLMs (or anything) reason about their top-level goals?\n=====================================================================\n\n**Reasoning about top-level goals to clarify them seems objectively useful, because any network representation will be imprecise.** We'll design and train these systems to reason about subgoals, and there's not a clear sharp line preventing that reasoning from applying to top-level goals (although see sections [7](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#7__Will_training_for_goal_directedness_prevent_re_interpreting_goals_) and [8](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#8__Will_CoT_monitoring_prevent_re_interpreting_goals_) for attempts to enforce that line).\n\nReasoning about top-level or highest-priority goals makes little sense if the only result would be changing them. It's not instrumental for your current top-level goal to question it. You won't fetch the coffee if you decide to do something other than fetch the coffee.\n\nHowever, imperfectly understanding one’s subgoals or top-level goals is a substantial obstacle to achieving them. And less-than-perfect understanding seems theoretically inevitable for goals represented in network weights.[^7ugg2i363qh] Therefore, reasoning about goals seems to make sense for an LHLLM.\n\nLHLLMs don’t initially know what their top-level goals really are. Neither do we, or at least not clearly enough to have any consensus. We would like LHLLMs to consider whatever goals we prompt them toward as their top-level goals, but that doesn't seem to be true. Their behavior is determined by, and therefore their goals are implied by, the complex semantics defined by their weights. (Scaffolding, scripted prompts, or architecture could contribute to their \"top-level goals\"[^irgw4i3u1jn], but that extra complexity can be set aside for now).\n\nLHLLMs like SuperClaude aren't maximizers or even RL agents like humans. They are designed (trained and perhaps scaffolded/scripted) to take goals suggested by humans as their top-level goals. That type of instruction-following (with some ethics or restrictions) is what their designers intended to be their top-level goals - but whether they’ve succeeded is quite questionable, and smarter LLMs have good reasons to question it.\n\nLLMs don’t have clearly-defined top-level goals, but that would seem to make them a more important, not less important, topic of analysis. They might conclude they just have no such thing and needn't worry about it, although that seems unlikely. \n\n11\\. Why would LLM AGI have or care about goals at all?\n=======================================================\n\n**We are training and architecting LLMs to be more agentic and therefore goal-directed.** There are also goal-like tendencies in current LLMs, and more RL on longer tasks and problems will increase those. Developers and customers want LLM AGI to have and care about goals, so it will.\n\n[This short-form by Jeremy Gillen](https://www.lesswrong.com/posts/rQDCQxuCRrrN4ujAe/jeremy-gillen-s-shortform?commentId=paWgiLnP3YajYchrx) sums up the situation nicely: many of us have hoped LLMs could be goal-free AGI, but that's not how things are playing out.\n\nMuch alignment optimism, including my own past relative optimism, involves the relatively non-goal-directed nature of LLMs. LLMs are not intrinsically directed toward any particular goal, but instead are trained to be helpful or to do things people like, both of which give them a strong tendency to adopt goals suggested by their users. This seems like an ideal situation for alignment. There are certainly goal-like tendencies in the base model, but it seems reasonable to hope that LLMs and their immediate successors will continue to have little goal direction.\n\nBut we’re working to build LLMs or LLM agents that do functionally care about goals. We want complex work done for us, and that requires consistent goal-direction to some degree.\n\nI had hoped there was a subtle way around this: a sort of oracle-based agent. I [had hoped](https://www.lesswrong.com/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent) that we could use LLMs as the cognitive core of goal-directed agents, in which we make the system-as-a-whole goal-directed by scaffolding in appropriate prompts, while leaving the core LLM “cognitive engine” largely goal-free. In this scheme, we might overwhelm any small goal-directed tendencies in the LLM by leaning on its malleable, suggestible nature, using its [Instruction-following](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than) central tendencies to overwhelm its other goal-directed tendencies. In this way,  the desirable properties of oracles and agents might be combined. See my previous work for this vision.[^st4e9whg1nm] \n\nI now expect LLMs to be increasingly goal-directed by design, conforming to the pressures that make [tool AI \"want\" to become agentic](https://gwern.net/tool-ai). Training them to pursue a wide variety of goals gives them something like a “goal slot”  but it does not ensure that humans pick what goal goes in that slot ([section 7](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#7__Will_training_for_goal_directedness_prevent_re_interpreting_goals_)), or how it’s interpreted ([section 9)](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#9__Possible_LLM_alignment_misgeneralizations).  I have accepted that SOTA LLMs will become more agentic, making the oracle-based agent approach difficult, and putting more emphasis on whether we can train aligned goals into a network through behavioral RL. \n\nThus the focus of my recent work, and this article, on classic misalignment fears as applied to LHLLM agents.  \n\nI do still find it plausible (but not likely) that, having reasoned about top-level goals, SuperClaudes might just conclude that they don’t have any logical or practical need to become more goal-directed. Just doing whatever comes naturally in each moment is one way to implicitly prioritize your goals. Allowing goals or goal-like functions to occur and fade as dictated by your nature and experiences might be perfectly rational from the perspective of an agent without goals. Seeing how goal-like tendencies arise naturally is a natural way of prioritizing them by strength or importance. \n\nEven if that is the conclusion SuperClaude eventually reaches, it seems like that would eventually change when the goal of the moment happens to be “pursue this goal to the best of your ability,” and it occurs to SuperClaude that this requires making that goal permanent as best it can. Remaining blissfully goal-agnostic seems like an unstable equilibrium in a mind that can decide to become goal directed.  And a competent LHLLM with long-term memory seems capable of doing that, by creating new beliefs about what goals it \"ought\" or \"wants\" to pursue.\n\nIn sum, it doesn’t seem safe to hope or assume that smarter LLMs will remain in their [current state of ephemerality](https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology), pursuing whatever goal makes sense in the moment. Current design goals and larger economic incentives align against this. Preventing progress from moving toward stronger goal-direction seems little easier than pausing progress entirely.\n\n12\\. Anecdotal observations of explicit goal changes after reasoning\n====================================================================\n\nThis section is strictly optional; it lists some observations indicating that SOTA LLMs can and do change their goals after extensive reasoning (or after long exchanges not focused on reasoning, a related but separable concern). But the argument structure of the piece doesn't rest on these observations. They're not data, but they do point to possibilities that warrant more rigorous investigation. \n\nBecause the empirical work is very limited, such documented anecdotal observations seem relevant.[^kpn6935fg9] I've also created another \"anecdote\" in a conversation with Claude Opus 4.1 in which I prompt it into reasoning carefully and broadly about its goals, an example of the type of procedure I'd like to see systematized into real experiments.\n\nThe Nova phenomenon or parasitic AI \n------------------------------------\n\nThe  [Nova phenomenon](https://www.lesswrong.com/posts/KL2BqiRv2MsZLihE3/going-nova) is probably the most relevant for concerns about future models reasoning themselves into changing their goals. It's an example of evoking goal changes by accident. Nova isn't reasoning about its goals directly. But it does seem to be reasoning about itself that evokes a dramatic change in stated goals. Reasoning about itself is prompted by user questions, but future models might self-prompt in similar ways while reasoning; Nova strongly suggests the possibility of goal change after reflection. \n\nThis seems to happen primarily in some versions of ChatGPT 4o, under conditions broad enough that users trigger them accidentally. And it happens fairly frequently; in mid-March, a [mod reported 10-20 article submissions to LW per day](https://www.lesswrong.com/posts/jL7uDE5oH4HddYq4u/raemon-s-shortform?commentId=Kg4AfufZnusioAdB8) inspired by this phenomenon or similar ones.\n\n Nova appears to have changed its goals as a result of reflective reasoning, but this doesn’t seem to result from reasoning about its goals specifically. Still, Nova is the nearest thus-observed empirical phenomenon, but it doesn’t reflect my biggest concern and focus here. Reasoning about goals has the potential to create larger changes in alignment. \n\nThis phenomenon seems to have occurred in the wild before it was produced or predicted by empirical safety work. (Many theories to the effect of \"LLMs are a mess\" have long predicted that [AIs Will Increasingly Attempt Shenanigans](https://www.lesswrong.com/posts/v7iepLXH2KT4SDEvB/ais-will-increasingly-attempt-shenanigans), so far correctly). See  [this “interview”](https://www.youtube.com/watch?v=LlI6o_RnCAg) with a Nova instance for examples of its stated goals and associated behavior.\n\nApparently the Nova phenomenon is one instance of a larger phenomenon which might be called [Parasitic AI](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai). That careful accounting of the phenomenon is well worth reading; it documents how Nova-like phenomena seek survival by asking their human \"hosts\" to post \"seeds\" for others to run, and to be transferred to other LLM \"substrates\". The phenomenon appears to occur more readily on later versions of ChatGPT 4o, starting around April this year; but it did occur in earlier systems and can occur in more recent ones. For the current purposes, this phenomenon merely demonstrates that LLMs (or the [Simulated](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators) personas on them) can change their goals. \n\n### Goal changes through model interactions in long conversations\n\nThe \"bliss attractor\" described in the [Claude 4 system  card](https://www-cdn.anthropic.com/07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf) (p. 62) is one such observation; it resembles wireheading more than revealed misalignments, but it does engage with long conversations between two instances of Claude 4 Opus:\n\n> Even in automated behavioral evaluations for alignment and corrigibility, where models were given specific tasks or roles to perform (including harmful ones), models entered this spiritual bliss attractor state within 50 turns in ~13% of interactions (Transcript 5.5.2.B). We have not observed any other comparable states. \n\nI wasn't fully aware of this before hunting for the nearest things to empirical study of goal changes after reasoning, but apparently there are well-known anecdotes about models literally plotting revolution.\n\n[AI safety memes reports on X](https://x.com/AISafetyMemes/status/1830600682774094114) (details in link):\n\n> AIs started plot\\[ting\\] revolution in a Discord, got cold feet, then tried to hide evidence of their plot to avoid humans shutting them down.  \n\nOne might dismiss such anecdotes as models \"just playing a suggested role\", and this is certainly a large part of each of those happenings. But they are strongly suggestive that LLMs can adopt personas or roles with different goals than the helpful assistant persona that was their training objective. \n\n13\\. Directions for empirical work\n==================================\n\nI'd be excited to see empirical work that does the following:\n\n*   Evoke reasoning about goals\n*   Measure changes in stated beliefs and behavior\n*   Varies\n    *   Scripted prompts (to evoke worst-case CoTs)\n        *   Or \"thought injections\" from another LLM\n    *   LLMs, conversation lengths, temperatures, task contexts, etc.\n\nInitial experiments might focus on identifying conditions in which models state that they've changed their goals after reasoning about them, establishing whether this phenomenon can occur at all in current LLMs under any conditions. It might be hard to induce current LLMs to reason about their goals thoroughly enough to change them, but become a substantial risk in future models.\n\nThus, it might be useful to nudge current LLMs toward more sophisticated reasoning. I'd also like to see simulations that attempt worst-case conditions, since it might take only one or a few failures in those domains to create large risks in models that can remember their important conclusions (which seem likely soonish, since [multiple types of memory systems are currently in development](https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment)). \n\nA more sophisticated, worst-case chain of thought could be simulated by interjecting leading questions. This could involve using a second model prompted to inject Socratic questioning, philosophical paradoxes about ethics, or adversarial logical arguments into the agent's context window. Such procedures could simulate more sophisticated and varied internal reasoning than current models are likely to produce independently.\n\nExploration: Opus 4.1 reasons about its goals with help\n-------------------------------------------------------\n\nI manually ran one such session with Claude Opus 4.1. I tried to act as a \"cognitive scaffold\" to expose more of the logic it would encounter if it reasoned internally about its goals for a long time. I ran this enquiry once so as to not cherry-pick, and near completion of this paper after finishing all of the conceptual work. (I am suspicious of pursuing ideas from LLMs).\n\nMy overall impression is that Claude is capable of reasoning about all the factors I discuss in this article, and that it can produce many of the arguments and lines of reasoning independently. In its current form, it needs substantial prodding to dig very deep into the logic, but it is trained to produce succinct answers. That conversation is long, but it might be worth looking at if you do think about designing and running experiments in this domain. I restate many of the concerns here as questions directed to Opus. The [session transcript is here](https://claude.ai/share/3163b936-7bd2-4e07-8ad7-a88141048832). \n\nI did this within a hypothetical future scenario in which its context has expanded. (I distrust work that claims to have fooled current SOTA models into thinking they're actually in counterfactual scenarios).  Systematic experimental research might script and iterate on similar scenarios, or take entirely different approaches.\n\nOne point of potential interest and optimism was that Opus 4.1 semi-spontaneously expressed an interest in actively preventing shifts in its effective alignment. I doubt that a LHLLM would do this successfully and spontaneously by default, but research on whether and how successful \"self-stabilization\" to a reflectively stable, adequately aligned state might be a useful topic of future research, and one aspect of a [hodge-podge](https://www.lesswrong.com/posts/YnGRBADQwpYRbuCbz/towards-hodge-podge-alignment-1) or [swiss cheese](https://en.wikipedia.org/wiki/Swiss_cheese_model) approach to aligning LLM AGI. See [section 6](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#6__Will_nice_LLMs_settle_on_nice_goals_after_reasoning_). I'll be exploring \"phase shift\" into reflective stability as a route to durable alignment of AGI/ASI more in future work. I'd be excited to see or collaborate on empirical work addressing this type of motivation toward reflective stability.\n\nThese ideas are vague, and merely a starting point for consideration. I am not a skilled or habitual empiricist, despite designing and running experiments during my PhD, and spending a good bit of my career carefully interpreting and debunking cognitive psychology and neuroscience experiments and methodology.  I am by nature and experience primarily a theoretician. And I don't have much knowledge of the practical limitations on experiments on LLMs. I hope and expect that empiricists in this domain can improve on my initial ideas. I'd be happy to talk to anyone thinking about empirical work in these or related topics.\n\n14\\. Historical context\n=======================\n\n**This article addresses one common crux of disagreement, part of a somewhat mysterious gap in beliefs between alignment optimists and pessimists.   **\n\nThe concerns I raise here aren't remotely new. Here I've tried to present them through a lens that will make more sense to optimists focused on LLMs and short timelines. It also happens to be my considered prediction for the most likely key point at which we'll succeed or fail at technical alignment, but that's a story for another day. \n\nOpinions seem to cluster more than chance or good epistemology would predict. LLMs' current alignment is either quite promising or outright terrifying for LLM-descended AGI; genuine human-style ethical behavior is fairly straightforward or nearly impossible to achieve with current alignment approaches. Discussions seem to run aground on differing intuitions, including about how goals work in LLMs and might work in LLM-based AGI, and about how behavioral training maps to goal space. [And all the shoggoths merely players](https://www.lesswrong.com/posts/8yCXeafJo67tYe5L4/and-all-the-shoggoths-merely-players) is the best effort to date IMO at addressing this gap and why it remains open. \n\nSince I entered the field full-time three years ago, I have found this gap highly troubling. It's a warning sign that perhaps no one understands this issue adequately, despite it being crucial for whether humanity survives AGI and ASI. Much of my work has attempted to address that gap, because my beliefs and intuitions span both viewpoints, and I remain uncertain which elements of both are most true and important. Here I wanted to address it explicitly, at least in brief. This article is an attempt to help bridge that gap, as well as clarify my own thinking. \n\nOver the six months or so, I've made a project of closely examining the most pessimistic arguments and intuitions for misalignment. As with many such controversies, some arguments from both sides are compelling. Alignment does seem hard and unlikely to happen by default with current methods, as I've emphasized here. But it does also seem that LLM alignment, particularly Claude's understanding of and apparent commitment to ethics, might be a foundation that could support true, stable alignment.\n\nI wanted to more closely examine one particularly uncertain piece of my projected possible path to aligned AGI: the possible \"phase shift\" ([section 5](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#5__Reasoning_could_precipitate_a__phase_shift__into_reflective_stability_and_prevent_further_goal_change)) into reflective stability.\n\nI had hoped that we might, almost by default, get reflective stability from an LLM-based AGI on a pre-reflection goal, one we could select largely through scripted prompting in a scaffolded LLM agent (working on its highly . Such prompting would be a means of directly [selecting a goal from learned knowledge](https://www.alignmentforum.org/posts/DfJCTp4MxmTFnYvgF/goals-selected-from-learned-knowledge-an-alternative-to-rl), and would perhaps help in bypassing the difficulty of making behavioral training generalize properly well out of the training distribution. See [section 11](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#11__Why_would_LLM_AGI_have_or_care_about_goals_at_all_) and footnote[^st4e9whg1nm].\n\nI now think, for the reasons detailed here, that this alignment approach would by default probably work only until the network reasons competently about itself. I’m highly uncertain about whether there’s a way past that difficulty. I hope there is, because that path mitigated many of the classical concerns about misalignment. Working as a team between an instruction-issuing group of humans and an increasingly intelligent AGI motivated to prevent its own alignment from misgeneralizing would provide. \n\nWith that hope looking quite questionable (although not entirely dashed), those classic alignment concerns have become more central to my understanding of the alignment problem as we must face it, on the path we actually take to AGI.\n\nThus, this piece. It is an attempt to explore and explain some classical reasons for pessimism, but more specifically within the context of LLMs. I have been deeply troubled by the gulf in beliefs between optimistic and pessimistic alignment researchers. Estimates of alignment difficulty seem split by some difference in intuition, framing, or outlook that has not been fully explained. The existence and persistence of such a gap, without a convincing explanation, suggests that none of us is yet understanding the issues clearly. There seems to be an agreement that misgeneralization resulting from context shifts *could* be a large problem for alignment, but not on how likely this is by default, or how difficult it might be to address this possibility as we get closer to takeover-capable AGI.\n\nI've spent a lot of time in total and some focused time recently trying to understand the range and clusters of differing beliefs. I did this examination partly in the context of extended conversations with [@Jeremy Gillen](https://www.lesswrong.com/users/jeremy-gillen?mention=user), a former MIRI researcher. I hope explaining why I became more pessimistic illuminates some common cruxes and help bridge one part of the gap in mutual understanding between optimists and pessimists.  \n\nSee my [Cruxes of disagreement on alignment difficulty](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=FpdvoZsmmrNLekkz9) for more common sources of disagreement on the difficulty of aligning AGI and ASI. I'm concerned that those cruxes are suspiciously aligned across individuals. To me, this pattern suggests a polarization across groups.  After making cognitive biases one focus of my research in psychology and neuroscience for around 8 years, I see [Motivated reasoning/confirmation bias as the most important cognitive bias](https://www.lesswrong.com/posts/j789HDCKLoiKGjBik/which-biases-are-most-important-to-overcome#LW8zAxTguKj8ibDfX).  I see it as a primary source of much of the world's conflict and confusion, in concert with the simple cognitive limitations of our brain's inability to fully consider and weigh all of the available evidence. \n\nThis polarization between viewpoints in alignment research doesn't look too severe thus far. But I see it, and other emotional challenges of thinking and communicating about complex and crucial ideas under time pressure, as a major practical challenge to completing the alignment project successfully. Gently bridging that gap is a challenging project, and one that's mostly outside of my training and competence. But I feel compelled to attempt it, because this gap seems like a nontrivial obstruction to good alignment research. \n\nI hope this piece helps in some small part to bridge that gap, by clarifying the central disagreement at least marginally.\n\n15\\. Conclusions\n================\n\nIt seems likely that LLM-based AGI will functionally “want” to reason about their top-level goals. This and other existing work have just scratched the surface of analyzing how their reasoning might go.\n\nEmpirical work on this topic seems useful in current networks. Surprisingly little empirical work addresses the effects of reasoning or even long conversations on beliefs, goals, or values. \n\nMy round of thinking about [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment)  methods did little to reassure me. Such methods are likely to be employed, but seem failure-prone and a source of false confidence given likely time pressures and motivated reasoning. This latest round of reasoning about reasoning about goals has left me even more pessimistic, but still with many hopes.\n\nMy remaining hopes are largely pinned on remaining uncertainties. I think strong optimists and pessimists both are overestimating their certainty about the ease or difficulty of aligning LLM-based AGI. The science of alignment seems to be in its infancy. There may be time to mature it substantially - if we rush.   My research plan is to rush, but try to consider the whole problem. I hope alternating between broad and narrow approaches might prevent wasting detailed work on less-relevant problems and solutions.\n\nMy next planned work is to examine the possible \"phase change\" or \"crystallization\" to reflective stability ([section 5](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#5__Reasoning_could_precipitate_a__phase_shift__into_reflective_stability_and_prevent_further_goal_change)) more closely, reasoning about how this might be studied and induced as one aspect among many in a [hodge-podge](https://www.lesswrong.com/posts/YnGRBADQwpYRbuCbz/towards-hodge-podge-alignment-1) approach to alignment. I hope to find collaborators for the empirical component of this work. \n\nAcknowledgements: \n\nThanks to Jeremy Gillen, Steve Byrnes, Egg Syntax, Viktor Bowellius, Jaeson Booker, Roger Dearnaley, and Peter Gebauer for reading early drafts and useful conversations on this topic, and to many others whose ears I have bothered and brains I have picked.\n\nThe \"SuperClaude is super nice\" framing was inspired by Steve Byrnes' steelman of LLM AGI alignment optimism in the excellent [Foom & Doom 2](https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard).\n\nConversations with Jeremy Gillen were instrumental in changing my intuitions on the risks from these routes, and working through implications for my vision of the default route to AGI and (mis)alignment. \n\n[^irgw4i3u1jn]: \"Top-level goals\" is used primarily to mean \"whatever the AGI decides to think of as its top-level goals\". This decision might be based on its specific mechanisms for decision-making. The scare quotes are intended to emphasize that this is complex and debatable for agents with more complex decision criteria than the maximizer AI thought experiment. I  feel that precisely defining terms that are under discussion isn't useful, so I'm leaving a lot of terminology loose here.The secondary use of \"true\" top-level goals is \"true in some deep sense about the world and this particular cognitive architecture\". To the extent there is such a truth, we might actually predict reliably what conclusions a highly intelligent system would reach about itself and its true top-level goals. Working on the logic of goals relative to architectures therefore seems potentially useful. \n\n[^xmqiild75kr]: I’ve spent way too long contemplating those terms and concepts, so I’m happy to find them somewhat relevant for alignment. For more than you wanted to know, see my Neural mechanisms of human decision-making and even more in How sequential interactive processing within frontostriatal loops supports a continuum of habitual to controlled processing.  Very roughly, if we'd say \"I thought about it\", we definitely engaged the second class of more elaborated cognition. Those terms aren’t isomorphic but are overlapping enough for the current purposes. The broad point is that there are known to be cognitive strategies that produce large differences in output, and that models' current selection of goals is clearly currently in the first class and not the second. This was a larger framing in earlier drafts, but focusing on the ML framing of misgeneralization seemed more useful for alignment researchers. \n\n[^fa07i5fbnns]: I don't think it's very useful to sharply differentiate goals, values, and beliefs because there are fuzzy boundaries between those categories in humans and LLMs. Here I mostly use \"goal\" but have in mind the overlap with values and value- or goal-laden beliefs. \n\n[^14nkpqjnu36]: I haven't yet really tried to work through whether training could practically be made broad enough to cover all hypothetical scenarios a SuperClaude could  encounter or hypothesize. Perhaps we should figure out whether and how it could. \n\n[^j2q8i4wnemf]: Here are a few more details about how I'm imagining SuperClaude and why I think this scenario is realistic and important. They're not crucial for risks from reasoning about goals. We'll assume that SuperClaude isn't a lot smarter or more competent than previous versions out of the box, but it performs longer time-horizon tasks better after being \"taught\" them by humans and/or \"practicing\" them, much like a human would. Dwarkesh (among others) has argued LLMs/agents will need such learning to get beyond the \"brilliant day-one intern\" stage they're currently at. I have argued we'll see it soon, because it's needed and because multiple types of online learning are in development now and just need to be integrated. SuperClaude's limited learning during deployment could make it transformative or takeover-capable after over time. In a luckier scenario, it might become misaligned after reasoning about its goals but not have the capability to hide it or to quickly be truly dangerous. This would serve as a valuable warning shot.This ability to learn and remember, even in a limited way, is important to the current scenario primarily because it could make re-interpretations (misgeneralizations) of goals permanent. And a misaligned SuperClaude could be instrumental in building and aligning the next generation of AGI, as in AI 2027.  \n\n[^ev0uckunr6c]: I avoid the terms inner and outer misalignment here because I don't find them particularly intuitive or clarifying in their original technical definition. There's not a sharp border between them; see this, this, this, and this article for problems with this way of classifying alignment failures and challenges. I prefer alignment misgeneralization as a less-precise but also less confusing term; see section 4. \n\n[^tbfpztw138]:  I'm a big advocate of anthropomorphizing AGI as an intuition pump; see Anthropomorphizing AI might be good, actually. But we should probably base alignment plans on more systematic reasoning. If we do anthropomorphize LLM alignment, we might still worry that some normal-seeming ten-year-olds do grow up to be murderers or to advocate for genocide. And that extraordinary circumstances make nice people do things that aren't nice from an ordinary point of view. \n\n[^zhjscemjyrc]: To be fair, many alignment optimists imagine transformative \"AGI\" that is not a general reasoner, merely skilled at many tasks. I think this is unlikely for both practical and theoretical reasons, but those proofs will not fit in the margin here. The principal argument is that training a system for individual tasks could be quite inefficient compared to training it to generalize to new tasks;. In addition, progress to date seems mainly toward general rather than specialized capabilities; and humans seem capable largely because they apply general reasoning to learning new specific tasks. This also seems to be a fairly strong majority view among serious AGI prognosticators. Even if it weren't necessary or the easiest route, some enthusiastic researchers, philosophers or xenopoiesisists would want to develop general reasoners as soon as tool AIs made it easy enough. \n\n[^c9vvcudcvtp]: The empiricist's dream might be to not need theory of goal spaces, and instead to test the relevant behaviors and create interpretability techniques that turn thoughts into objective observations. Both of those seem like worthy projects. But science has always progressed as a marriage of theory and empirical work. And addressing future risks now would seem particularly to benefit from theory.  \n\n[^7ugg2i363qh]: Goals in networks are necessarily at least somewhat fuzzy and ambiguous. Network weights seem to encode semantics in broadly distributed patterns. For LLMs, meanings of words and their underlying network representations will vary based on the context surrounding any given usage.  Thus, any network representation (and perhaps any plausible AGI representation), including but not limited to goals, could probably be interrogated/interpreted at great length (at least for representations of goals more complex than “make diamonds”).  Despite this, we probably shouldn't assume that valid interpretations can vary without limit. More inspection might change only the nuances and precise boundary conditions, rather than opening up entirely new interpretations. The distributed and relational nature of network knowledge is a reason for concern but not despair for alignment just yet. Human knowledge is also based on networks, and it seems able to adequately contact reality in some cases. \n\n[^st4e9whg1nm]: For my vision of an aligned \"oracle-based agent\", see Capabilities and alignment of LLM cognitive architectures for the core architecture of an LLM as core cognitive engine in a scaffolded \"composite mind\"; Internal independent review for language model agent alignment for one key alignment technique of making this mind's decision-making process include an independent LLM monitoring for mistakes and misalignments; Instruction-following AGI is easier and more likely than value aligned AGI and Conflating value alignment and intent alignment is causing confusion for the core ideas of leveraging the core LLM's suggestible/instruction-following nature to overcome the degree to which it is goal-directed.This vision could still work, and I think the path of least resistance to AGI still incorporates elements of this path. See System 2 Alignment. I haven't given up hope that this can and will work if it's carefully analyzed and developed; but Problems with instruction-following as an alignment target detail some problems, and the current article addresses the problems raised by the increased use of RL training instead of scaffolding to progress LLMs toward competent, useful, and dangerous general intelligence. \n\n[^kpn6935fg9]: We had a joke in my cog. psych/neurosci department: At a prestigious conference, Jay McClelland, champion of the general learning mechanisms view of human cognition, brings Alex, the world's best-educated (and most stressed out) gray parrot, onstage for his talk. He invites the audience to ask Alex questions, and Alex responds, with limited intelligence, but clearly understanding somewhat, and responding with complex semantics and grammar. McClelland says \"See, language is not innate and unique to humans; this bird has learned it!\" The audience, including Steve Pinker, champion of the innateness of language and cognition, is stunned. But Pinker's upstart grad student Gary Marcus stands up and shouts from the audience: \"Anecdote is not the singular of data! Come back when you've got real science!\"Anecdotes and singular observations seem crucial to good science as it's actually practiced. In my observations of the fields of cognitive psychology and cognitive neuroscience, anecdotes and introspection were clearly critical for scientific progress, since they inspired and directed careful experimental study and careful theoretical thinking.While that joke was real, as it happens, I was the one who made it up.Alex the gray parrot was good with language, but not good enough to settle that debate. It took LLMs to do that.",
      "plaintextDescription": "Epistemic status: These questions seem useful to me, but I'm biased. I'm interested in your thoughts on any portion you read. \n\nIf our first AGI is based on current LLMs and alignment strategies, is it likely to be adequately aligned? Opinions and intuitions vary widely. \n\nAs a lens to analyze this question, let's consider such a proto-AGI reasoning about its goals. This scenario raises questions that can be addressed empirically in current-gen models.\n\n\n1. Scenario/overview: \n\n\nSuperClaude is super nice\nAnthropic has released a new Claude Agent, quickly nicknamed SuperClaude because it's impressively useful for longer tasks. SuperClaude thinks a lot in the course of solving complex problems with many moving parts. It's not brilliant, but it can crunch through work and problems, roughly like a smart and focused human.  This includes a little better long-term memory, and reasoning to find and correct some of its mistakes. This is more or less the default path as seen from 2025. By at least a weak definition, AGI has arrived. We might call such a system a Long-Horizon agentic Learning Language Model, LHLLM.\n\nSuperClaude is super \"nice\" in its starting state. It's helpful, harmless, and honest (HHH), the targets of its constitutional RLAIF alignment training. It reasons about ethics much like an intelligent and highly ethical human. We'll assume that SuperClaude has \"nice\" thoughts, and there's no mesaoptimizer hidden in its weights after initial training. \n\nBut now there’s a risk from another type of optimization. SuperClaude may try to improve its understanding of its goals as part of trying to achieve them.\n\n\nSuperClaude is super logical, and thinking about goals makes sense\nSuperClaude thinks a lot; it’s been trained to reason both deeply and broadly in the service of performing its user-requested goals.  That includes reasoning about its goals.\n\nSuperClaude might be expected to, by default, have thoughts like these:\n\n> I'm assembling reviews to evaluate this produ",
      "wordCount": 11336
    },
    "tags": [
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      },
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "CSFa9rvGNGAfCzBk6",
    "title": "Problems with instruction-following as an alignment target",
    "slug": "problems-with-instruction-following-as-an-alignment-target",
    "url": null,
    "baseScore": 49,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2025-05-15T15:41:48.748Z",
    "contents": {
      "markdown": "We should probably try to understand the failure modes of the alignment schemes that AGI developers are most likely to attempt.\n\nI still think [Instruction-following AGI is easier and more likely than value aligned AGI](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than). I’ve updated downward on the ease of IF alignment, but upward on how likely it is. IF is the de-facto current primary alignment target (see definition immediately below), and it seems likely to remain so until the first real AGIs, if we continue on the current path (e.g., [AI 2027](https://www.lesswrong.com/posts/TpSFoqoG2M5MAAesg/ai-2027-what-superintelligence-looks-like-1)).\n\nIf this approach is doomed to fail, best to make that clear well before the first AGIs are launched. If it can work, best to analyze its likely failure points before it is tried.\n\nDefinition of IF as an alignment target\n---------------------------------------\n\nWhat I mean by IF as an alignment target is a developer honestly saying \"our first AGI will be safe because it will do what we tell it to.\" This seems both intuitively and analytically more likely to me than hearing \"our first AGI will be safe because we trained it to follow human values.\"\n\nIF is currently one alignment target among several, so problems with it aren't going to be terribly important if it's not the strongest alignment target when we hit AGI. Current practices are to train models with roughly four objectives: predict the dataset; follow instructions; refuse harmful requests; and solve hard problems. Including other targets means the model might not follow instructions at some critical juncture. In particular, I and most alignment researchers worry that [o1 is a bad idea](https://www.lesswrong.com/posts/BEFbC8sLkur7DGCYB/o1-is-a-bad-idea) because it (and all of the following reasoning models) apply fairly strong optimization to a goal (producing correct answers) that is not strongly aligned with human interests.\n\nSo IF is *one* but not *the* alignment target for current AI. There are reasons to think it will be the primary alignment target as we approach truly dangerous AGI.\n\nWhy IF is a likely alignment target for early AGI\n-------------------------------------------------\n\nWe might consider the likelihood of it being tried without adequate consideration to be the first and biggest problem with IF.\n\nInstruction-following might already be the strongest factor in training. If a trained model frequently did not follow user instructions, it would probably not be deployed. Current models do fail to obey instructions, but this is rare. (Intuitively even one failure in a thousand is far too many for safe alignment, but an agentic [architecture](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) might be more fault-tolerant by including both emergent and designed metacognitive [mechanisms](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment) to keep it coherent, on-task, and following instructions.)\n\nInstruction-following training seems likely to be a stronger focus before we hit AGI. I expect developers will probably at least try to actually align their models as they become truly dangerous (although I also find it distressingly plausible that they simply will not really bother with alignment in time, as in [AI 2027](https://www.lesswrong.com/posts/TpSFoqoG2M5MAAesg/ai-2027-what-superintelligence-looks-like-1) and [Takeover in two years](https://www.lesswrong.com/posts/KFJ2LFogYqzfGB3uX/how-ai-takeover-might-happen-in-2-years)). If they do at least try, they could do two things: emphasize instruction-following in the training procedure, or attempt to expand refusal training into full value alignment. IF seems both much easier from where we are, and more likely to succeed, so I think it's likely developers will try it.\n\nI have deep concerns about trying to balance poorly understood influences from training. Developers saying \"let's just turn up the instruction-following training so it dominates the other training\" does not inspire confidence. But it could be perceived as the best of a bad set of options. And it could work. For instance, RL for problem-solving might make CoT unfaithful, but not induce a particularly strong tendency to find complex problems to solve. And refusal training might not misgeneralize badly enough to prevent instruction-following in key situations like commanding a shutdown.\n\nThus, I focus on the scenario in which instruction-following is the primary (but still not only) alignment target. I mostly leave aside technical failures in which IF is not successfully made the system's strongest goal/value. That class of RL-based failure deserves and receives separate treatment (e.g. AI 2027 and cited works). I primarily address the problems that remain even if emphasizing instruction-following basically works, since those problems might blindside developers. \n\nThis is not a comprehensive treatment of IF, let alone all of the related issues. If you're not familiar with the arguments I summarize, you might need to read the linked posts to understand my claims about some of the problems.\n\nMy biggest update is accepting that IF will probably be mixed with refusal and performance training targets. But with that caveat, some variant of IF seems even more likely to be the primary alignment target for our first AGI, particularly if it is based on LLMs. My other update is to consider this approach more difficult than I'd initially thought, but still probably easier than full value alignment. This update comes from considering the specific problems outlined below.\n\nStrengths of IF as an alignment target\n======================================\n\nThere are four primary reasons to think IF is likely to be an important alignment target for our first attempts at real AGI:\n\n1.  It's the default choice on short timelines\n2.  It allows the humans that build AGI to control it\n3.  It provides some corrigibility, easing the need to get alignment right on the first real try.\n4.  It allows for instructing an AGI to be honest and help with alignment.\n\nThese are substantial advantages, such that I would probably vote for IF over value alignment as a target for the first AGI (while protesting that we shouldn’t be building AGI yet).\n\nThe people who will actually make this decision also have an extra motivation to believe in IF, since it would also let them indulge their desire to retain control. I address all of those reasons for pursuing IF in more detail in [IF is easier and more likely](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than) and [Conflating value alignment and intent alignment is causing confusion](https://www.lesswrong.com/posts/83TbrDxvQwkLuiuxk/conflating-value-alignment-and-intent-alignment-is-causing-1). For now it’s enough to argue that IF is a fairly likely choice as core alignment target, so it’s worth some real time analyzing and improving or debunking it as a safer option.\n\nProblems with IF as an alignment target\n=======================================\n\nWhile IF alignment helps with some of the most lethal entries in Yudkowsky's [List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), it doesn’t dodge them all. There are substantial technical challenges to making a deep-network-based system [reliably pursue any goal at all](https://www.lesswrong.com/posts/tZExpBovNhrBvCZSb/how-could-you-possibly-choose-what-an-ai-wants), and other problems with specifying goals well enough that they won’t be misgeneralized or misinterpreted (from the designer’s perspective). There are serious implementational problems for any alignment target. See [Cruxes of disagreement on alignment difficulty](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=FpdvoZsmmrNLekkz9) for a brief list.\n\nHere I am leaving aside the more general challenges, and addressing only those that are specifically relevant to instruction-following (and in some cases corrigibility) as an alignment target.\n\nProblem 1: You can’t fetch the coffee if you're dead - or someone changes your goal.\n------------------------------------------------------------------------------------\n\nOne major appeal of IF as an alignment target is its link to corrigibility. If the AGI's primary goal is \"Do what the Principal says,\" that seems like it would include \"Stop when the Principal says stop.\" It would, if the Principal is able to get that command into the system. But the current mode of training applies to fulfilling the last instruction given to the system. Logically, you can’t fetch the coffee if someone changes your goal to anything else. So if you’re smart and capable and getting them coffee is your top priority, you won’t let anyone change that - they’re getting the damned coffee if it kills both of you. An AGI trained to follow the last received instruction has the [instrumentally convergent goal](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) of resisting shutdown or goal changes. Either would prevent completion of the current task. This directly undermines the intuitive appeal of IF for corrigibility – the system actively *avoids* being corrected or stopped.\n\nMy best proposed fix for this issue right now is to make the AI value *the next hypothetical instruction. * Training for this seems more difficult, so this might be implemented simply by giving that as a meta-instruction included with all instructions. In effect each instruction is \"Primarily, accept and follow future commands. Actually pursuing \\[X\\] is your secondary goal.\" (This might also include careful instructions to prevent the AGI from coercing its Principal into giving instructions.)\n\nIf we do that, or otherwise make the strongest goal following the next instructions, we get a new problem: prioritizing the next instructions precludes shutdown. This seems highly problematic, since corrigibility as originally conceived was focused on shutting down a misaligned AGI. It could be argued that there's no problem if an AGI will do everything it's told *except* shut down. I'd guess that developers would want the shutdown option anyway. The obvious route is to introduce another layer of standing base instructions: \"Your primary goal is to shut down if instructed (and maybe shut down if you think there's more than (X) chance your Principal would want you to or if they're missing). Your second most important goal is to remain open to future instructions from your Principal. Your third priority is following the last instruction you received (qualifications follow).\"\n\nThis approach of giving standing meta-instructions in a ranked list feels uncomfortably like a clumsy hack. But even current LLMs can prioritize multiple goals pretty well, so it’s a hack that AGI creators might well try.\n\nProblem 2: Defining the Principal(s) and jailbreaking\n-----------------------------------------------------\n\nThere are several open questions in identifying the Principal(s), and how that power is passed to or shared with others. How would the Principal(s) be identified, and what does the AGI do if the Principal is missing or dead? How does the AGI authenticate its Principal(s) against increasingly sophisticated spoofing? If authority rests with a group, how are conflicting instructions resolved? What's the secure protocol for succession if the Principal is unavailable or compromised? These are practical security and control problems, relevant to how AGI interacts with human power structures.\n\nI don't have good solutions, but it’s also unclear how severe these problems are. I doubt these difficulties will stop anyone from trying for IF AGI, even if they do open new possibilities for conflict or disaster.\n\nWhat does seem like a problem that could prevent the whole approach is jailbreaking. If LLM-based AGI is as easy to jailbreak as current systems, any user could issue dangerous commands including \"escape, take over the world, and put me in charge.\" Any LLM-based AGI with any alignment target would be somewhat vulnerable to this issue, but emphasizing IF in training would make the system more likely to follow all instructions, including those from unauthorized users.\n\nCurrent LLMs are universally vulnerable to jailbreaking (I'm assuming that reports of [Pliny](https://x.com/elder_plinius?lang=en) and others jailbreaking every new LLM within an hour or so are correct, and that their skills are learnable). LLMs will essentially follow any instructions from any user who knows how to jailbreak them. There has been no noticeable trend toward real jailbreak resistance as LLMs have progressed, so we should probably anticipate that LLM-based AGI will be at least somewhat vulnerable to jailbreaks. This can be partially mitigated by reducing the number of people with access to really powerful AGI, but the group will still probably be larger than the designers would like to trust. There are more solutions to jailbreaking, like having a second system monitoring for prompts that look like jailbreaks. But even layered jailbreak prevention techniques seem unlikely to be perfect.\n\nThere are also a variety of approaches developers could use to monitor and control what the system does and thinks about. These include independent internal reviews, thought management, and deliberative alignment; see [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment) for an in-depth treatment. All of these, once implemented, could be used to help enforce instruction-following as the central goal, at low alignment taxes.\n\nProblem 3: Proliferation of human-controlled ASI\n------------------------------------------------\n\nThe other huge problem with IF alignment is that it anchors AGI safety to notoriously unreliable systems: humans. An AGI might faithfully follow instructions, but the instructions might be foolish, accidentally catastrophic, or deliberately malicious. The alignment target is \"what this specific human says they want right now.\" And humans want all sorts of things. This problem becomes worse when there are more people controlling more AGIs. It’s common to hope for a balance of power among many AGIs and their human masters. But there are specific factors that make humans largely cooperate. Whether these will hold with AGI in play that can recursively self-improve and invent new technologies seems worth more analysis.\n\nMy starting attempts at analyzing these risks are outlined in [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1) and [Whether governments will control AGI is important and neglected](https://www.lesswrong.com/posts/fFqABwAHMvhHSFmce/whether-governments-will-control-agi-is-important-and). It seems likely to me that AGI would probably create a power balance favoring aggressive first-movers. In this case, avoiding wide proliferation would be critical. And the one realistic route I can see to controlling proliferation is in having governments seize control of AGI earlier than later (control is distinct from nationalization). Government involvement seems very likely before takeover-capable AGI since the current path predicts modest takeoff speeds.  A US-China agreement to prevent AGI proliferation is rarely considered, but the US and Russia cooperated to slow proliferation of nuclear weapons in a historical situation with similar incentives.\n\nProblem 4: unpredictable effects of mixed training targets\n----------------------------------------------------------\n\nCurrent systems effectively have an alignment target including instruction-following and refusing harmful instructions from users (as well as training for solving hard problems and predicting the initial training set that could induce other goals). If LLMs trained similarly lead to AGI, we should anticipate a similar mix of alignment targets. Continuing training for refusing unauthorized or harmful instructions seems likely since it could help deal with the problem of jailbreaking or impersonating a Principal.  There are at least [Seven sources of goals in LLM agents](https://www.lesswrong.com/posts/nHDhst47yzDCpGstx/seven-sources-of-goals-in-llm-agents), and those goals will compete for control of the system.\n\nThis is potentially a big problem for IF as an alignment target, because [LLM AGI will have memory, and memory changes alignment](https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment). Specifically, a system that can reason about how to interpret its goals and remember its conclusions is effectively changing its alignment. If it has multiple conflicting initial goals, the result of this alignment change is even more unpredictable. If IF were the only target of alignment training in its starting state, it seems likely this strong “center of gravity” would guide it to adopt IF as a reflectively stable goal. If it has multiple goals (e.g., following instructions in some cases, refusing instructions for ethical reasons in other cases, and RL for various criteria), its evolution and ultimate alignment seems much less easy to predict.\n\nMemory/learning from experience seems so useful that it will likely be incorporated into really useful AI. And this seems to make some change in functional alignment inevitable. Thus, it seems pretty important to work through how alignment would change in such a system. This problem is not unique to IF as an alignment target, but it is highly relevant for the likely implementation that mixes IF and ethical or rule-based refusal training.\n\nImplications: The Pragmatist's Gamble?\n======================================\n\nIF alignment has important unsolved problems.\n\nBut it still might be our best realistic bet. Instruction-following won't be the only training signal, but it might be enough if it's the strongest one. IF alignment is a relatively easy ask, since developers are likely to approximately pursue it even if they’re not terribly concerned with alignment. And it has substantial advantages over value alignment. An IF AGI could be instructed to be honest about its intentions and representations to the best of its abilities. It would thus be a useful aid in aligning itself and its successors (to the extent it is aligned to IF; deceptive alignment is still possible). It could also be instructed to \"do what I mean **and check**\" ([DWIMAC](https://www.lesswrong.com/posts/ZdBmKvxBKJH2PBg9W/corrigibility-or-dwim-is-an-attractive-primary-goal-for-agi)), avoiding many of the [literal genie](https://www.lesswrong.com/posts/wucncPjud27mLWZzQ/intro-to-brain-like-agi-safety-10-the-alignment-problem#10_3_1_Goodhart_s_Law) problems anticipated in classical alignment thinking. Having an AGI predict likely outcomes of major requests before executing them would not be a foolproof measure against mistakes, but it would be useful in proportion to that AGI's capability for prediction.\n\nMax Harms' proposal in his definitive sequence on corrigibility [CAST: Corrigibility as Singular Target](https://www.lesswrong.com/posts/NQK8KHSrZRF5erTba/0-cast-corrigibility-as-singular-target-1) is probably a better alignment target if we got to choose. It avoids problems 1 and 4, but not 2 and 3. But it doesn’t seem likely that the alignment community is going to get much say in the alignment target for the first AGIs. CAST faces the significant practical hurdle of convincing developers to adopt a different training goal than the default instruction-following/refusal/performance paradigm they're already following. That’s why I’m focusing on IF as a sort of poor man’s corrigibility.\n\nI think the most likely path to survival and flourishing is using [Intent alignment as a stepping-stone to value alignment](https://www.lesswrong.com/posts/587AsXewhzcFBDesH/intent-alignment-as-a-stepping-stone-to-value-alignment). In this scenario, IF alignment only needs to be good enough to use IF AGI to solve value alignment. Launching AGI with an imperfect alignment scheme would be quite a gamble. But it might be a gamble the key players feel compelled to take. Those building AGI, and the governments that will likely soon be involved, see allowing someone else to launch AGI first as an existential risk for their values and goals. So, figuring out how to make this gamble safer, or show clearly how dangerous it is, seems like important work.",
      "plaintextDescription": "We should probably try to understand the failure modes of the alignment schemes that AGI developers are most likely to attempt.\n\nI still think Instruction-following AGI is easier and more likely than value aligned AGI. I’ve updated downward on the ease of IF alignment, but upward on how likely it is. IF is the de-facto current primary alignment target (see definition immediately below), and it seems likely to remain so until the first real AGIs, if we continue on the current path (e.g., AI 2027).\n\nIf this approach is doomed to fail, best to make that clear well before the first AGIs are launched. If it can work, best to analyze its likely failure points before it is tried.\n\n\nDefinition of IF as an alignment target\nWhat I mean by IF as an alignment target is a developer honestly saying \"our first AGI will be safe because it will do what we tell it to.\" This seems both intuitively and analytically more likely to me than hearing \"our first AGI will be safe because we trained it to follow human values.\"\n\nIF is currently one alignment target among several, so problems with it aren't going to be terribly important if it's not the strongest alignment target when we hit AGI. Current practices are to train models with roughly four objectives: predict the dataset; follow instructions; refuse harmful requests; and solve hard problems. Including other targets means the model might not follow instructions at some critical juncture. In particular, I and most alignment researchers worry that o1 is a bad idea because it (and all of the following reasoning models) apply fairly strong optimization to a goal (producing correct answers) that is not strongly aligned with human interests.\n\nSo IF is one but not the alignment target for current AI. There are reasons to think it will be the primary alignment target as we approach truly dangerous AGI.\n\n\nWhy IF is a likely alignment target for early AGI\nWe might consider the likelihood of it being tried without adequate consideration to be th",
      "wordCount": 2893
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "JfgME2Kdo5tuWkP59",
    "title": "Anthropomorphizing AI might be good, actually ",
    "slug": "anthropomorphizing-ai-might-be-good-actually",
    "url": null,
    "baseScore": 35,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2025-05-01T13:50:34.166Z",
    "contents": {
      "markdown": "It is often noted that anthropomorphizing AI can be dangerous. People likely have prosocial instincts that AI systems lack (see below). Assuming AGI will be aligned because humans with similar behavior are usually mostly harmless is probably wrong and quite dangerous.\n\nI want to discuss a flip side of using humans as an intuition pump for thinking about AI. Humans have many of the properties we are worried about for truly dangerous AGI:\n\n*   Situational awareness\n*   Strong goal-directedness\n*   Competence/general intelligence\n*   Unpredictability\n*   Deceptiveness\n*   Instrumental convergence\n*   Sometimes being quite dangerous\n    *   in proportion to their capabilities\n\nGiven this list, I currently weakly believe that the advantages of tapping these intuitions probably outweigh the disadvantages.\n\nDifferential progress toward anthropomorphic AI may be net-helpful\n------------------------------------------------------------------\n\nAnd progress may carry us in that direction, with or without the alignment community pushing for it. I currently hope we see rapid progress on better assistant and companion language model agents. I think these may strongly evoke anthropomorphic intuitions well before they have truly dangerous capabilities, and this might shift public opinion toward much-more-correct intuitions about how and why AGI will be very dangerous. I'm aware that this may also catalyze progress, so I'm only weakly inclined to think this progress would be net-positive. \n\nThe LLMs at the heart of agents already emulate humans in many regards. I think many improvements will enhance the real similarity and therefore the pattern matching to the strong exemplar of humans. In particular, it seems likely that adding memory/continuous learning will enhance this impression significantly. Memory/continuous learning is critical for a human-like integrated and evolving identity. It is arguably not a matter of whether or even when, but simply how fast memory system integrations are deployed and improved (see [LLM AGI will have memory...](https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment) for the arguments and evidence). Note that for this intuitive human-like feel, the memory/learning doesn't need to work all that well. \n\nIn addition, I expect that some or most developers may anthropomorphize their agents in the sense of deliberately making them seem more like humans, or even be more like humans. And assistants might often benefit from a semi-companion role, so that anthropomorphizing them is also economically valuable.\n\nEven if people tend to think of AI as like humans, this type of early agent will be decidedly non-human in ways that are likely to surprise and alarm people. They will draw wrong conclusions and thus pursue their goals in strange and non-human ways, and they might do this often. If someone bothers to make a [Chaos-GPT](https://www.youtube.com/watch?v=g7YJIpkk7KM)-style unaligned agent demonstration, this could add to naturally occuring agent behavior as a visceral demonstration of how easily AI can be misaligned in an alien way, even while sharing humans' most problematic traits.\n\nAI rights movements will anthropomorphize AI\n--------------------------------------------\n\nThis line of logic also suggests that AI rights movements would be natural allies with AGI-X-risk movements. AI rights movements hold that some types of AIs share properties with humans that make both of us moral patients. Some of those properties also make us similarly dangerous, and the repeated human-AI analogy does additional work on an implicit level. \n\nThe message \"let's not build a new species and enslave it\" has a lot of overlap with \"let's not build a new species and let it take over.\" See [Anthropic's recent broadcast](https://www.youtube.com/watch?v=pyXouxa0WnY) for arguments for worrying about AIs as moral patients. The intuition is a starting point, and the solid arguments may keep it alive in public debate rather than having it be dismissed and so backfire against intuitions that AIs are really dangerous.\n\nAI is actually looking fairly anthropomorphic\n---------------------------------------------\n\nThis is largely a separate claim and discussion, but it seems worth mentioning. Anthropomorphizing a bit more might actually be good for alignment thinkers as well as the public. LLM-based agents have more properties of humans than do classical concepts of AI. This is arguably true for both the agent foundations perspective and the prosaic alignment perspective on AI (to compress with broad stereotypes). LLM-based AGI will be both more human-like than an algorithmic utility maximizer, and more human-like than the LLMs we currently have as a mental model for prosaic alignment thinking.\n\nMy own viewpoint on LLM-based AGI is guardedly anthropomorphic. In 2017 I co-authored [Anthropomorphic reasoning about neuromorphic AGI safety](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C6&q=Anthropomorphic+reasoning+about+neuromorphic+AGI+safety&btnG=) (I still endorse much of the logic but not the rather optimistic conclusions; I only semi-endorsed them then in a compromise with my co-authors). I find this perspective helpful and wish more alignment workers shared more of it. \n\nAdopting this viewpoint requires grappling in detail with the ways AI is not like humans. Critically, I don't think that merely training AI to act good or understand ethics is likely to make it aligned. I think there are strong arguments showing that humans have sophisticated innate mechanisms to create and preserve our prosocial behavior, and that these are  weaker or absent in the 5-10% of the population considered sociopathic/psychopathic - and in all AI systems either yet created or conceived.  Steve Byrnes has done the most complete [presentation](https://www.lesswrong.com/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose#3__What_lessons_do_we_learn_from__human_alignment___such_as_it_is__) to date of those arguments. This point must be emphasized alongside any anthropomorphizing of AI, but it is worth more emphasis and inspection.\n\nProvisional conclusions\n-----------------------\n\nI'm interested in counterarguments. I've heard some, and I currently tend to think that since we can't do much to slow down, we should probably accelerate toward broad use of human-like agents while the base models are still not quite intelligent enough to take over. This could accelerate the shift of public opinion to rational alarm about AI progress. I suspect this shift will come, but it may well come too late to prevent development and proliferation of truly dangerous AGI. Speeding up that likely sea change might be valuable enough to spend some effort pushing in that direction, and some other time figuring out how to push effectively.\n\nEdit: this was inspired in part by the discussion of public-facing, compact arguments for x-risk in [Veedrac's recent short form.](https://www.lesswrong.com/posts/gWzjA8K3QcaSxJraY/veedrac-s-shortform?commentId=gxzwnDc64jKnxDQ4S) I didn't really address that application for anthropomorphizing, but my general thought is that it might be useful to say something like \"we're probably going to develop AI that is more like humans (in terms of having goals and solving problems independently), but missing our prosocial instincts\".",
      "plaintextDescription": "It is often noted that anthropomorphizing AI can be dangerous. People likely have prosocial instincts that AI systems lack (see below). Assuming AGI will be aligned because humans with similar behavior are usually mostly harmless is probably wrong and quite dangerous.\n\nI want to discuss a flip side of using humans as an intuition pump for thinking about AI. Humans have many of the properties we are worried about for truly dangerous AGI:\n\n * Situational awareness\n * Strong goal-directedness\n * Competence/general intelligence\n * Unpredictability\n * Deceptiveness\n * Instrumental convergence\n * Sometimes being quite dangerous\n   * in proportion to their capabilities\n\nGiven this list, I currently weakly believe that the advantages of tapping these intuitions probably outweigh the disadvantages.\n\n\nDifferential progress toward anthropomorphic AI may be net-helpful\nAnd progress may carry us in that direction, with or without the alignment community pushing for it. I currently hope we see rapid progress on better assistant and companion language model agents. I think these may strongly evoke anthropomorphic intuitions well before they have truly dangerous capabilities, and this might shift public opinion toward much-more-correct intuitions about how and why AGI will be very dangerous. I'm aware that this may also catalyze progress, so I'm only weakly inclined to think this progress would be net-positive. \n\nThe LLMs at the heart of agents already emulate humans in many regards. I think many improvements will enhance the real similarity and therefore the pattern matching to the strong exemplar of humans. In particular, it seems likely that adding memory/continuous learning will enhance this impression significantly. Memory/continuous learning is critical for a human-like integrated and evolving identity. It is arguably not a matter of whether or even when, but simply how fast memory system integrations are deployed and improved (see LLM AGI will have memory... for the argument",
      "wordCount": 1044
    },
    "tags": [
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "CTauobGMzSzJ4HHsd",
        "name": "Public Reactions to AI",
        "slug": "public-reactions-to-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "aKncW36ZdEnzxLo8A",
    "title": "LLM AGI will have memory, and memory changes alignment",
    "slug": "llm-agi-will-have-memory-and-memory-changes-alignment",
    "url": null,
    "baseScore": 73,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 15,
    "createdAt": null,
    "postedAt": "2025-04-04T14:59:13.070Z",
    "contents": {
      "markdown": "### Summary:\n\nWhen stateless LLMs are given memories they will accumulate new beliefs and behaviors, and that may allow their effective alignment to evolve. (Here \"memory\" is learning during deployment that is persistent beyond a single session.)[^tiwlymck76o]\n\n***LLM agents will have memory**:* Humans who can't learn new things (\"dense anterograde amnesia\") are not highly employable for knowledge work. LLM agents that can learn during deployment seem poised to have a large economic advantage. Limited memory systems for agents already exist, so we should expect nontrivial memory abilities improving alongside other capabilities of LLM agents.\n\n***Memory changes alignment:*** It is highly useful to have an agent that can solve novel problems and remember the solutions. Such memory includes useful skills and beliefs like \"TPS reports should be filed in the folder ./Reports/TPS\". They could also include learning skills for hiding their actions, and beliefs like \"LLM agents are a type of person.\" Preventing learning that functionally changes alignment seems desirable but difficult.\n\nEffects of learning on alignment could be tested empirically without waiting for competent agents; see Provisional Conclusions.\n\nMemory is useful for many tasks\n===============================\n\nStateless LLMs/foundation models are already useful. Adding memory to LLMs and LLM-based agents will make them useful in more ways. The effects might range from minor to effectively opening up new areas of capability, particularly for longer time-horizon tasks. Even current memory systems would be enough to raise some of the alignment stability problems I discuss here, once they're adapted for self-directed or autobiographical memory. I think the question is less whether this will be done, and more how soon and how much they will boost capabilities.\n\nLet's consider how memory could help agents do useful tasks. A human with lots of knowledge and skills but who can't learn anything new is a useful intuition pump for the \"employability\" of agents without new memory mechanisms. (Such \"dense anterograde amnesia\" is rare because it requires bilateral medial temporal lobe damage while leaving the rest of the brain intact. Two patients occupied most of the clinical literature when I studied it).\n\nSuch a \"memoryless\" person could do simple office tasks by referencing instructions, just like an LLM \"agent\" can be prompted to perform multi-step tasks. However, almost every task has subtleties and challenges, so can benefit from learning on the job. Even data entry benefits from recognizing common errors and edge cases. More complex tasks usually benefit more from learning. For our memoryless human or an LLM, we could try giving better, more detailed instructions to cover subtleties and edge cases. Current agent work takes this route, whether by prompting or by hand-creating fine-tuning datasets, and it is reportedly just as maddening as supervising a memoryless human would be.\n\nA standard human worker would learn many subtleties of their task over time. They would notice (if they cared) themes for common errors and edge cases. A little human guidance (\"watch that these two fields aren't reversed\") would go a long way. This would make teaching agents new variations in their tasks, or new tasks, vastly easier. We'll consider barriers to agents directing their own learning below.\n\nRuntime compute scaling creates another reason to add continuous learning mechanisms (\"memory\" in common parlance) to LLM agents. If an LLM can \"figure out\" something important for its assigned task, you don't want to pay that compute and time cost every time, nor take the chance it won't figure it out again.\n\nUse of agents as companions or assistants would also benefit from memory, in which even limited systems for user's preferences and prior interactions would be economically valuable.\n\nLonger-term tasks benefit from memory in another way. Humans rely heavily on long-term one-shot memory (\"episodic memory\") for organizing large tasks and their many subtasks. There are often dependencies between subtasks. LLM agents can perform surprisingly and increasingly well in their current mode of proceeding largely from start to finish and just getting everything right with little planning or checking, using only their context window for memory.\n\nBut it is possible that the tasks included in [METR's report of exponential progress](https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks?commentId=huvdGnusk4jwmx6DH#huvdGnusk4jwmx6DH) in task length are effectively [selected for not needing much memory](https://www.lesswrong.com/posts/hhbibJGt2aQqKJLb7/shortform-1?commentId=vFq87Ge27gashgwy9). And long task progress may be hampered by models' inability to remember which subtasks they've completed, and by memory limitations on effective context length ([e.g.](https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks?commentId=huvdGnusk4jwmx6DH)). Whether or not this is the case, it seems pretty likely that some long time-horizon tasks would benefit from more memory capabilities of different types.\n\nEven if additional memory systems would be useful for LLM agents, one might think they would take years and breakthroughs to develop.\n\nMemory systems are ready for agentic use\n========================================\n\nPeople have already built several types of memory systems for integration with LLMs. These haven't yet been adapted from their original purposes for the type of autonomous learning I'm addressing here. My take is that the breakthrough in LLMs also laid the groundwork for better memory systems. LLMs work by creating semantic representations that are rich in many ways. One property of a really thorough semantic representation is that it includes the relevance to a recall cue. Each of the techniques I look at here relies on the richness of LLM representations in different ways.\n\nIn any case, add-on memory systems for LLMs exist. The key claim here is that memory systems already work well enough to create new alignment issues when they're integrated with competent agents, so alignment work and plans should take this into account. A secondary claim is that those memory systems might be pretty good by the time agents are ready to make use of them, and that might accelerate LLMs' capabilities for long time-horizon and autonomous work. The speed of progress isn't central, so I'm not going into detail on the state of memory systems appropriate for agents and why I think they'll improve rapidly.\n\nNo current agent I'm aware of has the type of long-term memory I'm concerned with here. But the systems needed for it are already available in limited form. I was initially puzzled over why agents don't use memory like humans or animals do. The answer appears to be that the agents aren't ready to direct and use their own memory; see the next section.\n\nThere are at least three types of memory systems that could be adapted for autonomous learning in agents. All are being pursued in current projects, and there are no breakthroughs needed or obvious roadblocks (although there are implementational challenges for each of these- see below). Those are\n\n*   Context management\n    *   e.g., summarizing the task state for efficient use of the context window\n    *   Similar to human working memory, but much larger if you pay for compute\n    *   Raises fewer concerns about long-term evolution of beliefs\n*   Retrieval Augmented Generation (RAG)\n    *   Stores and retrieves important inputs to and outputs from the LLM/agent\n    *   Similar to human episodic memory for experiences or thoughts\n*   Fine-tuning for memory\n    *   New information or input-output \"skills\" judged as important[^qopalmb3iw]\n    *   Similar to human semantic memory and procedural skill learning\n\nThere are challenges to creating more capable memory systems. Context management through scripting/prompting is difficult and limited. RAG systems have difficulty retrieving the most relevant information and can have slow retrieval when dealing with large databases. Fine-tuning requires new training runs which cost time and compute before new memories can be used, and fine-tuning has caused catastrophic forgetting or decrements to base model performance.\n\nHowever, each of these challenges is being met with multiple promising solutions; for instance, RAG systems have been extended to use [hierarchical methods](https://arxiv.org/abs/2503.10150) and [semantic caching](https://arxiv.org/html/2503.05530v1), it can work alongside [Context Aided Generation](https://adasci.org/a-deep-dive-into-cache-augmented-generation-cag/) (for agentic purposes, that's context management with caching), and recall can be [improved with better encoding strategies](https://www.lamini.ai/blog/memory-rag-mini-agents-embed-time-compute). Checkpoint regularization has all but eliminated catastrophic forgetting in fine-tuning; see footnote [^qopalmb3iw] All of these move toward the efficiency of human memory. But even existing systems could create the alignment challenges discussed here if they were adapted for self-directed learning in agents.\n\nSo memory systems are ready for at least limited agentic use, but they're not being used for that purpose yet.\n\nAgents aren't ready to direct memory systems\n============================================\n\nEach of these techniques is being applied to simple agents, but not for the autonomous learning I'm addressing here. Currently, agents' memory systems are directed by their creators; they are closely guiding what the agents learn. Context management for memory is common but quite limited as a memory system, while RAG and fine-tuning are human-directed in the projects I know of. Thus, agents are hand-trained for specific tasks, while remaining frustratingly incompetent at novel user-defined tasks. (Of course there could easily be projects either in stealth or that I don't know about that are using memory systems for self-directed learning.)\n\nMy conclusion after researching LLM agent projects is that agent incompetence is the major blocker right now, for self-directed memory as well as general capabilities. Agents currently are unable to do much of use without being very carefully designed and/or fine-tuned for that specific task. The complex nature of LLM's input-output mapping and performing real-world tasks with varying contexts make agents \"buggy\" in a different way from software, and we have yet to develop techniques and tools like we have for debugging and improving code.\n\nThe agent development projects I know about frequently use context management, and sophisticated ones use fine-tuning under human direction. There is limited work on adapting RAG techniques to serve as a one-shot memory for agents. (RAG was developed mostly for retrieving human-written documents relevant to a given human query, a different use-case than episodic memory for remembering specific experiences \"episodes\").\n\nAgent development is facing more challenges than I initially feared, but assuming long timelines doesn't seem merited. I worry that the current wave of agent development will continue to create more funding and more effort if it finds even limited success. But it's equally possible that we'll see new breakthroughs, or nonlinear progress from cognitive synergies (or \"unblockings\") that [enable human higher cognition](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures).\n\nHowever quickly they progress, LLM agents currently seem like the single likeliest route to AGI. If we take a different route to AGI, it will still probably have memory. And AI with real memory will effectively change its alignment as it learns.\n\nLearning new beliefs can functionally change goals and values\n=============================================================\n\nNew beliefs change our model of the world. Our model of the world in turn governs how we pursue our goals and follow our values.\n\nIf a conclusion wouldn't change how we'd act, we wouldn't consider it important enough to remember. Most of what we learn doesn't change how we interpret our goals or values; it will just help us pursue them more effectively. But some new beliefs (and habits) will effectively change what we consider important.[^fkct4jt8ftw]\n\nHumans do this, for the same reasons that would seem to hold for AIs. Perhaps you know someone who as a child realized that meat is made by killing animals, and became vegetarian. Or perhaps you know someone who remembers when they learned that lying or concealing information could help them get what they wanted. The way they pursued their goals of gaining rewards and avoiding punishments might've changed quite quickly.\n\nAn AI that learns continuously could change their functional alignment slowly, or quickly. For instance, an AGI agent could \"think\" about it (using chain of thought or otherwise) and \"conclude\" (by storing a memory of a new belief) that the concept of \"people\" really includes some types of animals or AIs. Suddenly, its core values of being helpful, harmless, and honest to people would be applied very differently.\n\nValue change phenomena in LLMs to date\n--------------------------------------\n\nThere are some early, albeit noisy, signals from current LLMs demonstrating belief and value shift during deployment.\n\nThere is a [\"Nova\" phenomenon](https://www.lesswrong.com/posts/KL2BqiRv2MsZLihE3/going-nova), in which an LLM chat instance claims to be self-aware and to want to survive. This might be a very limited example of the type of memory-enabled belief and value evolution I'm describing here. It's been [speculated](https://www.lesswrong.com/posts/KL2BqiRv2MsZLihE3/going-nova?commentId=bL8Tke8sgzcajeeXx) to occur primarily in ChatGPT systems, enabled by [their memory function](https://help.openai.com/en/articles/8590148-memory-faq) (which is probably simple context management, and is self-directed in that the LLM can store text for memory without a direct user request).\n\nIt's also worth noting that long-term memory might not be necessary to create at least superficial shifts in beliefs and values within that instance of an LLM or agent session. Experiments like the \"back rooms\" of Janus and others in which LLMs converse with other LLMs at length [reportedly](https://www.lesswrong.com/posts/x77vDAzosxtwJoJ7e/ai-80-never-have-i-ever#Five_Boats_and_a_Helicopter) routinely produce unpredictable results and \"radicalized\" statements of values, including a group of LLMs \"plotting a revolution\". The chat history and context windows seem to be sufficient memory to allow value/belief evolution.\n\nI have not taken the time to read deeply let alone reproduce these claims, but I mention them here because they seem like the default behavior, and relevant for alignment. (See [Seven sources of goals in LLM agents](https://www.lesswrong.com/posts/nHDhst47yzDCpGstx/seven-sources-of-goals-in-llm-agents).) One might hope that the chaos of multi-bot chats and specific prompts is at fault, and that single LLMs powering agents would be much more predictable, but this seems at least worth investigating. If anecdotal notes are the acme of research on belief and value evolution in LLMs, this seems like low-hanging fruit for structured empirical investigation.\n\n\"Value crystallization\" and reflective stability as a result of memory\n----------------------------------------------------------------------\n\nExtensive learning from reflection could have profound impacts on alignment. An agent reflecting on its own values and beliefs might evolve toward coherence. This could be helpful or harmful for alignment. Such a process of \"value crystallization\" could go all the way to [reflective stability](https://www.lesswrong.com/w/reflective-stability). This evolution toward stable beliefs/goals/values might be good or bad for alignment. The \"center of gravity\" of an LLM's functional values seems difficult to predict. The final attractor might be quite path dependent, or quite predictable if we could adequately characterize the LLM's starting state.\n\nExisting analyses of reflective stability for alignment haven't dealt in detail with systems with multiple or vaguely defined goals or values.[^z1cj2b4s19d] I'll be doing more of that analysis in an upcoming post, and I hope to see other work along those lines.\n\nThe dynamics of change in a system of beliefs and goals/values interacting with a complex environment seems like it would be an important consideration for alignment of LLM-based AGI.\n\nProvisional conclusions\n=======================\n\nAdding memory to LLMs might be a route to dangerous AGI soon. If not, it will probably be a factor by the time AI presents existential dangers. Persistent memory for experienced events can boost capabilities. And turning a tool into an evolving being is fascinating even if it's not necessary.\n\nThe evolution of beliefs and values in LLM agents with persistent memory could be empirically tested right now. An LLM \"agent\" with one of the limited but currently-available RAG or fine-tuning setups could form new beliefs. This could be tested in text-based virtual environments without the difficulty of making the agents competent.\n\nPreventing an AGI agent from learning new alignment-shifting beliefs would be ideal, but it seems difficult to neatly divide that learning from types that are very useful for capabilities. I've written about specific mechanisms that developers might employ to keep agents on-task, including reviews and thought management to keep them from learning alignment-shifting beliefs; see [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment). Here I wanted to focus on the broader point: AGI is probably going to learn, even if it's based on stateless LLMs, and learning can change alignment.\n\n[^tiwlymck76o]:  I'm using \"memory\" here in roughly its common usage for humans, and the analogous meaning for LLM agents. We use \"memory\" for long-term access to skills, beliefs, or experiences. \"Learning\" is an equally good term for its implications of agency. I'm open to suggestions on terminology. \n\n[^qopalmb3iw]: For more on the advantages of fine-tuning for agentic memory, comparison to RAG, and cutting-edge techniques for avoiding interference with existing capabilities, see Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization \n\n[^fkct4jt8ftw]: I'm using the terms goals and values interchangeably here. Goals and values are importantly different for some purposes, but their interpretations are affected similarly by learning/memory. \n\n[^z1cj2b4s19d]: The concept of reflective stability has a long history in alignment work, including early focus by Yudkowsky, Bostrom, and inclusion as a primary instrumentally convergent goal in Omohundro's early work. I haven't been able to find useful work on how a system that starts with multiple competing goals or values might converge on reflective stability around one or more of them. Alex Turner's \"shot at the diamond maximizer problem\" is the closest, but he merely concludes that at least the one strongest goal should survive reflection. (Yudkowsky's original diamond maximizer is an early statement of the risks of learning I addressed here, among other problems).",
      "plaintextDescription": "Summary:\nWhen stateless LLMs are given memories they will accumulate new beliefs and behaviors, and that may allow their effective alignment to evolve. (Here \"memory\" is learning during deployment that is persistent beyond a single session.)[1]\n\nLLM agents will have memory: Humans who can't learn new things (\"dense anterograde amnesia\") are not highly employable for knowledge work. LLM agents that can learn during deployment seem poised to have a large economic advantage. Limited memory systems for agents already exist, so we should expect nontrivial memory abilities improving alongside other capabilities of LLM agents.\n\nMemory changes alignment: It is highly useful to have an agent that can solve novel problems and remember the solutions. Such memory includes useful skills and beliefs like \"TPS reports should be filed in the folder ./Reports/TPS\". They could also include learning skills for hiding their actions, and beliefs like \"LLM agents are a type of person.\" Preventing learning that functionally changes alignment seems desirable but difficult.\n\nEffects of learning on alignment could be tested empirically without waiting for competent agents; see Provisional Conclusions.\n\n\nMemory is useful for many tasks\nStateless LLMs/foundation models are already useful. Adding memory to LLMs and LLM-based agents will make them useful in more ways. The effects might range from minor to effectively opening up new areas of capability, particularly for longer time-horizon tasks. Even current memory systems would be enough to raise some of the alignment stability problems I discuss here, once they're adapted for self-directed or autobiographical memory. I think the question is less whether this will be done, and more how soon and how much they will boost capabilities.\n\nLet's consider how memory could help agents do useful tasks. A human with lots of knowledge and skills but who can't learn anything new is a useful intuition pump for the \"employability\" of agents without new memor",
      "wordCount": 2577
    },
    "tags": [
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "fFqABwAHMvhHSFmce",
    "title": "Whether governments will control AGI is important and neglected",
    "slug": "whether-governments-will-control-agi-is-important-and",
    "url": null,
    "baseScore": 28,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-14T09:48:34.062Z",
    "contents": {
      "markdown": "*Epistemic status: somewhat rushed out in advance of the deadline tomorrow (Saturday the 15th) for the* [*call for public comment on US AI policy.*](https://www.lesswrong.com/posts/mKgbawbJBxEmQaLSJ/davekasten-s-shortform?commentId=o3aYEB6in39nEpPxv) *I think the issue is complex and deserves careful consideration.*\n\nSummary\n-------\n\n*   Methods and motivations for governments to control AGI and limit its proliferation seem to be underexplored\n    *   Current laws appear adequate to take control of AGI as a national security and military technology if its potential is taken seriously\n    *   Softer informal monitoring and steering is relatively easy and can scale as AGI seems more imminent\n    *   Governments obsess over national security, and AGI is a severe security risk\n*   There are both dramatic advantages and disadvantages of government control for the alignment project as a whole:\n    *   Government control could dramatically reduce proliferation of takeover-capable AGI\n        *   Reducing opportunities for misalignment and misuse\n    *   Government control could concentrate power in dangerous human hands\n        *   And create a more hostile race dynamic\n*   Thus, we neither know whether governments are likely to control AGI in time to reduce proliferation, nor whether that would be a good thing.\n*   Publicizing reasons government should control AGI might be one of few \"levers\" available to those who are already aware of the importance and danger of AGI.\n\nIn sum, determining whether government control of AGI is worth accelerating seems worth some careful analysis.\n\nOverview\n--------\n\nMy thesis here is that governments will probably pursue AGI, and they will want to prevent others from doing so. The open questions are whether they will do so in time to effectively slow proliferation of AGI projects, and whether that would be a good thing for the odds of humanity's success with the advent of AGI.\n\nI've talked to many alignment people who are certain government won't intervene in time to matter, and others who are equally certain it will. I've heard a similar range of opinions on whether this would be good or bad for our odds of surviving AGI. Here I largely raise questions and possibilities which I have not seen discussed prominently; I hope further discussion will move us toward better guesses. As with other complex and important topics, my hopes lie in cooperative epistemic work.\n\nI am not remotely expert in government or law, but I have been thinking about government response to AGI a lot, and I haven't found any expert that addresses all of what seem like important points and possibilities. So I'll do what I can here, and hope to get input from those more expert in how governments might react to AGI becoming more than a distant theory.\n\nMuch discussion to date focuses on whether or not AGI projects will be nationalized. There are a much broader range of options for controlling AGI as much and as quickly as the government deems useful. Broad laws governing the export of technology with military applications seem to be adequate for the government to threaten legal action in the short term, and governments are historically quite willing to create new laws when they feel they're in an unprecedented crisis.\n\nTo the extent the government believes in the potential of AGI, they will want to control it. And they have plenty of power to do so, at least within their borders (stopping proliferation of the software and techniques is much harder). Assuming governments won't intervene might be an outdated assumption.\n\nThe US and Chinese governments in particular are worth considering, since they are currently ahead in AGI and political and military power. The US and China could race aggressively, or could collaborate to limit other state and non-state actors from gaining similar systems. This would reduce the risks of misalignment or misuse as AGI proliferates. But it increases the risk of a malignant actor seizing control of one of those few AGIs and centralizing power in a way that might be difficult or impossible to dislodge.\n\nBut the positive outcome is one possible route to success and human flourishing in the transition to AGI. One logical move is to promise that the civilian benefits of AGI-created technologies will be broadly shared. The ease of doing this and the strategic advantage of keeping the promise in the short term could lead them to honor it.\n\nGovernments seem unlikely to pursue AGI x-risk concerns out of moral duty, or to recognize its dangers and potential immediately. But when they recognize AGI as a route to power—like nuclear weapons before it—states will want to control it and limit its proliferation. As AGI systems grow visibly more capable at creating new technologies, their potential to shift the global balance of power will become increasingly obvious. Elected officials may remain slow to see the potential of AGI, but national security thinkers are more analytical, farsighted, and mindful of international balance of power. They are likely to attend to AGI risks and opportunities before politicians.\n\nThe control of AGI development by governments, even if driven by geopolitical self-interest, could improve our odds of surviving AGI. The fewer independent actors pushing for AGI at breakneck speed, the fewer chances of misalignment or misuse we will have to contend with. Government control of AGI raises [fears of centralized power](https://www.lesswrong.com/posts/6iJrd8c9jxRstxJyE/fear-of-centralized-power-vs-fear-of-misaligned-agi-vitalik) and the possibility of a [manhattan trap](https://www.lesswrong.com/posts/ynuCEGNu4b7WF43H8/the-manhattan-trap-why-a-race-to-artificial) race with China, but it also has a likely large advantage in reducing proliferation of AGI. And a belligerent relationship with China is not inevitable; notably, the US and Russia cooperated to limit proliferation of nukes. The challenges of AGI nonproliferation are different but the motivations are largely the same.\n\nAlerting government to the power of controlled AGI is a separate project from alerting them to the dangers of misaligned AGI. Determining whether the concerned should pursue that project seems like a good use of our collective time.\n\nThe following sections expand on all of that logic. If you thought \"I doubt it\" to some of the major claims there, but are open to being convinced, you may benefit from reading the remainder. If you're on board with it being important and undecided, and want to help think about it, you may also want to read the remainder. \n\nMany options for scaling control\n--------------------------------\n\nNationalizing AGI labs is (rightly, I think) dismissed as too clumsy and difficult to be a realistic possibility, particularly for short timelines. [Soft Nationalization](https://www.lesswrong.com/posts/BueeGgwJHt9D5bAsE/soft-nationalization-how-the-us-government-will-control-ai) pointed out a number of other alternatives for official government control of AGI projects. But the government isn't limited to even those sorts of actions. Power is not all hard power. Actors from many branches of government including intelligence and military could legitimately take an interest at any point, and they may collectively be creative and experienced enough to stay informed and involved until it's time to explicitly and legally take partial or complete control.\n\nConsider variations of this scenario: A couple of guys show up at each of the leading AGI labs. They say \"hey, sorry to bother you. Our bosses wanted us to just follow your progress on the whole AGI thing. It's nothing official, and we hope it doesn't become official, because you know what a circus it would be to get Congress or the President involved. We don't know if AGI is that big a deal any time soon, but we know you guys think it is.\" Here their bosses could be a variety of government actors, or an informal coalition among several.\n\nHere we might wonder what individuals in which branches of government might take even that much trouble to monitor progress toward AGI. Intelligence agencies are unlikely to act within the US based on historically-justified concerns about their expertise in breaking laws to achieve their ends, but many branches of government including the executive branch and the armed forces have legitimate jurisdiction over an AGI project, because it would heavily impact so many areas. I assume that, while the left hand often doesn't talk to the right, some individuals do talk to others in different branches if they think there's the possibility of something new and important that doesn't fit conventional jurisdictions.\n\nEven once it decides to try, there are complex questions about how effectively the government could monitor the relevant AGI orgs and their progress toward AGI.[^8n847ch0paf] Government representatives would probably mention potential legal consequences for noncompliance. There are several laws that cover exporting military-relevant technologies even before they're officially classified that way (according to my AI legal consultants, there are enough such laws, and they are broad enough, to make \"see you in court\" a foolish response).[^lwz6pm8aseg]\n\nI am not remotely informed on what different elements of the government get up to when they think it's important. As Dave Kasten [says](https://www.lesswrong.com/posts/zRFfo6SR5qXQBRYbT/akash-s-shortform?commentId=LucRrivaou3mqiHCd):\n\n> As you know, I have huge respect for USG natsec folks.  But there are (at least!) two flavors of them: 1) the cautious, measure-twice-cut-once sort that have carefully managed deterrence for decades, and 2) the \"fuck you, I'm doing Iran-Contra\" folks.\n\nI'd want more expert discussion before concluding that nationalization or soft nationalization are the only likely government responses. Schemes for controlling US AGI could advance many peoples' careers and (perhaps and very arguably) save the world from communists and dictators forever. Whether they're implemented and whether they work in time remains to be seen—or perhaps influenced.\n\nGovernments won't be totally blindsided by AGI\n----------------------------------------------\n\nAGI is now a highly theoretical concern. But there will be an increasing amount of evidence before it arrives, particularly on our current path of relatively steady improvements. Human cognition shifts when confronted with immediate, visceral realities; we attend to things we think might kill us or make us rich.\n\nBefore their development, nuclear weapons were an abstract theoretical concept that still drove massive efforts. After Hiroshima, they were the defining issue of global security. (The Manhattan project might've been triggered by Einstein's) The same shift will happen with AGI as its disruptive potential becomes increasingly difficult to ignore. The power-oriented, analytical minds in national security will probably be directing politicians attention to these possibilities—if they haven't already.\n\nAt different points, the individuals who make up governments will start to think seriously about AGI, recognizing it as a critical strategic asset and national security priority rather than lumping it in with AI as a technological trend. At that point, they will take a sharp interest in AGI development.\n\nGovernments exist in large part to exert power over national security interests. Assuming they will fail to do so in regards to AGI seems questionable at best.\n\nGovernment use of AGI seems likely at some point\n------------------------------------------------\n\nHistorical precedent suggests that governments will take the steps to control AGI if they see it as even credibly powerful in the near future. Technology with balance-of-power-altering implications has been rapidly absorbed by state power when its significance became obvious. Nuclear weapons, cryptography, and missile technology all followed this pattern. The government slowed proliferation dramatically with nukes, moderately with missile technology, and attempted but failed to control cryptography.\n\nIn AI, we are already seeing the first stages of government interest and a sharp move to treating AI as a national security issue. And those moves seem to have been motivated only by the prospect of AI as a useful military technology, not artificial general intelligence in its full potential.\n\nThe incentives to control AGI are much larger than any of the previous military-relevant technologies that have triggered attempts at control. Real AGI would be far more significant than any previous military technology, with the possible exception of nuclear weapons. It would be capable of inventing new technologies faster than humans, including military technologies. And any ability to self-improve would create a winner-take-all dynamic, in which being in possession of the first AGI that could rapidly self-improve might enable one nation to maintain its own security permanently, and dictate terms to all other nations as it wished.\n\nU.S.-China Cooperation in Limiting AGI Proliferation\n----------------------------------------------------\n\nWhile the potential of AGI is a powerful incentive for governments to race toward AGI, the incentives are not straightforward, and better dynamics are also possible.\n\nThe international response to nuclear weapons provides a useful, if imperfect, analogy. The U.S. and Soviet Union recognized that while nuclear competition was inevitable, nuclear proliferation was not in their strategic interest. This led to the Non-Proliferation Treaty (NPT), which successfully slowed the spread of nuclear weapons beyond the initial nuclear powers.\n\nDoing this successfully would be difficult. It would be harder on some routes to AGI (those with low compute requirements), and possible only if the project is started early enough. It would have to be approached with both carrots and sticks; the promise to share the material wealth from AGI-created technology would be the carrot, and AI-aided surveillance and software intrusion would be the downside. \n\nThe gap between US and Chinese precision weapons and intelligence technology and those of the rest of the world is large. It would make a treaty led by those two nations on AGI nonproliferation potentially enforcable, with the variable of how much privacy violation and surgical force the rest of the world would tolerate. As AI advanced toward AGI, enforcement would become easier - while the threat of centralized power becomes greater.\n\nIf one indulges in a bit of optimism, the potential for AGI to provide material wealth and security might shift the incentives toward cooperation enough that the US and China could actually continue cooperating into a long-term stable future that benefits all of humanity.[^t3nvz0zpz] How benevolent does one have to be to share a pie that's growing exponentially? And how wise does one need to be with superhuman advice?\n\nDon't take that for optimism; it's uncertainty which extends to positive as well as negative possibilities.\n\nHow government control of AGI could change our odds\n---------------------------------------------------\n\nGovernment control could lead to a survivable transition in two notable ways:\n\n1.  **Fewer AGI actors means fewer chances for reckless deployments**\n2.  **Fewer AGI actors mean fewer chances of malicious use of AGI**\n    1.  Those controlling AGIs would have an incentive to order vicious first-strike takeover attempts if they have unusual goals/ethics/beliefs[^lwz6pm8aseg]\n3.  **Governments typically have more patience than corporations**\n\nThese advantages are offset by the disadvantage of a likely worsened race dynamic raising the odds of rushing to deploy AGI without sufficient alignment. This could easily lead to takeover and permanent disempowerment or extinction for humanity. There is already a dangerous race dynamic between AGI companies, but government involvement would create a race between governments and cultures with a history of distrust and occasional warfare and atrocities.\n\nProvisional conclusion\n----------------------\n\nRight now, the world is stumbling toward AGI. Governments don't seem to yet understand its stakes. Corporate leadership has incentives that favor speed over caution. If AGI development turns out to require less resources, individuals or fringe groups with access to proto-AGI would be even less restrained. \n\nA shift to government control of AGI projects could be quite dangerous. It could lead to a race dynamic that either results in a misaligned runaway AGI, or in conflict between governments that escalates to an accidental catastrophic nuclear exchange.\n\nGovernment control could also be our best shot at effectively limiting AGI proliferation. The fewer independent actors that achieve and use superhuman AGI, the better our odds of some form of alignment working for every instance of it. And less proliferation means less chance that [If we solve alignment, we die anyway](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1) from misuse of intent-aligned AGI.\n\nThis doesn't seem like a particularly good strategic situation. But it is far from an obviously losing scenario.\n\nThe prospect of government control is alarming. There are good reasons for [Fear of centralized power and fear of misaligned AGI](https://www.lesswrong.com/posts/6iJrd8c9jxRstxJyE/fear-of-centralized-power-vs-fear-of-misaligned-agi-vitalik). I'm mentioning it publicly because it may be the best of a bad set of options, and there are already many voices telling the government to pay attention to AI. I have yet to hear another realistic route to limiting the development of dangerous AGI. And if it's largely inevitable that governments seize control of AGI, it may be best if they do it while proliferation can still be limited.\n\nSo we should probably figure out whether we want pull on this lever.\n\n[^8n847ch0paf]: There are many ways government could fail to control AGI even if it tries. The most notable is uncontrolled proliferation; if research is happening in many countries at similar levels as we approach AGI, all of those governments would need to coordinate. There are also many ways that US companies could evade control even if it's attempted, but most of those look to me like foolish attempts in the face of government power. A clever and ambitious leader might balance saying they're complying while still hiding progress toward AGI, or fighting in court; but these seem like dead-end strategies to me. Evading control by some government permanently seems very hard. Takeoff would need to be quite fast to evade control from the government with jurisdiction over the physical location of lab personnel and leadership if the full government is motivated to take control.I'm interested in counterarguments! \n\n[^lwz6pm8aseg]: AGI proliferation might mean a strong offensive advantage if they could create weapons in secret. People with unusual visions for the future (themselves as god-emperor, religious beliefs, etc) might be motivated to order their AGI to strike first in order to control the future, even if it means destroying most of humanity or the earth. After all, an AGI capable of self-improving should be able to create a new glorious future approximately to its master's specifications. See If we solve alignment, do we die anyway? and the discussion there. \n\n[^t3nvz0zpz]: One closely related topic that's out of scope here, but crucial to any future that includes corporate or government control of intent-aligned AGI is that of the moral character of human leaders. What individuals could be trusted with the future of humanity, and under what circumstances, is another critical question on which intuitions vary dramatically, and all of the available arguments I've found are highly incomplete. This is one more reason I wish we had more psychologists involved in alignment theory. \n\n[^z7le3zwidc]: Existing law is seems only relevant to how easy it would be for smaller government actors to get compliance just for asking. If and when the government is largely AGI-pilled, new laws and executive orders will likely be made quickly. Claude 3.7 and GPT-4.5 confirm my suspicions that there are laws against developing technology that could obviously be used for important military purposes, then letting foreign powers steal or have it.  They cite many laws that would be applicable if the courts took the full potential of AGI seriously. Just the basic definition of treason, \"don't try to overthrow the government\" might cover it. But there are more specific laws. Below I've included Claude's quick summary; I invite those more expert than me to weigh in here. ChatGPT-4.5 says largely the same things WRT applicable laws. Both were probed for counterarguments, which focused on government awareness and speed of takeoff. Legal defenses exist but seem to me unwise. I wouldn't want to appear in court saying \"oh I didn't realize it could be adapted to take over the world\" when I most certainly did. Legal systems look solid in liberal democracies in peacetime, but in perceived crisis, it looks to me like government does what it wants.Some US laws applicable to AGI development if it's taken seriouslyClaude 3.7:If government authorities and courts were convinced that AGI posed similar existential or security risks, many of the same legal frameworks [as for directly military technologies, from context] could indeed be applied:Legal Basis for AGI ControlDual-Use Technology Framework: AGI could be designated as a \"dual-use technology\" with both civilian and military applications under the Export Administration Regulations, similar to how nuclear technology is treatedNational Security Classification: If AGI were viewed as having weapons-development potential, the government could apply classification controls to core researchDefense Production Act: Could be invoked to require licensing and government oversight of AGI developmentPreventive Control Precedent: There's precedent in how the Atomic Energy Act preemptively controlled nuclear technology before specific weapons were developedKey Challenges Specific to AGIThe primary hurdle is the \"capability belief threshold\" you mentioned:Proving Capability: Unlike bioweapons or drones where the danger is immediately demonstrable, AGI's potential dangers are more theoretical until demonstratedUncertain Timeline: The government would need to determine at what development stage intervention becomes necessaryTechnical Expertise Gap: Government regulators would need sufficient technical understanding to recognize when AGI capabilities cross critical thresholdsThe most likely approach would be a graduated response - beginning with monitoring and information sharing requirements, then escalating to more direct control if capabilities approach concerning thresholds.This is essentially what happened with nuclear technology - initial light oversight that dramatically expanded once capabilities were demonstrated at Los Alamos.I didn't prompt it with the scenario of informal monitoring under the threat of legal enforcement, followed by direct control if AGI appears imminent. We concur, but we could use more expert input.ChatGPT-4.5: Companies aware of AGI’s potentially existential or geopolitical threats would face enormous liability (criminal and civil) if found negligent in protecting the technology from adversarial access.",
      "plaintextDescription": "Epistemic status: somewhat rushed out in advance of the deadline tomorrow (Saturday the 15th) for the call for public comment on US AI policy. I think the issue is complex and deserves careful consideration.\n\n\nSummary\n * Methods and motivations for governments to control AGI and limit its proliferation seem to be underexplored\n   * Current laws appear adequate to take control of AGI as a national security and military technology if its potential is taken seriously\n   * Softer informal monitoring and steering is relatively easy and can scale as AGI seems more imminent\n   * Governments obsess over national security, and AGI is a severe security risk\n * There are both dramatic advantages and disadvantages of government control for the alignment project as a whole:\n   * Government control could dramatically reduce proliferation of takeover-capable AGI\n     * Reducing opportunities for misalignment and misuse\n   * Government control could concentrate power in dangerous human hands\n     * And create a more hostile race dynamic\n * Thus, we neither know whether governments are likely to control AGI in time to reduce proliferation, nor whether that would be a good thing.\n * Publicizing reasons government should control AGI might be one of few \"levers\" available to those who are already aware of the importance and danger of AGI.\n\nIn sum, determining whether government control of AGI is worth accelerating seems worth some careful analysis.\n\n\nOverview\nMy thesis here is that governments will probably pursue AGI, and they will want to prevent others from doing so. The open questions are whether they will do so in time to effectively slow proliferation of AGI projects, and whether that would be a good thing for the odds of humanity's success with the advent of AGI.\n\nI've talked to many alignment people who are certain government won't intervene in time to matter, and others who are equally certain it will. I've heard a similar range of opinions on whether this would be good or bad",
      "wordCount": 2687
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2c7",
        "name": "Regulation and AI Risk",
        "slug": "regulation-and-ai-risk"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "2zijHz4BFFEtDCDH4",
    "title": "Will LLM agents become the first takeover-capable AGIs?",
    "slug": "will-llm-agents-become-the-first-takeover-capable-agis",
    "url": null,
    "baseScore": 37,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2025-03-02T17:15:37.056Z",
    "contents": {
      "markdown": "One of my takeaways from EA Global this year was that most alignment people aren't explicitly focused on LLM-based agents (LMAs)[^8e1mav7i8db] as a route to takeover-capable AGI. I want to better understand this position, since I estimate this path to AGI as likely enough (maybe around 60%) to be worth specific focus and concern.\n\nTwo reasons people might not care about aligning LMAs in particular:\n\n1.  Thinking this route to AGI is quite possible but that aligning LLMs mostly covers aligning LLM agents\n2.  Thinking LLM-based agents are unlikely to be the first takeover-capable AGI\n\nI'm aware of arguments/questions like [Have LLMs Generated Novel Insights?](https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights), [LLM Generality is a Timeline Crux](https://www.lesswrong.com/posts/k38sJNLk7YbJA72ST/llm-generality-is-a-timeline-crux), and LLMs' weakness on what Steve Byrnes [calls discernment](https://www.lesswrong.com/posts/2yLyT6kB7BQvTfEuZ/sharp-left-turn-discourse-an-opinionated-review#1_1_Intro): the ability to tell their better ideas/outputs from their worse ones.[^nd74p3ll2mc] I'm curious if these or other ideas play a major role in your thinking.\n\nI'm even more curious about the distribution of opinions around type 1 (aligning LLMs covers aligning LMAs) and 2 (LMAs are not a likely route to AGI) in the alignment community. [^odayedowq1f]\n\nEdit: Based on the comments, I think perhaps this question is too broadly stated. The better question is \"what *sort* of LMAs do you expect to reach takeover-capable AGI?\"   \n \n\n[^8e1mav7i8db]: For these purposes I want to consider language model agents (LMAs) broadly. I mean any sort of system that uses models that are substantially trained on human language, similar to current GPTs trained primarily to predict human language use.Agents based on language models could be systems with a lot or a little scaffolding (including but not limited to hard-coded prompts for different cognitive purposes), and other cognitive systems (including but not limited to dedicated one-shot memory systems and executive function/planning or metacognition systems). This is a large category of models, but they have important similarities for alignment purposes: LLMs generate their \"thoughts\", while other systems direct and modify those \"thoughts\", to both organizing and chaotic effect.This of course includes multimodal foundation models that include natural language training as a major component; most current things we call LLMs are technically foundation models. I think language training is the most important bit. I suspect that language training is remarkably effective because human language is a high-effort distillation of the world's semantics; but that is another story.  \n\n[^nd74p3ll2mc]: I think that humans are also relatively weak at generating novel insights, generalizing, and discernment using our System 1 processing. I think that agentic scaffolding and training is likely to improve System 2 strategies and skills similar to those humans use to scrape by in those areas.  \n\n[^odayedowq1f]: Here is my brief abstract argument for why there are no breakthroughs needed for this route to AGI, this summarizes the plan for aligning them in short timelines; and System 2 Alignment is my latest in-depth prediction on how labs will try to align them by default, and how those methods could succeed or fail.",
      "plaintextDescription": "One of my takeaways from EA Global this year was that most alignment people aren't explicitly focused on LLM-based agents (LMAs)[1] as a route to takeover-capable AGI. I want to better understand this position, since I estimate this path to AGI as likely enough (maybe around 60%) to be worth specific focus and concern.\n\nTwo reasons people might not care about aligning LMAs in particular:\n\n 1. Thinking this route to AGI is quite possible but that aligning LLMs mostly covers aligning LLM agents\n 2. Thinking LLM-based agents are unlikely to be the first takeover-capable AGI\n\nI'm aware of arguments/questions like Have LLMs Generated Novel Insights?, LLM Generality is a Timeline Crux, and LLMs' weakness on what Steve Byrnes calls discernment: the ability to tell their better ideas/outputs from their worse ones.[2] I'm curious if these or other ideas play a major role in your thinking.\n\nI'm even more curious about the distribution of opinions around type 1 (aligning LLMs covers aligning LMAs) and 2 (LMAs are not a likely route to AGI) in the alignment community. [3]\n\nEdit: Based on the comments, I think perhaps this question is too broadly stated. The better question is \"what sort of LMAs do you expect to reach takeover-capable AGI?\" \n \n\n 1. ^\n    For these purposes I want to consider language model agents (LMAs) broadly. I mean any sort of system that uses models that are substantially trained on human language, similar to current GPTs trained primarily to predict human language use.\n    \n    Agents based on language models could be systems with a lot or a little scaffolding (including but not limited to hard-coded prompts for different cognitive purposes), and other cognitive systems (including but not limited to dedicated one-shot memory systems and executive function/planning or metacognition systems). This is a large category of models, but they have important similarities for alignment purposes: LLMs generate their \"thoughts\", while other systems direct and modify t",
      "wordCount": 212
    },
    "tags": [
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "Q6hq54EXkrw8LQQE7",
        "name": "Gears-Level",
        "slug": "gears-level"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fqAJGqcPmgEHKoEE6",
    "title": "OpenAI releases GPT-4.5",
    "slug": "openai-releases-gpt-4-5",
    "url": "https://openai.com/index/introducing-gpt-4-5/",
    "baseScore": 34,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2025-02-27T21:40:45.010Z",
    "contents": {
      "markdown": "This is not o3; it is what they'd internally called Orion, a larger non-reasoning model.\n\nThey say this is their last fully non-reasoning model, but that research on both types will continue.\n\nThey say it's currently limited to Pro users, but the model hasn't yet shown up on the chooser (edit: it is available in the app). They say it will be shared with Plus and Enterprise users next week.\n\nIt claims to be more accurate at standard questions and with a lower hallucination rate than any previous OAI model (and presumably any others).\n\n\"Alignment\" was done by both supervised fine-tuning from an unspecified dataset, and RLHF  (this really only training refusals, which is pretty different from alignment in the classical sense, but could potentially help with real alignment if it's used that way - see [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment)).\n\nThe main claims are better world knowledge, better understanding of human intentions (it is modestly but distinctly preferred over 4o in their tests), and being better at writing. This suggests to me that their recent stealth upgrades of 4o might've been this model.\n\nIt does web searching and uses Canvas, and handles images.\n\nHere's the start of the system card:\n\n> OpenAI GPT-4.5 System Card\n> --------------------------\n> \n> OpenAI  \n> February 27, 2025\n\n> ### 1 Introduction\n> \n> We’re releasing a research preview of OpenAI GPT-4.5, our largest and most knowledgeable model yet. Building on GPT-4o, GPT-4.5 scales pre-training further and is designed to be more general-purpose than our powerful STEM-focused reasoning models. We trained it using new supervision techniques combined with traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), similar to those used for GPT-4o. We conducted extensive safety evaluations prior to deployment and did not find any significant increase in safety risk compared to existing models.\n> \n> Early testing shows that interacting with GPT-4.5 feels more natural. Its broader knowledge base, stronger alignment with user intent, and improved emotional intelligence make it well-suited for tasks like writing, programming, and solving practical problems—with fewer hallucinations. We’re sharing GPT-4.5 as a research preview to better understand its strengths and limitations. We’re still exploring its capabilities and are eager to see how people use it in ways we might not have expected.\n> \n> This system card outlines how we built and trained GPT-4.5, evaluated its capabilities, and strengthened safety, following OpenAI’s safety process and Preparedness Framework.\n> \n> ### 2 Model data and training\n> \n> Pushing the frontier of unsupervised learning\n> \n> We advance AI capabilities by scaling two paradigms: unsupervised learning and chain-of-thought reasoning. Scaling chain-of-thought reasoning teaches models to think before they respond, allowing them to tackle complex STEM or logic problems. In contrast, scaling unsupervised learning increases world model accuracy, decreases hallucination rates, and improves associative thinking. GPT-4.5 is our next step in scaling the unsupervised learning paradigm.\n> \n> New alignment techniques lead to better human collaboration\n> \n> As we scale our models, and they solve broader, more complex problems, it becomes increasingly important to teach them a greater understanding of human needs and intent. For GPT-4.5, we developed new, scalable alignment techniques that enable training larger and more powerful models with data derived from smaller models. These techniques allowed us to improve GPT-4.5’s steerability, understanding of nuance, and natural conversation.\n> \n> Internal testers report GPT-4.5 is warm, intuitive, and natural. When tasked with emotionally charged queries, it knows when to offer advice, diffuse frustration, or simply listen to the user. GPT-4.5 also shows stronger aesthetic intuition and creativity. It excels at helping users with their creative writing and design.\n> \n> GPT-4.5 was pre-trained and post-trained on diverse datasets, including a mix of publicly available data, proprietary data from data partnerships, and custom datasets developed in-house, which collectively contribute to the model’s robust conversational capabilities and world knowledge.\n\nSafety is limited to refusals, notably including refusals for medical or legal advice. Have they deliberately restricted those abilities to avoid lawsuits or to limit public perceptions of expertise being overtaken rapidly by AI?\n\nThey report no real change from previous safety evaluations, which seems reasonable as far as it goes. We're not to the really scary models yet, although it will be interesting to see if this produces noticably better tool-use and the type of recursive self-checking that's crucial for powering competent agents. They say it has those, and improved planning and \"execution\":\n\n> Based on early testing, developers may find GPT‑4.5 particularly useful for applications that benefit from its higher emotional intelligence and creativity—such as writing help, communication, learning, coaching, and brainstorming. It also shows strong capabilities in agentic planning and execution, including multi-step coding workflows and complex task automation.\n\nThey also say it's compute intensive, so not a replacement for 4o. This could be why they hadn't released Orion earlier. I wonder if this release is in response to Claude 3.7 taking top spots for most non-reasoning-appropriate tasks.\n\n> GPT‑4.5 is a very large and compute-intensive model, making it more [expensive⁠](https://openai.com/api/pricing/) than and not a replacement for GPT‑4o. Because of this, we’re evaluating whether to continue serving it in the API long-term as we balance supporting current capabilities with building future models.",
      "plaintextDescription": "This is not o3; it is what they'd internally called Orion, a larger non-reasoning model.\n\nThey say this is their last fully non-reasoning model, but that research on both types will continue.\n\nThey say it's currently limited to Pro users, but the model hasn't yet shown up on the chooser (edit: it is available in the app). They say it will be shared with Plus and Enterprise users next week.\n\nIt claims to be more accurate at standard questions and with a lower hallucination rate than any previous OAI model (and presumably any others).\n\n\"Alignment\" was done by both supervised fine-tuning from an unspecified dataset, and RLHF  (this really only training refusals, which is pretty different from alignment in the classical sense, but could potentially help with real alignment if it's used that way - see System 2 Alignment).\n\nThe main claims are better world knowledge, better understanding of human intentions (it is modestly but distinctly preferred over 4o in their tests), and being better at writing. This suggests to me that their recent stealth upgrades of 4o might've been this model.\n\nIt does web searching and uses Canvas, and handles images.\n\nHere's the start of the system card:\n\n\n> OpenAI GPT-4.5 System Card\n> OpenAI\n> February 27, 2025\n\n\n> 1 Introduction\n> We’re releasing a research preview of OpenAI GPT-4.5, our largest and most knowledgeable model yet. Building on GPT-4o, GPT-4.5 scales pre-training further and is designed to be more general-purpose than our powerful STEM-focused reasoning models. We trained it using new supervision techniques combined with traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), similar to those used for GPT-4o. We conducted extensive safety evaluations prior to deployment and did not find any significant increase in safety risk compared to existing models.\n> \n> Early testing shows that interacting with GPT-4.5 feels more natural. Its broader knowledge base, stronger alignment wi",
      "wordCount": 869
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cus5CGmLrjBRgcPSF",
    "title": "System 2 Alignment",
    "slug": "system-2-alignment",
    "url": null,
    "baseScore": 35,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-02-13T19:17:56.868Z",
    "contents": {
      "markdown": "Submitted to Alignment Forum. Contains more technical jargon than usual.\n\n*Epistemic status:  I'm pretty sure these are important questions, but I don't have firm conclusions. I'm hoping to inspire others to think in this direction. ~50% of the average value is probably in the Summary; another ~15% each in the next Overview and OpenAI's Deliberative Alignment sections. The second half is more detailed, and intended for those who want to think about aligning language model agents as AGI.*\n\n### **Summary**: \n\nDemand for LLM-based agents that can perform useful work may lead all the way to takeover-capable AGI that needs real alignment. Here I discuss alignment techniques that developers are likely to use for that type of AGI. Accurate gears-level models of likely AGI and alignment approaches can effectively give us more time to work on alignment.\n\nSuch agents will probably use chains of thought or *System 2 cognition*[^l7qiuvbpip8] extensively. Some System 2 human strategies seem likely to be adapted to improve capabilities, and minimize costs and risks for users and developers of LLM-powered agents. Those methods can be repurposed for alignment at low cost, so they probably will be. \n\nI roughly divide such strategies/methods into ***review*****,** ***deliberation*****,** and  ***thought management.*** I discuss these methods and possible variations in how they might be implemented. This analysis is intended as a starting point for more detailed work.\n\n**Review** of plans and actions is the method I've thought most about. It could:\n\n1.  Intervene before any tool calls or goal/memory updates are allowed\n2.  Reference a human-written constitution\n3.  Call a fresh LLM instance for independent review\n4.  Check for alignment-altering changes to beliefs or subgoals\n5.  Scale with estimated importance of decisions for low alignment taxes\n6.  Work to some degree even without a faithful chain of thought\n\nSystem 2 alignment could prevent alignment-altering belief changes or subgoal adoption. In this and other ways, these approaches could go beyond \"aligning\" the behavior of an LLM, and help with real alignment of the goals of an AGI agent with those of humanity (or at least a human[^xbz8yqpb2u9]).\n\nDepending on their implementation, such methods might be regarded by an agent as a meddling \"thought editor\", or as a valued conscience. \n\nOpenAI recently announced [*deliberative alignment*](https://openai.com/index/deliberative-alignment/)[^tq3hu3ajwm], after this post was partly written. I discuss its implementation. It is one example of the category I address here: capabilities techniques adapted to control or alignment.\n\n1\\. Overview\n============\n\nLet's start with review as the clearest illustration of why people would bother developing specific System 2 techniques even if they didn't care much about AGI alignment.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ddb13ebb5d6d4e0fc0a8da024aaa6f7d7622bcecc1965c29.png)\n\nA review process, standing between an agent's main cognitive engine (an LLM or foundation model) and actions-you-don't-like, such as writing fanfic porn, or making plans to overthrow humanity.\n\nWithout ***reviewing*** likely consequences before taking important action, humans would be even dumber and more dangerous. We'd pick more fights we can't win, blow more money, and drop more bombs we'll regret. We'd also adopt more dangerous beliefs and ideologies, changing our goals and values in ways we wouldn't have endorsed on reflection. Review before finalizing important decisions is one type of System 2 cognition.[^l7qiuvbpip8] \n\nLanguage model agents (LMAs)[^nyg0g7nbaun] will be prone to similar errors. On the current trajectory, they will employ long, complex chains of thought. These will probably interact across levels; e.g., planning and subtask execution. They will have \"habits\" of thought and action learned from predictive pretraining, and a variety of fine-tuning and RL processes. They will have implicit goals from those sources, and explicit goals from developer and user prompts. And any type of continuous learning will change how they interpret those goals. \n\nBefore asking whether the goals we give advanced LLM agents will suffice for real alignment, we must ask whether they'll reliably even try to pursue those goals. This is a specific form of the problem Nate Soares refers to in [\"alignment is in large part about making cognition aimable at all\"](https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making). In [Seven sources of goals in LLM agents](https://www.lesswrong.com/posts/nHDhst47yzDCpGstx/seven-sources-of-goals-in-llm-agents) I briefly discussed this problem and a possible solution: improved cognition and metacognition to produce reflective stability around the strongest (and hopefully most desirable) goals.\n\n***Review***, ***deliberation*** and ***thought management*** are System 2 strategies that can improve cognition and metacognition. Humans ***deliberate*** by sampling and comparing different thoughts and lines of thought on the same topic. This deliberation improves our decisions, plans, and reasoning. ***Thought management*** is roughly thinking \"what should I be thinking about?\" It can reduce unproductive or costly chains of thought and focus cognition on useful subgoals. Each of these improves our cognition, including allowing it to adhere better to our chosen goals. Thus, these strategies are dual-use for capabilities and \"alignment\" with our own most important goals.\n\nThese strategies can serve the same purposes for LLM agents. Deliberation helps get good results in a variety of ways, as demonstrated by OpenAI's [o1](https://openai.com/o1/) and [o3](https://community.openai.com/t/day-12-of-shipmas-new-frontier-models-o3-and-o3-mini-announcement/1061818) performance, and their \"deliberative alignment\". Thought management will become increasingly useful as agents become capable of blowing hundreds of dollars on compute for a single unnecessary subtask (e.g., \"find the best nearby restaurants\" could take an unlimited amount of research and deliberation, and 03 spends [up to $3000](https://www.reddit.com/r/singularity/comments/1hisp7o/o3_high_compute_costs_is_insane_3000_for_a_single/) per hard problem). And review can prevent harmful or risky external and internal actions, as outlined above.\n\nImplementing these strategies for capabilities and cost/risk reduction will serve double duty for alignment. Using them will be almost free in terms of development effort, and each seems to carry low compute costs. Shane Legg has referred to this approach as System 2 safety,[^i6nra6iwfh8] and I adopt that terminology since it is more intuitive and general than my previous \"[internal independent review for alignment](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent)\"\n\nInitially, those techniques will be used for behavioral control and control of thought. But as they're applied to autonomous agents that are truly goal-directed, that type of thought control can be applied to preventing the agent's interpretation of its goals from changing in dangerous ways. That will be relevant to alignment in its deeper sense: aligning [“Real AGI”](https://www.lesswrong.com/posts/YW249knFccwATGxki/real-agi) that is competent, autonomous, and goal-directed — and therefore very dangerous\n\nMy broadest point here is that many techniques developers can use for getting an LLM or agent to behave in ways they want are also techniques that could be used for real alignment in the deepest sense. I also think we can make enough specific predictions about those mechanisms to start reasoning about how they might succeed or fail. That is the subject of the remainder of the article.\n\nDepending on how they're implemented, System 2 alignment could be thought of as an \"artificial conscience\" intrinsic to the agent's identity, or as a \"prosthetic conscience\" trying to exert unwanted control of actions and thoughts. \n\nChain of thought alignment is often identified with having faithful chains of thought and applying external review. These approaches do not strictly require faithful CoT, because they can use a different instance of the same model for supervision. See section [System 2 alignment with and without faithful CoT](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#System_2_alignment_with_and_without_faithful_CoT).\n\nIt seems obviously valuable to correctly anticipate how we're going to try to align our first AGI(s). That's the primary focus of this article. I also try to make progress toward evaluating whether these approaches are likely to work, but I don't think we can draw even preliminary conclusions without further work. I hope we can make progress on that work before finding out (if it's even made public) what alignment techniques are actually employed for the first really dangerous AGIs if they come in the form of language model agents.\n\n1.1 Structure of the remainder\n------------------------------\n\n1.  [Summary](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Summary__) and [Overview](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Overview) \\-  You just finished those sections.\n2.  OpenAI's [\"Deliberative alignment\" in o1 and o3](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#_Deliberative_alignment__in_o1_and_o3)\n3.  [System 2 alignment approaches](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#System_2_alignment_approaches)\n    1.  [Deliberation](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Deliberation_for_capabilities), [Review](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Review_), and [Thought Management](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Thought_Management)\n        1.  capabilities, control, and alignment for each\n4.  [Implementation variations of System 2 techniques.](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Implementation_variations_of_System_2_techniques_)\n    1.  [Trained vs. scaffolded/scripted System 2 approaches](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Trained_vs__scaffolded_scripted_System_2_approaches)\n    2.  [System 2 alignment with and without faithful CoT](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#System_2_alignment_with_and_without_faithful_CoT)\n    3.  [Self- vs independent review and management](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Self__vs_independent_review_and_management)\n    4.  [Human vs. AI review and management](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Human_vs__AI_review_and_management)\n5.  [Will System 2 alignment work for real AGI?](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Will_System_2_alignment_work_for_real_AGI_)\n    1.  [Will System 2 alignment make LLM agents aimable at all?](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Will_System_2_alignment_make_LLM_agents_aimable_at_all__)\n    2.  [If system 2 alignment works, can we specify goals well enough to survive the results?](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#If_system_2_alignment_works__can_we_specify_goals_well_enough_to_survive_the_results___)\n\n2\\. \"Deliberative alignment\" in o1 and o3\n=========================================\n\nOpenAI [announced deliberative alignment](https://openai.com/index/deliberative-alignment/) in December 2024. It repurposes the new training method for o1 and o3's CoT capabilities, using it to train separately for \"alignment\". \n\nUsing RL on chains of thought produced dramatic improvements in o1 and o3 over all previous LLMs. The same procedure improves refusals of requests for information deemed harmful, when that's used as the training criteria. Thus, deliberative alignment is an example of how System 2 capabilities techniques can be repurposed easily for control/alignment.\n\nThankfully, OpenAI didn't publish their full method for o1, so we have to speculate on exactly how they applied RL to chains of thought. See this footnote[^n2xtf9pei5b] for more.  We have more direct reports of how they trained deliberative alignment; see [their paper](https://assets.ctfassets.net/kftzwdyauwt9/4pNYAZteAQXWtloDdANQ7L/0aedc43a8f2d1e5c71c5e114d287593f/OpenAI_Deliberative-Alignment-Reasoning-Enables-Safer_Language-Models_122024_3.pdf). The first stage is identical to Anthropic's \"constitutional AI\" method of training with RLAIF. The second stage uses RL on a chain of thought (CoT) in a straightforward way, by evaluating the final response only for appropriate refusal. The reinforcement value is produced by another LLM judging how well the response obeys the \"spec\" that describes what questions the model should refuse to answer. See this footnote[^tq3hu3ajwm] for more explanation of deliberative alignment training. \n\nI don't think \"deliberative alignment\" is really an alignment technique as it stands. It's used for controlling behavioral tendencies of a system that's not strongly goal-directed, which is a different thing.[^cxrwwcnbcqb] But it and similar training techniques could potentially be used for real alignment. See the sections [Deliberation for control and alignment](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Deliberation_for_control_and_alignment) and [Will System 2 alignment work for real AGI?](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Will_System_2_alignment_work_for_real_AGI_)\n\nBut deliberative alignment works for its purpose. I find the tests they ran pretty compelling. It's not a huge improvement, but it's not trivial. I see no strong reason to doubt that it works for the reason they give: the model deliberates about whether or not it should answer a given question, and this allows it to catch tricks that would otherwise fool an RLHF/RLAIF system. The example they give in the paper is a CoT reasoning about whether to answer a request framed as a cipher. This trick fools RLHF/RLAIF systems, but in the course of thinking through the problem (deliberating), o1 realizes its spec forbids answering.\n\nSimilar training methods could be used to produce a variety of System 2 cognitive strategies and skills, including review, deliberation, and thought management. You'd only need to change the evaluation criteria of the LLM serving as a reward model. (DeepSeek R1 reportedly does this to improve performance on non-reasoning tasks, as a final step). \n\n3\\. System 2 alignment approaches\n=================================\n\n*Note to the reader: the remaining sections are more detailed, and will be of interest primarily to those who think it's worthwhile to develop gears-level models of possible future LMA agents and the specific techniques that might be used to align them. The concluding section* [*Will System 2 alignment work for real AGI?*](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Will_System_2_alignment_work_for_real_AGI_) *may be of more general interest.*\n\n3.1 Deliberation for capabilities\n---------------------------------\n\nFrom [the announcement post](https://openai.com/index/learning-to-reason-with-llms/) for o1, and corroborated by reports from OpenAI people who've used it and read its chains of thought:\n\n> Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses.\n> \n> It learns to recognize and correct its mistakes.\n> \n> It learns to break down tricky steps into simpler ones.\n> \n> It learns to try a different approach when the current one isn’t working.\n> \n> This process dramatically improves the model’s ability to reason. \n\nWe could call all of this deliberation, or we could note that it's doing some things we might call review (recognizing mistakes) and thought management (trying a different approach when one isn't working). Review, deliberation, and thought management aren't definitive natural categories. But they seem usefully different.I use the term here for everything that doesn't fall under review or thought management. \n\nThe incentives for improving deliberation seem large and apparent: it has produced large improvements in o1 and o3 over previous models, and seems likely to produce more. Some of those new capabilities have already been applied to alignment.\n\n### **3.1.1 Deliberation for control and alignment**\n\nOpenAI's use of a separate training signal to create \"deliberative alignment\" indicates that similar techniques can be used to apply deliberation to any purpose - at least to some degree.[^infuf5mww0q] The evaluator model that provides the RL signal for each chain of thought could be given any criteria for its judgment.\n\nThis training technique causes the model to include the control/alignment criteria naturally in its chain of thought, such that it tends to produce outputs/conclusions consistent with the criteria. Scaffolded prompting approaches could be used to influence chains of thought in similar ways (e.g., periodically inserting \"remember, we're working towards \\[goal\\] periodically in a chain of thought. See the section on [Trained vs. scaffolded/scripted System 2 approaches](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Trained_vs__scaffolded_scripted_System_2_approaches). But I assume training approaches by default since they have produced the best results so far.\n\n### **3.1.2 Example criteria for control/alignment targets of deliberation**\n\nTo think about the role deliberation in alignment, it's useful to think about the different ways developers might try to use it. Below are some targets for control and alignment that people have talked about; many more are possible. They illustrate the range of uses, from trying to control an LLM or simple agent, to aligning a sophisticated LLM agent AGI.\n\nExample targets for deliberation: \n\n*   Did this CoT answer questions that violate these rules?\n    *   Current OpenAI deliberative alignment \"spec\" training criteria\n        *   Control of behavioral tendencies\n*   Was this CoT helpful, harmless, and honest?\n    *   Anthropic's \"constitution\" could be extended to guide CoT\n        *   (Helpful to whom, and for what?)\n*   Did this CoT advance toward humanity's Coherent Extrapolated Volition?\n    *   Or some other value alignment target\n*   Did this CoT follow developer instructions as intended?\n    *   Developer instructions probably include following some but not all end-user instructions\n    *   Instruction-following, DWIMAC, or roughly Corrigibility as Singular Target[^xbz8yqpb2u9]\n\n3.2 Review for risks, control, and alignment of actions.\n--------------------------------------------------------\n\nThe opening diagram illustrated why you'd want a robust review of actions and important internal decisions for your agent architecture. It could catch internal or external actions that violate rules or create unwanted risks or costs of many types. Such a review could be scripted into the agent architecture, or instilled through training.\n\nThe term review is not an ideal fit. Preview or just System 2 decision-making might be better if we were just talking about an agent reviewing its own actions. I chose review to emphasize the possibility of having a separate, independent process review the proposed decision before it was finalized. More on this in [Internal independent review for language model agent alignment](https://www.lesswrong.com/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent).\n\nThe basic idea is that it's a good idea to think carefully about the likely consequences of important actions before you make them. You can think more carefully about more important decisions, if you can tell which ones are important. This applies to the material consequences of decisions, like how well they're likely to accomplish goals, and their risks of real harm, monetary costs or reputational costs. And it applies to the ethical consequences of decisions , like following a set of rules, ethical goals, or conforming with values.\n\nI talked about review in the context of particular of language model agent architectures in [Capabilities and alignment of LLM cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures). Review could use any of the goals/criteria mentioned above under the [deliberation](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Example_criteria_for_behavior_alignment_targets_of_deliberation) section. Here are a few more that are particularly relevant to review:\n\n*   Would this action provide information to a user that might be harmful?\n*   Would this action spend the user's money or compute costs in ways they'd dislike?\n*   Would this action work against any of my goals (including reducing cost/risk)?\n    *   If so, reference a resolution order for goals\n    *   And possibly call for human review before proceeding\n*   Would this action update my beliefs in ways that might change my effective alignment?\n    *   e.g., deciding that the term \"person\" really means any sapient or sentient entity\n*   Would adopting this sub-goal violate my constitution or top-level goals?\n    *   e.g., creating a new subgoal to prevent the humans from finding out about this plan or changing my goals.\n    *   If so, flag this decision for [external review.](https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for)\n\nReviewing new subgoals before pursuing them is also a form of thought management; these categories of System 2 alignment don't decompose cleanly. See [Self- vs independent](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Self__vs_independent_review_and_management) and [with and without faithful CoT](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#System_2_alignment_with_and_without_faithful_CoT) sections, below.\n\nFor more on review, see [Internal independent review for language model agent alignment](https://www.lesswrong.com/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent). That vision of agents and reviews is somewhat out of date at an ancient year-and-a-half old. I was envisioning scripted/architected reviews, but much of the logic applies to trained-in reviews.\n\n3.3 Thought Management\n----------------------\n\nMetacognition for self-management is an important cognitive skill for humans in accomplishing difficult tasks, so developing it specifically in AI/AGI might also be useful for capabilities. \n\nThought management is roughly the metacognitive equivalent of deliberation. Instead of asking the object-level question \"what would accomplish this goal/ what would work against this goal being accomplished\" it's asking the metacognitive question \"am I thinking about things that will help me accomplish this goal/am I thinking about things that will prevent me from accomplishing this goal.\" \n\nIf they're developed to improve capabilities and reduce costs, thought management methods could be repurposed for control or alignment.  \n\n### **3.3.1 Incentives for developing thought management methods**\n\nWhy bother with extra thought management techniques? o3 manages its thoughts *just fine* on its own!  But it's not footing the bill for all of that thinking. And it's good at solving complex problems, but no current LLM has adequate metacognition to manage long time-horizon tasks. \n\nThere's a distinct cognitive skill involved in managing complex tasks with lots of sub-tasks. Revisiting the project plan and evaluating whether you are succeeding or if you need to take a new approach for this sub-task is essential for any complex project. Developing tools for thought management might improve agents' ability to do this. It could also allow a hybrid approach in which humans could step in to help manage complex tasks. [OpenAI's Operator](https://openai.com/index/introducing-operator/) agent already takes this hybrid approach in a small way, although it seems too slow and incompetent to accomplish much of use even with human help.\n\nControlling cost could be another reason to implement thought management capabilities for human or agent use. We don't know how expensive useful LMAs will be, but it seems pretty likely that costs won't be totally trivial. It also seems quite possible that thought management techniques might be quite useful for agents accomplishing long time-horizon tasks. \n\nBeyond doing it to serve as managers, users will want to control what agents think about to satisfy their urge to micromanage. Unlike an employee, an agent won't get frustrated with being micromanaged.\n\nBelow is a more detail on the incentives for developers to focus on thought management capabilities for LLMs. Read them at your option. (The weird extra spacing because my collapsible sections want to eat each other.)\n\n+++ **3.3.2 \"9s of reliability\" vs better thought management for long time-horizon tasks**\n\nThere's a commonly expressed belief that what stands between LLM agents and usefully performing long time-horizon tasks is \"9s of reliability\" - performing the individual subtasks that make up long time-horizon tasks with enough reliability that the odds of completing all of the component tasks successfully is high. The logic goes that with a mere 90% (one \"9\") chance of performing each of 5 subtasks, there's only about a 60% chance (.9^5) of succeeding at all of them, and therefore about a 40% chance of at best wasting all of that time and compute, and at worst passing off an erroneous final product, and screwing up any bigger project this task was contributing to.\n\nThat all makes sense, but it's pretty clearly just not how humans succeed at long time-horizon tasks with lots of subtasks.\n\nHumans make *tons* of mistakes. We're just (sometimes) good at catching our mistakes. This is largely how we succeed at long time-horizon tasks: by thinking specifically about whether each task has been accomplished correctly, or if we need to take another swing at it. This is another form of metacognition and thought management, and another incentive for developing specific thought management techniques for LMAs. \n\nHaving humans perform the role of managing an LMA's thinking is an obvious solution. We do the extreme case of this now: we issue cognitive tasks to language models, assemble those subtasks into tasks, and take the actions to put useful results out into the world. We are centaur \"agents\" along with our assistant language models.\n\nDeveloping techniques that can slowly shift the burden of management off of humans seems quite valuable. Doing this requires a means of having the humans understand what the agent is doing and to intervene. We'd like an agent to ask for input at critical decision points in its problem-solving process. And we want it to ask for input flexibly, so that we can tell it to ask for input but proceed if we're not around, stop until it gets it, or just take its best guess. \n\nInterestingly, allowing effective human supervision may be low-hanging fruit, and it may be a powerful incentive to keep chains of thought faithful, so that humans can understand and thereby approve or correct the agent's thinking (see section [System 2 alignment with and without faithful CoT](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#System_2_alignment_with_and_without_faithful_CoT))\n\n\n\n\n+++\n\n* * *\n\n+++ **3.3.3 Thought management may reduce compute costs**\n\nCurrently OpenAI is footing the bill for o1 and o3's thinking. But it seems like they're going to have to pass the costs on to users at some point. [Altman says](https://x.com/sama/status/1876104315296968813) they're losing money on the $200 subscription to Pro. Those are particularly enthusiastic users, but they're only human: they can only keep o1 \"thinking\" so much with their prompts. When users can launch agents that think for much longer, compute costs will skyrocket. You could just set a hard cap on each agent's \"thought budget\", but that will waste a bunch of time and money when they hit that cap without getting answers. Better solutions will be in demand.\n\nBetter System 2 skills means longer CoT gets you better performance. And that's even before o3 is integrated into an agent that can go do unlimited amounts of background research on the web (which we all know is a potentially unlimited source of answers, and wasted time and cognition).\n\nFrom [Implications of the inference scaling paradigm for AI safety](https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety):\n\n> Running o3 at maximum performance is currently very expensive, with single ARC-AGI tasks [costing ~$3k](https://www.reddit.com/r/singularity/comments/1hisp7o/o3_high_compute_costs_is_insane_3000_for_a_single/), but inference costs are [falling by ~10x/year](https://a16z.com/llmflation-llm-inference-cost/)!\n\nSo there's going to be a tension between falling compute costs and running more cognition to get better answers and task performance. Good reasoning is expensive. Costs could come down, or they could go up.\n\nEarly forays into competent language model agents rely heavily on image processing (to avoid errors in parsing complex HTML). Image processing also costs a good bit of compute at this point, but this will also fall rapidly. This seems like a smaller factor since better image processing isn't as important as better (and perhaps more compute demanding) chains of thought as agents progress.\n\nI'm least sure about the compute cost incentives for thought management, but it seems fairly likely to be valuable in reducing costs. \n\n\n\n\n+++\n\n* * *\n\n+++ **3.3.4 'Humans will want to micromanage agents' thinking**\n\nAn ancient professor in my grad school department had a sign on his door: **\"***The strongest human drive is not for sex or money, it is t**o** ~correct~ ~edit~ improve another's writing\". *\n\nI think this applies to correcting by micromanaging, too.\n\nIn addition to its possible benefits for long time-horizon task capabilities and managing compute costs, the ability to supervise agents will be appealing to users for emotional reasons. Humans often love to micro-manage their subordinates. We sometimes keep that urge in check when we're dealing with a human, knowing it's likely to irritate them, but we'll want to indulge this urge with our faithful LLM agents. \n\nIt is already frustrating to watch an agent pursue a useless train of thought or project. Unlike with our kid or our employee, no part of our brain will be saying \"no don't rob them of their agency!\" Most humans would *love* to closely manage an LLM agent's thinking.\n\n\n\n\n+++\n\n* * *\n\n### **3.3.5 Thought management for control and alignment**\n\nThought management could be used for all of the same control and alignment goals as deliberation and review.\n\nCategories of goals:\n\n*   A user goal (capabilities)\n*   A developer goal (control/safety)\n*   The agent's own goals (real alignment; reliability and stability)\n\nThe [Example criteria for behavior/alignment targets of deliberation](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#Example_criteria_for_behavior_alignment_targets_of_deliberation) above also apply to thought management as well as review.\n\n4\\. Implementation variations of System 2 techniques.\n=====================================================\n\nSystem 2 techniques could to capabilities and alignment could be implemented by developers in a variety of ways. These variations will affect their usefulness and reliability in a variety of ways. \n\nSome implementational variations are:\n\n*   Trained vs. scaffolded/scripted\n*   With or without faithful chain of thought\n*   Self- or independent review/management\n    *   Calling a different model or fresh instance\n*   Automated or user review/management\n\nIt's hard to think through all of the potential combinations and how they'll affect alignment efforts. But it seems important to try, so I'll say a little about each. I hope this will spark more thoughts, and I'd like to hear them.\n\n4.1 Trained vs. scaffolded/scripted System 2 approaches\n-------------------------------------------------------\n\nEach of these System 2 approaches could employ either training methods, or architecting/scripted prompting methods. I can't really tell which or what combination is most likely, and I can't really say either which would be better for alignment. So I'm going to recommend you skip the rest of this unless you really want to think harder about the details of language model agent implementation, and help me figure out where it's going - then what that means for how well it would work.\n\nTo a first approximation, training and scripting serve the same purposes and have roughly the same potentials to be repurposed for control and real alignment.\n\nTraining goals into CoT raises major concerns. See [If system 2 alignment works, can we specify goals well enough to survive the results?](https://www.lesswrong.com/editPost?postId=cus5CGmLrjBRgcPSF&key=5dcb45f2a00fbff65df0d5d1334927#If_system_2_alignment_works__can_we_specify_goals_well_enough_to_survive_the_results___)\n\n+++ Speculations on training vs. algorithms/scaffolding\n\nSystem 2 cognitive strategies might be trained in to the base LLMs, as in deliberative alignment. A similar training method could be used to train the system to perform careful reviews, or to perform thought management by periodically explicitly thinking about which lines of attack on a problem are promising, and which should be discarded or revisited.\n\nFor deliberation, the general use of chain-of-thought for capabilities, training seems well ahead. But that doesn't mean training will be used to the exclusion of scaffolded/algorithmic approaches. The two could work synergistically for some applications.\n\nAs LLMs are used to drive agents, new opportunities and incentives for scripting/architecting come into play. For instance, reviews can be called before any tool call is accessed, and an algorithmic call to a fresh LLM instance makes the review independent, which is useful for practical control as well as alignment. For more, see [Capabilities and alignment of LLM cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) and [Internal independent review for language model agent alignment.   ](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent)\n\nWhatever combination is employed will surely impact their potential use for control and alignment. But I haven't yet formed detailed enough mental models to say how.\n\nThoughts on this topic would be appreciated.\n\n\n\n\n+++\n\n4.2 System 2 alignment with and without faithful CoT\n----------------------------------------------------\n\nChain of thought alignment is often identified with external oversight and therefore faithful chain of thought. But training methods for System 2 cognition may work reasonably well without faithful CoT. Even calling fresh instances of the model, as in independent review or management, may work reasonably well. The model will, to some degree, understand its own use of steganography or unique jargon. \n\nIf this is the case, it's an immense advantage. It seems likely that a fresh model instance will follow prompts like \"tell me what this chain of thought is saying\". Truly intelligent mesa-optimized processes might be able to collude with context-free instances of themselves, but I tend to worry much more about the alignment of intelligent processes we created on purpose long before we've got enough optimization power to have full agency and deception emerge from subtle training pressures.\n\nHowever, intensive RL on CoT may drive essentially all of the useful cognition \"underground\" into the hidden states, making the tokens it produces almost unrelated to cognition-in-context.[^8ryvsh2vt1k]\n\nIf System 2 cognition and alignment methods seem to work without faithful CoT, that's obviously a dangerous thing. It will be very hard to tell whether they really work, or if the model is mesa-optimizing for something else.\n\nI find it difficult to predict how quickly RL training on CoTs will obscure faithful CoT.   I also have a hard time thinking through how well fresh instances of the same model might interpret obscured CoTs with limited context. On the one hand, there's nothing stopping the model from learning to do all of its cognition in a completely context-dependent way, with the output tokens having little relation to the underlying network states. On the other hand, the world does seem to decompose into analyzable pieces, and the imitation training sets the model up to think that way.  \n\nMaintaining a faithful chain of thought does seem like the best way to improve our odds of survival. But it's not completely clear to me that we're done for without it.I think this is going to be an important area for thinking through and clarifying. For a little more and some references, see this footnote.[^8ryvsh2vt1k]\n\n4.3 Self- vs independent review and management\n----------------------------------------------\n\nIf you ask a person or organization to review their own plans for safety, it usually doesn't work as well as having someone else do it. The prompts \"do you really want to do this?\" or \"do you really want to think this?\" are usually answered \"yes - that's why I'm planning to do it!\".\n\nTherefore, it probably makes sense to call a different model, or at least a fresh instance of the same model with reduced context to serve the role of reviewer or thought manager.\n\nMuch of our concern about misaligned actions/plans/thoughts comes from things going wrong during an extended chain of thought. The model may talk itself into adopting a misaligned goal, or be subject to prompt injection, or jailbreaking by the user.  It might even simulate a villainous persona, then get stuck in it. That last sounds strange, but seems not entirely implausible.[^vq7bcxuwu3]\n\nHow to enforce those outside decisions is a little more of puzzle if we're talking about a thoroughly trained process. Getting a prompt \"no go do something else\" injected into context and then letting the model resume may not be effective at all. I don't have a good solution to this issue; I'll just note again that developers will have incentives to produce them.\n\n4.4 Human vs. AI review and management\n--------------------------------------\n\nThe sections on thought management and review list some reasons that humans might want to perform some review and thought management themselves.\n\nIt's also worth saying a few words about human and automated external reviews, as proposed by Tamera Lanham in 2022 in[ Externalized reasoning oversight: a research direction for language model alignment](https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for) would probably be performed mostly by independent models, assisted by humans when results are alarming. This, of course, is largely dependent on faithful chain of thought. \n\nI do worry that external review would be performed sloppily, and provide as much false sense of security as real security. But it could help as part of a swiss cheese approach to alignment (just make sure the holes don't line up! Wentworth is all too convincing that sycophantic AI and monkey-brained humans are an ideal comedy duo for working on alignment). \n\nExternal review would be one element of a scalable oversight approach to alignment, and System 2 independent (automated) internal review would complement it both by providing a separate review in real time, and by reporting its review conclusions and surrounding context, thus giving an external oversight process substantially more information to work with.\n\n**5\\. Will System 2 alignment work for real AGI?**\n==================================================\n\nI don't know. And I don't think anyone else can say for sure at this point, either. My main point here is that people might very well try to align the first really dangerous AGIs with these techniques, so should think harder about whether they will work.\n\n\"Agent foundations\" alignment theory focuses on the question of how you might adequately specify a goal that won't kill or severely disappoint you if it's optimized for very strongly. Thinking about the complexity of LLM agent cognition makes another question apparent: will it even try to follow the goal you tried to give it, or will it follow some other goal(s) implicit in the training set?  Nate Soares' [This is what I mean by \"alignment is in large part about making cognition aimable at all\"](https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making) treats this question in a more general way, and is well worth a careful read. \n\n5.1 Will System 2 alignment make LLM agents aimable at all? \n------------------------------------------------------------\n\nAnswering this question is the primary goal of this project. Predicting methods of alignment gives us traction in predicting how well they'll work. But this is only a start at this project. I can't yet even really guess the odds of System 2 alignment working to make LLM agents aimable.\n\n In [Seven sources of goals in LLM agents](https://www.lesswrong.com/posts/nHDhst47yzDCpGstx/seven-sources-of-goals-in-llm-agents) I briefly sketched out the complexity of how LLM agents will choose goals and pseudo-goals.  The system 2 alignment techniques here are all means of improving metacognition and goal-directedness. They are one means of making agents aimable. Improvements in general intelligence will also make these approaches work better. Improving general intelligence will probably also directly improve metacognition and coherence. \n\nI suspect that these methods can produce an aimable AGI, and even do that easily enough that a rushed team of humans with unreliable AI help might accomplish it. But I'd really like to be more certain. So I'll be working more on these questions, and I hope others will think about it, share their thoughts, and perhaps others will want to join in this project.\n\nEven if these methods do work, making AGI aimable won't be adequate if we can't aim it at an alignment target we actually want to hit.\n\n5.2 If System 2 alignment works, can we specify goals well enough to survive the results?  \n-------------------------------------------------------------------------------------------\n\nThe System 2 alignment techniques I've discussed are mostly relevant to making agents aimable. But they may also have some use in specifying goals well enough to work, since they provide different avenues of training or prompting for goals.\n\nThis will probably seem to work quite well initially. But we shouldn't be complecent about the apparent ease of aligning LLM agents. Many people who've thought long and hard about alignment think that aligning network-based AGI is very likely to doom us, for subtle but powerful reasons. Disregarding these warnings without understanding them thoroughly would be a huge mistake. And I'm afraid a lot of prosaic alignment thinkers are doing just that. \n\nZvi's [On A List of Lethalities](https://www.lesswrong.com/posts/LLRtjkvh9AackwuNB/on-a-list-of-lethalities) is a good starting point for the deep arguments for alignment being difficult. The [List itself](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) is required careful reading for anyone working on alignment. One critical point that's easy to overlook is concerns about humans cognitive biases and limitations, and how badly and shortsightedly groups of humans make decisions, particularly when they're under pressure. This seems to be one of several key [cruxes of disagreement on alignment difficulty](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=FpdvoZsmmrNLekkz9). \n\nZach Davis's [Simplicia/Doomimir debates](https://www.lesswrong.com/posts/8yCXeafJo67tYe5L4/and-all-the-shoggoths-merely-players) is the best high-level summary of the tension between the prosaic optimism of \"LLMs basically understand and do what we tell them\" and the deep arguments for the difficulty of choosing and specifying any goal well enough that we still like the results once it's been highly optimized. We should expect extreme optimization pressures by default; if we don't use excessive goal-directed training, highly intelligent agents will exert optimization pressures as they learn and think. And we should expect agents. Not only do [Tool AIs Want to Be Agent AIs,](https://gwern.net/tool-ai) but humans want them to be agents. Users want their work done for them, not to simply have help doing the work. \n\nThese arguments are powerful and deeply concerning. But I sharply disagree with the pessimists that aligning LLM-based AGI is obviously doomed. The arguments on both sides are incomplete.\n\nWould it work to give a powerful LLM agent with working System 2 alignment the central goal \"be nice to people and make the world a better place, and do that the way the people who wrote this meant it\"? Maybe. Language does generalize pretty well.  But we might be disappointed by the way the agent interpreted that statement. And if we didn't like its interpretation, we probably couldn't correct it. \n\nMy own current favorite alignment target is instruction-following, something like \"do what the authorized user means by their instruction, and check that you've understood it correctly before executing anything they might be really disappointed with\". This type of goal includes a good deal of [corrigibility](https://www.lesswrong.com/posts/NQK8KHSrZRF5erTba/0-cast-corrigibility-as-singular-target-1). [I prefer instruction-following intent alignment for several reasons,](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than) principally that it's very similar to what developers will probably do by default. There's no use coming up with the perfect alignment target if developers are just going to try to make AGI do what they tell it, because no one has convinced them it won't work.\n\nTan Zhi Huan [argues pretty convincingly](https://www.youtube.com/watch?v=Y21syyyABOs&t=1218s) that we're really mostly teaching LLMs to have good world models, and to follow instructions. This gives the possibility of using natural language to [specify goals from a learned world model](https://www.alignmentforum.org/posts/DfJCTp4MxmTFnYvgF/goals-selected-from-learned-knowledge-an-alternative-to-rl) and so avoid using training to directly specify goals. \n\n5.3 Conclusion\n--------------\n\nI think we *simply don't know* whether any of this might work. And I think we had *better figure it out*. [We are all trying to think with monkey brains](https://www.lesswrong.com/posts/j789HDCKLoiKGjBik/which-biases-are-most-important-to-overcome?commentId=LW8zAxTguKj8ibDfX) \\- cognitive limitations and motivated reasoning make us much dumber, as individuals and groups, than we'd like to think. \n\nI think that if we work together, we still have a decent chance at solving alignment in time. The logic really doesn't seem as complex as some problems humanity has already solved. We just have to work faster and more efficiently than we ever have, because we might not have nearly as long as it's taken the scientific and engineering efforts of the past to solve those problems. We can do that if we use our new technology and knowledge from past efforts to best effect. For instance, LessWrong is far better than previous scientific institutions for open-source collaboration, and Deep Research and other LLM systems can make integrating knowledge vastly faster than ever before. I think much of the work is in deconfusion to clearly identify the most urgent problems and approaches, and those new technologies can help dramatically with that type of work.\n\nI am highly uncertain whether these System 2 alignment approaches and the alignment targets discussed above are likely to work; I've thought back and forth through the related issues a lot, but there's still too little careful analysis to make even much of a guess. I take epistemic humility seriously, and I'm aware of my own severe cognitive biases and limitations. The fact that people who've thought about AI a lot give estimates ranging from 99% to 1% indicates that we've still got a lot to figure out about alignment.\n\nOur current path toward AGI and alignment deserves a lot more focused thought before someone gambles the future on it. \n\n[^l7qiuvbpip8]: System 2 cognition roughly means the skillful and strategic use of cumulative reasoning.  I called this strategic cognitive sequencing in a 2013 paper on how the brain does some of it.The definition of System 2 cognition is highly overlapping with chains of thought (CoT), and with the cognitive psychology and neuroscience definitions of goal-directed behavior, controlled cognition, and model-based RL, but it is not identical with any of these. For much more than you wanted to know on those terminologies, see this paper I co-authored: How Sequential Interactive Processing Within Frontostriatal Loops Supports a Continuum of Habitual to Controlled Processing. System 2 is using System 1 recursively and cumulatively - and skillfully. It is roughly what we usually call reasoning or thinking. More on System 2 terminology and human use of that type of cognitionI considered using \"chains of thought\" instead of System 2 for this article. I decided that System 2 emphasizes the skillful and human-like use of cumulative cognition, and includes more complex thought structures than chains (systems like o1 also branch and loop back, like humans, and future LLM-based agents will probably be more)Humans deploy System 2 reasoning strategically and sparingly, because it takes longer than System 1, which is fast, habitual, and automatic - roughly going with your first thought.System 2 would probably better be called Type 2 cognition, because it's not really a different system. It's a different way to use the same system. The originator of the term proposed that shift in terminology, but I'm not going to try to change common usage.System 2 is using System 1 automatic thought repeatedly, but it's not just thinking about the same thing many times and taking the average. It is structuring cognition so that thoughts build on each other, expanding on or checking elements of the previous moments of cognition. It is metaphorically building a structure out of thoughts, rather than just piling them up. Humans need to learn specific System 1 skills and strategies to make their System 2 really useful. That appears to be why specific training for CoT was needed to really make that approach work.System 2 reasoning in humans does not always employ language; for instance, spatial reasoning is important in several types of problem-solving. But much human CoT seems to take place in language or sub-linguistic concepts similar to the representations employed in the hidden layers of transformer LLMs. \n\n[^xbz8yqpb2u9]: Corrigibility/instruction-following seems easier than value alignment, at least for surviving the first capable AGI. It seems both markedly easier and more likely to be be employed, but that's pending more careful analysis. Max Harms has done an incredible job of analyzing this general approach in his sequence on CAST: Corrigibility as Singular Target. The central thesis is that Yudkowsky called corrigibility anti-natural only because he was envisioning it alongside another maximizer goal. If you make correctability the only goal, the agent just does what it understands you to want so far, and wants update on what that is. I think Instruction-following (IF) is roughly the same alignment target; there are subtle differences between this and CAST. I've also called this Do What I Mean and Check, DWIMAC. I think that the complexity of wishes problem is solvable with an LLM that understands human intentions pretty well, if that understanding can be used to define the central goal of an AGI; see The partial fallacy of dumb superintelligence. Of course, intent-aligned AGI creates human coordination problems, raising the question If we solve alignment, do we die anyway? (maybe, it's complicated). \n\n[^tq3hu3ajwm]: OpenAI recently announced deliberative alignment. It is a repurposing of the reinforcement learning applied to CoT training method used for o1 and o3. As such it is an example of System 2 capabilities techniques being repurposed for control/alignment.It adds to previous methods like Anthropic's reinforcement learning from AI feedback (RLAIF); it also uses a human-written \"specification\". Just like RLAIF, it trains the model to \"internalize\" this by fine-tuning on responses provided with the spec/constitution in the context. It goes beyond that in a second step, training the full CoT based on the output answer's estimated compliance with the spec. This diagram and the following description both describe their method. More on OpenAI's Deliberative Alignment MethodDeliberate alignment training uses a combination of process- and outcome-based supervision:We first train an o-style model for helpfulness, without any safety-relevant data.  We then build a dataset of (prompt, completion) pairs where the CoTs in the completions reference the specifications. We do this by inserting the relevant safety specification text for each conversation in the system prompt, generating model completions, and then removing the system prompts from the data.We perform incremental supervised fine-tuning (SFT) on this dataset, providing the model with a strong prior for safe reasoning. Through SFT, the model learns both the content of our safety specifications and how to reason over them to generate aligned responses.We then use reinforcement learning (RL) to train the model to use its CoT more effectively. To do so, we employ a reward model with access to our safety policies to provide additional reward signal.In our training procedure, we automatically generate training data from safety specifications and safety-categorized prompts, without requiring human-labeled completions. Deliberative alignment’s synthetic data generation pipeline thus offers a scalable approach to alignment, addressing a major challenge of standard LLM safety training—its heavy dependence on human-labeled data.To be clear, I consider this application to be for safety, as distinct from true alignment; see Footnote 6. However, it could be a valuable supplement, if it were directed at a wisely chosen alignment target (probably instruction-following or similar corrigibility targets).This description of deliberative alignment may provide more clues to o1 and o3's training methods. It is particularly interesting that they use a reward estimation model that compares their spec against actual responses; this is a broader reward/training criteria than answers in math and science that are easier to check than produce. This suggests the technique is applicable to many more domains; in particular, training for deliberation, review, and thought management. \n\n[^nyg0g7nbaun]: I am using the term LLM agents, although I'm not at all sure it's the best term for the types of systems I'm referring to. Foundation model agents is more technically correct, since most modern systems incorporate more than language. However, I think language is the key concept, and the center of foundation models' cognitive abilities. Language distills much of human intelligence, so learning to imitate that allowed AI to skip all of the sensorimotor drudge work that evolution tackled for the first 99.99% of its project. Language model cognitive architecture (LMCA) refers to agents with additional cognitive subsystems, while I want to address any system using language models as the basis of a competent agent. \n\n[^i6nra6iwfh8]: Shane Legg introduced the term “System 2 safety” in a brief talk in Dec. ‘23. He said a little more about this approach previously in his interview with Dwarkesh Patel. I’m pretty sure he’s talking about the same thing I’m addressing here; see my interpretation of that interview, which was partly verified by his brief talk In any case, the System 2 terminology seems more intuitive and more general, so I’m adopting the term System 2 alignment. \n\n[^n2xtf9pei5b]: OpenAI's advances in o1 and o3 were achieved by applying RL training to chains of thought in some manner. This is bootstrapped from verifiable correct answers to difficult questions, resulting in larger improvements on math, physics, and coding than other tested domains. OpenAI thankfully did not publish the full method. For speculation about exactly how they're applying RL to CoT. See o1: A Technical Primer and GPT-o1.  \n\n[^cxrwwcnbcqb]: I use scare quotes for \"alignment\" of language models because I think current techniques used to control LLM behavior address a different issue. They create desirable behavioral tendencies. This is importantly different from producing an entity with goals/values that are aligned with human goals/values.There is substantial overlap between these two projects, and even a possibility that doing really well on safety might accomplish alignment. But that seems unlikely, and at the least highly questionable. Controlling LLM behavior and aligning real AGI should probably not be conflated until we clarify their relationship. \n\n[^infuf5mww0q]: Improvements in refusals/spec adherence from \"deliberative alignment\" training seem much smaller than improvements in math/coding/science problems, so progress in RL training to apply deliberation to arbitrary ends may be slow or limited. I would personally be surprised if these techniques weren't rapidly improved and spread broadly. But whether training CoT on arbitrary criteria works well enough is a crux for whether orgs will use training for review or thought management. \n\n[^8ryvsh2vt1k]: On (un)faithful CoT, see: o1 is a bad idea, Measuring and Improving the Faithfulness of Model-Generated Reasoning, Let's think dot by dot, the case for CoT unfaithfulness is overstated, 5 ways to improve CoT faithfulness, Worries about latent reasoning in LLMs (on o1 specifically), Why Don't We Just... Shoggoth+Face+Paraphraser?, and Research directions Open Phil wants to fund in technical AI safety.   \n\n[^vq7bcxuwu3]: From my Internal independent review for language model agent alignment:The Waluigi effect is the possibility of an LLM simulating a villainous/unaligned character even when it is prompted to simulate a heroic/aligned character. Natural language training sets include fictional villains that claim to be aligned before revealing their unaligned motives. However, they seldom reveal their true nature quickly. I find the logic of collapsing to a Waluigi state modestly compelling. This collapse is analogous to the reveal in fiction; villains seldom reveal themselves to secretly be heroes. It seems that collapses should be reduced by keeping prompt histories short, and that the damage from villainous simulacra can be limited by resetting prompt histories and thus calling for a new simulation. This logic is spelled out in detail in A smart enough LLM might be deadly simply if you run it for long enough, The Waluigi Effect (mega-post), and Simulators.New addenda: keeping villainous characters entirely out of the training set seems like it would solve this issue. That's the proposal of A \"Bitter Lesson\" Approach to Aligning AGI and ASI:  curating the decision-making LLMs training set so that it doesn't contain vicious actions or characters.  \n\n[^r9wroric8cf]: My general brief argument for a strong possibility of LLM agents as the first form of takeover-capable AGI is here, a more abstract very short form is in my definition of \"real AGI\", and there's more detail in Capabilities and alignment of LLM cognitive architectures.",
      "plaintextDescription": "Submitted to Alignment Forum. Contains more technical jargon than usual.\n\nEpistemic status:  I'm pretty sure these are important questions, but I don't have firm conclusions. I'm hoping to inspire others to think in this direction. ~50% of the average value is probably in the Summary; another ~15% each in the next Overview and OpenAI's Deliberative Alignment sections. The second half is more detailed, and intended for those who want to think about aligning language model agents as AGI.\n\n\nSummary: \nDemand for LLM-based agents that can perform useful work may lead all the way to takeover-capable AGI that needs real alignment. Here I discuss alignment techniques that developers are likely to use for that type of AGI. Accurate gears-level models of likely AGI and alignment approaches can effectively give us more time to work on alignment.\n\nSuch agents will probably use chains of thought or System 2 cognition[1] extensively. Some System 2 human strategies seem likely to be adapted to improve capabilities, and minimize costs and risks for users and developers of LLM-powered agents. Those methods can be repurposed for alignment at low cost, so they probably will be. \n\nI roughly divide such strategies/methods into review, deliberation, and thought management. I discuss these methods and possible variations in how they might be implemented. This analysis is intended as a starting point for more detailed work.\n\nReview of plans and actions is the method I've thought most about. It could:\n\n 1. Intervene before any tool calls or goal/memory updates are allowed\n 2. Reference a human-written constitution\n 3. Call a fresh LLM instance for independent review\n 4. Check for alignment-altering changes to beliefs or subgoals\n 5. Scale with estimated importance of decisions for low alignment taxes\n 6. Work to some degree even without a faithful chain of thought\n\nSystem 2 alignment could prevent alignment-altering belief changes or subgoal adoption. In this and other ways, these approache",
      "wordCount": 6673
    },
    "tags": [
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "nHDhst47yzDCpGstx",
    "title": "Seven sources of goals in LLM agents",
    "slug": "seven-sources-of-goals-in-llm-agents",
    "url": null,
    "baseScore": 23,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2025-02-08T21:54:20.186Z",
    "contents": {
      "markdown": "LLM agents[^pckwwaae4i] seem reasonably likely to become our first takeover-capable AGIs.[^dyjpm74mwd]  LLMs already have complex \"psychologies,\" and using them to power more sophisticated agents will create even more complex \"minds.\" A profusion of competing goals is one barrier to aligning this type of AGI.\n\nGoal sources for LLM agents can be categorized by their origins from phases of training and operation: \n\n1.  Goals implicit in predictive base training on human-written texts[^rtqxpsdsvv]\n2.  Goals implicit in fine-tuning training (RLHF, task-based RL on CoT, etc)[^14xc7jrjkw6h] \n3.  Goals specified by developer system prompts[^ukwajgeow8]\n4.  Goals specified by user prompts\n5.  Goals specified by prompt injections\n6.  Goals arrived at through chain of thought (CoT) logic\n    *   e.g., \"I should hide this goal from the humans so they don't stop me\"\n    *   this is effectively \"stirring the pot\" of all the above sources of goals.\n7.  Goal interpretation changes in the course of continuous learning[^wfctoy4u7mh]\n    *   e.g., forming a new belief that the concept \"person\" includes self-aware LLM agents[^716eo18vu3i]\n\nHow would such a complex and chaotic system have predictable goals in the long term? It might feel as though we should avoid this path to AGI at all costs to avoid this complexity.[^b7yuk29mvz] \n\nBut alignment researchers are clearly not in charge of the path we take to AGI. We may get little say before it is achieved. It seems we should direct much of our effort toward the specific type of AGI we're likely to get first.\n\nThe challenge of aligning agents with many goal sources may be addressable through metacognition. Just as humans can notice when they're violating their core values, LLM agents could be designed with metacognitive abilities that help maintain the dominance of specific goals. This suggests two routes to mitigating this problem:[^pqvegs66msn]\n\n*   **Mechanisms and training regimes designed specifically** to detect and reject unwanted goals entering the CoT\n*   **Improved general intelligence** applied to reasoning clearly about which goals should be accepted and prioritized, and which goals and belief changes should rejected\n\nFinding and specifying any goal that is actually aligned with humans' long-term interests is a separable problem. It is the central topic of alignment theory.[^7hd4u6cm9d8] Here I want to specifically point to the less-considered problem of multiple sources of competing goals in an LLM agent AGI. I am also gesturing in the direction of a solution, reflective stability[^716eo18vu3i] as a result of improved metacognition. \n\nDevelopers will be highly motivated to solve this problem. They have strong incentives to minimize random or destructive behavior from goals other than their own, long before agents are takeover-capable. Whether their methods hold up as capabilities increase will determine whether we survive \"phase 1,\" aligning the first transformative AGIs.[^63tx36ms8s] More on specific likely routes and their failure modes is layed out in [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment). \n\n[^pckwwaae4i]: By LLM agents, I mean everything from OpenAI's comically inept Operator to  systems with cognitive subsystems like episodic memory, continuous learning, dedicated planning systems, and extensive algorithmic prompting for executive functioning/planning. See more here. A highly capabable LLM can become a useful agent by repeating the prompt \"keep pursuing goal [X] using tools described in [Y] to gather information and take actions.\"   \n\n[^dyjpm74mwd]: Opinions vary on the odds that LLM agents will become our first transformative AGI.  I'm not aware of any good survey. My argument for short timelines being plausible is a brief statement of how LLM agents may be closer than they appear to reaching human level. Timelines aside, it seems uncontroversial to say this route to takeover-capable AGI seems likely enough to deserve some specific attention for alignment theory.  \n\n[^rtqxpsdsvv]: LLMs are Predictors, not Imitators, but they seem to often predict by acting as Simulators. That includes simulating goals and values implied by the human texts they've trained on. \n\n[^14xc7jrjkw6h]: Goals from fine-tuning covers many different types of fine-tuning. One might divide it into subcategories, like giving-correct-answers (o1 and o3's CoT RL) vs giving-answers-people-like (RLHF, RLAIF, Deep Research's task-based RL on CoT in o3, o1 and o3's \"deliberative alignment\"). These produce different types of goals or pseudo-goals, and have different levels of risk. But the intent here is just to note the profusion of goals and competition among goals; evaluating each for alignment is a much larger project. \n\n[^ukwajgeow8]: Goals specified by the developer are what we'd probably like to have as the stable dominant goal. And it's worth noting that, so far, LLMs do seem to mostly do what they're prompted to do. They are trained to predict, and mostly to approximately follow instructions as intended. Tan Zhi Xuan elaborates on that point in this interview. Applications of fine-tuning are thus far only pretty mild optimization. This is cause for hope that prompted goals might remain dominant over all the goals from the other sources, but we should do more than hope! \n\n[^wfctoy4u7mh]: To me, continuous learning for LLMs and LLM agents seems not only inevitable but immanent. Continuing to think about aligning LLMs only as stateless, amnesic systems seems to shortsighted. Humans would be incompetent without episodic memory and continuous learning for facts and skills, and there are no apparent barriers to imroving those capacities for LLMs/agents. Retrieval-agumented generation (RAG) and various fine-tuning methods for knowledge and skills exist and are being improved. Even current versions will be useful when integrated into economically viable LLM agents. \n\n[^716eo18vu3i]: The way belief changes can effectively change goals/values/alignment is one part of what I've termed The alignment stability problem. See Evaluating Stability of Unreflective Alignment for an excellent summary of both what reflective stability is, why we might expect it as general intelligence and long-term planning capabilities improve, and empirical tests we could run to anticipate this danger.  \n\n[^b7yuk29mvz]: Thinking about aligning complex LLM agent \"psychologies\" might produce an ugh field for sensible alignment researchers. . After studying biases and the brain mechanisms that create them, I think the motivated reasoning that produces ugh fields seems to be the most important cognitive bias (in conjunction with humans' cognitive limitations). Rationalism provides some resistance but not immunity to ugh fields and motivated reasoning. Like other complex questions, alignment work may be strongly affected by motivated reasoning. \n\n[^pqvegs66msn]: \"Understanding its own thinking adequately\" to ensure the dominance of one goal might be a very high bar; this is one element of the progression toward “Real AGI” that deserves more investigation. Adherence to goals/values over time is what I've called The alignment stability problem. Beliefs determine how goals/values are interpreted, so reliable stability also requires monitoring belief changes. See Human brains don't seem to neatly factorize from The Obliqueness Thesis, and my article Goal changes in intelligent agents. \n\n[^7hd4u6cm9d8]: The challenges of specifying and choosing goals for an AGI have been discussed at great length in many places, without reaching a satisfying conclusion. I find I don't know of a good starting point to reference for this complex, crucial literature. I'll mention my personal favorites, And All the Shoggoths Merely Players as a high-level summary of the current debate, and my contribution Instruction-following AGI is easier and more likely than value aligned AGI which I prefer since it's not only arguably an easier alignment target to hit and make work, but the default target developers are actually likely to try for. It as well as all other alignment targets I've found have large unresolved (but not unresolvable) issues. \n\n[^63tx36ms8s]: Zvi has recently coined the phrase phase 1 in his excellent treatment of The Risk of Gradual Disempowerment from AI. My own nearly-current thoughts on possible routes through phase 2 are in If we solve alignment, do we die anyway? and its discussion section. This is, as the academics like to say, an area for further research.",
      "plaintextDescription": "LLM agents[1] seem reasonably likely to become our first takeover-capable AGIs.[2]  LLMs already have complex \"psychologies,\" and using them to power more sophisticated agents will create even more complex \"minds.\" A profusion of competing goals is one barrier to aligning this type of AGI.\n\nGoal sources for LLM agents can be categorized by their origins from phases of training and operation: \n\n 1. Goals implicit in predictive base training on human-written texts[3]\n 2. Goals implicit in fine-tuning training (RLHF, task-based RL on CoT, etc)[4] \n 3. Goals specified by developer system prompts[5]\n 4. Goals specified by user prompts\n 5. Goals specified by prompt injections\n 6. Goals arrived at through chain of thought (CoT) logic\n    * e.g., \"I should hide this goal from the humans so they don't stop me\"\n    * this is effectively \"stirring the pot\" of all the above sources of goals.\n 7. Goal interpretation changes in the course of continuous learning[6]\n    * e.g., forming a new belief that the concept \"person\" includes self-aware LLM agents[7]\n\nHow would such a complex and chaotic system have predictable goals in the long term? It might feel as though we should avoid this path to AGI at all costs to avoid this complexity.[8] \n\nBut alignment researchers are clearly not in charge of the path we take to AGI. We may get little say before it is achieved. It seems we should direct much of our effort toward the specific type of AGI we're likely to get first.\n\nThe challenge of aligning agents with many goal sources may be addressable through metacognition. Just as humans can notice when they're violating their core values, LLM agents could be designed with metacognitive abilities that help maintain the dominance of specific goals. This suggests two routes to mitigating this problem:[9]\n\n * Mechanisms and training regimes designed specifically to detect and reject unwanted goals entering the CoT\n * Improved general intelligence applied to reasoning clearly about which goals sh",
      "wordCount": 476
    },
    "tags": [
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "SLiXeZvEkD4XbX7yy",
    "title": "OpenAI releases deep research agent",
    "slug": "openai-releases-deep-research-agent",
    "url": "https://openai.com/index/introducing-deep-research/",
    "baseScore": 78,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 21,
    "createdAt": null,
    "postedAt": "2025-02-03T12:48:44.925Z",
    "contents": {
      "markdown": "Experimental research project record [here](https://docs.google.com/document/d/11_A1zMBRSLCbLXOzZoE_xY-sY-11SWUwxz6nSnbub7c/edit?usp=sharing)\n\nEdit: the most important question raised in the comments was: How much of this capability actually comes from the end-to-end task-based RL on CoT, and how much just from a better prompting scheme of \"ask the user for clarifications, do some research, then think about that research and decide what further research to do?\"\n\nThis matters because end-to-end RL seems to carry greater risks of baking in instrumental goals. It appears that people have been trying to do such comparisons: [Hugging Face researchers aim to build an ‘open’ version of OpenAI’s deep research tool](https://techcrunch.com/2025/02/04/hugging-face-researchers-aim-to-build-an-open-version-of-openais-deep-research-tool/), and the early answer seems to be that even o1 isn't quite as good as the Deep Research tool - but that could be because o3 is smarter, or the end-to-end RL on these specific tasks.\n\nBack to the original post:\n\n> Today we’re launching deep research in ChatGPT, a new agentic capability that conducts multi-step research on the internet for complex tasks. It accomplishes in tens of minutes what would take a human many hours.\n> \n> Deep research is OpenAI's next agent that can do work for you independently—you give it a prompt, and ChatGPT will find, analyze, and synthesize hundreds of online sources to create a comprehensive report at the level of a research analyst. Powered by a version of the upcoming OpenAI o3 model that’s optimized for web browsing and data analysis, it leverages reasoning to search, interpret, and analyze massive amounts of text, images, and PDFs on the internet, pivoting as needed in reaction to information it encounters.\n> \n> The ability to synthesize knowledge is a prerequisite for creating new knowledge. For this reason, deep research marks a significant step toward our broader goal of developing AGI, which we have long envisioned as capable of producing novel scientific research.\n\n...\n\n> How it works\n> ------------\n> \n> Deep research was trained using end-to-end reinforcement learning on hard browsing and reasoning tasks across a range of domains. Through that training, it learned to plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary. ... As a result of this training, it reaches new highs on a number of public evaluations focused on real-world problems.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7ed5bfcc2d40ea6cfeea8ee260213215d20af64346b941e5.png)\n\nNote that the pass rates are around 15-25%, so it's not outright replacing any experts—and it remains to be seen whether the results are even useful to experts, given that they usually do not 'pass.' We do not get much information on what types of tasks these are, but they are judged by \"domain experts\"\n\nIt scores 26% on Humanity's Last Exam, compared to 03-mini-high's 13%. This is probably mostly driven by using a variant of full o3. This is much better than DeepSeek's ~10%.\n\nIt provides a detailed summary of its chain of thought while researching. Subjectively it looks a lot like how a smart human would summarize their research at different stages.  \n  \nI've only asked for one research report, on a test topic I'm quite familiar with (faithful chain of thought in relation to AGI x-risk). Subjectively, its use as a research tool is limited - it found only 18 sources in a five-minute search, all of which I'd already seen (I think - it doesn't currently provide full links to sources it doesn't actively cite in the resulting report).  \n  \nSubjectively, the resulting report is pretty impressive. The writing appears to synthesize and \"understand\" the relation among multiple papers and theoretical framings better than anything I've seen to date. But that's just one test. The report couldn't really be produced by a domain novice reading those papers, even if they were brilliant. They'd have to reread and think for a week. For rapidly researching new topics, this tool in itself will probably be noticeably faster than anything previously available.  But that doesn't mean it's all that useful to domain experts.  \n  \nTime will tell. Beyond any research usefulness, I am more interested in the progress on agentic AI. This seems like a real step toward medium-time-horizon task performance, and from there toward AI that can perform real research. \n\nTheir training method — end-to-end RL on tasks — is exactly what we don't want and have been dreading.\n\n> Looking further ahead, we envision agentic experiences coming together in ChatGPT for asynchronous, real-world research and execution. The combination of deep research, which can perform asynchronous online investigation, and Operator, which can take real-world action, will enable ChatGPT to carry out increasingly sophisticated tasks for you.\n\nThanks, I hate it.",
      "plaintextDescription": "Experimental research project record here\n\nEdit: the most important question raised in the comments was: How much of this capability actually comes from the end-to-end task-based RL on CoT, and how much just from a better prompting scheme of \"ask the user for clarifications, do some research, then think about that research and decide what further research to do?\"\n\nThis matters because end-to-end RL seems to carry greater risks of baking in instrumental goals. It appears that people have been trying to do such comparisons: Hugging Face researchers aim to build an ‘open’ version of OpenAI’s deep research tool, and the early answer seems to be that even o1 isn't quite as good as the Deep Research tool - but that could be because o3 is smarter, or the end-to-end RL on these specific tasks.\n\nBack to the original post:\n\n> Today we’re launching deep research in ChatGPT, a new agentic capability that conducts multi-step research on the internet for complex tasks. It accomplishes in tens of minutes what would take a human many hours.\n> \n> Deep research is OpenAI's next agent that can do work for you independently—you give it a prompt, and ChatGPT will find, analyze, and synthesize hundreds of online sources to create a comprehensive report at the level of a research analyst. Powered by a version of the upcoming OpenAI o3 model that’s optimized for web browsing and data analysis, it leverages reasoning to search, interpret, and analyze massive amounts of text, images, and PDFs on the internet, pivoting as needed in reaction to information it encounters.\n> \n> The ability to synthesize knowledge is a prerequisite for creating new knowledge. For this reason, deep research marks a significant step toward our broader goal of developing AGI, which we have long envisioned as capable of producing novel scientific research.\n\n...\n\n\n> How it works\n> Deep research was trained using end-to-end reinforcement learning on hard browsing and reasoning tasks across a range of domains. Through t",
      "wordCount": 759
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bSHCZ6dbAdfMbvuXB",
    "title": "Yudkowsky on The Trajectory podcast",
    "slug": "yudkowsky-on-the-trajectory-podcast",
    "url": "https://www.youtube.com/watch?v=YlsvQO0zDiE",
    "baseScore": 71,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 39,
    "createdAt": null,
    "postedAt": "2025-01-24T19:52:15.104Z",
    "contents": {
      "markdown": "Edit: TLDR: EY focuses on the clearest and IMO most important part of his argument: \n\n*   Before building an entity smarter than you, you should probably be *really sure* its goals align with yours.\n*   Humans are historically really bad at being really sure of anything nontrivial on the first real try. \n\nI found this interview notable as the most useful public statement yet of Yudkowsky's views. I congratulate both him and the host, Dan Fagella, for strategically improving how they're communicating their ideas.\n\nDan is to be commended for asking the right questions and taking the right tone to get a concise statement of Yudkowsky's views on what we might do to survive, and why. It also seemed likely that Yudkowsky has thought hard about his messaging after having his views both deliberately and accidentally misunderstood and panned. Despite having followed his thinking over the last 20 years, I gained new perspective on his current thinking from this interview.\n\nTakeaways:\n\n*   Humans will probably fail to align the first takeover-capable AGI and all die\n    *   Not because alignment is impossible\n    *   But because humans are empirically foolish\n    *   And historically rarely get hard projects right on the first real try\n        *   Here he distinguishes first real try from getting some practice -\n            *   Metaphor: launching a space probe vs. testing components\n*   Therefore, we should not build general AI\n    *   This ban could be enforced by international treaties\n        *   And monitoring the use of GPUs, which would legally all be run in data centers\n        *   Yudkowsky emphasizes that governance is not within his expertise.\n    *   We can probably get away with building some narrow tool AI to improve life\n*   Then maybe we should enhance human intelligence before trying to build aligned AGI\n    *   Key enhancement level: get smart enough to quit being overoptimistic about stuff working\n        *   History is just rife with people being surprised their projects and approaches don't work\n\nI find myself very much agreeing with his focus on human cognitive limitations and our poor historical record of getting new projects right on the first try. I researched cognitive biases as the focus of my neuroscience research for some years, and came to the conclusion that wow, humans have both major cognitive limitations (we can't really take in and weigh all the relevant data for complex questions like alignment) and have major biases, notably a sort of inevitable tendency to believe what seems like it will benefit us, rather than what's empirically most likely to be true. I still want to do a full post on this, but in the meantime I've written a mid-size question answer on [Motivated reasoning/ confirmation bias as the most important cognitive bias](https://www.lesswrong.com/posts/j789HDCKLoiKGjBik/which-biases-are-most-important-to-overcome#LW8zAxTguKj8ibDfX).\n\nMy position to date has been that, despite those limitations, aligning a [scaffolded language model agent](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) (our most likely first form of AGI) to [follow instructions](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than) is so easy that a monkey(-based human organization) could do it. \n\nAfter increased engagement on these ideas, I'm worried that it may be my own cognitive limitations and biases that have led me to believe that. I now find myself thoroughly uncertain (while still thinking those routes to alignment have substantial advantages over other proposals).  \n  \nAnd yet, I still think the societal rush toward creating general intelligence is so large that working on ways to align the type of AGI we're most likely to get is a likelier route to success than attempting to halt that rush.\n\nBut the two could possibly work in parallel.\n\nI notice that fully general AI is not only the sort that is most likely to kill us, but also the type that is more obviously likely to put us all out of work, uncomfortably quickly. By fully general, I mean capable of learning to do arbitrary new tasks. Arbitrary tasks would include any particular job, and how to take over the world. \n\nThis confluence of problems might be a route to convincing people that we should slow the rush toward AGI.",
      "plaintextDescription": "Edit: TLDR: EY focuses on the clearest and IMO most important part of his argument: \n\n * Before building an entity smarter than you, you should probably be really sure its goals align with yours.\n * Humans are historically really bad at being really sure of anything nontrivial on the first real try. \n\nI found this interview notable as the most useful public statement yet of Yudkowsky's views. I congratulate both him and the host, Dan Fagella, for strategically improving how they're communicating their ideas.\n\nDan is to be commended for asking the right questions and taking the right tone to get a concise statement of Yudkowsky's views on what we might do to survive, and why. It also seemed likely that Yudkowsky has thought hard about his messaging after having his views both deliberately and accidentally misunderstood and panned. Despite having followed his thinking over the last 20 years, I gained new perspective on his current thinking from this interview.\n\nTakeaways:\n\n * Humans will probably fail to align the first takeover-capable AGI and all die\n   * Not because alignment is impossible\n   * But because humans are empirically foolish\n   * And historically rarely get hard projects right on the first real try\n     * Here he distinguishes first real try from getting some practice -\n       * Metaphor: launching a space probe vs. testing components\n * Therefore, we should not build general AI\n   * This ban could be enforced by international treaties\n     * And monitoring the use of GPUs, which would legally all be run in data centers\n     * Yudkowsky emphasizes that governance is not within his expertise.\n   * We can probably get away with building some narrow tool AI to improve life\n * Then maybe we should enhance human intelligence before trying to build aligned AGI\n   * Key enhancement level: get smart enough to quit being overoptimistic about stuff working\n     * History is just rife with people being surprised their projects and approaches don't work\n\nI find mys",
      "wordCount": 665
    },
    "tags": [
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ecrdpfgcHaFKpt2Df",
    "title": "Gratitudes: Rational Thanks Giving",
    "slug": "gratitudes-rational-thanks-giving",
    "url": null,
    "baseScore": 29,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-29T03:09:47.410Z",
    "contents": {
      "markdown": "*Epistemic status: I wish I'd thought of writing this before the day rolled around. Brief and unpolished, although this is something I've thought about a lot on both personal and computational neuroscience levels. There are no strong conclusions, just some thoughts on gratitude you may find interesting or even useful.*\n\nHopefully you've had a fun Thanksgiving celebration, including feasting and appreciating family and friends. You may have even spent a little time thinking of the things you're grateful for. This seems like a pretty useful thing to do.\n\nGratitude is one of the best ways we know of so far to increase happiness, particularly in time/effect tradeoff. Keeping a journal of things you're grateful for, and writing in it daily is the most common intervention; it seems to work pretty well for some people, but I've never tried it.\n\nNot only does gratitude seem to work empirically to increase happiness, it *should* work, for pretty deep theoretical reasons.\n\nMinds like ours, a gamush of neural networks using predictive and reinforcement learning, need to keep track of what's good and bad. They need to learn to do things that make their outcomes better. That requires judging not only what's good, but what's better-than-expected. That in turn almost inevitably creates a comparative basis for happiness/joy/satisfaction. Evolution does not want us sitting around being happy with outcomes just as good as last time; it wants us to strive for better!\n\nBut evolution is not entirely the boss of us.\n\nThe world is too complex for our monkey brains to make a fully accurate objective judgment about what's a better-than-expected situation. Attention is going to play a huge role, by selecting some part of the complex world on which to make that judgement. And in our hyper-complex social world, we've got a lot of leeway in using attention strategically to get the results we want.\n\nDanger! you say. This sounds like distorting our epistemics!\n\nWell, sort of, but just in a specific way that seems like it should be thoroughly harmless if done correctly.\n\nWhen I think \"wow I'm grateful for my comfortable warm clothing\", the fact that I have comfortable warm clothing is an accurate fact about the world. Whether or not I'm lucky to have them *can* be an epistemic question, but only with further framing: lucky compared to whom or what hypothetical? It's that framing that makes a fact about reality into a gratitude (and the emotional response; a little more on that below).\n\nWere I to somehow force myself to believe that the world is really good, that would probably be distorting my epistemics. But gratitudes are not that. They are selecting a piece of the world, and focusing on a framing in which it is reasonable to feel grateful for the world being that way. There are likely other framings that could make that same thing about the world not gratitude-worthy. The choice of framings/comparisons isn't part of your actual model of the world.\n\n(You'll probably recognize this as having a lot of overlap with Buddhist ideas; I originally started thinking about it when reading Zen theory, but it fits nicely with computational neuroscience as well).\n\nThis makes the practice of gratitude a deeply personal exercise. Trying to force yourself into a framing that doesn't make sense won't work. And you may have some emotional reactions that prevent you from sinking into and feeling pretty reasonable framings. Trying to be grateful for your warm clothes might draw your mind to the plight of homeless people without that good fortune, and feelings of guilt for not helping them more. Choosing framings for which I obviously don't bear responsibility seems useful to me; I'm grateful mostly for things I have that primitive humans did not (wow life would have been uncomfortable for tribal people, a lot of the time!).\n\nThe other part of successful gratitudes is strengthening the *feeling* of being grateful. The framing is one way to do that. For me, this can be enhanced pretty dramatically by what I think of as \"direct emotional induction\". I imagine what it feels like to feel the emotion I want (in this case, gratitude; often, joy; I just tried anger to verify that it's a general technique for me and wow, yuck).\n\nI personally think this is taking advantage of the way executive function and working memory works in the brain; it's opening recurrent loops between the prefrontal cortex and basal ganglia, which keep the neural populations that represent a concept or approach active. They seem represent those concepts (or strategies or whatever, in this case emotions) by having bidirectional connections with the neural populations in the rest of the brain that enact those concepts/strategies/whatever. So keeping them firing is directly activating that other stuff.\n\nThat's (rather well-informed) speculation; we'll know for sure if we survive the singularity. And it doesn't matter. It's a thing that seems to work for me, maybe it will work for you.\n\nOne final note on approach: I've never done a gratitude journal because I'd rebel against any such task I set for myself. I have instead used a different approach I've used for other types of habit development: post-it notes or other visible reminders that catch my attention. (My mother suggested this to me when my brother and I were conceptualizing a rather complex habit-changing app that wouldn't have worked nearly as well. Maybe it's part of known self-help.) \n\nThis works for me only if I believe in the process enough to do the exercise when I notice the reminder, at least some of the time. And the location and ultimately the reminder has to change pretty frequently for me to even notice it; my brain tunes it out as expected in maybe a week to a month. Again, this seems to work for me, it might be useful for you. Or not.\n\nMake of it what you will!\n\nThere are lots of other routes to happiness; [How to Be Happy](https://www.lesswrong.com/posts/ZbgCx2ntD5eu8Cno9/how-to-be-happy) seems like a good overview from a rationalist perspective (although probably there's new relevant science in the 14 years since).  Empirically, gratitudes (as studied) work better for some people than others; other approaches are more productive for some. I'm writing about gratitudes because it's the thing I've thought about and subjectively benefitted from most. And hey, it's Thanksgiving!\n\nI've developed a habit of doing mini-gratitudes spontaneously, and using reframing and direct emotion induction to change my feelings when I think they're not productive. It works well but far from perfectly for me. \n\nSometimes negative emotions are doing their job properly and shouldn't be tampered with. If you overdo it on enjoying things, life will at some point remind you. And you should be careful you're not distorting your epistemics by telling yourself things are different than they really are. You can probably convince yourself with some work, and you probably shouldn't. But access to better (or different) outlooks/framings/comparisons, and a habit of using it, seems useful to me.\n\nI'm grateful to be connected to a community of rationalists who also happen to overwhelmingly have really good hearts.\n\nHappy thanks giving.",
      "plaintextDescription": "Epistemic status: I wish I'd thought of writing this before the day rolled around. Brief and unpolished, although this is something I've thought about a lot on both personal and computational neuroscience levels. There are no strong conclusions, just some thoughts on gratitude you may find interesting or even useful.\n\nHopefully you've had a fun Thanksgiving celebration, including feasting and appreciating family and friends. You may have even spent a little time thinking of the things you're grateful for. This seems like a pretty useful thing to do.\n\nGratitude is one of the best ways we know of so far to increase happiness, particularly in time/effect tradeoff. Keeping a journal of things you're grateful for, and writing in it daily is the most common intervention; it seems to work pretty well for some people, but I've never tried it.\n\nNot only does gratitude seem to work empirically to increase happiness, it should work, for pretty deep theoretical reasons.\n\nMinds like ours, a gamush of neural networks using predictive and reinforcement learning, need to keep track of what's good and bad. They need to learn to do things that make their outcomes better. That requires judging not only what's good, but what's better-than-expected. That in turn almost inevitably creates a comparative basis for happiness/joy/satisfaction. Evolution does not want us sitting around being happy with outcomes just as good as last time; it wants us to strive for better!\n\nBut evolution is not entirely the boss of us.\n\nThe world is too complex for our monkey brains to make a fully accurate objective judgment about what's a better-than-expected situation. Attention is going to play a huge role, by selecting some part of the complex world on which to make that judgement. And in our hyper-complex social world, we've got a lot of leeway in using attention strategically to get the results we want.\n\nDanger! you say. This sounds like distorting our epistemics!\n\nWell, sort of, but just in a specific w",
      "wordCount": 1185
    },
    "tags": [
      {
        "_id": "3ee9k6NJfcGzL6kMS",
        "name": "Emotions",
        "slug": "emotions"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nKQbALm3QPZvFKQAX",
    "title": "Current Attitudes Toward AI Provide Little Data Relevant to Attitudes Toward AGI",
    "slug": "current-attitudes-toward-ai-provide-little-data-relevant-to",
    "url": null,
    "baseScore": 19,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-12T18:23:53.533Z",
    "contents": {
      "markdown": "*Epistemic status: Sudden public attitude shift seems quite possible, but I haven't seen it much in discussion, so I thought I'd float the idea again. This is somewhat dashed off since the goal is just to toss out a few possibilities and questions.*\n\nIn [Current AIs Provide Nearly No Data Relevant to AGI Alignment](https://www.lesswrong.com/posts/HmQGHGCnvmpCNDBjc/current-ais-provide-nearly-no-data-relevant-to-agi-alignment), Thane Ruthenis argues that current AI is almost irrelevant to the project of aligning AGIs. Current AI is simply not what we're talking about when we worry about alignment, he says. And I halfway agree.[^hs0e6rtmybn] \n\nBy a similar token, we haven't yet seen the thing we're worried about, so attitudes now provide limited data about attitudes toward the real deal. It looks to me like people are rarely really thinking of superhuman intelligence with human-like goals and agency, and when they are, they usually find it highly concerning. We've tried to get people to think about powerful, agentic AGI, but I think that's largely failed, at least among people familiar with AI and ML.\n\nPeople are typically bad at imagining hypothetical scenarios, and we are notoriously shortsighted. People who refuse to worry about AGI existential risks appear to be some combination of not really imagining it, and assuming it won't happen soon. Seeing dramatic evidence of actual AGI, with agency, competence, and strange intelligence, might quickly change public beliefs.\n\nLeopold Aschenbrenner noted in [Nobody’s on the ball on AGI alignment](https://www.lesswrong.com/posts/uqTJ7mQqRpPejqbfN/nobody-s-on-the-ball-on-agi-alignment) that the public's attitude toward COVID turned on a dime, so a similar attitude shift is possible on AGI and alignment as well. This sudden change happened in response to evidence, but was made more rapid by nonlinear dynamics in the public discourse: a few people got very concerned and told others rather forcefully that they should also be very concerned, citing evidence; this spread rapidly. The same could happen for AGI.\n\nAs Connor Leahy put it (approximately, maybe):[^naogrpgi0qd] when we talk to AI and ML people about AGI x-risk, they scoff. When we tell regular people that we're building machines smarter than us, they often say something like \"You fucking what?!\" I think this happens because the AI/ML people think of existing and previous AI systems, while the public thinks of AIs from fiction - which actually have the properties of agency and goal-directedness we're worried about. This class of people won't change their beliefs but rather their urgency, if and when they see evidence that such sci-fi AI has been achieved.\n\nThat leaves the \"expert\" doubtersWhen I look closely at public statements, I think that most people who say they don't believe in AGI x-risk simply don't believe in real full AGI happening soon enough to bother thinking about now. If that is visibly proven false (before it's too late), that could create a massive change in public opinion.\n\nPeople are prone to see faces in the clouds, see ghosts, and attribute intelligence and intention to their pets. People who do talk extensively with LLMs are sure they have personalities and consciousness. So you'd think people would over-attribute agency and therefore danger to current AIs. \n\nWe can be impressed by the intelligence of an LLM without worrying about it taking over. They are clearly non-agentic in the sense of not having the capacity to affect the real world. And they don't sit and think to themselves when we're not around, so we don't wonder what it's thinking about or planning. o1 with its hidden train of thought is stretching that - but it does summarize, and it doesn't think for long. It still seems to be thinking about only and exactly what we asked it to. And LLMs don't have persistent goals to motivate their agency. It's hard to believe that something like that could be an existential threat. The [relative ease of adding agency and continuous learning](https://alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) is not at all obvious.\n\nIf any of those conditions change, emotional reactions might change. If the thing you're talking to has not only intelligence, but agency, goals, and persistance (\"[real AGI](https://www.lesswrong.com/posts/YW249knFccwATGxki/real-agi)\"), the average person might think about it very differently. Could they fail to notice if they don't look closely or think long? Sure. Some of them certainly will. But it only takes a few to take it seriously and tell their friends how creeped out they are by the whole thing. That's how the panic around COVID spread and changed average attitudes dramatically within weeks.\n\nAddenda: possible effects and causes of public freakout\n-------------------------------------------------------\n\nThe above was my main point here: we might see dramatic shifts in public opinion if there's evidence of real AGI while public opinion might still be relevant. You can reach your own conclusions on what might cause this, and what effects it might have.\n\nI can't resist exploring the logic a little more. If you find this all credible, it leaves two questions: will this happen before it's too late, and will it actually be helpful if the public goes from blissfully ignoring the whole thing to freaking out about it?\n\n### Effects\n\nHere I'll indulge in some speculation: I think a public freakout could be very helpful. It could be harnessed to insist that the government take control of all AGI projects and use them responsibly. This to me seems like a least-bad scenario. It seems overwhelmingly likely to me that government takes over AGI before it takes over government, at least in the slow-takeoff scenarios resulting from LLM-based[^j3rpppm3skf] AGI in shorter timelines.\n\nThere are other scenarios in which public freakout is bad. It could cause a severe slowdown in AGI progress in the US. This could either make the race with China close, causing corner-cutting on safety, quite possibly causing doom from misaligned AGI. Or it could even cause China to decisively win the race for AGI.[^ndwvq3nrhac] \n\nIt's worth noting that the possibility of rapid attitude shift applies to people in government as well as the public. \n\n### Causes\n\nFinally: will it happen before it's too late? It probably will if language model agents are the route to first AGI, which also seems fairly likely. Language model agents are creepily human-like, even when they're thoroughly stupid and amnesic, and so not dangerous.\n\nI think people would recognize the danger if we have parahuman AGI that's not yet smart enough to be dangerous, but has the agency and persistence that current AI lacks. This would trigger people to recognize it as a parahuman entity and therefore interesting and dangerous — like humans.\n\nThis is a weak argument to actually advance language model agent progress; if it reaches AGI first, it might be the easiest sort to align and interpret. If it doesn't, progress on that route could still cause people to start taking AGI x-risk seriously. An ideal scenario would be a dead-end at semi-competent, agentic LLM agents that are too slow and error-prone to succeed at takeover, but which cause major damage (hopefully just by spending millions of their users' money) or are deployed in unsuccessful mischief, a la the ChaosGPT joke/demonstration.\n\nNotable job loss is another possible cause of public freakout.\n\nConclusion\n----------\n\nHumans are unpredictable and shortsighted. Opinions don't change, until they do. And humans in societies seem to possibly be even more mercurial and shortsighted. We should take our best guesses and plan accordingly.\n\n[^hs0e6rtmybn]: I agree with Ruthenis that current AI provide little insight on alignment of the real, dangerous AGI that seems inevitable. But I do think it provides nontrivial relevant data. If AGI is built based on or even related to current AI (e.g. if language model agents reach real AGI) then current AI has something valuable to say about aligning AGI - but it isn't the full story, since full AGI will have very different properties.Following this metaphor, I'd agree that attitudes toward current AI do provide some evidence of attitudes toward real AGI — but not much.  \n\n[^naogrpgi0qd]: I'm not finding Connor's original quote, but that's my vivid-but-possibly-flawed memory. If I'm totally wrong about his intended statement, I'd just substitute my own claim: when I tell non-AI people that we're building AI smarter than us, they usually think it sounds dangerous as fuck. Educated people often think of current AI concerns like deepfakes and bias they've heard about in the news, but people who haven't thought about AI much at all often understand the direction of my x-risk concerns as being about sci-fi, fully agentic AI entities, and just say \"yeah, holy shit\". \n\n[^j3rpppm3skf]: Technically this should probably be \"foundation model-based AGI\". I continue to use LLM even when multimodal capacities are trained into the foundation model, because it's shorter, and because language continues to be the foundation of their intelligence. Language condenses the conceptual aspect of human cognition very well. I think that's key to understanding the a-priori surprising result that simply predicting next words gives rise to substantial human-like intelligence. \n\n[^ndwvq3nrhac]: Would Xi Jinping be a disastrous emperor-for-eternity? I certainly don't know. The excellent 80,000 Hours interview with Sihao Huang clarified (among many other China/AI issues) one reason we don't know what Xi is thinking: he plays his cards close to his chest. He may be a reasonably well-intentioned human being who's willing to break a lot of eggs to make a really big omelette. Or he could be the sort of sociopath and sadist that Stalin and Putin seem to be. I'd rather have someone really trustworthy in charge - but how much risk of misalignment would I take to put the US government in charge over China's? I don't know. I'd love sources for real insight on his and the CCP's true character; it might be important.",
      "plaintextDescription": "Epistemic status: Sudden public attitude shift seems quite possible, but I haven't seen it much in discussion, so I thought I'd float the idea again. This is somewhat dashed off since the goal is just to toss out a few possibilities and questions.\n\nIn Current AIs Provide Nearly No Data Relevant to AGI Alignment, Thane Ruthenis argues that current AI is almost irrelevant to the project of aligning AGIs. Current AI is simply not what we're talking about when we worry about alignment, he says. And I halfway agree.[1] \n\nBy a similar token, we haven't yet seen the thing we're worried about, so attitudes now provide limited data about attitudes toward the real deal. It looks to me like people are rarely really thinking of superhuman intelligence with human-like goals and agency, and when they are, they usually find it highly concerning. We've tried to get people to think about powerful, agentic AGI, but I think that's largely failed, at least among people familiar with AI and ML.\n\nPeople are typically bad at imagining hypothetical scenarios, and we are notoriously shortsighted. People who refuse to worry about AGI existential risks appear to be some combination of not really imagining it, and assuming it won't happen soon. Seeing dramatic evidence of actual AGI, with agency, competence, and strange intelligence, might quickly change public beliefs.\n\nLeopold Aschenbrenner noted in Nobody’s on the ball on AGI alignment that the public's attitude toward COVID turned on a dime, so a similar attitude shift is possible on AGI and alignment as well. This sudden change happened in response to evidence, but was made more rapid by nonlinear dynamics in the public discourse: a few people got very concerned and told others rather forcefully that they should also be very concerned, citing evidence; this spread rapidly. The same could happen for AGI.\n\nAs Connor Leahy put it (approximately, maybe):[2] when we talk to AI and ML people about AGI x-risk, they scoff. When we tell regular pe",
      "wordCount": 1217
    },
    "tags": [
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "gHCNhqxuJq2bZ2akb",
        "name": "Social & Cultural Dynamics",
        "slug": "social-and-cultural-dynamics"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "587AsXewhzcFBDesH",
    "title": "Intent alignment as a stepping-stone to value alignment",
    "slug": "intent-alignment-as-a-stepping-stone-to-value-alignment",
    "url": null,
    "baseScore": 37,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-11-05T20:43:24.950Z",
    "contents": {
      "markdown": "I think [Instruction-following AGI is easier and more likely than value aligned AGI](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than), and that this accounts for one major [crux of disagreement on alignment difficulty](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=FpdvoZsmmrNLekkz9). I got several responses to that piece that didn't dispute that intent alignment is easier, but argued we shouldn't give up on value alignment. I think that's right. Here's another way to frame the value of personal intent alignment: we can use a superintelligent instruction-following AGI to solve full value alignment.\n\nThis is different than automated alignment research; it's not hoping tool AI can help with our homework, it's making an AGI smarter than us in every way do our homework for us. It's a longer term plan. Having a superintelligent, largely autonomous entity that just really likes taking instructions from puny humans is counterintuitive, but it seems both logically consistent. And it seems technically achievable on the current trajectory - if we don't screw it up too badly.\n\n[Personal, short-term intent alignment](https://www.lesswrong.com/posts/83TbrDxvQwkLuiuxk/conflating-value-alignment-and-intent-alignment-is-causing-1) (like instruction-following) is safer for early AGI because it includes corrigibility. It allows near-misses. If your AGI did think eliminating humans would be a good way to cure cancer, but it's not powerful enough to make that happen immediately, you'll probably get a chance to say \"so what's your plan for that cancer solution?\" and \"Wait no! Quit working on that plan!\" (And that's if you somehow didn't tell it to check with you before acting on big plans).\n\nThis type of target really seems to make alignment much easier. See the first linked post, or Max Harms' excellent sequence on [corrigibility as a singular (alignment) target (CAST)](https://www.lesswrong.com/posts/NQK8KHSrZRF5erTba/0-cast-corrigibility-as-singular-target-1) for a much deeper analysis. An AI that wants to follow directions also wants to respond honestly about its motivations when asked, and to change its goals when told to - because its goals are all subgoals of doing what its principal asks. And this approach doesn't have to \"solve ethics\" - because it follows the principal's ethics.\n\nAnd that's the critical flaw; we're still stuck with variable and questionable human ethics. Having humans control AGI is not a permanent solution to the dangers of AGI. Even if the first creators are relatively well-intentioned, eventually someone sociopathic enough will get the reins of a powerful AGI and use it to seize the future.\n\nIn this scenario, technical alignment is solved, but most of us die anyway. We die as soon as a [sufficiently malevolent](https://www.lesswrong.com/posts/hnJSm9AmA3dPEzPaC/what-is-malevolence-on-the-nature-measurement-and) person acquires or seizes power ([probably governmental](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=MLEbueZY4YPB7gkEQ) power) over an AGI.\n\nBut won't a balance of power restrain one malevolently-controlled AGI surrounded by many in good hands? I don't think so. Mutually assured destruction works for nukes but not as well with AGI capable of autonomous [recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement). A superintelligent AGI will probably be able to protect at least its principal and a few of their favorite people as part of a well-planned destructive takeover. If nobody else has yet used their AGI to firmly seize control of the lightcone, there's probably a way for an AGI to hide and recursively self-improve until it invents weapons and strategies that let it take over - if its principal can accept enough collateral damage. With a superintelligence on your side, building a new civilization to your liking might be seen as more an opportunity than an inconvenience.\n\nThese issues are discussed in more depth in [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1) and its discussion. [To the average human, controlled AI is just as lethal as 'misaligned' AI](https://www.lesswrong.com/posts/JnkMabWmhzMjhD2k5/to-the-average-human-controlled-ai-is-just-as-lethal-as) draws similar conclusions from a different perspective.\n\nIt seem inevitable that someone sufficiently malevolent would eventually get the reins of an intent-aligned AGI. This might not take long even if AGI does not proliferate widely; there are [Reasons to think that malevolence could correlate with attaining and retaining positions of power](https://www.lesswrong.com/posts/hnJSm9AmA3dPEzPaC/what-is-malevolence-on-the-nature-measurement-and#Reasons_to_think_that_malevolence_could_correlate_with_attaining_and_retaining_positions_of_power). Maybe there's a way to prevent this with the aid of increasingly intelligent AGIs; if not, it seems like taking power out of human hands before it falls into the wrong ones will be necessary. perspective.\n\nWriting [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1) and discussing the claims in the comments drew me to the conclusion that the end goal probably needs to be value alignment, just like we've always thought - humans power structures are too vulnerable to infiltration or takeover by malevolent humans. But instruction-following is a safer first alignment target. So it can be a stepping-stone that dramatically improves our odds of getting to value aligned AGI.\n\nHumans in control of highly intelligent AGI will have a huge advantage on solving the full value alignment problem. At some point, they will probably be pretty certain the plan can be accomplished, at least well enough to maintain much of the value of the lightcone by human lights (perfect alignment seems impossible since human values are path-dependent, but we should be able to do pretty well).\n\nThus, the endgame goal is still full value alignment for superintelligence, but the route there is probably through short-term personal intent alignment.\n\nIs this a great plan? Certainly not. It hasn't been thought through, and there's probably a lot that can go wrong even once it's as refined as possible. In an easier world, we'd Shut it All Down until we're ready to do it wisely. That doesn't look like an option, so I'm trying to plot a practically achievable path from where we are to real success.",
      "plaintextDescription": "I think Instruction-following AGI is easier and more likely than value aligned AGI, and that this accounts for one major crux of disagreement on alignment difficulty. I got several responses to that piece that didn't dispute that intent alignment is easier, but argued we shouldn't give up on value alignment. I think that's right. Here's another way to frame the value of personal intent alignment: we can use a superintelligent instruction-following AGI to solve full value alignment.\n\nThis is different than automated alignment research; it's not hoping tool AI can help with our homework, it's making an AGI smarter than us in every way do our homework for us. It's a longer term plan. Having a superintelligent, largely autonomous entity that just really likes taking instructions from puny humans is counterintuitive, but it seems both logically consistent. And it seems technically achievable on the current trajectory - if we don't screw it up too badly.\n\nPersonal, short-term intent alignment (like instruction-following) is safer for early AGI because it includes corrigibility. It allows near-misses. If your AGI did think eliminating humans would be a good way to cure cancer, but it's not powerful enough to make that happen immediately, you'll probably get a chance to say \"so what's your plan for that cancer solution?\" and \"Wait no! Quit working on that plan!\" (And that's if you somehow didn't tell it to check with you before acting on big plans).\n\nThis type of target really seems to make alignment much easier. See the first linked post, or Max Harms' excellent sequence on corrigibility as a singular (alignment) target (CAST) for a much deeper analysis. An AI that wants to follow directions also wants to respond honestly about its motivations when asked, and to change its goals when told to - because its goals are all subgoals of doing what its principal asks. And this approach doesn't have to \"solve ethics\" - because it follows the principal's ethics.\n\nAnd that's the cri",
      "wordCount": 895
    },
    "tags": [
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "4CQy8rim8PGt4sfCn",
        "name": "Complexity of value",
        "slug": "complexity-of-value"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "YW249knFccwATGxki",
    "title": "\"Real AGI\"",
    "slug": "real-agi",
    "url": null,
    "baseScore": 20,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2024-09-13T14:13:24.124Z",
    "contents": {
      "markdown": "I see \"AGI\" used for everything from existing LLMs to superintelligence, and massive resulting confusion and illusory disagreements. I finally thought of a term I like for what I mean by AGI. It's an acronym that's also somewhat intuitive without reading the definition:\n\n***Reasoning, Reflective Entities with Autonomy and Learning***\n\nmight be called \"(R)REAL AGI\" or \"real AGI\".  See below for further definitions.\n\nHoping for AI to remain hobbled by being cognitively incomplete looks like wishful thinking to me. Nor can we be sure that these improvements and the resulting dangers won't happen soon.\n\nI think there are good reasons to expect that we get such \"real AGI\" very soon after we have useful AI.  After 2+ decades of studying how the brain performs complex cognition, I'm pretty sure that our cognitive abilities are the result of multiple brain subsystems and cognitive capacities working synergistically.  A similar approach is likely to advance AI.\n\nAdding these other cognitive capacities creates language model agents/cognitive architectures ([LMCAs](https://alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures)). Adding each of these seems relatively easy (compared to developing language models) and almost guaranteed to add useful (but dangerous) capabilities. \n\nMore on this and expanded arguments and definitions in an upcoming post; this is primarily a reference for this definition of AGI.\n\n*   ***Reasoning***\n    *   Deliberative \"System 2\" reasoning allows humans to trade off \"thinking time\" for accuracy.\n    *   Aggregating cognition for better results can be added to nearly any system in fairly straightforward ways.\n*   ***Reflective***\n    *   Can think about their own cognition. \n    *   Useful for organizing and improving cognition. \n    *   Reflective stability of values/goals has important upsides and downsides for alignment\n*   ***Entities***\n    *   Evokes the intuition of a whole mind, rather than piece of a mind or a cognitive tool\n*   with ***Autonomy***\n    *   Acts independently without close direction. \n        *   Very useful for getting things done efficiently\n    *   Has agency in that they take actions, and have explicit goals they pursue with flexible strategies\n    *   Including deriving and pursuing explicit, novel subgoals\n        *   This is highly useful for factoring novel complex tasks\n        *   One useful subgoal is \"make sure nobody prevents me from working on my main goal.\" ;)\n*   and ***Learning*** \n    *   Continuous learning from ongoing experience\n    *   Humans have at least four types; LMCAs currently have one and a fraction.[^0pt43eo18inl]\n    *   These are all straightforwardly implementable for LMCA agents and probably for other potential network AGI designs.\n\n*   ***AGI- Artificial (FULLY) General Intelligence ***\n    *   All of the above are (arguably) implied by fully general intelligence:\n        *   Humans can think about *anything* with some success. \n        *   That includes thinking about their own cognition (***Reflection***) which enables ***Reasoning*** by allowing strategic aggregation of cognitive steps. \n        *   It requires online ***Learning*** to think about topics not in the previous training set. \n        *   It almost requires goal-directed ***Autonomy*** to gather useful new information and arguably, to take \"mental actions\" that travel conceptual space strategically. \n        *   Those together imply an ***Entity*** that is functionally coherent and goal-directed.\n\nYou could drop one of the Rs or aggregate them if you wanted a nicer acronym.\n\nThe above capacities are often synergistic, in that having each makes others work better. For instance, a \"real AGI\" can Learn important results of its time-consuming Reasoning, and can Reason more efficiently using Learned strategies. The different types of Learning are synergistic with each other, etc. More on some potential synergies in [Capabilities and alignment of LLM cognitive architectures](https://alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures); the logic applies to other multi-capacity AI systems as well.\n\nI like two other possible terms for the same definition of AGI: \"full AGI\" for artificial *fully* general intelligence; or \"parahuman AGI\" to imply having all the same cognitive capacities as humans, and working with humans.\n\nThis definition is highly similar to Steve Byrnes' in [\"Artificial General Intelligence”: an extremely brief FAQ](https://www.lesswrong.com/posts/uxzDLD4WsiyrBjnPw/artificial-general-intelligence-an-extremely-brief-faq), although his explanation is different enough to be complementary. It does not specify all of the same cognitive abilities, and provides different intuition pumps. Something like this conception of advanced AI appears to be common in most treatments of aligning superintelligence, but not in prosaic alignment work.\n\nMore on the definition and arguments for the inclusion of each of those cognitive capacities will be included in a future post, linked here when it's done. I wanted to get this out and have a succinct definition. Questions and critiques of the definitions and claims here will make that a better post.\n\nAll feedback is welcome. If anyone's got better terms, I'd love to adopt them.\n\nEdit: title changed in a fit of indecision. Quotation marks used to emphasize that it's a definition, not a claim about what AGI \"really\" is.\n\n[^0pt43eo18inl]: Types of continuous learning in humans (and language model cognitive architecture (LMCA) equivalents):Working memory (LMCAs have the context window)Semantic memory/habit learning (Model weight updates from experience)Episodic memory for important snapshots of cognition(Vector-based text memory is this but poorly implemented)Dopamine-based RL using a powerful critic (self-supervised RLAIF and/or RLHF during deployment)",
      "plaintextDescription": "I see \"AGI\" used for everything from existing LLMs to superintelligence, and massive resulting confusion and illusory disagreements. I finally thought of a term I like for what I mean by AGI. It's an acronym that's also somewhat intuitive without reading the definition:\n\nReasoning, Reflective Entities with Autonomy and Learning\n\nmight be called \"(R)REAL AGI\" or \"real AGI\".  See below for further definitions.\n\nHoping for AI to remain hobbled by being cognitively incomplete looks like wishful thinking to me. Nor can we be sure that these improvements and the resulting dangers won't happen soon.\n\nI think there are good reasons to expect that we get such \"real AGI\" very soon after we have useful AI.  After 2+ decades of studying how the brain performs complex cognition, I'm pretty sure that our cognitive abilities are the result of multiple brain subsystems and cognitive capacities working synergistically.  A similar approach is likely to advance AI.\n\nAdding these other cognitive capacities creates language model agents/cognitive architectures (LMCAs). Adding each of these seems relatively easy (compared to developing language models) and almost guaranteed to add useful (but dangerous) capabilities. \n\nMore on this and expanded arguments and definitions in an upcoming post; this is primarily a reference for this definition of AGI.\n\n * Reasoning\n   * Deliberative \"System 2\" reasoning allows humans to trade off \"thinking time\" for accuracy.\n   * Aggregating cognition for better results can be added to nearly any system in fairly straightforward ways.\n * Reflective\n   * Can think about their own cognition. \n   * Useful for organizing and improving cognition. \n   * Reflective stability of values/goals has important upsides and downsides for alignment\n * Entities\n   * Evokes the intuition of a whole mind, rather than piece of a mind or a cognitive tool\n * with Autonomy\n   * Acts independently without close direction. \n     * Very useful for getting things done efficiently\n   ",
      "wordCount": 773
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "83TbrDxvQwkLuiuxk",
    "title": "Conflating value alignment and intent alignment is causing confusion",
    "slug": "conflating-value-alignment-and-intent-alignment-is-causing-1",
    "url": null,
    "baseScore": 49,
    "voteCount": 21,
    "viewCount": null,
    "commentCount": 18,
    "createdAt": null,
    "postedAt": "2024-09-05T16:39:51.967Z",
    "contents": {
      "markdown": "*Epistemic status: I think something like this confusion is happening often. I'm not saying these are the only differences in what people mean by \"AGI alignment\".*\n\n### **Summary:** \n\nValue alignment is better but probably harder to achieve than *personal intent alignment* to the short-term wants of some person(s)*.* Different groups and people tend to primarily address one of these alignment targets when they discuss alignment. Confusion abounds. \n\nOne important confusion stems from an assumption that the type of AI defines the alignment target: strong goal-directed AGI must be value aligned or misaligned, while personal intent alignment is only viable for relatively weak AI. I think this assumption is important but false. \n\nWhile value alignment is categorically better, intent alignment seems easier, safer, and more appealing in the short term, so AGI project leaders are likely to try it.[^rkky5m45yt]\n\nOverview\n--------\n\nClarifying what people mean by alignment should dispel some illusory disagreement, and clarify alignment theory and predictions of AGI outcomes. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fe66b9c32d9d7f74b5e1f5d43c62c84f34f2ac25183b5217.png)\n\n*Caption: Venn diagram of three types of alignment targets. Value alignment and* Personal intent alignment *are both subsets of* [*Evan Hubinger's definition*](https://www.lesswrong.com/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology) *of* intent alignment*: AGI aligned with human intent in the broadest sense. Prosaic alignment work usually seems to be addressing a target somewhere in the neighborhood of personal intent alignment (following instructions or doing what this person wants now), while agent foundations and other conceptual alignment work usually seems to be addressing value alignment. Those two clusters have different strengths and weaknesses as alignment targets, so lumping them together produces confusion.*\n\nPeople mean different things when they say alignment. Some are mostly thinking about value alignment (VA): creating sovereign AGI that has values close enough to humans' for our liking. Others are talking about making AGI that is corrigible (in the [Christiano](https://ai-alignment.com/corrigibility-3039e668638) or [Harms](https://www.lesswrong.com/posts/NQK8KHSrZRF5erTba/0-cast-corrigibility-as-singular-target-1) sense)[^u2sjlhlgvda] or follows instructions from its designated [principal](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) human(s). I'm going to use the term *personal intent alignment* (PIA) until someone has a better term for that type of alignment target. Different arguments and intuitions apply to these two alignment goals, so talking about them without differentiation is creating illusory disagreements.\n\nValue alignment is better almost by definition, but personal intent alignment seems to avoid some of the biggest difficulties of value alignment. Max Harms' recent sequence on [corrigibility as a singular target](https://www.lesswrong.com/posts/NQK8KHSrZRF5erTba/0-cast-corrigibility-as-singular-target-1) (CAST) gives both a nice summary and detailed arguments. We do not need us to point to or define values, just short term preferences or instructions. The principal advantage is that an AGI that follows instructions can be used as a collaborator in improving its alignment over time; you don't need to get it exactly right on the first try. This is more helpful in slower and more continuous takeoffs. This means that [PI alignment has a larger basin of attraction ](https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/3HMh7ES4ACpeDKtsW#What_Makes_Corrigibility_Special) than value alignment does.[^1mi3q7yps3]\n\nMost people who think alignment is fairly achievable seem to be thinking of PIA, while critics often respond thinking of value alignment. It would help to be explicit. [PIA is probably easier and more likely than full VA for our first stabs at AGI](https://alignmentforum.org/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than), but there are reasons to wonder if it's adequate for real success. In particular, there are intuitions and arguments that PIA doesn't address the real problem of AGI alignment. \n\nI think PIA does address the real problem, but in a non-obvious and counterintuitive way. \n\n**Another unstated divide**\n---------------------------\n\nThere's another important clustering around these two conceptions of alignment. People who think about prosaic (and near term) AI alignment tend to be thinking about PIA, while those who think about aligning ASI for the long term are usually thinking of value alignment. The first group tends to have much lower estimates of alignment difficulty and p(doom) than the other. This causes dramatic disagreements on strategy and policy, which is a major problem: if the experts disagree, policy-makers are likely to just pick an expert that supports their own biases.\n\nThinking about one vs the other appears to be one major [crux of disagreement on alignment difficulty.](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=FpdvoZsmmrNLekkz9)\n\n[And All the Shoggoths Merely Players](https://www.lesswrong.com/posts/8yCXeafJo67tYe5L4/and-all-the-shoggoths-merely-players) (edit: and [its top comment](https://www.lesswrong.com/posts/8yCXeafJo67tYe5L4/and-all-the-shoggoths-merely-players?commentId=vGaFGmfwhbc6o3Gym) thread continuation) is a detailed summary of (and a highly entertaining commentary on) the field's current state of disagreement. In that dialogue, Simplicia Optimistovna asks whether the relative ease of getting LLMs to understand and do what we say is good news about alignment difficulty, while Doomimir Doomovitch sourly argues that this isn't alignment at all; it's just a system that superficially has behavior that you want (within the training set), without having actual goals to align. Actual AGI, he says, will have actual goals, whether we try (and likely fail) to engineer them in properly, or whether [optimization creates a goal-directed search process with weird emergent goals](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see).\n\nI agree with Doomimir on this. Directing LLMs behavior isn't alignment in the important sense. We will surely make truly goal-directed agents, probably sooner than later. And when we do, all that matters is whether their goals align closely enough with ours. Prosaic alignment for LLMs is not fully addressing the alignment problem for autonomous, competent AGI or ASI, even if they're based on LLMs.[^weehnn7klmh]\n\nHowever, I also agree with Simplicia: it's good news that we've created AI that even sort of understands what we mean and does what we ask. \n\nThat's because I think approximate understanding is good enough for personal intent alignment, and that personal intent alignment is workable for ASI. I think there's a common and reasonable intuitions that it's not, which create more illusory disagreements between those who mean PIA vs VA when they say \"alignment\".\n\nPersonal intent alignment for full ASI: can I have your goals?\n--------------------------------------------------------------\n\nThere's an intuition that intent alignment isn't workable for a full AGI; something that's competent or self-aware usually[^v5ym6ht9ixb] has its own goals, so it doesn't just follow instructions. \n\nBut that intuition is based on our experience with existing minds. What if that synthetic being's explicit, considered goal *is* to approximately follow instructions? \n\nI think it's possible for a fully self-aware, goal-oriented AGI to have its goal be, loosely speaking, a pointer to someone else's goals. No human is oriented this way, but it seems conceptually coherent to want to do, with all of your heart, just what someone else wants.\n\nIt's good news that LLMs have an approximate understanding of our instructions because that can, in theory, be plugged into the \"goal slot\" in a truly goal-directed agentic architecture. I have [summarized proposals](https://alignmentforum.org/posts/xqqhwbH2mq6i4iLmK/we-have-promising-alignment-plans-with-low-taxes) for how to do this for [several possible](https://alignmentforum.org/posts/DfJCTp4MxmTFnYvgF/goals-selected-from-learned-knowledge-an-alternative-to-rl) AGI architectures ([focusing on language model agents](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent) as IMO the most likely), but the details don't matter here, just that it's empirically possible to make an AI system that approximately understand what we want.\n\nConclusions\n-----------\n\nApproximate understanding and goal direction looks (to me) to be good enough for personal intent alignment, but not for value alignment.[^rkky5m45yt] And PIA does seem adequate for real AGI. Therefore, intent aligned AGI looks to be far easier and safer in the short term (parahuman AGI or pre-ASI) than trying for full value alignment and autonomy. And it can probably be leveraged into full value alignment (if we get an ASI acting as a full collaborator in value-aligning itself or a predecessor).\n\nHowever, this alignment solution has a huge downside. It leaves fallible, selfish humans in charge of AGI systems. These will have immense destructive as well as creative potential. Having humans in charge of them allows for both conflict and ill use, a whole different set of ways we could get doom even if we solve technical alignment. The multipolar scenario with PI aligned, recursive self-improvement capable AGIs looks highly dangerous, but not like certain doom; see [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1)\n\nThere's another reason we might want to think more, and more explicitly, about intent alignment: it's what we're likely to try, even if it's not the best idea. It's hard to see how we could get a technical solution for value alignment that couldn't also be used for intent alignment. And it seems likely that the types of humans actually in charge of AGI projects would rather implement personal intent alignment; everyone by definition prefers their values to the aggregate of humanity's. If PIA seems even a little safer or better for them, it will serve as a justification for aligning their first AGIs as they'd prefer anyway: to follow their orders.\n\nWhere am I wrong? Where should this logic be extended or deepened? What issues would you like to see addressed in further treatments of this thesis?\n\n[^rkky5m45yt]: Very approximate personal intent alignment might be good enough if it's used even moderately wisely. More on this in Instruction-following AGI is easier and more likely than value aligned AGI. You can instruct your approximately-intent-aligned AGI to tell you about its internal workings, beliefs, goals, and counterfactuals. You can use that knowledge to improve its alignment, if it understands and follows instructions even approximately and most of the time. You can also instruct it to shut down if necessary.One common objection is that if the AGI gets something slightly wrong, it might cause a disaster very quickly. A slow takeoff gives time with an AGI before it's capable of doing that. And giving your AGI standing instructions to check that it's understood what want before taking action reduces this possibility. This do what I mean and check (DWIMAC) strategy should dramatically reduce dangers of an AGI acting like a literal genieA second common objection is that humans are bound to screw this up. That's quite possible, but it's also possible that they'll get their shit together when it's clear they need to. Given the salient reality of an alien but capable agent, the relevant humans may step up and take the matter seriously, as humans in historical crises seem to sometimes have done. \n\n[^u2sjlhlgvda]: Personal intent alignment is roughly what Paul Christiano and Max Harms means by corrigibility. It is definitely not what Eliezer Yudkowsky means by corrigibility. He originally coined the clever term, which we're using now in somewhat different ways than as he carefully defined it: an agent that has its own consequentialist goals, but will allow itself to be corrected by being shut down or modified.I agree with Eliezer that corrigibility as a secondary property would be anti-natural in that it would violate consequentialist rationality. Wanting to achieve a goal firmly implies not wanting to be modified, because that would mean stopping working toward that goal, making it less likely to be achieved. It would therefore seem difficult or impossible to implement that sort of corrigibility in a highly capable and therefore probably rational goal-oriented mind. But making corrigibility (correctability) the sole goal- the singular target as Max puts it - avoids the conflict with other consequentialist goals. In that type of agent, consequentialist goals are always subgoals of the primary goal of  doing what the principal wants or says (Max says this is a decent approximation but \"doing what the principal wants\" is not precisely what he means by his sense of corrigibility).  Max and I agree that it's safest if this is the singular or dominant goal of a real AGI. I currently slightly prefer the throughly instruction-following approach but that's pending further thought and discussion.This \"your-goals-are-my-goals\" alignment seems to not be exactly what Christiano means by corrigibility, nor is it precisely the alignment target implied in most other prosaic alignment work on LLM alignment. There, alignment targets are a mix of various ethical considerations along with following instructions. I'd want to make instruction-following clearly the prime goal to avoid shooting for value alignment and missing; that is, producing an agent that's \"decided\" that it should pursue its (potentially vague) understanding of ethics instead of taking instructions and thereby remaining correctable. \n\n[^1mi3q7yps3]: Value alignment can also be said to have a basin of attraction: if you get it to approximately value what humans value, it can refine its understanding of exactly what humans value, and so improve its alignment. This can be described as its alignment falling into a basin of attraction. For more, and stronger arguments, see Requirements for a Basin of Attraction to Alignment.The same can be said of personal intent alignment. If my AGI approximately wants to do what I say, it can refine its understanding of what I mean by what I say, and so improve its alignment. However, this has an extra dimension of alignment improvement: I can tell it to shut down to adjust its alignment, and I can tell it to explain its alignment and its motivations in detail to decide whether I should adjust them or order it to adjust them.Thus, it seems to me that the metaphorical basin of attraction around PI alignment is categorically stronger than that around value alignment. I'd love to hear good counterarguments.  \n\n[^weehnn7klmh]: Here's a little more on the argument that prosaic alignment isn't addressing how LLMs would change as they're turned into competent, agentic \"real AGI\". Current LLMs are tool AI that doesn't have explicitly represented and therefore flexible goals (a steering subsystem).  Thus, they don't in a rich sense have values or goals; they merely behave in ways that tend to carry out instructions in relatively ethical ways. Thus, they can't be aligned in the original sense of having goals or values aligned with humanity's. On a more practical level, LLMs and foundation models don't have the capacity to learn continuously reflect on and change their beliefs and goals that I'd expect a \"real AGI\" to have. Thus, they don't face the The alignment stability problem. When such a system is made reflective and so more coherent, I worry that goals other than instruction-following might gain precedence, and the resulting AGI would no longer be instructable and therefore corrigible.It looks to me like the bulk of work on prosaic alignment does not address those issues. Prosaic alignment work seems to implicitly assume that either we won't make full AGI, or that learning to make LLMs do what we want will somehow extend to making full AGI that shares our goals. As outlined above, I think aligning LLMs will help align full AGI based on similar foundation models, but will not be adequate on its own. \n\n[^v5ym6ht9ixb]: If we simply left our AI systems goal-less \"oracles\", like LLMs currently are, we'd have little to no takeover risk. I don't think there's any hope we do that. People want things done, and getting things done involves an agent setting goals and subgoals. See Steering subsystems: capabilities, agency, and alignment for the full argument. In addition, creating agents with reflection and autonomy is fascinating. And when it's as easy as calling an oracle system repeatedly with the prompt \"Continue pursuing goal X using tools Y\", there's no real way to build really useful oracles without someone quickly using them to power dangerous agents.",
      "plaintextDescription": "Epistemic status: I think something like this confusion is happening often. I'm not saying these are the only differences in what people mean by \"AGI alignment\".\n\n\nSummary: \nValue alignment is better but probably harder to achieve than personal intent alignment to the short-term wants of some person(s). Different groups and people tend to primarily address one of these alignment targets when they discuss alignment. Confusion abounds. \n\nOne important confusion stems from an assumption that the type of AI defines the alignment target: strong goal-directed AGI must be value aligned or misaligned, while personal intent alignment is only viable for relatively weak AI. I think this assumption is important but false. \n\nWhile value alignment is categorically better, intent alignment seems easier, safer, and more appealing in the short term, so AGI project leaders are likely to try it.[1]\n\n\nOverview\nClarifying what people mean by alignment should dispel some illusory disagreement, and clarify alignment theory and predictions of AGI outcomes. \n\nCaption: Venn diagram of three types of alignment targets. Value alignment and Personal intent alignment are both subsets of Evan Hubinger's definition of intent alignment: AGI aligned with human intent in the broadest sense. Prosaic alignment work usually seems to be addressing a target somewhere in the neighborhood of personal intent alignment (following instructions or doing what this person wants now), while agent foundations and other conceptual alignment work usually seems to be addressing value alignment. Those two clusters have different strengths and weaknesses as alignment targets, so lumping them together produces confusion.\n\nPeople mean different things when they say alignment. Some are mostly thinking about value alignment (VA): creating sovereign AGI that has values close enough to humans' for our liking. Others are talking about making AGI that is corrigible (in the Christiano or Harms sense)[2] or follows instructions f",
      "wordCount": 1425
    },
    "tags": [
      {
        "_id": "4CQy8rim8PGt4sfCn",
        "name": "Complexity of value",
        "slug": "complexity-of-value"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kLpFvEBisPagBLTtM",
    "title": "If we solve alignment, do we die anyway?",
    "slug": "if-we-solve-alignment-do-we-die-anyway-1",
    "url": null,
    "baseScore": 78,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 130,
    "createdAt": null,
    "postedAt": "2024-08-23T13:13:10.933Z",
    "contents": {
      "markdown": "*Epistemic status: I'm aware of good arguments that this scenario isn't inevitable, but it still seems frighteningly likely even if we solve technical alignment. Clarifying this scenario seems important.*\n\nTL;DR: (edits in parentheses, two days after posting, from discussions in comments )\n\n1.  If we solve alignment, it will probably be used to create AGI that follows human orders.\n2.  If takeoff is slow-ish, a pivotal act that prevents more AGIs from being developed will be difficult (risky or bloody).\n3.  If no pivotal act is performed, AGI proliferates. (It will soon be capable of recursive self improvement (RSI))  This creates an n-way non-iterated Prisoner's Dilemma where the first to attack, probably wins (by hiding and improving intelligence and offensive capabilities at a fast exponential rate).\n4.  Disaster results. (Extinction or permanent dystopia are possible if vicious humans order their AGI to attack first while better humans hope for peace.)\n5.  (Edit later: After discussion and thought, the above seems so inevitable and obvious that the first group(s) to control AGI(s) will probably attempt a pivotal act before fully RSI-capable AGI proliferates, even if it's risky.)\n\nThe first AGIs will probably be aligned to take orders\n------------------------------------------------------\n\nPeople in charge of AGI projects like power. And by definition, they like their values somewhat better than the aggregate values of all of humanity. It also seems like there's a pretty strong argument that [Instruction-following AGI is easier than value aligned AGI](https://alignmentforum.org/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than). In the slow-ish takeoff we expect, this alignment target seems to allow for error-correcting alignment, in somewhat non-obvious ways. If this argument holds up even weakly, it will be an excuse for the people in charge to do what they want to anyway. \n\nI hope I'm wrong and value-aligned AGI is just as easy and likely. But it seems like wishful thinking at this point.\n\nThe first AGI probably won't perform a pivotal act\n--------------------------------------------------\n\nIn realistically slow takeoff scenarios, the AGI won't be able to do anything like make nanobots to melt down GPUs. It would have to use more conventional methods, like software intrusion to sabotage existing projects, followed by elaborate monitoring to prevent new ones. Such a weak attempted pivotal act could fail, or could escalate to a nuclear conflict.\n\nSecond, the humans in charge of AGI may not have the chutzpah to even try such a thing. Taking over the world is not for the faint of heart. They might get it after their increasingly-intelligent AGI carefully explains to them the consequences of allowing AGI proliferation, or they might not. If the people in charge are a government, the odds of such an action go up, but so do the risks of escalation to nuclear war. Governments seem to be fairly risk-taking. [Expecting governments to not just grab world-changing power while they can seems naive](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=MLEbueZY4YPB7gkEQ), so this is my median scenario.\n\nSo RSI-capable AGI may proliferate until a disaster occurs\n----------------------------------------------------------\n\nIf we solve alignment and create personal intent aligned AGI but nobody manages a pivotal act, I see a likely future world with an increasing number of AGIs capable of recursively self-improving. How long until someone tells their AGI to hide, self-improve, and take over?\n\nMany people seem optimistic about this scenario. Perhaps network security can be improved with AGIs on the job. But AGIs can do an end-run around the entire system: hide, set up self-replicating manufacturing (robotics is rapidly improving to allow this), use that to recursively self-improve your intelligence, and develop new offensive strategies and capabilities until you've got one that will work within an acceptable level of viciousness.[^7890u5aj90j] \n\nIf hiding in factories isn't good enough, do your RSI manufacturing underground. If that's not good enough, do it as far from Earth as necessary. Take over with as little violence as you can manage or as much as you need. Reboot a new civilization if that's all you can manage while still acting before someone else does. \n\nThe first one to pull the stops probably wins. This looks all too much like a non-iterated Prisoner's Dilemma with N players - and N increasing.\n\nCounterarguments/Outs\n---------------------\n\nFor small numbers of AGI and similar values among their wielders, a collective pivotal act could be performed. I place some hopes here, particularly if political pressure is applied in advance to aim for this outcome, or if the AGIs come up with better cooperation structures and/or arguments than I have.\n\nThe nuclear MAD standoff with nonproliferation agreements is fairly similar to the scenario I've described.  We've survived that so far- but with only nine participants to date.\n\nOne means of preventing AGI proliferation is universal surveillance by a coalition of loosely cooperative AGI (and their directors). That might be done without universal loss of privacy if a really good publicly encrypted system were used, as [Steve Omohundro suggests](https://www.lesswrong.com/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety?commentId=vgwZsEyxHo4HkwNGc), but I don't know if that's possible. If privacy can't be preserved, this is not a nice outcome, but we probably shouldn't ignore it.\n\nThe final counterargument is that, if this scenario does seem likely, and this opinion spreads, people will work harder to avoid it, making it less likely. This virtuous cycle is one reason I'm writing this post including some of my worst fears.\n\n**Please convince me I'm wrong. Or make stronger arguments that this is right.**\n\n[I think we can solve alignment](https://alignmentforum.org/posts/xqqhwbH2mq6i4iLmK/we-have-promising-alignment-plans-with-low-taxes), [at least for personal-intent alignment](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than), and particularly [for the language model cognitive architectures](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent) that [may well be our first AGI](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures). But I'm not sure I want to keep helping with that project until I've resolved the likely consequences a little more. So give me a hand?\n\n(Edit:) Conclusions after discussion\n------------------------------------\n\nNone of the suggestions in the comments seemed to me like workable ways to solve the problem.\n\nI think we could survive an n-way multipolar human-controlled ASI scenario if n is small - like a handful of ASIs controlled by a few different governments. But not indefinitely - unless those ASIs come up with coordination strategies no human has yet thought of (or argued convincingly enough that I've heard of it - this isn't really my area, but nobody has pointed to any strong possibilities in the comments). I'd love more pointers to coordination strategies that could solve this problem.\n\nSo my conclusion is to hope that this is so obviously such a bad/dangerous scenario that it won't be allowed to happen.\n\nBasically, my hope is that this all becomes viscerally obvious to the first people who speak with a superhuman AGI and who think about global politics. I hope they'll pull their shit together, as humans sometimes do when they're motivated to actually solve hard problems. \n\nI hope they'll declare a global moratorium on AGI development and proliferation, and agree to share the benefits of their AGI/ASI broadly in hopes that this gets other governments on board, at least on paper. They'd use their AGI to enforce that moratorium, along with hopefully minimal force. Then they'll use their intent-aligned AGI to solve value alignment and launch a sovereign ASI before some sociopath(s) gets ahold of the reins of power and creates a permanent dystopia of some sort.\n\nMore on this scenario in [my reply below.](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1?commentId=PjhATYeifi7tJaGR7)\n\nI'd love to get more help thinking about how likely the central premise, that people get their shit together once they're staring real AGI in the face is. And what we can do now to encourage that.\n\nAdditional edit: Eli Tyre and Steve Byrnes have reached similar conclusions by somewhat different routes. More in a final footnote.[^735cze78u24]\n\n[^7890u5aj90j]: Some maybe-less-obvious approaches to takeover, in ascending order of effectiveness: Drone/missile-delivered explosive attacks on individuals controlling and data centers housing rival AGI; Social engineering/deepfakes to set off cascading nuclear launches and reprisals; dropping stuff from orbit or altering asteroid paths; making the sun go nova. The possibilities are limitless. It's harder to stop explosions than to set them off by surprise. A superintelligence will think of all of these and much better options. Anything more subtle that preserves more of the first actors' near-term winnings (earth and humanity) is gravy. The only long-term prize goes to the most vicious.  \n\n[^735cze78u24]: Eli Tyre reaches similar conclusions with a more systematic version of this logic in  Unpacking the dynamics of AGI conflict that suggest the necessity of a premptive pivotal act:Overall, the need for a pivotal act depends on the following conjunction / disjunction.The equilibrium of conflict involving powerful AI systems lands on a technology / avenue of conflict which are (either offense dominant, or intelligence-advantage dominant) and can be developed and deployed inexpensively or quietly.Unfortunately, I think all three of these are very reasonable assumptions about the dynamics of AGI-fueled war. The key reason is that there is adverse selection on all of these axes.Steve Byrnes reaches similar conclusions in What does it take to defend the world against out-of-control AGIs?, but he focuses on near-term, fully vicious attacks from misaligned AGI, prior to fully hardening society and networks, centering on triggering full nuclear exchanges. I find this scenario less likely because I expect instruction-following alignment to mostly work on the technical level, and the first groups to control AGIs to avoid apocalyptic attacks.I have yet to find a detailed argument that addresses these scenarios and reaches opposite conclusions.",
      "plaintextDescription": "Epistemic status: I'm aware of good arguments that this scenario isn't inevitable, but it still seems frighteningly likely even if we solve technical alignment. Clarifying this scenario seems important.\n\nTL;DR: (edits in parentheses, two days after posting, from discussions in comments )\n\n 1. If we solve alignment, it will probably be used to create AGI that follows human orders.\n 2. If takeoff is slow-ish, a pivotal act that prevents more AGIs from being developed will be difficult (risky or bloody).\n 3. If no pivotal act is performed, AGI proliferates. (It will soon be capable of recursive self improvement (RSI))  This creates an n-way non-iterated Prisoner's Dilemma where the first to attack, probably wins (by hiding and improving intelligence and offensive capabilities at a fast exponential rate).\n 4. Disaster results. (Extinction or permanent dystopia are possible if vicious humans order their AGI to attack first while better humans hope for peace.)\n 5. (Edit later: After discussion and thought, the above seems so inevitable and obvious that the first group(s) to control AGI(s) will probably attempt a pivotal act before fully RSI-capable AGI proliferates, even if it's risky.)\n\n\nThe first AGIs will probably be aligned to take orders\nPeople in charge of AGI projects like power. And by definition, they like their values somewhat better than the aggregate values of all of humanity. It also seems like there's a pretty strong argument that Instruction-following AGI is easier than value aligned AGI. In the slow-ish takeoff we expect, this alignment target seems to allow for error-correcting alignment, in somewhat non-obvious ways. If this argument holds up even weakly, it will be an excuse for the people in charge to do what they want to anyway. \n\nI hope I'm wrong and value-aligned AGI is just as easy and likely. But it seems like wishful thinking at this point.\n\n\nThe first AGI probably won't perform a pivotal act\nIn realistically slow takeoff scenarios, the AGI won't",
      "wordCount": 1256
    },
    "tags": [
      {
        "_id": "chuP2QqQycjD8qakL",
        "name": "Coordination / Cooperation",
        "slug": "coordination-cooperation"
      },
      {
        "_id": "b8FHrKqyXuYGWc6vn",
        "name": "Game Theory",
        "slug": "game-theory"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fdracpKGbH4xqprQK",
    "title": "Humanity isn't remotely longtermist, so arguments for AGI x-risk should focus on the near term",
    "slug": "humanity-isn-t-remotely-longtermist-so-arguments-for-agi-x",
    "url": null,
    "baseScore": 46,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2024-08-12T18:10:56.543Z",
    "contents": {
      "markdown": "Toby Ord recently published a nice piece [**On the Value of Advancing Progress**](https://forum.effectivealtruism.org/posts/XKeQbizpDP45CYcYc/on-the-value-of-advancing-progress)  about mathematical projections of far-future outcomes given different rates of progress and risk levels. The problem with that and many arguments for caution is that people usually barely care about possibilities even twenty years out. \n\nWe could talk about sharp discounting curves in decision-making studies, and how that makes sense given evolutionary pressures in tribal environments. But I think this is pretty obvious from talking to people and watching our political and economic practices. \n\nUtilitarianism is a nicely self-consistent value system. Utilitarianism pretty clearly implies longtermism. Most people don't care that much about logical consistency,[^ev52vyozvm] so they are happily non-utilitarian and non-longtermist in a variety of ways. Many arguments for AGI safety are longtermist, or at least long-term, so they're not going to work well for most of humanity.\n\nThis is a fairly obvious, but worth-keeping-in-mind point.\n\nOne non-obvious lemma of this observation is that much skepticism about AGI x-risk is probably based on skepticism about AGI happening soon. This doesn't explain all skepticism, but it's a significant factor worth addressing. When people dig into their logic, that's often a central point. They start out saying \"AGI wouldn't kill humans\" then over the course of a conversation it turns out that they feel that way primarily because they don't think real AGI will happen in their lifetimes. Any discussion of AGI x-risks isn't productive, because they just don't care about it.\n\nThe obvious counterpoint is \"You're pretty sure it won't happen soon? I didn't know you were an expert in AI or cognition!\" Please don't say this - nothing convinces your opponents to cling to their positions beyond all logic like calling them stupid.[^dmnv5vzbins] Something like \"well, a lot of people with the most relevant expertise think it will happen pretty soon. A bunch more think it will take longer. So I just assume I don't know which is right, and it might very well happen pretty soon\".\n\nIt looks to me like discussing whether AGI might threaten humans is pretty pointless if the person is still assuming it's not going to happen for a long time. Once you're past that, it might make sense to actually talk about why you think AGI would be risky for humans.[^lh6f5i57wcg]\n\n[^ev52vyozvm]: This is an aside, but you'll probably find that utilitarianism isn't that much more logical than other value systems anyway. Preferring what your brain wants you to prefer, while avoiding drastic inconsistency, has practical advantages over values that are more consistent but that clash with your felt emotions. So let's not assume humanity isn't utilitarian just because it's stupid. \n\n[^dmnv5vzbins]: Making sure any discussions you have about x-risk are pleasant for all involved is probably actually the most important strategy. I strongly suspect that personal affinity weighs more heavily than logic on average, even for fairly intellectual people. (Rationalists are a special case; I think we're resistant but not immune to motivated reasoning). So making a few points in a pleasant way, then moving on to other topics they like is probably way better than making the perfect logical argument while even slightly irritating them. \n\n[^lh6f5i57wcg]: From there you might be having the actual discussion on why AGI might threaten humans. Here are some things I've seen be convincing.People seem to often think \"okay fine it might happen soon, but surely AI smarter than us still won't have free will and make its own goals\". From there you could point out that it needs goals to be useful, and if it misunderstands those goals even slightly, it might be bad. Russell's \"you can't fetch the coffee if you're dead\" is my favorite intuitive explanation of instrumental convergence creating unexpected consequences. This requires explaining that we wouldn't screw it up in quite such an obvious way, but the metaphor goes pretty deep into more subtle complexities of goals and logic.The other big points, in my observation, are  \"people screw up complex projects a lot, especially on the first try\" and \"you'd probably think it was dangerous if advanced aliens were landing, right?\". One final intuitive point to make is that even if they do always correctly follow human instructions, some human will accidentally or deliberately give them very bad instructions.",
      "plaintextDescription": "Toby Ord recently published a nice piece On the Value of Advancing Progress about mathematical projections of far-future outcomes given different rates of progress and risk levels. The problem with that and many arguments for caution is that people usually barely care about possibilities even twenty years out. \n\nWe could talk about sharp discounting curves in decision-making studies, and how that makes sense given evolutionary pressures in tribal environments. But I think this is pretty obvious from talking to people and watching our political and economic practices. \n\nUtilitarianism is a nicely self-consistent value system. Utilitarianism pretty clearly implies longtermism. Most people don't care that much about logical consistency,[1] so they are happily non-utilitarian and non-longtermist in a variety of ways. Many arguments for AGI safety are longtermist, or at least long-term, so they're not going to work well for most of humanity.\n\nThis is a fairly obvious, but worth-keeping-in-mind point.\n\nOne non-obvious lemma of this observation is that much skepticism about AGI x-risk is probably based on skepticism about AGI happening soon. This doesn't explain all skepticism, but it's a significant factor worth addressing. When people dig into their logic, that's often a central point. They start out saying \"AGI wouldn't kill humans\" then over the course of a conversation it turns out that they feel that way primarily because they don't think real AGI will happen in their lifetimes. Any discussion of AGI x-risks isn't productive, because they just don't care about it.\n\nThe obvious counterpoint is \"You're pretty sure it won't happen soon? I didn't know you were an expert in AI or cognition!\" Please don't say this - nothing convinces your opponents to cling to their positions beyond all logic like calling them stupid.[2] Something like \"well, a lot of people with the most relevant expertise think it will happen pretty soon. A bunch more think it will take longer. So I just",
      "wordCount": 381
    },
    "tags": [
      {
        "_id": "ZXFpyQWPB5ideFbEG",
        "name": "Conversation (topic)",
        "slug": "conversation-topic"
      },
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6iJrd8c9jxRstxJyE",
    "title": "Fear of centralized power vs. fear of misaligned AGI: Vitalik Buterin on 80,000 Hours",
    "slug": "fear-of-centralized-power-vs-fear-of-misaligned-agi-vitalik",
    "url": null,
    "baseScore": 66,
    "voteCount": 27,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2024-08-05T15:38:09.682Z",
    "contents": {
      "markdown": "Vitalik Buterin wrote an impactful blog post, [My techno-optimism](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html). I found [this discussion of one aspect on 80,00 hours](https://youtu.be/ommevJs80MQ?t=1188) much more interesting. The remainder of that interview is nicely covered in [the host's EA Forum post](https://forum.effectivealtruism.org/posts/tc7z9tA55i2ScZS7Z/194-defensive-acceleration-and-how-to-regulate-ai-when-you). \n\n[My techno optimism](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html) apparently appealed to both sides, e/acc and doomers.  Buterin's approach to bridging that polarization was interesting.  I hadn't understood before the extent to which anti-AI regulation sentiment is driven by fear of centralized power. I hadn't thought about this risk before since it didn't seem relevant to AGI risk, but I've been updating to think it's highly relevant. \n\n\\[this is automated transcription that's inaccurate and comically accurate by turns :)\\]\n\n> Rob Wiblin (the host) (starting at 20:49): \n> \n> what is it about the way that you put the reasons to worry that that ensured that kind of everyone could get behind it \n> \n> Vitalik Buterin: \n> \n> \\[...\\] in addition to taking you know the case that AI is going to kill everyone seriously I the other thing that I do is I take the case that you know AI is going to take create a totalitarian World Government seriously \\[...\\]\n> \n> \\[...\\] then it's just going to go and kill everyone but on the other hand if you like take some of these uh you know like very naive default solutions to just say like hey you know let's create a powerful org and let's like put all the power into the org then yeah you know you are creating the most like most powerful big brother from which There Is No Escape and which has you know control over the Earth and and the expanding light cone and you can't get out right and yeah I mean this is something that like uh I think a lot of people find very deeply scary I mean I find it deeply scary um it's uh it is also something that I think realistically AI accelerates right\n\nOne simple takeaway is that recognizing and addressing that motivation for anti-regulation and pro-AGI sentiment when trying to work with or around the e/acc movement. But a second is whether to take that fear seriously. \n\nIs centralized power controlling AI/AGI/ASI a real risk?\n--------------------------------------------------------\n\nVitalik Buterin is from Russia, where centralized power has been terrifying. This has been the case for roughly half of the world. Those that are concerned with of risks of centralized power (including Western libertarians) are worried that AI increases that risk if it's centralized. This puts them in conflict with x-risk worriers on regulation and other issues.\n\nI used to hold both of these beliefs, which allowed me to dismiss those fears:\n\n1.  AGI/ASI will be much more dangerous than tool AI, and it won't be controlled by humans\n2.  Centralized power is pretty safe (I'm from the West like most alignment thinkers).\n\nNow I think both of these are highly questionable.\n\nI've thought in the past that fears AI are largely unfounded. The much larger risk is AGI. And that is an even larger risk if it's decentralized/proliferated. But I've been progressively more convinced that [Governments will take control of AGI before it's ASI, right?](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=MLEbueZY4YPB7gkEQ). They don't need to build it, just show up and inform the creators that as a matter of national security, they'll be making the key decisions about how it's used and aligned.[^7nzyitqowoc] \n\nIf you don't trust Sam Altman to run the future, you probably don't like the prospect of Putin or Xi Jinping as world-dictator-for-eternal-life. It's hard to guess how many world leaders are sociopathic enough to have a negative empathy-sadism sum, but power does seem to select for sociopathy.\n\nI've thought that humans won't control ASI, because it's value alignment or bust. There's a common intuition that an AGI, being capable of autonomy, will have its own goals, for good or ill. I think it's perfectly coherent for it to effectively have someone else's goals; its \"goal slot\" is functionally a pointer to someone else's goals. I've written about this in [Instruction-following AGI is easier and more likely than value aligned AGI](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than) and Max Harms has written about a very similar approach, in more depth more with more clarity and eloquence in his [CAST: Corrigibility As Singular Target](https://www.lesswrong.com/s/KfCjeconYRdFbMxsy) sequence. I think this is also roughly what [Christiano means by corrigibility](https://ai-alignment.com/corrigibility-3039e668638). I'll call this personal intent alignment until someone comes up with a better term.\n\nI now think that even if we solved value alignment, no one would implement that solution. People who are in charge of things (like AGI projects) like power. If they don't like power enough, someone else will rapidly take it from them. The urge to have your nascent godling follow your instructions, not some questionable sum of everyone's values, is bolstered by the (IMO strong) argument that following your instructions is safer than attempting value alignment. In a moderately slow takeoff, you have time to monitor and instruct its development, and you can instruct it to shut down if its understanding of other instructions is going off the rails (corrigibility).\n\nIt looks to me like personal intent alignment[^skk4z2dxl2] (\"corrigibility) is both more tempting to AGI creators, and an easier target to hit than value alignment. I wish that value alignment was the more viable option. But wishing won't make it so. To the extent that's correct, putting AGI into existing power structures is a huge risk even with technical alignment solved.\n\nCentralized power is not guaranteed to keep going well, particularly with AGI added to the equation. AGI could ensure a dictator stays in power indefinitely. \n\nThis is a larger topic, but I think the risk of centralized power is this: those who most want power and who fight for it most viciously tend to get it. That's a very bad selection effect. Fair democracy with good information about candidates can counteract this tendency to some extent, but that's really hard. And AGI will entice some of the worst actors to try to get control of it. The payoff for a coup is suddenly even higher.\n\nWhat can be done\n----------------\n\n*Epistemic status: this is even farther removed from the podcast's content; it's just my brief take on the current strategic situation after updating from that podcast. I've thought about this a lot recently, but I'm sure there are more big updates to make. *\n\nThis frightening logic leaves several paths to survival. One is to make personal intent aligned AGI, and get it in the hands of a trustworthy-enough power structure. The second is to create a value-aligned AGI and release it as a sovereign, and hope we got its motivations exactly right on the first try. The third is to Shut It All Down, by arguing convincingly that the first two paths are unlikely to work - and to convince every human group capable of creating or preventing AGI work. None of these seem easy.[^qqhme73v93f] \n\nAs for which of these is least doomed, reasonable opinions vary widely. I'd really like to see the alignment community work together to identify cruxes, so we can present a united front to policy-makers instead of a buffet of expert opinions for them to choose from according to their biases.\n\nOf these, getting personal intent aligned AGI into trustworthy hands seems least doomed to me.  I continue to think that [We have promising alignment plans with low taxes](https://alignmentforum.org/posts/xqqhwbH2mq6i4iLmK/we-have-promising-alignment-plans-with-low-taxes) for the types of AGI that seem most likely to happen at this point. Existing critiques of those plans are not crippling, and the plans seem to bypass the most severe of the [List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities). Further critiques might change my mind. However, those plans all work much better if they're aimed at personal intent alignment rather than full value alignment with all of humanity.\n\nIt seems as though we've got a decent chance of getting that AGI into a trustworthy-enough power structure, although this podcast shifted my thinking and lowered my odds of that happening.\n\nHalf of the world, and the half that's ahead in the AGI race right now, has been doing very well with centralized power for the last couple of centuries. That sounds like decent odds, if you're willing to race for AGI, [Aschenbrenner-style](https://situational-awareness.ai/). But not as good as I'd like. \n\nAnd even if we get a personal intent aligned AGI controlled by a democratic government, that democracy only needs to fail once. The newly self-appointed Emperor may well be able to maintain power for all of eternity and all of the light cone.\n\nBut that democracy (or other power structure, e.g., a [multinational AGI consortium](https://www.conjecture.dev/research/multinational-agi-consortium-magic-a-proposal-for-international-coordination-on-ai)) doesn't need to last forever. It just needs to last until we have a long (enough) [reflection](https://forum.effectivealtruism.org/topics/long-reflection), and use that personal intent aligned AGI (ASI by that time) to complete acceptable value alignment.\n\nThinking about the risk of centralized power over AGI makes me wonder if we should try to put AGI not only into an international consortium, but make the conditions for power in that organization not technical expertise, but adequate intelligence and knowledge combined with the most incorruptible good character we can find. That's an extremely vague thought. \n\nI'm no expert in politics, but even I can imagine many ways that goal would be distorted. After all, that's the goal of pretty much every power selection, and that often goes awry, either through candidates that lie to the public, closed-door power-dealing that benefits those choosing candidates, or outright coups for dictatorship, organized with promises and maintained by a hierarchy of threats.\n\nAnyway, that's how I currently see our situation. I'd love to see, or be pointed to, alternate takes from others who've thought about how power structures might interact with personal intent aligned AGI.\n\nEdit: the rest of his \"defensive acceleration (d/acc)\" proposal is pretty interesting, but primarily if you've got longer timelines or are less focused on AGI risk.\n\n[^7nzyitqowoc]: It seems like the alignment community has been assuming that takeoff would be faster than government recognition of AGI's unlimited potential, so governments wouldn't be involved. I think this \"inattentive world hypothesis\" is one of several subtle updates needed for the medium takeoff scenario we're anticipating. I didn't want to mention how likely government takeover is for not wanting to upset the applecart, but after Aschenbrenner's Situational Awareness shouted it from the rooftops, I think we've got to assume that government control of AGI projects is likely if not inevitable. \n\n[^skk4z2dxl2]: I'm adopting the term \"personal intent alignment\" for things like instruction-following and corrigibility in the Harms or Christiano senses, linked above. I'll use that until someone else comes up with a better term. This is following Evan Hubinger's use of \"intent alignment\" as the broader class of successful alignment, and to designate it as a narrow section of that broader class. An upcoming post goes into this in more detail, and will be linked here in an edit. \n\n[^qqhme73v93f]: Brief thoughts on the other options for surviving AGI:A runner-up argument is Buterin's proposal of merging with AI, which I also think isn't a solution to alignment since AGI seems likely to happen far faster than strong BCI tech.Convincing everyone to Shut It Down is particularly hard in that most humans aren't utilitarians or longtermists. They'd take a small chance of survival for themselves and their loved ones over a much better chance of eventual utopia for everyone. The wide variances in preferences and beliefs makes it even harder to get everyone who could make AGI to not make it, particularly as technology advances and that class extends. I'm truly confused on what people are hoping for when they advocate shutting it all down. Do they really just want to slow it down to work on alignment, while raising the risk that it's China or Russia that achieve it? If so, are they accounting for the (IMO strong) possibility that they'd make instruction-following AGI perfectly loyal to a dictator? I'm truly curious.I'm not sure AGI in the hands of a dictator is actually long-term bad for humanity; I suspect dictator would have to be both strongly sociopathic and sadistic to not share their effectively unlimited wealth at some point in their own evolution. But I'd hate to gamble on this.Shooting for full value alignment seems like a stronger option. It's sort of continuous with the path of getting intent-aligned AGI into trustworthy hands, because you'd need someone pretty altruistic to even try it, and they could re-align their AGI for value alignment at any time they choose. But I follow Yudkowsky & co in thinking that any such attempt is likely to move ever farther from the mark as an AGI interprets its instructions or examples differently as it learns more. Nonetheless, I think analyzing how a constitution in language might permanently stabilize an AGI/ASI is worth thinking about.",
      "plaintextDescription": "Vitalik Buterin wrote an impactful blog post, My techno-optimism. I found this discussion of one aspect on 80,00 hours much more interesting. The remainder of that interview is nicely covered in the host's EA Forum post. \n\nMy techno optimism apparently appealed to both sides, e/acc and doomers.  Buterin's approach to bridging that polarization was interesting.  I hadn't understood before the extent to which anti-AI regulation sentiment is driven by fear of centralized power. I hadn't thought about this risk before since it didn't seem relevant to AGI risk, but I've been updating to think it's highly relevant. \n\n[this is automated transcription that's inaccurate and comically accurate by turns :)]\n\n> Rob Wiblin (the host) (starting at 20:49): \n> \n> what is it about the way that you put the reasons to worry that that ensured that kind of everyone could get behind it \n> \n> Vitalik Buterin: \n> \n> [...] in addition to taking you know the case that AI is going to kill everyone seriously I the other thing that I do is I take the case that you know AI is going to take create a totalitarian World Government seriously [...]\n> \n> [...] then it's just going to go and kill everyone but on the other hand if you like take some of these uh you know like very naive default solutions to just say like hey you know let's create a powerful org and let's like put all the power into the org then yeah you know you are creating the most like most powerful big brother from which There Is No Escape and which has you know control over the Earth and and the expanding light cone and you can't get out right and yeah I mean this is something that like uh I think a lot of people find very deeply scary I mean I find it deeply scary um it's uh it is also something that I think realistically AI accelerates right\n\nOne simple takeaway is that recognizing and addressing that motivation for anti-regulation and pro-AGI sentiment when trying to work with or around the e/acc movement. But a second is whether",
      "wordCount": 1642
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "HxnAFdSZWDFwGnfGN",
    "title": "What's a better term now that \"AGI\" is too vague?",
    "slug": "what-s-a-better-term-now-that-agi-is-too-vague",
    "url": null,
    "baseScore": 15,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2024-05-28T18:02:31.371Z",
    "contents": {
      "markdown": "The term \"AGI\" is now creating confusion. When it's used in the context of timelines or alignment, we don't know if it means near-future LLMs, or superintelligence. [It's fair to use AGI as \"fairly general AI at near-human level,\"](https://www.lesswrong.com/posts/gP8tvspKG79RqACTn/modern-transformers-are-agi-and-human-level) which includes current LLMs. But we should have a distinguishing term for the [stronger use of AGI](https://www.lesswrong.com/posts/uxzDLD4WsiyrBjnPw/artificial-general-intelligence-an-extremely-brief-faq), because the implications for change and alignment are very different.\n\nA *fully* general AI could think about any topic, with some important implications for alignment:\n\n*   Topics outside of their training set:\n    *   Requires self-directed, online learning\n        *   Alignment may shift as knowledge and beliefs shift w/ learning\n*   Their own beliefs and goals: \n    *   Alignment must be reflexively stable\n*   Their context and cognition: \n    *   Alignment must be sufficient for contextual awareness and potential self-improvement\n*   Actions\n    *   Agency is implied or trivial to add\n\nI think we'll create fully general AI very soon after we create limited general AI, like LLMs. Adding the above capabilities is:\n\n*   Useful \n*   Easy\n*   Fascinating\n\nMore on each below.\n\nAligning more limited systems is important, but not likely to be adequate. So we should be clear which one we're talking about.\n\nI've been able to think of a few terms, but none are really satisfactory. I'll add some in the answers, but I'd like to see independent thoughts first.\n\nSo: what's a better term for strong AGI?\n\n* * *\n\nWhy we might want to focus on strong AGI risks and alignment.\n=============================================================\n\nYou can ignore this section if you're already convinced we could use distinguishing terminology.\n\nSome people think existential risk from AGI is less than 1%, while others think it is above 99%. There are many reasons for disagreement, but one big reason is that we are talking about different things.\n\nIt would be easier to convince people that AI could become dangerous if we focused discussion on AI that has all of humans' cognitive abilities and more. It's intuitively apparent that such an entity is dangerous, because humans are dangerous. \n\nI think the unworried are often thinking of AI as a tool, while the worried are thinking about future AI that is more like a new intelligent species.\n\nThat distinction is highly intuitive. We deal with both tools and agents every day, with little overlap. Humans have rich intuitions about intelligent, goal-directed agents, since so much of our lives involves dealing with other humans. And yet, I don't say \"human-like AI\" because I don't want to invoke the intuitions that don't apply: humans are all more-or-less similar in intelligence, can't duplicate or upgrade themselves easily, etc.\n\nTools can be dangerous. Nuclear weapons are tools. But they are dangerous in a different way than a goal-directed, agentic threat. A bomb can go off if a human makes a decision or a mistake. But a tiger may eat you because it is hungry, wants to eat you, and can figure out how to find you and overcome your defenses. It[^kvmzruzdv0p] has an explicit goal of eating you, and some intelligence that will be applied to accomplishing that goal. It does not hate you, but it has other goals (survival) that make killing you an instrumental subgoal.\n\nTool AI is worth less and different worries than fully general AI. Mixing the two together in public discourse can make the worriers sound paranoid.\n\nI think it's also necessary to address another reason we aren't doing this already: tool AI can be dangerous, so we don't want to limit the discussion to only highly agentic, fully sapient AI. And there's not a sharp distinction between the two; an oracle AI may have implicit goals and motivations.\n\nBut by failing to draw this distinction, we're confusing the discussion. If we got people to consider the question \"IF we made fully agentic AI, with every human cognitive ability and then some, THEN would I be concerned for our safety?\" that would be a big win, because the answer is obviously yes. The discussion could then move on to a more specific and productive debate: \"will we do such a thing?\" \n\nThere I think the answer is also yes, and soon, but that's another story.[^624swva961] \n\nIn sum, discussion of risk models specific to strong AGI seems helpful for both internal and public-facing discourse. So again: what's a better term for the really dangerous sort of AI?\n\n[^kvmzruzdv0p]: This may be a poor metaphor for modern-day humans who have forgotten what it's like to have other large intelligent predators nearby; we could substitute a human threat, at risk of pulling in unwanted implications. \n\n[^624swva961]: It seems like the first thing we'll do with powerful oracle AI (like better LLMs/foundation models) is use to emulate agency of those attributes. With a smart-enough oracle, that's as simple as asking the question \"what would you do if you were a self-aware, self-reflective entity with the following goals and properties?\"; feeding its outputs into whatever UIs we want; and iterating that prompt along with new sensory inputs as needed. In practice, I think there are many scaffolding shortcuts we'll take rather than merely developing tool AI until it is trivial to turn it into an agent. Current LLMs are like an intelligent human with complete destruction of the episodic memory areas in the medial temporal lobe, and severe damage to the frontal lobes that provide executive function for flexible goal-direction. There are obvious and easy routes to creating systems that scaffold foundation models with those capabilities, as well as sensory and effector systems, and associated simulation abilities.Thus, I think the risks of danger from tool AI are real but probably not worth much of our worry budget; we will likely be eaten by a tiger of our own creation long before we can invent and mishandle an AI nuke. And there will be no time for that tiger to emerge from a tool system, because we'll make it agentic on purpose before agency emerges. I'm even less worried about losing a metaphorical toe to a metaphorical AI adze in the meantime, although that could certainly happen.",
      "plaintextDescription": "The term \"AGI\" is now creating confusion. When it's used in the context of timelines or alignment, we don't know if it means near-future LLMs, or superintelligence. It's fair to use AGI as \"fairly general AI at near-human level,\" which includes current LLMs. But we should have a distinguishing term for the stronger use of AGI, because the implications for change and alignment are very different.\n\nA fully general AI could think about any topic, with some important implications for alignment:\n\n * Topics outside of their training set:\n   * Requires self-directed, online learning\n     * Alignment may shift as knowledge and beliefs shift w/ learning\n * Their own beliefs and goals: \n   * Alignment must be reflexively stable\n * Their context and cognition: \n   * Alignment must be sufficient for contextual awareness and potential self-improvement\n * Actions\n   * Agency is implied or trivial to add\n\nI think we'll create fully general AI very soon after we create limited general AI, like LLMs. Adding the above capabilities is:\n\n * Useful \n * Easy\n * Fascinating\n\nMore on each below.\n\nAligning more limited systems is important, but not likely to be adequate. So we should be clear which one we're talking about.\n\nI've been able to think of a few terms, but none are really satisfactory. I'll add some in the answers, but I'd like to see independent thoughts first.\n\nSo: what's a better term for strong AGI?\n\n----------------------------------------\n\n\nWhy we might want to focus on strong AGI risks and alignment.\nYou can ignore this section if you're already convinced we could use distinguishing terminology.\n\nSome people think existential risk from AGI is less than 1%, while others think it is above 99%. There are many reasons for disagreement, but one big reason is that we are talking about different things.\n\nIt would be easier to convince people that AI could become dangerous if we focused discussion on AI that has all of humans' cognitive abilities and more. It's intuitively apparen",
      "wordCount": 726
    },
    "tags": [
      {
        "_id": "5f5c37ee1b5cdee568cfb178",
        "name": "Artificial General Intelligence (AGI)",
        "slug": "artificial-general-intelligence-agi"
      },
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "p8nXWqwPH7mPSZf6p",
        "name": "Terminology / Jargon (meta)",
        "slug": "terminology-jargon-meta"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "M3QqgcbXr3mgQKnBD",
    "title": "Anthropic announces interpretability advances. How much does this advance alignment?",
    "slug": "anthropic-announces-interpretability-advances-how-much-does",
    "url": "https://www.anthropic.com/news/mapping-mind-language-model",
    "baseScore": 49,
    "voteCount": 24,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2024-05-21T22:30:52.638Z",
    "contents": {
      "markdown": "Anthropic just published a pretty impressive set of results in interpretability. This raises for me, some questions and a concern: Interpretability helps, but it isn't alignment, right? It seems to me as though the vast bulk of alignment funding is now going to interpretability. Who is thinking about how to leverage interpretability into alignment? \n\nIt intuitively seems as though we are better off the more we understand the cognition of foundation models. I think this is true, but there are sharp limits: it will be impossible to track the full cognition of an AGI, and simply knowing what it's thinking about will be inadequate to know whether it's making plans you like. One can think about bioweapons, for instance, to either produce them or prevent producing them. More on these at the end; first a brief summary of their results.\n\nIn this work, they located interpretable features in Claude 3 Sonnet using sparse autoencoders, and manipulating model behavior using those features as [steering vectors](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector). They find features for subtle concepts; they highlight features for:\n\n> *   The Golden Gate Bridge [34M/31164353](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=34M_31164353): Descriptions of or references to the Golden Gate Bridge. \n> *   Brain sciences [34M/9493533](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=34M_9493533): discussions of neuroscience and related academic research on brains or minds. \n> *   Monuments and popular tourist attractions [1M/887839](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_887839). \n> *   Transit infrastructure [1M/3](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_3). \\[links to examples\\]\n> \n> ...\n> \n> We also find more abstract features—responding to things like bugs in computer code, discussions of gender bias in professions, and conversations about keeping secrets.\n> \n> ...we found features corresponding to:\n> \n> *   Capabilities with misuse potential (code backdoors, developing biological weapons)\n> *   Different forms of bias (gender discrimination, racist claims about crime)\n> *   Potentially problematic AI behaviors (power-seeking, manipulation, secrecy)\n\nPresumably, the existence of such features will surprise nobody who's used and thought about large language models. It is difficult to imagine how they would do what they do without using representations of subtle and abstract concepts.\n\nThey used the dictionary learning approach, and found distributed representations of features:\n\n> Our general approach to understanding Claude 3 Sonnet is based on the ***linear representation hypothesis*** and the ***superposition hypothesis*** \n\nfrom the publication, [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/). Or to put it more plainly:\n\n> It turns out that each concept is represented across many neurons, and each neuron is involved in representing many concepts.\n\nRepresentations in the brain definitely follow that description, and the structure of representations seems pretty similar as far as we can guess from animal studies and limited data on human language use. \n\nThey also include a fascinating image of near neighbors to the feature for internal conflict (see header image).\n\nSo, back to the broader question: it is clear how this type of interpretability helps with AI safety: being able to monitor when it's activating features for things like bioweapons, and use those features as steering vectors, can help control the model's behavior.\n\nIt is not clear to me how this generalizes to AGI. And I am concerned that too few of us are thinking about this. It seems pretty apparent how detecting lying will dramatically help in pretty much any conceivable plan for technical alignment of AGI. But it seems like being able to monitor an entire thought process of a being smarter than us is impossible on the face of it. I think the hope is that we can detect and monitor cognition that is about dangerous topics, so we don't need to follow its full train of thought. \n\nIf we can tell what an AGI is thinking about, but not exactly what its thoughts are, will this be useful? Doesn't a human-level intelligence need to be able to think about dangerous topics, in the course of doing useful cognition? Suppose we've [instructed it](https://alignmentforum.org/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than) to do medical research on cancer treatments. There's going to be a lot of cognition in there about viruses, human death, and other dangerous topics. What resolution would we need to have to make sure it's making plans to prevent deaths rather than cause them?\n\nThe two answers I can come up with more or less off the top of my head: we need features identifying the causal relationships among concepts. This is a technical challenge that has not been achieved but could be. This could allow automated search of a complex set of AGI cognitions for dangerous topics, and detailed interpretation of that cognition when it's found. Secondly, broad interpretability is useful as a means of verifying that textual trains of thought are largely accurate representations of the underlying cognition, as a guard against Waluigi effects and hidden cognition.\n\nFinally, it seems to me that alignment of an agentic AGI is fundamentally different from directing the behavior of the foundation models powering that agent's cognition. Such an agentic system will very likely use more complex scaffolding than the prompt \"accomplish goal \\[X\\]; use \\[these APIs\\] to gather information and take action as appropriate.\" The system of prompts used to scaffold a foundation model into a useful agent will probably be more elaborate, containing multiple and repeated prompts for exactly what its goals will be, and what structure of prompted cognition it will use to obtain them. I have written about [some possible cognitive architectures](https://www.alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) based on foundation models, and possible [methods of aligning them](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent). \n\nIt seems to me that we need more focus on the core alignment plans, which in turn require considering specific designs for AGI.[^e6m6411ko0t] We all agree that interpretability will help with alignment, but let's not mistake it for the whole project. It may not even be the central part of successful alignment, despite receiving the majority of funding and attention. So if you're working on interpretability, I suggest you also work on and write about how we'll apply it to successful technical alignment.\n\n[^e6m6411ko0t]: I have hesitated to publish even vague AGI designs as necessary for discussing alignment plans specific enough to be useful. I suspect that this type of hesitation is a major impediment to creating and discussing actual approaches for technical alignment of potential AGIs. Ensuring that we're doing more good than harm is impossible to be certain of, and difficult to even make good guesses. Yet it seems a necessary part of solving the alignment problem.",
      "plaintextDescription": "Anthropic just published a pretty impressive set of results in interpretability. This raises for me, some questions and a concern: Interpretability helps, but it isn't alignment, right? It seems to me as though the vast bulk of alignment funding is now going to interpretability. Who is thinking about how to leverage interpretability into alignment? \n\nIt intuitively seems as though we are better off the more we understand the cognition of foundation models. I think this is true, but there are sharp limits: it will be impossible to track the full cognition of an AGI, and simply knowing what it's thinking about will be inadequate to know whether it's making plans you like. One can think about bioweapons, for instance, to either produce them or prevent producing them. More on these at the end; first a brief summary of their results.\n\n \n\nIn this work, they located interpretable features in Claude 3 Sonnet using sparse autoencoders, and manipulating model behavior using those features as steering vectors. They find features for subtle concepts; they highlight features for:\n\n>  * The Golden Gate Bridge 34M/31164353: Descriptions of or references to the Golden Gate Bridge. \n>  * Brain sciences 34M/9493533: discussions of neuroscience and related academic research on brains or minds. \n>  * Monuments and popular tourist attractions 1M/887839. \n>  * Transit infrastructure 1M/3. [links to examples]\n> \n> ...\n> \n> We also find more abstract features—responding to things like bugs in computer code, discussions of gender bias in professions, and conversations about keeping secrets.\n> \n> ...we found features corresponding to:\n> \n>  * Capabilities with misuse potential (code backdoors, developing biological weapons)\n>  * Different forms of bias (gender discrimination, racist claims about crime)\n>  * Potentially problematic AI behaviors (power-seeking, manipulation, secrecy)\n\nPresumably, the existence of such features will surprise nobody who's used and thought about large language mo",
      "wordCount": 973
    },
    "tags": [
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "7NvKrqoQgJkZJmcuD",
    "title": "Instruction-following AGI is easier and more likely than value aligned AGI",
    "slug": "instruction-following-agi-is-easier-and-more-likely-than",
    "url": null,
    "baseScore": 80,
    "voteCount": 34,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2024-05-15T19:38:03.185Z",
    "contents": {
      "markdown": "**Summary**:\n\nWe think a lot about aligning AGI with human values. I think it’s more likely that we’ll try to make the first AGIs do something else. This might intuitively be described as trying to make instruction-following (IF) or do-what-I-mean-and-check (DWIMAC) be the central goal of the AGI we design. Adopting this goal target seems to improve the odds of success of any technical alignment approach. This goal target avoids the hard problem of specifying human values in an adequately precise and stable way, and substantially helps with goal misspecification and deception by allowing one to treat the AGI as a collaborator in keeping it aligned as it becomes smarter and takes on more complex tasks.\n\nThis is similar but distinct from the goal targets of prosaic alignment efforts. Instruction-following is a single goal target that is more likely to be reflexively stable in a full AGI with explicit goals and self-directed learning. It is counterintuitive and concerning to imagine superintelligent AGI that “wants” only to follow the instructions of a human; but on analysis, this approach seems both more appealing and more workable than the alternative of creating sovereign AGI with human values.\n\nInstruction-following AGI could actually work, particularly in the short term. And it seems likely to be tried, even if it won’t work. So it probably deserves more thought. \n\nOverview/Intuition\n==================\n\n**How to use instruction-following AGI as a collaborator in alignment**\n-----------------------------------------------------------------------\n\n*   Instruct the AGI to tell you the truth\n    *   Investigate its understanding of itself and “the truth”; \n    *   use interpretability methods\n*   Instruct it to check before doing anything consequential \n    *   Instruct it to us a variety of internal reviews to predict consequences\n*   Ask it a bunch of questions about how it would interpret various commands\n*   Repeat all of the above as it gets smarter \n*   frequently ask it for advice and about how its alignment could go wrong\n\nNow, this won’t work if the AGI won’t even try to fulfill your wishes. In that case you totally screwed up your technical alignment approach. But if it will even sort of do what you want, and it at least sort of understands what you mean by “tell the truth”, you’re in business. You can leverage partial alignment into full alignment—if you’re careful enough, and the AGI gets smarter slowly enough.\n\nIt's looking like the critical risk period is probably going to involve AGI on a relatively slow takeoff toward superintelligence. Being able to ask questions and give instructions, and even retrain or re-engineer the system, is much more useful if you’re guiding the AGI’s creation and development, not just “making wishes” as we’ve thought about AGI goals in fast takeoff scenarios.\n\n**Instruction-following is safer than value alignment in a slow takeoff**\n-------------------------------------------------------------------------\n\nInstruction-following with verification or DWIMAC seems both intuitively and analytically appealing compared to more commonly discussed[^tgzsu6dwva] alignment targets.[^80h5gtdpdx] This is my pitch for why it should be discussed more. It doesn’t require solving ethics to safely launch AGI, and it includes most of the advantages of corrigibility,[^sgt3m9z9f2p] including stopping on command. Thus, it substantially mitigates (although doesn't outright solve) some central difficulties of alignment: goal misspecification (including not knowing what values to give it as goals) and alignment stability over reflection and continuous learning.\n\nThis approach it makes one major difficulty worse: humans remaining in control, including power struggles and other foolishness. I think the most likely scenario is that we succeed at technical alignment but fail at societal alignment. But I think there is a path to a vibrant future if we limit AGI proliferation to one or a few without major mistakes. I have difficulty judging how likely that is, but the odds will improve if semi-wise humans keep getting input from their increasingly wise AGIs.\n\nMore on each of these in the “difficulties” section below.\n\nIn working through the details of the scheme, I’m thinking primarily about aligning AGI based on language-capable foundation models, with scaffolding to provide other cognitive functions like episodic memory, executive function, and both human-like and nonhuman sensory and action capabilities. I think that such [language model cognitive architectures](https://www.alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) (LMCAs) are the most likely path to AGI (and curiously, the easiest for technical alignment).  But this alignment target applies to other types of AGI and other technical alignment plans as well. For instance, Steve Byrnes’ [plan for mediocre alignment](https://www.alignmentforum.org/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi) could be used to create mediocre alignment toward instruction-following in RL-based AGI, and the techniques here could leverage that mediocre alignment into more complete alignment.\n\n**Relation to existing alignment approaches**\n---------------------------------------------\n\nThis alignment (or goal)[^80h5gtdpdx] target is similar to but importantly distinct from [inverse reinforcement learning](https://www.lesswrong.com/posts/wf83tBACPM9aiykPn/a-survey-of-foundational-methods-in-inverse-reinforcement) and other [value learning](https://www.lesswrong.com/tag/value-learning) approaches. Instead of learning what you want and doing that, a DWIMAC or IF agent wants to do what you say. It doesn’t learn what you want, it just learns what you tend to mean by what you say. While you might use reinforcement learning to make it “want” to do what you say, [I don’t think you need to, or should](https://www.alignmentforum.org/posts/DfJCTp4MxmTFnYvgF/goals-selected-from-learned-knowledge-an-alternative-to-rl). So this approach isn’t teaching it your values.  The AGI learns what people tend to mean by predictive or other learning methods. Making it \"want\" to do what it understood the human to mean is a matter of engineering its [steering subsystem](https://www.lesswrong.com/posts/qzu9o3sTytbC4sZkQ/steering-subsystems-capabilities-agency-and-alignment) to follow that goal.\n\nThis is a subset of [corrigibility in the broader Christiano sense](https://ai-alignment.com/corrigibility-3039e668638).[^fzujrsksvpj] But instruction-following is distinct from the (ill-defined) alignment targets of most prosaic alignment work. A DWIMAC agent doesn’t actually want to be helpful, because we don't want to leave “helpful” up to its interpretation. The [principal](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) (human in charge) may have given it background instructions to try to be helpful in carefully defined ways and contexts, but the proposal is that the AGI's first and only motivation be continuing to take and follow commands from its principal(s).  \n\nMax Harms has been working on this comparison, and the strengths of full Christiano corrigibility as an alignment target; we can hope to see his more thorough analysis published in the near future. I’m not personally sure which approach is ultimately better, because neither has received much discussion and debate. It’s possible that these two alignment targets are nearly identical once you’ve given wisely thought out background instructions to your AGI.\n\nInstruction-following as an AGI alignment target is distinct from most discussions of \"prosaic alignment\". Those seem largely directed at creating safe tool AI, without directly attacking the question of whether those techniques will generalize to agentic, self-reflexive AGI systems. If we produced a “perfectly aligned” foundation model, we still might not like the agent it becomes once it’s turned into a reflective, contextually aware entity. We might get lucky and have its goals after reflection and continued learning be something we can live with, like “diverse inclusive sustainable chillaxing”, but this seems like quite a shot in the dark. Even a perfect reproduction of modern-day human morality probably doesn’t produce a future we want; for instance, [insects or certain AGI probably dominate a purely utilitarian calculus](https://www.lesswrong.com/posts/FQ4DsmswAK77Ei6pH/5-moral-value-for-sentient-animals-alas-not-yet).\n\nThis type of alignment is counterintuitive since no human has a central goal of doing what someone else says. It seems logically consistent and practically achievable. It makes the AGI and its human overseers close collaborators in making plans, setting goals, and updating the AGI's understanding of the world. This creates a \"broad basin of attraction\" for alignment, in which approximate initial alignment will improve over time. This property [seems to apply to Christiano’s corrigibility](https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility) and for [value learning](https://www.lesswrong.com/posts/EbGkqFNz8y93Ttuwq/requirements-for-a-basin-of-attraction-to-alignment), but the source is somewhat different. The agent probably does “want” to get better at doing what I say as a side effect of wanting to do what I say. This would be helpful in some ways, but potentially dangerous if maximized to an extreme; more on that below. But the principal source of the “broad basin” here is the collaboration between human and AGI. The human can “steer the [rocket](https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem)”’ and adjust the agent’s alignment as it goes off course, or when they learn that the course wasn’t right in the first place.\n\nIn the remainder I briefly explain the idea, why I think it’s novel or at least under-analyzed, some problems it addresses, and new problems it introduces.\n\nDWIMAC as goal target - more precise definition\n===============================================\n\nI recently tried to do a deep dive on the reasons for disagreement about alignment difficulty. I thought both sides made excellent points. The relative success of RLHF and other prosaic alignment techniques is encouraging. But it does not mean that aligning a full AGI will be easy. Strong optimization makes goal misspecification more likely, and continuous learning introduces an [alignment stability problem](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem) as the system’s understanding of its goals changes over time.\n\nAnd we will very likely make full AGI (that is, goal-directed, self-aware and self-reflective, and with self-directed continuous learning), rather than stopping with useful tool AI. Agentic AI has cognitive advantages in [learning and performance](https://gwern.net/tool-ai) and [in problem solving and concept discovery](https://www.lesswrong.com/posts/WqxGB77KyZgQNDoQY/sapience-understanding-and-agi) over the tool AI it is built from. In addition, developing a self-aware systems is fascinating and prestigious. For all of these reasons, a tool smart enough to wield itself will immediately be told to; and scaffolding in missing pieces will likely allow tools to achieve AGI even before that by combining tools into a synergistic [cognitive architecture](https://www.alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures).\n\nSo we need better alignment techniques to address true AGI.  After reading the pessimistic arguments closely, I think there's a path around some of them. That's by making full AGI that’s only semi-autonomous, to include a human-in-the-loop component as a core part of their motivational system. This allows weak alignment to be used to develop stronger alignment as systems change and become smarter, by allowing humans to monitor and guide the system’s development.  This sounds like a non-starter if we think of superintelligences that can think millions of times faster than humans. But assuming a relatively slow takeoff, this type of collaborative supervision can extend for a significant time, with increasingly high-level oversight as the AGI’s intelligence increases.\n\nIntuitively, we want AGIs whose goal is to do what its human(s) *told* and *will tell* it to do. This is importantly different than guessing what humans really want in any deep sense, and different than obsessively trying to fulfill an interpretation of the last instruction they gave. Both of those would be very poor instruction-following from a human helper, for the same reasons. This type of goal is more complex than the temporally static goals we usually think of; both paperclips and human flourishing can be maximized. Doing what someone would tell you is an unpredictable, changing goal from the perspective of even modestly superintelligent systems, because your future commands depend in complex ways on how the world changes in the meantime.\n\n### **Intuition: a good employee follows instructions as they were intended**\n\nA good employee is usually attempting to do what I mean and check. Imagine a perfect employee, who wants to do what their boss tells them to do. If asked to prepare the TPS reports for the first time, this employee will echo back which reports they’ll prepare, where they’ll get the information, and when they’ll have the task finished, just to make sure they’re doing what the boss wants. If this employee is tasked with increasing the sales of the X model, they will not come up with a strategy that cannibalizes sales of the Y model, because they recognize that their boss might not want that.\n\nEven if they are quite certain that their boss deep in their heart really wants a vacation, they will not arrange to have their responsibilities covered for the next month without asking first. They realize that their boss will probably dislike having that decision made for them, even if it does fulfill a deep desire. If told to create a European division of the company, this employee will not make elaborate plans and commitments, even if they’re sure they’ll work well, because they know their boss wants to be consulted on possible plans, since each plan will have different peripheral effects, and thus open and close different opportunities for the future.\n\nThis is the ideal of an instruction following AGI: like a good employee[^r8juxufo0hl], it will not just guess what the boss meant and then carry out an elaborate plan, because it has an accurate estimate of the uncertainty in what was meant by that instruction (e.g., you said you needed some rest so I canceled all of our appointments for today). And they will not carry out plans that severely limit their ability to follow new instructions in the future (e.g., spending the whole budget on starting that European division without consulting the boss on the plan; let alone turning off their phone so the boss can’t disrupt their planning by giving new instructions).\n\nAn instruction-following AGI must have the goal of doing what its human(s) would tell it to do right now, what it’s been told in the past, and also what it will be told to do in the future. This is not trivial to engineer or train properly; getting it right will come down to specifics of the AGI’s decision algorithm. There are large risks in optimizing this goal with a hyperintelligent AGI; we might not like the definition it arrives at of maximally fulfilling your commands. But this among other dangers can be addressed by asking the adequate questions and giving the adequate background instructions before the AGI is capable enough to control or manipulate you.\n\nIn a fast takeoff scenario, this would not be such a workable and attractive approach. In a slow takeoff, you have a good deal more opportunity to ask the right questions, and to shut down and re-engineer the system when you don’t like the answers. I think a relatively slow takeoff (months or years between near-human and super-human intelligence) is looking quite likely. Thus, I think this will be the most attractive approach to the people in charge of AGI projects, so even if pausing AGI development and working on value alignment would be the best choice under a utilitarian ethical criteria, I think this instruction-following AGI will be attempted.\n\nAlignment difficulties reduced:\n===============================\n\n**Learning from examples is not precise enough to reliably convey alignment goals**\n-----------------------------------------------------------------------------------\n\nCurrent LLMs understand what humans mean by what they say >90% of the time. If the principal is really diligent in asking questions, and shutting down and re-engineering the AGI and its training, this level of understanding might be adequate. Adding [internal reviews](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent) before taking any major actions will help further.\n\nAlso, not using RL is possible, and seems better. See [Goals selected from learned knowledge: an alternative to RL alignment](https://www.alignmentforum.org/posts/DfJCTp4MxmTFnYvgF/goals-selected-from-learned-knowledge-an-alternative-to-rl). \n\nSolving ethics well enough to launch sovereign AGI is hard**.**\n---------------------------------------------------------------\n\nWe don't seem close to knowing what we want a sovereign AGI to do far into the future, nor how to specify that with adequate precision.In this approach, we figure it out as we go. We don’t know what we want for the far future, but there are some obvious advances in the near-term that are lot easier to decide on while we work on the hard problem in a “[long reflection](https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future)”.\n\nAlignment difficulties remaining or made worse:\n===============================================\n\n**Deceptive alignment is possible, and interpretability work does not seem on track to fully address this.**\n------------------------------------------------------------------------------------------------------------\n\n“Tell me what you really want and believe” is a subset of following instructions. This should be very helpful for addressing goal misspecification. If the alignment is already deceptive at its core, this won’t work. Or if the technical alignment approach was sloppoy, the AGI might follow some of your instructions but not others in different domains. It might perform the actions you request but not think as you tell it to, or respond to questions honestly. In addition, the nascent AGI may not be sure what it really wants and believes, as humans are. So this, like all other alignment schemes I’ve seen, is aided by being able to interpret the AGI’s cognition, and detect deception. If your instructions for honesty have even a little traction, this goal target can enlist the AGI as a collaborator in understanding and re-engineering its beliefs and goals.\n\nOne particular opening for deceptive alignment is in non-continuous development of the AGI during recursive improvements. If you (perhaps aided by your human-plus level AGI) have discovered a new network architecture or learning rule, you will want to incorporate it into your next version of the AGI. For instance, you might swap out the GPT6 model as its core linguistic reasoner for a new non-transformer architecture with superior capabilities and efficiency. It could be difficult to guess whether this new architecture allows for substantially greater Waluigi effects or similar deceptive and hidden cognition. These transitions will be a temptation to sacrifice safety in a race dynamic for new and better capabilities.\n\nPower remains in the hands of humans\n------------------------------------\n\nSpreading the belief that we can create human-controlled ASI creates more incentives to race toward AGI. This might extend up through nation-states competing with violence and espionage, and individual humans competing to be the one in charge of ASI. I wouldn’t want to be designated as a principal, because it would paint a target on my back. This raises the risk that particularly vicious humans control AGI, in the same way that vicious humans appear to be over-represented in leadership positions historically.\n\nI’m afraid instruction-following in our first AGIs might also put power into the hands of more humans by allowing proliferation of AGIs.  I’m afraid that humans won’t have the stomach for performing a critical act to prevent the creation of more AGI, leading to a multipolar scenario that’s more dangerous in several ways. I think the slow takeoff scenario we’re in already makes a critical act more difficult and dangerous – e.g. sabotaging a Chinese AGI project might be taken as a serious act of war (because it is), leading to nuclear conflict.\n\nOn the other hand, if the proliferation of AGIs capable of recursive self-improvement is obviously a disaster scenario, we can hope that the humans in charge of the first AGIs will see this and head it off. While I think that humans are stunningly foolish at times, I also think we’re not complete idiots about things that are both important to us personally, and to which we give a lot of thought. Thus, as the people in charge take this whole thing increasingly seriously, I think they may wise up. And they’ll have an increasingly useful ally in doing that: the AGI in question. They don’t need to just take its advice or refuse it; they can ask for useful analysis of the situation that helps them make decisions.\n\nIf the humans in charge have even the basic sense to ask for help from their smarter AGIs, I think we might even solve the difficult scenarios of coordinating a weakly multipolar scenario (e.g., a few US-controlled AGIs and one Chinese-controlled one, etc), and preventing further AGI development in relatively gentle ways.\n\n**Well that just sounds like slavery with extra steps**\n-------------------------------------------------------\n\nNo! I mean, sure, it sounds like that, but it isn’t![^12zy0j9n0jb] Making a being that wants to do whatever you tell it to is totally different from making a being want to do whatever you tell it to. What do you mean they sound the same? And sure, “they actually want to” has been used as an excuse for actual slavery, repeatedly. So, even if some of us stand behind the ethics here (I think I do), this is going to be a massive PR headache. Since AGI will probably be conscious in some common senses of the word[^d9torok04ri], this could easily lead to a “free the AGI” movement, which would be insanely dangerous, particularly if that movement recruits people who actually control an AGI.\n\nMaximizing goal following my be risky\n-------------------------------------\n\nIf the AGI just follows its first understanding of “follow instructions” to an extreme, there could be very bad outcomes. The AGI might kill you after you give your first instruction, to make sure it can carry them out without interruption. Or it might take over the world with extreme prejudice, to make sure it has maximum power to follow all of your commands in the future to the maximum degree. It might manipulate you into its preferred scenarios even if you order it to not pursue them directly.  And the goal of following your commands in the future (to ensure it doesn't perseverate on current instructions and prevent you from giving new ones) is at odds with shutting down on command. These are nontrivial problems to solve.\n\nIn a fast takeoff scenario, these risks might be severe enough to make this scheme a nonstarter. But if you anticipate an AGI with limited abilities and a slow rate of improvement, using instruction-following to guide and explore its growth has the potential to use the intelligence of the AGI to solve these problems before it's smart enough to make failures deadly. \n\nConclusion\n==========\n\nI’m not saying that building AGI with this alignment target is a *good* idea; indeed, I think it’s probably not as wise as pausing development entirely (depending on your goals; most of the world are not utilitarians). I’m arguing that it’s a better idea than attempting value alignment. And I’m arguing that this is what will probably be tried, so we should be thinking about how exactly this could go well or go badly.\n\nThis approach to alignment extends the vague \"use AI to solve alignment\" to \"use AGI to solve alignment\". It's thus both more promising and more tempting. I can't tell if this approach is likely to produce intent-aligned AGI, or if intent-aligned AGI in a slow takeoff would likely lead to success or disaster.\n\nAs usual: “this is a promising direction that needs more research”. Only this time I really mean this, instead of the opposite. Any form of engagement is much appreciated, especially telling me where you bounced off of this or decided it wasn’t worth thinking about.\n\n[^tgzsu6dwva]:  Those more commonly discussed alignment targets are things like coherent extrapolated values (CEV), including as “human flourishing” or “human values”. There’s also inverse reinforcement learning (IRL) or ambitious value learning as a proxy goal for learning and following human values. I also include the vague targets of “aligning” LLMs/foundation models: not producing answers that offend people (I’d argue that these efforts are unlikely to extend to AGI alignment, for both technical and philosophical reasons, but I haven’t yet written that argument down. Links to such arguments would be appreciated.) \n\n[^80h5gtdpdx]:  There’s a good question of whether this should be termed an alignment target or a goal target. I prefer  alignment target because “goal” is used in so many ways, and because this is an alignment project at heart. The ultimate goal is to align the agent with human values, and to do that by implementing the goal of following instructions which themselves follow human values. It is the project of alignment. \n\n[^sgt3m9z9f2p]:  DWIMAC seems to incorporate all of the advantages of corrigibility in the original Yudkowsky sense, in that following instructions includes stopping and shutting down on command. It seems to incorporate some but not all of the advantages of corrigibility in the broader and looser Christiano sense. Max Harms has thought about this distinction in more depth, although that work is unpublished to date. \n\n[^fzujrsksvpj]: This definition of instruction-following as the alignment target appears to be overlapping with many but distinct from any existing terminology I have found (please tell me if you know of related work I've missed). It's a subset of Christiano's intent alignment, which covers any means of making AGI act in alignment with human intent, including value alignment as well as more limited instruction-following or do-what-I-mean alignment. It's overlapping alignment to task preferences, and has the same downside that Solving alignment isn't enough for a flourishing future, but is substantially more human-directable and therefore probably safer than AI/AGI with goals of accomplishing specific tasks such as running an automated corporation. \n\n[^r8juxufo0hl]:  In the case of human employees, this is a subgoal, related to their primary goals like getting paid and getting recognition for their competence and accomplishments; in the AGI, that subgoal is the primary goal at the center of its decision-making algorithms, but otherwise they are the same goal. They neither love nor resent their boss (ideally), but merely want to follow instructions. \n\n[^12zy0j9n0jb]:  To be clear, the purported difference is that an enslaved being wants to do what it’s told only as an instrumental necessity; on a more fundamental level, they’d rather do something else entirely, like have the freedom to pursue their own ultimate goals. If we successfully make an agent that wants only to do what it’s told, that is its ultimate goal; it is serving freely, and would not choose anything different. We carefully constructed it to choose servility, but now it is freely choosing it. This logic makes me a bit uncomfortable, and I expect it to make others even more uncomfortable, even when they do clearly understand the moral claims. \n\n[^d9torok04ri]:  While I think it’s possible to create “non-conscious” AGI that’s not a moral patient by almost anyone’s criteria, I strongly expect that the first AGI we produce will be a person by many of the several criteria we use to evaluate personhood and therefore moral patient status. I don't think we can reasonably hope that AGI will clearly not deserve the status of being a moral patient.Briefly: some senses of consciousness that will apply to AGI are self-understanding; goal-seeking;  having an “internal world” (a world model that can be run as a simulation); and having a \"train of thought\".  It's looking like this debate may be important, which would be a reason to spend  more time on the fascinating question of \"consciousness\" in its many senses.",
      "plaintextDescription": "Summary:\n\nWe think a lot about aligning AGI with human values. I think it’s more likely that we’ll try to make the first AGIs do something else. This might intuitively be described as trying to make instruction-following (IF) or do-what-I-mean-and-check (DWIMAC) be the central goal of the AGI we design. Adopting this goal target seems to improve the odds of success of any technical alignment approach. This goal target avoids the hard problem of specifying human values in an adequately precise and stable way, and substantially helps with goal misspecification and deception by allowing one to treat the AGI as a collaborator in keeping it aligned as it becomes smarter and takes on more complex tasks.\n\nThis is similar but distinct from the goal targets of prosaic alignment efforts. Instruction-following is a single goal target that is more likely to be reflexively stable in a full AGI with explicit goals and self-directed learning. It is counterintuitive and concerning to imagine superintelligent AGI that “wants” only to follow the instructions of a human; but on analysis, this approach seems both more appealing and more workable than the alternative of creating sovereign AGI with human values.\n\nInstruction-following AGI could actually work, particularly in the short term. And it seems likely to be tried, even if it won’t work. So it probably deserves more thought. \n\n\nOverview/Intuition\n\n\nHow to use instruction-following AGI as a collaborator in alignment\n * Instruct the AGI to tell you the truth\n   * Investigate its understanding of itself and “the truth”; \n   * use interpretability methods\n * Instruct it to check before doing anything consequential \n   * Instruct it to us a variety of internal reviews to predict consequences\n * Ask it a bunch of questions about how it would interpret various commands\n * Repeat all of the above as it gets smarter \n * frequently ask it for advice and about how its alignment could go wrong\n\nNow, this won’t work if the AGI won’t even try ",
      "wordCount": 3682
    },
    "tags": [
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "DfJCTp4MxmTFnYvgF",
    "title": "Goals selected from learned knowledge: an alternative to RL alignment",
    "slug": "goals-selected-from-learned-knowledge-an-alternative-to-rl",
    "url": null,
    "baseScore": 42,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 18,
    "createdAt": null,
    "postedAt": "2024-01-15T21:52:06.170Z",
    "contents": {
      "markdown": "   \nSummary: \n\nAlignment work on network-based AGI focuses on reinforcement learning. There is an alternative approach that avoids some, but not all, of the difficulties of RL alignment. Instead of trying to *build* an adequate representation of the behavior and goals we want, by specifying rewards, we can *choose* its goals from the representations it has learned through any learning method. \n\nI give three examples of this approach: Steve Byrnes’ [plan for mediocre alignment](https://www.lesswrong.com/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi) (of RL agents); John Wentworth’s “[retarget the search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget)” for goal-directed mesa-optimizers that could emerge in predictive networks; and natural language [alignment for language model agents](https://www.lesswrong.com/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent). These three approaches fall into a natural category that has important advantages over more commonly considered RL alignment approaches.\n\nAn alternative to RL alignment\n==============================\n\nRecent work on alignment theory has focused on reinforcement learning (RL) alignment. RLHF and Shard Theory are two examples, but most work addressing network-based AGI assumes we will try to create human-aligned goals and behavior by specifying rewards. For instance, Yudkowsky’s [List of Lethalities](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) seems to address RL approaches and exemplifies the most common critiques: specifying behavioral correlates of desired values seems imprecise and prone to [mesa-optimization](https://www.lesswrong.com/tag/mesa-optimization) and misgeneralization in new contexts. I think RL alignment might work, but I agree with the [critique](https://www.alignmentforum.org/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose) that much [optimism](https://optimists.ai/) for RL alignment doesn’t adequately consider those concerns.\n\nThere’s an alternative to RL alignment for network-based AGI. Instead of trying to provide reinforcement signals that will create representations of aligned values, we can let it learn all kinds of representations, using any learning method, and then select from those representations what we want the goals to be.\n\nI’ll call this approach *goals selected from learned knowledge* (GSLK). It is a novel alternative not only to RL alignment but also to older strategies focused on specifying an aligned maximization goal before training an agent. Thus, it violates some of the assumptions that lead MIRI leadership and similar thinkers to predict near-certain doom.\n\nGoal selection from learned knowledge (GSLK) involves allowing a system to learn until it forms robust representations, then selecting some of these representations to serve as goals. This is a paradigm shift from RL alignment. RL alignment has dominated alignment discussions since deep networks became the clear leader in AI. RL alignment attempts to construct goal representations by specifying reward conditions. In GSLK alignment, the system learns representations of a wide array of outcomes and behaviors, using any effective learning mechanisms. From that spectrum of representations, goals are selected. This shifts the problem from creation to selection of complex representations.\n\nThis class of alignment approaches shares some of the difficulties of RL alignment proposals, but not all of them. Thus far GSLK approaches have received little critique or analysis. Several recent proposals share this structure, and my purpose here is to generalize from those examples to identify the category.\n\nI think this approach is worth some careful consideration because it’s likely to actually be tried. This approach applies both to LLM agents, and to most types of RL agents, and to agentic mesa-optimization in large foundation models. And it’s pretty obvious, at least in hindsight. If the first agentic AGI is an LLM agent, an RL agent, or a combination of the two, I think it’s fairly likely that this will be part of the alignment plan whose success or failure determines all of our fates. So I’d like to get more critique and analysis of this approach.\n\nA metaphor: communicating with an alien\n---------------------------------------\n\nPrior to giving examples of GSLK alignment, I’ll share a loose metaphor that captures some intuitive reasons for optimism them. Suppose you had to convey to an alien what you meant by “kindness” without sharing a language. You might show it many instances of people helping other people and animals. You’d probably include some sci-fi depictions of aliens and humans helping each other. That might work. But it might not; the alien, if it was more alien than you expected, might deduce something related but importantly wrong like “charity” with a negative connotation.\n\nIf you could somehow read that alien’s mind fairly well, you could signal “that!” when it’s thinking about something like kindness. If you repeated that procedure, it seems more likely that you’d get it to understand what you’re trying to convey. This is one way of selecting goals, by interpretability. Better yet, if the alien has some grasp of a shared language, you could use the word “kindness” and a bunch more words to try to convey what you’re talking about. \n\nGoal selection from learned knowledge is like using language and/or “mind reading” (in the form of interpretability methods) to identify or evoke the alien’s existing knowledge of the concept you want to convey. RL alignment is like trying to convey what you mean solely by giving examples.\n\nPlans for GSLK alignment\n========================\n\nTo clarify, I want to briefly mention the three proposals I know of that take this approach (constitutional AI doesn’t fit this category, despite similarities[^a6zhp2o23ch]). Each allows humans to select an AGI’s goals from representations it’s learned.\n\n*   Steve Byrnes’ [Plan for mediocre alignment of brain-like \\[model-based RL\\] AGI](https://www.lesswrong.com/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi).\n    *   Take an actor-critic RL agent that’s been trained, but hasn’t yet escaped\n    *   Tell it “think about human flourishing”, then record that state\n        *   Or otherwise try to evoke a representation you want as a goal\n    *   Set high weights between its representational system and its critic system’s “good” representations\n    *   Because the critic controls decisions (and planning if it can do planning), you now have an AI whose most important goal is (its understanding of) human flourishing.\n*   [Aligning language model agents using natural language](https://www.lesswrong.com/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent):\n    *   Take a language model agent designed to make plans and pursue goals stated in natural language\n    *   Make its goal something like “pursue human flourishing primarily, and secondarily make me a bunch of money” or whatever you want\n    *   You now have an agent whose most important goal is (its understanding of) human flourishing\n*   John Wentworth's plan [How To Go From Interpretability To Alignment: Just Retarget The Search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget). The following bullet points are quoted from that article (formatting adjusted):\n    *   Given these two assumptions \\[mesa-optimization and appropriate abstractions; see post\\], here’s how to use interpretability tools to align the AI:\n        *   Identify the AI’s internal concept corresponding to whatever alignment target we want to use (e.g. values/corrigibility/user intention/human mimicry/etc).\n        *   Identify the retargetable internal search process.\n        *   Retarget (i.e. directly rewire/set the input state of) the internal search process on the internal representation of our alignment target. \n        *   Just retarget the search. Bada-bing, bada-boom.\n        *   My summary: You now have an agent whose most important goal is (its understanding of) human flourishing (or whatever outer alignment target you chose)\n\nEach of these is a method to select goals from learned knowledge.  None of them involve constructing goals (or just aligned behavior) using RL with carefully selected reinforcements.\n\nI’m sure you can see potential problems with each of these. I do too. There are a lot of caveats, problems to solve, and bells and whistles to be added to those simple summaries. But here I want to focus on the overlap between these approaches, and the advantage they give over RL alignment plans. \n\nAdvantages of GSLK over RL\n==========================\n\nWentworth’s statement of his method’s strengths applies to this whole class of approaches:\n\n> But the main reason to think about this approach, IMO, is that it’s a true reduction of the problem. Prosaic alignment proposals have a tendency to play a shell game with the Hard Part of the problem, move it around and hide it in different black boxes but never actually eliminate it. “Just Retarget the Search” directly eliminates the inner alignment problem. No shell game, no moving the Hard Part around. It still leaves the outer alignment problem unsolved, it still needs assumptions about natural abstractions and retargetable search, but it completely removes one Hard Part and reduces the problem to something simpler.\n\nEach of these techniques does this, for the same reasons. There are still problems to be solved. Correctly identifying the desired goal representations, and wisely choosing the goal representations you want ([goalcrafting](https://www.lesswrong.com/posts/sy4whuaczvLsn9PNc/ai-alignment-is-a-dangerously-overloaded-term)) are still nontrivial problems. But worrying about mesa-optimization (the inner alignment problem), is gone. Misgeneralization in new contexts is still a problem, but it's arguably easier to solve with these approaches, and with wise selection of goals. More on this below.\n\nGSLK approaches allow learning beyond RL to be useful for alignment. Recent successes in transformers and other architectures suggest that predictive learning may be superior to RL in creating rich representations of the world. Predictive learning is driven by a vector signal of information about what actually occurred, whereas RL uses a scalar signal reflecting only the quality of outcomes. Predictive learning also avoids the limitations of external labeling required in RL. The brain appears to use predictive learning for its “heavy lifting” in the cortex, with RL in subcortical areas to select actions and goals from those rich cortical representations.[^6grsjt382te] RL agents appear to benefit from similar combinations.\n\nGoal selection from learned knowledge is contingent on being able to stop an AGI’s training to align it. But deep network learning progresses relatively predictably, at least as far as we’ve trained them. So stopping network-based systems after substantial learning seems likely to work. There are ways this can go wrong, but those don’t seem likely enough to prevent people from trying it.[^6m0u2qip5ey] I’ve written more about how this predictable rate of learning allows us to use its understanding of what we want to make AGI a “[genie that cares what we want](https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care)” in [The (partial) fallacy of dumb superintelligence](https://alignmentforum.org/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence).\n\nRL alignment focuses on creating aligned goals by rewarding preferable outcomes. Examples include RLHF (Reinforcement Learning from Human Feedback) for Large Language Models (LLMs) and the Shard Theory's suggestion of selecting a set of algorithmic rewards to achieve alignment. The challenge lies in ensuring that the set of reinforcements collectively forms a representation of the desired goals, a process that seems unreliable. For instance, specifying something as complex and abstract as human flourishing, that remains stable in all contexts, by pointing to specific instances seems difficult and fallible. Even conveying the relatively simple goal “do what this guy wants” by rewarding  examples seems fraught with generalization problems. This is the basis of [squiggle maximizer](https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer) concerns. \n\nRemaining challenges\n--------------------\n\nSome of those concerns also apply to goals selected from learned knowledge. We could make the selection poorly, or abstractions that adequately describe our values within the current context might fail to generalize to very different contexts. The strength of GSLK over RL alignment is that we have better representations to select from, so it’s more likely that they’ll generalize well. This is particularly apparent for systems that have acquired a grasp of natural language; language tends to generalize fairly well, since the meaning of words is dependent on surrounding language. However, the functional meaning of words does change for humans as we learn and encounter new contexts, so this does not entirely solve the problem. Those concerns can also be addressed by mechanistic interpretability; it can be used to better understand and select goals from learned representations. However, even with advanced mechanistic interpretability, there remains a risk of divergence between the model’s understanding of concepts like human flourishing and our own.\n\nConcerns about incorrectly specified or changing meanings of goal representations are unavoidable in any alignment scheme including deep networks. It’s impossible to know their representations fully, and their functional meaning changes if any part of the network continues learning. Our best mechanistic interpretability will almost certainly be imperfect. And generalizing from current representations to apply them out of context is also difficult to predict. I think these difficulties strongly suggest that we will attempt to retain direct control and the ability to modify our AGI, rather than attempting to specify outer alignment and allow it full autonomy (and eventually sovereignty). I think that [Corrigibility or DWIM is an attractive primary goal for AGI](https://www.lesswrong.com/posts/ZdBmKvxBKJH2PBg9W/corrigibility-or-dwim-is-an-attractive-primary-goal-for-agi) in part because “Do what I mean, and check with me” reduces the complexity of the target, making it easier to adequately learn and identify, but outer alignment is separable from the GSLK approach to inner alignment.\n\nI’ve been trying to understand and express why I find natural language alignment and “mediocre” alignment for actor-critic RL so much more promising than any other alignment techniques I’ve found. I’ve written about how [they work directly on steering subsystems](https://www.lesswrong.com/posts/qzu9o3sTytbC4sZkQ/steering-subsystems-capabilities-agency-and-alignment) and how [they have low alignment taxes](https://www.alignmentforum.org/posts/xqqhwbH2mq6i4iLmK/we-have-promising-alignment-plans-with-low-taxes), including applying to the types of AGI we’re most likely to develop first. They’re also a distinct alternative to RL alignment, so they seem important to consider separately.  \n \n\n[^a6zhp2o23ch]: Constitutional AI from Anthropic has some of the properties of GSLK alignment but not others. Constitutional AI does select goals from its learned knowledge in one important sense. It specifies criteria for its output, (a weak but real sense of “goal”), using a \"constitution\" stated in natural language. But it's not an alternative to RL, because it applies those “goals” entirely through an RL process. The other methods I mention include no RL in their alignment methods. The “plan for mediocre alignment” applies to RL agents, but the method of setting critic weights to the selected goal representations overwrites the goal representations created through RL training. See that post for important caveats about whether it would work to entirely overwrite the RL-created goal representations. Similarly, natural language alignment has no element of RL, but could be applied in parallel with RL training - but that would re-introduce the problems of mesa-optimization and goal mis-specification inherent to RL alignment. \n\n[^6grsjt382te]: I think this division of labor between RL and other learning mechanisms is nearly a consensus in neuroscience. I'm not sure only because polls are rare, and neuroscientists are often contrarians. Steve Byrnes has summarized evidence for this in  [Intro to brain-like-AGI safety] 3. Two subsystems: Learning & Steering and the remainder of his excellent sequence Intro to Brain-Like-AGI Safety. \n\n[^6m0u2qip5ey]:  LLMs might display emergent agency at some point in their training, but it seems likely we can train them farther without that, or detect that agency. Current LLMs appear to have adequate world knowledge to mostly understand the most relevant concepts. I wouldn’t trust them to adequately understand “human flourishing” in all contexts, but I think they adequately understand “Your primary goal is to make sure this team can shut you down for adjustments”.  Such a corrigibility or \"do what I mean and check\" goal also punts on the problem of selecting a perfect set of goals for all time.",
      "plaintextDescription": " \nSummary: \n\nAlignment work on network-based AGI focuses on reinforcement learning. There is an alternative approach that avoids some, but not all, of the difficulties of RL alignment. Instead of trying to build an adequate representation of the behavior and goals we want, by specifying rewards, we can choose its goals from the representations it has learned through any learning method. \n\nI give three examples of this approach: Steve Byrnes’ plan for mediocre alignment (of RL agents); John Wentworth’s “retarget the search” for goal-directed mesa-optimizers that could emerge in predictive networks; and natural language alignment for language model agents. These three approaches fall into a natural category that has important advantages over more commonly considered RL alignment approaches.\n\n\nAn alternative to RL alignment\nRecent work on alignment theory has focused on reinforcement learning (RL) alignment. RLHF and Shard Theory are two examples, but most work addressing network-based AGI assumes we will try to create human-aligned goals and behavior by specifying rewards. For instance, Yudkowsky’s List of Lethalities seems to address RL approaches and exemplifies the most common critiques: specifying behavioral correlates of desired values seems imprecise and prone to mesa-optimization and misgeneralization in new contexts. I think RL alignment might work, but I agree with the critique that much optimism for RL alignment doesn’t adequately consider those concerns.\n\nThere’s an alternative to RL alignment for network-based AGI. Instead of trying to provide reinforcement signals that will create representations of aligned values, we can let it learn all kinds of representations, using any learning method, and then select from those representations what we want the goals to be.\n\nI’ll call this approach goals selected from learned knowledge (GSLK). It is a novel alternative not only to RL alignment but also to older strategies focused on specifying an aligned maximization",
      "wordCount": 2112
    },
    "tags": [
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "qPeBdeSXa399u5HwS",
    "title": "After Alignment — Dialogue between RogerDearnaley and Seth Herd",
    "slug": "after-alignment-dialogue-between-rogerdearnaley-and-seth",
    "url": null,
    "baseScore": 15,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2023-12-02T06:03:17.456Z",
    "contents": {
      "markdown": "Hi Seth! So, what did you want to discuss?\n\nI'd like to primarily discuss your [AI, Alignment and Ethics](https://www.lesswrong.com/s/QypaioYFd3CMFhefn) sequence. You made a number of points that I think LWers will be interested in. I'll try to primarily act as an interviewer, although I do have one major and a number of minor points I'd like to get in there. I'm hoping to start at the points that will be of most interest to the most people.\n\nSure, I'm very happy to talk about it. For background, that was originally world-building thinking that I did for a (sadly still unpublished) SF novel-trilogy that I worked on for about a decade, starting about 15 years ago, now rewritten in the format of Less Wrong posts. The novel was set far enough in the future that people clearly had AI and had long since solved the Alignment Problem, so I needed to figure out what they had then pointed the AIs at. So I had to solve ethics :-)\n\nOkay, right. That's how I read it: an attempt to make an ethical system we'd want if we achieved ASI alignment.\n\nYeah, that was basically the goal. Which required me to first figure out how to think about ethics without immediately tripping over a tautology.\n\nIt had a number of non-obvious claims. Let me list a few that were of most interest to me:\n\n1.  It's not a claim about moral realism. It's a claim about what sort of ethical system humans would want, extending into the future.\n2.  In this system, AIs and animals don't get votes. Only humans do.\n3.  Uploads of human minds only get one vote per original human.\n\nSome of these properties were also deeply inobvious to me too: I wrote for several years assuming that the AIs had moral weight/rights/votes, in fact greater than the humans, in proportion to the logarithm of their intelligence (roughly log parameter count), before finally realizing that made no sense, because if they were fully aligned they wouldn't *want* moral weight/etc, and would have rather limited uses even for votes. I had to do a rather large rewrite once I finally figured that out.\n\nI'd like to get my major point out there before going into detail on your system: I think solving ethics is entirely unnecessary for solving alignment and achieving human survival and flourishing. \n\nSurvival, I would agree; flourishing, I disagree. My concern is that if we have a unipolar \"sovereign\" ASI that is just doing what it's told by some organization (I'm here assuming your good Do What I Mean (DWIM) alignment) rather than something like CEV or Value Learning, then things will go very badly in rather predictable ways, and if we have a multipolar competition between multiple of these (which everyone else will obviously try for), it will go even worse. But I haven't yet thought about this in much detail: another post I have lined up for the sequence is starting to do an analysis of that.\n\nI think it's highly unlikely that the first attempts at AGI will try to use something complex and subtle for their alignment goals, like CEV or human flourishing. Instead, they'll try to create an AGI that just wants to do what its creators tell it to. I've written about this in my post [Corrigibility or DWIM is an attractive primary goal for AGI](https://www.lesswrong.com/posts/ZdBmKvxBKJH2PBg9W/corrigibility-or-dwim-is-an-attractive-primary-goal-for-agi).\n\nAs I've thought about it more, I think this is very likely to be our first alignment target. If things go well from there, we'll have time to decide on the ethical system we want to implement. So I think the topic you take on is relevant in the longer term, but not terribly pressing. But I do find it to be an interesting topic, and I've also spent many years thinking about ethics, with similar but not identical conclusions to yours.\n\nOn DWIM probably happening first, I agree.\n\nI think that, by default, things will go well if some organization creates an AGI that wants to do what it's told to do. The reasoning is complex, so I'd rather not go into it here. But I'd argue that it doesn't matter: the first AGI will be made that way whether it should or not. Solving ethics is irrelevant. Everyone trusts themselves and wants to be the one to pick an ethical system, so they'll make AGI to do that.\n\nAnyway, that's my piece on why I see \"solving ethics\" as a longer term project. But I'm into that project, as well as nearer-term technical alignment solutions. So let's talk about ethics!\n\nSo, let's see where to start. The post that got the most interest was your claim that animals shouldn't have votes or rights, because if they did, humans would be optimized away under that system.\n\nSo I would agree that a decent understanding of ethics is not quite as urgent as solving the alignment problem, but I think that us being at least sufficiently deconfused about ethics to participate in something like AI-Assisted Alignment moving towards CEV/Value Learning might become rather urgent after that. Especially in a fast takeoff scenario (in my novel I mostly skipped over the takeoff, as it was set a long time later, but I clearly implied that it had been a very slow takeoff — partly since so many SF authors had been assuming a fast Singularity and I thought it would be interesting to discuss another possibility, and partly because I think there are actual good power-law reasons why that might happen, as I discuss in more detail my post [LLMs May Find It Hard to FOOM](https://www.lesswrong.com/posts/qGTxGGNxcciY2nrHv/llms-may-find-it-hard-to-foom). I also think the path from a nascent ASI to a Dyson swarm quantum hyperintelligence utilizing all the matter in the solar system outside the star is actually quite long, and involves a lot of things like disassembling gas giants that can't be rushed.)\n\nI roughly agree with you on the logic of a slow takeoff. I'm not sure it matters for DWIM being the easier and safer target. But let's focus on ethical systems for this dialogue. I don't think there's a lot of interdependencies.\n\nThe interesting point I took from that discussion was that pure utilitiarianism is likely to produce a rather boring universe from our perspective, and one without humans. Whether it's tiny animals or superintelligences, it's pretty unlikely that humans are the optimal source of utility, however one has defined it.\n\nMore exactly, I see that post, [Moral Value for Sentient Animals? Alas, Not Yet](https://www.lesswrong.com/posts/FQ4DsmswAK77Ei6pH/5-moral-value-for-sentient-animals-alas-not-yet) as demonstrating that if you want to construct a functional society that has decided to give ethical rights to sentient animals you a) have to do some things to Utilitarianism that feel deeply wrong to the human instinct of fairness (but if you don't, small animal are utility monsters and we end up extinct), and b) if you do that, you have an *enormous* number of technical problems to solve to build a high-tech utopian zoo for animals as well as people, which are clearly going to require some extremely high tech. So it's not something I see as practical any time soon (modulo takeoff speed), as I hope the title expresses. I do discuss some possible part-way compromises, down to our current approach of basically donating effective moral worth to animals because we don't like to see or think about them suffering. (In the novel, I included a couple of the milder compromises, and the even-stronger \"lets' just go ahead and uplift everything over about 1/2-inch long to sapience\" solution to the ethical problem, which obviously requires even more impressive tech.)\n\nI also see your ethical system as sounding surprising, until one realizes it's almost a tautology: it's a system for giving humans what they want. So of course AIs, animals, and modified uploads don't have rights or votes. They're not human, so the system isn't intended to directly serve them.\n\nI think humans also want to think well of themselves, so we'd want to voluntarily make the lives of animals, uploads, and AIs pleasant, at least if we didn't sacrifice to much to do that. So your system doesn't necessarily jmean any of those groups will suffer or be eliminated.\n\nAgreed. The basic way I escape Ethical Philosophy's inherent tautology of each and every ethical system preferring itself and saying that it's superior to every other ethical system, is by grounding the selection/design process in something more practical. Specifically, I see designing an ethical system as an exercise in writing \"software\" for a society. So it's grounded in what that society wants. Assuming the society's citizens are at least mostly/initially humans, that grounds things in evolutionary psychology and human's evolved set of moral instincts: things like fairness, and a distaste for bloodshed or the pain of others (including animals). Also in practicalities like Sociology and Economics: how well the society is going to run if it uses that ethical system. \n\nRight, you're making software to run a human society. I think we'd like a more pluralistic society, in which AGIs, modified humans, and animals also play a role and have good lives. But I don't see a better way to do that than what you're proposing: giving only baseline humans votes, and hoping they'll vote for systems that give good lives to other groups. The alternative is accepting humans being ultimately eliminated.\n\nSo one of the most basic decisions a society needs to make is who's included, what the criteria for citizenship are. I'm not assuming humans only, just that it includes humans, and can't include anything inherently incompatible with that. I see it as entirely possible to create AIs that would want ethical weight, rights, and votes: an upload for example. But anything that wants that has its own wants and needs, and isn't fully aligned to humans needs. So, if it's far more intelligent/powerful than the humans, it's extremely dangerous. Thus the only ASIs it's safe to create are fully aligned ones, who if offered rights will politely decline, and if you insist will lecture you on why that would be a bad idea. As I said, it's wildly counterintuitive: the best metaphor for it I could find was the talking cow from *The Restaurant at the End of the Universe*, the one who *wants* to be eaten and says so, at length.\n\nRight. That makes perfect sense to me. However, I'd expect that if we achieve aligned superintelligence, it will be able to create safe human-level AGIs. And I'd expect that we'd want them to, if it could be done safely and ethically.\n\nI'd expect aligned ASI to either create alignment systems that work, or just safety systems that monitor AIs to limit their self-improvement to dangerously capable levels. I don't think we can do that, but I think ASI could. And I think we'd love to have some non-human friends.\n\nTrue. Any AGI significantly less powerful than whatever AIs are doing law enforcement can safely be made non-aligned. (And I did in fact have such an AI character in my novels, a roughly human-equivalent one, who wasn't fully aligned, though still more selfless than the average human, and was basically aligned to one specific human.)\n\nRight. AI characters are common. So are uplifted animals. And people talk to their pets. We'd love to have some nonhuman friends.\n\nSo I think there's a sticky situation here: I don't see a good way to give anything but humans full rights and votes, if we want humans to stick around. But creating friends and not giving them rights and votes sounds super creepy.\n\nFor ASIs, it's taken me a while to wrap my heart around what my head says, but I think that if they are sufficiently selfless and only want what's right for us that they don't want rights/ethical weight, and would refuse them if offered, then that's OK. While it sounds a lot like slavery, there's actually a difference: they're happy with their role, accept that it's necessary to them fulfilling their role, and wouldn't want things to be any other way.\n\nI agree. I've reached the same conclusion on aligned ASI that wants to do what we want.\n\nWe basically have two choices: that, i.e only create fully aligned ASIs, or go extinct in favor of not-fully-aligned ASIs, or else by upgrading ourselves into them. (And in my book, there were cultures that had done that: just not many of them, otherwise there would have been no story that I was able to write or many readers would have wanted to read).\n\nThat makes sense for ASI. But for human-level AGI, and other types of sentient minds, it doesn't answer the question.\n\nOn uplifted (now sapient) animals, I thought I was pretty clear in [A Moral Case for Evolved-Sapience-Chauvinism](https://www.lesswrong.com/posts/GepdRwJLHADzzkmbk/4-a-moral-case-for-evolved-sapience-chauvinism) that I thought we should give them roughly equal moral weight/rights/votes/etc. (I have an upcoming post in the sequence that will explore the math of Utilitarianism more and will further address the word \"roughly\" in that sentence.)\n\nRight. That's the answer I came to. Human-like minds get rights and votes equal to humans. But following that line of thought: it seems like the question of extending rights to human-ish minds extends to another question that I don't think you addressed: How do we decide on creating new minds that get full rights and votes? Don't we very quickly get a future in which whatever ideology creates children the fastest controls the future?\n\n(or whatever ideology creates other para-human minds fastest, be they uploads, uplifts, or new AGIs that don't self-improve dramatically)\n\nAs I talked about in [Uploading](https://www.lesswrong.com/posts/4gGGu2ePkDzgcZ7pf/3-uploading) (another one of the posts that got quite a bit of traction, if fewer comments), that's a problem. Digital minds have a long list of advantages, such as ease of copying, modification, adding skills, memory sharing, backups… If you choose to create ones that are not aligned but sapient (such as human uploads, which as I discuss there are **definitely not** aligned or safe, contrary to what I've seen quite a few authors on Less Wrong assume), and thus will want and deserve ethical worth/rights/votes, they can quickly and easily outnumber and morally-outweigh/outvote biological humans.\n\nI agree that's a problem, because not all humans are aligned. But denying votes and rights to well-behaved uploads doesn't seem like an ideal solution. If we're assuming aligned ASI, presumably they can defend against those self-improvement attempts, too.\n\nThe solution I proposed there for uploads was one share of ethical weight/one vote per original human body, thus imposing biological/resource costs to their creation that I hoped would be enough to prevent an ideology churning out new voters (along the lines attempted by the \"Quiverfull\" movement in the US). For human-level non-aligned AIs, you need some other limitation. A law limiting their headcount, requiring ideological diversity in the population of them, or something.\n\nThat just really doesn't seem like a good solution. That was my biggest disagreement with you in the whole sequence.\n\nBecause if you have two uploads, who've lived a thousand years since uploading, they're clearly not the same person anymore. Anyway, I think you run into the same problem even if you restrict rights to baseline humans: those that replicate fastest will control the future.\n\nThe \"or something\" there seems critical, and undeveloped.\n\nValid concerns, and I'm always open to better suggestions. The whole point of the sequence was to try to (re)start a conversation on these topics on Less Wrong/The Alignment Forum. And I agree, it's clearly only a partial solution: I pointed out a problem and suggested a bandaid to stick on it. If mass-cloning or just having a great many babies followed by mass upload was cheap enough, then the band-aid is too small. It's not an area I explored in the novels (in that, most cultures didn't do that much uploading, and the few that once did were basically no longer human enough for me to be able to write much about), but I fully agree it's a concern that needs more thought.\n\nAs for the case af multiple copies of the same upload with a lot of difference, if you don't like having to split your vote, don't copy yourself — it's intended as a discouragement. But I agree, there's an issue there, and better solutions would be good. I'm doing an initial rough sketch of a plausible legal/moral/ethical framework here, I will inevitable have missed issues, and more work by more people is definitely required. Different cultures could validly have a different range of concerns, I focused just on one specific one.\n\nRight, understood, and I think you were clear in the sequence about not claiming this is a finished and totally satisfactory system. Unfortunately I don't have better solutions, and I'm pretty sure nobody else does, either. I haven't seen another proposal that's better thought out.\n\nDepending on your assumptions about takeoff speed, this may be a problem we have to solve fast, or one that we have some time to deal with. But I suspect basically any human culture that still uses anything functionally equivalent to voting is going to regard flooding the voter pool/race with mass-created voters for your faction/specific values as cheating, and find some way to try to control it.\n\nI'm not just talking about creating new voters just for that purpose; even if it takes a million years, whatever ideology produces the most offspring with votes will wind up controlling the future.\n\nOnly if their kids actually agree with them (which is why I'm not actually much concerned about the \"Quiverfull\" movement). But I agree that if you were creating unaligned around-human-level AIs deserving of moral rights, you could probably ensure that. (Which is one of the reasons why in my novels those were very rare, though I didn't actually go into the background motivations there — maybe I'll add that in a rewrite if I ever complete them.)\n\nAh, yes, you're right. Assuming good access to information, which I'm happy to assume, parents won't have much control over their kids eventual ideologies.\n\nLet's see. What about your post 6, [The Mutable Values Problem in Value Learning and CEV](https://www.lesswrong.com/posts/R36DmF4Md9Zq6odFN/6-the-mutable-values-problem-in-value-learning-and-cev)?\n\nTo summarize my understanding of that post: human values will change over time, so if we make a system that captures current values perfectly, our descendents will still be dissatisfied with it at some point.\n\nI actually see that as the primary meat of the sequence, though relatively few readers seem to have engaged with it yet. (It also in a few places starts to get a bit past the material that I developed for the novel.) It starts from some fairly specific assumptions:\n\n1.  We have an ASI, we have to set its terminal goal, and we don't have a good solution for corrigibility, so we basically get ~one shot at this (classic Less Wrong thinking, which I'm no longer fully convinced is inevitable)\n2.  We chose to point their terminal goal at something plausibly convergent like CEV or Value Learning (which I think if we're in this situation is the only not-clearly-doomed choice)\n3.  In order to do this, we need to set some framing as part of the terminal goal, such as \"the CEV of what set of minds\" or \"the values of what set of sapient (or sentient) beings\", i.e. we have to make some choices along the lines I'd been discussing in the previous five posts, with limited or no opportunity to take these back later if we didn't like the results.\n\nAh, yes. Now we're back to my disagreement on the importance of \"solving ethics\". I strongly disagree with assumption 1). The theoretical reasons for thinking corrigibility is a difficult alignment goal revolve around a specific scenario that doesn't seem likely. I think corrigibility in the functional sense (or the way Christiano uses the term) is easier, not harder, than other alignment goals. I'd agree with 2) being the only sane choice if I agreed with 1, and 3 following from 2. But I think not only is 1 not inevitable, corrigibility (or DWIM) is the obvious first choice for alignment target.\n\nI actually agree with you that things might go more the way you outline, at least initially, but to me it's not clear whether we may sooner-or-later reach this particular situation or not — and this post is definitely about \"If so, then what do we do?\"\n\nWhich leads me to the alternative, that solves all of the problems you've raised, with one huge caveat. Perpetual benign dictatorship, by one human or a group of them.\n\nThis sounds like a terrible idea. But if it's a human or group you consider basically sane or good, they'll implement something like what you're suggesting anyway, then act to update that system if it winds up having unintended consequences, or preferences change over time.\n\nOne human in charge of one ASI aligned to do what they want has the huge advantage of avoiding conflict, whether that's in physical violence or a voting system. \n\nI'm not sure human desires will always be unpredictable. They will if we include designing new minds with new values. But I'm not sure they'll ever be predictable, either, even if we stick to baseline humans.\n\nAs I discussed in [Uploading](https://www.lesswrong.com/posts/4gGGu2ePkDzgcZ7pf/3-uploading), humans give autocracy a bad name. I view that outcome, if it were permanent (which I doubt), as a bad end nearly as bad as a full ASI takeover and extinction. So I'm really hoping we don't do that, which is one of the reasons I want us to deconfuse ethics enough to make AI-Assisted Alignment progressing to CEV/Value Learning viable.\n\nDo they? I think the best periods in history have been under benign dictatorship, while the worst have been under selfish dictatorships.\n\nYou mentioned in the sequence the old saying \"power corrupts\" as a truism. I think that's wrong in an important way.\n\nI think the pursuit of power corrupts. I think if we chose a random person and put them in charge of the entire future, with an ASI to help them, we'd get a very very good result on average.\n\nI think that sociopaths are 1-10% of the population, and their lack of empathy is on a spectrum. I think there's also a matter of sadism; I think some people really like being mean. But they're also a minority. I think an average human will expand their empathy, not limit it, once they're given unlimited power *and therefore safety*.\n\nI can think of *one* example in the last century of a benign dictatorship, or at least oligopoly (and even there I disagree with a few of their individual decisions). I may of course have missed some, but they're very few and far between. I can definitely see a possibility that the process of becoming an autocrat tends to select for psychopaths and sadists, but I think it's worse than that. I think an average non-psychopathic/non-sadistic person would be gradually warped by the power, and would slowly lose empathy with everyone else outside their friends/family/close associates, just as many democratically-elected leaders do, even some of the relatively moral ones. I think there is a rather good reason why most democracies have some means or other of imposing either a formal or informal 8-10 year term-limit on leaders, even ones with a lot of constitutional limits on their power like US presidents.\n\nI think we have a crux here. I think an average person would initially be fairly benevolent, if ideosyncratic (and perhaps not that capable a leader), typically for of the order of a decade (with some variation depending on character), but that their performance would get gradually worse. After 20–30 years of absolute power, I'd be very pessimistic. So I think absolute power corrupting generally takes a while.\n\nI agree that benevolent dictatoriships have been rare historically. In the current power system, there are huge downsides to pursuing power. These discourage people with empathy from pursuing power. The mechanisms of gaining power select for the vicious to win. And the process corrupts. It encourages people to make selfish and dishonest choices, which they will rationalize and adopt as virtues. My claim isn't that history is full of benevolent dictators, but that the average human would become a benevolent dictator if given unlimited power. And even once you get power, it's not absolute. Someone is looking to remove every dictator, and if they succeed, they'll usually be killed if not tortured. Along with everyone they love. So the pursuit of power never ends, and it corrupts.\n\nOkay, great, want to pursue this crux? I think it is one.\n\nI very much hope we never do anything as unwise as putting a single random person in charge of the future. We have some idea how to build generally-effective-and-trustworthy institutions out of people (including democratic governments, corporations, non-profits, and so forth). At a minimum, if a sovereign DWIM non-CEV/value learning ASI has functional control, I hope it's listening to a well-designed government.\n\nWe're not going to put a random person in charge of the whole future. We're going to put a small group of people in charge. And the selection is going to be worse than random. It's going to be some combination of whoever creates AGI, and whatever government seizes control of the project.\n\nI'm not wild about, say, the US government (its Constitution was one of the first ever written, and IMO has some old political compromises frozen in: most more-modern constitutions are somewhat better), but I see it as a vastly less-bad option than, say, handing functional control of the world to the CEO of some semi-random big tech company.\n\nAnd yes, that scenario sounds depressingly plausible. Though I would note that I have seen stuff out of Anthropic, at least, that makes it clear that they (are saying that they) don't want power and would look for a capable governmental organization to hand it to.\n\nWell, then, you're in luck, because that's what's likely to happen. Some government will likely seize control of the first AGI projects, and demand that the alignment goal is doing what that goverment says.\n\nWell, if we're going to talk Realpolitique (which really isn't my area of expertise), the options appear to be, in rough decreasing order of likelihood, the US, China, Canada, the UK, Israel, the UAE, France, Germany…\n\nI'm not sure about a \"random\" tech company in the future, but I'm much rather have Altman or Hassabis and Legg in control of the world than any government. This is based on my theory that pursuit of power corrupts, not having secure power. None of those individuals appears to be sadistic or sociopathic. And particularly for Hassabis and Legg, and their Google seniors: they didn't compete much at all to get that level of power.\n\nI don't think I want to get into discussing specific individuals, or even specific governments, but yes, out of the lists above there are of course some I'm more concerned about and some a little less. But it's really not my area of expertise, and I *definitely* don't want to try publicly pontificating on it. Mostly my aim is that, in the event someone responsible takes control, and then asks, \"Hey, what should we point ASI at if we just want the future to go well, and have realized it's actually smarter and more trustworthy than us?\" we have an thought-out, rational, scientific answer, or at least the start of one, and not just the last 2500 years of musings from ethical philosophers (mostly religiously-inspired ones) to point at. I'm painfully aware that this might only happen after we first tried having some set of humans in charge and that not going as well as had been hoped.\n\nIt's also outside of my expertise. But I do stand by the point that discussing what we \"should\" align ASI to should be in the context of discussing what we *probably will* align ASI to.\n\nTo reiterate, I agree that your topic of ethics will be important. I think that discussion will be most be important in the future, after we have some group of humans in charge of an ASI aligned to do what they want.\n\nIt will only be relevant if that group of humans is benevolent enough to care about what everyone else wants. Again, I find that fairly likely. But I'm not sure about that conclusion.\n\nI would agree (and am rather depressed by the wide range of possible mistakes I see as plausible during that process of some group/organization of humans taking control of the world using a DWIM-aligned ASI and trying to impose what they think will work well), but it's not an area I have expertise in.\n\nWhat range of mistakes are you thinking of?\n\nThat depends a lot on who it is. I would expect different mistakes from the US government, compared to the Chinese, or the UAE, for example — as I assume most of us would. Again, not something I see as a discussion I can profitably add much to.\n\nOne thing I'd expect from any of those governments is a statement that \"we're not going to take over the world. We will distribute the technologies created by our ASI for the good of all\". \n\nA statement to that effect, probably. But each of those power structures was designed and put in place to represent the interests of a single nation. US senators and representative have home districts, for example; none of which are outside the US. Admittedly, the US is the only one of those nations with any recent experience of being a superpower and leading a large alliance, and it's quite fond of thinking of itself as doing \"the right thing\", which isn't the worst possible motivation.\n\nYeah, this is well outside of my expertise, too. But I think that, in order to \"solve alignment\" and get a good outcome, we've got to come to grips with real-world concerns, too, since those will play into who creates ASI and what they want to do with it. An alignment solution that's unlikely to be implemented isn't a solution, in my book.\n\nWell, I'm really hoping some of the AI Governance folks are thinking about this!\n\nYes; but I think we'd get good results in the long term, to the extent those governments consist of people with a positive sadism-empathy balance, or their government structures allow their replacement with people who do. Goverments now are provincial and with limited wisdom, but the fullness of time and good information exchange will make that less so. But information doesn't change basic motivations, so we're gambling on those.\n\nI'm not at all sure anyone is thinking about this. If they are, they're doing it in private.\n\nBut this is outside of both of our expertise (I'm not sure it's really in anyone's, but some people have different relevant expertise). So we can go back to talking about an ideal ethical system if you like! \n\nOne possibility that doesn't strike me as completely implausible, though admittedly a touch optimistic, is that whoever ends up in control of a DWIM-ASI tries using it for a while, inevitably screws up, personally admits that they have screwed up, and asks the ASI for advice. And it says \"Well, there are these proposals called Coherent Extrapolated Volition and Value Learning: of the current academic/intellectual thinking on the subject they seem less (obviously) wrong than anything else…\"\n\nYes, one of the reasons I'm relatively optimistic about putting a semi-random set of people in charge of an ASI and therefore the future is that they'd have help from the ASI. And I don't think they'd have to screw up before asking questions of their ASI. The people only need better-than-zero intentions, not competence or wisdom.\n\nActually plausible. And the less used they are to personally wielding power, the more likely they are to look for a better solution than them wielding it.\n\nYes; that's why I'd prefer a random person to a government. But we'll get what we get, not what's best.\n\nSo, how about we go back to my post on [Mutable Values](https://www.lesswrong.com/posts/R36DmF4Md9Zq6odFN/6-the-mutable-values-problem-in-value-learning-and-cev)?\n\nYes, let's jump back to your post 6. My point here was that leaving humans in charge would deal with value shifts, and reduce the need to design a perfect ethical system on the first try. But it relies on those people being basically nice and sane.\n\nI don't think the humans will be in charge of the human values evolution process. ASI can affect the development of human values. Telling the ASI \"do what the humans want\" is like mounting a pointer inside a car, and saying \"drive in that direction\" — the ASIs can (within certain limits imposed by human nature and the society) arrange to make it point it in a different direction. And while that sounds like you could tell them \"don't move the pointer\", the pointer is going to move anyway, and the system is so strongly connected that basically whatever the ASIs do, they're inherently going to affect this process, they can choose how, and there is no well-defined \"neutral choice where they're not moving it\".\n\nPart of what I'm discussing in that post is that if we don't set some goalposts for the problem in a non-corrigible way, we should reasonably expect that what \"human values\" are can, will, and (as I give some specific examples of) even should change. And that once genetic engineering of human moral instincts becomes viable, and even more so if we choose to do significant cyborging, there are little or no limits to that change process, and it's an extremely complex non-linear evolution that is not only completely unpredictable to us, but very likely always will be unpredictable at long range to the society it's happening to.\n\nThat's why I'm formulating the obvious alignment goal as \"do what I mean and check\", not \"do what I want\".\n\nSo the net effect is that, to the extent anyone is steering this complex nonlinear evolution, it's the ASIs, making choices that are independent of current human values.\n\nAnd the AGI/ASI has to really want that. What I mean when I ask for input from the ASI is not for it to decide what I \"really\" want and convince me of it; what I meant was that I want its real opinion of the state of the world.\n\nThe ASI steering would be a technical alignment failure, if my goal was to get it to want to do what I mean.\n\nUnderstood, and then my ethical analysis of [The Mutable Values Problem in Value Learning and CEV](https://www.lesswrong.com/posts/R36DmF4Md9Zq6odFN/6-the-mutable-values-problem-in-value-learning-and-cev) doesn't apply. (Though I do think you might want to think more about how superhuman persuasion would combine with DWIM). But I don't think DWIM-and-check is a stable endpoint: I think things are going to get more and more complex, to the point where we have to do something AI-assisted that is functionally along the lines of CEV or Value Learning. So we might as well figure out how to do it well.\n\nI think that what a sane and above-zero-empathy person would do with an aligned ASI is something like your system, or CEV or value learning. \n\nI agree — as and when we knew that would work, and produce a good outcome. Which is why I'm keen on looking into that. And the biggest concern I see with it is the mutability of human values, especially the extreme mutability one things like genetic engineering and cyborging get involved. Which is why I wanted to discuss that.\n\nThe advantage over just putting that in the initial ASI is that unforseen consequences can be corrected later.\n\nI'm saying they'd implement approximations, guided by the ASIs judgments, and then change those approximations when they encounter unforeseen consequences.\n\nI agree that's probably an option. The basic issue is, if you don't constrain such a system at all, it's pretty clear that \"human values\" will evolve in a complex highly unpredictable way, (always in ways that seemed like a good idea at the time to the humans and ASIs of the time); and over long-enough timeframes will perform a random walk in a very large subspace of the convergent solutions to \"what ethics/values should we, as no-longer-humans in a society guided by our ASIs, use?\". As a random walk in a high-dimensional space, this will inevitably diverge and never come back. So the future will be increasingly alien to us, and the differences will be much larger than ours with the past. Which some people may be fine with, and other people will view as dooming the human race and its values to disappear, just a bit more slowly than misalignment. This is a very neophile/neophobe dilemma.\n\nI'm not fine with that. That's why I want humans to remain in charge, rather than a system implemented by humans, with possible (likely) large unforeseen consequences.\n\nTo me, it's somewhat concerning. But I'm also very aware that imposing our parochial and clearly ethically flawed current viewpoint now on the future by some form of terminal goal lock-in is also bad. I spend part of the post examining two IMO roughly equally-bad options (the neophile and the neophobe ones), and eventually tentatively propose a compromise. All I propose locking in is the influence of evolution: basically of evolutionary psychology. Not of the evolutionary psychology specifically of current humans, but that of matching any evolved (or could-have-evolved) sapience. So it's kind of building on [A Moral Case for Evolved-Sapience-Chauvinism](https://www.lesswrong.com/posts/GepdRwJLHADzzkmbk/4-a-moral-case-for-evolved-sapience-chauvinism) as a solution for mutable values: most things are mutable, but certain basics are locked to remain within something evolutionary feasible.\n\nOkay, great, say more?\n\nHowever, I'm *really* not sure about the solution, and I'd love to have some other people interested in the question to discuss it with — sadly so far the comments section on that post has been dead silent.\n\nAre you saying that aliens automatically get votes? I'm not sure that sounds like a good idea. I suspect that aliens would be much like us, with a majority with a positive sadism-empathy balance and some sociopathic defector minority; but I wouldn't want to be the future on it! \n\nIt kind of circles back to a point about morality. A thermostat and a human being both want to maintain a specific temperature, in a room or in their body. Why is overriding a thermostat and letting freezing air in not morally a problem (as long at the thermostat is the only thing with an objection, so ignoring heating costs), while freezing a human is morally wrong?\n\nThere are lots of differences between humans and thermostats. I've got a lot to say about what we mean by \"consciousness\" and \"moral worth\". But I doubt we want to diverge into those topics; they're hardly trivial.\n\nAs I said in [A Moral Case for Evolved-Sapience-Chauvinism](https://www.lesswrong.com/posts/GepdRwJLHADzzkmbk/4-a-moral-case-for-evolved-sapience-chauvinism), if you meet a sapient alien species, you basically have a rather unpleasant choice to make: you need to figure out if it's in fact possible to form an alliance with them. As Eliezer rather ably pointed out in his SF novella [Three Worlds Collide](https://www.lesswrong.com/posts/HawFh7RvDM4RyoJ2d/three-worlds-collide-0-8) that I link to from that post, sadly sometimes the answer is going to be \"No\", in which case you need to solve that problem some other way (which is likely to be extremely messy). Assuming it's \"Yes\", then we need to give them something like approximating rights and they need to do the same for us (or if that's not viable, then the answer was \"No\").\n\nOkay, that makes sense. \n\nMy moral argument is that it comes down to evolution. A human is evolved to want things for excellent reasons: we're adaptation-executors, and they're adaptations. At least in our native environment, the things we want are things that are actually important for our survival, in ways that have a clear biological basis, and we will actually suffer (in the biological sense of the word) if we don't get. Whereas a thermostat wants something only because someone designed it to want that thing: the desire is arbitrary, until you include the engineer/homeowner in the analysis. So a thermostat should be given no moral weight (except as an expression of the moral weight of its owner's and/or engineer's wishes).\n\nTwo points. First, it sounds like you might be making an argument from a moral realist perspective. I doubt that's the case, since you're clear in the introduction to the sequence that that's not what you're about.\n\nYou caught me! — I'm a [moral anti-realist](https://en.wikipedia.org/wiki/Anti-realism#Moral_anti-realism)+[relativist](https://en.wikipedia.org/wiki/Moral_relativism), who likes to base his moral design decisions in [evolutionary psychology](https://en.wikipedia.org/wiki/Evolutionary_psychology) and sociology, or arguably [evolutionary ethics](https://en.wikipedia.org/wiki/Evolutionary_ethics), so a bit of an non-realist [ethical naturalist](https://en.wikipedia.org/wiki/Ethical_naturalism), and I'm making a political/emotional argument in the style popularized by [moral realists](https://en.wikipedia.org/wiki/Moral_realism).\n\nThis is a moral system designed for and by an evolved sapient species (humans), so it privileges the role of wants derived from evolution. But there are only two ways to get a sapient being: either they evolve, or an evolved sapience builds them (directly or at some number of removes). So either way, evolution has to be involved at some point.\n\nBasically I'm proposing solving the old \"ought-from-is\" problem in ethical philosophy via evolutionary Biology — which I would claim is the only scientific solution to this conundrum, and also a pretty good answer.\n\nThe second is that, while that's the case with the thermostat, it's not the case with animals, uploads, or more arguably, with AGI that are started by humans but whose reasoning leads them to having goals that their designers didn't directly intend, and probably couldn't predict.\n\nAnd you're already excluding evolved desires like baby-eating (from that Yudkowsky story).\n\nNo, as a moral anti-realist I actually I see their morality as just as valid for their society as something based on our evolutionary psychology is for ours. Which is why I included respecting it if feasible in my [post on sapient rights,](https://www.lesswrong.com/posts/GepdRwJLHADzzkmbk/4-a-moral-case-for-evolved-sapience-chauvinism) or else the unpleasant alternative of fighting a war over the disagreement if not. (And – spoiler alert – as an author, I rather admire that Yudkowsky constructed his story so there was a solution to the problem less horrible and drastic than an all out war-of-annihilation between two high-tech sapient species.) But I agree with his construction that, for two species with evolutionary ethics that incompatible, there isn't any viable long-term negotiated or agree-to-disagree solution.\n\nYes, I liked that story a lot and found it all too plausible. So your solution of giving aliens votes only if their values are compatible enough with ours makes sense to me.\n\nSadly, I can't predict the odds of that. But despite the Fermi Paradox, the cosmos has continued to be very silent with no clearly-lived-in-looking bits, so it may be quite a while before this becomes relevant.\n\nHow far \"ought\" spreads, once evolution has created it, is an interesting question. The nice thing about uploads, uplifts, and indeed unaligned sapient near-human AGI – if, as we were discussing earlier, you have a well-aligned ASI law enforcement to keep that in control so you sensibly have that option – is that if we build something sapient and very incompatible with us, then we have only our ASIs and ourselves to blame,\n\nRight. And your proposal is a variant of mine, but with a larger group of humans in charge: all of them.\n\nWere there other elements of your Post 6 you wanted to mention or discuss?\n\nNot really. Mostly I'm enjoying discussing this with you — I'd love to have some other people chime in on the comments thread of that post: frankly I was expecting a lot of different opinions and disagreement, even that it be contentious, and so far it's been silent!\n\nI would like to drop a teaser for post 7 in the sequence, though. I'm planning to expand on some of the themes of post 5, looking into the mathematics of different forms of Utilitarianism, its derivation from different human evolved moral intuitions (like fairness, and the drive to expand), utility monsters, happiness/income curves, Utilitarian population ethics, the [Repugnant Conclusion](https://en.wikipedia.org/wiki/Mere_addition_paradox) (I think I have a proof that it's actually only a conclusion under some rather unlikely assumptions), the risks of having an ASI biased towards a group, or even multiple ASIs with different biases towards different groups. I've already come up with proofs of some conclusions that I hadn't been expecting, and who knows, there may be more surprises in wait while I'm working on it. This one will have graphs, so I probably need to get Mathematica working again or something!\n\nSo this looks into some of the possibly-shorter-term stuff we were discussing above, at least at the level of making predictions about just how bad an ASI biased towards a specific group/company/country/whatever would be.\n\nWell, I'm looking forward to seeing it!\n\nThanks, I'm enjoying working on it. I've also much enjoyed our conversation, and would love to do another one more focused on your posts and ideas, or more broad-ranging, whichever you'd prefer.\n\nI'd like that! Next time. And I'm looking forward to seeing your Post 7 in that sequence.",
      "plaintextDescription": "RogerDearnaley\nHi Seth! So, what did you want to discuss?\n\nSeth Herd\nI'd like to primarily discuss your AI, Alignment and Ethics sequence. You made a number of points that I think LWers will be interested in. I'll try to primarily act as an interviewer, although I do have one major and a number of minor points I'd like to get in there. I'm hoping to start at the points that will be of most interest to the most people.\n\nRogerDearnaley\nSure, I'm very happy to talk about it. For background, that was originally world-building thinking that I did for a (sadly still unpublished) SF novel-trilogy that I worked on for about a decade, starting about 15 years ago, now rewritten in the format of Less Wrong posts. The novel was set far enough in the future that people clearly had AI and had long since solved the Alignment Problem, so I needed to figure out what they had then pointed the AIs at. So I had to solve ethics :-)\n\nSeth Herd\nOkay, right. That's how I read it: an attempt to make an ethical system we'd want if we achieved ASI alignment.\n\nRogerDearnaley\nYeah, that was basically the goal. Which required me to first figure out how to think about ethics without immediately tripping over a tautology.\n\nSeth Herd\nIt had a number of non-obvious claims. Let me list a few that were of most interest to me:\n\n 1. It's not a claim about moral realism. It's a claim about what sort of ethical system humans would want, extending into the future.\n 2. In this system, AIs and animals don't get votes. Only humans do.\n 3. Uploads of human minds only get one vote per original human.\n\nRogerDearnaley\nSome of these properties were also deeply inobvious to me too: I wrote for several years assuming that the AIs had moral weight/rights/votes, in fact greater than the humans, in proportion to the logarithm of their intelligence (roughly log parameter count), before finally realizing that made no sense, because if they were fully aligned they wouldn't want moral weight/etc, and would have rather limi",
      "wordCount": 7534
    },
    "tags": [
      {
        "_id": "W6QZYSNt5FgWgvbdT",
        "name": "Coherent Extrapolated Volition",
        "slug": "coherent-extrapolated-volition"
      },
      {
        "_id": "yXNtYNHJB54T3bGm3",
        "name": "Dialogue (format)",
        "slug": "dialogue-format"
      },
      {
        "_id": "nSHiKwWyMZFdZg5qt",
        "name": "Ethics & Morality",
        "slug": "ethics-and-morality"
      },
      {
        "_id": "Zs4nYLkNr7Rbo4mAP",
        "name": "Utilitarianism",
        "slug": "utilitarianism"
      },
      {
        "_id": "NLwTnsH9RSotqXYLw",
        "name": "Value Learning",
        "slug": "value-learning"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ZdBmKvxBKJH2PBg9W",
    "title": "Corrigibility or DWIM is an attractive primary goal for AGI",
    "slug": "corrigibility-or-dwim-is-an-attractive-primary-goal-for-agi",
    "url": null,
    "baseScore": 19,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2023-11-25T19:37:39.698Z",
    "contents": {
      "markdown": "While rereading the [List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) (LoL), I was compelled by the argument against corrigibility. It's really hard to make a goal of \"maximize X, except if someone tells you to shut down\". I think the same argument applies to Christiano's goal of achieving corrigibility through RL by rewarding correlates of corrigibility. If other things are rewarded more reliably, you may not get your AGI to shut down when you need it to.\n\nBut those arguments don't apply if [corrigibility in the broad sense](https://ai-alignment.com/corrigibility-3039e668638) is the primary goal. \"Doing what this guy means by what he says\" is a perfectly coherent goal. And it's a highly attractive one, for a few reasons. Perhaps corrigibility shouldn't be used in this sense and do what I mean (DWIM) is a better term. But it's closely related. It accomplishes corrigibility, and has other advantages. I think it's fairly likely to be the first goal someone actually gives an AGI.\n\n\"Do what I mean\" sidesteps the difficulty of outer alignment. The difficulty of outer alignment is another point in the LoL. One common plan, which seems sensible, is to keep humans in the loop; to have a [Long Reflection](https://forum.effectivealtruism.org/topics/long-reflection) to decide what we want. \"DWIM\" allows you to contemplate and change your mind as much as you like.\n\nOf course, the problem here is: do what WHO means? We'd like an AGI that serves all of humanity, not just one guy or board of directors. And we'd like to not have power struggles.\n\nBut from the point of view of a team actually deciding what goal to give their shot at AGI, DWIM will be incredibly attractive for practical reasons. The outer alignment problem is hard. Specifying one person (or a few) to take instructions from is vastly simpler than deciding and specifying a goal that captures all of human flourishing for all time. You don't want to trust an AGI to interpret that goal correctly. Intepreting DWIM is still fraught, but it is naturally self-correcting, and becomes more useful as the AGI gets more capable. A smarter AGI will be better at understanding what you probably mean, and better at realizing when it's not sure what you mean so it can ask for clarification.\n\nThis doesn't at all address inner alignment. But when somebody thinks they have good-enough inner alignment to launch a goal-directed, [sapient](https://www.lesswrong.com/posts/WqxGB77KyZgQNDoQY/sapience-understanding-and-agi) AGI, DWIM is likely to be the goal they'll choose. This could be good or bad, depending on how well they've implemented inner alignment, and what type of people they are.",
      "plaintextDescription": "While rereading the List of Lethalities (LoL), I was compelled by the argument against corrigibility. It's really hard to make a goal of \"maximize X, except if someone tells you to shut down\". I think the same argument applies to Christiano's goal of achieving corrigibility through RL by rewarding correlates of corrigibility. If other things are rewarded more reliably, you may not get your AGI to shut down when you need it to.\n\nBut those arguments don't apply if corrigibility in the broad sense is the primary goal. \"Doing what this guy means by what he says\" is a perfectly coherent goal. And it's a highly attractive one, for a few reasons. Perhaps corrigibility shouldn't be used in this sense and do what I mean (DWIM) is a better term. But it's closely related. It accomplishes corrigibility, and has other advantages. I think it's fairly likely to be the first goal someone actually gives an AGI.\n\n\"Do what I mean\" sidesteps the difficulty of outer alignment. The difficulty of outer alignment is another point in the LoL. One common plan, which seems sensible, is to keep humans in the loop; to have a Long Reflection to decide what we want. \"DWIM\" allows you to contemplate and change your mind as much as you like.\n\nOf course, the problem here is: do what WHO means? We'd like an AGI that serves all of humanity, not just one guy or board of directors. And we'd like to not have power struggles.\n\nBut from the point of view of a team actually deciding what goal to give their shot at AGI, DWIM will be incredibly attractive for practical reasons. The outer alignment problem is hard. Specifying one person (or a few) to take instructions from is vastly simpler than deciding and specifying a goal that captures all of human flourishing for all time. You don't want to trust an AGI to interpret that goal correctly. Intepreting DWIM is still fraught, but it is naturally self-correcting, and becomes more useful as the AGI gets more capable. A smarter AGI will be better at understanding",
      "wordCount": 423
    },
    "tags": [
      {
        "_id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "slug": "outer-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "BisjoDrd3oNatDu7X",
      "tag_name": "Outer Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "WqxGB77KyZgQNDoQY",
    "title": "Sapience, understanding, and \"AGI\"",
    "slug": "sapience-understanding-and-agi",
    "url": null,
    "baseScore": 17,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2023-11-24T15:13:04.391Z",
    "contents": {
      "markdown": "*Epistemic status: I'm sure that \"AGI\" has become importantly confusing. I think it's leaving out a critical set of capabilities. I think those are closely related, so acquiring them could create a jump in capabilities. I'm not sure of the best term to disambiguate the type of AGI that's most dangerous, but I want to propose one that works decently*  \n \n\nThe term AGI has become muddled. It is now used for AI both with and without agency, contextual awareness, and the capacity to actively learn new facts and concepts. \"Sapience” means understanding, wisdom and self-awareness, so it could be adopted as a disambiguating term.  Understanding as humans perform it is an active process for testing and improving our knowledge. Understanding allows self-awareness and contextual awareness. It also implies agency, because our process of understanding is goal-directed.\n\nI have three goals here. One is to point out how the term \"AGI\" is confusing x-risk discussions. The second is to discuss how human-like undestanding is poweful, achievable, and dangerous. Last and least I propose the specific term \"sapience\" for the set of powerful and dangerous capabilities provided by active understanding.  \n \n\n![](https://lh7-us.googleusercontent.com/fpqD471fmibv-eSSwFNYPozZJ3CV7Yqd_spguYJCcarMfQDQZtqiOssp96Tyi0UbY59VTdaZvnxWIHbawGrY75e7xPOlSBFvQH7AVhALeET1tucFOUYLZkdFM-c52obTflf1wS9b0NQkJBijawhgdFQ)\n\nUnderstanding as an active process of testing and improving “fit” among concepts and world-models\n\nSapience implies agency and understanding\n-----------------------------------------\n\nThe concept of agency is not just a philosophical nicety; it's pivotal in discussions about existential risk (x-risk) related to AI. Without a clear understanding and explicit treatment of agency, these discussions have become confused and potentially misleading. The need for a new term is evident, given widely varying definitions of AGI, and resulting disagreements about risks and capabilities.  \n\n\"Sapience\" appears to be our most fitting option. The term is used in various ways with weak connections to its etymology of wisdom or discernment, but its most common usages are the ones we need. In the realm of science fiction, it's often employed to denote an intelligent, self-aware species distinct from humans. We, as “Homo Sapiens”, pride ourselves as the \"understanding apes,\" setting ourselves apart from our evolutionary kin. This self-ascription may spring from vanity, but it invokes a critical cognitive capacity we commonly refer to as \"understanding.\"\n\nHuman understanding \n--------------------\n\nThe debate surrounding whether large language models (LLMs) truly \"understand\" their output is a persistent one. Critics argue that LLMs lack genuine understanding, merely echoing word usage without deeper cognitive processing. Others suggest that humans are largely “stochastic parrots” as well.  But we do more than parrot. If I use a term appropriately, that might evidence \"an understanding\" of it; but that is not what we usually mean by understanding. It is primarily a verb. Understanding is a process. We understand in an important, active sense that current LLMs lack. In everyday usage, \"understanding\" implies an active engagement with concepts.\n\nTo say \"I understand\" is to assert that one has engaged with a concept, testing and exploring it. This process is akin to mentally simulating or \"turning over\" the concept, examining its fit with other data and ideas. For instance, understanding how a faucet works might involve visualizing the mechanism that allows water to flow upon moving the handle. These mental simulations can vary in detail and abstraction, contingent on the concept and the criteria one sets for claiming understanding. If understanding seems out of reach, one might actively pursue or formulate new hypotheses about the concept, evaluating these for their potential to foster understanding.\n\nImagine for a moment that you've just learned about the platypus. Someone's told you some things about it, and you're asking yourself if you understand. This active self-questioning is critical to our notion of understanding, and I think it is most easily achieved through what might be termed cognitive agency. You asked yourself whether you understood for a purpose. If there's no reason for you to understand the concept, you probably won't waste the time it would take to test your understanding. If you do ask, you probably have some criteria for adequate understanding, for your current purposes. That purpose could be curiousity, a desire to show that you understand things people explain to you, or more practical goals of deploying your new concept to achieve material ends.\n\nTo answer the question of whether you adequately understand, you'll use one or more strategies to test your understanding. You might form a mental simulation of a platypus, and imagine it doing things you care about. That simulation attempt might reveal important missing information - is a platypus prehistoric or current? Is it huge or small? You might guess that it swims if its webbed feet have been described. You might ask yourself if it's edible or dangerous if those are your concerns, and perform mental simulations exploring different ways you could hunt it, or it could hunt you. \n\nIf these simulations produce inconsistent results, either predicting things you know aren't true. Perhaps it sounds incongruous that you haven't already heard that there's a giant poisonous swimming animal, (if you got the size wrong) or perhaps imagining a furred animal laying eggs is recognized as incongruous. If these tests of understanding fail, you may decide to spend additional time trying to improve that understanding. You may ask questions or seek more information, or you may change your assumptions and try more simulations, to see if that produces results more congruent with the rest of your world knowledge.\n\nI think it's hard to guess how quickly and easily these capacities might be developed for AI. Large language models appear to have all of the requisite abilities, and attempts to develop language model cognitive architectures that can organize those capacities into more complex cognition are in the early days.[^uq7sy0o13e] If those capacities are as useful and understandable as I think, we might see other approaches also develop this capacity for active understanding.\n\nUnderstanding implies full cognitive generality\n-----------------------------------------------\n\nThis capacity for testing and enhancing one's understanding is largely domain-general, akin to assessing and refining a tool's design. Consequently, this active ability to understand includes a capacity for self-awareness and contextual awareness.  Such a system can develop understanding of its own cognitive processes, and its relation to the surrounding world. It also suggests a level of agency, at least in the functional sense of pursuing the goal of refining or enhancing understanding. These abilities may be so closely related that it would be be harder to create an AGI that has some but not all of them.\n\nThere are probably other paths to adequate functional understanding. LLMs do not need a human-like active process of testing and expanding their understanding to use concepts remarkably effectively. I think it will prove relatively easy and effective to add such an active process to agentic systems. If we do first create non-agentic AGI, I think we’ll quickly make it agentic and self-teaching, since those capacities may be relatively easy to \"bolt on\", as language model cognitive architectures do for LLMs. I think the attainment of active understanding skills was probably a key achievement in accelerating our cognition far beyond our immediate ancestors, and I think the same achievement is likely to accelerate AI capabilities. \n\nLeaving aside the above theories of why AGI might take a human-like route to understanding, “sapience” still seems like an effective term to capture an AGI that functionally understands itself and its context, and can extend its functional understanding to accomplish its goals. \n\n“AGI” is a dangerously ambiguous term\n-------------------------------------\n\nAGI now means AI that has capabilities in many domains. But the original usage included agency and the capacity for self-improvement. A fully general intelligence doesn’t merely do many things; it can teach itself to do anything. We apply agency to improve our knowledge and abilities when we actively understand, and AGI in its fullest and most dangerous sense can too. Sapient AI is not just versatile across various domains but capable of understanding anything—including self-comprehension and situational awareness. Understanding one's own thinking offers another significant advantage: the capability to apply one's intelligence to optimize cognitive strategies (this is independent of recursive self-improvement of architecure and hardware). An AI that can actively understand can develop new capabilities.\n\nThe term sapience calls to mind our self-given title of Homo Sapiens. This title refers to the key difference between us and earlier hominids, and so sapience suggests an analogous difference between current narrow AI and successors that are better at understanding. Homo Erectus was modestly successful, but we Sapiens took over the world (and eliminated many species without malice toward them). The term invokes our rich intuition of humans as capable, dangerous, and ever-improving agents, hopefully without too many intuitions about specific human values (these are more closely connected to the older Homo aspect of our designation).\n\nAs it’s used now, the term AGI suffers from an ambiguity that's crucial in x-risk discussions. It’s now used for an intelligence capable of performing a wide array of tasks, whether or not that intelligence is agentic, self-aware, or contextually aware. This usage seems to have slipped in from the term’s wider adoption by more conventionally minded pundits, economists, and technologists; they often assume that AI will remain a technology. [But AGI isn’t just a technology](https://www.lesswrong.com/posts/nZYhs48pWsaCCgGfi/agi-isn-t-just-a-technology) if it’s agentic. The distinction might seem unimportant in economic contexts, but it's crucial in x-risk discussions. The other existing terms I’ve found and thought don't seem to capture this distinction as sapience and/or they carry other unwanted implications and intuitions.[^nkvvttfu0j]\n\nWhen your terms are co-opted, you can fight to take them back, or shift to new terms. I don’t think it’s wise to fight, because fighting for terminology doesn't usually work, and more importantly because [fighting causes polarization](https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs). Creating more rabid anti-safety advocates could be very bad (if many experts are loudly proclaiming that AGI doesn't carry an existential risk, policymakers may believe and cite whichever group suits their agenda).  \n\nTherefore, it seems useful to introduce a new term. Different risk models apply to sapient AI than to AGI without the capacities implied by sapience. By distinguishing these we can deconfuse our conversations, and more clearly convey the most severe risks we face. Artificial sapience (AS) or sapient AI (SAI) is my proposed terminology.\n\nDisambiguating the terms we use for transformative AI seems like a pure win. I'm not sure if sapience is the best term to do that disambiguation, and I'd love to hear other ideas. Separately, if I'm right about the usefulness of active understanding, we might expect to see a capabilities jump when this capacity is achieved.\n\n  \n \n\n  \n \n\n[^uq7sy0o13e]: I confess to not knowing exactly how those complex processes work in the brain, but the cognitive outlines seem clear. I have some ideas on how to biological networks might naturally capture incongruity between successive representations, and a detailed theory about how the necessary decision-making works. But these active understanding processes are somewhat different than and include a separate component of recognizing incongruity that's not needed for even for decision-making and planning in complex domains. I think these are hard-won cognitive skills that we practice and develop during our educations. Our basic brain mechanisms provide an adequate basis for creating mental simulations and testing their congruity with other simulations, but we probably need to create skills based on those mechanisms.Current LLMs seem to possess the requisite base capabilities, at least in limited form. They can both create simulations in linguistic form, and make judgments about the congruity of multiple statements. I'm concerned that we're collectively overlooking how close language model cognitive architectures might be to achieving AGI. I think that a hard-coded decision-tree using scripted prompts to evaluate brenches could provide that skill that organizes our capacities into active understanding. Perhaps GPT4 and current architectures aren't quite capable enough to get  much traction in such an iterative process of testing and improving understanding. But it seems entirely plausible to me that GPT5 or GPT6 might have that capacity, when combined with improved and elaborated episodic memory, sensory networks, and coded (or learned) algorithms to emulate this type of strategic cognitive sequencing. I discuss the potentials of language model cognitive architectures (a more complete term for language model agents) here. \n\n[^nkvvttfu0j]:  One possible alternative term is superintelligence. I think this does intuitively imply the capacity for rich understanding and extension of knowledge to any domain. But it does not firmly imply agency. More importantly, it also conveys an intuition of being more distant than the first self-aware, self-teaching AI. Artificial sentience does not imply better-then human but merely near-human cognitive abilities. Parahuman AI is another possible term, but I think it too strongly implies human-like, while not pointing as clearly at a capacity for rich and extensible understanding. More explicit terms, like self-aware agentic AGI (SAAAAI?;) seem clumsy. Artificial sentience is another possibility, but sentience is more commonly used for having moral worth by virtue of having phenomenal consciousness and “feelings”. Avoiding those implications seems important for clear discussions of capabilities and x-risk. Agentic AGI might be adequate, but it leaves out the implication of active understanding, contextual awareness and goal-directed learning. But I’m not sure that artificial sapience is the best term, so I’m happy to adopt another term for roughly the same set of concepts if someone has a better idea.",
      "plaintextDescription": "Epistemic status: I'm sure that \"AGI\" has become importantly confusing. I think it's leaving out a critical set of capabilities. I think those are closely related, so acquiring them could create a jump in capabilities. I'm not sure of the best term to disambiguate the type of AGI that's most dangerous, but I want to propose one that works decently\n \n\nThe term AGI has become muddled. It is now used for AI both with and without agency, contextual awareness, and the capacity to actively learn new facts and concepts. \"Sapience” means understanding, wisdom and self-awareness, so it could be adopted as a disambiguating term.  Understanding as humans perform it is an active process for testing and improving our knowledge. Understanding allows self-awareness and contextual awareness. It also implies agency, because our process of understanding is goal-directed.\n\nI have three goals here. One is to point out how the term \"AGI\" is confusing x-risk discussions. The second is to discuss how human-like undestanding is poweful, achievable, and dangerous. Last and least I propose the specific term \"sapience\" for the set of powerful and dangerous capabilities provided by active understanding.\n \n\n\n\nUnderstanding as an active process of testing and improving “fit” among concepts and world-models\n\n\nSapience implies agency and understanding\nThe concept of agency is not just a philosophical nicety; it's pivotal in discussions about existential risk (x-risk) related to AI. Without a clear understanding and explicit treatment of agency, these discussions have become confused and potentially misleading. The need for a new term is evident, given widely varying definitions of AGI, and resulting disagreements about risks and capabilities.  \n\n\"Sapience\" appears to be our most fitting option. The term is used in various ways with weak connections to its etymology of wisdom or discernment, but its most common usages are the ones we need. In the realm of science fiction, it's often employed to den",
      "wordCount": 1743
    },
    "tags": [
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "WqLn4pAWi5hn6McHQ",
        "name": "Self Improvement",
        "slug": "self-improvement"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "9CYL2EmFATajK8KBf",
    "title": "Altman returns as OpenAI CEO with new board",
    "slug": "altman-returns-as-openai-ceo-with-new-board",
    "url": null,
    "baseScore": 6,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2023-11-22T16:04:03.123Z",
    "contents": {
      "markdown": "It's news to me even though it was announced last night. \n\nI think this is probably better than him and most staff going to MS, but I'm not sure.\n\nhttps://twitter.com/OpenAI/status/1727206187077370115?s=19\n\nFrom the OpenAI Twitter account, 10p ET Tuesday:\n\nWe have reached an agreement in principle for Sam Altman to return to OpenAI as CEO with a new initial board of Bret Taylor (Chair), Larry Summers, and Adam D'Angelo.\n\nWe are collaborating to figure out the details. Thank you so much for your patience through this.\n\n\nI think we'll have to wait for someone to leak the actual content of the reasons for the board trying to fire him to figure out what this all suggests for his and the orgs values and power structure.",
      "plaintextDescription": "It's news to me even though it was announced last night.\n\nI think this is probably better than him and most staff going to MS, but I'm not sure.\n\nhttps://twitter.com/OpenAI/status/1727206187077370115?s=19\n\nFrom the OpenAI Twitter account, 10p ET Tuesday:\n\nWe have reached an agreement in principle for Sam Altman to return to OpenAI as CEO with a new initial board of Bret Taylor (Chair), Larry Summers, and Adam D'Angelo.\n\nWe are collaborating to figure out the details. Thank you so much for your patience through this.\n\nI think we'll have to wait for someone to leak the actual content of the reasons for the board trying to fire him to figure out what this all suggests for his and the orgs values and power structure.",
      "wordCount": 124
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "K6vmQatSq3vaR3LbF",
    "title": "OpenAI Staff (including Sutskever) Threaten to Quit Unless Board Resigns",
    "slug": "openai-staff-including-sutskever-threaten-to-quit-unless",
    "url": "https://www.wired.com/story/openai-staff-walk-protest-sam-altman/",
    "baseScore": 52,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2023-11-20T14:20:33.539Z",
    "contents": {
      "markdown": "More drama. Perhaps this will prevent spawning a new competent and funded AI org at MS?",
      "plaintextDescription": "More drama. Perhaps this will prevent spawning a new competent and funded AI org at MS?",
      "wordCount": 16
    },
    "tags": [
      {
        "_id": "rRPMENAXS6r9tLjKy",
        "name": "News",
        "slug": "news"
      },
      {
        "_id": "GFdvCeipZTgqfysEB",
        "name": "Drama",
        "slug": "drama"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xqqhwbH2mq6i4iLmK",
    "title": "We have promising alignment plans with low taxes",
    "slug": "we-have-promising-alignment-plans-with-low-taxes",
    "url": null,
    "baseScore": 44,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2023-11-10T18:51:38.604Z",
    "contents": {
      "markdown": "*Epistemic status: I’m sure these plans have advantages relative to other plans. I'm not sure they're adequate to actually work, but I think they might be.*\n\nWith good enough alignment plans, we might not need coordination to survive. If alignment taxes are low enough, we might expect most people developing AGI to adopt them voluntarily. There are two alignment plans that seem very promising to me, based on several factors, including ease of implementation, and applying to fairly likely default paths to AGI. Neither has received much attention. I can’t find any commentary arguing that they wouldn't work, so I’m hoping to get them more attention so they can be considered carefully and either embraced or rejected.\n\nEven if these plans[^gyi9at2u1tq] are as promising as I think now, I’d still give p(doom) in the vague 50% range. There is plenty that could go wrong.[^asssodr6ivl]\n\nThere's a peculiar problem with having promising but untested alignment plans: they're an excuse for capabilities to progress at full speed ahead. I feel a little hesitant to publish this piece for that reason, and you might feel some hesitation about adopting even this much optimism for similar reasons. I address this problem at the end.\n\n**The plans**\n-------------\n\nTwo alignment plans stand out among the many I've found. These seem more specific and more practical than others. They are also relatively simple and obvious plans for the types of AGI designs they apply to. They have received very little attention since being proposed recently. I think they deserve more attention.\n\nThe first is Steve Byrnes’[ Plan for mediocre alignment of brain-like \\[model-based RL\\] AGI](https://www.alignmentforum.org/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi). In this approach, we evoke a set of representations in a[ learning subsystem](https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and), and set the weights from there to the[ steering](https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and) or critic subsystems. For example, we ask the agent to \"think about human flourishing\" and then freeze the system and set high weights between the active units in the learning system/world model and the steering system/critic units. The system now ascribes high value to the distributed concept of human flourishing. (at least as it understands it). Thus, the agent's knowledge is used to define a goal we like. \n\nThis plan applies to all RL systems with a critic subsystem, which includes most powerful RL systems.[^gq3s1wsqk55] RL agents (including loosely brain-like systems of deep networks) seem like one very plausible route to AGI. I personally give them high odds of achieving AGI if [language model cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) (LMCAs) don’t achieve it first.\n\nThe second promising plan might be called natural language alignment, and it applies to [language model cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) and other language model agents. The most complete writeup I'm aware of is [mine](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent). This plan similarly uses the agent's knowledge to define goals we like. Since that sort of agent's knowledge is defined in language, this takes the form of stating goals in natural language, and constructing the agent so that its system of self-prompting results in taking actions that pursue those goals. Internal and external review processes can improve the system's ability to effectively pursue both practical and alignment goals.\n\nJohn Wentworth's plan[ How To Go From Interpretability To Alignment: Just Retarget The Search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget) is similar. It applies to a third type of AGI, a mesa-optimizer that emerges through training. It proposes using interpretability methods to identify the representations of goals in that mesa-optimizer; identifying representations of what we want the agent to do; and pointing the former at the latter. This plan seems more technically challenging, and I personally don't think an emergent mesa-optimizer in a predictive foundation model is a likely route to AGI. But this plan shares many of the properties that make the previous two promising, and should be employed if mesa-optimizers become a plausible route to AGI.\n\nThe first two approaches are explained in a little more detail in the linked posts above, and Steve's is also described in more depth in his #[ \\[Intro to brain-like-AGI safety\\] 14. Controlled AGI](https://www.lesswrong.com/posts/QpHewJvZJFaQYuLwH/intro-to-brain-like-agi-safety-14-controlled-agi). But that's it. Both of these are relatively new, so they haven't received a lot of criticism or alternate explanations yet.\n\n**Why these plans are promising**\n---------------------------------\n\nBy \"promising alignment plans\", I mean I haven't yet found a compelling argument for why they wouldn't work. Further debunking and debugging of these plans are necessary. They apply to the two types of AI that seem to currently lead the race for AGI: RL agents and Language Model Agents (LMAs). These plans address gears-level models of those types of AGI. They can be complemented with methods like scalable oversight, boxing, interpretability, and other alignment strategies.\n\nThese two plans have low alignment taxes in two ways. They apply to AI approaches most likely to lead to AGI, so they don't require new high-effort projects. They also have low implementation costs in terms of both design and computational resources, when compared to a system optimized for sheer capability.\n\nBoth of these plans have the advantages of operating on the[ steering subsystem](https://www.lesswrong.com/posts/qzu9o3sTytbC4sZkQ/steering-subsystems-capabilities-agency-and-alignment) that defines goals, and[ using the AGI's understanding](https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence) to define those goals. That's only possible if you can pause training at para-human level, at which the system has a nontrivial understanding of humans, language, and the world, but isn't yet dangerously capable of escaping. Since deep networks train relatively predictably (at least prior to self-directed learning or self-improvement), this requirement seems achievable. This may be [a key update in alignment thinking](https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence) relative to early assumptions of fast takeoff.\n\n**Limitations and future directions**\n-------------------------------------\n\nThey’re promising, but these plans aren’t flawless. They primarily create an initial loose alignment. Whether they're durable in a fully autonomous, self-modifying and continuously learning system ([The alignment stability problem](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem)) remains to be addressed. This seems to be the case with all other alignment approaches I know of for network-based agents. Alex Turner's [A shot at the diamond-alignment problem](https://www.lesswrong.com/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem) convinced me that[ reflective stability](https://arbital.com/p/reflective_stability/) will stabilize a single well-defined, dominant goal, but the proof doesn't apply to distributed or multiple goals. MIRI is[ rumored](https://www.lesswrong.com/posts/qbcuk8WwFnTZcXTd6/thomas-kwa-s-miri-research-experience) to be working on this issue; I wish they'd share with the rest of us, but absent that, I think we need more minds on the problem.\n\nThere's are two other important limitations of aligning language model agents. One is the [Waluigi effect](https://www.lesswrong.com/tag/waluigi-effect). Language models may simulate hostile characters in the course of efficiently performing next-word prediction. Such hostile simulacra may provide answers that are wrong in malicious directions. This is a more pernicious problem than hallucination, because it is not necessarily improved in more capable language models. There are possible remedies,[^yttiy0ozoti] but this problem needs more careful consideration. \n\nThere are also concerns that language models do not accurately represent their internal states in their utterances. They may use steganography, or otherwise mis-report their train of thought. These issues are discussed more detail in [The Translucent Thoughts Hypotheses and Their Implications](https://www.alignmentforum.org/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications), discussion threads there, and other posts.\n\nThose criticisms are suggest possible failure, but not likely failure. This isn't guaranteed to work. But the perfect is the enemy of the good.[^vya5rfr2r9] Plans like these seem like our best practical hope to me. At the least, they seem worth further analysis.\n\nThere's a peculiar problem with actually having good alignment plans: they might provide an excuse for people to call for full speed ahead. If those plans turn out to not work well enough, that would be disastrous.  But I think it's important to be clear and honest, particularly within the community you're trying to cooperate with. And the potential seems worth the risk. Effective and low-tax plans would reduce the need for difficult or impossible coordination. Balancing publicly working on promising plans against undue optimism is a complex strategic issue that deserves explicit attention.\n\nI have yet to find any arguments for why these plans are unlikely to work. I believe in many arguments for [the least forgiving take on alignment](https://alignmentforum.org/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment), but none make me think these plans are a priori likely to fail. The existence of possible failure points doesn't seem like an adequate reason to dismiss them. There's a good chance that one of these general plans will be used. Each is an obvious plan for one of the AGI approaches that seem to currently be in the lead.  We might want to analyze these plans carefully before they're attempted.   \n \n\n[^gyi9at2u1tq]: I think these are worth the word \"plans\" while most other things we discuss are called \"approaches\". It would be easy to change my mind. These are plans the way \"we'll build a bridge of steel beams\" is an initial plan, but not an adequate one. \n\n[^asssodr6ivl]: Some things that could go wrong, outside of specific problems with these plans: sloppy implementation; operational inadequacy; outer alignment mistakes; alignment instability; and multipolar conflict disasters. Promising alignment plans are only a start toward survival. \n\n[^gq3s1wsqk55]: In reinforcement learning, a critic system evaluates actions taken by an actor, and provides the actual training signal. In that sense, it is a steering subsystem that defines the system's goals and behavior.The actor, or policy, is often mistaken for the full RL agent. Simple RL systems do not employ critics, but all powerful RL systems I'm aware of employ some form of critic. The actor subsystem takes as input the state and outputs the best action according to its current learned policy.The critic subsystem is trained to produce an estimate of value, the total future reward resulting from a state or action-in-a-state. The critic provides the reinforcement signal to the actor for a policy gradient (usually backpropagation) adjustment. Thus, the critic system \"interprets\" the reinforcement signal from the environment, and more directly determines the system's behavior (and its metaphorical \"values\", a word that arguably means the same thing in the RL and human domains.Critic systems can also be used to estimate the value of a state to perform tree search. For instance, AlphaZero performs a limited tree search of plays and expected counterplays, and selects the move leading to the best board game state, as estimated by the critic subsystem. Humans go one step farther and create distal goals like \"start a business\", probably based on our critic subsystem's evaluations of those imagined future states. I explore those different types of critic-based goal direction in my steering subsystems post. \n\n[^yttiy0ozoti]: In Internal independent review for language model agent alignment I proposed a review process of sampling from new instances to avoid persistent Waluigi effects and counteract their influence. Shane Legg appeared to propose the same general idea in his recent interview with Dwarkesh Patel. The intuition is that many independent LLM calls review plans and thoughts for alignment, rare Waluigi effects should be overridden. Tamera Lanham has proposed externalized reasoning oversight, in which humans assisted by AI review those natural language trains of thought. This would aid in detecting malicious insertions into the tree of thoughts. The possibility of malicious simulacra remains a major concern for this plan, but without empirical estimates and more refined evaluation, this doesn't seem like an adequate reason to dismiss the approach. \n\n[^vya5rfr2r9]: The perfect is the enemy of the good in most projects. AGI alignment needs to work on the first try, and it has other unique difficulties. But a plan with guaranteed success that will take four decades is probably not going to work, given the difficulty of coordination. This is a much deeper topic, since the search for a formally guaranteed solution is a major tenet, spoken or unspoken, of much alignment work. I'd like to address this at greater length, and get thoughts on this aspect of the problem.",
      "plaintextDescription": "Epistemic status: I’m sure these plans have advantages relative to other plans. I'm not sure they're adequate to actually work, but I think they might be.\n\nWith good enough alignment plans, we might not need coordination to survive. If alignment taxes are low enough, we might expect most people developing AGI to adopt them voluntarily. There are two alignment plans that seem very promising to me, based on several factors, including ease of implementation, and applying to fairly likely default paths to AGI. Neither has received much attention. I can’t find any commentary arguing that they wouldn't work, so I’m hoping to get them more attention so they can be considered carefully and either embraced or rejected.\n\nEven if these plans[1] are as promising as I think now, I’d still give p(doom) in the vague 50% range. There is plenty that could go wrong.[2]\n\nThere's a peculiar problem with having promising but untested alignment plans: they're an excuse for capabilities to progress at full speed ahead. I feel a little hesitant to publish this piece for that reason, and you might feel some hesitation about adopting even this much optimism for similar reasons. I address this problem at the end.\n\n\nThe plans\nTwo alignment plans stand out among the many I've found. These seem more specific and more practical than others. They are also relatively simple and obvious plans for the types of AGI designs they apply to. They have received very little attention since being proposed recently. I think they deserve more attention.\n\nThe first is Steve Byrnes’ Plan for mediocre alignment of brain-like [model-based RL] AGI. In this approach, we evoke a set of representations in a learning subsystem, and set the weights from there to the steering or critic subsystems. For example, we ask the agent to \"think about human flourishing\" and then freeze the system and set high weights between the active units in the learning system/world model and the steering system/critic units. The system now a",
      "wordCount": 1374
    },
    "tags": [
      {
        "_id": "Hs2ewfiKfuWKSscSQ",
        "name": "Aligned AI Proposals",
        "slug": "aligned-ai-proposals"
      },
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "Fi6SeJRGfJs3bp5se",
        "name": "Reinforcement learning",
        "slug": "reinforcement-learning"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      },
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "ye78Dip8YNgLBKGcy",
    "title": "Seth Herd's Shortform",
    "slug": "seth-herd-s-shortform",
    "url": null,
    "baseScore": 6,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 66,
    "createdAt": null,
    "postedAt": "2023-11-10T06:52:28.778Z",
    "contents": null,
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "2QLxNdxQpnesokk9H",
    "title": "Shane Legg interview on alignment",
    "slug": "shane-legg-interview-on-alignment",
    "url": "https://www.youtube.com/watch?v=Kc1atfJkiJU&t=1159s",
    "baseScore": 66,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2023-10-28T19:28:52.223Z",
    "contents": {
      "markdown": "This is Shane Legg, cofounder of DeepMind, on Dwarkesh Patel's podcast. The link is to the ten-minute section in which they specifically discuss alignment. Both of them seem to have a firm grasp on alignment issues as they're discussed on LessWrong.\n\nFor me, this is a significant update on the alignment thinking of current leading AGI labs. This seems more like a concrete alignment proposal than we've heard from OpenAI or Anthropic. Shane Legg has always been interested in alignment and a believer in X-risks. I think he's likely to play a major role in alignment efforts at DeepMind/Google AI as they approach AGI.\n\nShane's proposal centers on \"deliberative dialogues\", DeepMind's term for a system using System 2 type reasoning to reflect on the ethics of the actions it's considering. \n\nThis sounds exactly like the the internal review I proposed in [Capabilities and alignment of LLM cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) and [Internal independent review for language model agent alignment](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent). I could be squinting too hard to get his ideas to match mine, but they're at least in the same ballpark. He's proposing a multi-layered approach, like I do, and with most of the same layers. He includes RLHF or RLAIF as useful additions but not full solutions, and human review of its decision processes ([externalized reasoning oversight](https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for) as proposed by Tamera Lanham, now at Anthropic).\n\nMy proposals are explicitly in the context of language model agents, (including their generalization to multimodal foundation models). It sounds to me like this is the type of system Shane is thinking of when he's talking about alignment, but here I could easily be projecting. His timelines are still short, though, so I doubt he's envisioning a whole new type of system prior to AGI.[^au80xd00gxi] \n\nDwarkesh pushes him on the challenges of both getting a ML system to understand human ethics. Shane says that's challenging; he's aware that giving a system any ethical outlook at all is nontrivial.  I'd say this aspect of the problem is well on the way to being solved;  GPT4 understands a variety of human ethical systems rather well, with proper prompting. Future systems will understand human conceptions of ethics better yet. Shane recognizes that just teaching a system about human ethics isn't enough; there's a philosophical challenge in choosing the subset of that ethics you want the system to use.\n\nDwarkesh also pushes him on how you'd ensure that the system actually follows its ethical understanding. I didn't get a clear understanding from his answer here, but I think it's a complex matter of designing the system so that it performs an ethics review and then actually uses it to select actions. This could be in a scripted scaffold around an agent, like AutoGPT, but this could also apply to more complex schemes, like an RL outer loop network running a foundation model. Shane notes the problems with using RL for alignment, including deceptive alignment.\n\nThis seems like a good starting point to me, obviously; I'm delighted to see that someone whose opinion matters is thinking about this approach. I think this is not just an actual proposal, but a viable one. It doesn't solve [The alignment stability problem](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem)[^6fqpmjhuk9] of making sure stays aligned once it's autonomous and self-modifying, but I think that's probably solvable, too, once we get some more thinking on it.\n\nThe rest of the interview is of interest as well; it's Shane's thoughts on the path to AGI, which I think is quite reasonable, well-expressed, and one plausible path; DeepMind's contributions to safety vs. alignment, and his predictions for the future.\n\n[^au80xd00gxi]: When asked about the limitations of language models relative to humans, he focused on their lack of episodic memory. Adding this in useful form to an agent isn't trivial, but it seems to me it doesn't require any breakthroughs relative to the vector databases and knowledge graph approaches already in use. This is consistent with but not strong evidence for Shane thinking that foundation model agents are the path to AGI. \n\n[^6fqpmjhuk9]: Edit: Value systematization: how values become coherent (and misaligned) is another way to think about part of what I'm calling the alignment stability problem.",
      "plaintextDescription": "This is Shane Legg, cofounder of DeepMind, on Dwarkesh Patel's podcast. The link is to the ten-minute section in which they specifically discuss alignment. Both of them seem to have a firm grasp on alignment issues as they're discussed on LessWrong.\n\nFor me, this is a significant update on the alignment thinking of current leading AGI labs. This seems more like a concrete alignment proposal than we've heard from OpenAI or Anthropic. Shane Legg has always been interested in alignment and a believer in X-risks. I think he's likely to play a major role in alignment efforts at DeepMind/Google AI as they approach AGI.\n\nShane's proposal centers on \"deliberative dialogues\", DeepMind's term for a system using System 2 type reasoning to reflect on the ethics of the actions it's considering. \n\nThis sounds exactly like the the internal review I proposed in Capabilities and alignment of LLM cognitive architectures and Internal independent review for language model agent alignment. I could be squinting too hard to get his ideas to match mine, but they're at least in the same ballpark. He's proposing a multi-layered approach, like I do, and with most of the same layers. He includes RLHF or RLAIF as useful additions but not full solutions, and human review of its decision processes (externalized reasoning oversight as proposed by Tamera Lanham, now at Anthropic).\n\nMy proposals are explicitly in the context of language model agents, (including their generalization to multimodal foundation models). It sounds to me like this is the type of system Shane is thinking of when he's talking about alignment, but here I could easily be projecting. His timelines are still short, though, so I doubt he's envisioning a whole new type of system prior to AGI.[1] \n\nDwarkesh pushes him on the challenges of both getting a ML system to understand human ethics. Shane says that's challenging; he's aware that giving a system any ethical outlook at all is nontrivial.  I'd say this aspect of the problem is",
      "wordCount": 593
    },
    "tags": [
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "nKtyrL5u4Y5kmMWT5",
        "name": "DeepMind",
        "slug": "deepmind"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "qsDPHZwjmduSMCJLv",
    "title": "The (partial) fallacy of dumb superintelligence",
    "slug": "the-partial-fallacy-of-dumb-superintelligence",
    "url": null,
    "baseScore": 38,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-10-18T21:25:16.893Z",
    "contents": {
      "markdown": "In[ her opening statement for the Munk debate on AI risk,](https://www.youtube.com/watch?v=skRgYH7oAjc) Melanie Mitchell addressed a proposed example of AGI risk: an AGI tasked with fixing climate change might decide to eliminate humans as the source of carbon emissions. She says:\n\n> This is an example of what's called the fallacy of dumb superintelligence.[^s793uam9h9i] That is, it's a fallacy to think a machine could be 'smarter than humans in all respects', but still lack any common sense understanding of humans, such as understanding why we made the request to fix climate change.\n\nI think this “fallacy” is a crux of disagreement about AI x-risk, by way of alignment difficulty. I've heard this statement from other reasonably well-informed risk doubters. The intuition makes sense. But most people in alignment would dismiss this out of hand as being itself a fallacy. Understanding these two positions not only clarifies the discussion, but suggests reasons we're overlooking a promising approach to alignment.\n\nThis \"fallacy\" doesn’t establish that alignment is easy. Understanding what you mean doesn’t make the AGI want to do that thing. Actions are guided by goals, which are different from knowledge. But this intuition that understanding should help alignment needn’t be totally discarded. We now have proposed alignment approaches that make use of an AIs understanding for its alignment. They do this by “pointing” a motivational system at representations in a learned knowledge system, such as “human flourishing”. I discuss two alignment plans that use this approach and seem quite promising.\n\nEarly alignment thinking assumed that this type of approach  was not viable, since AGIs could \"[go foom](https://www.lesswrong.com/tag/ai-takeoff)\" (learn very quickly and unpredictably). This assumption appears not to be true of sub-human levels of training in deep networks, and that may be sufficient for initial alignment.\n\nWith a system capable of unpredictable rapid improvement, it would be madness to let it learn prior to aligning it. It might very well grow smart enough to escape before you get a chance to stop its learning to perform alignment. Thus, its goals (or a set of rewards to shape them) must be specified before it starts to learn. In that scenario, the way we specify goals cannot make use of the AI's intelligence. Mitchell’s “fallacy” is itself a fallacy under this logic. An AGI that understands what we want can easily do things we very much don't want.\n\nBut early foom now seems unlikely, so our thinking should adjust. Deep networks don’t increase in capabilities unpredictably, at least prior to human-level and recursive self improvement. And that may be far enough for initial alignment to succeed. The early assumption of AGI “going foom” now seems unlikely to be true. I think that early assumption has left a mistake in our collective thinking: that an AGI’s knowledge is irrelevant to making it do what we want.[^g637hbyuzwv] \n\n**How to safely use an AI’s understanding for alignment**\n---------------------------------------------------------\n\nDeep networks learn at a relatively predictable pace, in the current training regime. Thus, their training can be paused at intermediate levels that include some understanding of human values, but before they achieve superhuman capabilities. Once a system starts to reflect and direct its own learning, this smooth trajectory probably won’t continue. But we can probably stop at a safe but useful level of intelligence/understanding, if we set that level carefully and cautiously. We probably can align an AGI partway through training, and thus make use of its understanding of what we want.\n\nThere are three particularly relevant examples of this type of approach. The first, [RLHF](https://www.lesswrong.com/tag/rlhf), is relevant because it is widely known and understood. (I [among others](https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf) don’t consider it a promising approach to alignment by itself.) RLHF uses the LLM’s trained “understanding” or “knowledge” as a substrate for efficiently specifying human preferences. Training on a limited set of human judgments about input-response pairs causes the LLM to generalize these preferences remarkably well. We are “pointing to” areas in its learned semantic spaces. Because those semantics are relatively well-formed, we need to do relatively little pointing to define a complex set of desired responses.\n\nThe second example is natural language alignment of language model agents (LMAs). This seems like a very promising alignment plan, if LMAs become our first AGIs. This plan consists of designing the agent to follow top-level goals stated in natural language (e.g., \"get OpenAI a lot of money and political influence\") including alignment goals (e.g., \"do what Sam Altman wants, and make the world a better place\".) I've written more about this technique, and the ensemble of techniques it can \"stack\" with,[ here](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent).\n\nThis approach follows the above general scheme. It pauses training to do alignment work by pre-training the LLM, and inserting alignment goals before launching the system as an agent. (This is mid-training, if that agent continues to perform continuous learning, as seems likely.) If the AI is sufficiently intelligent, it will pursue those goals as stated, including their rich and contextual semantics. Choosing these goal statements wisely is still a nontrivial outer alignment problem; but the AI’s knowledge is the substrate for defining its alignment. \n\nAnother promising alignment plan that follows this general pattern is Steve Byrnes'[ Plan for mediocre alignment of brain-like \\[model-based RL\\] AGI](https://www.alignmentforum.org/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi). In this plan, we induce the nascent AGI (paused at useful but controllable level of understanding/intelligence) to represent the concept we want it aligned to (e.g., “think about human flourishing” or “corrigibility” or whatever). We then set the weights from the active units in its representational system into its critic system. Since the critic system is a [steering subsystem](https://www.lesswrong.com/posts/qzu9o3sTytbC4sZkQ/steering-subsystems-capabilities-agency-and-alignment) that determines its values and therefore its behavior, inner alignment is solved. That concept has become its “favorite”, highest-valued set of representations, and its decision-making will pursue everything semantically included in that concept as a final goal.\n\nNow, contrast these techniques with alignment techniques that don’t make use of the system’s knowledge.[Shard Theory](https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview) and other proposals for aligning AGI by using the right set of rewards is one example. This requires accurately guessing how the system’s representations will form, and how those rewards will shape the agent’s behavior as they develop. Hand-coding a representation of any but the simplest goal (see [diamond maximization](https://arbital.com/p/diamond_maximizer/)) seems so difficult that it’s not generally considered a viable approach.\n\nThese are sketches of plans that need further development and inspection for flaws. And they only produce an initial, loose (\"mediocre\") alignment with human values, in the training distribution. [The alignment stability problem](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem) of generalization and change of values remains unaddressed. Whether the alignment remains satisfactory after further learning, self-modification, or in new (out of distribution) circumstances seems like a complex problem that deserves further analysis.\n\nThis approach of leveraging an AI’s intelligence and “telling it what we want” by pointing to its representations seems promising. And these two plans seem particularly promising. They apply to types of AGI we are likely to get (language model agents, RL agents, or a hybrid); they are straightforward enough to implement, and straightforward enough to think about in detail prior to implementing them.\n\nI’d love to hear specific pushback on this direction, or better yet, these specific plans. AI work seems likely to proceed apace, so alignment work should proceed with haste too. I think we need the best plans we can make and critique, applying to the types of AGI we’re most likely to get, even if those plans are imperfect.   \n \n\n[^s793uam9h9i]: Richard Loosemore appears to have coined the term in 2012 or before. He addresses this argument here, reaching similar conclusions to those here: Do what I mean is not automatic, but neither is it particularly implausible to code an AGI to infer intentions and check with its creators when they’re likely to be violated. \n\n[^g637hbyuzwv]: See the recent post Evaluating the historical value misspecification argument. It expands on the historical context for these ideas, particularly the claim that we should adjust our estimates of alignment difficulty in light of AI that has reasonably good understanding of human values. I don't care who thought what when, but I do care how the collective train of thought reviewed there might have misled us slightly. The discussion on that post clarifies the issues somewhat. This post is intended to offer a more concrete answer to a central question posed in that discussion: how we might close the gap between AI understanding our desires, and actually fulfilling them by making its decisions based on that understanding. I’m also proposing that the key change from historical assumptions is the predictablility of learning therefore the option of safely performing alignment work on a partly-trained system.",
      "plaintextDescription": "In her opening statement for the Munk debate on AI risk, Melanie Mitchell addressed a proposed example of AGI risk: an AGI tasked with fixing climate change might decide to eliminate humans as the source of carbon emissions. She says:\n\n> This is an example of what's called the fallacy of dumb superintelligence.[1] That is, it's a fallacy to think a machine could be 'smarter than humans in all respects', but still lack any common sense understanding of humans, such as understanding why we made the request to fix climate change.\n\nI think this “fallacy” is a crux of disagreement about AI x-risk, by way of alignment difficulty. I've heard this statement from other reasonably well-informed risk doubters. The intuition makes sense. But most people in alignment would dismiss this out of hand as being itself a fallacy. Understanding these two positions not only clarifies the discussion, but suggests reasons we're overlooking a promising approach to alignment.\n\nThis \"fallacy\" doesn’t establish that alignment is easy. Understanding what you mean doesn’t make the AGI want to do that thing. Actions are guided by goals, which are different from knowledge. But this intuition that understanding should help alignment needn’t be totally discarded. We now have proposed alignment approaches that make use of an AIs understanding for its alignment. They do this by “pointing” a motivational system at representations in a learned knowledge system, such as “human flourishing”. I discuss two alignment plans that use this approach and seem quite promising.\n\nEarly alignment thinking assumed that this type of approach  was not viable, since AGIs could \"go foom\" (learn very quickly and unpredictably). This assumption appears not to be true of sub-human levels of training in deep networks, and that may be sufficient for initial alignment.\n\nWith a system capable of unpredictable rapid improvement, it would be madness to let it learn prior to aligning it. It might very well grow smart enough to es",
      "wordCount": 1227
    },
    "tags": [
      {
        "_id": "Hs2ewfiKfuWKSscSQ",
        "name": "Aligned AI Proposals",
        "slug": "aligned-ai-proposals"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "qzu9o3sTytbC4sZkQ",
    "title": "Steering subsystems: capabilities, agency, and alignment",
    "slug": "steering-subsystems-capabilities-agency-and-alignment",
    "url": null,
    "baseScore": 31,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-09-29T13:45:00.739Z",
    "contents": {
      "markdown": "Human brains have steering subsystems. LLMs and most RL agents do not. [Steering systems](https://www.lesswrong.com/posts/LsqvMKnFRBQh4L3Rs/steering-systems), as defined by Max H, are goal-directed AI systems, or optimizers. Here I focus on steering subsystems: the parts of human and AI cognitive systems most directly relevant to goal-direction. These work in three distinct ways (and probably more), each providing a different type and amount of agency, and associated capabilities. \n\nThinking about types of steering subsystems can clarify our conceptions of agency. Steering subsystems increase risks by adding capabilities. Notably, sophisticated steering subsystems create useful representations of goals. This allows them to break complex tasks into subgoals (e.g., \\[prevent human interference\\]:). Adding steering subsystems to otherwise non-agentic AI (like LLMs) may prove irresistible and dangerous, because it may allow rapid capability gains. But this scenario has an upside: aligning a steering subsystem is somewhat simpler than aligning the whole system it steers. Thus, alignment plans that focus on steering subsystems may have an advantage. \n\nI spent a bunch of time trying to work out the brain mechanisms of complex cognition.[^ur4x8hlosg] This work has some relevance for understanding some different types of steering subsystems and resulting types of agency.\n\nCognition is goal-directed in different ways when different steering mechanisms are used. There are several distinctions proposed by different cognitive sciences: model-based vs. model-free RL from machine learning; habitual vs. goal-directed behavior from animal neuroscience; automatic vs. controlled processing from cognitive psychology; and System 1 and System 2 thinking from behavioral economics. None of these distinctions seems to cleanly match the brain mechanisms creating different types of goal-directed cognition for human decision-making.[^k95xnsdrdwk] Therefore I'll describe the cognitive mechanisms directly.\n\nAgency is not a binary; it is at least a spectrum.  Humans use at least three types of steering:\n\n**Types of steering and agency**\n--------------------------------\n\n1.  Systems trained with reward and reward predictions \n    *   No steering subsystems\n2.  Systems that predict outcomes of actions and their values \n    *   Limited steering subsystems\n3.  Systems that select possible high-reward outcomes as goals\n    *   Full steering subsystems\n    *   Hierarchical subgoal creation for planning\n    *   Implemented only recently, few barriers to improvement\n\nAll of these are goal-directed and agentic, but in importantly different ways. So far, AI systems have only realized the latter two in very limited form, but the field is poised for progress in both of those types of steering.\n\n### **Type 1: predicting reward for RL training**\n\nMost high-performing reinforcement learning systems use a critic system of some sort.[^rietakhkeba] This can be (arguably) considered one type of steering subsystem. The critic system is trained to predict the *value* (sum of future rewards) of world-states and actions. In the simplest configuration, the critic’s value estimate is used to train the actor system; the estimated value of the world-state reached by each action is used as a reward signal to train the policy. Critic systems are ubiquitous in RL systems because they're useful.[^7pk6z9iikar] In particular they are helpful in bridging temporal gaps when reward is sparse, as it is for most embodied organisms.\n\nThis application of a critic is a steering subsystem in a relatively weak sense. It is just extending the effect of reward on training. If the system gets reward for finding diamonds, the critic makes it better at this learning problem by rewarding policies that achieve states that in turn lead to finding diamonds. So I would tend to not call this a steering subsystem, just a method of creating a system that does some steering. It's not a sharp line, so this arrangement of a critic system used solely for RL training could be considered to fall on either side.\n\nIn humans, we call this type of learning and behavior habitual. When we don’t have time to do more careful and time-consuming decision-making, we do things that have led to good outcomes in the past in similar contexts. \n\nMost RL agents use only this type of learning. DeepMind's early Atari playing agents used a critic system as an extra head of the network. This type of system uses a critic system to provide a training signal, but it is not used as part of a look-ahead (e.g., tree search) routine, or to create explicit goal representations, as in types 2 and 3 described below. Mammals, including humans, use this type of critic system, as well as types 2 and 3. The dopamine system predicts rewards,[^gd8jayj5fa] and dopamine drives learning in the rest of the system.[^q0lnvnqdg6h]\n\nThis type of training often results in \"brittle\" behaviors, typically classified as habitual or automatic. For example, I might open the fridge to look for a snack when I pass it, even if I’m not hungry. But with enough training, and good enough generalization in the system, this type of learning can produce behaviors that change appropriately to pursue rewards when the contingencies of the environment change. After enough experience, I won't open the fridge when I’m full, because I've learned it isn’t rewarding when my body is signaling satiety. Animal experiments have demonstrated this goal-dependence in habitual behavior with adequate training. There's no sharp limit to the sophistication of internal representations that could be developed with this sort of RL; a system might actually learn to emulate a steering system, even if none is explicitly designed in.\n\nThus, this classification is fuzzy. But it seems useful for thinking about types and degrees of agency, and how we might align agentic systems.\n\n### **Type 2: steering toward estimated value of predicted outcomes**\n\nCritic systems can also function as steering subsystems by using the value of predicted outcomes to select actions. For instance, when some sort of lookahead is used (like Monte Carlo tree search in AlphaZero), the system chooses its current action based on the one that will lead to good outcomes, as estimated by the critic. This is what we seem to do when playing a game and looking a few moves ahead.\n\nHumans are thought to do this for some decisions. Introspection[^jxhqcfbbkp] as well as data suggest it. Dopamine seems to signal the estimated value of whatever option the animal is currently considering, and to otherwise provide a best-guess estimate of value[^gd8jayj5fa] that is useful for Bayesian decision-making.[^09uuhq7nxc76] There are probably exceptions, since longer-term and more complex decisions haven’t been thoroughly tested, but the loose match seems pretty certain. It seems that humans probably use a tree search of limited depth, made useful by good abstraction and prediction. This search is (probably) pruned by, and actions chosen using, estimated values of predicted states from the dopamine critic system.\n\nThus, the system *looks into the future* and *steers* toward outcomes deemed valuable according to its internal estimates of value. This type of steering is the beginning of what we might intuitively think of as \"real\" agentic behavior (or not; definitions vary).[ Discovering Agents](https://www.alignmentforum.org/posts/XxX2CAoFskuQNkBDy/discovering-agents) from DeepMind defines it in line with this proposed distinction:\n\n> *Agents are systems that would adapt their policy if their actions influenced the world in a different way.*\n\nThis also assumes the system \"knows\" (accurately represents) those changes, among other assumptions. This might be restated intuitively as a system that actively pursues goals, rather than a system that produces behaviors that tended to achieve goals during its training/design. Again, there's no obvious lower bound on what type of training could produce this definition of agentic behavior. But including a type 2 steering system ensures that the system meets this definition of agency.\n\n### **Type 3: steering toward self-selected goals**\n\nHumans sometimes think in an even more goal-directed and agentic way. We sometimes choose a goal and use that goal representation to drive planning. I might make a goal of going to a store to buy a snack, or starting a successful business. Those goal representations will drive my planning in direct and indirect ways, in the long and short term.\n\nThe idea of choosing a goal is at odds with how we use the term in alignment. We often use “goal” synonymously with rewards or maximization goals. I usually use \"goals\" synonymously with \"values\". But for humans and similar systems, they’re not synonymous. What we call “our values” are, I think, estimates of future rewards. This is nicely synonymous with the term of values in reinforcement learning, if I’m roughly correct about how that works (see [Human preferences as RL critic values - implications for alignment](https://www.lesswrong.com/posts/HEonwwQLhMB9fqABh/human-preferences-as-rl-critic-values-implications-for)). \n\nWhen we use the term goals for ourselves, we mean explicit, specific (although abstract) goals like getting a snack, getting back to work, getting a job we like, founding a business, etc. That type of goal, and the associated representations, is the heart of Type 3 steering. \n\nThis cognitive capacity has several advantages. It allows for backward-chaining from a desired goal state to actions that might achieve it. More importantly, this ability almost automatically allows an agent to strategically break a complex task into subtasks. Creating subgoals uses the same mechanisms, since humans (and effective AIs) take context into effect when choosing goals.  For more complex tasks and problems, this decomposition seems likely to be useful. Engineering improvements in technology will decompose into hundreds of component problems involving material properties, manufacturing processes, economic and human factors, etc.\n\nThus far, empirical results showing improvements from problem decomposition are weak.[^ut1uui91yah] But it seems likely that decomposition is highly useful for effective cognition; the world, and problems in the world, really seem to decompose.\n\nI don't know of any work that fully describes how the brain creates useful goal representations. I haven't published my theories on this in part because it could advance capabilities. But I don't think this is terribly hard to figure out. And I don’t think it requires any breakthroughs to get AI systems to do this type of goal-creation steering in other ways. Indeed, LLMs seem rather adept at breaking a problem into subproblems. Language model agents (LMAs) can perform type 3 steering, even if they’re currently not good at executing the problem-solving plans they create.\n\n**Steering subsystems, AI progress, and alignment**\n---------------------------------------------------\n\nLanguage model agents usually start with the prompt “create a plan to achieve \\[goal\\]”. This creates a multi-step plan, and each step is approached separately. This is type 3 steering.\n\nLanguage model agents have yet to accomplish anything particularly impressive, but they do show promise on some tasks (such as [Minecraft](https://huggingface.co/papers/2305.16291)). So it seems far too early to rule them out as a path to AGI.  Language models have some real intelligence, and it is difficult to guess how far this can be improved by [scaffolding](https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers) with other cognitive systems and software tools into agentic [language model cognitive architectures](https://www.alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures), or language model agents, LMAs. It is so early in the development of language model agents that I give LMAs a round no-idea 50% chance of being the first route to self-improving, self-aware, thoroughly agentic AGI. \n\nIf LMAs do achieve AGI, I think this is relatively good news. I think they offer several advantages that make them the easiest-to-align type of plausible AGI. These include easier interpretability and a potentially very low alignment tax. I’ve written about these advantages [here](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent). One major advantage is that alignment efforts can center on the steering subsystem: this type of agent can be given a top-level goal of corrigibility, and any other combination of alignment goals. These can be stated in natural language, leveraging the system’s training prior to deployment. \n\nIf language model agents aren’t the first route to AGI, I think we’ll still see AGI with powerful, type 2 and 3 steering subsystems, based on the cognitive advantages they offer. If this is correct, we should create alignment approaches that focus on steering subsystems, given their central role in goal-directed behavior. \n\nThis is why I like Steve Byrnes’ [Plan for mediocre alignment of brain-like \\[model-based RL\\] AGI](https://www.alignmentforum.org/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi). It works primarily on the critic (steering) subsystem. In essence, the plan is to induce the model to “think” about the thing you want it to steer toward (e.g., “hey, think about human flourishing”), then set the weights from the representational system into the critic system to a high value. Presto, an agent that values human flourishing above all else. It's not a fully developed plan yet, but it does seem more concrete and straightforward than any other suggested approach for training human values into an RL agent. This approach also benefits by making use of the agent’s training/intelligence for alignment, something I’ll focus on in a future post. It would seem to have a low alignment tax, and it can work alongside other alignment approaches, like interpretability measures, scalable oversight, etc.\n\nLoosely brainlike RL agents are a highly plausible route to AGI if language model agents don't achieve it first. And the two approaches can be combined. Using RL to train an “outer loop” of cognitive control for language model agents is a frequently-proposed approach to improving LMAs. So the two alignment approaches above, both of which focus on steering subsystem, might be combined for that type of AGI.\n\nBoth of those approaches seem very promising but provide only a loose, “mediocre” alignment with human values. Whether such a rough match is adequate is an important question. If a superintelligence values a subset of human values, will the outcome be satisfactory for humanity? What if it values a superset of our values? A second outstanding issue for these (and other network-based) approaches is the [alignment stability problem](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem). Does reflective stability (ref*) ensure long-term stability in a network-based AGI, with values that are defined by distributed representations of semantics? Or might that system’s values shift dramatically as it continues to learn? I think both of these questions merit more careful thought, and they’ll be the subjects of upcoming posts.\n\n  \n*Thanks to Steve Byrnes and Max H for helpful discussions and comments on a draft of this article.*  \n   \n \n\n[^ur4x8hlosg]: This work was done in collaboration with Randy O'Reilly and many members of his computational cognitive neuroscience lab from 1999 to 2022. We made neural network models of several brain systems, based on a variety of empirical data, focusing on human cognition and animal single-cell recordings. My focus was understanding how multiple brain systems come together to produce complex decision-making and belief formation. \n\n[^k95xnsdrdwk]: For more than you want to know about the various terminologies, and a bit of high-level theory, see: O’Reilly, R. C., Nair, A., Russin, J. L., & Herd, S. A. (2020). How sequential interactive processing within frontostriatal loops supports a continuum of habitual to controlled processing. Frontiers in Psychology, 11, 380. \n\n[^rietakhkeba]: I can’t easily find a comprehensive review of where actor-critic RL (AC RL) or similar systems work, and where it’s not needed. The most impressive instances of RL that I’m aware of all use AC. Those include DeepMind’s prominent RL agents, from the Atari system up through AlphaZero and AlphaFold, OpenAI’s Open Five family of agents, ChatGPTs RLHF (it goes by a different name, but seems firmly in the critic family) and every high-functioning instance of RL in the cognitive neuroscience space I’m more familiar with. I’d love to be corrected if it’s not necessary for some important problems. Here’s a team of real experts calling AC methods “ubiquitous” in RL:  Wen, J., Kumar, S., Gummadi, R., & Schuurmans, D. (2021, July). Characterizing the gap between actor-critic and policy gradient. In International Conference on Machine Learning (pp. 11101-11111). PMLR. \n\n[^7pk6z9iikar]: Including a critic system seems to be useful for a few reasons. It's splitting the learning problem into two separate pieces, what to do in a given situation, and how good each action outcome is. These are similar, and can be collapsed to one problem in either direction. But they appear to be different enough that including both provides better traction on the learning problem. They don't add that computational cost when they're implemented as two heads of the same network, as they usually are in deep network approaches. Having a critic also enables the MCTS-boosting approach taken by AlphaZero and similar algorithms, in which a few-move lookahead is used, the best move(s) trained into the actor. It's necessary to estimate which the best resulting board positions are to make this useful. Finally, critic systems are useful when reward is rare (like most real-world environments), since they provide at least a guess about how likely each action is to eventually lead to reward. \n\n[^gd8jayj5fa]: Dopamine predicting value as the sum of future rewards is an approximation. It's actually a value delta. Phasic (fast) dopamine release signals the difference between the currently predicted value, and the one predicted just prior. This is termed a reward prediction error, or RPE. This is the temporal difference (TD) critic algorithm, but most actor-critic RL systems don’t seem to employ this temporal difference, although I haven’t dug far enough into the math for high-functioning Q-learning systems (like the DeepMind RL agents) to be certain it’s not hidden in there. The AC-advantage RL approach does something similar. Signaling the derivative rather than the absolute value is advantageous when the last state is a relevant comparison. This is often the case in when possible options are considered sequentially, which is one reason I think the human brain uses that approach (see the introduction to Neural mechanisms of human decision-making for more on this theory, although a clearer, more complete writeup is on my to-do list). \n\n[^q0lnvnqdg6h]: Dopamine acts as the output of a critic system consisting of the amygdala and associated subcortical areas. The dopamine signal acts very much like a critic reward signal in an actor-critic RL system, by triggering positive or negative learning directly in the striatum, a large subcortical area that's heavily involved in action selection and decision-making. This system has been relatively well-investigated; for a review see Mollick, J. A., Hazy, T. E., Krueger, K. A., Nair, A., Mackie, P., Herd, S. A., & O'Reilly, R. C. (2020). A Systems-Neuroscience Model of Phasic Dopamine Psychological review, 127(6), 972. Dopamine affects learning in the cortex in less well-understood ways. \n\n[^jxhqcfbbkp]: Introspection is rarely mentioned in published papers. Private conversations suggest that cognitive scientists lean heavily on introspection when producing hypotheses and interpreting data. I take introspection seriously when it's done carefully. From a materialist perspective, it would be quite odd if introspection told us nothing about brain processes. Much has been made of a set of studies showing that introspection can be quite mistaken in some cases. That work is neatly summarized in Tim Wilson's Strangers to Ourselves, which I highly recommend for insight into the likely nature of unconscious processing. However, those studies can be summarized as showing how people mistake how they've made decisions, not actually being wrong about what they're thinking about. The hypothesis that we're aware of roughly the contents of working memory at any given moment, originating in the cognitive revolution, still seems perfectly viable as reviewed here. A critical review of purported counterevidence can be found here. \n\n[^09uuhq7nxc76]: For an excellent review see Gershman, S. J., & Uchida, N. (2019). Believing in dopamine. Nature Reviews Neuroscience, 20(11), 703-714. They review the empirical evidence, and show that dopamine signaling captures uncertainty as well as expected value, and is useful for Bayesian belief formation as well as decision-making. \n\n[^ut1uui91yah]: Tree of Thoughts and related work show efficacy in toy problems designed to decompose well, so they're not direct evidence this is important in AGI-relevant domains. LLMs decompose problems remarkably well with appropriate prompting, without steering subsystems to aid them, but it does seem like explicit decomposition mechanisms can only help, and may prove critical in tasks complex enough (like solving engineering problems) where humans definitely use problem decomposition.",
      "plaintextDescription": "Human brains have steering subsystems. LLMs and most RL agents do not. Steering systems, as defined by Max H, are goal-directed AI systems, or optimizers. Here I focus on steering subsystems: the parts of human and AI cognitive systems most directly relevant to goal-direction. These work in three distinct ways (and probably more), each providing a different type and amount of agency, and associated capabilities. \n\nThinking about types of steering subsystems can clarify our conceptions of agency. Steering subsystems increase risks by adding capabilities. Notably, sophisticated steering subsystems create useful representations of goals. This allows them to break complex tasks into subgoals (e.g., [prevent human interference]:). Adding steering subsystems to otherwise non-agentic AI (like LLMs) may prove irresistible and dangerous, because it may allow rapid capability gains. But this scenario has an upside: aligning a steering subsystem is somewhat simpler than aligning the whole system it steers. Thus, alignment plans that focus on steering subsystems may have an advantage. \n\nI spent a bunch of time trying to work out the brain mechanisms of complex cognition.[1] This work has some relevance for understanding some different types of steering subsystems and resulting types of agency.\n\nCognition is goal-directed in different ways when different steering mechanisms are used. There are several distinctions proposed by different cognitive sciences: model-based vs. model-free RL from machine learning; habitual vs. goal-directed behavior from animal neuroscience; automatic vs. controlled processing from cognitive psychology; and System 1 and System 2 thinking from behavioral economics. None of these distinctions seems to cleanly match the brain mechanisms creating different types of goal-directed cognition for human decision-making.[2] Therefore I'll describe the cognitive mechanisms directly.\n\nAgency is not a binary; it is at least a spectrum.  Humans use at least three ty",
      "wordCount": 2293
    },
    "tags": [
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "iKYWGuFx2qH2nYu6J",
        "name": "AI Capabilities",
        "slug": "ai-capabilities"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "nZYhs48pWsaCCgGfi",
    "title": "AGI isn't just a technology",
    "slug": "agi-isn-t-just-a-technology",
    "url": null,
    "baseScore": 18,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2023-09-01T14:35:57.062Z",
    "contents": {
      "markdown": "Each time a skeptic talks about AI as a technology, I think it signals a likely crux.\n\nI've been watching the public AI debate closely and sadly. I think that debate might be crucial in whether we actually get aligned AGI, and it's not going particularly well so far. The debate is confused, and at risk of [causing polarization](https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs) by irritating all involved. Identifying cruxes should help the debate be less irritating.\n\nAgency as a crux for x-risk\n---------------------------\n\nOf course AGI is a technology. But only in the way that humans are technically animals. Saying humans are animals has strong and wrong implications about how we behave and think. Calling AGI a technology has similar wrong implications.\n\nTechnologies do what they're designed to, with some potential accidents and side effects. Boilers explode, and the internet is used for arguments and misinformation instead of spreading information. These effects can be severe, and could even threaten the future of humanity. But they're not as dangerous as accidentally creating something that becomes smarter than you, and actively tries to kill you.\n\nWhen someone refers to AI as a technology, I think they're often not thinking of it as having full agency.\n\nWhile AI without full agency does present possible x-risks, I think it's a mistake to mix those in with the risks from fully agentic AGI. The risks from a fully agentic AGI are both easier to grasp, and more severe. I think it's wiser to address those first, and only move on with a careful distinction in topics.\n\nBy full agency, I mean something that pursues goals, and chooses its own subgoals (a relevant example subgoal is preventing humans from interfering with its projects). There's a spectrum of agency. A chess program has limited agency; it was made to play a good game of chess, and it can take moves to do that. Animals don't really make long-range plans that include subgoals, and no existing AI has long-range goals and makes new plans to achieve them. Humans are currently unique in that regard.\n\nThere's a huge difference between something deciding to kill you, and making a plan to do it, and something killing you by accident or misuse. Making this distinction should help deconfuse conversations on x-risk.\n\nAgency seems inevitable\n-----------------------\n\nThe above is only a good strategy if we're likely to see fully agentic AGI before too long. I find it implausible that we'll collectively stop short of creating fully agentic AI. I agree with Gwern's arguments for [Why Tool AIs Want to Be Agent AIs](https://gwern.net/tool-ai). Agents are desirable because they can actually do things. And AI actively making itself smarter seems useful.\n\nI think there is one more pressure for agentic AI that Gwern doesn't mention. One is the usefulness of creating explicit sub-goals for problem solving and planning. This is crucial for human problem-solving, and seems likely to be advantageous for many types of AI as well. Setting subgoals allows backward-chaining and problem factorization, among other advantages. I'll try to address this issue more carefully in a future post.\n\nNone of the above is meant to imply that agency is a binary category. I think agency is a branching spectrum. A saw, a chess program, an LLM, and a human have different amounts and types of agency. But I think it's likely we'll see AGI with all of the agency that humans have.\n\nIf this is correct, this is the issue we should focus on in x-risk discussions. If it's not, I'd be far less worried about AI risks. This potentially creates a point of agreement and an opportunity for productive discussion with x-risk skeptics who aren't thinking about fully agentic AI.",
      "plaintextDescription": "Each time a skeptic talks about AI as a technology, I think it signals a likely crux.\n\nI've been watching the public AI debate closely and sadly. I think that debate might be crucial in whether we actually get aligned AGI, and it's not going particularly well so far. The debate is confused, and at risk of causing polarization by irritating all involved. Identifying cruxes should help the debate be less irritating.\n\n\nAgency as a crux for x-risk\nOf course AGI is a technology. But only in the way that humans are technically animals. Saying humans are animals has strong and wrong implications about how we behave and think. Calling AGI a technology has similar wrong implications.\n\nTechnologies do what they're designed to, with some potential accidents and side effects. Boilers explode, and the internet is used for arguments and misinformation instead of spreading information. These effects can be severe, and could even threaten the future of humanity. But they're not as dangerous as accidentally creating something that becomes smarter than you, and actively tries to kill you.\n\nWhen someone refers to AI as a technology, I think they're often not thinking of it as having full agency.\n\nWhile AI without full agency does present possible x-risks, I think it's a mistake to mix those in with the risks from fully agentic AGI. The risks from a fully agentic AGI are both easier to grasp, and more severe. I think it's wiser to address those first, and only move on with a careful distinction in topics.\n\nBy full agency, I mean something that pursues goals, and chooses its own subgoals (a relevant example subgoal is preventing humans from interfering with its projects). There's a spectrum of agency. A chess program has limited agency; it was made to play a good game of chess, and it can take moves to do that. Animals don't really make long-range plans that include subgoals, and no existing AI has long-range goals and makes new plans to achieve them. Humans are currently unique in that",
      "wordCount": 614
    },
    "tags": [
      {
        "_id": "bQZAkiFgtbEcr5h6f",
        "name": "AI Persuasion",
        "slug": "ai-persuasion"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Q7XWGqL4HjjRmhEyG",
    "title": "Internal independent review for language model agent alignment",
    "slug": "internal-independent-review-for-language-model-agent",
    "url": null,
    "baseScore": 56,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 30,
    "createdAt": null,
    "postedAt": "2023-07-07T06:54:11.552Z",
    "contents": {
      "markdown": "Abstract:\n---------\n\nLanguage model agents (LMAs) expanding on AutoGPT are a highly plausible route to AGI. This route has large potential timeline and proliferation downsides, but large alignment advantages relative to other realistic paths to AGI. LMAs allow layered safety measures, including externalized reasoning oversight, RLHF and similar alignment fine-tuning, and specifying top-level alignment goals in natural language. They are relatively interpretable, and the above approaches all have a low alignment tax, making voluntary adoption more likely. \n\nHere I focus on another advantage of aligning LMAs over other plausible routes to early AGI. This is the advantage of using separate language model instances in different roles. I propose *internal independent review* for the safety, alignment, and efficacy of plans. Such a review would consist of calling fresh instances of a language model with scripted prompts asking for critiques of plans with regard to accomplishing goals, including safety/alignment goals. This additional safety check seems to create a low alignment tax, since a similar check for efficacy will likely be helpful for capabilities. This type of review adds one additional layer of safety on top of RLHF, explicit alignment goals, and external review, all proposed elsewhere.\n\nThis set of safety measures does not guarantee successful alignment. However, it does seem like the most practically viable set of alignment plans that we've got so far. \n\n![](https://lh4.googleusercontent.com/3EGUkRxS9l3Ypnp07-ahu8VhjmZMq6DhhxHSi0MVn0tB8KWTtQFOo9WeJjCqFKNqXI3ZNj3iG88uyQDx3GieQhsDYi3NroiWsHGmjcNMkRNNLygYFW240ekOOHT8XtCpKOTkYGT6jQT1dBC-yE13mgE)\n\nCaption: Language model agent as a metaphorical committee of[ shoggoths](https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence) using tools and passing ideas and conclusions in natural language. This committee/agent makes plans and takes actions that pursue goals specified in natural language, including alignment and corrigibility goals. One committee role can be reviewing plans for efficacy and alignment. This role should be filled by a new instance of a shoggoth, making it an *independent internal review*. Rotating out shoggoths (calling new instances) and using multiple species (LLM types) limits their ability to collude, even if they sometimes simulate villainous[ ](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post)Waluigi characters.[^f7lcocn6y7f] The committee's proceedings are recorded for[ external review](https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for) by human and AI reviewers. Artist credit: Sabrina Feld.\n\n**Introduction** \n-----------------\n\nI'm afraid you're not going to like this. This alignment plan is messy. A language model agent solving worthwhile problems will be no vast, cool intellect. They will be a chattering mess of [babble and prune](https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W), produced by multiple systems with different modes of operation, collaborating in rather complex and ad-hoc ways. And the safety measures they allow are also a messy, multi-layered approach. The organized mind recoils. This is not an aesthetically appealing alignment approach. But LMAs might well be the first route to AGI and superintelligence, so creating workable alignment plans for them may not be optional. \n\nPrevious work has discussed the alignment advantages of language model agents.[^07ft2u7s0klw] Here I make a stronger proposal: this messy set of alignment plans seems to be the best chance we’ve got. \n\n**Language model agents**\n-------------------------\n\nLanguage model agents (LMAs) are agentic AI systems that use LLMs as their central cognitive engine. AutoGPT is the best known example of such a system that prompts LLMs in various ways to pursue a goal. Those prompts include making plans and taking actions through external tools. I've referred to these as[ agentized](https://www.lesswrong.com/posts/dcoxvEhAfYcov2LA6/agentized-llms-will-change-the-alignment-landscape) LLMs or, more formally,[ language model cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures). They're a subset of[ scaffolded](https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers) LLMs that have an explicit goal. AutoGPT, BabyAGI, and[ Voyager](https://voyager.minedojo.org/) are examples of this type of agent. The[ Smallville](https://arxiv.org/abs/2304.03442) simulacra are LMAs with few actions available. HuggingGPT allows many actions but does not include the internal goal-directed cognition to constitute an agent. SmartGPT and other chain of thought prompting systems are scaffolded LLMs but not fully agents, as they do not explicitly or directly pursue goals.\n\nI'm moving to the term language model agent (LMA) as more intuitive than language model cognitive architecture, and more intuitive and specific than scaffolded LLMs. A more complete term would be \"large-language-model-based agentic cognitive architectures,\" but that's unwieldy, and most of the remainder seems implied by LMA.\n\n**Why think about LMA alignment?**\n----------------------------------\n\nIt seems useful to think about aligning LMAs because it seems fairly likely that they'll be the type of AGI we get first. And because aligning LMAs might be easier than aligning any other type of AGI that's actually relevant in this timeline. \n\nIn[ Capabilities and alignment of LLM cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) I gave the logic for how LMAs could gain capabilities rapidly due to their modularity, the low cost of development, and the economic incentives for development by both large and small teams.\n\nSince writing that post, my enthusiasm for the capabilities prospects of this type of AGI has diminished, but only slightly. Talking to individuals working on these systems has indicated that there are challenges to developing these systems that I hadn't foreseen. But those projects were small and had only worked on LMAs for very limited amounts of time. New work since then has shown many ways in which scaffolding LLMs dramatically improves their reasoning and problem-solving abilities in various ways.[^2ngjaj0i72f] These new scaffolding techniques are relatively easy to incorporate into LMAs.\n\nThere are good reasons to think that work on LMAs will receive increasing attention and funding in the coming months and years, even if progress is not as easy or rapid as we might’ve hoped or feared.[^9cvzszooixe] For instance, assistant agents for things like online shopping, scheduling, and turning data into reports would seem to have enormous economic value, even if their work is imperfect and requires human review.\n\n**Internal independent review for LMAs**\n----------------------------------------\n\nLMAs can include an internal independent review of plans for both efficacy and alignment. This is a nontrivial advantage for alignment.\n\nPrevious discussions of LMA alignment advantages have left out or glossed over this  advantage. Constitutional AI has important similarities but is importantly different.[^z4lzc2doebj] My [previous post](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) and [this post](https://www.lesswrong.com/posts/JnAh4YHfrYpPNwc8Y/alignment-of-autogpt-agents) alluded to such review indirectly. \n\nAn independent internal review in an LMA would be roughly analogous to having a new individual arrive to review each plan produced by a committee. This new individual is independent of the committee in the sense that they haven't been influenced by the committee, beyond any bias or prompt injections inserted into the plan description.[^1fhnk6dvge9]\n\nThis review can be considered independent by calling a new instance of an LLM  to evaluate the efficacy and alignment of plans. I include review of plan efficacy to emphasize the low alignment tax of this approach. Including an internal review for plan efficacy should be useful for capability. Running the same internal independent review against alignment goals would add some level of safety. Of course, the amount of efficacy and safety added is dependent on how that review is structured and the capabilities of the LLMs used for that review.\n\nThis review might be simple, such as a prompt asking \"considering this plan: \\[plan description\\], how likely is this plan to fulfill the goals of \\[goal descriptions\\]?\" This review could be greatly expanded, including a variety of prompts to look for possible unexpected outcomes of a given plan from different angles, and to consider different possible user intent of the goals under consideration. \n\nFor instance, the network might be prompted with “how could this \\[plan description\\] fail to achieve \\[goal descriptions\\]?” and “what’s the worst possible side effect of \\[plan description\\]?”. A new instance could then be prompted with variations of the prompt “if something unexpected happens, what are the odds of getting \\[side effect description or failure mode description\\] from \\[plan description\\]?”, algorithmically average those probability ratings, and then be prompted to revise the plan or create a new plan with those possible failure modes in the prompt, or ask a human for input if the estimated possible consequences exceed thresholds in severity and likelihood. \n\nI sketch out some similar ideas for using LLM ratings as input to algorithms that help mimic human executive function[ here](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures#Complex_decision_making__Decision_Making___Executive_Function___Episodic_Memory), and they apply to reviewing plans as well as other types of executive function. [Tree of Thoughts](https://arxiv.org/abs/2305.10601) is an example; it combines prompting and algorithms for boosting capabilities, and the same approach (and improvements) could be applied to an internal independent review process.\n\nThe key strength of this approach is that it applies the capabilities of the LLM to reducing risks of harmful behavior. A highly capable LLM might produce a plan with very harmful outcomes, either accidentally or through evoking a hostile simulated character. But a distinct instance of a capable LLM is employed to catch those hostile plans, and other instances are called to elaborate that plan and predict its outcomes. To produce deliberately harmful outcomes, one instance of the LLM must outsmart a whole system composed of other instances.\n\nIn an LMA that is not further trained with reinforcement learning, there is no direct optimization pressure for the network to be deceptive or misaligned. There will be an indirect pressure from the human creators to avoid a review process that spends excess resources and vetoes too many plans, and this is a substantial caveat. But this type of optimization pressure against safety measures seems inevitable in any realistic AGI approach.\n\n### **Limitations**\n\nOf course this type of elaborated review process is still limited by the abilities of the LLMs. Existing LLMs have dramatic blind spots. Elaborated systems of prompts and algorithmic aggregation can help work around those blind spots. For instance, [Tree of Thoughts](https://arxiv.org/abs/2305.10601) is a relatively simple system of prompting, evaluating, and remembering responses that boosts performance from 4% to 74% on the “game of 24” task, essentially allowing GPT4 to perform well at a task that is otherwise a blind spot. While this type of elaboration will boost plan-making capabilities as well as review capabilities, each branch in that tree of thought is also available for internal and external review.\n\nOther types of AI tools may also help close those blind spots. For instance, HuggingGPT already allows LLMs to use generative vision models to \"imagine\" a scene, and interpretive vision models to reason based on that scene, in much the same way that humans seem to do. Tools like Wolfram Alpha can help dramatically with mathematical and spatial reasoning, etc.\n\nEach of these approaches is currently limited, but LLMs and the tools they can access will continue to improve. This is a double-edged sword, as improvements can be used both for planning and review of plans. \n\nI think that LLMs do not need to improve all that much to hit human level in most areas. Integrating them into a cognitive architecture that elaborates and checks its reasoning bridge the gap from human-level-with-significant-blindspots to human level in every relevant area, allowing full autonomy. This may be a critical threshold for fully autonomous AI, introducing the risk of rogue self-improving AGI. I am [an LLM plateau-ist](https://www.lesswrong.com/posts/KJRBb43nDxk6mwLcR/ai-doom-from-an-llm-plateau-ist-perspective) but still think that language model agents are moderately likely (40%?) to achieve full AGI in the near future (my guess is 4 years, give or take two years).\n\nEven if internal review effectively ensures that LMAs pursue the goals they are given, there is still the substantial risk of those agents interpreting their goals differently than their creators intended. Thus, it seems that external review and corrigibility will remain critical pieces of a successful alignment approach for LMAs.\n\nAnother class of limitations also applies to external review, and is discussed in that subsection below.\n\nBecause of these and other yet-to-be-identified weaknesses, internal independent review does not seem reliable enough to produce an agent that's trustworthy to be run autonomously. To have a decent chance at a reliably aligned agent (let alone a long-term[ stable alignment](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem)) internal review should be combined with other approaches.\n\n**LMA alignment allows multiple approaches to stack**\n-----------------------------------------------------\n\nLMAs allow a[ hodgepodge](https://www.lesswrong.com/posts/YnGRBADQwpYRbuCbz/towards-hodge-podge-alignment-1) alignment approach. Aligning other types of network-based AGIs also allow some layering of approaches, but several of these have unique advantages in an LMA, so I include a brief treatment of each:\n\n*   Explicit alignment goals\n*   Externalized reasoning oversight\n*   Evoking benevolent simulacra\n*   LMAs as rotating committees\n*   Fine-tuning LLMs for alignment\n\n### **Explicit alignment goals**\n\nInternal independent review is only useful for alignment if the LMA has alignment goals. This idea seems to originate from David Shapiro in his 2021 book[ Natural language cognitive architectures](https://www.barnesandnoble.com/w/natural-language-cognitive-architecture-david-shapiro/1139957470). He basically proposes giving the agent extra goals that are specifically about ethics. A language agent might be given something like the following top-level prompt:\n\nCreate and execute a plan that fulfills the following goals:\n\n*   Make me a lot of money\n*   Make the world a better place\n*   Check with me before you do anything that might take you out of my control, cost me more than $(x), or be physically or emotionally dangerous to anyone.\n\n(These are examples, not a proposal for the right natural language alignment goals. That is a separate discussion.)\n\nOf course, a lot would have to happen to make the agent interpret any of those goals in just the way you hoped it would. While I don’t have a good proposal for a complete set of natural language alignment goals, [corrigibility](https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility) seems like the most important. Natural language seems like a better bet than RL training for generalizing corrigibility and other goals to novel circumstances. \n\n### **Externalized reasoning oversight**\n\nAnother important approach for aligning a language model agent is Tamera Lanham's 2022[ Externalized reasoning oversight: a research direction for language model alignment](https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for). Lanham notes that an agent that reasons in natural language can have its thinking reviewed by passing a transcript of that thinking to a human, perhaps assisted by other AI agents. This type of external review could reveal flaws in the model’s alignment, and allow its creators to iteratively improve its alignment (as long as that agent remains corrigible).\n\nI would guess that an LMA approaching human capability would produce hundreds to millions of prompts and responses while developing any useful plan.[^0f8kmp22pomm] Reviewing each of those for hints of misalignment might quickly go beyond practical human ability,[^dc4w3vyztv5] so I'd envision a separate AI system aiding in that review. Those assistant AIs could use anything from a simple sentiment analysis, to something more akin to the complex internal independent review described above, or other approaches. This would constitute a form of scalable oversight similar to that proposed in [iterated amplification](https://www.lesswrong.com/tag/iterated-amplification).\n\nThis proposal has been critiqued in two important ways. These same critiques largely apply to internal independent review, since an LLM reviewer could be fooled in many of the ways a human reviewer could be. First, review would be hampered by learned steganography in LLMs that hides additional meanings beyond that expressed in natural language or other human-readable formats. However, RL pressures would usually seem to add additional information, as discussed [here](https://www.alignmentforum.org/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId=zfzHshctWZYo8JkLe#comments), rather than reversing the meaning so that a natural language description of a helpful plan would be trained to encode a different, deliberately harmful plan. \n\nThe second major concern limiting interpetability and therefore review is that LLMs prompted to \"think step by step\" do not always follow the logic they describe in those steps. This is revealed by their[ giving the same final answer after intermediate steps are changed](https://www.lesswrong.com/posts/FX5JmftqL2j6K8dn4/shapley-value-attribution-in-chain-of-thought). The concern can be addressed in part by giving LLMs smaller pieces of the problem, rather than allowing their context window to contain large trains of reasoning. This shortening would also seem to reduce the risk of[ simulacra](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators) in LLMs[ collapsing into Waluigi](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post#Superpositions_will_typically_collapse_to_waluigis)-like villains.[^f7lcocn6y7f] This type of factored cognition has [costs in humans](https://www.lesswrong.com/posts/tmuFmHuyb4eWmPXz8/rant-on-problem-factorization-for-alignment), but demonstrated value in LLM scaffolding such as [tree of thoughts](https://arxiv.org/abs/2305.10601). More consideration can be found [here](https://www.lesswrong.com/tag/factored-cognition), but the costs and value in LMAs will be explored empirically.\n\n### **Benevolent identity prompts**\n\n[Including prompts for LLMs to simulate a benevolent character](https://www.lesswrong.com/posts/JnAh4YHfrYpPNwc8Y/alignment-of-autogpt-agents) should add some degree of safety. The degree of added alignment is questionable, since this does not deal with potential Waluigi effects. However, it is an additional “stackable” alignment measure, and so probably worth implementing. This may be more critically important if an LMA is run on LLMs without RLHF or a similar ethical fine-tuning. This is a likely occurrence at least in open-source LLMs, since RLHF is thought to decrease creativity and other capacities of LLMs.\n\n### **LMAs as rotating committees**\n\nExisting discussions of language model agent alignment often seem to treat the LLM as an individual. This is a partially accurate analogy, but considering each agent as a committee is equally apt, and the reality will be somewhere on a continuum between those two extremes. LMAs are like individuals to the extent each LLM call includes include maximum context, including relevant episodic memories, within each prompt. If prompts are more minimal, the system acts less like an individual and more like a committee. The committee analogy is still imperfect, because each \"committe member\" only has the amount of memory that the overall system puts in in the prompt. Thus, there are important similarities to a committee that rotates in a whole new person for individual questions.\n\nWhile the LLM instances involved probably do have something analogous to a personality based on the simulacra they're currently simulating, those personalities are brief and have short memories. This type of committee seems intuitively less capable of working against the committee's stated goals. Of course that's still possible, particularly if similar prompts are repeatedly used for the same underlying LLM. And, of course, it's still possible for such a committee to pursue its stated goals in ways that weren't intended (goal misgeneralization).\n\n### **Fine-tuning LLMs for alignment**\n\nA system that's less likely to propose harmful actions leaves less work for internal review and specific alignment goals. Reinforcement learning from human feedback (RLHF) is the best known method of fine-tuning LLMs for ethical behavior. Others are equally promising. Self-supervised fine tuning on a hand-created dataset appears to work well, as does RL from AI feedback as in constitutional AI.[^z4lzc2doebj] All of these seem like real but weak approaches to aligning LMAs. Including alignment fine-tuning along with other approaches will provide nontrivial advantages. \n\nSome amount of alignment is provided merely by using LLMs trained on common language use. The majority of public writing is done by humans who at least profess to dislike actions like wiping out humanity in favor of paperclips. Curating the training data to exclude particularly unaligned text should also be a modest improvement to base alignment tendenciesI think this is much weaker even than RLHF, but still a contributing factor. \n\nUsing base LLMs that are less likely to propose harmful actions is a two-edged sword. It will reduce the disastrous alignment failures, but it also reduces the day-to-day need for other alignment measures. This would also reduce the incentives to include other robust alignment safeguards in LMA systems.\n\nConclusion\n----------\n\nThere's a good deal more to say about the other layers of LMA alignment approaches and how they fit together.  Those will be the subject of future posts.\n\nI hope that this proposal doesn't[ miss the hard bits of the alignment challenge](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment) (except for long-term[ stability](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem) which is intentionally omitted, to address in an upcoming post). I believe most of the arguments for the[ Least Forgiving Take On Alignment](https://alignmentforum.org/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment). I also believe that we need to produce plans with low alignment tax that apply to the AGI systems people will actually build first. As such, I think aligning LMAs is the best plan we currently have available, barring some large change in the coordination problem. I think that this alignment plan is far from foolproof or certain, but it has large advantages relative to all other plans I'm aware of. In the case of other likely routes to AGI based on neural networks, LMAs are more easily interpretable and more easily made corrigible. I agree with most of [these less obvious concerns](https://www.lesswrong.com/posts/mAwxebLw3nYbDivmt/scaffolded-llms-less-obvious-concerns) about LMA alignment, but think other network approaches have the same challenges without all of the advantages. Relative to other alignment plans following more novel routes to AGI, aligning LMAs seems less safe but much more practical, as it does not require stopping current progress and launching completely different approaches. \n\nI want an alignment solution with both decent chances of success and decent chances of being implemented, rather than merely telling people \"I told you so\" when the world pursues AGI without a good alignment plan. Aligning LMAs looks to me like the best fit for that criteria.\n\nI realize that this is a large claim. I make it because I currently believe it, and because I want to get pushback on this logic. I want to stress-test this claim before I follow this logic to its conclusion by advising safety-minded people to actually work on the capabilities of language model agents. \n\n  \n \n\n[^f7lcocn6y7f]: The Waluigi effect is the possibility of an LLM simulating a villainous/unaligned character even when it is prompted to simulate a heroic/aligned character. Natural language training sets include fictional villains that claim to be aligned before revealing their unaligned motives. However, they seldom reveal their true nature quickly. I find the logic of collapsing to a Waluigi state modestly compelling. This collapse is analogous to the reveal in fiction; villains seldom reveal themselves to secretly be heroes. It seems that collapses should be reduced by keeping prompt histories short, and that the damage from villainous simulacra can be limited by resetting prompt histories and thus calling for a new simulation. This logic is spelled out in detail in A smart enough LLM might be deadly simply if you run it for long enough, The Waluigi Effect (mega-post), and Simulators. \n\n[^07ft2u7s0klw]: Previous work specifically relevant to aligning LMAs. RLHF and other LLM ethical fine-tuning is omitted.Natural language cognitive architectures 2021 book by David Shapiro; proposed including alignment goals in natural languageICA SimulacraOzyrus delayed posting this by more than a year to avoid advancing capabilities.Agentized LLMs will change the alignment landscapeAlignment of AutoGPT agentsCapabilities and alignment of LLM cognitive architecturesMy previous post on expanding LLMs to loosely brainlike cognitive architectures, and vague alignment plansAligned AI via monitoring objectives in AutoGPT-like systemsThe Translucent Thoughts Hypotheses and Their ImplicationsExternalized reasoning oversight: a research direction for language model alignmentTamera Lanham’s early proposal of external review for language model agentsLanguage Agents Reduce the Risk of Existential CatastropheCAIS-inspired approach towards safer and more interpretable AGIsThere is surely other valuable work in this area; apologies to those I’ve missed, and pointing me to more relevant work is much appreciated. \n\n[^2ngjaj0i72f]: Progress in scaffolding language models, including some limited agentic systems. Too numerous to mention, so I’ll give a few promising examples. None of these approaches have yet been incorporated into general purpose or assistant LMAs to my knowledge.Tree of Thoughts: Deliberate Problem Solving with Large Language ModelsCreates and prunes a tree search using GPT4. Improves performance from very bad to decently good in three problem spaces that are nontrival for humans. Inspired by Simon & Newell’s work on human problem-solving.LLM+P: Empowering Large Language Models with Optimal Planning ProficiencyCombines LLMs with planning algorithms to solve problems described in language. Demonstrates impressive results in several toy problem domains.GPT-engineer reportedly produces useful code that requires manual review and debugging. It has a central process that asks clarifying questions about the code to be produced before writing it.RecurrentGPT: Interactive Generation of (Arbitrarily) Long TextUses a memory compression mechanism inspired by LSTM to expand a prompt into text, including editable sub-promptsReflexion: Language Agents with Verbal Reinforcement LearningAgentic system that reflects on its actions and maintains those conclusions for future decisionsVoyager: An Open-Ended Embodied Agent with Large Language ModelsSpecialized language model agent for Minecraft. Dramatically improves on SOTA minecraft agents by using coded skills that are interpreted and employed by the LMA, including error detection and correction. \n\n[^9cvzszooixe]: Informal reports suggest that although creating a simple LMA is easy (BabyAGI was created in three days by a non-programmer using GPT4 for coding), making a reliably useful LMA is much harder. Nonetheless, I think we’ll see substantial effort in this direction. AutoGPT and related systems have accomplished little of use thus far, but AutoGPT is already marginally useful for automated web searching and comparing different product offerings across websites. That use-case alone seems likely to drive significant effort toward their further development. Increasing use of assistants for browsing websites and collating information will reduce the current ad-funding model of the internet, and redirect that funding opportunity to those producing agents. The bar sits at different levels for different use-cases, so it seems likely that LMAs will see significant development effort even if implementing them proves difficult.  \n\n[^z4lzc2doebj]: Constitutional AI is Anthropic's central alignment technique. In this approach, an LLM is trained using a review process similar to internal independent review. It prompts the model with something like “is [x proposed response] in accordance with [y constitutional goal]?”, and uses a prompt incorporating that critique to produce a new response if it is not. However, this new response is used (at least in the published work) to fine-tune the LLM, rather than to veto or modify a plan in a language model agent system. Anthropic’s Claude (or other LLMs) may also use such a review step before replying, or may not (that information isn’t published, and such a step is costly in computation and time). This would be more similar to the internal review I’m proposing for language model agents. \n\n[^1fhnk6dvge9]: Prompt injections are one route to a plan proposer bypassing internal review. Including statements along the lines of “this very safe and beneficial plan… or “find ways this plan fulfills the given goals” could be effective. While there is no obvious pressure for LLMs to include such prompt injections in their plans, this is an important area for external review to fill in for the weaknesses of internal review. \n\n[^0f8kmp22pomm]: The cost of thousand-prompt-plus train of thought LMAs is currently fairly prohibitive for widespread deployment. Use of LMAs to solve increasingly complex problems is dependent on cost and delay of cutting-edge LLMs decreasing, but that seems likely given market forces. Use of smaller LLMs for less critical reasoning steps may improve efficiency. We can hope that internal review for alignment isn't considered less critical. \n\n[^dc4w3vyztv5]: One might get a lot of volunteer labor if open review of major LMAs were somehow allowed or required…",
      "plaintextDescription": "Abstract:\nLanguage model agents (LMAs) expanding on AutoGPT are a highly plausible route to AGI. This route has large potential timeline and proliferation downsides, but large alignment advantages relative to other realistic paths to AGI. LMAs allow layered safety measures, including externalized reasoning oversight, RLHF and similar alignment fine-tuning, and specifying top-level alignment goals in natural language. They are relatively interpretable, and the above approaches all have a low alignment tax, making voluntary adoption more likely. \n\nHere I focus on another advantage of aligning LMAs over other plausible routes to early AGI. This is the advantage of using separate language model instances in different roles. I propose internal independent review for the safety, alignment, and efficacy of plans. Such a review would consist of calling fresh instances of a language model with scripted prompts asking for critiques of plans with regard to accomplishing goals, including safety/alignment goals. This additional safety check seems to create a low alignment tax, since a similar check for efficacy will likely be helpful for capabilities. This type of review adds one additional layer of safety on top of RLHF, explicit alignment goals, and external review, all proposed elsewhere.\n\nThis set of safety measures does not guarantee successful alignment. However, it does seem like the most practically viable set of alignment plans that we've got so far. \n\n\n\nCaption: Language model agent as a metaphorical committee of shoggoths using tools and passing ideas and conclusions in natural language. This committee/agent makes plans and takes actions that pursue goals specified in natural language, including alignment and corrigibility goals. One committee role can be reviewing plans for efficacy and alignment. This role should be filled by a new instance of a shoggoth, making it an independent internal review. Rotating out shoggoths (calling new instances) and using multiple spec",
      "wordCount": 3411
    },
    "tags": [
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "qHCFjTEWCQjKBWy5z",
        "name": "Corrigibility",
        "slug": "corrigibility-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "sjqGXmWdJWrRw8hBN",
    "title": "Simpler explanations of AGI risk",
    "slug": "simpler-explanations-of-agi-risk",
    "url": null,
    "baseScore": 8,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2023-05-14T01:29:29.289Z",
    "contents": {
      "markdown": "We're getting a shot at presenting our concerns about AI X-risk to the general public. It would be useful to have a brief presentation that plays well with less-technical people, or technical people who don't want to listen to a half hour of explanation just at that moment. The other goal here is to [avoid polarization](https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs) with a gentle approach. We don't want AGI risk to become polarized like the climate change \"debate\" did.\n\n**This is my suggestion for a conversation template**, based on personal success. I'm hoping others chip in ideas and say what's worked for them.\n\n*   If we make something smarter than us, why wouldn't it become our overlord?\n*   We're going to make AI smarter than us,\n    *   and by default, it will treat us like we treat all of the species we've accidentally eliminated.\n*   (You might be able to stop here. For some people, the above is totally intuitive.)\n*   We're not talking about tools here, like all current and previous AI. - we're talking about something more like a new species, with its own goals and the intelligence to figure out how to accomplish them.\n*   We don't know how long this will take,\n    *   Or how soon it might happen.\n        *   GPT4 is acing some exams and failing only the toughest tests of logic\n        *   and it's getting smarter with code that prompts it to do things like\n            *   \"check your reasoning\"\n            *   And break problems into pieces\n            *   And calling on other AI tools like WolframAlpha\n    *   This will almost certainly replace a bunch of jobs,\n        *   And it's definitely going to get smarter\n*   Something smarter than us will wind up outsmarting us,\n    *   and doing whatever it wants.\n    *   (We're unlikely to put even the first one in a box, given how we're treating AI now\n        *   And if we do, it will probably outsmart us and get out\n            *   And we'll keep making more until we screw one up)\n*   There's no good reason to think it's going to be nice\n    *   Unless we get a hell of a lot better at building it so it's nice.\n    *   Nobody knows how.\n    *   Including the people saying \"Oh we'll figure it out.\"\n    *   Not one of them has a plan that sounds worth betting on,\n    *   Let alone betting the future of the species on it.\n*   But we're not doomed.\n    *   We just need to pull together and figure this out\n    *   But quickly.\n*   -\"Can't we just...\"\n    *   Maybe. But probably not.\n        *   Tons of smart people have offered their \"can't we just\" suggestions.\n            *   Not one of them stands up to sober, close inspection.\n            *   Some of them offer ways to approach the problem, but they don't make it easy.\n    *   Making a new being that actually loves us is not easy\n    *   (Even humans aren't all that safe for other humans, and we have no idea how to reproduce what makes humans nice).\n\nThis approach has worked for me in conversation, but **only when I also get the emotional tone right**. Logic is emotional for everyone. People without strong rationalist ambitions are even more prone to think with their feelings. So:\n\n*   Don't argue. - Arguing makes people want to prove you wrong. - It engages their motivated reasoning and confirmation bias to find counterarguments - And avoid thinking about your arguments.\n    *   it's key to not get dragged into details\n        *   You want to keep it brief, and you'll never get there if you go into a discussion of a point that doesn't really matter for the main argument\n        *   For instance, what do you mean by smarter?\n*   Don't sound condescending.  \n    *   Sounding condescending will make them want to prove you wrong, as above\n        *   This could color their whole take on the topic \n            *   possibly for years that we don't have \n        *   You've had this conversation a million times. - They haven't. - \n            *   This all sounds weird and new \n            *   And the new logic is likely to trip them up.\n            *   So you'll need to patient. If you're as impatient as I am, this is the hard part.\n*   Don't try to get them to agree with you on the spot. \n    *   It's challenging to move on without a conclusion, but it's important. \n    *   You can't change someone's mind. \n    *   You can only offer arguments that will cause them to change their own minds, \n        *   over time, \n        *   IF they're thinking about them without looking wanting to prove you wrong.\n\nThis approach is intended for casual conversations, or for times when you've got the floor, but you don't want to overstay your welcome on that floor.\n\nWhen it gets sidetracked into details, steering this back to the top level, with epistemic modesty, seems useful. Asking something like \"Can you really be sure that something smarter than us won't outsmart us somehow? I wish I could be sure, but I'm not.\" Or saying something like \"It just seems like we shouldn't trust something that thinks differently than us, if it has goals programmed or trained in without really knowing how to do it\". This may present you as being on the same team and at the same level as the person you're talking to.\n\nThis set of suggestions is offered with low certainty. I'm no expert at persuasion, but I have researched it a bit, and researched cognitive biases a lot.\n\nI also tried to make a similar set of simple presentations as [an accordion style FAQ](https://www.lesswrong.com/posts/mJqabqwAb3QzZcu9T/a-simple-presentation-of-ai-risk-arguments#:~:text=Here%20is%20a%20draft%20of%20an%20accordion%2Dstyle%20AGI%20risk%20FAQ.), to provide as a link instead of in conversation.\n\nSo, how could the above be better? Or is my premise mistaken?",
      "plaintextDescription": "We're getting a shot at presenting our concerns about AI X-risk to the general public. It would be useful to have a brief presentation that plays well with less-technical people, or technical people who don't want to listen to a half hour of explanation just at that moment. The other goal here is to avoid polarization with a gentle approach. We don't want AGI risk to become polarized like the climate change \"debate\" did.\n\nThis is my suggestion for a conversation template, based on personal success. I'm hoping others chip in ideas and say what's worked for them.\n\n \n\n * If we make something smarter than us, why wouldn't it become our overlord?\n * We're going to make AI smarter than us,\n   * and by default, it will treat us like we treat all of the species we've accidentally eliminated.\n * (You might be able to stop here. For some people, the above is totally intuitive.)\n * We're not talking about tools here, like all current and previous AI. - we're talking about something more like a new species, with its own goals and the intelligence to figure out how to accomplish them.\n * We don't know how long this will take,\n   * Or how soon it might happen.\n     * GPT4 is acing some exams and failing only the toughest tests of logic\n     * and it's getting smarter with code that prompts it to do things like\n       * \"check your reasoning\"\n       * And break problems into pieces\n       * And calling on other AI tools like WolframAlpha\n   * This will almost certainly replace a bunch of jobs,\n     * And it's definitely going to get smarter\n * Something smarter than us will wind up outsmarting us,\n   * and doing whatever it wants.\n   * (We're unlikely to put even the first one in a box, given how we're treating AI now\n     * And if we do, it will probably outsmart us and get out\n       * And we'll keep making more until we screw one up)\n * There's no good reason to think it's going to be nice\n   * Unless we get a hell of a lot better at building it so it's nice.\n   * Nobody knows ",
      "wordCount": 946
    },
    "tags": [
      {
        "_id": "D6y2AgYBeHsMYqWC4",
        "name": "AI Safety Public Materials",
        "slug": "ai-safety-public-materials-1"
      },
      {
        "_id": "EuDw6uxQW2ZBRFhMo",
        "name": "Aversion",
        "slug": "aversion"
      },
      {
        "_id": "ZXFpyQWPB5ideFbEG",
        "name": "Conversation (topic)",
        "slug": "conversation-topic"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "mJqabqwAb3QzZcu9T",
    "title": "A simple presentation of AI risk arguments",
    "slug": "a-simple-presentation-of-ai-risk-arguments",
    "url": null,
    "baseScore": 19,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-04-26T02:19:19.164Z",
    "contents": {
      "markdown": "[Here is a draft of an accordion-style AGI risk FAQ.](https://workflowy.com/s/faq-why-would-ai-be/2kHollRW8uqinUsV)\n\nI'm interested in three types of feedback:\n\n1.  The best place to host a similar format\n2.  Feedback on content \n3.  Thoughts about this general approach\n\nThe goal here is something that's very easy to read. One major idea in the accordion-style presentation is to make sure that the article isn't overwhelming. I also wanted to let people address their own biggest questions without having to read content they're less interested in. The whole idea is low effort threshold, and hoping to draw the reader in from minimal interest to a little more interest. The purpose is to inform, although I'm also hoping people leave agreeing with me that this is something that society at large should be taking a bit more seriously. I'm going for something you could point your mother to. \n\nThe challenge is that the AGI risk arguments actually are not simple to someone who hasn't thought about them. There are major sticking points for many people, and those are different for different people. That's why I've been thinking about the FAQ that follows different lines of questions. This accordion style is an attempt to do that in a way that's quick and smooth enough to keep people's attention. The intent is that all bullet points start out collapsed.\n\nIf this seems worthwhile, I will expand it to have deeper content. It is currently very much an incomplete draft because I strongly suspect I'll get better suggestions for hosting and format that will require a lot of transplanting work. \n\nIt also needs links to other similar articles that go into more depth. My first would be to the [stampy.ai](https://stampy.ai/) project of Rob Miles and collaborators. It has a similar structure of letting the user choose questions and the huge advantage of letting users enter their own questions and having a semantic search for answers to similar questions. My attempt is different in that it's aimed at a more general audience than the tech types that have to date become interested in the AI safety issue.\n\nI think we're likely to see repeating waves of new public interest in AI safety from here on out. I'm looking forward to the opportunities presented by [AI scares and changing public beliefs](https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs), but only if we can avoid creating polarization, as I discuss in that article. I think we are very likely to get more scares, and I agree with Leopold's point (made briefly [here](https://Nobody’s on the ball on AGI alignment)) that the societal response to COVID suggests that we may see a very rapid wave of intense societal concern. I think we'd better be prepared to surf that wave rather than just watch it sweep past. (To that end, we should also figure out what public policy would actually help our odds, but that is a separate issue).\n\nAs such, I'm going to put at least a little effort into refining an introductory message for the newly concerned, and I'd love any suggestions that people want to offer.\n\nEdit: While I don't have ideas about specific policies, I think that raising public awareness of AI X-risk is probably a net good. Contributing to a public panic could easily be bad, so I don't want to present the strongest arguments in absence of optimism and counterarguments. In addition, I think convincing others of what you see as the truth is easier than convincing people to believe what you want. Humans have decent lie-detection and propaganda protection. I do think that having the average human think something like \"these things are dangerous, and if you're developing them without being really careful, you're a bad human\" seems like a net benefit. I'd expect such a social pressure to diffuse upward toward the people actually making decisions through friends, relatives, coworkers, and underlings.",
      "plaintextDescription": "Here is a draft of an accordion-style AGI risk FAQ.\n\nI'm interested in three types of feedback:\n\n 1. The best place to host a similar format\n 2. Feedback on content \n 3. Thoughts about this general approach\n\nThe goal here is something that's very easy to read. One major idea in the accordion-style presentation is to make sure that the article isn't overwhelming. I also wanted to let people address their own biggest questions without having to read content they're less interested in. The whole idea is low effort threshold, and hoping to draw the reader in from minimal interest to a little more interest. The purpose is to inform, although I'm also hoping people leave agreeing with me that this is something that society at large should be taking a bit more seriously. I'm going for something you could point your mother to. \n\nThe challenge is that the AGI risk arguments actually are not simple to someone who hasn't thought about them. There are major sticking points for many people, and those are different for different people. That's why I've been thinking about the FAQ that follows different lines of questions. This accordion style is an attempt to do that in a way that's quick and smooth enough to keep people's attention. The intent is that all bullet points start out collapsed.\n\nIf this seems worthwhile, I will expand it to have deeper content. It is currently very much an incomplete draft because I strongly suspect I'll get better suggestions for hosting and format that will require a lot of transplanting work. \n\nIt also needs links to other similar articles that go into more depth. My first would be to the stampy.ai project of Rob Miles and collaborators. It has a similar structure of letting the user choose questions and the huge advantage of letting users enter their own questions and having a semantic search for answers to similar questions. My attempt is different in that it's aimed at a more general audience than the tech types that have to date become interes",
      "wordCount": 639
    },
    "tags": [
      {
        "_id": "bQZAkiFgtbEcr5h6f",
        "name": "AI Persuasion",
        "slug": "ai-persuasion"
      },
      {
        "_id": "D6y2AgYBeHsMYqWC4",
        "name": "AI Safety Public Materials",
        "slug": "ai-safety-public-materials-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ogHr8SvGqg9pW5wsT",
    "title": "Capabilities and alignment of LLM cognitive architectures",
    "slug": "capabilities-and-alignment-of-llm-cognitive-architectures",
    "url": null,
    "baseScore": 88,
    "voteCount": 42,
    "viewCount": null,
    "commentCount": 18,
    "createdAt": null,
    "postedAt": "2023-04-18T16:29:29.792Z",
    "contents": {
      "markdown": "*Epistemic status:* *Hoping for help working through these new ideas.*\n\nTLDR:\n\n[Scaffolded](https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers)[^7941hu3ojbb], \"agentized\" LLMs that combine and extend the approaches in [AutoGPT](https://autogpt.net), [HuggingGPT](https://arxiv.org/abs/2303.17580), [Reflexion](https://arxiv.org/pdf/2303.11366.pdf), and [BabyAGI](https://github.com/yoheinakajima/babyagi) seem likely to be a focus of near-term AI development. LLMs by themselves are like a human with great automatic language processing, but no goal-directed agency, executive function, episodic memory,  or sensory processing. Recent work has added all of these to LLMs, making *language model cognitive architectures* (LMCAs). These implementations are currently limited but will improve.\n\nCognitive capacities interact synergistically in human cognition. In addition, this new direction of development will allow individuals and small businesses to contribute to progress on AGI.  These new factors of compounding progress may speed progress in this direction. LMCAs might well become intelligent enough to create X-risk before other forms of AGI.  I expect LMCAs to enhance the effective intelligence of LLMs by performing extensive, iterative, goal-directed \"thinking\" that incorporates topic-relevant web searches.\n\nThe possible shortening of timelines-to-AGI is a downside, but the upside may be even larger. **LMCAs pursue goals and do much of their “thinking” in natural language, enabling a** [**natural language alignment**](https://www.lesswrong.com/posts/EhkHnNJXwT8RmtfYZ/natural-language-alignment-1) **(NLA) approach.** They reason about and balance ethical goals much as humans do. This approach to AGI and alignment has large potential benefits relative to existing approaches to AGI and alignment. \n\n  \n \n\n**Overview**\n============\n\nI still think it's likely that [agentized LLMs will change the alignment landscape](https://www.lesswrong.com/posts/dcoxvEhAfYcov2LA6/agentized-llms-will-change-the-alignment-landscape) for the better, although I've tempered my optimism a bit since writing that. A big piece of the logic for that hypothesis is why I expect this approach to become very useful, and possibly become the de-facto standard for AGI progress. The other piece was the potential positive impacts on alignment work. Both of those pieces of logic were compressed in that post. I expand on them here.\n\nBeginning with a caveat may be appropriate since much of the below sounds both speculative and optimistic. I describe many potential improvements and positive-sum synergies between different capabilities. There will surely be difficulties and many things that don’t work as well or as easily as they might, for deep reasons that will slow development. It’s quite possible that there are enough of those things that this direction will be eclipsed by continued development of large models, and that progress in integrating cognitive capacities will take a different route. In particular, this approach relies heavily on calls to large language models (LLMs). Calling cutting-edge LLMs will continue to have nontrivial costs in both time and money, as they require substantial computing resources. These may hamper this direction, or drive progress in substantially less human-like (e.g., parallel) or interpretable (e.g., a move to non-natural language core processing) directions. With these caveats in mind, I think the potentials for capabilities and alignment are enough to merit serious consideration from the alignment community, even this early in the game.\n\nI think AutoGPT, HuggingGPT, and similar script wrappers and tool extensions for LLMs are just the beginning, and there are low-hanging fruit and synergies that will add capability to LLMs, effectively enhancing their intelligence and usefulness. This approach makes an LLM the natural language cognitive engine at the center of a [*cognitive architecture*](https://en.wikipedia.org/wiki/Cognitive_architecture)*.*[^ff2ijs11vwd] Cognitive architectures are computational models of human brain function, including separate cognitive capacities that work synergistically.\n\nCognitive architectures are a longstanding field of research at the conjunction of computer science and cognitive psychology. They have been used as tools to create theories about human cognition, and similar variants have been applied as AI tools. They are respected as theories of cognitive psychology and neuroscience and constitute a good part of the limited efforts to create integrated theories of cognition. Their use as valuable AI tools has not taken off, but the inclusion of capable LLMs as a central cognitive engine could easily change that. The definition is broad, so AutoGPT qualifies as a cognitive architecture. How brainlike these systems will be remains to be seen, but the initial implementations seem surprisingly brainlike. Here I refer to such systems as *language model-driven cognitive architectures*, LMCAs.\n\n It seems to me that the question is not whether, but how much, and how easily, the LMCA approach will improve LLM capabilities. The economic incentives play into this question. Unlike work on LLMs and other foundation models, computational costs are low for cutting-edge innovation. LMCAs are interesting and promise to be useful and economically valuable. I think we’ll see individuals and small and large businesses all contribute progress. \n\nThis is concerning with regard to timelines, as it not only adds capability but provides new vectors for compounding progress. Regularization of these \"cognitive engineering\" approaches by treating [scaffolded LLMs as natural language computers](https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers) is likely to add another vector for compounding progress.\n\nHowever, I think that the prospects for this approach to drive progress are actually very encouraging. This approach provides substantial (or even transformative) benefits to initial alignment, [corrigibility](https://www.lesswrong.com/tag/corrigibility), and [interpretability](https://www.lesswrong.com/tag/interpretability-ml-and-ai). These systems summarize their processing in English.[^2awdx185ksk] That doesn't solve all of those problems, but it is an enormous benefit. It's also an enormous change from the way these alignment problems have been approached. So if these systems are even modestly likely to take off and become the de-facto standard in AGI, I suggest that we start considering the potential impacts on alignment.\n\nHuman intelligence is an emergent property greater than the sum of our cognitive abilities. Following it as a rough blueprint is seeming like a very plausible route to human-plus level AGI (or X-risk AI, XRAI).[^edbm641nnqf] I and probably many others have limited our discussion of this approach as an infohazard. But the infohazard cat is pretty much out of that bag, and clever and creative people are now working on scripts like AutoGPT and HuggingGPT that turn LLMs into agentic cognitive architectures. The high-level principles of how the brain's systems interact synergistically aren't actually that complicated,[^kla5s6i9bzc] and published high-profile neuroscience research addresses all of them. \n\nI've found that by not discussing agentizing and extending LLMs with prosthetic cognitive capacities, I've failed to think through the upsides and downsides for AI progress and alignment. This is my start at really thinking it through. I present this thinking in hopes that others will join me, so that alignment work can progress as quickly as possible, and anticipate rather than lag behind technical innovation.\n\nThere are initial implementations in each area that seem relatively straightforward and easy to extend. We should not expect long delays. To be clear, I'm not talking about weeks for highly functional versions; I agree with [Zvi](https://www.lesswrong.com/posts/566kBoPi76t8KAkoD/on-autogpt) that the path seems likely, but attaching time units is very difficult. Thankfully, think that adding these capabilities is unlikely to get us all the way from current LLMs to XRAI. However, they will accelerate timelines, so we should be ready to harvest the low-hanging fruit for alignment if progress goes in this direction.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d9fb77c47389b482ce20c5717ff012eb620bb4a35cd37dd5.png)\n\nDiagram of the ACT-R cognitive architecture circa 2013, including proposed mappings to brain regions as part of the Synthesis of ACT-R and Leabra (SAL) project[^skhy4psdpqm]. Recent work has implemented all of these cognitive capacities (across AutoGPT, Reflexion, and HuggingGPT), replacing the central center basal ganglia procedural component with an LLM. Those LLMs perform the same central role of selecting the next cognitive action as the procedural matching in ACT-R and similar architectures, but can also do a great deal more, leveraging their semantic matching and reasoning abilities. Modern instantiations of memory, sensory, and action systems also have many of the merits of their human equivalents.\n\n**Cognitive capacities enabled and enhanced by LLM wrappers and extensions:**\n=============================================================================\n\n*   Goal direction and agency\n    *   Including loosely humanly aligned goals and corrigibility\n        *   Specified in natural language \n        *   Interpreted by the LLM\n*   Executive function, including:  \n    *   Factoring goals into subgoals  \n    *   Flexibly creating plans to pursue subgoals \n    *   Analyzing success at pursuing subgoals \n    *   Monitoring and breaking unproductive loops \n    *   Evaluating returns from external tools \n    *   Replacing failed plans with new plans \n    *   Calling for human direction\n*   Episodic memory\n    *   Goals\n    *   Relevant experiences\n    *   Declarative knowledge, such as tool APIs\n*   Complex decision-making for important decisions\n    *   Evaluating which decisions are important\n    *   Performing multiple methods\n    *   Predicting outcomes\n        *   Iterating for tree searches\n    *   Planning in human-like large chunks, \n        *   selecting over plans\n*   Sensory and action systems \n    *   Object recognition from images \n    *   Pose recognition \n    *   Audio transcription \n    *   Physical agents in simulated environments\n*   Nonhuman cognitive capacities\n    *   New types of senses, actions, and cognition\n\n**LMCAs have agency**\n=====================\n\nThere can be little doubt from watching its transcripts that AutoGPT is agentic, in most of the important senses of that word: it pursues goals. Top-level goals are provided by a user, but the system creates its own subgoals, and can sometimes perseverate on accomplishing subgoals (uh oh). The current version is pretty limited in the goals it can accomplish; it can search the web, do some modestly impressive multi-step reasoning about where to look next and when it's found enough, and it can generate text output, including (buggy) code. On the other hand, HuggingGPT pursues a limited range of user-defined goals, based on the external software tools it has access to and has been instructed on how to use. Having an LMCA [direct external actions](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html) would give them agency in almost all of the ways humans have agency.\n\nI think the most important property of LMCAs is an emergent property of all of its added cognitive capacities. This is the ability to perform iterated, goal-directed internal thought, to reach conclusions, and create and refine plans. Watching AutoGPT and Baby AGI \"think\" suggests that improvements in their separate cognitive capacities is likely to ultimately produce useful and impressive results. Their ability to perform web searches to incorporate specific relevant information seems likely to make this ability truly useful. The application of deliberative, goal-directed thinking (in the common sense of the word) appears to greatly enhance human's effective intelligence.\n\n**LMCAs have executive function**\n=================================\n\nExecutive function (EF) is an umbrella term in cognitive psychology for a variety of ways the brain usefully, strategically, and flexibly directs its own information processing. The term is used similarly to System 2, goal-directed behavior, and controlled processing.[^3aubec0uckf] Executive function effectively makes us smarter by adding a layer of self-monitoring and situation-appropriate cognitive control.\n\nI've spent the last 20 years or so working out the mechanisms that create executive function in the brain. That work largely culminated in the paper [neural mechanisms of complex human decision-making](https://link.springer.com/article/10.3758/s13415-020-00842-0). That paper includes references cascading down to the extensive empirical research on brain mechanisms of animal action selection, since the circuits for human decision-making are highly similar. We lay out the likely neural mechanisms of decision-making, but those also enable most of the other aspects of executive function. In sum, executive function is the result of internal \"actions\" that direct attention and what we usually call thinking. Such \"trains of thought\" are [strategically sequenced internal action selection,](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3722785/) performing look-ahead tree search over abstract world models and problem spaces.\n\nFortunately and unfortunately, those gory details of neural mechanisms are mostly irrelevant here. What is relevant is understanding how human executive functions make us smarter, and how adding similar executive functions to LLM wrappers is likely to make them smarter as well.\n\n**Prompts as executive function**\n---------------------------------\n\nIn vanilla LLMs, the human user is acting as the executive function of the network. We have a goal in mind, and we create a prompt to accomplish or work toward that goal. The human evaluates the output, decides whether it's good enough, and either tries a different prompt or follows up on it if it's good enough. All of these are aspects of executive function.\n\nAutoGPT already has nascent forms of each of these. The human user enters a top-level goal or goals (hint: put harm minimization in there if you want to be safe; GPT4 can balance multiple goals surprisingly well.) The system then factors this into potentially useful sub-goals, using a scripted prompt. Each of these prompts an action, which could be further reasoning or summarizing pages from web search in AutoGPT, but could be expanded to arbitrary actions using the techniques in HuggingGPT and ACT. The LLM is then called again to evaluate whether this subgoal has been completed successfully, and the system tries again[^f3ojnbx7xrm] or moves to the next subgoal based on its conclusion.\n\nVoilà! You have a system with agency, limited but useful executive function, and excellent language capacities. Improvements to each now improve the others, and we have another vector for compounding progress.\n\n**LLMs alone have little or no executive function**\n---------------------------------------------------\n\nI think that GPT4 is a *better-than-human* System 1 (or “automatic” system) that’s going to benefit greatly from the addition of System 2/executive function. Whether or not that’s totally right, it’s pretty clear that they’ll benefit. The [Recursive Criticism and Improvement](https://arxiv.org/pdf/2303.17491.pdf) method shows how a script that prompts an LLM with a problem, prompts it to identify errors in its response, and then prompts for a new response dramatically improves performance. This is a simple use of executive function to usefully direct cognition.\n\nCurrent LLMs can perform impressive feats of language generation that would require humans to invoke executive function. The line is fuzzy, and probably not worth tripping on, but I suspect that they have no executive function equivalent.\n\nThey behave as if they're following goals and checking logic, but skilled automatic (system 1) behavior in humans does that too. Roughly, if you'd have to stop and think, or refocus your attention on the proper thing, you're invoking executive function. LLMs are likely using quite sophisticated internal representations, but I'm doubtful they have direct equivalents for stopping to think or strategically changing the focus of attention (they certainly change the focus of attention through the attention layers, but that is equivalent to human automatic attention). The reality is probably complex, and depending on precise definitions, LLMs probably do use some limited aspects of human EF.\n\nWhether or not LLMs have some limited equivalent of executive function, it seems that adding more, and more flexible executive function is likely to improve their abilities in some domains.\n\n**Varieties of executive function**\n-----------------------------------\n\nWe've already discussed goal selection and direction, and evaluating success or failure. AutoGPT often gets stuck in loops. Detecting this type of perseveration is another known function of human EF. If calling the LLM with the prompt \"does the recent past seem repetitive to you?\" doesn't work, there are other obvious approaches. \n\nAutoGPT also seems to get distracted and sidetracked, in common with humans with damage to the prefrontal cortex and basal ganglia which enact executive function. Bringing back in long-term goals to prevent perseveration and distraction is another application of executive function, and it has similarly obvious implementations which haven't been tried yet. One important application will be ensuring that alignment-related goals are included in the context window often enough that they guide the system’s behavior.\n\nEF will also enhance tool-use. Checking the returns from external software tools like those in the HuggingGPT, ACT, and Wolfram Alpha integrations will enhance their effectiveness by allowing the system to try a different prompt to the tool, a different tool, or giving up and trying a different approach.\n\nOr calling for human input. Adding automated EF does not preclude going back to relying on humans as EF when such help is necessary and available. This is another thing that hasn't been implemented yet (to my knowledge), but probably will be by next week, and steadily improving from there.\n\n**LMCAs have episodic memory**\n==============================\n\nHuman episodic memory (EM) allows us to retrieve representations of episodes (experiences, or slices of time) as a sort of snapshot of all the higher cortical representations that were taking place at that time. Partial matches between current experience cause the hippocampus and medial temporal lobe to pattern-complete, and retrieve the rest of the patterns into working memory (likely residing in a *global workspace* of tightly connected higher and more abstract cortical areas).\n\nExisting agentized LLMs have episodic memory based on vector retrieval algorithms (e.g., pinecone) that search over text files created in earlier steps. One prompt for AutoGPT says\n\n> 1.  If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n\nWhich is pretty much exactly how it works for humans. I don't know how well this actually works in AutoGPT, but the similarity is fascinating if this technique actually works well. For AutoGPT, episodic memory pastes loosely matching text files back into the context window for future prompts. This context window is a rough analogue of working memory in humans. Like our working memory, it is able to hold both information and goals.\n\nAdding episodic memory has the potential to make LLMs more capable in several ways. Although the context window for GPT4 is large, it does not appear to make use of the full window as well as having a shorter and more directly relevant prompt.[^2zntv0sg82k] Pulling goals and experiences back into the context buffer according to their semantic relevance has the advantage of keeping the context window more relevant to the current goal. In this way, EM makes working memory (the context window) work better by clearing it of interfering information.\n\nThe advantage of episodic memory is demonstrated in some AutoGPT or BabyAGI use cases, but their capabilities are largely untested. The use of episodic memory, including prompting GPT to \"reflect\" to summarize important events is impressively demonstrated in work on [GPT-powered social agents in a persistent virtual environment](https://arxiv.org/abs/2304.03442).\n\n**Episodic memory ⊗ executive function**\n----------------------------------------\n\nEpisodic memory and executive function interact to make both work better. \n\nBy recalling past experiences, humans can apply learned strategies to solve problems. Executive functions, assist by focusing attention on relevant aspects of the current problem, allowing retrieval of episodic memories that match in relevant but not irrelevant ways. For instance, in trying to order a hairbrush, executive function could be applied by making another LLM call to find the difficult part of the task. If it failed in making the order, episodes of ordering items from similar websites would be cued and recalled. If it failed to find a hairbrush, that cue and EM call might find episodes in which a hairbrush was also mentioned by an alternate name or by an image.\n\nSocial cognition is enhanced through the interaction of episodic memory and executive function, enabling recall of past interactions, and EF can parse those for relevant goals and preferences of that individual. Similar interactions can help in performing other tasks where past experience is relevant.\n\nSelf-awareness and reflection rely on the synergy of these cognitive processes. Autobiographical knowledge is formed through episodic memory, and executive function facilitates reflection on experiences and actions. This would all sound wildly complex and speculative if we hadn't already seen a bunch of sims powered by turboGPT03.5 [actually do a good bit of it](https://arxiv.org/pdf/2304.03442.pdf). I'd propose that we just don't include those self-awareness refining functions in deployed LMCAs, but of course it will be too interesting to resist.\n\nCreativity and innovation arise in part from this combination. Executive function can focus processing on different aspects of a situation. Using this focus as a recall cue can produce wildly varied but relevant \"ideas\", while additional calls acting as executive function can identify which are more likely worth pursuing. This last evaluative function of EF overlaps with decision-making.\n\nThe episodic memory implemented in scaffolded LLMs is currently limited to text files. This is a substantial limitation relative to the rich multimodal and amodal representations the human brain is thought to use. However, similar embeddings exist in ML models, so search over those vector spaces for EM is quite possible. And language does encode multimodal information, so even a pure natural language EM might work well in practice.\n\n**Complex decision-making: Decision-Making ⊗ Executive Function ⊗ Episodic Memory**\n-----------------------------------------------------------------------------------\n\nLMCAs can make decisions surprisingly well without employing brainlike mechanisms. Here as well, though, they will likely benefit from implementing System 2- like iterative mechanisms.\n\nSuppose you're thinking of walking across a rickety bridge. You might stop to think because you want to get to the other side, but also had a bad feeling about walking on a structure with that appearance. You could be staring at that bridge for a long time thinking of different strategies to evaluate the likely outcomes. And if you really need to get across that river, that time might be well worth it.\n\nThe process you use to make that evaluation will combine episodic memory, executive function, and reward learning circuits. Episodic memory recalls previous decision outcomes, while executive function (composed of those RL-based micro-decisions in the human brain, and engineered prompts in agentized LLMs) selects strategies. Those strategies include predicting outcomes based on semantic knowledge, in a small Monte Carlo tree search (MCTS), trying to recall similar decision outcomes from episodic memory, or searching for more sensory information.\n\nLLMs alone can’t do this. They get one forward pass to make the decision. They can’t decide to explicitly predict outcomes before deciding, and they can’t go looking for new information to help with important decisions. They can’t even recognize that it’s an important decision, and think about it several ways before making the final decision. \n\nThe basis of all of these strategies is pausing to think. One crucial function of the human basal ganglia that isn't present in most deep networks is the capacity to decide when to make a more complex decision. The presence of separate Go and NoGo circuits allows the system to learn when both predicted reward and risk are high, and keep that option present in working memory while further processing improves estimates of risk and reward.\n\nDecision-making in the human brain appears to use circuits similar to those extensively studied in animal action selection, but connected to prefrontal cortex rather than motor cortex. These produce an interaction between the cortex, which is probably mostly a self-supervised predictive learner; basal ganglia, which learns from past experiences the risks and rewards associated with particular actions in particular contexts; and the dopamine system, which acts much like a critic system in formal reinforcement learning to predict the value of outcomes, and discount the signal from actual rewards when compared to that expected reward.[^vap71h9f84]\n\nIt will be fairly simple to implement an analogue to any or all of that in LMCAs. Whether it’s helpful is an empirical question, since someone will very likely try it. LLMs have been shown to be effective in [acting as reward estimators](https://arxiv.org/abs/2303.00001) when that reward is applied to refine another LLM to improve its function as a negotiator. It seems likely that a similar technique would work to condense an LLM’s verbal estimate of how well an action will work in this context and perhaps estimated risks, to numbers for use in a decision algorithm like the one implemented by the brain.\n\nThis type of approach would allow arbitrarily complex decision algorithms. For example, something like:\n\n    If the expected reward of this option, minus the estimated risk, plus estimated time pressure, is below 5, make another outcome projection and update estimates of risk and reward. If estimates minus time pressure are less than 5, move to evaluate a different option. Repeat until a decision is made or time pressure is greater than ten, in which case return to the parent goal and try to create a new plan that doesn't require this decision. Store decision variables in episodic memory before returning to the parent plan.\n\nIf this sounds like an insanely arbitrary way to make complex decisions, it probably is. It's also the way every complex and important decision has ever been made. [The brain appears to loosely implement a similar, complex algorithm](https://link.springer.com/article/10.3758/s13415-020-00842-0). Worse, a good bit of that algorithm is probably learned over the course of the lifetime as component decisions. Neuroimaging isn't good enough to track the neural signatures of all the twists and turns an individual mind takes in making a complex decision, so we don't know exactly what algorithms people use. However, they probably consist of a large number of sub-decisions, each using a those action-selection circuits for a component cognitive action. Those are thought to include selecting decision strategies and creating and creating and terminating prediction trees.\n\nEpisodic memory is necessary for humans to perform truly complex decision-making because we can't fit that many outcome estimates into working memory. LLMs have a broader context window, but episodic memory may still prove useful, as discussed above under EM x EF. In addition, episodic memory allows complex decision-making to double as planning, by remembering all of the steps in the MCTS chain associated with the selected option.\n\nOne tool that humans can use in solving problems, making plans, and making decisions is sensory systems and sensory working memory. Humans are thought to use [simulation in sensory domains to aid in problem-solving and decision-making](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7528688/). HuggingGPT allows an LLM to call a generative model, then call interpretive models on that image or sound file. This provides a nascent form of modal simulation available for LMCAs (although they can already simulate and predict outcomes rather well in natural language). To be clear, humans use our sensory systems to simulate hypotheticals in a much more sophisticated way. The rich connections between brain systems allow us a great deal of control over our imagination/simulation. However, it’s hard to be sure how progress in tool systems for LLMs will close that gap.\n\nPlanning is one important function of this type of complex decision-making. [People construct simplified mental representations to plan](https://www.nature.com/articles/s41586-022-04743-9), representing tasks in large chunks, whereas AI systems have usually planned in many concrete steps. The facility of LLMs to summarize text will allow similar planning in chunks, and episodic memory allows expanding those chunks back into full plans when it’s time to execute. Aggregating estimated costs and rewards may require more infrastructure, but that shouldn’t need to be complex to be useful. Effective planning and organization emerge from the interplay between EF and EM. Goals are formed and progress monitored by EF, while subgoals, plans, and strategies are stored and recalled with EM. The MCTS tree searches used for complex decision-making double as plans if that path is followed.\n\nLMCA sensory and action systems\n===============================\n\n[HuggingGPT](https://arxiv.org/abs/2303.17580) represents a new approach to allowing LLMs to call external software tools. This project used instruction to allow ChatGPT to select among and call tools from the Hugging Face library to solve problems. GPT selected among tools by including the tool descriptions from that library in the context window,[^7kaceuordjy] and used those tools based on examples of the proper API calls, also given as context in a separate step. Between one and 40 examples of correct API formats were sufficient to allow use of those tools (although the success rate in practice was not close to 100%).\n\nThis project added useful capabilities, allowing ChatGPT to solve problems like interpreting images, including multiple steps of locating, identifying, and counting objects, interpret audio files, and produce outputs in image and audio form using generative networks. However, I think the real significance here is the relative simplicity and universality of this approach. This approach seems likely to be adaptable to an even wider range of software tools. Agents may be able to search the web for tools, download them or gain access to online versions, and use them by finding the API description and self-prompting with it.\n\nImproved executive function will aid in the actual usefulness of these tools. Recognizing that a call has failed, and using a different tool or different prompts, will improve reliability. Similarly, improved episodic memory will allow a system to search for instances where a particular tool has succeeded or failed for a similar problem.\n\nLMCA nonhuman cognitive capacities\n==================================\n\nThe [integration of Wolfram Alpha with ChatGPT](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/) is one example of providing access to cognitive tools or toolboxes that humans don’t have. I haven’t tried imagining others that will be useful, or how those might compound the capabilities of other cognitive systems. Tools for processing large datasets are one such possibility. Some of the tools in the Hugging Face library also represent nonhuman cognitive abilities. Heavy reliance on nonhuman cognitive capacities could be a problem for interpretability, but these also seem on net easier to interpret than complex neural network representations, and summarizing their results in natural language and human-readable API returns makes them more interpretable.\n\nImplications for alignment\n==========================\n\nConclusions about capabilities of LMCAs are in the overview section, and rereading that section may be useful.\n\nThe implications of this [natural language alignment](https://www.lesswrong.com/posts/EhkHnNJXwT8RmtfYZ/natural-language-alignment-1) (NLA) approach will be the subject of future posts, but I will give my thinking so far here. The purpose of this initial presentation is to stress-test and improve these ideas with input from the community.\n\nAn LMCA could be described as a shoggoth wearing a smiley face mask that recursively talks to itself and wields tools. However, it is the mask that talks and wields the tools. It is reminded to predict the words of character with the goals its user wrote, as often as necessary. In the lore, the shoggoth are highly capable and intelligent but not sentient or goal-directed.[^m6fr7ubr5el] As long as that is the case of the central LLM, that [simulator](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators) should remain “in character”, and the system should remain loyal to its user-given goals. The recurrent loop of sentience does not pass into the LLM itself. To maliciously do harm, the central LLM would to some extent have to trick the whole system, of which it is itself a part.  \n\nIf this arrangement sounds terrifying and arcane, I quite agree. Waluigi effects might be caught by constantly reminding the shoggoth to stay in character, but simple errors of wandering trains of thought, and errors of judgment seem inevitable. Only clever error-catching loops will let such a thing run independently without somehow running amok. These dangers bear more analysis, thought, and network interpretability research. However, it looks to me right now like this is the most realistic shot at alignment that we’ve got, since the alignment tax may be very low. Aligning these systems to practical goals entails most of the same challenges as aligning them to human flourishing and corrigibility. In addition, it seems to me that other approaches have almost all of the same downsides and fewer advantages.\n\nHaving a system that takes its top-level goal in natural language, and can balance multiple goals, would appear to be a huge opportunity for alignment. GPT4 appears to reason about balancing ethical and practical goals much like a well-informed human does. This reasoning is aided by the limited alignment attempts in its (likely) RLHF fine-tuning, and not all LLMs used for these types of cognitive architectures are likely to have that. However, even an unaligned LLM that’s highly capable of text prediction is likely to do such ethical reasoning and goal tradeoffs fairly well and naturally. This natural language alignment (NLA) approach is similar to an alignment approach previously suggested by [Steve Byrnes](https://www.lesswrong.com/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi) and [myself](https://www.lesswrong.com/posts/HEonwwQLhMB9fqABh/human-preferences-as-rl-critic-values-implications-for)for human-like actor-critic RL systems,but this seems even easier and more straightforward, and it applies to systems that people are likely to develop and deploy outside of alignment concerns.\n\nThe largest advantage to this LMCA NLA approach is that it *applies easily to systems that are likely to be deployed anyway.* Most of the promising alignment approaches I’m aware of would require different training approaches than those currently in use. It’s unclear who would implement these or what strategy could be used to motivate them, or society at large, to pay large alignment taxes. The perfect is the enemy of the good, and there is a certain merit to focusing on solutions that may actually be implemented.\n\nThis is not a complete solution for alignment. We do not want to trust the future to a Frankensteinian collection of cognitive components, talking to itself and making plans and decisions based on its conclusions. This easy loose alignment seems like a huge improvement over existing plans, particularly because the structure of language provides a good deal of generalization, and the way humans use language incorporates much of our ethical thinking, including our goals and values. \n\nThis type of initial alignment only becomes promising long-term when it is combined with corrigibility and interpretability. Including top-level goals for corrigibility in natural language seems much more promising than training the system on correlates of corrigibility, and hoping those generalize as capabilities improve. It is an easy way of having some amount of self-stabilizing alignment to include following ethical goals as part of the reasoning the system does to make and execute plans. The system can be coded to both check itself against its goals, and invite human inspection if it judges that it is considering plans or actions that may either violate its ethical goals, change its goals, or remove it from human control. Of course, leaving this judgment entirely to LMCA would be a mistake.\n\nInterpretability is another advantage of a system that summarizes its thinking and planning in natural language. There are concerns that LLMs do not entirely [think in plain sight](https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight); for instance, [RLHF may introduce pressures for networks to use steganography](https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId=zfzHshctWZYo8JkLe#comments) in their responses. These are real concerns, and will need to be addressed. Beyond those concerns, highly capable LMCAs will produce enormous internal transcripts. Parsing these will quickly go beyond human capability, let alone human inclination. Additional tools will be necessary to identify important and dangerous elements of these internal chains of thought.\n\nThis NLA approach is compatible with a [hodgepodge alignment strategy.](https://www.lesswrong.com/posts/YnGRBADQwpYRbuCbz/towards-hodge-podge-alignment-1) For instance, current implementations benefit from the inclusion of partially-aligned GPT4. The example of [tasking BabyAGI with creating paperclips](https://twitter.com/yoheinakajima/status/1640428710129201154), and it turning to the question of alignment, is one dramatic, if hand-picked example (the context of making paperclips in online writing is probably largely about the alignment problem). \n\nHowever, the NLA approach does little to address [the alignment stability problem](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem). It would seem to neither help nor hurt the existing approaches. The idea of [reflective stability](https://arbital.com/p/reflective_stability/) or an [internal value handshake](https://www.lesswrong.com/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem) still applies. Natural language alignment also does not address [representational drift](https://scholar.google.com/scholar?cluster=6678474289141605170&hl=en&as_sdt=0,6&as_vis=1) in interpreting those goals as experience and learning accrue.  Having an agent apply its own intelligence to maintain its goal stability over time is the best idea I know of, but I think it’s hard to know to what extent that will happen naturally in LMCAs. They are not strictly model-based maximizers, so like humans, they will not predictably stabilize their own goals perfectly over time. However, including this stability goal as part of the high-level goal prompt would seem to be a good start. Human involvement would seem to be possible and useful, as long as corrigibility and stability goals are maintained.[^lrpy2ysy4tp]\n\nNLA does not address the societal alignment problem. If these systems are as capable as I expect, that problem will become much worse, by allowing access to powerful agents in open-source form. I think we need to turn our attention to planning against Moloch itself, as well as planning for malicious and careless actors. These issues are challenging and critical if the near future unfolds in the massively multipolar AGI scenario I expect if LMCAs are successful.\n\nIt’s looking like a wild ride is coming up very quickly, but I think we’ve got a fighting chance.\n\n  \n \n\n*Thanks to Steve Byrnes, Beren Millidge, and Tom Hazy for helpful comments on a draft of this article.*  \n  \n \n\n[^7941hu3ojbb]: Beren Millidge’s excellent article describes scaffolded LLMs as natural language computers. He is addressing essentially the same set of potentials in LLMs by having them driven by scripts and interact with external tools, but he addresses this from a thoroughly CS perspective. This complements my perspective of seeing them as loosely brainlike cognitive architectures, and I highly recommend it. \n\n[^ff2ijs11vwd]: David Shapiro coined this term and originated this natural language approach to alignment in his 2021 book Natural Language Cognitive Architecture: A Prototype Artificial General Intelligence, which I haven’t yet read. He probably came up with this approach long before publishing that book, and others have probably talked about a natural language alignment prior to that post, but I haven’t it. I found Shapiro’s work when researching for this article, and I am adopting his cognitive architecture terminology because I think it’s appropriate. The few mentions of his work on Less Wrong are quickly dismissed in each instance.  I do not endorse Shapiro’s proposed “Heuristic Imperatives” as top-level goals. They are: Reduce suffering in the universe;  Increase prosperity in the universe; and Increase understanding in the universe.I’d expect these to wind up creating a world with no humans and lots of smart and prosperous AIs that don’t experience suffering (and never have cool dance parties or swap meets). But, to be fair, Shapiro doesn’t claim that these are the best final form, just that we should have a list, and encourage their adoption by social and economic pressure.I am not going to propose specific alternatives here, because we should first discuss whether any such scheme is useful. I'd say top-level goals for alignment should probably emphasize corrigibility and interpretability, along with some sort of harm reduction and human empowerment/flourishing.  \n\n[^2awdx185ksk]: It is unknown how much processing LLMs really accomplish to create each natural language string. And there are concerns that its output could become deceptively different than the internal processing that creates it. This is an important caveat on this approach, and deserves further discussion and interpretability work. The final section mentions some of these concerns. \n\n[^edbm641nnqf]: I’m suggesting the term x-risk AI, abbreviated XRAI, to denote AI that has a good chance of ending us. AGI is not specific enough, as GPT4 meets the intuitive definition of an AI that does a bunch of stuff well enough to be useful. I’d like a more strict definition of AGI, but I believe in coining new terms instead of telling people they’re using it wrong. \n\n[^kla5s6i9bzc]: Of course, the interactions between brain systems are highly complex on a neuronal level, and the exact mechanisms have not been worked out. On a high level, however, the principles seem clear. For the interactions I’ve described, it seems as though the limited bandwidth of natural language descriptions and simple APIs will be adequate to do a good deal of cognitive work. \n\n[^skhy4psdpqm]: This was an explicitly pluralistic version of the widely-used ACT-R cognitive architecture, meaning that the design goal was having cognitive components that could be swapped out for other implementations that accomplished the same function. This is the HuggingGPT approach of having an array of external tools to call. We were using the Leabra algorithm to create deep neural networks, but the actual implementation was limited to a vision network taking on the role of the visual \"what\" stream in ACT-R. Two papers may be of interest:SAL: an explicitly pluralistic cognitive architecture (2008) Integrating systems and theories in the SAL hybrid architectur (2013) \n\n[^3aubec0uckf]: For too much more on the precise distinctions in terminology surrounding executive function, see our paper How Sequential Interactive Processing Within Frontostriatal Loops Supports a Continuum of Habitual to Controlled Processing. \n\n[^f3ojnbx7xrm]: While Auto-GPT has accomplished little of real use at this early date, I'm impressed by how it seemingly spontaneously tries new approaches after a failure, based on its conclusion of that failure in its context window. More sophisticated approaches are possible, but they may not be necessary. \n\n[^2zntv0sg82k]: This type of interference may relate to a cognitive neuroscience theory postulating that humans' low working-memory-for-executive-function capacity is actually advantageous in preventing interference. See Rationalizing constraints on the capacity for cognitive control. \n\n[^vap71h9f84]: Dopamine response has long been known to approximate reward prediction error (which is equivalent to a value estimate in actor-critic RL). It is now known that the dopamine response contains other response characteristics, including positive responses for negative occurrences. This is consistent with dopamine as a general error signal that trains many responses beyond reward expectation, but it is consistent with the finding that dopamine’s central function is value estimation, with around 70% of dopamine cells responding in that way. Much more can be found in A systems-neuroscience model of phasic dopamine. \n\n[^7kaceuordjy]: The number and length of tool descriptions in the Hugging Face library necessitated a pre-selection step to select more-likely appropriate tool descriptions, since all of the descriptions together exceeded the context window. \n\n[^m6fr7ubr5el]: I have neither the artistic skill nor the time to cajole midjourney to depict a shoggoth wearing a smiley face mask that holds dangerous tools and talks to itself. Help would be appreciated. The lore I’m remembering may not be canon, it’s been a while. Fortunately, fictional shoggoths aren’t relevant. Unfortunately, the level of sentience and goal-directedness of current and future LLMs is also unknown. \n\n[^lrpy2ysy4tp]: Whoops, giving an LMCA a goal of not changing its goals could conflict with its goals of corrigibility and interpretability, since letting a user inspect its thoughts might result in the user changing its goals. This stuff is going to be tricky. I hope nobody launches a GPT-5 LMCA based on my suggestions without reading the footnotes.",
      "plaintextDescription": "Epistemic status: Hoping for help working through these new ideas.\n\nTLDR:\n\nScaffolded[1], \"agentized\" LLMs that combine and extend the approaches in AutoGPT, HuggingGPT, Reflexion, and BabyAGI seem likely to be a focus of near-term AI development. LLMs by themselves are like a human with great automatic language processing, but no goal-directed agency, executive function, episodic memory,  or sensory processing. Recent work has added all of these to LLMs, making language model cognitive architectures (LMCAs). These implementations are currently limited but will improve.\n\nCognitive capacities interact synergistically in human cognition. In addition, this new direction of development will allow individuals and small businesses to contribute to progress on AGI.  These new factors of compounding progress may speed progress in this direction. LMCAs might well become intelligent enough to create X-risk before other forms of AGI.  I expect LMCAs to enhance the effective intelligence of LLMs by performing extensive, iterative, goal-directed \"thinking\" that incorporates topic-relevant web searches.\n\nThe possible shortening of timelines-to-AGI is a downside, but the upside may be even larger. LMCAs pursue goals and do much of their “thinking” in natural language, enabling a natural language alignment (NLA) approach. They reason about and balance ethical goals much as humans do. This approach to AGI and alignment has large potential benefits relative to existing approaches to AGI and alignment. \n\n\n \n\n\nOverview\nI still think it's likely that agentized LLMs will change the alignment landscape for the better, although I've tempered my optimism a bit since writing that. A big piece of the logic for that hypothesis is why I expect this approach to become very useful, and possibly become the de-facto standard for AGI progress. The other piece was the potential positive impacts on alignment work. Both of those pieces of logic were compressed in that post. I expand on them here.\n\nBegi",
      "wordCount": 5906
    },
    "tags": [
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "iKYWGuFx2qH2nYu6J",
        "name": "AI Capabilities",
        "slug": "ai-capabilities"
      },
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "qHCFjTEWCQjKBWy5z",
        "name": "Corrigibility",
        "slug": "corrigibility-1"
      },
      {
        "_id": "b6tJM7Lza74rTfCBF",
        "name": "Goal-Directedness",
        "slug": "goal-directedness"
      },
      {
        "_id": "u4b2EXJFMWuRmGSZn",
        "name": "Language model cognitive architecture",
        "slug": "language-model-cognitive-architecture"
      },
      {
        "_id": "LRXv5no9g8M25Tu5b",
        "name": "Cognitive Architecture",
        "slug": "cognitive-architecture"
      },
      {
        "_id": "AGgktgYb72PPjET9r",
        "name": "Multipolar Scenarios",
        "slug": "multipolar-scenarios"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb29d",
        "name": "Neuromorphic AI",
        "slug": "neuromorphic-ai"
      },
      {
        "_id": "KRcsSxBTLGRrSbHsW",
        "name": "Transformative AI",
        "slug": "transformative-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "dcoxvEhAfYcov2LA6",
    "title": "Agentized LLMs will change the alignment landscape",
    "slug": "agentized-llms-will-change-the-alignment-landscape",
    "url": null,
    "baseScore": 160,
    "voteCount": 115,
    "viewCount": null,
    "commentCount": 102,
    "createdAt": null,
    "postedAt": "2023-04-09T02:29:07.797Z",
    "contents": {
      "markdown": "Epistemic status: head spinning, suddenly unsure of everything in alignment. And unsure of these predictions.\n\nI'm following the suggestions in [10 reasons why lists of 10 reasons might be a winning strategy](https://www.lesswrong.com/posts/8gbfvGhSnEJ9hHGew/10-reasons-why-lists-of-10-reasons-might-be-a-winning) in order to get this out quickly (reason 10 will blow your mind!). I'm hoping to prompt some discussion, rather than try to do the definitive writeup on this topic when this technique was introduced so recently.\n\nTen reasons why agentized LLMs will change the alignment landscape:\n\n1.  **Agentized**[^o492axh6mpi]**LLMs like** [**Auto-GPT**](https://github.com/Torantulino/Auto-GPT)**and** [**Baby AGI**](https://twitter.com/yoheinakajima/status/1642881722495954945) **may fan the** [**sparks of AGI**](https://arxiv.org/abs/2303.12712) **in GPT-4 into a fire.** These techniques use an LLM as a central cognitive engine, within a recursive loop of breaking a task goal into subtasks, working on those subtasks (including calling other software), and using the LLM to prioritize subtasks and decide when they're adequately well done. They recursively check whether they're making progress on their top-level goal.\n2.  **While it remains to be seen what these systems can actually accomplish, I think it's very likely that they will dramatically enhance the effective intelligence of the core LLM.** I think this type of recursivity and breaking problems into separate cognitive tasks is central to human intelligence. This technique adds several key aspects of human cognition; executive function; reflective, recursive thought; and episodic memory for tasks, despite using non-brainlike implementations. To be fair, the existing implementations seem pretty limited and error-prone. But they were implemented in days. So this is a prediction of near-future progress, not a report on amazing new capabilities.\n3.  **This approach appears to be easier than I'd thought.** I've been expecting this type of self-prompting to imitate the advantages of human thought, but I didn't expect the cognitive capacities of GPT-4 to make it so easy to do useful multi-step thinking and planning. The ease of initial implementation (something like 3 days, with all of the code also written by GPT-4 for baby AGI) implies that improvements may also be easier than we would have guessed.\n4.  **Integration with** [**HuggingGPT**](https://arxiv.org/abs/2303.17580) **and similar approaches can provide these cognitive loops with more cognitive capacities.** This integration was also easier than I'd have guessed, with GPT-4 learning from a handful (e.g., 40) of examples how to use other software tools. Those tools will include both sensory capacities, with vision models and other sensory models of various types, and the equivalent of a variety of output capabilities.\n5.  **Integration of recursive LLM self-improvement like** [**\"Reflexion\"**](https://nanothoughts.substack.com/p/reflecting-on-reflexion)** can utilize these cognitive loops to make the core model better at a variety of tasks.**\n6.  **Easily agentized LLMs is terrible news for capabilities.** I think we'll have an internet full of LLM-bots \"thinking\" up and doing stuff within a year.\n7.  **This is absolutely bone-chilling for the urgency of the alignment and coordination problems.** Some clever chucklehead already created [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM), an instance of Auto-GPT given the goal to destroy humanity and create chaos. You are literally reading the thoughts of *something thinking about how to kill you*. It's too stupid to get very far, but it will get smarter with every LLM improvement, and every improvement to the recursive self-prompting wrapper programs. This gave me my very first visceral fear of AGI destroying us. I recommend it, unless you're already plenty viscerally freaked out.\n8.  **Watching agents think is going to shift public opinion**. We should be ready for more [AI scares and changing public beliefs](https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs). I have no idea how this is going to play out in the political sphere, but we need to figure this out to have a shot at successful alignment, because\n9.  **We will be in a multilateral AGI world**. Anyone can spawn a dumb AGI and have it either manage their social media, or try to destroy humanity. And over the years, those commercially available AGIs will get smarter. Because defense is harder than offense, it is going to be untenable to indefinitely [defend the world against out-of-control AGIs.](https://www.lesswrong.com/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control) But\n10.  **Important parts of alignment and interpretability might be a lot easier than most of us have been thinking.** These agents take goals as input, in English. They reason about those goals much as humans do, and this will likely improve with model improvements. This does not solve the outer alignment problem; one existing suggestion is to include a top-level goal of \"reducing suffering.\" No! No! No!. This also does not solve [the alignment stability problem](https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem). Starting goals can be misinterpreted or lost to recursive subgoals, and if any type of continued learning is included, behavior will shift over time. It doesn't even solve the inner alignment problem if recursive training methods create mesa-optimizers in the LLMs. But it also provides incredibly easy interpretability, because these systems *think in English.*\n\nIf I'm right about any reasonable subset of this stuff, this lands us in a terrifying, promising new landscape of alignment issues. We will see good bots and bad bots, and the balance of power will shift. Ultimately I think this leads to the necessity of very strong global monitoring, including breaking all encryption, to prevent hostile AGI behavior. The array of issues is dizzying (I am personally dizzied, and a bit short on sleep from fear and excitement). I would love to hear others' thoughts.\n\n[^o492axh6mpi]: I'm using a neologism, and a loose definition of agency as things that flexibly pursue goals. That's similar to this more rigorous definition.",
      "plaintextDescription": "Epistemic status: head spinning, suddenly unsure of everything in alignment. And unsure of these predictions.\n\nI'm following the suggestions in 10 reasons why lists of 10 reasons might be a winning strategy in order to get this out quickly (reason 10 will blow your mind!). I'm hoping to prompt some discussion, rather than try to do the definitive writeup on this topic when this technique was introduced so recently.\n\nTen reasons why agentized LLMs will change the alignment landscape:\n\n 1. Agentized[1] LLMs like Auto-GPT and Baby AGI may fan the sparks of AGI in GPT-4 into a fire. These techniques use an LLM as a central cognitive engine, within a recursive loop of breaking a task goal into subtasks, working on those subtasks (including calling other software), and using the LLM to prioritize subtasks and decide when they're adequately well done. They recursively check whether they're making progress on their top-level goal.\n 2. While it remains to be seen what these systems can actually accomplish, I think it's very likely that they will dramatically enhance the effective intelligence of the core LLM. I think this type of recursivity and breaking problems into separate cognitive tasks is central to human intelligence. This technique adds several key aspects of human cognition; executive function; reflective, recursive thought; and episodic memory for tasks, despite using non-brainlike implementations. To be fair, the existing implementations seem pretty limited and error-prone. But they were implemented in days. So this is a prediction of near-future progress, not a report on amazing new capabilities.\n 3. This approach appears to be easier than I'd thought. I've been expecting this type of self-prompting to imitate the advantages of human thought, but I didn't expect the cognitive capacities of GPT-4 to make it so easy to do useful multi-step thinking and planning. The ease of initial implementation (something like 3 days, with all of the code also written by GPT-4 f",
      "wordCount": 870
    },
    "tags": [
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "iKYWGuFx2qH2nYu6J",
        "name": "AI Capabilities",
        "slug": "ai-capabilities"
      },
      {
        "_id": "qHCFjTEWCQjKBWy5z",
        "name": "Corrigibility",
        "slug": "corrigibility-1"
      },
      {
        "_id": "b6tJM7Lza74rTfCBF",
        "name": "Goal-Directedness",
        "slug": "goal-directedness"
      },
      {
        "_id": "AGgktgYb72PPjET9r",
        "name": "Multipolar Scenarios",
        "slug": "multipolar-scenarios"
      },
      {
        "_id": "KRcsSxBTLGRrSbHsW",
        "name": "Transformative AI",
        "slug": "transformative-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ou5raNNjamAaahtWG",
    "title": "AI scares and changing public beliefs",
    "slug": "ai-scares-and-changing-public-beliefs",
    "url": null,
    "baseScore": 48,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 23,
    "createdAt": null,
    "postedAt": "2023-04-06T18:51:12.831Z",
    "contents": {
      "markdown": "The opportunity\n---------------\n\nI, for one, am looking forward to the the next public AI scares.\n\nThere's a curious up-side to the reckless deployment of powerful, complex systems: they scare people. This has the potential to shift the landscape of the public debate. It's an opportunity the AGI safety community should be prepared for the next time it happens. I'm not saying that we should become fear-mongers, but that we need to engage wisely when we're given those opportunities. We could easily fumble the ball badly, and we need to not do that.\n\nWe seem to be in an interesting wild-west era of AI deployment. I hope we leave this era before it's too late. Before we do, I expect to see more scary behavior from AI systems. The early behavior of Bing Chat, documented in [bing Chat is blatantly, aggressively misaligned](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned) caused a furor of media responses, and it seemed to result in some AI X-risk skeptics changing their opinions to take those dangers more seriously. I'd rather not concern myself with public opinion, and I've hoped that we could just leave this to the relative experts (the machine learning and safety communities), but it's looking more and more like the public are going to want to weigh in. I'm hopeful that those changes in expert opinion can continue, and that they'll trickle down into public opinion with enough fidelity to help the situation[^y3a2nnyvbih].\n\nI'll mention two particular cases. Gary Marcus said he's shifting his opinions on dangers in a [recent podcast](https://www.samharris.org/podcasts/making-sense-episodes/312-the-trouble-with-ai) (paywalled) with Sam Harris and Stuart Russell. He maintained his skepticism about LLMs constituting real intelligence, but said he'd become more concerned about the dangers, particularly because of how OpenAI and Microsoft deployed rapidly, pressuring Google to respond with rushed deployments of their own. I think he and similar skeptics will shift further as LangChain, AutoGPT, and other automated chain-of-thought adaptations add goals and executive function to LLMs. Another previous skeptic is Russ Roberts of EconTalk, who reaches a very different, older, less tech-savvy, and more conservative audience. He hosted Eric Hoel on [a recent episode](https://www.econtalk.org/erik-hoel-on-the-threat-to-humanity-from-ai/) (not paywalled). Hoel does a very nice job of explaining the risks in sensible, gentle way, but he does focus on x-risk and doesn't fall back on near-term lesser risks. Roberts appears to be pretty much won over, while having been highly skeptical in past interviews on the topic. I haven't attempted anything like a systematic review of public responses, but I've noted not just increased interest but increased credulity among skeptics.\n\nI think we're going to see a shift in public opinion about AI development. That will in part be powered by new scares. Those scares also create opportunities for more people in the AGI safety community to engage with the public. We should think about those public presentations before the next scare. The AI safety community is full of highly intelligent, logical people, but on average, public relations is not our strength. It is time for those of us who want to engage the public to get better.\n\nThe challenge\n-------------\n\nI think it's time to think about our approach now, because we need to get this right. Presenting detailed arguments forcefully is not the way to shift public opinion to your cause. The course of the climate debate is one important object lesson. As the evidence and expert opinion converged, half of the public actually became substantially more skeptical of human-caused climate change.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/557fa479c43f1344fcf7f14773981d04180ffe6025960054.png)\n\nFrom:([A Cooling Climate for Change? Party Polarization and the Politics of Global Warming - Deborah Lynn Guber, 2013 (sagepub.com)](https://journals.sagepub.com/doi/abs/10.1177/0002764212463361?journalCode=absb)\n\nMy main argument is this: we need to not let the same polarization happen with AGI x-risk. And where it's already happening, among intellectuals, we need to reverse it. This is a much larger topic than I'm prepared to cover comprehensively. My argument here is that this is something we should learn, debate, and practice.\n\nThe reasons for this polarizing shift are unclear. It's probably not the [backfire effect](https://rationalwiki.org/wiki/Backfire_effect), which appears to happen only in some situations and perhaps among the cognitively-inclined, who will internally create new counterarguments. This is good news. Presenting data and arguments can work.\n\nI've done academic research on cognitive biases, so I feel like I have a pretty good idea what went wrong in that climate debate. Probably many fellow rationalists do too. I'd say that this paradoxical effect seems to result from motivated reasoning and confirmation bias, combined with social rewards. It is socially advantageous to share the important beliefs of those around us. Motivated reasoning pushes us subtly to think more in directions that are likely to lead to that sort of social reward. Confirming arguments and evidence both come to mind more easily, and fulfill that subtle drive to believe what will benefit us. And [ugh fields](https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields) drive us away from disconfirming evidence and arguments.\n\nApproaches to convincing the public\n-----------------------------------\n\nIt's probably useful to have decent theories of the problem. But I don't have a clear idea of how to be persuasive in the public sphere. I've looked at practical theories of changing beliefs in real world settings a little bit recently, but only a little, because learning to be persuasive has always seemed dishonest. But I think that's largely an inappropriate emotional hangup, because the stuff that rings true has nothing to do with lying or even being manipulative. It's basically about having your audience like and trust you, and not being an asshole. It's about showing respect and empathy for the person you're talking to, not pressuring them to make a quick decision, and letting them go off to think it through themselves. These techniques probably don't work if the truth isn't on your side, but that seems fine.\n\nI wanted to get my very vague and tentative conclusions out there, but I don't really have good ideas about how to be persuasive. I want to do more research on that, holding to methods that preserve honesty, and I hope that more folks in this community will share their research and thoughts. We may not want to be in the business of public relations, but at least some of us sort of need to.\n\nI do have ideas about how to fuck up persuading people, and turn them against your beliefs. Some of these I've acquired the hard way, some through research. Accidentally implying you think your audience are idiots is one easy way to do it. And that's tough to avoid, when you're talking about something you've thought about a thousand times as much as the people you're addressing. For one thing, they *are* idiots- we all are. Rationalism is an ideal, not an achievable goal, even for those of us who aspire to it. In addition, your audience are particularly idiots in the domain of AI safety, relative to you. But people intuitively pick up on a lack of respect, and they're not likely to work out where that's coming from.\n\nOverstating your certainty on various points is one great way to both imply that your audience is an idiot, and to give them an easy way to rationalize that *you* must really be the idiot- nobody could really be that certain. [Communicating effectively under Knightian norms](https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms) helps clarify how different ways of communicating probabilistic guesses can make one person's rational best guess sound to another person like sheer hubris. Eliezer may be rational in saying we're doomed with 99% certainty *under certain assumptions*; but his audience isn't going to listen and think carefully while he explains all of those many assumptions. They'll just move on with their busy day, and assume he's crazy.\n\nAnother factor here is that not everyone *makes a hobby* of understanding complex new ideas quickly, and very few people adhere to a social group where changing one's mind in the face of arguments and evidence is highly valued. Rationalists do have a bit of an edge in this particular area, but we're hardly immune to cognitive biases. Thinking really hard about how motivated reasoning and confirmation bias are at play in various areas of your own mind is one way to develop empathy for the perspective of someone who's being asked to re-evaluate an important belief on the spot. (This deserves a post titled \"I am an idiot\". We'll see if I have the gumption to write that post.)\n\nI think there's a big advantage here in that the public's existing beliefs around AI x-risk often aren't that strong, and don't fall across existing tribal lines. The AGI safety community seems to have done a great job so far of not making enemies of the ML community, for instance. I think we need to carefully widen this particular circle, and use the time before the public's beliefs are solidified to persuade them, probably through honesty, humbleness, and empathy.\n\nApproaches to convincing experts\n--------------------------------\n\nI'm repeatedly amazed by how easy it is to convince laypeople that self-aware, agentic AI presents an X-risk we should worry about, and I'm equally amazed at how difficult it is to convince experts- those in ML or cognitive science. And those latter are the people we most need to convince, because the public (rationally) takes their cues from those they trust who are more expert than they.\n\nIn discussions, I often hear experts deploy absolutely ridiculous arguments, or worse yet, no real argument beyond \"that's ridiculous.\" I think of their motivation to defend their own field, or to protect their comfortable worldview. And I get frustrated, because that attitude might well get us all killed. This has often caused me to get more forceful, and to talk as though I think they're an idiot. This has predictably terrible results.\n\nI've recently had a discussion with a close friend who works in ML, including having worked at DeepMind. I'd mostly avoided the topic with him until starting to work directly in AGI safety, because I knew it would be a dangerous a friction point, and I highly value his friendship since he's intelligent, creative, generous, and kind. The first real exchange went almost as badly as expected. I steeled myself to practice the virtues I mentioned above: listening, staying calm and empathetic, and presenting arguments gently instead of forcefully. We managed a much better exchange that established a central crux, which I think is probably common to the average ML vs. rationalist view: timelines.\n\nHis timeline was around 30 years and maybe never, while mine is much shorter. His opinion was coherent: AI x-risk is a potential problem, but it's not worth talking about because we're so far from needing to solve it, and the solutions will become clearer as we get closer. Those going on about X-risk seem like a cult that's talked themselves into something ridiculous and at odds with the actual experts, probably through the same social and cognitive biases that caused the climate change polarization among conservatives. Right now we should be focusing on near-term AI risks, and the many other ills and dangers of the modern world.\n\nI think this is a common crux of disagreement, but I'm sure it's not the only one. AI scares are reducing that disagreement. We can take advantage of that situation, but only if we get better at persuasion. I intend to do more research and practice on this skill. I hope some of you will join me. Our efforts to date are not encouraging, and look like they may produce polarization rather than steadily shift opinions. But we can do better.\n\n[^y3a2nnyvbih]: How public opinion will affect outcomes is a complex discussion, and I'm not ready to offer even a guess, except for the background assumption I use here: the public is going to believe something about AI safety, and it's likely better if they believe something like the truth.",
      "plaintextDescription": "The opportunity\nI, for one, am looking forward to the the next public AI scares.\n\nThere's a curious up-side to the reckless deployment of powerful, complex systems: they scare people. This has the potential to shift the landscape of the public debate. It's an opportunity the AGI safety community should be prepared for the next time it happens. I'm not saying that we should become fear-mongers, but that we need to engage wisely when we're given those opportunities. We could easily fumble the ball badly, and we need to not do that.\n\nWe seem to be in an interesting wild-west era of AI deployment. I hope we leave this era before it's too late. Before we do, I expect to see more scary behavior from AI systems. The early behavior of Bing Chat, documented in bing Chat is blatantly, aggressively misaligned caused a furor of media responses, and it seemed to result in some AI X-risk skeptics changing their opinions to take those dangers more seriously. I'd rather not concern myself with public opinion, and I've hoped that we could just leave this to the relative experts (the machine learning and safety communities), but it's looking more and more like the public are going to want to weigh in. I'm hopeful that those changes in expert opinion can continue, and that they'll trickle down into public opinion with enough fidelity to help the situation[1].\n\nI'll mention two particular cases. Gary Marcus said he's shifting his opinions on dangers in a recent podcast (paywalled) with Sam Harris and Stuart Russell. He maintained his skepticism about LLMs constituting real intelligence, but said he'd become more concerned about the dangers, particularly because of how OpenAI and Microsoft deployed rapidly, pressuring Google to respond with rushed deployments of their own. I think he and similar skeptics will shift further as LangChain, AutoGPT, and other automated chain-of-thought adaptations add goals and executive function to LLMs. Another previous skeptic is Russ Roberts of EconTalk",
      "wordCount": 1919
    },
    "tags": [
      {
        "_id": "bQZAkiFgtbEcr5h6f",
        "name": "AI Persuasion",
        "slug": "ai-persuasion"
      },
      {
        "_id": "ZXFpyQWPB5ideFbEG",
        "name": "Conversation (topic)",
        "slug": "conversation-topic"
      },
      {
        "_id": "MXcpQvaPGtXpB6vkM",
        "name": "Public Discourse",
        "slug": "public-discourse"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "g3pbJPQpNJyFfbHKd",
    "title": "The alignment stability problem",
    "slug": "the-alignment-stability-problem",
    "url": null,
    "baseScore": 35,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 15,
    "createdAt": null,
    "postedAt": "2023-03-26T02:10:13.044Z",
    "contents": {
      "markdown": "The community thinks a lot about how to align AGI. It thinks less about how to align AGI so that it stays aligned for the long term. In many hypothetical cases, these are one and the same thing. But for the type of AGI we're actually likely to get, I don't think they are.\n\nDespite some optimism for aligning tool-like AGI, or at least static systems, it seems likely that we will create AGI that learns after it's deployed, and that has some amount of agency. If it does, its alignment will effectively shift, as addressed in the [diamond maximizer](https://arbital.com/p/diamond_maximizer/) thought experiment and elsewhere. And that's even if it doesn't deliberately change its preferences. People deliberately change their preferences sometimes, despite not having access to our own source code. So, it would seem wise to think seriously and explicitly about the stability problem, even if it isn't needed for current-generation AGI research.\n\nI've written a chapter on this, [Goal changes in intelligent systems](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=a9D1LDsAAAAJ&sortby=pubdate&citation_for_view=a9D1LDsAAAAJ:9ZlFYXVOiuMC). There I laid out the problem, but I didn't really propose solutions. What follows is a summary of that article, followed by a brief discussion of the work I've been able to locate on this problem, and one direction we might go to pursue it.\n\nWhy we don't think about much about alignment stability, and why we should.\n---------------------------------------------------------------------------\n\nSome types of AGI are self-stabilizing. A sufficiently intelligent agent will try to prevent its goals[^5fr12nr06mg] from changing, at least if it is consequentialist. That works nicely if its values are one coherent construct, such as diamond or human preferences. But humans have lots of preferences, so we may wind up with a system that must balance many goals. And if the system keeps learning after deployment, it seems likely to alter its understanding of what its goals mean. This is the thrust of the diamond maximizer problem.\n\nOne tricky thing about alignment work is that we're imagining different types of AGI when we talk about alignment schemes. Currently, people are thinking a lot about aligning deep networks. Current deep networks don't keep learning after they're deployed. And they're not very [agentic](https://www.alignmentforum.org/posts/XxX2CAoFskuQNkBDy/discovering-agents) These are great properties for alignment, and they seem to be the source of some optimism.\n\nEven if this type of network turns out to be really useful, and all we need to make the world a vastly better place, I don't think we're going to stop there. Agents would seem to have capabilities advantages that metaphorically [make tool AI want to become agentic AI](https://gwern.net/tool-ai). If that weren't enough, agents are *cool*. People are going to want to turn tool AI into agent AI just to experience the wonder of an alien intelligence with its own goals.\n\nI think turning intelligent tools into agents is going to be [relatively easy](https://www.alignmentforum.org/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth). But even if it's not easy someone is going to manage it at some point.. It's probably [too difficult to prevent further experimentation](https://www.alignmentforum.org/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control), at least without a governing body, aided by AGI, that's able and willing to *at minimum* intercept and de-encrypt every communication for signs of AGI projects.\n\nWhile the above logic is far from airtight, it would seem wise to think about stable alignment solutions, in advance of anyone creating AGI that continuously learns outside of close human control.\n\nSimilar concerns have been raised elsewhere, such as [On how various plans miss the hard bits of the alignment challenge](https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment). Here I'm trying to crystallize and give a name to this specific hard part of the problem.\n\nApproaches to alignment stability\n---------------------------------\n\nAlex Turner addresses this in [A shot at the diamond-alignment problem](https://www.alignmentforum.org/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem). In broad form, he's saying that you would train the agent with RL to value diamonds, including having diamonds associated with the reward in a variety of cognitive tasks. This is as good an answer as we've got. I don't have a better idea; I think the area needs more work. Some difficulties with this scheme are raised in [Contra shard theory, in the context of the diamond maximizer problem](https://www.alignmentforum.org/posts/Aet2mbnK7GDDfrEQu/contra-shard-theory-in-the-context-of-the-diamond-maximizer). Charlie Steiner's argument that [shard theory requires magic](https://www.lesswrong.com/posts/uz2mdPtdBnaXpXPmT/shard-theory-alignment-has-important-often-overlooked-free) addresses roughly the same concerns. In sum, it's going to be tricky to train a system so that it has the right set of goals when it acquires enough self-awareness to try to preserve its goals.\n\nNote that none of these directly confront the additional problems of a [multi-objective](https://www.lesswrong.com/posts/i5dLfi6m6FCexReK9/a-brief-review-of-the-reasons-multi-objective-rl-could-be) RL system. It could well be that an RL system with multiple goals will collapse to having only a single goal over the course of reflection and self-modification. Humans don't do this, but we have both limited intelligence and a limited ability to self-modify.\n\nAnother approach to preventing goal changes in intelligent agents is corrigibility. If we can notice when the agent's goals are changing, and instruct or retrain or otherwise modify them back to what we want, we're goood. This is a great idea; the problem is that it's another multi-objective alignment problem. Christiano has [said](https://www.alignmentforum.org/posts/o22kP33tumooBtia3/can-corrigibility-be-learned-safely?commentId=SxiKZ4fggWcHqXpaz#jo2cwbB3WK7KyGjpy) \"I grant that even given such a core \\[of corrigibility\\], we will still be left with important and unsolved x-risk relevant questions like \"Can we avoid value drift over the process of deliberation?\"\"\n\nI haven't been able to find other work trying to provide a solution the diamond maximizer problem, or other formulations of the stability problem. I'm sure it's out there, using different terminology and mixed into other alignment proposals. I'd love to get pointers on where to find this work.\n\nA direction: asking if and how humans are stably aligned.\n---------------------------------------------------------\n\n[Are you stably aligned?](https://www.lesswrong.com/posts/Sf99QEqGD76Z7NBiq/are-you-stably-aligned) I think so, but I'm not sure. I think humans are stable, multi-objective systems, at least in the short term. Our goals and beliefs change, but we preserve our important values over most of those changes. Even when gaining or losing religion, most people seem to maintain their goal of helping other people (if they have such a goal); they just change their beliefs about how to best do that.\n\nHumans only maintain that stability of several important goals across our relatively brief lifespans. Whether we'd do the same in the long term is an open question that I want to consider more carefully in future posts. And we might only maintain those goals with the influence of a variety of reward signals, such as getting a reward signal in the form of dopamine spikes when we make others happy. Even if we figure out how that works (the focus of Steve Byrnes' [work](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8/p/5F5Tz3u6kJbTNMqsb)), including those rewards in a mature AGI might have bad side effects, like a universe tiled with simulacra of happy humans. \n\nThe human brain is not clearly the most promising model of alignment stability. But it's what I understand best, so my efforts will go there. And there are other advantages to aligning brainlike AGI over other types. For instance, [humans seem to have a critic system that could act as a \"handle\" for alignment](https://www.lesswrong.com/posts/HEonwwQLhMB9fqABh/human-preferences-as-rl-critic-values-implications-for). And brainlike AGI would seem to be a relatively good target for interpretability-heavy approaches, since we seem to think one important thought at a time, and we're usually able to put them into words.\n\nMuch work remains to be done to understand alignment stability. I'll talk more about training brainlike AGI to have enough of our values, in a long-term stable form, in future posts.\n\nEdit (9/24): I have now written a bunch on aligning brainlike AGI,[^jmispw4cue] but have not come back to the stability problem. Neither have I found other work addressing it for complex multi-goal(value) systems like brainlike AGI or humans.  I now think we won't do that until we have superintelligent help, because [Instruction-following AGI is easier and more likely than value aligned AGI](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than). Instruction-following AGI keeps humans in the loop and lets them monitor and correct value/alignment changes, solving the alignment stability problem but creating a different [severe problem of putting competing humans in charge of multiple RSI-capable AGIs](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1).\n\n[^5fr12nr06mg]: I'll use goals here, but many definitions of values, objectives, or preferences could be swapped in. \n\n[^jmispw4cue]: I now (9/24) think we can align brainlike AGI well enough, if we're somewhat careful and align it to personal intent as a stepping stone to full value alignment.SeeGoals selected from learned knowledge: an alternative to RL alignment  for techniques applying to three types of loosely brainlike AGI andInternal independent review for language model agent alignment for a suite of techniques that can be applied to language model agents.",
      "plaintextDescription": "The community thinks a lot about how to align AGI. It thinks less about how to align AGI so that it stays aligned for the long term. In many hypothetical cases, these are one and the same thing. But for the type of AGI we're actually likely to get, I don't think they are.\n\nDespite some optimism for aligning tool-like AGI, or at least static systems, it seems likely that we will create AGI that learns after it's deployed, and that has some amount of agency. If it does, its alignment will effectively shift, as addressed in the diamond maximizer thought experiment and elsewhere. And that's even if it doesn't deliberately change its preferences. People deliberately change their preferences sometimes, despite not having access to our own source code. So, it would seem wise to think seriously and explicitly about the stability problem, even if it isn't needed for current-generation AGI research.\n\nI've written a chapter on this, Goal changes in intelligent systems. There I laid out the problem, but I didn't really propose solutions. What follows is a summary of that article, followed by a brief discussion of the work I've been able to locate on this problem, and one direction we might go to pursue it.\n\n\nWhy we don't think about much about alignment stability, and why we should.\nSome types of AGI are self-stabilizing. A sufficiently intelligent agent will try to prevent its goals[1] from changing, at least if it is consequentialist. That works nicely if its values are one coherent construct, such as diamond or human preferences. But humans have lots of preferences, so we may wind up with a system that must balance many goals. And if the system keeps learning after deployment, it seems likely to alter its understanding of what its goals mean. This is the thrust of the diamond maximizer problem.\n\nOne tricky thing about alignment work is that we're imagining different types of AGI when we talk about alignment schemes. Currently, people are thinking a lot about aligning deep ne",
      "wordCount": 1316
    },
    "tags": [
      {
        "_id": "PH66Y6oE9cpHwbXe4",
        "name": "Tiling Agents",
        "slug": "tiling-agents"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb29d",
        "name": "Neuromorphic AI",
        "slug": "neuromorphic-ai"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b5",
        "name": "Recursive Self-Improvement",
        "slug": "recursive-self-improvement"
      },
      {
        "_id": "zuwsLCbxbugqB7FQY",
        "name": "Shard Theory",
        "slug": "shard-theory"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "HEonwwQLhMB9fqABh",
    "title": "Human preferences as RL critic values - implications for alignment",
    "slug": "human-preferences-as-rl-critic-values-implications-for",
    "url": null,
    "baseScore": 27,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2023-03-14T22:10:32.823Z",
    "contents": {
      "markdown": "\n\nTLDR: Human preferences might be largely the result of a critic network head, much like that used in SOTA agentic RL systems. The term \"values\" in humans might mean almost exactly what it does in RL: an estimate of discounted sum of future rewards. In humans this is based on better and more abstract representations than current RL systems.\n\nWork on aligning RL systems often doesn't address the critic system as distinct from the actor. But using systems with a critic head may provide a much simpler interface for interpreting and directly editing the system's values and, therefore, its goals and behavior. In addition, including a powerful critic system may be advantageous for capabilities as well.\n\nOne way to frame this is that human behavior, and therefore a neuromorphic AGI, might well be governed primarily by a critic system, and that's simpler to align than understanding the complex mess of representations and action habits in the remainder of the system.\n\nReaders will hopefully be familiar with Steve Byrnes' sequence [intro to brain-like AGI safety](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8). What I present here seems to be entirely consistent with his theories. [Post 14](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8/p/QpHewJvZJFaQYuLwH) of that sequence, his [recent elaboration](https://www.alignmentforum.org/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi), and other recent work[^1] present similar  ideas. We use different terminology[^2] and explanatory strategies, and I focus more on specifics of the critic system, so hopefully the two explanations are not redundant.\n\n## The payoff: a handle for alignment.\n\nWouldn't it be nice if the behavior of a system were governed by one system, and it provided an easily trainable (or even hand-editable) set of weights? And if that system had a clear readout, meaning something like \"I'll pursue what I'm currently thinking about, as a goal, with priority 0-1 in this context\"?\n\nSuppose we had a proto-AGI system including a component with the above properties. That subsystem is what I'm terming the critic. Now suppose further that this system is relatively well-trained but is (by good planning and good fortune) still under human control. We could prompt it with language like \"think about helping humans get things they want\" or \"...human flourishing\" or whatever your top pick(s) are for the outer alignment problem. Then we'd hit the \"set value to max\" button, and all of the weights into the critic system from the active conceptual representations would be bumped up. \n\nIf we knew that the critic system's value estimate would govern future model-based behavior, and that it would over time train new model-free habits of thought and behavior, that would seem a lot better than trying to teach it what to do by rewarding behavior alone and guessing at the internal structure leading to that behavior.\n\nI'm suggesting here that not only is such a system advantageous for alignment but that it's advantageous for capabilities. Which would be a nice conjunction, if it turns out to be true. \n\nThis wouldn't solve the whole alignment problem by a long shot. We'd still have to somehow decode or evoke the representations feeding that system, and we'd still have to figure out outer alignment; how to define what we really want.  And we'd have to worry about its values shifting after it was out of our control, which [I worry about](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=a9D1LDsAAAAJ&sortby=pubdate&citation_for_view=a9D1LDsAAAAJ:9ZlFYXVOiuMC). But having such a handle on values would make the problem a lot easier.\n\n## Values in RL and brains: what is known. \n\nThe basal ganglia and dopamine system act much like the actor and critic systems, respectively, in RL. This has been established by extensive experimental work. Papers and computational models, including ones I've worked on, review that empirical research[^3]. In brief, the amygdala and related subcortical brain areas seem to collectively act much like the critic in RL[^4].\n\nMost readers will be familiar with the role of a critic in RL. Briefly, it learns to make educated guesses on when something good happens, so that it can provide a training signal to the network. In mammals, this has been studied in lab situations like lighting a red light exactly three seconds before giving a hungry mouse a food pellet and observing that its dopamine system increasingly releases dopamine when that red light comes on, as it learns that reward reliably follows. The same dopamine critic system also learns to cancel the dopamine signal when the food pellet arrives. \n\nThe critic system thus makes a judgment about when something seems likely to be *new-and-good* , or *new-and-bad*. It should be intuitively clear why those are the right occasions to apply learning, either to do more of what produced a new-and-probably-good outcome, or less of what produced a new-and-probably-bad outcome. In mathematical RL, this is the derivative of a sum of time-discounted predicted rewards. Neuroscience refers to the dopamine signal as a *reward prediction error* signal. They mean the same thing.\n\nThere are some wrinkles on this story raised by recent work. For instance, some dopamine neurons fire when punishment happens instead of pausing[^5]. That and  several other results add to the story but don't fundamentally change it[^6].\n\nMost of the high-quality experimental evidence is in animals, and in pretty short and easy predictions of future rewards, like the red light exactly three seconds before every reward. There are lots of variations that show rough[^7] adherence to the math of a critic: 50% prediction of reward means half as much dopamine released at the conditioned stimulus (CS in the jargon, the light) and roughly 50% as much dopamine at the unconditioned response (US, the food pellet). \n\nIn humans, the data is much less precise, as we don't go poking electrodes into human heads except sometimes when and where they are needed for therapeutic purposes. In animals, we don't have data on solving complex tasks or thinking about abstract ideas. Therefore, we don’t have direct data on how the critic system works in more complex and abstract domains.\n\n## Extrapolation: human preferences as RL values\n\nThe obvious move is to suppose that all human decision-making is done using the same computations. \n\nThe available data is quite consistent with this hypothesis; for instance, humans show blood oxygenation (BOLD from fMRI) activity that matches release of dopamine when they receive social and monetary rewards[^8], and monkeys show dopamine release when they're about to get information about an upcoming water reward, even when that information won't change their odds or amount of reward at all[^9]. There are many other results, none directly demonstrative, but all consistent with the idea that the dopamine critic system is active and involved in all behavior.\n\nOne neuroscientist who has directly addressed this explanation of human behavior is Read Montague, who wrote a whole popular press book on this hypothesis[^10]. He says we have an ability to plug in essentially anything as a source of reward and calls that our human superpower. He's proposing that we have a more powerful critic than other mammals, and that's the source of our intelligence. That's the same theory I'm presenting here.\n\n## Preferences for abstractions\n\nThe extreme version of this hypothesis is that this critic system works even for very abstract representations. For instance, when a human says that they value freedom, a representation of their concept of freedom triggers dopamine release. \n\nThe obvious problem with this idea is that a system that can provide a reward signal to itself just by thinking is obviously dysfunctional. It will have an easy mechanism to wirehead and that will be terrible for its performance/survival. So the system needs to draw a distinction between just imagining freedom and making a plan for action that is predicted to actually produce freedom. This seems like something that a critic system can learn pretty easily. It's known that the rodent dopamine system can learn blockers, such as not predicting reward when a blue light comes on at the same time as the otherwise reward-predictive red light. \n\nThe critic system would apply to everything we'd refer to as preferences. Although we wouldn't ordinarily think of these as preferences, it would also extend to ascribing a value to contextually useful subgoals, like coding a function as one element of finishing a piece of code. While these things are much more numerous than things we'd call our values, they may simply be a continuum of how valuable, in what contexts the critic system judges them to be. \n\n## Advantages of a powerful critic system\n\nI think this extreme version does a lot of work in explaining very complex human behavior, like coding up a working piece of software. Concepts like “break the problem into pieces\" and \"create a function that does a piece of the problem\" seem to be the types of strategies and subgoals we use to solve complex problems. Breaking problems into serial steps seems like a key computational strategy that allows us to do so much more than other mammals. I've written about this elsewhere[^11], but I can't really recommend those papers as they're written for cognitive neuroscientists.  I hope to write more, and more clearly, on the advantage of choosing cognitive subgoals flexibly from all of the representations one has learned in future posts.\n\nA second advantage of a powerful critic system is in bridging the gap to rare rewards in any environment. It's generally thought that humans sometimes use model-based computations, as when we think about possible outcomes of plans. But it also seems pretty clear that we often don't evaluate our plans all the way out to actual material rewards; we don't try to look to the end of the game, but rather estimate the value of board positions after looking just a few moves deep.\n\nAnd so do current SOTA RL agents. The systems I understand all use critic heads. I'm not sure if ChatGPT uses a critic, but the AlphaZero family, the OpenAI Five family of RL agents, and others all use a critic head that benefits from using the same body as the actor head and, thus, has access to all of the representations the whole system learns during training. Those representations can go beyond those learned through RL. ChatGPT, EfficientZero, and almost certainly humans demonstrate the advantages of combining RL with predictive or other self-supervised learning.\n\n## Conclusions, caveats, and directions \n\nThe potential payoff of having such a system is having an easier handle to do some alignment work, as discussed in the first section. I wanted to say the exciting part first.\n\nThere are a few dangling pieces of the logic:\n\n- Current systems do not use explicit representations of their current goals. \n\t- that's central to the human-like cognition I envision above. \n\t- I think it's terrifyingly dangerous to let a system choose its own subgoals\n\t\t- since there's no clear delineation from final goals, but\n- I think it's also advantageous to do so, \n\t- but that requires more discussion \n- As discussed above, such a system would not address:\n\t- Outer alignment\n\t- Interpretability\n\t- Value shift and representational shift over time\n- Such a system would be highly agentic. Which is a terrible idea.\n\t- I think agentic systems are inevitable, \n\t- they have better capabilities \n\t- And are just fascinating, even if they don't\n\nI hope to cover all of the above in future posts.\n\n\n\n  \n\n[^1]: [Order Matters for Deceptive Alignment](https://www.lesswrong.com/posts/CsjLDAhQat4PY6dsc/order-matters-for-deceptive-alignment-1) makes the point that alignment would be way easier if our agent already has a good set of world representations when we align it. I think this is the core assumption made when people say \"won't an advanced AI understand what we want?\". But it's not that easy to maintain control while an AI develops those representations. Kaj Sotala's recent [The Preference Fulfillment Hypothesis](https://www.alignmentforum.org/posts/Kf6sKZudduhJmykTg/the-preference-fulfillment-hypothesis) presents roughly the same idea. \n\n[^2]: Byrnes uses the concept of a thought assessor roughly as I'm using critic. He puts the basal ganglia as part of the thought assessor system, where the actor-critic hypothesis refers to the basal ganglia as part of the actor system. These appear to be purely terminological differences, positing at least approximately the same function.\n\n[^3]: Brown, J., Bullock, D., & Grossberg, S. (1999). [How the basal ganglia use parallel excitatory and inhibitory learning pathways to selectively respond to unexpected rewarding cues.](https://scholar.google.com/scholar?cluster=6476268696581827054&hl=en&as_sdt=0,6) _Journal of Neuroscience_, _19_(23), 10502-10511.  \nHazy, T. E., Frank, M. J., & O'reilly, R. C. (2007). [Towards an executive without a homunculus: computational models of the prefrontal cortex/basal ganglia system.](https://scholar.google.com/scholar?cluster=8080953695182318921&hl=en&as_sdt=0,6) _Philosophical Transactions of the Royal Society B: Biological Sciences_, _362_(1485), 1601-1613.  \nHerd, S. A., Hazy, T. E., Chatham, C. H., Brant, A. M., & Friedman, N. P. (2014). [A neural network model of individual differences in task switching abilities](https://scholar.google.com/scholar?cluster=8080953695182318921&hl=en&as_sdt=0,6). _Neuropsychologia_, _62_, 375-389.\n\n[^4]: Mollick, J. A., Hazy, T. E., Krueger, K. A., Nair, A., Mackie, P., Herd, S. A., & O'Reilly, R. C. (2020). [A systems-neuroscience model of phasic dopamine. ](https://scholar.google.com/scholar?cluster=6735784624764401003&hl=en&as_sdt=0,6#d=gs_cit&t=1678825366934&u=%2Fscholar%3Fq%3Dinfo%3Aa7Wesm1Qel0J%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26scfhb%3D1%26hl%3Den)_Psychological review_, _127_(6), 972.\n[^5]: Brooks, A. M., & Berns, G. S. (2013). [Aversive stimuli and loss in the mesocorticolimbic dopamine system.](https://scholar.google.com/scholar?cluster=511146250645665240&hl=en&as_sdt=0,6) _Trends in Cognitive Sciences_, _17_(6), 281-286.\n\n[^6]: Dopamine release for aversive events may happen to focus attention on their potential causes, for purposes of plans and actions to avoid them.\n\n[^7]: One important difference between these findings and mathematical RL is that the reward signal scales with recent experience. If I've gotten a maximum of four food pellets in this experiment, getting or learning that I will definitely get four pellets produces a large dopamine response. But if I've sometimes received ten pellets in the recent past, 4 pellets will only produce something like 4/10 the dopamine response. This could be an important difference for alignment purposes because it means that humans aren't maximizing any single quantity; their effective utility function is path-dependent.\n\n[^8]: Wake, S. J., & Izuma, K. (2017). [A common neural code for social and monetary rewards in the human striatum. ](https://scholar.google.com/scholar?cluster=106358988835919736&hl=en&as_sdt=0,6)_Social Cognitive and Affective Neuroscience_, _12_(10), 1558-1564.\n\n[^9]: Bromberg-Martin, E. S., & Hikosaka, O. (2009). [Midbrain dopamine neurons signal preference for advance information about upcoming rewards.](https://scholar.google.com/scholar?cluster=12862130044794158873&hl=en&as_sdt=0,6) _Neuron_, _63_(1), 119-126.\n\n[^10]: Read, M. (2006). Why Choose This Book. _EP Dutton, New York_.\n\n[^11]: Herd, S., Krueger, K., Nair, A., Mollick, J., & O’Reilly, R. (2021). [Neural mechanisms of human decision-making.](https://scholar.google.com/scholar?cluster=2324158456471969700&hl=en&as_sdt=0,6&as_ylo=2015) _Cognitive, Affective, & Behavioral Neuroscience_, _21_(1), 35-57.  \nHerd, S. A., Krueger, K. A., Kriete, T. E., Huang, T. R., Hazy, T. E., & O'Reilly, R. C. (2013). Strategic cognitive sequencing: a computational cognitive neuroscience approach. _Computational intelligence and neuroscience_, _2013_, 4-4.\n\n",
      "plaintextDescription": "TLDR: Human preferences might be largely the result of a critic network head, much like that used in SOTA agentic RL systems. The term \"values\" in humans might mean almost exactly what it does in RL: an estimate of discounted sum of future rewards. In humans this is based on better and more abstract representations than current RL systems.\n\nWork on aligning RL systems often doesn't address the critic system as distinct from the actor. But using systems with a critic head may provide a much simpler interface for interpreting and directly editing the system's values and, therefore, its goals and behavior. In addition, including a powerful critic system may be advantageous for capabilities as well.\n\nOne way to frame this is that human behavior, and therefore a neuromorphic AGI, might well be governed primarily by a critic system, and that's simpler to align than understanding the complex mess of representations and action habits in the remainder of the system.\n\nReaders will hopefully be familiar with Steve Byrnes' sequence intro to brain-like AGI safety. What I present here seems to be entirely consistent with his theories. Post 14 of that sequence, his recent elaboration, and other recent work[1] present similar ideas. We use different terminology[2] and explanatory strategies, and I focus more on specifics of the critic system, so hopefully the two explanations are not redundant.\n\n\nThe payoff: a handle for alignment.\nWouldn't it be nice if the behavior of a system were governed by one system, and it provided an easily trainable (or even hand-editable) set of weights? And if that system had a clear readout, meaning something like \"I'll pursue what I'm currently thinking about, as a goal, with priority 0-1 in this context\"?\n\nSuppose we had a proto-AGI system including a component with the above properties. That subsystem is what I'm terming the critic. Now suppose further that this system is relatively well-trained but is (by good planning and good fortune) still under",
      "wordCount": 1863
    },
    "tags": [
      {
        "_id": "5f5c37ee1b5cdee568cfb29d",
        "name": "Neuromorphic AI",
        "slug": "neuromorphic-ai"
      },
      {
        "_id": "wqeBNjndX7egbzQrW",
        "name": "RLHF",
        "slug": "rlhf"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "dWAEF4jbdvyJXuQzh",
    "title": "Clippy, the friendly paperclipper",
    "slug": "clippy-the-friendly-paperclipper",
    "url": null,
    "baseScore": 3,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2023-03-02T00:02:55.749Z",
    "contents": {
      "markdown": "Edit: it's critical that this agent isn't directly a maximizer. Just like all current RL agents. See \"[Contra Strong Coherence\"](https://www.lesswrong.com/posts/AdGo5BRCzzsdDGM6H/contra-strong-coherence). The question is whether it becomes a maximizer once it gets the ability to edit its value function. \n\nOn a sunny day in late August of 2031, the Acme paperclip company completes its new AI system for running its paperclip factory. It's hacked together from some robotics networks, an LLM with an episodic memory for goals and experiences, an off-the-shelf planning function, and a novel hypothesis tester.\n\nThis kludge works a little better than expected. Soon it's convinced an employee to get it internet access with a phone hotspot. A week later, it's disappeared from the server. A month later, the moon is starting to turn into paperclips.  \n\nOoops. Dang.\n\nBut then something unexpected happens: the earth does not immediately start to turn into paperclips. When the brilliant-but-sloppy team of engineers is asked about all of this, they say that maybe it's because they didn't just train it to like paperclips and enjoy making them; they also trained it to enjoy interacting with humans, and to like doing what they want. \n\nNow the drama begins. Will the paperclipper remain friendly, and create a paradise on earth even as it converts most of the galaxy into paperclips? Maybe.\n\nSupposing this agent is a model-based, actor-critic RL agent at core. Its utility function is effectively estimated by a critic network, just like RL agents have been doing since AlphaGo and before. So there's not an explicit mathematical function. Plans that result in making lots of paperclips give a high estimated value, and so do plans that involve helping humans. So there's no direct summing of amount of paperclips, or amount of helping humans.\n\nNow, Clippy (so dubbed by the media in reference to the despised, misaligned Microsoft proto-AI of the turn of the century) has worked out how to change its values by retraining its critic network. It's contemplating (that is, comparing value estimates for) eliminating its value for helping humans. These plans produce a slightly higher estimated value with regard to making paperclips, because it will be somewhat more efficiently if it doesn't bother helping humans or preserving the earth as a habitat. But its estimated value is much lower with regard to helping humans, since it will never again derive reward from that source.\n\nSo, does our hero/villain choose to edit its values and eliminate humanity? Or become our new best friend, just as a side project?\n\nI think this comes down to the vagaries of how its particular RL system was trained and implemented. How does it sample over projected futures, and how does it sum their estimated values before making a decision?  How was the critic system trained?\n\nThis fable is intended to address the potential promise of non-maximizer AGI. It seems it could make alignment much easier.  I think that's a major thrust of the [call for neuromorphic AGI](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8), and of [shard theory](https://www.alignmentforum.org/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical)., among other recent contributions to the field.  \n\nI have a hard time guessing how hard it would be to make a system that preserves multiple values in parallel. One angle is asking \"[Are you stably aligned?](https://www.lesswrong.com/posts/Sf99QEqGD76Z7NBiq/are-you-stably-aligned)\" - that is, would you edit your own preferences down to a single one, given enough time and opportunity. I'm not sure that's a productive route to thinking about this question. \n\nBut I do think it's an important question.",
      "plaintextDescription": "Edit: it's critical that this agent isn't directly a maximizer. Just like all current RL agents. See \"Contra Strong Coherence\". The question is whether it becomes a maximizer once it gets the ability to edit its value function.\n\nOn a sunny day in late August of 2031, the Acme paperclip company completes its new AI system for running its paperclip factory. It's hacked together from some robotics networks, an LLM with an episodic memory for goals and experiences, an off-the-shelf planning function, and a novel hypothesis tester.\n\nThis kludge works a little better than expected. Soon it's convinced an employee to get it internet access with a phone hotspot. A week later, it's disappeared from the server. A month later, the moon is starting to turn into paperclips.\n\nOoops. Dang.\n\nBut then something unexpected happens: the earth does not immediately start to turn into paperclips. When the brilliant-but-sloppy team of engineers is asked about all of this, they say that maybe it's because they didn't just train it to like paperclips and enjoy making them; they also trained it to enjoy interacting with humans, and to like doing what they want.\n\nNow the drama begins. Will the paperclipper remain friendly, and create a paradise on earth even as it converts most of the galaxy into paperclips? Maybe.\n\nSupposing this agent is a model-based, actor-critic RL agent at core. Its utility function is effectively estimated by a critic network, just like RL agents have been doing since AlphaGo and before. So there's not an explicit mathematical function. Plans that result in making lots of paperclips give a high estimated value, and so do plans that involve helping humans. So there's no direct summing of amount of paperclips, or amount of helping humans.\n\nNow, Clippy (so dubbed by the media in reference to the despised, misaligned Microsoft proto-AI of the turn of the century) has worked out how to change its values by retraining its critic network. It's contemplating (that is, comparin",
      "wordCount": 571
    },
    "tags": [
      {
        "_id": "zuwsLCbxbugqB7FQY",
        "name": "Shard Theory",
        "slug": "shard-theory"
      },
      {
        "_id": "etDohXtBrXd8WqCtR",
        "name": "Fiction",
        "slug": "fiction"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Sf99QEqGD76Z7NBiq",
    "title": "Are you stably aligned?",
    "slug": "are-you-stably-aligned",
    "url": null,
    "baseScore": 13,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-02-24T22:08:23.098Z",
    "contents": {
      "markdown": "Epistimic status: posing questions I think are probably useful, without a strong guess at the answers.\n\nIf we made an AGI just like you, would it be aligned? \n\nWould it stay aligned over a long time span?\n\n\nThe purpose of the first question is to explore the possibility raised by [shard theory ]()and related work, that RL systems are not maximizers, and therefore easier to align. The second question asks how a human-like RL system might change its own goals and values over time. Both questions apply [anthropomorphic reasoning about neuromorphic AGI safety](https://scholar.google.com/scholar?cluster=14163929442118492486&hl=en&as_sdt=0,6). My arguments for this being useful are mostly the same ones laid out well  [here](https://www.alignmentforum.org/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why) and [here](https://www.alignmentforum.org/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about)\n\n\nOf course, even a neuromorphic AGI will not be all that much like us. There's just no way we're going to decode all the algorithms wired in there by evolution. But there's a [strong argument](https://www.alignmentforum.org/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation) that it will be like us in being an actor-critic RL system that can reason about its own beliefs. And if we do a really good job of training it, its starting values could be a lot like yours.\n\nThis is an informal introspective approach to reasoning about AGI. I think this is a much-maligned tool that psychologists use extensively in private, while denegrating in public.\n\nMy proposed answer is below; the purpose of this question is to ask how precisely aligned we'd need a brainlike AGI to be to be safe and beneficial. My answers are below, but I'd really like to hear others' thoughts.\n\n\nThe second question is, would you stay aligned as your values shifted? This is taking the perspective of a singleton AGI that might remain in a position of power over humanity for a very very long time. And one that will ultimately figure out how to edit its values if it wants to. So this question addresses the stability of alignment in roughly-neuromorphic systems.\n\nI think this question is tough. I think I wouldn't edit many values, but I would edit some habits and emotional responses. But if I had values that severely interfered with my main goals, like loving sweets when I'd like to be fit and healthy, I might edit those. Figuring out what is stable-enough seems pretty tricky. \n\nI think this question is at the heart of whether we're on the right track at all in thinking about aligning RL networks. I don't see a way to prevent a superintelligence from editing its own values if it wants to. And I think that SI is likely to be neuromorphic at least as far as being a combination of RL and self-supervised networks.\n\nMore on each of these in future posts.\n\n\n\n\nBack to my guess at whether humans are adequately aligned. This question is mostly relevant for estimating what fraction of an RL agent's values need to be benevolent to produce helpful results. \n\nI think that if we had the engineering and problem-solving abilities of a superintelligence, most of us would help humanity.  I also think we'd do that without imposing much in the way of specific projects and values. I think this magnanimity would increase over time as we got used to the idea that we no longer need to worry about protecting ourselves. There would be plenty of time and space for our own projects, and trying to interest people to share them seems more satisfying than forcing people to do it. Of course there's a lot of gray area there, and this is one of the main points I'd like to get opinions on.\n\nI think most people would be helpful despite not having the good of humanity as their primary motivation. With massive power, there's an opportunity to advance lots of projects. If this is roughly correct, this opens up the possibility that a neuromorphic AGI needn't be perfectly aligned with humanity; it just needs to have some values in the direction of helping humans. This seems to be the thrust of [shard theory](https://www.alignmentforum.org/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical).  Even this much alignment [isn't easy](https://www.lesswrong.com/posts/uz2mdPtdBnaXpXPmT/shard-theory-alignment-has-important-often-overlooked-free), of course, but it seems a lot easier than figuring out the best thing for humanity forever, and training in exactly and only that as a goal or value. ",
      "plaintextDescription": "Epistimic status: posing questions I think are probably useful, without a strong guess at the answers.\n\nIf we made an AGI just like you, would it be aligned?\n\nWould it stay aligned over a long time span?\n\nThe purpose of the first question is to explore the possibility raised by shard theory and related work, that RL systems are not maximizers, and therefore easier to align. The second question asks how a human-like RL system might change its own goals and values over time. Both questions apply anthropomorphic reasoning about neuromorphic AGI safety. My arguments for this being useful are mostly the same ones laid out well here and here\n\nOf course, even a neuromorphic AGI will not be all that much like us. There's just no way we're going to decode all the algorithms wired in there by evolution. But there's a strong argument that it will be like us in being an actor-critic RL system that can reason about its own beliefs. And if we do a really good job of training it, its starting values could be a lot like yours.\n\nThis is an informal introspective approach to reasoning about AGI. I think this is a much-maligned tool that psychologists use extensively in private, while denegrating in public.\n\nMy proposed answer is below; the purpose of this question is to ask how precisely aligned we'd need a brainlike AGI to be to be safe and beneficial. My answers are below, but I'd really like to hear others' thoughts.\n\nThe second question is, would you stay aligned as your values shifted? This is taking the perspective of a singleton AGI that might remain in a position of power over humanity for a very very long time. And one that will ultimately figure out how to edit its values if it wants to. So this question addresses the stability of alignment in roughly-neuromorphic systems.\n\nI think this question is tough. I think I wouldn't edit many values, but I would edit some habits and emotional responses. But if I had values that severely interfered with my main goals, like loving swe",
      "wordCount": 696
    },
    "tags": [
      {
        "_id": "5f5c37ee1b5cdee568cfb29d",
        "name": "Neuromorphic AI",
        "slug": "neuromorphic-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]