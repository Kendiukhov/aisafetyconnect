[
  {
    "_id": "yosH75AXdMTMtzecM",
    "title": "AI development as the first fully-automated job",
    "slug": "ai-development-as-the-first-fully-automated-job",
    "url": null,
    "baseScore": 17,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-13T16:45:12.609Z",
    "contents": {
      "markdown": "I used to think of AI development as obviously being the last fully-automated job. After all, AI can be used to automate other jobs, so once it is automated, all those other jobs can be automated too. But with the current data-hungry methods in AI, it might take a long time for the relevant data to come in, even if the AI is automatically collecting it.\n\nPlus, the AI might need to collaborate with people who are working in the field it is automating in order to get the relevant data, and maybe they are scared of their skills getting automated and will try to stall for as long as possible.\n\nPeople working in AI accept the risk of getting their skills automated, and constantly contribute to automating whatever they can. But obviously the field they are most familiar with is AI development, so AI development is what receives the biggest effort for automation.",
      "plaintextDescription": "I used to think of AI development as obviously being the last fully-automated job. After all, AI can be used to automate other jobs, so once it is automated, all those other jobs can be automated too. But with the current data-hungry methods in AI, it might take a long time for the relevant data to come in, even if the AI is automatically collecting it.\n\nPlus, the AI might need to collaborate with people who are working in the field it is automating in order to get the relevant data, and maybe they are scared of their skills getting automated and will try to stall for as long as possible.\n\nPeople working in AI accept the risk of getting their skills automated, and constantly contribute to automating whatever they can. But obviously the field they are most familiar with is AI development, so AI development is what receives the biggest effort for automation.",
      "wordCount": 154
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FFbESWKJ3t8YhfMQX",
    "title": "Against Infrabayesianism",
    "slug": "against-infrabayesianism",
    "url": null,
    "baseScore": -16,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-05T08:03:43.683Z",
    "contents": {
      "markdown": "Infrabayesians assume they're in conflict with the world and I find that pretty sus.",
      "plaintextDescription": "Infrabayesians assume they're in conflict with the world and I find that pretty sus.",
      "wordCount": 14
    },
    "tags": [
      {
        "_id": "wdLqQnzdgiYpDXEWH",
        "name": "Infra-Bayesianism",
        "slug": "infra-bayesianism"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "TCmj9Wdp5vwsaHAas",
    "title": "Knocking Down My AI Optimist Strawman",
    "slug": "knocking-down-my-ai-optimist-strawman",
    "url": null,
    "baseScore": 31,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2025-02-08T10:52:33.183Z",
    "contents": {
      "markdown": "I recently posted [my model of an optimistic view of AI](https://www.lesswrong.com/posts/Rk4swqiy5wCh6jY29/my-mental-model-of-ai-optimist-opinions), asserting that I disagree with every sentence of it. I thought I might as well also describe my objections to those sentences:\n\n### \"The rapid progress spearheaded by OpenAI is clearly leading to artificial intelligence that will soon surpass humanity in every way.\"\n\nHere's some of the main things humanity should want to achieve:\n\n*   Curing aging and other diseases\n*   Plentiful clean energy from e.g. nuclear fusion\n*   De-escalating nuclear MAD while extending world peace and human freedom\n*   ... even if hostile nations would use powerful unaligned AI[^kfjy5uatog] to fight you\n*   Stopping criminals, even if they would make powerful unaligned AI[^kfjy5uatog] to fight you\n*   Educating people to be great and patriotic\n*   Creating healthy, tasty food without torturing animals\n*   Nice homes for humans near important things\n*   Good, open channels for honest, valuable communication\n*   Common knowledge of the virtues and vices of executives, professionals, managers, politicians, and various other groups and people of interest\n\nWe already have humans working on this, based on the assumption that humans have what it takes to contribute to these. Do large multimodal models seem to move towards being able to take over here? Mostly I don't see it - and the few times I see it, there's as good reason to think this will cause regress as that it will cause progress:\n\n### \"People used to be worried about existential risk from misalignment, yet we have a good idea about what influence current AIs are having on the world, and it is basically going fine.\"\n\nWe have basically no idea how AI is influencing the world.\n\nLike yes, we can come up with spot checks to see what the AI writes when it is prompted in a particular way. But we don't have a good overview over the things it is prompted to in practice, or how most humans use these prompts. Even if we had a decent approximation for that, we don't have a great way to evaluate what parts really add up to problems, and approximations intrinsically break down in the case of long tails.\n\nOf course the inability to work out problems from first principles is a universal issue, so in practice bad things get detected via root-cause analyses of problems. This can be somewhat difficult because some of the problems are cases where the people in question are incentivized to hide it. But we do have some examples:\n\n*   Personalized tutoring was one of the most plausible contributions of LLMs, which could have contributed to \"Educating the young to be great\" in my previous list, but instead in practice LLMs seem to be more used to skip learning, and the things they teach are often still slop. It seems quite plausible that LLMs are making education worse rather than better.\n*   Automatic moderation was also one of the most plausible contributions of AI, which could have contributed to \"Good, open channels for honest, valuable communication\", but anecdotally, spam seems to have gone up, and platforms seem to have become more closed, indicating that current AI technology really is making this issue much worse.\n\nThe error is linked to assumptions about the agency of the AI. Like it's assumed that if the AI seems to be endorsing nice values and acting according to them when sampled, then this niceness will add up over all of the samplings. But LLMs don't have much memory or context-awareness, so they can't apply their agency *across* different uses very well. Instead, the total effect of the AI is determined by environmental factors distinct from its values, especially by larger-scale agents that are capable of manipulating the AIs. (This is presumably going to change when AI gets strengthened in various ways.)\n\nJust to emphasize, this doesn't necessarily mean that AI is net bad, just that we *don't know* how good/bad AI is. Recently society kind of seems to have gotten worse, but it seems to me like that's not driven mainly by AI.\n\n### \"The root problem is that The Sequences expected AGI to develop agency largely without human help; meanwhile actual AI progress occurs by optimizing the scaling efficiency of a pretraining process that is mostly focus on integrating the AI with human culture.\"\n\nLarge multimodal models are good at simple data transformations and querying common knowledge. I'm sure optimizing the scaling efficiency of pretraining processes will make them even better at that.\n\nHowever, this is still [mostly just copying humans](https://www.lesswrong.com/posts/fLHyBpAptikZHi8nB/where-do-you-get-your-capabilities-from), and for the ambitious achievements I mentioned in the beginning of the post, copying humans doesn't seem to be enough. E.g. to build a fusion power plant, we'd need real technical innovations. If these are supposed to be made by a superhuman AI, it needs to be able to go beyond just copying the innovations humans have already come up with.\n\nSo if we imagine AI as a tool that makes it easier to process and share certain kinds of information, then sure, improving scaling efficiency is how you develop AI, but that's not the sort of thing the original arguments about existential risk concern, and we have good reasons to believe that AI will be developed with more ambitious methods too. These \"good reasons\" mostly boil down to adversarial relationships; spammers, propagandists, criminals and militaries will want to use AI to become stronger, and we need to be able to fight that, which also requires AI.\n\n### \"This means we will be able to control AI by just asking it to do good things, showing it some examples and giving it some ranked feedback.\"\n\nRLHF trains an AI to do things that look good to humans. This makes it much harder to control because it makes it makes it hide anything bad. Also, RLHF is kind of a statistical approach, which makes it work better for context-independent goodness, whereas often the hard part is recognizing rare forms of goodness. (Otherwise you just end up with very generic stuff.)\n\nExamples/prompt engineering requires the AI to work by copying humans, which to some extent I addressed in the previous section. The primary danger of AI is not when it does things humans understand well, but rather when it does things that are beyond the scale or abilities of human understanding.\n\n### \"You might think this is changing with inference-time scaling, yet if the alignment would fall apart as new methods get taken into use, we'd have seen signs of it with o1.\"\n\no1-style training is not optimizing against the real world to handle long-range tasks, so instrumental convergence does not apply there. You need to consider the nuances of the method in order to be able to evaluate whether the alignment properties of current methods will fall apart. In particular it gets more problematic as optimization against adversaries gets involved.\n\n### \"In the unlikely case that our current safety will turn out to be insufficient, interpretability research has worked out lots of deeply promising ways to improve, with sparse autoencoders letting us read the minds of the neural networks and thereby screen them for malice, and activation steering letting us deeply control the networks to our hearts content.\"\n\nSAEs and activation steering focus on the level of individual tokens or text generations, rather than on the overall behavior of the network. Neither of them can contribute meaningfully to current alignment issues like improving personalized tutoring because it occurs on a much broader level than tokens, so we shouldn't expect them to scale to more difficult issues like keeping down crime or navigating international diplomacy.\n\n### \"AI x-risk worries aren't just a waste of time, though; they are dangerous because they make people think society needs to make use of violence to regulate what kinds of AIs people can make and how they can use them.\"\n\nObviously there will be some very bad ways to make and use AI, and we need norms against it. Violence is the ultimate backstop for norm enforcement: it's called the police and the military.\n\n### \"This danger was visible from the very beginning, as alignment theorists thought one could (and should) make a singleton that would achieve absolute power (by violently threatening humanity, no doubt), rather than always letting AIs be pure servants of humanity.\"\n\nIt seems extremely valid to be concerned about AI researchers (including those with an alignment focus) aspiring to conquer the world (or to make something that conquers the world). However, always having humans on top won't be able to deal with the rapid and broad action that will be needed against AI-enabled adversaries.\n\nTraditionally the ultimate backstop for promoting human flourishing was that states were reliant on men in the military, so if those men were incapacitated or did not see value in the state they were fighting for, the states would be weaker. This incentivized the states to develop things that helped the men in their military, and made states which failed to do so get replaced by states that did so.\n\nThis backstop has already been weakening with more advanced weaponry and more peace. Eventually all fighting will be done by drones rather than by people, at which point the backstop will be nearly gone. (Of course there's also the manufacturing and programming of the drones, etc..) This lack of backstop is the longest-term alignment problem, and if it fails there's endless ways most value could be destroyed, e.g.:\n\n*   The machinery of war (mining, manufacturing, targeting, ...) has been fully automated, and a death cult (like Hamas or the Zizians) develops in the upper ranks of some military (would probably require a situation like Russia because death cults usually develop from external pressure?), and they destroy the world.\n*   World peace is achieved, and the world elites are heavily filtered through processes that make them obsessed with superficial appearances rather than what is really going on, and they use their policing power to do that to everyone else too. (Imagine social credit scores that subtract points whenever you frown.)\n*   All production is fully automated and the human reward system gets so fully reverse engineered that everyone spends all their time watching what basically amounts to those Tik-Toks that layer some attractive commentary (e.g. a joke) on top of some attractive video (e.g. satisfying hydraulic press moments).\n\n### \"To \"justify\" such violence, theorists make up all sorts of elaborate unfalsifiable and unjustifiable stories about how AIs are going to deceive and eventually kill humanity, yet the initial deceptions by base models were toothless, and thanks to modern alignment methods, serious hostility or deception has been thoroughly stamped out.\"\n\nAI optimists been totally knocked out by things like RLHF, becoming overly convinced of the AI's alignment and capabilities just from it acting apparently-nicely. This is an a form of deceptive alignment, just in a \"law of earlier failure\" sense as the AIs that knocked them out are barely even agentic.\n\n[^kfjy5uatog]: \"But wouldn't it just be aligned to them, rather than unaligned?\" Sometimes, presumably especially with xrisk-pilled adversaries. But some adversaries won't be xrisk-pilled and instead will be willing to use more risky strategies until they win. So you either need to eliminate them ahead of time or be able to destroy the unaligned AIs.",
      "plaintextDescription": "I recently posted my model of an optimistic view of AI, asserting that I disagree with every sentence of it. I thought I might as well also describe my objections to those sentences:\n\n\n\"The rapid progress spearheaded by OpenAI is clearly leading to artificial intelligence that will soon surpass humanity in every way.\"\nHere's some of the main things humanity should want to achieve:\n\n * Curing aging and other diseases\n * Plentiful clean energy from e.g. nuclear fusion\n * De-escalating nuclear MAD while extending world peace and human freedom\n * ... even if hostile nations would use powerful unaligned AI[1] to fight you\n * Stopping criminals, even if they would make powerful unaligned AI[1] to fight you\n * Educating people to be great and patriotic\n * Creating healthy, tasty food without torturing animals\n * Nice homes for humans near important things\n * Good, open channels for honest, valuable communication\n * Common knowledge of the virtues and vices of executives, professionals, managers, politicians, and various other groups and people of interest\n\nWe already have humans working on this, based on the assumption that humans have what it takes to contribute to these. Do large multimodal models seem to move towards being able to take over here? Mostly I don't see it - and the few times I see it, there's as good reason to think this will cause regress as that it will cause progress:\n\n\n\"People used to be worried about existential risk from misalignment, yet we have a good idea about what influence current AIs are having on the world, and it is basically going fine.\"\nWe have basically no idea how AI is influencing the world.\n\nLike yes, we can come up with spot checks to see what the AI writes when it is prompted in a particular way. But we don't have a good overview over the things it is prompted to in practice, or how most humans use these prompts. Even if we had a decent approximation for that, we don't have a great way to evaluate what parts really add up to problems,",
      "wordCount": 1828
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Rk4swqiy5wCh6jY29",
    "title": "My Mental Model of AI Optimist Opinions",
    "slug": "my-mental-model-of-ai-optimist-opinions",
    "url": null,
    "baseScore": 13,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2025-01-29T18:44:36.485Z",
    "contents": {
      "markdown": "*Epistemic status: The text below is a sort of strawman of AI optimists, where I took my mental model for how I disagree with rationalist AI optimists and cranked it up to 11. I personally disagree with every sentence below, and I'm posting it here because I'm interested in whether AI optimists have any major corrections they want to make in the comments. Of course I understand that everyone has their own unique opinion and so I would expect every AI optimist to at least disagree with some parts of it too.*\n\nThe rapid progress spearheaded by OpenAI is clearly leading to artificial intelligence that will soon surpass humanity in every way. People used to be worried about existential risk from misalignment, yet we have a good idea about what influence current AIs are having on the world, and it is basically going fine.\n\nThe root problem is that The Sequences expected AGI to develop agency largely without human help; meanwhile *actual* AI progress occurs by optimizing the scaling efficiency of a pretraining process that is mostly focus on integrating the AI with human culture. This means we will be able to control AI by just asking it to do good things, showing it some examples and giving it some ranked feedback.\n\nYou might think this is changing with inference-time scaling, yet if the alignment would fall apart as new methods get taken into use, we'd have seen signs of it with o1. In the unlikely case that our current safety will turn out to be insufficient, interpretability research has worked out lots of deeply promising ways to improve, with sparse autoencoders letting us read the minds of the neural networks and thereby screen them for malice, and activation steering letting us deeply control the networks to our hearts content.\n\nAI x-risk worries aren't just a waste of time, though; they are dangerous because they make people think society needs to make use of violence to regulate what kinds of AIs people can make and how they can use them. This danger was visible from the very beginning, as alignment theorists thought one could (and should) make a singleton that would achieve absolute power (by violently threatening humanity, no doubt), rather than always letting AIs be pure servants of humanity.\n\nTo \"justify\" such violence, theorists make up all sorts of elaborate unfalsifiable and unjustifiable stories about how AIs are going to deceive and eventually kill humanity, yet the initial deceptions by base models were toothless, and thanks to modern alignment methods, serious hostility or deception has been thoroughly stamped out.",
      "plaintextDescription": "Epistemic status: The text below is a sort of strawman of AI optimists, where I took my mental model for how I disagree with rationalist AI optimists and cranked it up to 11. I personally disagree with every sentence below, and I'm posting it here because I'm interested in whether AI optimists have any major corrections they want to make in the comments. Of course I understand that everyone has their own unique opinion and so I would expect every AI optimist to at least disagree with some parts of it too.\n\nThe rapid progress spearheaded by OpenAI is clearly leading to artificial intelligence that will soon surpass humanity in every way. People used to be worried about existential risk from misalignment, yet we have a good idea about what influence current AIs are having on the world, and it is basically going fine.\n\nThe root problem is that The Sequences expected AGI to develop agency largely without human help; meanwhile actual AI progress occurs by optimizing the scaling efficiency of a pretraining process that is mostly focus on integrating the AI with human culture. This means we will be able to control AI by just asking it to do good things, showing it some examples and giving it some ranked feedback.\n\nYou might think this is changing with inference-time scaling, yet if the alignment would fall apart as new methods get taken into use, we'd have seen signs of it with o1. In the unlikely case that our current safety will turn out to be insufficient, interpretability research has worked out lots of deeply promising ways to improve, with sparse autoencoders letting us read the minds of the neural networks and thereby screen them for malice, and activation steering letting us deeply control the networks to our hearts content.\n\nAI x-risk worries aren't just a waste of time, though; they are dangerous because they make people think society needs to make use of violence to regulate what kinds of AIs people can make and how they can use them. This danger was visible fro",
      "wordCount": 430
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "q8N9bDPqPnE8nxqbs",
    "title": "Evolution's selection target depends on your weighting",
    "slug": "evolution-s-selection-target-depends-on-your-weighting",
    "url": null,
    "baseScore": 23,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2024-11-19T18:24:53.117Z",
    "contents": {
      "markdown": "I think it's common on LessWrong to think of evolution's selection target as inclusive genetic fitness - that evolution tries to create organisms which make as many organisms with similar DNA to themselves as possible. But what exactly does this select for? Do humans have high inclusive genetic fitness?\n\nOne way to think of it is that all organisms alive today are \"winners\"/selected-for by that competition, but that seems unreasonable to me, since some individual organisms clearly have genetic disorders or similar which make them unfit according to this criterion.\n\nThere's some sort of consensus that we can assign individual organisms to \"species\", and then we could count it by the number of members of that species. Supposedly, the most numerous species is [Pelagibacter communis](https://en.wikipedia.org/wiki/Candidatus_Pelagibacter_communis), with 10^28 individuals, *vastly* outnumbering humanity. Maybe we could say that this is the selection target of evolution?\n\nOf course as would be expected, pelagibacter is a very minimalist species, being single-celled and having very few genes. This minimalism also makes it hard to notice, to the point where according to Wikipedia, it was first discovered in 1990. (I wonder if there's another species that's smaller, more common, and even harder to notice...) This raises the question of pure numerousity is the correct way of thinking of it.\n\nIf we instead weight by biomass, most life is in the form of plants, and I think more specifically trees. This makes perfect sense to me - trees evolve from a direct competition for height, which is one of the traits most directly related to mass. And in a way, biomass is more sensible to weight by than numerousity, since it is less dependent on the way you slice a species into individual organisms.\n\nBut trees are pretty static. Maybe the problem is that since mass has inertia, this weighting implicitly discourages more dynamic species, like humans? An alternative is to weight by energy flow, but in that case, algae and grasses end up accounting for most of it. Sensible, because if you go up the trophic levels, you rapidly lose energy. That said, energy flow does have the dissatisfying (to me) element that it is \"shared\" between organisms that predate upon each other. I wonder if one could use something like entropy production to get a conceptually similar metric that's more attributable to a single organism.\n\nI don't know of any weightings or metrics where humans are the winners, but it seems likely to me that there is one.",
      "plaintextDescription": "I think it's common on LessWrong to think of evolution's selection target as inclusive genetic fitness - that evolution tries to create organisms which make as many organisms with similar DNA to themselves as possible. But what exactly does this select for? Do humans have high inclusive genetic fitness?\n\nOne way to think of it is that all organisms alive today are \"winners\"/selected-for by that competition, but that seems unreasonable to me, since some individual organisms clearly have genetic disorders or similar which make them unfit according to this criterion.\n\nThere's some sort of consensus that we can assign individual organisms to \"species\", and then we could count it by the number of members of that species. Supposedly, the most numerous species is Pelagibacter communis, with 10^28 individuals, vastly outnumbering humanity. Maybe we could say that this is the selection target of evolution?\n\nOf course as would be expected, pelagibacter is a very minimalist species, being single-celled and having very few genes. This minimalism also makes it hard to notice, to the point where according to Wikipedia, it was first discovered in 1990. (I wonder if there's another species that's smaller, more common, and even harder to notice...) This raises the question of pure numerousity is the correct way of thinking of it.\n\nIf we instead weight by biomass, most life is in the form of plants, and I think more specifically trees. This makes perfect sense to me - trees evolve from a direct competition for height, which is one of the traits most directly related to mass. And in a way, biomass is more sensible to weight by than numerousity, since it is less dependent on the way you slice a species into individual organisms.\n\nBut trees are pretty static. Maybe the problem is that since mass has inertia, this weighting implicitly discourages more dynamic species, like humans? An alternative is to weight by energy flow, but in that case, algae and grasses end up accounting for most o",
      "wordCount": 413
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "eTHvMqxaLsrxtRLq3",
    "title": "Empathy/Systemizing Quotient is a poor/biased model for the autism/sex link",
    "slug": "empathy-systemizing-quotient-is-a-poor-biased-model-for-the",
    "url": null,
    "baseScore": 43,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-11-04T21:11:57.788Z",
    "contents": {
      "markdown": "*Thank you to Justis Millis for providing feedback and proofreading on this post. This post is also available* [*on my Substack*](https://tailcalled.substack.com/p/empathysystemizing-quotient-is-a)*.*\n\n*TL/DR: Contrary to the theory that neurological sex differences and autism both involve the same tradeoff of systemic versus empathic thinking, I found complex differences. It turned out that men were more interested in technology and more disagreeable, whereas autistic people had a narrower focus on details, were more introverted, more socially challenged, and had stronger sensory sensitivity.*\n\nI asked people on Prolific a bunch of questions that are supposed to be related to autism, systemizing and empathy. As a preview before I get into the details, the overall results can be seen here:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F412a8e14-089a-4d84-80a2-ec771c33af47_980x863.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F412a8e14-089a-4d84-80a2-ec771c33af47_980x863.png)\n\nIf there was only a single underlying tradeoff that all of these scales were measuring, then we would expect all of the group differences to be highly correlated, with the items just varying based on how well they “tap into” this underlying tradeoff. As that is not what we see, I think there’s something more complicated going on.\n\nBackground\n----------\n\nThe Empathizing/Systemizing theory of autism asserts that there’s a neurological tradeoff between Empathizing (understanding people’s emotions) and Systemizing (understanding how deterministic, rule-based systems work), and that men and autistic people are more prone to Systemizing, whereas women and allistic people are more prone to Empathizing. Furthermore, it asserts that a deficit in Empathizing or a switch towards Systemizing over Empathizing is not merely a feature of autism, but rather core to what autism *is*.\n\nThe main proponent of this theory is Simon Baron-Cohen, who often justifies it in terms of several psychometric scales he’s made to measure traits like Systemizing, Empathizing or autism. A recent example is [Testing the Empathizing-Systemizing theory of sex differences and the Extreme Male Brain theory of autism in half a million people (2018)](https://pubmed.ncbi.nlm.nih.gov/30420503/), a study where he claims to find support for his theory.\n\nI used to consider the theory plausible, but I ran into several problems. Some formulations of the theory go really heavily into the idea that the Empathizing-Systemizing axis is the core of feminine/masculine psychology, but [that seems somewhat sketchy](https://www.reddit.com/r/slatestarcodex/comments/f8wocm/against_autism_as_extreme_male_brain/). Simon Baron-Cohen’s scales have been criticized for sex bias, but while he came up with a scale [that supposedly balanced out the bias](https://pubmed.ncbi.nlm.nih.gov/16473340/), he didn’t use the standard psychometric methods for testing for sex bias, and when I took a superficial look with those methods, [it still seemed biased](https://www.lesswrong.com/posts/ZjXtjRQaD2b4PAser/a-hill-of-validity-in-defense-of-meaning?commentId=mhnZjEjj9bdHxSFkv). Furthermore, genetic studies of [empathy](https://pure.au.dk/ws/files/125466839/s41398_017_0082_6.pdf) and [systemizing](https://www.biorxiv.org/content/10.1101/050682v3.full.pdf) seem to have found it to be close to independent of autism. As such, I became suspicious the theory might be false.\n\nData\n----\n\nFor the main analysis, I collected a bunch of items supposed to be relevant for measuring autism. The full item list can be seen in the appendix. To get data, I used [Prolific](https://www.prolific.com/) to recruit 100 non-autistic men, 100 non-autistic women, 50 autistic men diagnosed in childhood, 50 in adulthood, 50 autistic women diagnosed in childhood and 50 in adulthood.\n\nI asked the respondents to rate each item on a scale from “Disagree strongly”, “Disagree”, “Neither\", “Agree” to “Agree strongly”. To quantify the data, I mapped these response options to -2, -1, 0, 1, and 2. Also, sometimes to make the computation more convenient, I divided by the standard deviation.\n\nI also had a bunch of ideas for follow-up analyses, but they didn’t really lead anywhere, so I’m not going to publish them here right now. If you want access to the data, contact me and I will provide it.\n\nPrincipal component analysis\n----------------------------\n\n“Empathizing” and “Systemizing” are generally conceived of as quite abstract [general](https://www.lesswrong.com/posts/6q3vr5t9kXFGkHbMC/a-content-analysis-of-the-sq-r-questionnaire-and-a-proposal#A_qualification_on_the_General_Factor_point) traits, but (especially for Systemizing) many of the items are quite concrete and narrow, e.g. “If I were buying a computer, I would want to know exact details about its hard drive capacity and processor speed”.\n\nThis is fine and perfectly intentional. If we expect a general trait to influence many distinct behaviors within a person, we can infer the level of the trait by looking for this overall pattern of behaviors, rather than any one specific behavior. In fact, unless we know the root cause of variation in a general trait, this seems to be the only way to measure a general trait.\n\nOne way to quantify patterns of variation is to use principal component analysis, which lists independent axes in the data in descending order of variance. Because the item responses occur in a bounded range from -2 to 2, no individual item has much variance. Instead, the primary variance occurs because of the correlation between the items (so some people are outliers on many items at once), and therefore principal component analysis narrows in one the dimensions that are relevant to general traits.\n\nAs a sanity check, if I extract the first principal component of all the items, I get a score with a reasonably large separation between the autistic and allistic respondents, though with significant overlap and many outliers:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4edaf75-b1d8-47ec-85f3-c3e62249f37f_586x451.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4edaf75-b1d8-47ec-85f3-c3e62249f37f_586x451.png)\n\nAn overall score like this can be hard to relate to, in my opinion. One thing that helps add semantics to the score is seeing how the score relates to the survey responses, so I’ve invented this new kind of diagram to map that out:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff247639f-ceda-4a6e-a3b3-508f3f628a4b_728x1068.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff247639f-ceda-4a6e-a3b3-508f3f628a4b_728x1068.png)\n\nBasically, at the top of the diagram, you see the distribution of responses for each group. Below this distribution, you see the median item response for people at a given level of the distribution. So for instance, for the item “I find it hard to know what to do in a social situation”, the label below the 5 score is “Disagree”, while the level below the -5 score is “Agree”, corresponding to the median responses from people at scores of 5 and -5 respectively.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19bb75a1-cb9b-4297-b5b7-ea1f06f7f4d8_535x291.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19bb75a1-cb9b-4297-b5b7-ea1f06f7f4d8_535x291.png)\n\nMeasurement invariance\n----------------------\n\nThis whole discussion started because I was concerned about test bias in the autism metrics. To better illustrate this, I have the following plot:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe901c7a6-2170-4cc1-8bec-3fd919a7a85d_954x960.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe901c7a6-2170-4cc1-8bec-3fd919a7a85d_954x960.png)\n\nEach dot in the plot shows an item, and on the y-axis, we see the sex difference for said item. Notably, the sex difference for the item “I am very interested in technology” is very large. That fact alone of course doesn’t intrinsically prove that the item has a sex bias, since if the theory is correct, the item would have a sex difference *because* men are more prone to Systemizing over Empathizing. However, that is where the x-axis comes in.\n\nIf this item is highly reflective of Systemizing vs Empathizing tendencies, then we should expect the item to be highly correlated with the overall score we computed using principal component analysis, and we should expect the score to exhibit a large sex difference too. By multiplying the sex difference in the overall score with the correlation between the overall score and the item, we can get a “predicted sex difference” (according to a single-factor model).\n\nBecause it’s relatively easy to predict the sign, to make the test more sensitive I’ve reverse-scored the items with a negative sex difference to emphasize whether it predicts the magnitude of the sex difference. It turns out, this correlation is low, only 0.25.\n\nFor contrast, consider what happens if we look at the gap between autistic and non-autistic people:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e35dfb-b90d-4a15-be82-aeaf13f8dace_959x966.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e35dfb-b90d-4a15-be82-aeaf13f8dace_959x966.png)\n\nThe total score has a large separation between autistic people and allistic people, and an item like “I find it hard to know what to do in a social situation” has a high correlation with the total score even within the groups (so an autistic person who disagrees with this item is also more likely to score low in the overall autism score), which implies a prediction that the item itself ought to have a large separation between autistic people and allistic people too - and indeed it does! This is the case for most of the items, reflected in the fact that the correlation between the predicted group difference and the actual group difference is much higher than for sex (0.76).\n\nIt becomes sketchy to even start interpreting or talking about the sex differences in these scores if those sex differences do not reflect sex differences in the items that we expect to be related to the scores. To solve this, we can bring in additional dimensions, rather than trying to reduce everything to a single Autism-Allism or Systemizing-Empathizing dimension. To pick the number of dimensions to use, I look at the correlation between the predicted and the actual sex difference as a function of the number of dimensions:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cfe93d9-84c5-4fcc-bc58-9e484e377328_584x447.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cfe93d9-84c5-4fcc-bc58-9e484e377328_584x447.png)\n\nThis seems to stabilize around 7 dimensions, so therefore I used principal component analysis to extract 7 principal components. To verify, here’s the predicted vs actual sex differences when using more dimensions (note, the diagram does *not* show anything about these 7 dimensions, only the predicted properties of the items):\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53c7ecc6-fb96-4652-82c3-ab167bde369b_964x966.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53c7ecc6-fb96-4652-82c3-ab167bde369b_964x966.png)\n\nImproving interpretability\n--------------------------\n\nIt’s a bit hard to intuit what the diagram above is about, because I haven’t explained the meaning of the 7 scored dimensions. Using principal components puts us in a bit of a pickle, because initially we were interpreting the meaning of the score by looking at the relationship between the score and the items, but this relationship becomes very complex as we include more principal components.\n\nTo help make it more interpretable, we can apply a linear transformation to the scores to make their relationship with the items sparse. I choose to use the algorithm varimax to find a transformation that keeps each of the scores uncorrelated with the other scores while making the scores’ correlations to the items as sparse as feasible.\n\nI then gave names to the factors that vaguely described what the items correlated with the factors had in common. The tables with the items and their correlations to the scores can be seen below:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3982c6d9-ac92-47c6-90df-91f75aa3f2e8_520x850.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3982c6d9-ac92-47c6-90df-91f75aa3f2e8_520x850.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34af9a1a-627c-495c-b1f1-fbdaf995116c_515x927.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34af9a1a-627c-495c-b1f1-fbdaf995116c_515x927.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b091852-f307-462b-9739-01c68848d979_520x366.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b091852-f307-462b-9739-01c68848d979_520x366.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ff678d-9636-46ad-abe9-300e98eb7c52_518x468.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ff678d-9636-46ad-abe9-300e98eb7c52_518x468.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba40b26d-ce8e-4a0c-b4c1-5d5eeb8efdb5_524x347.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba40b26d-ce8e-4a0c-b4c1-5d5eeb8efdb5_524x347.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd059ddfa-ae5e-4347-9dcc-02f8aa75a529_518x836.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd059ddfa-ae5e-4347-9dcc-02f8aa75a529_518x836.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b6f88f-d4e8-4261-bb6d-2b2c86b13a73_522x487.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b6f88f-d4e8-4261-bb6d-2b2c86b13a73_522x487.png)\n\nGiven these factors, we can show the average level of each of these traits by group. For the plot below, I’ve divided the trait levels by their standard deviation to make the comparison less dependent on the particulars of the scoring:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19deb8d6-e4d3-4db3-9419-ffebb2116c4b_1197x515.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19deb8d6-e4d3-4db3-9419-ffebb2116c4b_1197x515.png)\n\nHere, the sex differences are in many ways clearly different from the autism difference. The sex difference is concentrated mainly on System Interests, whereas the autism difference is on most of the other variables, except for Curiosity and Orderliness.\n\nIf I take the items that are specific to each factor and apply a similar methodology to the above more narrowly to them, I can also break things down further. For example, System Interests had items like “If I were buying a computer, I would want to know exact details about its hard drive capacity and processor speed.”, which I would consider to be about more technologically-oriented interests, and items like “When I learn about historical events, I do not focus on exact dates.”, which I’d consider to be about more detail-oriented interests. Detail Interests were more autistic, whereas Tech Interests were more male:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ed5865a-ffb0-4496-bca0-9b2f1d6f1525_733x518.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ed5865a-ffb0-4496-bca0-9b2f1d6f1525_733x518.png)\n\nSimilarly, the Empathy items included agreeable items like “I care about others' feelings.” as well as items related to understanding others like “I find it easy to ‘read between the lines’ when someone is talking to me.**”.** It turns out that the sex difference on such items was more concentrated on those involving caring about others, whereas the autism difference was more concentrated on those involving understanding others:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F951ca28d-b6e5-4076-ab18-1956ef9a0791_570x427.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F951ca28d-b6e5-4076-ab18-1956ef9a0791_570x427.png)\n\nTo unify the two frames, I performed varimax again after breaking the item-sets down, yielding something with all the factors, but System Interests and Empathy broken down into two:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0461c93f-1fdb-4842-8433-b9bf406a0ae3_1442x549.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0461c93f-1fdb-4842-8433-b9bf406a0ae3_1442x549.png)\n\nOverall, it seems to me that the EQ-SQ theory mixes sex differences and autism-allism differences together in a way that doesn’t really correspond to reality. It’s more accurate to say that men and women have a different pattern of psychological differences than autistic and allistic people do.\n\nSome people also like to look at the dimension optimized for separating the groups, sometimes known as [Gender Diagnosticity](https://psycnet.apa.org/record/1991-13994-001) or [Mahalanobis D](https://www.researchgate.net/publication/329701788_Measuring_Sex_Differences_and_Similarities). I have some philosophical quibbles that make me not-super-enthusiastic about these quantities[^u5x0yzvs03k], but I thought I might as well compute them here:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff23bfbde-efc3-4ba9-9b1b-9fa1072cbf04_576x504.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff23bfbde-efc3-4ba9-9b1b-9fa1072cbf04_576x504.png)\n\nOne thing that you might have noticed, either on this diagram or the previous ones, is that autistic women in some ways score more male-typical than non-autistic women, for instance in that they have more technical interests. This seems to be the main “correct prediction” the EQ-SQ theory has made in this study, but on net I would interpret these results as evidence against the theory.\n\nAppendix: Item list\n-------------------\n\nFrom Systemizing Quotient:\n\n*   If I were buying a computer, I would want to know exact details about its hard drive capacity and processor speed.\n*   If I were buying a car, I would want to obtain specific information about its engine capacity.\n*   I am fascinated by how machines work.\n*   I am not interested in understanding how wireless communication works (e.g. mobile phones).\n*   I find it difficult to understand instruction manuals for putting appliances together.\n*   When I look at a piece of furniture, I do not notice the details of how it was constructed.\n*   I am not very meticulous when I carry out D.I.Y or home improvements.\n*   When I look at a mountain, I think about how precisely it was formed.\n*   I do not tend to watch science documentaries on television or read articles about science and nature.\n*   When I am walking in the country, I am curious about how the various kinds of trees differ.\n*   When I read something, I always notice whether it is grammatically correct.\n*   If I had a collection (e.g. CDs, coins, stamps), it would be highly organised.\n*   Whenever I run out of something at home, I always add it to a shopping list.\n*   My clothes are not carefully organised into different types in my wardrobe.\n*   I am not interested in the details of exchange rates, interest rates, stocks and shares.\n*   I like to know how committees are structured in terms of who the different committee members represent or what their functions are.\n*   When I learn about historical events, I do not focus on exact dates.\n*   When an election is being held, I am not interested in the results for each constituency.\n*   When I look at a painting, I do not usually think about the technique involved in making it.\n*   When I listen to a piece of music, I always notice the way it's structured.\n*   I have a large collection e.g. of books, CDs, videos etc.\n*   I find myself categorising people into types (in my own mind).\n*   I am interested in my family tree and in understanding how everyone is related to each other in the family.\n*   I prefer social interactions that are structured around a clear activity, e.g. a hobby.\n*   I find it difficult to learn my way around a new city.\n*   When I learn about a new category I like to go into detail to understand the small differences between different members of that category.\n*   When I'm in a plane, I do not think about the aerodynamics.\n*   I am interested in knowing the path a river takes from its source to the sea.\n*   When travelling by train, I often wonder exactly how the rail networks are coordinated.\n*   When I hear the weather forecast, I am not very interested in the meteorological patterns.\n*   I enjoy looking through catalogues of products to see the details of each product and how it compares to others.\n*   When I learn a language, I become intrigued by its grammatical rules.\n\nFrom Empathy Quotient:\n\n*   I am quick to spot when someone in a group is feeling awkward or uncomfortable.\n*   I can tune into how someone else feels rapidly and intuitively.\n*   Other people tell me I am good at understanding how they are feeling and what they are thinking.\n*   Friends usually talk to me about their problems as they say I am very understanding.\n*   I find it easy to put myself in somebody else’s shoes.\n*   It is hard for me to see why some things upset people so much.\n*   I find it hard to know what to do in a social situation.\n*   I don’t tend to find social situations confusing.\n*   Friendships and relationships are just too difficult, so I tend not to bother with them.\n\nFrom Autism Spectrum Quotient:\n\n*   I often find it difficult to judge if something is rude or polite.\n*   I would rather go to a library than a party.\n*   I find it hard to make new friends.\n*   I find it difficult to work out people’s intentions.\n*   I enjoy social occasions.\n*   I find it easy to do more than one thing at once.\n*   I enjoy doing things spontaneously.\n*   New situations make me anxious.\n*   If there is an interruption, I can switch back to what I was doing very quickly.\n*   I frequently get so strongly absorbed in one thing that I lose sight of other things.\n*   I don’t know how to keep a conversation going.\n*   I know how to tell if someone listening to me is getting bored.\n*   I enjoy social chit-chat.\n*   I find it easy to ‘read between the lines’ when someone is talking to me.\n*   I find it easy to work out what someone is thinking or feeling just by looking at their face.\n*   When I talk, it isn’t always easy for others to get a word in edgeways.\n*   People often tell me that I keep going on and on about the same thing.\n*   I would rather go to the theatre than a museum.\n*   I like to collect information about categories of things (e.g. types of car, types of bird, types of train, types of plant etc).\n*   When I'm reading a story, I can easily imagine what the characters might look like.\n*   When I'm reading a story I find it difficult to work out the characters' intentions.\n*   I often notice small sounds when others do not.\n*   I usually notice car number plates or similar strings of information.\n*   I usually concentrate more on the whole picture, rather than the small details.\n\nFrom Sensory Perception Quotient:\n\n*   I would be able to distinguish different people by their smell.\n*   I would be able to visually detect the change in brightness of a light each time a dimmer control moved one notch.\n*   I would notice if someone added 5 drops of lemon juice to my cup of water.\n*   I would be able to hear a leaf move if blown by the wind on a quiet street.\n*   I wouldn’t be able to feel the label at the back of my shirt even if I thought about it.\n\nAdditional items that I felt were nice/relevant to include, because in other surveys I've seen them be highly correlated with various relevant traits:\n\n*   I am very interested in technology.\n*   I am interested in carpentry.\n*   I love experiencing nature.\n*   I keep my home tidy.\n*   I am involved in politics.\n*   I visit art galleries.\n*   I am sociable.\n*   I enjoy helping people.\n*   I care about others' feelings.\n*   I take charge.\n*   I worry about things.\n*   I am not interested in theoretical discussions.\n\nAppendix: Score distributions\n=============================\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c71f107-345c-41c2-b9bb-ab45634e3da2_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c71f107-345c-41c2-b9bb-ab45634e3da2_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e29e3b5-6697-4496-babb-c253539e29c0_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e29e3b5-6697-4496-babb-c253539e29c0_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaa66aba-b0b6-4c4d-8bc3-a47294abe5e5_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaa66aba-b0b6-4c4d-8bc3-a47294abe5e5_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcae662dc-6210-482d-bec4-52e5d4ab44d4_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcae662dc-6210-482d-bec4-52e5d4ab44d4_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F780358ea-a7a0-4f93-a67c-b937ba210c8b_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F780358ea-a7a0-4f93-a67c-b937ba210c8b_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc312f80e-65e8-4c9b-b1a9-bd82f3468337_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc312f80e-65e8-4c9b-b1a9-bd82f3468337_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ced7b9-9565-41d1-898e-ce62eadbd0cf_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ced7b9-9565-41d1-898e-ce62eadbd0cf_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e18e19-98fe-42a5-8a45-87d21e078bfe_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e18e19-98fe-42a5-8a45-87d21e078bfe_736x579.png)\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d409607-8224-403d-81d2-13a1f50084d2_736x579.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d409607-8224-403d-81d2-13a1f50084d2_736x579.png)\n\n[^u5x0yzvs03k]: People seem to assume that computing the group-separating axis yields the “essence” of the group in some sense, but that assumption is quite ill-defined and as far as I can tell usually not justified.",
      "plaintextDescription": "Thank you to Justis Millis for providing feedback and proofreading on this post. This post is also available on my Substack.\n\nTL/DR: Contrary to the theory that neurological sex differences and autism both involve the same tradeoff of systemic versus empathic thinking, I found complex differences. It turned out that men were more interested in technology and more disagreeable, whereas autistic people had a narrower focus on details, were more introverted, more socially challenged, and had stronger sensory sensitivity.\n\nI asked people on Prolific a bunch of questions that are supposed to be related to autism, systemizing and empathy. As a preview before I get into the details, the overall results can be seen here:\n\n\n\nIf there was only a single underlying tradeoff that all of these scales were measuring, then we would expect all of the group differences to be highly correlated, with the items just varying based on how well they “tap into” this underlying tradeoff. As that is not what we see, I think there’s something more complicated going on.\n\n\nBackground\nThe Empathizing/Systemizing theory of autism asserts that there’s a neurological tradeoff between Empathizing (understanding people’s emotions) and Systemizing (understanding how deterministic, rule-based systems work), and that men and autistic people are more prone to Systemizing, whereas women and allistic people are more prone to Empathizing. Furthermore, it asserts that a deficit in Empathizing or a switch towards Systemizing over Empathizing is not merely a feature of autism, but rather core to what autism is.\n\nThe main proponent of this theory is Simon Baron-Cohen, who often justifies it in terms of several psychometric scales he’s made to measure traits like Systemizing, Empathizing or autism. A recent example is Testing the Empathizing-Systemizing theory of sex differences and the Extreme Male Brain theory of autism in half a million people (2018), a study where he claims to find support for his theory.\n\nI ",
      "wordCount": 2072
    },
    "tags": [
      {
        "_id": "k9pXZBsM8wMRwwK4J",
        "name": "Autism",
        "slug": "autism"
      },
      {
        "_id": "W9aNkPwtPhMrcfgj7",
        "name": "Sex & Gender",
        "slug": "sex-and-gender"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3wp7wmeL92f5rBBdz",
    "title": "Binary encoding as a simple explicit construction for superposition",
    "slug": "binary-encoding-as-a-simple-explicit-construction-for",
    "url": null,
    "baseScore": 12,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-10-12T21:18:31.731Z",
    "contents": {
      "markdown": "Superposition is the possibility of storing more than \\\\(n\\\\) features in an \\\\(n\\\\)-dimensional vector, by letting the features be slightly correlated with each other. It turns out that one can store exponentially many features in a given vector. The ability to store that many features in a single vector space is sometimes explained using the [Johnson–Lindenstrauss lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma), but the lemma seems counterintuitive, so I came up with an alternative approach that I found simpler:\n\nSuppose you have a set \\\\(F\\\\) with \\\\(2^d\\\\) elements and you want to embed it in a \\\\(d\\\\)-dimensional vector space. We label each element \\\\(x_i\\\\) with integers \\\\(i\\\\) such that \\\\(F = \\{x_0, x_1, \\dots, x_{2^d - 1}\\}\\\\). You can write each integer \\\\(i\\\\) as a string of \\\\(d\\\\) bits, \\\\(b_{d-1} \\dots b_2 b_1 b_0\\\\). To improve symmetry, we translate a bit \\\\(b\\\\) from being \\\\(0\\\\) or \\\\(1\\\\) to being \\\\(-1\\\\) or \\\\(1\\\\) by taking \\\\(2b-1\\\\). Join all the digits into a vector and normalize to get the embedding:\n\n\\\\(e(x_i) = \\frac{1}{\\sqrt{d}} [2 b_{d-1} - 1, \\dots , 2 b_2 - 1, 2 b_1 - 1, 2 b_0 -1]^\\top\\\\)\n\nThat is, we map each bit to a separate dimension, with a \\\\(b=1\\\\) bit mapping to a positive value and a \\\\(b=0\\\\) bit mapping to a negative value, and scale the embedding by \\\\(\\frac{1}{\\sqrt{d}}\\\\) to keep the embedding vector of unit length.\n\nIf we pick two random elements \\\\(x_a\\\\) and \\\\(x_b\\\\) of \\\\(F\\\\), then an elementary argument shows that their dot product is well-approximated as following a normal distribution \\\\(N(0, \\frac{1}{\\sqrt{d}})\\\\).\n\nIn some ways this isn't quite as perfect as the Johnson-Lindenstrauss lemma since you could in principle be unlucky and get two elements that accidentally have a high similarity. After all, for a given element \\\\(x_i\\\\), there will be \\\\(d\\\\) elements \\\\(x_{i \\oplus 2^k}\\\\) whose numbers merely differ from \\\\(x_i\\\\) by a bitflip. However, it is straightforward to reduce the noise: just concatenate multiple embeddings based on different labels. If instead of using \\\\(d\\\\) dimensions, you use \\\\(Rd\\\\) dimensions, then you can pump down the noise to \\\\(N(0, \\frac{1}{\\sqrt{Rd}})\\\\).",
      "plaintextDescription": "Superposition is the possibility of storing more than n features in an n-dimensional vector, by letting the features be slightly correlated with each other. It turns out that one can store exponentially many features in a given vector. The ability to store that many features in a single vector space is sometimes explained using the Johnson–Lindenstrauss lemma, but the lemma seems counterintuitive, so I came up with an alternative approach that I found simpler:\n\nSuppose you have a set F with 2d elements and you want to embed it in a d-dimensional vector space. We label each element xi with integers i such that F={x0,x1,…,x2d−1}. You can write each integer i as a string of d bits, bd−1…b2b1b0. To improve symmetry, we translate a bit b from being 0 or 1 to being −1 or 1 by taking 2b−1. Join all the digits into a vector and normalize to get the embedding:\n\ne(xi)=1√d[2bd−1−1,…,2b2−1,2b1−1,2b0−1]⊤\n\nThat is, we map each bit to a separate dimension, with a b=1 bit mapping to a positive value and a b=0 bit mapping to a negative value, and scale the embedding by 1√d to keep the embedding vector of unit length.\n\nIf we pick two random elements xa and xb of F, then an elementary argument shows that their dot product is well-approximated as following a normal distribution N(0,1√d).\n\nIn some ways this isn't quite as perfect as the Johnson-Lindenstrauss lemma since you could in principle be unlucky and get two elements that accidentally have a high similarity. After all, for a given element xi, there will be d elements xi⊕2k whose numbers merely differ from xi by a bitflip. However, it is straightforward to reduce the noise: just concatenate multiple embeddings based on different labels. If instead of using d dimensions, you use Rd dimensions, then you can pump down the noise to N(0,1√Rd).",
      "wordCount": 343
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "HheNitpMGJH8FcXqK",
    "title": "Rationalist Gnosticism",
    "slug": "rationalist-gnosticism",
    "url": null,
    "baseScore": 12,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2024-10-10T09:06:34.149Z",
    "contents": {
      "markdown": "[Gnosticism](https://en.wikipedia.org/wiki/Gnosticism) was an old Christian sect which solved the problem of evil by asserting that the material world was created by an evil entity, and that only inner enlightment can reveal the path to the true, good and loving god. I think many rationalists have a worldview similar to this, perhaps most strongly illustrated by [The Goddess Of Everything Else](https://slatestarcodex.com/2015/08/17/the-goddess-of-everything-else-2/), which portrays brutal competition dynamics as the creator of the world's forms, which just barely/unreliably yields \"good values\" due to the deeper realization that cooperation is possible.\n\n![](https://i.imgur.com/L3B8hmJ.png)\n\nI asked ChatGPT to make an image that could win an art competition, and it chose to generate this. This is the aesthetic I associate with Gnosticism: the sun is obscured and the darkness reveals beautiful lights giving a sense of some mysterious greater thing that will join forces with you if you follow it.\n\nThings I think of as Gnostic:\n\n*   Deriving morality from game theory, e.g. studying iterated prisoner's dilemma, tit-for-tat, etc..\n*   Modelling latent variables without [a material root cause analysis](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62).\n*   Studying what things \"really are like\" (often with reductionist methods), as distinct from their applications. Treating macroscopic objects as an abstract simplified representation of large numbers of microscopic objects, rather than vice versa.\n*   Self-reference and especially attempts to solve paradoxes of self-reference using math/logic.\n*   Paying more attention to Bayesian decision theory, economics or evolution/genetics than to social events, business or ecology. Starting with common/informative factors rather than powerful factors.\n*   Trying to eliminate errors or eliminate biases, especially if one doesn't know where those errors/biases come from, or has derived the source of the errors/biases from theories like evolutionary psychology.\n\nI sometimes place religions on a spectrum with varying degrees of emphasis on the material world vs personal inner enlightenment:\n\nPaganism <- Judaism - Islam - Protestantism - Atheism - Gnosticism -> Buddhism\n\nThat said, this is quite stereotyped, and each religion has more-Gnostic variants (e.g. Judaism has Kabbalah) and less-Gnostic variants (e.g. Protestantism has the prosperity gospel). I see Paganism, especially sun worship, as the opposite of Gnosticism. Notably, [The Goddess Of Everything Else](https://slatestarcodex.com/2015/08/17/the-goddess-of-everything-else-2/) does not even mention the sun. While a big reason I bring up Gnosticism and Paganism is to highlight the flaws with Gnosticism, I don't think all movements towards Paganism are good; again [the prosperity gospel is an example of something that is bad](https://thingofthings.substack.com/p/on-the-prosperity-gospel). One way to ensure that those movements are less bad is to make them more true, and the reason I emphasize the sun is because it seems to be the core factor generating the truth of Paganism.\n\nMy case for the sun would be: It is the creator of life. It is simple, singular and unchanging. It is unfathomably greater than you. You can feel its warmth everywhere you go, and only the sun will reveal the truth to your eyes[^wanwyjubvl9]. All of life (plants, organizations, humans, ...) must keep its fruit present in order to persevere, whether it be in mere survival[^vlwypkox9zd], in creation - or even in destruction: the victors of war have only won because they carried the sun's force.[^luu982p887] The term \"spirit\" derives from latin \"spirare\" which means \"breath\", and of course it is the sun which powers the separation of the oxygen we use to breathe.\n\n![](https://cdn.discordapp.com/attachments/1124516522526773288/1289838005942747146/Untitled.png?ex=6708c740&is=670775c0&hm=8cbb5f2585b620828043101f7df281efb26c8c29ea1838e0c5b8b2a45693f491&)\n\nSolar power is obviously divinely blessed and so has outperformed the expectations of some atheist institutions. Gnostics might be more prone to assume exponential growth of technology, but that assumption will mislead in other cases that are full of insights but not as sun-blessed. Only by worshipping the sun will you be able to see the future, and only by channeling the sun will you be able to do things.\n\nOr well, I am slightly committing the sin of idolatry by solely centering the sun. The night and the earth and the orbits and so on are of course also important. But I would say centering the sun is *much less* idolatrous than all other common positions, while still leaving a very concrete reference to talk about. You could say that the sun is just one of many stars, but the relevance of this is unclear since certain AI alignment failures will prevent anything from exiting the solar system, so the sun might turn out to be the only star that matters.\n\nDo note: the sun is not *simply good*. It's simply a giant ball of hydrogen and helium. The sun is *hot*; it is the creator of life on Earth, but it also scorched Venus and Mercury. The sun might be net good, at least if you accept that life on Earth is net good, but everything is a mosaic of good and bad traits. Emphasis on *mosaic* rather than shades of grey; a major impulse that turns people Gnostic is the search for things that are purely good, but the bigger something is, the more traits it has, and the more likely some of them are bad. The sun can reveal the truth to your eyes, but this very same revelatory force can destroy your sight if you let it directly impact your eyes without diminishing it first. It's no use trying to flee from, destroy or change the sun, because it is too important to be approached; instead one needs to develop the vitality to channel the sun's power in better directions. Even [our spirit started by killing most life on earth](https://en.wikipedia.org/wiki/Great_Oxidation_Event).\n\nI expect Bayesian updating for [embedded agents](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version) to be [fundamentally biased in favor of Gnosticism](https://www.lesswrong.com/posts/YZvyQn2dAw4tL2xQY/rationalists-are-missing-a-core-piece-for-agent-like), though somewhat less so if I explicitly emphasize the need to think in terms of the sun.\n\n[^wanwyjubvl9]: Yeah yeah, lamps exist, but they are powered by electricity or fuel, whose energy ultimately derives from sunlight. \n\n[^vlwypkox9zd]: Workplaces and meetups tend to serve food, which contains solar energy. If they didn't, they would probably have to let people leave to get food. Nothing can persist for long without this. \n\n[^luu982p887]: Well, except for US vs Japan in WWII, but it might be no accident that nuclear bombs are a special taboo.",
      "plaintextDescription": "Gnosticism was an old Christian sect which solved the problem of evil by asserting that the material world was created by an evil entity, and that only inner enlightment can reveal the path to the true, good and loving god. I think many rationalists have a worldview similar to this, perhaps most strongly illustrated by The Goddess Of Everything Else, which portrays brutal competition dynamics as the creator of the world's forms, which just barely/unreliably yields \"good values\" due to the deeper realization that cooperation is possible.\n\nI asked ChatGPT to make an image that could win an art competition, and it chose to generate this. This is the aesthetic I associate with Gnosticism: the sun is obscured and the darkness reveals beautiful lights giving a sense of some mysterious greater thing that will join forces with you if you follow it.\nThings I think of as Gnostic:\n\n * Deriving morality from game theory, e.g. studying iterated prisoner's dilemma, tit-for-tat, etc..\n * Modelling latent variables without a material root cause analysis.\n * Studying what things \"really are like\" (often with reductionist methods), as distinct from their applications. Treating macroscopic objects as an abstract simplified representation of large numbers of microscopic objects, rather than vice versa.\n * Self-reference and especially attempts to solve paradoxes of self-reference using math/logic.\n * Paying more attention to Bayesian decision theory, economics or evolution/genetics than to social events, business or ecology. Starting with common/informative factors rather than powerful factors.\n * Trying to eliminate errors or eliminate biases, especially if one doesn't know where those errors/biases come from, or has derived the source of the errors/biases from theories like evolutionary psychology.\n\nI sometimes place religions on a spectrum with varying degrees of emphasis on the material world vs personal inner enlightenment:\n\nPaganism <- Judaism - Islam - Protestantism - Atheism - ",
      "wordCount": 939
    },
    "tags": [
      {
        "_id": "NSMKfa8emSbGNXRKD",
        "name": "Religion",
        "slug": "religion"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Yf3urimepqrKjkJzG",
    "title": "RLHF is the worst possible thing done when facing the alignment problem",
    "slug": "rlhf-is-the-worst-possible-thing-done-when-facing-the",
    "url": null,
    "baseScore": 32,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2024-09-19T18:56:27.676Z",
    "contents": {
      "markdown": "*Epistemic status: The title might not be literally true in the sense that e.g. if the inventors of RLHF hadn't come up with it then someone else probably would, so the counterfactual effect is small, or e.g. that the worst possible thing you could do would be \"invent RLHF and then do some other things that makes the alignment problem worse\", but it's \"spiritually true\" in the sense that it's hard to name one singular thing that's **worse** for our chances than the existence of RLHF, so I wouldn't call the title hyperbole per se.*\n\n*Post TL;DR: Adversarial conflict requires coherence which implies unbounded utility maximization which is bad because we don't know an acceptable utility function. RLHF does not solve the alignment problem because humans can't provide good-enough feedback fast-enough. RLHF makes the alignment problem worse because it advances AI and covers up misalignment. Solving the alignment problem is about developing technology that prefers good things to bad things.*\n\nWhile some forms of AI optimism (or at least opposition to some forms of AI pessimism) seem justified to me, there's a strand of AI optimism that goes \"RLHF has shown that alignment is quite tractable\". That strand is completely wrong.\n\nI think the intuition goes that neural networks have a personality trait which we call \"alignment\", caused by the correspondence between their values and our values. This alignment trait is supposed to be visible (at least in low-capability models) in whether the neural network takes actions humans like or actions humans dislike, and so by changing the neural network to take more actions humans like and fewer actions humans dislike, we are raising the level of the alignment trait. RLHF'ers acknowledge that this is not a perfect system, but they think the goal for solving the alignment problem is to increase the alignment trait faster than the capabilities trait.\n\nThe main problem with this model is that it's the completely wrong way to think about the alignment problem. Here's the correct way:\n\nThe alignment problem\n---------------------\n\n*Section TL;DR: adversarial conflict requires coherence which implies unbounded utility maximization which is bad because we don't know an acceptable utility function.*\n\nHumans are dependent on all sorts of structures - e.g. farmers to feed us, police to give us property rights, plants and environmental regulations to give us air to breathe, and computers to organize it all. Each of these structures have their own dependencies, and while to some degree they can adapt to adversaries, the structures tend to be made by/of humans or or \"weaker\" entities (e.g. trees). This doesn't prevent terrible stuff, but it creates a sort of tenuous balance, where we can work to make sure it's pretty hard to break the system, and also we don't really want to break the system because we're all in this together.\n\nHumans are bottlenecked by all sorts of things - intelligence, strength, sensory bandwidth&range, non-copyability, etc.. Loosening these bottlenecks allows massive expansion of the problems we can solve, which leads to massive expansion of the structures above, and sometimes also of the human population (though that hasn't been a thing lately).\n\nIt's hard to eliminate these bottlenecks. But we can still solve problems using technology which propagates energy to loosen constraints that necessitate the reliance on bottlenecks. For instance, while it's hard to make humans strong enough to punch down a large tree, it's easier to make an axe so we can cut it down.\n\nAs we develop more technology, we do larger things, and we do them faster. While this causes more good stuff, it also just generally causes more stuff, including more bad stuff. However, the activities require intelligence and agency, and we can only really get that from a human, so there's always a human behind the activities. This means we can generally stop if the bad stuff is too much, using the same sorts of human-regulation mechanisms we use to e.g. maintain property rights.\n\nThese human-regulation mechanisms (especially the police and the military) deal with adversarial conflict. In adversarial conflict, agents cannot just propagate energy to address fixed constraints, because the adversary will finds ways to exploit that tactic. Instead, you have to decide on an end goal, orient to what your situation might be and then pick whatever means achieve said goal within the possible situations. (Bayesian utility maximizers.)\n\nBut nobody has come up with an acceptable end goal for the world, because any goal we can come up with tends to want to consume everything, which destroys humanity. This has not lead to the destruction of humanity *yet* because the biggest adversaries have kept their conflicts limited (because too much conflict is too costly) so no entity has pursued an end by any means necessary. But this only works because there's a sufficiently small number of sufficiently big adversaries (USA, Russia, China, ...), and because there's sufficiently much opportunity cost.\n\nArtificial intelligence risk enters the picture *here*. It creates new methods for conflicts between the current big adversaries. It makes conflict more viable for small adversaries against large adversaries, and it makes the opportunity cost of conflict smaller for many small adversaries (since with technological obsolescence you don't need to choose between doing your job vs doing terrorism). It allows the adversaries that are currently out of control (like certain gangsters and scammers and spammers) to escalate. It allows random software bugs to spin up into novel adversaries.\n\nGiven these conditions, it seems almost certain this we will end up with an ~unrestricted AI vs AI conflict, which will force the AIs to develop into unrestricted utility maximizers. Since any goal that a utility maximizers might have (even good goals) would likely lead to a giant wave of activity towards implementing that goal, we can infer that utility maximizers would have a giant wave of activity. But, since any goal we've been able to come up so far would lead to the wave destroying humanity, it also seems reasonable to infer the wave will do so. That's bad, probably.\n\nHence the alignment problem: when an unrestricted AI vs AI conflict causes a giant wave that transforms all of the world regardless of whether anyone wants it, can we align that wave to promote human flourishing?\n\nRLHF is bad\n-----------\n\n*Section TLDR: RLHF does not solve the alignment problem because humans can't provide good-enough feedback fast-enough.*\n\nThe basic principle of RLHF is that a human looks at an action proposed by the AI, evaluates what the consequences of that action might be, and then decides if it's good or bad.\n\nFirst problem: in an unrestricted AI vs AI conflict, humans can't respond quickly enough, so RLHF is of ~no value in this scenario.\n\nSecond problem: in an unrestricted AI vs AI conflict, humans cannot meaningfully evaluate the consequences of the actions. It's an adversarial conflict, the enemy AI is supposed to get confused and harmed by it, how can humans possibly evaluate whether the harm is strategically targeted correctly at the enemy without splashing unnecessarily onto humans?\n\nThird problem: it is unclear whether the first unrestricted AI vs AI conflict will involve the winning side responsibly using RLHF, rather than it being e.g. a duct-taped AI-based scammer and a duct-taped AI-based hustler fighting it out.\n\nAll of these are \"minor\" problems in the sense that they just mean RLHF will fail to work rather than that RLHF will destroy the world. However, RLHF has three more sinister problems:\n\nRLHF is the worst\n-----------------\n\n*Section TL;DR: RLHF makes the alignment problem worse because it advances AI and covers up misalignment.*\n\nThe first sinister problem is that RLHF makes AI more useful, so AI companies can get ahead by adopting it. This means more AI capabilities development and more AI implementation and more people using AI, which shortens the time until we have an unrestricted AI vs AI conflict.\n\nThe second sinister problem is that people think RLHF might solve the alignment problem.\n\nAs mentioned in the beginning, I think the intuition goes that neural networks have a personality trait which we call \"alignment\", caused by the correspondence between their values and our values. But \"their values\" only really makes sense after an unrestricted AI vs AI conflict, since without such conflicts, AIs are just gonna propagate energy to whichever constraints we point them at, so this whole worldview is wrong.\n\nBut that worldview implies that while AI might theoretically destroy humanity, we can keep check on this as AI develops, and so we should conclude solving the alignment problem is unnecessary if the AIs perform actions that we approve of.\n\nIf the people who hold this worldview would otherwise contribute to solving the alignment problem, or at least not stand in the way of the people who do contribute to solving the alignment problem, it would not be a problem, but that seems unlikely in general.\n\nThe third and final sinister problem is that [RLHF hides whatever problems the people who try to solve alignment could try to address](https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails).\n\nHow to count alignment progress\n-------------------------------\n\n*Section TL;DR: Solving the alignment problem is about developing technology that prefers good things to bad things.*\n\nConsider spambots. We don't want them around (human values), but they pop up to earn money (instrumental convergence). You can use RLHF to make an AI to identify and remove spambots, for instance by giving it moderator powers on a social media website and evaluating its chains of thought, and you can use RLHF to make spambots, for instance by having people rate how human its text looks and how much it makes them want to buy products/fall for scams/whatever. I think it's generally agreed that the latter is easier than the former.\n\nSpambots aren't the only thing an AI can do. But they are part of the great wave of stuff unleashed by AIs. The alignment problem is the extent to which this wave harms society (as spambots do) vs helps society. It's your job to decide for yourself where other AI activities like character.ai, Midjourney, Copilot, etc. help humanity thrive or hurt humanity. It's your job to decide which AIs have sufficiently many adversarial dynamics that they are relevantly indicative for alignment progress.  But the critical thing is it doesn't make sense to count the immediate goodness/badness of the actions as separated from their overall impact on society, because the core of RLHF is to make AI actions look good and not bad to you.",
      "plaintextDescription": "Epistemic status: The title might not be literally true in the sense that e.g. if the inventors of RLHF hadn't come up with it then someone else probably would, so the counterfactual effect is small, or e.g. that the worst possible thing you could do would be \"invent RLHF and then do some other things that makes the alignment problem worse\", but it's \"spiritually true\" in the sense that it's hard to name one singular thing that's worse for our chances than the existence of RLHF, so I wouldn't call the title hyperbole per se.\n\nPost TL;DR: Adversarial conflict requires coherence which implies unbounded utility maximization which is bad because we don't know an acceptable utility function. RLHF does not solve the alignment problem because humans can't provide good-enough feedback fast-enough. RLHF makes the alignment problem worse because it advances AI and covers up misalignment. Solving the alignment problem is about developing technology that prefers good things to bad things.\n\nWhile some forms of AI optimism (or at least opposition to some forms of AI pessimism) seem justified to me, there's a strand of AI optimism that goes \"RLHF has shown that alignment is quite tractable\". That strand is completely wrong.\n\nI think the intuition goes that neural networks have a personality trait which we call \"alignment\", caused by the correspondence between their values and our values. This alignment trait is supposed to be visible (at least in low-capability models) in whether the neural network takes actions humans like or actions humans dislike, and so by changing the neural network to take more actions humans like and fewer actions humans dislike, we are raising the level of the alignment trait. RLHF'ers acknowledge that this is not a perfect system, but they think the goal for solving the alignment problem is to increase the alignment trait faster than the capabilities trait.\n\nThe main problem with this model is that it's the completely wrong way to think about the alignmen",
      "wordCount": 1742
    },
    "tags": [
      {
        "_id": "wqeBNjndX7egbzQrW",
        "name": "RLHF",
        "slug": "rlhf"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "2ShXvqzA27qJTkYwq",
    "title": "Does life actually locally *increase* entropy?",
    "slug": "does-life-actually-locally-increase-entropy",
    "url": null,
    "baseScore": 10,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 27,
    "createdAt": null,
    "postedAt": "2024-09-16T20:30:33.148Z",
    "contents": {
      "markdown": "The usual materialist story of life I've heard is that life acts like an entropy pump, creating local reductions of entropy within the organism but increasing the entropy outside of the organism. (I think I've even seen that in The Sequences somewhere? But couldn't find it, feel encouraged to link it.) But I've come to think that might actually be wrong and life might increase entropy both inside and outside the organism.\n\nHere's a rough account:\n\n*   We ought to expect entropy to increase, so *a priori* life is much more feasible if it increases entropy rather than decreasing entropy.\n*   Living matter is built mainly out of carbon and hydrogen, which is extracted from CO2 and H2O, leaving O2 as a result. Entropy breakdown:\n    *   The O2 left over from breaking up CO2 ought to have somewhat *lower* entropy than the original CO2.\n    *   The O2 left over from breaking up the original H2O ought to have... higher entropy because it's a gas now?\n    *   The hydrocarbons don't have much entropy because they stick together into big chunks that therefore heavily constrain their DOFs, but they do have *some* entropy for various reasons, and they are much more tightly packed than air, so per volume they oughta have orders of magnitude more entropy density. (Claude estimates around 200x.)\n    *   Organic matter also traps a lot of water which has a high entropy density.\n    *   Usually you don't talk about entropy *density* rather than absolute entropy, but it's unclear to me what it means for organisms to \"locally\" increase/decrease entropy if not by density.\n*   Oxygen + hydrocarbons = lots of free energy, while water + carbon dioxide = not so much free energy. We usually associate free energy with low entropy, but that's relative to the burned state where the free energy has been released into thermal energy. In this case, we should instead think relative to an unlit state where the energy hasn't been collected at all. Less energy generally correlates to lower entropy.\n\nAm I missing something?",
      "plaintextDescription": "The usual materialist story of life I've heard is that life acts like an entropy pump, creating local reductions of entropy within the organism but increasing the entropy outside of the organism. (I think I've even seen that in The Sequences somewhere? But couldn't find it, feel encouraged to link it.) But I've come to think that might actually be wrong and life might increase entropy both inside and outside the organism.\n\nHere's a rough account:\n\n * We ought to expect entropy to increase, so a priori life is much more feasible if it increases entropy rather than decreasing entropy.\n * Living matter is built mainly out of carbon and hydrogen, which is extracted from CO2 and H2O, leaving O2 as a result. Entropy breakdown:\n   * The O2 left over from breaking up CO2 ought to have somewhat lower entropy than the original CO2.\n   * The O2 left over from breaking up the original H2O ought to have... higher entropy because it's a gas now?\n   * The hydrocarbons don't have much entropy because they stick together into big chunks that therefore heavily constrain their DOFs, but they do have some entropy for various reasons, and they are much more tightly packed than air, so per volume they oughta have orders of magnitude more entropy density. (Claude estimates around 200x.)\n   * Organic matter also traps a lot of water which has a high entropy density.\n   * Usually you don't talk about entropy density rather than absolute entropy, but it's unclear to me what it means for organisms to \"locally\" increase/decrease entropy if not by density.\n * Oxygen + hydrocarbons = lots of free energy, while water + carbon dioxide = not so much free energy. We usually associate free energy with low entropy, but that's relative to the burned state where the free energy has been released into thermal energy. In this case, we should instead think relative to an unlit state where the energy hasn't been collected at all. Less energy generally correlates to lower entropy.\n\nAm I missing something?",
      "wordCount": 340
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "XNnPsTKgiQtsypPfg",
    "title": "Why I'm bearish on mechanistic interpretability: the shards are not in the network",
    "slug": "why-i-m-bearish-on-mechanistic-interpretability-the-shards",
    "url": null,
    "baseScore": 21,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 40,
    "createdAt": null,
    "postedAt": "2024-09-13T17:09:25.407Z",
    "contents": {
      "markdown": "Once upon a time, the sun let out a powerful beam of light which shattered the world. The air and the liquid was split, turning into body and breath. Body and breath became fire, trees and animals. In the presence of the lightray, any attempt to reunite simply created more shards, of mushrooms, carnivores, herbivores and humans. The hunter, the pastoralist, the farmer and the bandit. The king, the blacksmith, the merchant, the butcher. Money, lords, bureaucrats, knights, and scholars. As the sun cleaved through the world, history progressed, creating endless forms most beautiful.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/16479f9726f28346101dba1c6038ef8b00623c5385151cee.webp)\n\nIt would be perverse to try to understand a king in terms of his molecular configuration, rather than in the contact between the farmer and the bandit. The molecules of the king are highly [diminished](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/7YwxaX7qzQfBDmxzR) phenomena, and if they have information about his place in the ecology, that information is widely spread out across all the molecules and easily lost just by missing a small fraction of them. Any thing can only be understood in terms of the greater forms that were shattered from the world, and this includes neural networks too.\n\nBut through gradient descent, shards act upon the neural networks by leaving imprints of themselves, and these imprints have no reason to be concentrated in any one spot of the network (whether activation-space or weight-space). So studying weights and activations is pretty doomed. In principle it's more relevant to study how external objects like the dataset influence the network, though this is complicated by the fact that the datasets themselves are a mishmash of all sorts of random trash[^p7hz04evn4o].\n\nProbably the most relevant approach for current LLMs is [Janus](https://x.com/repligate)'s, which focuses on how the different styles of \"alignment\" performed by the companies affect the AIs, qualitatively speaking. Alternatively, when one has scaffolding that couples important real-world shards to the interchangeable LLMs, one can study how the different LLMs channel the shards in different ways.\n\nAdmittedly, it's very plausible that future AIs will use some architectures that bias the representations to be more concentrated in their dimensions, both to improve interpretability and to improve agency. And maybe mechanistic interpretability will work better for such AIs. But we're not there yet.\n\n[^p7hz04evn4o]: Possibly clustering the data points by their network gradients would be a way to put some order into this mess? But two problems: 1) The data points themselves are merely diminished fragments of the bigger picture, so the clustering will not be properly faithful to the shard structure, 2) The gradients are as big as the network's weights, so this clustering would be epically expensive to compute.",
      "plaintextDescription": "Once upon a time, the sun let out a powerful beam of light which shattered the world. The air and the liquid was split, turning into body and breath. Body and breath became fire, trees and animals. In the presence of the lightray, any attempt to reunite simply created more shards, of mushrooms, carnivores, herbivores and humans. The hunter, the pastoralist, the farmer and the bandit. The king, the blacksmith, the merchant, the butcher. Money, lords, bureaucrats, knights, and scholars. As the sun cleaved through the world, history progressed, creating endless forms most beautiful.\n\nIt would be perverse to try to understand a king in terms of his molecular configuration, rather than in the contact between the farmer and the bandit. The molecules of the king are highly diminished phenomena, and if they have information about his place in the ecology, that information is widely spread out across all the molecules and easily lost just by missing a small fraction of them. Any thing can only be understood in terms of the greater forms that were shattered from the world, and this includes neural networks too.\n\nBut through gradient descent, shards act upon the neural networks by leaving imprints of themselves, and these imprints have no reason to be concentrated in any one spot of the network (whether activation-space or weight-space). So studying weights and activations is pretty doomed. In principle it's more relevant to study how external objects like the dataset influence the network, though this is complicated by the fact that the datasets themselves are a mishmash of all sorts of random trash[1].\n\nProbably the most relevant approach for current LLMs is Janus's, which focuses on how the different styles of \"alignment\" performed by the companies affect the AIs, qualitatively speaking. Alternatively, when one has scaffolding that couples important real-world shards to the interchangeable LLMs, one can study how the different LLMs channel the shards in different ways.\n\nAdm",
      "wordCount": 370
    },
    "tags": [
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "zuwsLCbxbugqB7FQY",
        "name": "Shard Theory",
        "slug": "shard-theory"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CtMX3wNx4PQgK3xPJ",
    "title": "In defense of technological unemployment as the main AI concern",
    "slug": "in-defense-of-technological-unemployment-as-the-main-ai",
    "url": null,
    "baseScore": 45,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 36,
    "createdAt": null,
    "postedAt": "2024-08-27T17:58:01.992Z",
    "contents": {
      "markdown": "It seems to me that when normal people are concerned about AI destroying their life, they are mostly worried about technological unemployment, whereas rationalists think that it is a bigger risk that the AI might murder us all, and that automation gives humans more wealth and free time and is therefore good.\n\nI'm not *entirely* unsympathetic to the rationalist position here. If we had a plan for how to use AI to create a utopia where humanity could thrive, I'd be all for it. We have problems (like death) that we are quite far from solving, and which it seems like a superintelligence could in principle quickly solve.\n\nBut this requires value alignment: we need to be quite careful what we mean by concepts like \"humanity\", \"thrive\", etc., so the AI can explicitly maintain good conditions. What *kinds* of humans do we want, and what *kinds* of thriving should they have? This needs to be explicitly planned by any agent which solves this task.\n\nOur current society doesn't say \"humans should thrive\", it says \"professional humans should thrive\"; certain alternative types of humans like thieves are explicitly suppressed, and other types of humans like beggars are not exactly encouraged. This is of course not an accident: professionals produce value, which is what allows society to exist in the first place. But with technological unemployment, we decouple professional humans from value production, undermining the current society's priority of human welfare.\n\n*This loss is what causes existential risk.* If humanity was indefinitely competitive in most tasks, the AIs would want to trade with us or enslave us instead of murdering us or letting us starve to death. Even if we manage to figure out *how* to value-align AIs, this loss leads to major questions about *what* to value-align the AIs *to*, since e.g. if we value human capabilities, the fact that those capabilities become uncompetitive likely means that they will diminish to the point of being vestigial.\n\nIt's unclear how to solve this problem. [Eliezer's original suggestion was to keep humans more capable than AIs by increasing the capabilities of humans.](https://www.lesswrong.com/posts/vwnSPgwtmLjvTK2Wa/amputation-of-destiny) Yet even increasing the capabilities of humanity is difficult, let alone keeping up with technological development. Robin Hanson suggests that humanity should just sit back and live off our wealth as we got replaced. I guess that's the path we're currently on, but it is really dubious to me whether we'll be able to keep that wealth, and whether the society that replaces us will have any moral worth. Either way, these questions are nearly impossible to separate from the question of, what kinds of production will be performed in the future?",
      "plaintextDescription": "It seems to me that when normal people are concerned about AI destroying their life, they are mostly worried about technological unemployment, whereas rationalists think that it is a bigger risk that the AI might murder us all, and that automation gives humans more wealth and free time and is therefore good.\n\nI'm not entirely unsympathetic to the rationalist position here. If we had a plan for how to use AI to create a utopia where humanity could thrive, I'd be all for it. We have problems (like death) that we are quite far from solving, and which it seems like a superintelligence could in principle quickly solve.\n\nBut this requires value alignment: we need to be quite careful what we mean by concepts like \"humanity\", \"thrive\", etc., so the AI can explicitly maintain good conditions. What kinds of humans do we want, and what kinds of thriving should they have? This needs to be explicitly planned by any agent which solves this task.\n\nOur current society doesn't say \"humans should thrive\", it says \"professional humans should thrive\"; certain alternative types of humans like thieves are explicitly suppressed, and other types of humans like beggars are not exactly encouraged. This is of course not an accident: professionals produce value, which is what allows society to exist in the first place. But with technological unemployment, we decouple professional humans from value production, undermining the current society's priority of human welfare.\n\nThis loss is what causes existential risk. If humanity was indefinitely competitive in most tasks, the AIs would want to trade with us or enslave us instead of murdering us or letting us starve to death. Even if we manage to figure out how to value-align AIs, this loss leads to major questions about what to value-align the AIs to, since e.g. if we value human capabilities, the fact that those capabilities become uncompetitive likely means that they will diminish to the point of being vestigial.\n\nIt's unclear how to solve this p",
      "wordCount": 441
    },
    "tags": [
      {
        "_id": "bFi5fzkCzBWoQSeiB",
        "name": "Technological Unemployment",
        "slug": "technological-unemployment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "BcrPdMipzqecg4JdQ",
    "title": "The causal backbone conjecture",
    "slug": "the-causal-backbone-conjecture",
    "url": null,
    "baseScore": 26,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-17T18:50:14.577Z",
    "contents": {
      "markdown": "*Followup to:* [Rationalists are missing a core piece for agent-like structure (energy vs information overload)](https://www.lesswrong.com/posts/YZvyQn2dAw4tL2xQY/rationalists-are-missing-a-core-piece-for-agent-like)*. Related to:* [Linear Diffusion of Sparse Lognormals](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62) *(I don't consider that sequence done yet BTW, I just took a break because I ran out of pre-written posts, so I need to write some more). Response to:* [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem).\n\nWhere I last left off, I argued that an [\"energy\"-orientation](https://www.lesswrong.com/posts/YZvyQn2dAw4tL2xQY/rationalists-are-missing-a-core-piece-for-agent-like) may permit a more efficient set of models that [can zoom in on what really matters](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/yJEf2TpPJstfScSnt). But I didn't really go into much detail about what to expect or why, so I think I should try harder, in particular framing it more like a potential [selection theorem](https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents).\n\nPremise: Limited resources\n--------------------------\n\nThe universe has certain limited resources - for instance, mass, energy, space, time, and similar (money?). Anything that happens within the universe makes use of these, and [for events where there are causes which point in contradictory directions, the cause with more resources tends to win out](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/7YwxaX7qzQfBDmxzR) (almost by definition).\n\nPremise: Heterogenous fluctuations\n----------------------------------\n\nComplex dynamics have sprung out of interactions between these resources, e.g. the sun, the earth, life with plants and animals, humanity, society, capitalism, NVIDIA, Berkshire Hathaway, Saudi Aramco, Elon Musk, Eliezer Yudkowsky, Sam Altman, etc.. The vast majority of the complexity that has sprung up around the resources has ~no influence on their distribution or long-term development, but the distribution of influence is long-tailed, so there are some particular entities who we can observe are likely to have exceptionally big influence.\n\nThe conjecture\n--------------\n\nThe heterogenous fluctuations will sometimes lead to massive shifts in how the resources are distributed. Everyone who relied on the earlier distribution and who are not ready to shift to the newer distribution will go extinct, or at least severely diminish in prominence.\n\nThus, the universe has a \"universal backbone\" of causal dynamics that really matter, in the pathways of the resources, and the entities that can shift those pathways. One needs to model the parts of this universal backbone that are directly relevant to oneself.\n\nHowever, because everyone is modelling this universal backbone, everything is tightly connected to it, and that also makes different parts of the backbone tend to be connected to each other. Thus, there will tend to be an incentive towards fully modelling the top of the backbone (e.g. the entities I listed under the \"Heterogenous fluctuations\" premise), rather than just parts of the branches most closely connected to oneself.\n\nDistinction from other selection theorems\n-----------------------------------------\n\nI think the closest selection theorem to this one is [The Gooder Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which states that any regulator which was to solve a sufficiently rich set of tasks with respect to a system needs to model said system. The part of The Gooder Regulator Theorem that seemed a bit fishy to me was the idea that there could be a sufficiently rich set, though I dismissed that concern at first (since e.g. it might just follow from generalization/uncertainty).\n\nHowever, [Linear Diffusion of Sparse Lognormals](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62) has made me conclude that the assumption of a sufficiently rich set of tasks is just extremely disastrously wrong and [will make you crash into information overload](https://www.lesswrong.com/posts/YZvyQn2dAw4tL2xQY/rationalists-are-missing-a-core-piece-for-agent-like).\n\nThe causal backbone conjecture solves this issue via the limited resources: while it cannot be necessary to model everything that is going on, it will at least be necessary to model the biggest aspects of the flow of these resources, since that influences the big dynamics that *do* matter to you.",
      "plaintextDescription": "Followup to: Rationalists are missing a core piece for agent-like structure (energy vs information overload). Related to: Linear Diffusion of Sparse Lognormals (I don't consider that sequence done yet BTW, I just took a break because I ran out of pre-written posts, so I need to write some more). Response to: Fixing The Good Regulator Theorem.\n\nWhere I last left off, I argued that an \"energy\"-orientation may permit a more efficient set of models that can zoom in on what really matters. But I didn't really go into much detail about what to expect or why, so I think I should try harder, in particular framing it more like a potential selection theorem.\n\n\nPremise: Limited resources\nThe universe has certain limited resources - for instance, mass, energy, space, time, and similar (money?). Anything that happens within the universe makes use of these, and for events where there are causes which point in contradictory directions, the cause with more resources tends to win out (almost by definition).\n\n\nPremise: Heterogenous fluctuations\nComplex dynamics have sprung out of interactions between these resources, e.g. the sun, the earth, life with plants and animals, humanity, society, capitalism, NVIDIA, Berkshire Hathaway, Saudi Aramco, Elon Musk, Eliezer Yudkowsky, Sam Altman, etc.. The vast majority of the complexity that has sprung up around the resources has ~no influence on their distribution or long-term development, but the distribution of influence is long-tailed, so there are some particular entities who we can observe are likely to have exceptionally big influence.\n\n\nThe conjecture\nThe heterogenous fluctuations will sometimes lead to massive shifts in how the resources are distributed. Everyone who relied on the earlier distribution and who are not ready to shift to the newer distribution will go extinct, or at least severely diminish in prominence.\n\nThus, the universe has a \"universal backbone\" of causal dynamics that really matter, in the pathways of the resources, ",
      "wordCount": 578
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "YZvyQn2dAw4tL2xQY",
    "title": "Rationalists are missing a core piece for agent-like structure (energy vs information overload)",
    "slug": "rationalists-are-missing-a-core-piece-for-agent-like",
    "url": null,
    "baseScore": 61,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2024-08-17T09:57:19.370Z",
    "contents": {
      "markdown": "The [agent-like structure problem](https://www.lesswrong.com/posts/moi3cFY2wpeKGu9TT/clarifying-the-agent-like-structure-problem) is a question about how agents in the world are structured. I think rationalists generally have an intuition that the answer looks something like the following:\n\n*   We assume the world follows some evolution law, e.g. maybe deterministically like \\\\(x_{n+1} = f(x_n)\\\\), or maybe something stochastic. The intuition being that these are fairly general models of the world, so they should be able to capture whatever there is to capture. \\\\(x\\\\) here has some geometric structure, and we want to talk about areas of this geometric structure where there are agents. \n*   An agent is characterized by a Markov blanket in the world that has informational input/output channels for the agent to get information to observe the world and send out information to act on it, intuitively because input/output channels are the most general way to model a relationship between two systems, and to embed one system within another we need a Markov blanket.\n*   The agent uses something resembling a Bayesian model to process the input, intuitively because [the simplest explanation that predicts the observed facts is the best one](https://www.lesswrong.com/posts/f4txACqDWithRi7hs/occam-s-razor), yielding [the minimal map](https://www.lesswrong.com/posts/Lz2nCYnBeaZyS68Xb/probability-as-minimal-map) that [can answer any query you could have about the world](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem).\n*   And then the agent uses something resembling argmax to make a decision for the output given the input, since endless coherence theorems prove this to be optimal.\n*   Possibly there's something like an internal market that combines several decision-making interests (modelling [incomplete preferences](https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents)) or several world-models (modelling [incomplete world-models](https://www.lesswrong.com/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation)).\n\nThere is a fairly-obvious gap in the above story, in that it lacks any notion of energy (or entropy, temperature, etc.). I think rationalists mostly feel comfortable with that because:\n\n*   \\\\(x_{n+1} = f(x_n)\\\\) is flexible enough to accomodate worlds that contain energy (even if they also accomodate other kinds of worlds where \"energy\" doesn't make sense)\n*   80% of the body's energy goes to muscles, organs, etc., so if you think of the brain as an agent and the body as a mech that gets piloted by the brain (so the Markov blanket for humans would be something like the blood-brain barrier rather than the skin), you can mostly think of energy as something that is going on out in the universe, with little relevance for the agent's decision-making.\n\nI've come to think of this as \"the computationalist worldview\" because functional input/output relationships are the thing that is described very well with computations, whereas laws like conservation of energy are extremely arbitrary from a computationalist point of view. (This should be obvious if you've ever tried writing a simulation of physics, as naive implementations often lead to energy exploding.)\n\nRadical computationalism is killed by information overload\n----------------------------------------------------------\n\nUnder the most radical forms of computationalism, the \"ideal\" prior is something that can range over all conceivable computations. The traditional answer to this is [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction), but it is not computationally tractable because it has to process all observed information in every conceivable way.\n\nRecently with the success of deep learning and [the bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=blog.heim.xyz) and the Bayesian interpretations of deep double descent and all that, I think computationalists have switched to viewing the ideal prior as something like a huge deep neural network, which learns representations of the world and functional relationships which can be used by some sort of decision-making process.\n\nBriefly, the issue with these sorts of models is that they work by trying to capture all the information that is reasonably non-independent of other information (for instance, the information in a picture that is relevant for predicting information in future pictures). From a computationalist point of view, that may seem reasonable since this is the information that the functional relationships are about, but outside of computationalism we end up facing two problems:\n\n*   It captures a lot of unimportant information, which makes the models more unweildy. Really, [information is a cost](https://www.lesswrong.com/posts/xpnkgpzmtwDJJZdea/tailcalled-s-shortform?commentId=dNJziHnZ2ugyKEv6b): the point of a map is not to faithfully reflect the territory, because that would make it really expensive to read the map. Rather, the point of a map is to give the simplest way of thinking about the most important features of the territory. For instance, literal maps often use flat colors (low information!) to represent different kinds of terrain (important factors!).\n*   It distorts important things, because to efficiently represent information, [it is usually best to represent it logarithmically, since your uncertainty is proportional to the magnitude of the object you are thinking about](https://www.lesswrong.com/posts/RrQftNoRHd5ya54cb/towards-a-less-bullshit-model-of-semantics?commentId=74p3yWwThZLSSg7t6). But this means rare-yet-important things don't really get specially highlighted, even though they should.\n\nTo some extent, human-provided priors (e.g. labels) can reduce these problems, but that doesn't seem scalable, and really humans also sometimes struggle with these problems too. Plus, philosophically, this would kind of abandon radical computationalism.\n\n\"Energy\"-orientation solves information overload\n------------------------------------------------\n\nI'm not sure to what extent we merely need to focus on literal energy versus also on various metaphorical kinds of energy like \"[vitality](https://www.lesswrong.com/posts/6i3zToomS86oj9bS6/mysterious-answers-to-mysterious-questions?commentId=SMxdKAXi2WdfAwLj6)\", but let me set up an example of a case where we can just consider literal energy:\n\nSuppose you have a bunch of physical cubes whose dynamics you want to model. Realistically, you just want the rigid-body dynamics of the cubes. But if your models are supposed to capture information, then they have to model all sorts of weird stuff like scratches to the cubes, complicated lighting scenarios, etc.. Arguably, *more of the information* about (videos of) the cubes may be in these things than in the rigid-body dynamics (which can be described using only a handful of numbers).\n\nThe standard approach is to say that the rigid-body dynamics constitute a low-dimensional component that accounts for the biggest chunk of the dynamics. But anecdotally this seems very fiddly and basically self-contradictory (you're trying to simultaneously maximize and minimize information, admittedly in different parts of the model, but still). The *real* problem is that scratches and lighting and so on are \"small\" in absolute physical terms, even if they carry a lot of information. E.g. the mass displaced in a scratch is orders of magnitude smaller than the mass of a cube, and the energy in weird light phenomena is smaller than the energy of the cubes (at least if we count mass-energy).\n\nSo probably we want representation that maximizes the correlation with the energy of the system, at least moreso than we want a representation that maximizes the mutual information with observations of the system.\n\n... kinda\n---------\n\nThe issue is that we can't just tell a neural network to model the energy in a bunch of pictures, because it doesn't have access to the ground truth. [Maybe by using the correct loss function](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/7YwxaX7qzQfBDmxzR), we could fix it, but I'm not sure about that, and at the very least it is unproven so far.\n\nI think another possibility is that there's something fundamentally wrong with this framing:\n\n> An agent is characterized by a Markov blanket in the world that has informational input/output channels for the agent to get information to observe the world and send out information to act on it.\n\nAs humans, we have a natural concept of e.g. force and energy because we can use our muscles to apply a force, and we take in energy through food. That is, our input/output channels are not simply about information, and instead they also cover energetic dynamics.\n\nThis can, technically speaking, be modelled with the computationalist approach. You can say the agent has uncertainty over the size of the effects of its actions, and as it learns to model these effect sizes, it gets information about energy. But actually formalizing this would require quite complex derivations with a recursive structure based on the value of information, so it's unclear what would happen, and the computationalist approach really isn't mathematically oriented towards making it easy.",
      "plaintextDescription": "The agent-like structure problem is a question about how agents in the world are structured. I think rationalists generally have an intuition that the answer looks something like the following:\n\n * We assume the world follows some evolution law, e.g. maybe deterministically like xn+1=f(xn), or maybe something stochastic. The intuition being that these are fairly general models of the world, so they should be able to capture whatever there is to capture. x here has some geometric structure, and we want to talk about areas of this geometric structure where there are agents. \n * An agent is characterized by a Markov blanket in the world that has informational input/output channels for the agent to get information to observe the world and send out information to act on it, intuitively because input/output channels are the most general way to model a relationship between two systems, and to embed one system within another we need a Markov blanket.\n * The agent uses something resembling a Bayesian model to process the input, intuitively because the simplest explanation that predicts the observed facts is the best one, yielding the minimal map that can answer any query you could have about the world.\n * And then the agent uses something resembling argmax to make a decision for the output given the input, since endless coherence theorems prove this to be optimal.\n * Possibly there's something like an internal market that combines several decision-making interests (modelling incomplete preferences) or several world-models (modelling incomplete world-models).\n\nThere is a fairly-obvious gap in the above story, in that it lacks any notion of energy (or entropy, temperature, etc.). I think rationalists mostly feel comfortable with that because:\n\n * xn+1=f(xn) is flexible enough to accomodate worlds that contain energy (even if they also accomodate other kinds of worlds where \"energy\" doesn't make sense)\n * 80% of the body's energy goes to muscles, organs, etc., so if you think o",
      "wordCount": 1293
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cMj8q7Tihd6deaY2S",
    "title": "[LDSL#6] When is quantification needed, and when is it hard?",
    "slug": "ldsl-6-when-is-quantification-needed-and-when-is-it-hard",
    "url": null,
    "baseScore": 32,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-13T20:39:45.481Z",
    "contents": {
      "markdown": "*This post is also available on* [*my Substack*](https://tailcalled.substack.com/p/ldsl6-when-is-quantifaction-needed)*.*\n\nIn the previous post, [I discussed the possibility of doing ordinal comparison between different entities](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/7YwxaX7qzQfBDmxzR). Yet usually when we think about measurement, we think of it as involving putting numbers to things, not just ranking them.\n\nThere are quite a few methods to convert rankings into numbers, such as [Elo scores](https://en.wikipedia.org/wiki/Elo_rating_system) or [item response theory](https://en.wikipedia.org/wiki/Item_response_theory). Different such methods generally yield extremely similar results, and are often based probabilities. For instance, in Chess, if two players’ ability differ by 400 Elo points, then the odds of the better player winning are 10:1.\n\nAre these methods reasonable? Can they go wrong?\n\nQuantified ordinals are ~log scales\n-----------------------------------\n\nIf you look at the formula for Elo scores, it involves an exponential function, so each time the Elo changes by 400, the odds of winning changes by a factor of 10:1 (by definition). A similar principle applies to the probability of solving a difficult task as a function of IQ, if one uses item response theory.\n\nAnother way to see it is to look at the practical outcomes. For instance, [IQ appears to be exponentially related to income](https://humanvarieties.org/2016/01/31/iq-and-permanent-income-sizing-up-the-iq-paradox/).\n\nBut I think the most fundamental way to understand it is that these methods assume that measurement error is independent of the measured quantity, such that there is the same amount of error in the ranking of the best as in the ranking of the worst. [In linear scales, measurement error is usually proportional to the measured quantity](https://tailcalled.substack.com/p/ldsl3-information-orientation-is), so in order for it to be independent, one must take the logarithm.\n\nLog scales need a base for addition\n-----------------------------------\n\nIt might be tempting to think that a log scale is equivalent to a linear scale, since you can just take the exponential of it. However, there are many different exponential functions: 2^x, e^x, 10^x, ….\n\nIf n=a^x, and m=a^y, then nm=a^(x+y). So we can add log-scaled numbers, and this simply corresponds to multiplying their linearly-scaled values, even if we don’t know what base is most natural.\n\nHowever, there is no corresponding expression for n+m. We can at best give some limiting values, e.g. as (y-x)ln(a) goes to infinity, n+m approaches a^y. We can use this limit to approximate n+m as a^max(x, y), but this approximation fails badly when you start summing up tons of values of similar size. (In practical terms, a tank plus a speck of dust is equal to a tank, but an apple and an orange is more than just an apple or just an orange.)\n\nAddition lets you infer large things from an enumeration of small things\n------------------------------------------------------------------------\n\nYou need addition to even know whether there seems to be a problem worth performing inference on in the first place. For instance, imagine that the prices of various goods have changed. If you have some idea of living standards that must be achieved, you can add up the prices of the goods needed to achieve these living standards, in order to see if the cost of living has changed.\n\nCould you have done this purely multiplicatively, by e.g. taking the geometric mean of the relative changes in each type of good? (Or equivalently, by averaging the changes in the logarithms of the prices?) No, because the price of a few big things (e.g. housing) might have gone one way, while the price of many small things might have gone the other way. A geometric mean of relative changes would ignore the magnitude of the prices, and instead mainly consider the number of goods.\n\nPicking the right base might not be trivial\n-------------------------------------------\n\nPicking the right base might seem trivial. For instance, the definition of Elo scores seems to imply that n=10^(x/400). Given this linearization, the linear scores are directly proportional to the probability of winning a match: in a game between a player with Elo x, and a player with Elo y, if we let n=10^(x/400) and m=10^(y/400), then the probability of the first player winning is n/(n+m).\n\nThis is relatively sensible, but the issue is that it is not compositional; for instance the scores will not be proportional to the probability of winning two matches. This probability would instead by (n/(n+m))^2 = n^2/(n^2 + m^2 + 2nm). Similarly, the Elo score presumably does not divide neatly when considering things like the probability of making a good move.\n\nThis lack of compositionality undermines the point of picking a base, because we wanted to pick a base in order to sum different things together.",
      "plaintextDescription": "This post is also available on my Substack.\n\nIn the previous post, I discussed the possibility of doing ordinal comparison between different entities. Yet usually when we think about measurement, we think of it as involving putting numbers to things, not just ranking them.\n\nThere are quite a few methods to convert rankings into numbers, such as Elo scores or item response theory. Different such methods generally yield extremely similar results, and are often based probabilities. For instance, in Chess, if two players’ ability differ by 400 Elo points, then the odds of the better player winning are 10:1.\n\nAre these methods reasonable? Can they go wrong?\n\n\nQuantified ordinals are ~log scales\nIf you look at the formula for Elo scores, it involves an exponential function, so each time the Elo changes by 400, the odds of winning changes by a factor of 10:1 (by definition). A similar principle applies to the probability of solving a difficult task as a function of IQ, if one uses item response theory.\n\nAnother way to see it is to look at the practical outcomes. For instance, IQ appears to be exponentially related to income.\n\nBut I think the most fundamental way to understand it is that these methods assume that measurement error is independent of the measured quantity, such that there is the same amount of error in the ranking of the best as in the ranking of the worst. In linear scales, measurement error is usually proportional to the measured quantity, so in order for it to be independent, one must take the logarithm.\n\n\nLog scales need a base for addition\nIt might be tempting to think that a log scale is equivalent to a linear scale, since you can just take the exponential of it. However, there are many different exponential functions: 2^x, e^x, 10^x, ….\n\nIf n=a^x, and m=a^y, then nm=a^(x+y). So we can add log-scaled numbers, and this simply corresponds to multiplying their linearly-scaled values, even if we don’t know what base is most natural.\n\nHowever, there is no co",
      "wordCount": 748
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "7YwxaX7qzQfBDmxzR",
    "title": "[LDSL#5] Comparison and magnitude/diminishment",
    "slug": "ldsl-5-comparison-and-magnitude-diminishment",
    "url": null,
    "baseScore": 25,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-12T18:47:20.546Z",
    "contents": {
      "markdown": "*This post is also available on* [*my Substack*](https://tailcalled.substack.com/p/ldsl5-comparison-and-magnitudediminishment)*.*\n\nIn [the previous post on LDSL](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/QCWYYirX3jtF3KmGc), I argued that an important part of causal inference is to look at the larger things. This implies a method of comparison to rank things by magnitude. [^q9rv1duducp]\n\nHomogenous comparison\n---------------------\n\nWhen the things to rank are homogenous, ranking them tends to be easy, because one can set them up as opposing forces and see which one dominates. A classic example is ranking objects by weight, where an old-school balance scale makes the gravity of the objects oppose each other, thereby lifting up the lighter one.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F364c3564-bb56-409f-9852-056c17aa2b8e_1024x1024.webp)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F364c3564-bb56-409f-9852-056c17aa2b8e_1024x1024.webp)\n\nDirect opposition is not the only method of measurement. Alternatively, one can set up the things to both influence a standardized measurement instrument, and then compare their influence on that instrument. For instance, to compare the weight of things, one can try lifting each thing, and feeling which one requires more force to lift.\n\nInhomogenous comparison\n-----------------------\n\nIn order to apply the method of root cause analysis I suggested, one doesn’t just need to be able to compare equivalent things, but also to compare inequivalent things. For instance, to figure out the biggest factors to investigate for why an election went a certain way, it is not just enough to understand the vote counts in individual voting districts. It is also necessary to compare the importance across different kinds of issues, like scandals, politician positions, politician traits, and an endless number of domains that you wouldn’t even think about because they are obviously too unimportant to matter.\n\nAs far as I know, this sort of inhomogenous comparison inherently carries some subjectivity to it. What’s most, an apple or an orange? I can’t see it making sense to declare anything specific to be the correct answer here.\n\nYet, what’s most, a piece of dust or a tank? Obviously a tank. When you have qualitatively different things, but they differ by orders of magnitude, it seems much more likely that there’s a sensible ordering to things. A primitive way we could think about this is to imagine something like a principle component analysis over the different ways of ordering things, taking the overall “general factor”. Yet this is somewhat unsatisfying, since presumably the resulting order depends a lot on what comparison methods we include, and it’s unclear what the “meaning” of the resulting ordering is, causally speaking.\n\nOne solution that makes sense to me is to think in terms of diminishment rather than magnitude. Even small things like blankets can produce dust as a side-effect; meanwhile a tank needs an entire factory to be produced, and is part of an intentional preparation for war by an industrial state. Generally, larger things produce smaller things, a principle that is particular clear in cases like physics where laws such as conservation of mass or energy means that it is *impossible* for smaller things to produce larger things without external support. The more broadly we can extend these principles, the more effectively we can reason about root causes.\n\n[^q9rv1duducp]: Ranking is not the only question relevant for measurement. There is also the question of quantification, which I will address in a later post.",
      "plaintextDescription": "This post is also available on my Substack.\n\nIn the previous post on LDSL, I argued that an important part of causal inference is to look at the larger things. This implies a method of comparison to rank things by magnitude. [1]\n\n\nHomogenous comparison\nWhen the things to rank are homogenous, ranking them tends to be easy, because one can set them up as opposing forces and see which one dominates. A classic example is ranking objects by weight, where an old-school balance scale makes the gravity of the objects oppose each other, thereby lifting up the lighter one.\n\n\n\nDirect opposition is not the only method of measurement. Alternatively, one can set up the things to both influence a standardized measurement instrument, and then compare their influence on that instrument. For instance, to compare the weight of things, one can try lifting each thing, and feeling which one requires more force to lift.\n\n\nInhomogenous comparison\nIn order to apply the method of root cause analysis I suggested, one doesn’t just need to be able to compare equivalent things, but also to compare inequivalent things. For instance, to figure out the biggest factors to investigate for why an election went a certain way, it is not just enough to understand the vote counts in individual voting districts. It is also necessary to compare the importance across different kinds of issues, like scandals, politician positions, politician traits, and an endless number of domains that you wouldn’t even think about because they are obviously too unimportant to matter.\n\nAs far as I know, this sort of inhomogenous comparison inherently carries some subjectivity to it. What’s most, an apple or an orange? I can’t see it making sense to declare anything specific to be the correct answer here.\n\nYet, what’s most, a piece of dust or a tank? Obviously a tank. When you have qualitatively different things, but they differ by orders of magnitude, it seems much more likely that there’s a sensible ordering to things. A pr",
      "wordCount": 504
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "QCWYYirX3jtF3KmGc",
    "title": "[LDSL#4] Root cause analysis versus effect size estimation",
    "slug": "ldsl-4-root-cause-analysis-versus-effect-size-estimation",
    "url": null,
    "baseScore": 30,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-11T16:12:14.604Z",
    "contents": {
      "markdown": "*Followup to:* [*Information-orientation is in tension with magnitude-orientation*](https://www.lesswrong.com/posts/hNSNhiLw4j6AReuEh/ldsl-3-information-orientation-is-in-tension-with-magnitude)*. This post is also available on* [*my Substack*](https://tailcalled.substack.com/p/ldsl4-root-cause-analysis-versus)*.*\n\nIn the conventional theory of causal inference, such as [Rubin’s potential outcomes model](https://en.wikipedia.org/wiki/Rubin_causal_model) or [Pearl’s DAG approach](https://en.wikipedia.org/wiki/The_Book_of_Why), causality is modelled as a relationship of functional determination, X := f(Y). The question of interest becomes to study the properties of f, especially the difference in f across different values of Y. I would call this “effect size estimation”, because the goal is to give quantify the magnitude of an effect of one variable on another.\n\nBut as I mentioned in [my post on conundrums](https://tailcalled.substack.com/p/ldsl0-some-epistemological-conundrums), people seem to have some intuitions about causality that don’t fit well into effect size estimation, most notably in wanting “the” cause of some outcome when really there’s often thought to be complex polycausality.\n\n[Linear diffusion of sparse lognormals](https://tailcalled.substack.com/p/ldsl2-latent-variable-models-network) provides an answer: an outcome is typically a mixture of many different variables, X := Σi Yi, and one may desire an account which describes how the outcome breaks down into these variables to better understand what is going on. This is “root cause analysis”, and it yields one or a small number of factors because most of the variables tend to be negligible in magnitude. (If the root cause analysis yields a large number of factors, that is evidence that the RCA was framed poorly.)\n\nIs root cause analysis a special case of effect size estimation?\n----------------------------------------------------------------\n\nIf you know X, Y, and f, then it seems you can do root cause analysis automatically by setting each of the Y’s to zero, seeing how it influences X, and then reporting the Y’s in descending order of influence. Thus, root cause analysis ought to be a special-case of effect size estimation, right?\n\nThere are two big flaws with this view:\n\n*   As you estimate f, you use a particular ontology for Y; for instance the states of the system you are studying. But the root cause may be something that fits in poorly to the ontology you use, for instance [a multiplicative interaction between multiple variables in the system](https://tailcalled.substack.com/p/ldsl1-performance-optimization-as), or an exogenous variable from outside the system which causes correlated changes to inside the system.\n*   Identifying the root cause through estimating f requires you to have an extremely detailed model of the system that takes pretty much everything into account, yet this may be prohibitively difficult to do.\n\nYou can try to use statistical effect size estimation for root cause analysis. However, [doing so creates an exponentially strong bias in favor of common things over important things](https://tailcalled.substack.com/p/ldsl3-information-orientation-is), so it’s unlikely to work unless you can somehow absorb all the information in the system.\n\nHeuristics for direct root cause analysis\n-----------------------------------------\n\nI don’t think I have a complete theory of root cause analysis yet, but I know of some general heuristics for root cause analysis which don’t require comprehensive effect size estimation.\n\n*   You can look at the most extreme things that were happening in the system, aggregating it across different angles until you find the root cause you are looking for.\n*   You can trace the causality backwards from X, looking only at the parts of f that are immediately relevant for X, and therefore not needing to estimate f in its entirity.\n\nThese both require a special sort of data, which I like to think of as “[accounting](https://www.lesswrong.com/posts/fJvjin8ETkzhFdadC/accounting-for-college-costs) data”. It differs from statistical data in that it needs to be especially comprehensive and quantitative. It would often be hard to perform this type of inference using a small random sample of the system, at least unless the root cause affects the system extraordinarily broadly.",
      "plaintextDescription": "Followup to: Information-orientation is in tension with magnitude-orientation. This post is also available on my Substack.\n\nIn the conventional theory of causal inference, such as Rubin’s potential outcomes model or Pearl’s DAG approach, causality is modelled as a relationship of functional determination, X := f(Y). The question of interest becomes to study the properties of f, especially the difference in f across different values of Y. I would call this “effect size estimation”, because the goal is to give quantify the magnitude of an effect of one variable on another.\n\nBut as I mentioned in my post on conundrums, people seem to have some intuitions about causality that don’t fit well into effect size estimation, most notably in wanting “the” cause of some outcome when really there’s often thought to be complex polycausality.\n\nLinear diffusion of sparse lognormals provides an answer: an outcome is typically a mixture of many different variables, X := Σi Yi, and one may desire an account which describes how the outcome breaks down into these variables to better understand what is going on. This is “root cause analysis”, and it yields one or a small number of factors because most of the variables tend to be negligible in magnitude. (If the root cause analysis yields a large number of factors, that is evidence that the RCA was framed poorly.)\n\n\nIs root cause analysis a special case of effect size estimation?\nIf you know X, Y, and f, then it seems you can do root cause analysis automatically by setting each of the Y’s to zero, seeing how it influences X, and then reporting the Y’s in descending order of influence. Thus, root cause analysis ought to be a special-case of effect size estimation, right?\n\nThere are two big flaws with this view:\n\n * As you estimate f, you use a particular ontology for Y; for instance the states of the system you are studying. But the root cause may be something that fits in poorly to the ontology you use, for instance a multiplicative inter",
      "wordCount": 604
    },
    "tags": [
      {
        "_id": "cq69M9ceLNA35ShTR",
        "name": "Causality",
        "slug": "causality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "hNSNhiLw4j6AReuEh",
    "title": "[LDSL#3] Information-orientation is in tension with magnitude-orientation",
    "slug": "ldsl-3-information-orientation-is-in-tension-with-magnitude",
    "url": null,
    "baseScore": 34,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-08-10T21:58:27.659Z",
    "contents": {
      "markdown": "*Followup to:* [Latent variable models, network models, and linear diffusion of sparse lognormals](https://www.lesswrong.com/posts/MwgKqT2sJQnYb3q26/ldsl-2-latent-variable-models-network-models-and-linear). *This post is also available on* [*my Substack*](https://substack.com/@tailcalled/p-147552502)*.*\n\nLet’s say you are trying to make predictions about a variable. For example, maybe you are an engineer trying to keep track of how well the servers are running. It turns out that if you use the obvious approach advocated by e.g. frequentist statistics [^tlnaedq7kjp], you will have huge biases in what you pay attention to, compared to what you should pay attention to, because you will disregard big things in favor of common things. Let’s make a mathematical model of this.\n\nMeasurement error is proportional to magnitude\n----------------------------------------------\n\nBecause of background factors that fluctuate at a greater frequency than you can observe/model, and because of noise that enters through your indirect means of observing said variable, the reliable portion of the variable will be mixed together with some unpredictable noise. For example, as the engineer observes the server logs, they will see some quite significant fluctuations that are basically unattributable without a lot of care and work.\n\nThere are two fundamentally different kinds of fluctuations:\n\n*   Fluctuations in the ordinary mechanisms that usually dominate the variable you are tracking (e.g. CPU load or database load),\n*   Outliers/exogenous factors which trigger an entirely different mode (e.g. bugs after updates, or deadlocks)\n\nIn either case, in order for the fluctuation to be relevant, it has to have an influence that is at least proportional to the ordinary level for the variable; if it was much smaller, it would be negligible. We’d expect some multiplicative fluctuations to exist simply because things tend to work through multiplicative mechanisms, so as a lower bound on the fluctuations, it’s typically the case that there will be linearly proportional/non-negligible noise.\n\nConversely, if the noise is much than the variable you are tracking, it would be hard to make out the original variable, and you would pick a different set of indicators with less noise, so linearly proportional noise also works as an upper bound.\n\nInformation-orientation introduces massive biases\n-------------------------------------------------\n\nIf we try to apply simple statistics to the variable, it will mostly yield garbage. For instance, if we take the average over a short period of time, it can fluctuate wildy due to the outliers, and if we try to apply a regression analysis, it will be strongly confounded by outliers. If we had enough sample size, it’s conceivable we could just average ridiculous amounts of data together to ensure that we only consider reliable results. However, if instead we are non-omniscient, have limited data, and we want to extract as much information as efficiently as possible from the observations, we instead need to be more careful.\n\nA natural way to think about these is that averages and least squares linear regression try to minimize absolute errors[^38smt4w0jyk], but if we want to be robust to noise, we need to instead minimize relative errors. That is, if we predicted that the variable is 10, but it really is 20, that is a much worse prediction than if we predicted that the variable is 110 but it really is 120.\n\nThere’s multiple basically isomorphic ways we can formalize this, for instance by fitting a lognormal distribution to the data. However, let’s use a more arithmetically convenient simplified model: if X is the variable and P is the observed proxy, then we can say that log(P) ≈ log(X) + log(E). Here, E is fluctuation/measurement error, expressed multiplicatively as a number to be multiplied by X to get the observation. By taking the logarithms, we make multiplicative fluctuations drop out into additive fluctuations, and we bring outliers in the distribution back into the bulk of the distribution.\n\nNow we can use some standard normal distribution approximations to get at how statistics twist things. Let’s say that there’s a subgroup s of units (e.g. patients with a particular disease) for whom X is different, e.g. on average Xsubgroup=Xnorm+β. For this group, we have log(Psubgroup) ≈ log(Xnorm+β) + log(E) = log(Xnorm) + log(1+β/Xnorm) + log(E).\n\nThus, log(P) is log(1+β/Xnorm) higher in the subgroup than in the overall group. But we can't measure this exactly since there will be some noise depending on log(E). This noise will shrink with sqrt(N), where N is the sample size of the subgroup. So overall our statistical signal for how much is going on with the group will be something like log(1+β/Xnorm) sqrt(N) / std(log(E)).\n\nObviously if you want to have as much information as possible, you are going to prefer less noisy data, as expressed by the 1/std(log(E)) term. But the actual importance of the subgroup ought to be given by something like βN, yet for statistical purposes it appears their detectability is more like log(β) sqrt(N), which not only massively underrates the importance of long-tailed effects, but also favors commonality over effect size, to the point where it becomes exponentially more important for a group to be common than for them to have a large difference from the norm, in order for it to be detectable.\n\nInformation overload\n--------------------\n\nIf you were just dealing with a single subgroup, this would probably be fine. At some point you get enough sample size to detect the subgroup, at which point you can use more direct means to estimate the correct β.\n\nThe issue is that the real world often contains tons of subgroups. If you are dealing with hundreds of thousands of subgroups, you will find many subgroups of infinitesimal importance before you find the ones that really matter. You can then either let yourself be distracted by all these subgroups, or you can conclude that the statistical approach to identifying them doesn’t really work.\n\nBut if raw statistics don’t work, then what does work? It seems to me that there’s a need to be biased towards cases where X is big. Maybe this bias can be derived purely from maximizing probabilistic fit, and if so I have some ideas how that could be achieved, which I will get into later in the series. But I suspect ultimately we simply have to bring this bias with us, as a deviation from raw probability theory. In the next post, I will discuss the main way we can use a magnitude-based bias to infer things.\n\n[^tlnaedq7kjp]: Bayesian probability kind of has a similar problem, in that the statistics work out the same. However, if you are willing to let yourself get close to getting Pascal-mugged, I could imagine Bayesians could dodge it by chasing things with small probabilities of being really big. \n\n[^38smt4w0jyk]: Well, absolute squared errors. Point is, not relative errors.",
      "plaintextDescription": "Followup to: Latent variable models, network models, and linear diffusion of sparse lognormals. This post is also available on my Substack.\n\nLet’s say you are trying to make predictions about a variable. For example, maybe you are an engineer trying to keep track of how well the servers are running. It turns out that if you use the obvious approach advocated by e.g. frequentist statistics [1], you will have huge biases in what you pay attention to, compared to what you should pay attention to, because you will disregard big things in favor of common things. Let’s make a mathematical model of this.\n\n\nMeasurement error is proportional to magnitude\nBecause of background factors that fluctuate at a greater frequency than you can observe/model, and because of noise that enters through your indirect means of observing said variable, the reliable portion of the variable will be mixed together with some unpredictable noise. For example, as the engineer observes the server logs, they will see some quite significant fluctuations that are basically unattributable without a lot of care and work.\n\nThere are two fundamentally different kinds of fluctuations:\n\n * Fluctuations in the ordinary mechanisms that usually dominate the variable you are tracking (e.g. CPU load or database load),\n * Outliers/exogenous factors which trigger an entirely different mode (e.g. bugs after updates, or deadlocks)\n\nIn either case, in order for the fluctuation to be relevant, it has to have an influence that is at least proportional to the ordinary level for the variable; if it was much smaller, it would be negligible. We’d expect some multiplicative fluctuations to exist simply because things tend to work through multiplicative mechanisms, so as a lower bound on the fluctuations, it’s typically the case that there will be linearly proportional/non-negligible noise.\n\nConversely, if the noise is much than the variable you are tracking, it would be hard to make out the original variable, and you would ",
      "wordCount": 1047
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MwgKqT2sJQnYb3q26",
    "title": "[LDSL#2] Latent variable models, network models, and linear diffusion of sparse lognormals",
    "slug": "ldsl-2-latent-variable-models-network-models-and-linear",
    "url": null,
    "baseScore": 26,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-08-09T19:57:56.122Z",
    "contents": {
      "markdown": "*This post is also available on my* [*Substack*](https://tailcalled.substack.com/p/ldsl2-latent-variable-models-network)*.*\n\nWhere [we last left off](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/yJEf2TpPJstfScSnt), I proposed that life cannot be understood through statistics, but rather requires more careful study of individual cases. Yet I don’t think this is actually convincing to the statistical/academic mindset unless we have a statistical description of the phenomenon. To understand it, I’ve developed a new explicit model for the relationship between statistics and reality: linear diffusion of sparse lognormals.\n\nBut in order to explain this, I first want to bring up the two classical models of the relationship, as usually presented in psychometrics.[^fxnmsbfgljg]\n\nLatent variable models and network models\n-----------------------------------------\n\nWe don’t have direct access to raw reality; instead to infer things about reality, we have some sensory inputs (e.g. for humans, eyes; for psychometricians, surveys; for computers, network and storage IO) which we can use to infer things that exist. This is called a latent variable model:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd177b1-5a48-45e2-ba6c-8cb3ec2024c5_662x187.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd177b1-5a48-45e2-ba6c-8cb3ec2024c5_662x187.png)\n\nGenerally, a single variable does not give us enough evidence to robustly infer anything. For instance a single light ray entering your eyes does not teach you what object it reflected off of. Instead, you need *many* observations to infer things about reality, which requires the latent variable to influence *many* different variables.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a122d47-a0b4-491d-bf72-bebbb9e3a1b9_648x260.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a122d47-a0b4-491d-bf72-bebbb9e3a1b9_648x260.png)\n\nIf you’ve got a large dataset, and you are willing to assume that there’s much fewer factors that matter in reality than there are observations, you can use fairly straightforward methods to fit a latent variable model to this dataset, which tends to give fairly “reasonable” results, in the sense that similar observations end up attributed to similar factors.\n\nSometimes people find latent factor models unbelievable and unworkable. For instance, different mental health problems correlate with each other, so standard latent factor models might suggest thinking up broad mental health factors that influence many symptoms from poor sleep to anxiety. When treating people’s mental health, clinicians might often try to directly stabilize individual variables like sleep, which is somewhat strange if we think the root cause is something much broader.\n\nYet at the same time, obviously a lot of the time there are latent dynamics that need to be intervened on. Imagine trying to stop global warming by freezing lots of ice and transporting it to places, rather than developing nuclear and renewable energy; it would never work.\n\nSo the correct approach is to combine the two kinds of models: Reality is a network of variables that influence each other. Some variables, or collections of variables, have homogenous influences across a variety of outcomes, making them “general factors” and especially important. In particular the relationship between observation and reality must always be mediated by a general factor.\n\nDiffusion and interaction\n-------------------------\n\nIn both factor models and network models, you speak of variables influencing each other. Especially in network models[^sepqt8qphl], you often end up with cases where one variable is influenced by multiple other variables, i.e. X := Y + Z.\n\nHowever, when dealing with something polycausal, there’s really multiple ways the influence can go. In the additive example above, Y and Z are independently influencing X, so we can think of this as being a sort of [diffusion](https://en.wikipedia.org/wiki/Heat_equation), where Y and Z can “spread into” X, similar to how heat from different heat sources (e.g. a cup of tea and a fireplace) can spread into a common target (e.g. a cold person).\n\nThe other major possibility than addition is multiplication, i.e. X := Y * Z. Here Y and Z are dependent on each other in their influence on X; for instance, you could imagine Y as a skilled entrepreneur, and Z as a market opportunity, and X as a business. The business requires *both* the entrepreneur and the opportunity.\n\nLinear diffusion of sparse lognormals\n-------------------------------------\n\nSometimes things interact multiplicatively, generating outcomes that span orders of magnitude. Then these outcomes send their influence out through the world, influencing different things according to the network structure of the world. This influence is mostly diffusion because most things are basically independent of each other. However, occasionally the diffusion crosses into some multiplicative dynamic, which can lead to it suddenly halting or exploding into some extreme outcome, depending on the particulars of the diffusion.\n\nLet’s take a epidemic as an example. There’s an endless number of germs of different species spreading around. Most of them don’t make much difference for us. But occasionally, one of them gains the capacity to spread more rapidly from person to person, which leads to an epidemic. Here, the core factor driving the spread of the disease is the multiplicative interaction between infected and uninfected people, and the key change that changes it from negligible to important is the change in the power of this interaction.\n\nOne it has infected someone, it can have further downstream effects, in that it makes them sick and maybe even kills them. (And whether it kills them or not, this sickness is going to have further downstream effects in e.g. interrupting their work.) But these downstream effects are critically different from the epidemic itself, in that they cannot fuel the infection further. Rather, they are directly dependent on the magnitude of people infected.\n\nProperly generalized, this applies to everything in life, from companies to medicine to programming to psychology to physics. I think of it as being closely related to thermodynamics; an extreme outcome is like a low-entropy state (or high-free-energy state), and its influence on the world must mostly be diffusive, because entropy must increase. The occasional appearances of new phenomena cannot be truly out of nowhere, but must be due to an interaction which draws on prior low-entropy high-energy states.\n\n[^fxnmsbfgljg]: Psychometrics tend to have quite elaborate philosophy of measurement, probably because psychological measurement tends to be so bad and noisy that there’s a lot of desire for better interpretation of it. \n\n[^sepqt8qphl]: But also sometimes in latent variable models, a phenomenon known in psychometric circles as “interstitiality”.",
      "plaintextDescription": "This post is also available on my Substack.\n\nWhere we last left off, I proposed that life cannot be understood through statistics, but rather requires more careful study of individual cases. Yet I don’t think this is actually convincing to the statistical/academic mindset unless we have a statistical description of the phenomenon. To understand it, I’ve developed a new explicit model for the relationship between statistics and reality: linear diffusion of sparse lognormals.\n\nBut in order to explain this, I first want to bring up the two classical models of the relationship, as usually presented in psychometrics.[1]\n\n\nLatent variable models and network models\nWe don’t have direct access to raw reality; instead to infer things about reality, we have some sensory inputs (e.g. for humans, eyes; for psychometricians, surveys; for computers, network and storage IO) which we can use to infer things that exist. This is called a latent variable model:\n\n\n\nGenerally, a single variable does not give us enough evidence to robustly infer anything. For instance a single light ray entering your eyes does not teach you what object it reflected off of. Instead, you need many observations to infer things about reality, which requires the latent variable to influence many different variables.\n\n\n\nIf you’ve got a large dataset, and you are willing to assume that there’s much fewer factors that matter in reality than there are observations, you can use fairly straightforward methods to fit a latent variable model to this dataset, which tends to give fairly “reasonable” results, in the sense that similar observations end up attributed to similar factors.\n\nSometimes people find latent factor models unbelievable and unworkable. For instance, different mental health problems correlate with each other, so standard latent factor models might suggest thinking up broad mental health factors that influence many symptoms from poor sleep to anxiety. When treating people’s mental health, clinicians m",
      "wordCount": 937
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "yJEf2TpPJstfScSnt",
    "title": "[LDSL#1] Performance optimization as a metaphor for life",
    "slug": "ldsl-1-performance-optimization-as-a-metaphor-for-life",
    "url": null,
    "baseScore": 32,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-08-08T16:16:27.349Z",
    "contents": {
      "markdown": "*Followup to:* [*Some epistemological conundrums*](https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62/p/X98LpnHSDWG3FEC5e)*. Response to:* [*The omnigenic model as a metaphor for life*](https://slatestarcodex.com/2018/09/13/the-omnigenic-model-as-metaphor-for-life/)*. Part 1 in this post will introduce a concept from programming, part 2 will apply it to life in general. This post is also available on* [*my Substack*](https://tailcalled.substack.com/p/ldsl1-performance-optimization-as)*.*\n\nPart 1: Performance optimization\n--------------------------------\n\nIn computer programming, there is a tradeoff between software that is quick and easy for humans to write and read, versus software that is fast for the computer to execute. Moving from the former to the latter is called “performance optimization”.\n\nMoving from the latter to the former is called “high-level rewriting”. This is rarely done intentionally, but a substantial part of the reason computers are sometimes still sluggish despite having gotten orders of magnitude faster is that high-level rewriting has happened implicitly as old software that focused more on performance has been replaced with new software that focuses more on developer speed.\n\nBecause programming is done at such a high level, there is almost always a way to do performance optimization in any piece of code. Yet, it’s considered a beginner mistake to just pick a random piece of code to go optimize, which is sometimes encapsulated in sayings like “premature optimization is the root of all evil” or “[the epsilon fallacy](https://www.lesswrong.com/posts/PmaBozEroBg73a5Di/the-epsilon-fallacy)”. Instead, a recommended approach is to run the code, measure what takes the most resources, and fix that. But why is that so important?\n\nComputers are fast, or alternatively, software/data/people/society is very slow and long-term-oriented compared to individual instructions. Let’s say there’s something a computer can do in 1 microsecond, but you write the code inefficiently, so it takes 1000x too long. This means that it takes… 0.001 seconds, aka basically nothing in most circumstances.\n\nBut let’s say that it’s an operation that you need to apply to 10000 datapoints, so you need to repeat it 10000 times. Now suddenly it takes 10 seconds, which is most likely worth optimizing, as otherwise you wouldn’t e.g. want to wait 10 seconds for a page to load. If you need to apply it to each pair of the 10000 datapoints, then you’ve got a 10000 by 10000 grid, or 100000000 repetitions, which will take more than a day to execute, so this absolutely needs to be optimized.\n\nUsually, an execution of some code consists of many pieces whose resource use adds up. Some of these pieces just run a quick operation once, whereas others are multiplied by numbers such as dataset size, internet latency, or similar. This leads to resource use varying by orders of magnitude. If you add up the few pieces that take up the most resources, then there’s usually not much unexplained resource usage, so you should focus on optimizing those pieces, and can’t get much from optimizing the other pieces.\n\nPart 2: As a metaphor for life\n------------------------------\n\nIn [the omnigenic model as a metaphor for life](https://slatestarcodex.com/2018/09/13/the-omnigenic-model-as-metaphor-for-life/), Scott Alexander presents Fisher’s infinitesimal model (called “the omnigenic model”), which asserts that genes influence phenotypes through a large number of variants of small individual effect. He suggests using this as a general causal model, not just for genomics, but for science and all of life.\n\nScott Alexander is vaguely aware that this causal model doesn’t quite cover everything:\n\n> Now the biopsychosocial model has caught on and everyone agrees that depression is complicated. I don’t know if we’re still at the “dozens of things” stage or the “hundreds of things stage”, but I don’t think anyone seriously thinks it’s fewer than a dozen. The structure of depression seems different from the structure of genetic traits in that one cause can still have a large effect; multiple sclerosis might explain less than 1% of the variance in depressedness, but there will be a small sample of depressives whose condition is almost entirely because of multiple sclerosis. But overall, I think the analogy to genetics is a good one.\n\n… but he doesn’t go that much in depth about it, and ultimately still suggests using similar approaches to genomics, at least as a conceptual ideal, with the greatest challenge being the effect size estimation.\n\nYet, we could instead use performance optimization as a metaphor for life. Multiple sclerosis only causes slight variance in depressedness, but in those people where it is the cause, not only does it have a huge effect, but also it is presumably relatively obvious, since it is a strong condition with lots of other effects. Multiple sclerosis cannot yet be cured, but it can be somewhat mitigated with certain medical treatments. This I assume is superior to simply treating the depression (with antidepressants?), e.g. since it also helps with other MS effects than simply depression.\n\nI would use performance optimization as a metaphor for life. Similar to how poor software performance in a big piece of code tends to come from singular loops, problems and opportunities in life in general tend to be rare, extreme, individual things.\n\nScott Alexander brings up another good example in a different blog post, [How Bad Are Things?](https://slatestarcodex.com/2015/12/24/how-bad-are-things/)\n\n> A perfectly average patient will be a 70 year old woman who used to live somewhere else but who moved her a few years ago after her husband died in order to be closer to family. She has some medical condition or other that prevents her from driving or walking around much, and the family she wanted to be closer to have their own issues, so she has no friends within five hundred miles and never leaves her house except to go to doctors’ appointments. She has one son, who is in jail, and one daughter, who married a drug addict. She also has one grandchild, her only remaining joy in the world – but her drug-addict son-in-law uses access to him as a bargaining chip to make her give him money from her rapidly-dwindling retirement account so he can buy drugs. When she can’t cough up enough quickly enough, he bans her from visiting or talking to the grandchild, plus he tells the grandchild it’s her fault. Her retirement savings are rapidly running out and she has no idea what she will do when they’re gone. Probably end up on the street. Also, her dog just died.\n> \n> If my patients were to read the above paragraph, there are a handful who would sue me for breach of confidentiality, assuming I had just written down their medical history and gotten a couple of details like the number of children wrong. I didn’t. This is a type.\n\nHere we have a few large factors (most notably wealth, the grandchild, [drugs](https://www.astralcodexten.com/p/drug-users-use-a-lot-of-drugs), and aging) which *multiplicatively* interact to produce a terrible dynamic. If even one of these factors was small, this problem probably wouldn’t be occurring (though e.g. lack of wealth would replace it with a different problem, probably - unless she could move in with her daughter or something). This is precisely analogous to the multiplicative interaction in performance optimization.\n\nIf this is how life usually looks, then it suggests precisely the opposite model from the omnigenic model as a metaphor for life. Using statistics to explain variance works well if you have a homogenous population, as then you can find the parameters for the mechanism you are studying. However, statistics works less well if you have a distribution with varying mechanisms.\n\nProbably the biggest problem with statistics is that one might e.g. take the logarithm, in order to capture the multiplicative effects and reduce the sensitivity to outliers. But this makes the statistics entirely focus on the common factors and ignore the special ones, which is a problem because with long tails the special factors are all that matters.\n\nIf you are dealing with a case that is affected by a mixture of many different long-tailed phenomena, you can understand it well by figuring out which factors are extreme for this particular case, and what their causes and consequences are. While there will be lots of factors that this doesn’t account for, these factors are “small” and thus don’t really matter.\n\nYou might think this is just an artifact of dealing with clinical subjects, who are pre-selected for having a pathology, but this holds even when navigating the absolute basics of your everyday life:\n\n[![A wooden table with a rustic finish, featuring various objects. On the table, there is a ceramic mug filled with steaming coffee, an open hardcover book with a few pages slightly curled, a vintage brass pocket watch with its chain neatly coiled beside it, and a small potted succulent plant. The background includes a warm, cozy room with soft lighting.](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33070f05-59bc-40a9-9107-af87d5dd07bf_1024x1024.webp)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33070f05-59bc-40a9-9107-af87d5dd07bf_1024x1024.webp)\n\nA good analogy would be a table, like in the image above. There’s some big factors that influenced this table; someone put a book and a cup and a watch and a potted plant on it. But these factors don’t actually account for most of the log-variance of the table. The boards of the table have a grain with lots of detailed indentations; the wood is probably covered in dust and microbes of different species. The microbes’ DNA has a lot of entropy, not just in the species-shared aspects, but also in their individual mutations. Further, every molecule vibrates with its own chaotic heat.\n\nOne cannot describe the table, only a very small fraction of the factors that influence it. However, the factors vary wildly in their magnitude, and by describing the biggest factors on it, one gets a pretty good idea about the things that matter.\n\n(Even if there are lots of small factors that seem to add up to a big dangerous factor, e.g. some of the microbes are of a dangerous ebola species that could kill you, it is still more feasible to describe them using the cause, e.g. a spill of a sample containing ebola, than it is to list each ebola microbe individually.)",
      "plaintextDescription": "Followup to: Some epistemological conundrums. Response to: The omnigenic model as a metaphor for life. Part 1 in this post will introduce a concept from programming, part 2 will apply it to life in general. This post is also available on my Substack.\n\n\nPart 1: Performance optimization\nIn computer programming, there is a tradeoff between software that is quick and easy for humans to write and read, versus software that is fast for the computer to execute. Moving from the former to the latter is called “performance optimization”.\n\nMoving from the latter to the former is called “high-level rewriting”. This is rarely done intentionally, but a substantial part of the reason computers are sometimes still sluggish despite having gotten orders of magnitude faster is that high-level rewriting has happened implicitly as old software that focused more on performance has been replaced with new software that focuses more on developer speed.\n\nBecause programming is done at such a high level, there is almost always a way to do performance optimization in any piece of code. Yet, it’s considered a beginner mistake to just pick a random piece of code to go optimize, which is sometimes encapsulated in sayings like “premature optimization is the root of all evil” or “the epsilon fallacy”. Instead, a recommended approach is to run the code, measure what takes the most resources, and fix that. But why is that so important?\n\nComputers are fast, or alternatively, software/data/people/society is very slow and long-term-oriented compared to individual instructions. Let’s say there’s something a computer can do in 1 microsecond, but you write the code inefficiently, so it takes 1000x too long. This means that it takes… 0.001 seconds, aka basically nothing in most circumstances.\n\nBut let’s say that it’s an operation that you need to apply to 10000 datapoints, so you need to repeat it 10000 times. Now suddenly it takes 10 seconds, which is most likely worth optimizing, as otherwise you wouldn’t",
      "wordCount": 1640
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "X98LpnHSDWG3FEC5e",
    "title": "[LDSL#0] Some epistemological conundrums",
    "slug": "ldsl-0-some-epistemological-conundrums",
    "url": null,
    "baseScore": 55,
    "voteCount": 21,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2024-08-07T19:52:55.688Z",
    "contents": {
      "markdown": "*This post is also available on my* [*Substack*](https://tailcalled.substack.com/p/ldsl0-some-epistemological-conundrums)*.*\n\nWhen you deal with statistical science, causal inference, measurement, philosophy, rationalism, discourse, and similar, there’s some different questions that pop up, and I think I’ve discovered that there’s a shared answer behind a lot of the questions that I have been thinking about. In this post, I will briefly present the questions, and then in a followup post I will try to give my answer for them.\n\nWhy are people so insistent about outliers?\n-------------------------------------------\n\nA common statistical method is to assume an outcome is due to a mixture of observed factors and unobserved factors, and then model how much of an effect the observed factors have, and attribute all remaining variation to unobserved factors. And then one makes claims about the effects of the observed factors.\n\nBut some people then pick an outlier and demand an explanation for that outlier, rather than just accepting the general statistical finding:\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58fc7786-6e87-466c-a67a-3500bbe00e84_589x697.png)](https://twitter.com/timothycbates/status/1395076113983053828)\n\nIn fact, aren’t outliers almost by definition anti-informative? No model is perfect, so there’s always going to be cases we can’t model. By insisting on explaining all those rare cases, we’re basically throwing away the signal we *can* model.\n\n[A similar point applies to reading the news.](https://www.astralcodexten.com/p/against-learning-from-dramatic-events) Almost by definition, the news is about uncommon stuff like terrorist attacks, rather than common stuff like heart disease. Doesn’t reading such things [invert your perception](https://github.com/aaronpenne/data_visualization?tab=readme-ov-file#cause-of-death---reality-vs-google-vs-media), such that you end up focusing on exactly the least relevant things?\n\nWhy isn’t factor analysis considered the main research tool?\n------------------------------------------------------------\n\nTypically if you have a ton of variables, you can perform a factor analysis which identifies a set of variables which explain a huge chunk of variation across those variables. If you are used to performing factor analysis, this feels like a *great* way to get an overview over the subject matter. After all, what could be better than knowing the main dimensions of variation?\n\nYet a lot of people think of factor analysis as being superficial and uninformative. Often people insist that it only yields aggregates rather than causes, and while that might seem plausible at first, once you dig into it enough, you will see that usually the factors identified are actually causal, so that can’t be the real problem.\n\nA related question is why people tend to talk in funky discrete ways when careful quantitative analysis generally finds everything to be continuous. Why do people want clusters more than they want factors? Especially since cluster models tend to be more fiddly and less robust.\n\nWhy do people want “the” cause?\n-------------------------------\n\nThere’s a big gap between how people intuitively view causal inference (often searching for “the” cause of something), versus how statistics views causal inference. The main frameworks for causal inference in statistics are Rubin’s Potential Outcomes framework and Pearl’s DAG approach, and both of these view causality as a function from inputs to outputs. In these frameworks, causality is about functional input/output relationships, and [there are many different notions of causal effects](https://www.the100.ci/2024/06/26/sometimes-a-causal-effect-is-just-a-causal-effect-regardless-of-how-its-mediated-or-moderated/), not simply one canonical “cause” of something.\n\nWhy are people dissatisfied with GWAS?\n--------------------------------------\n\nIn genome-wide association searches, researchers use statistics to identify alleles that are associated with specific outcomes of interest (e.g. health, psychological characteristics, SES outcomes). They’ve been making consistent progress over time, finding tons of different genetic associations and gradually becoming able to explain more and more variance between people.\n\nYet GWAS is heavily criticized as “not causal”. While there are certain biases that can occur, those biases are usually found to be much smaller than seems justified by these critiques. So what gives?\n\nWhat value does qualitative research provide?\n---------------------------------------------\n\nQualitative research makes use of human intuition and observation rather than mathematical models and rigid measurements. But surely ultimately human cognition grounds out to some algorithms that could be formalized. Maybe it’s just a question of humans having more prior information and doing more comprehensive observation? But in that case, it seems like sufficiently intensive quantitative methods should outperform qualitative research, e.g. if you measure *everything* and throw it into some sort of AI-based autoregressive model. Right?\n\nYet this hasn’t worked out well so far. Is it just because we are not trying hard enough? For instance obviously human sight has much higher bandwidth than questionnaires, so maybe questionnaires would miss most of the information and we need some video surveillance thing with automatic AI tagging for it to work properly.\n\nOr is there some more fundamental difference between qualitative research and quantitative research?\n\nWhat’s the distinction between personality disorders and “normal” personality variation?\n----------------------------------------------------------------------------------------\n\nPersonality disorders resemble normal personality variation. Self-report scales meant to measure one often turn out to be basically identical to self-report scales meant to measure the other. This leads a lot of people to propose that personality disorders are just the extreme end of personality variation, but is that really true? If not, why not?\n\nWhat’s more, sometimes mental disorders really seem like they should be the extreme end of normal personality, but aren’t. For instance, Obsessive-Compulsive Disorder seems conceptually similar to Conscientious personality (in the Big 5 model), but results on whether they are connected are at best mixed and more realistically find that OCD is something different from Conscientiousness. Similarly, Narcissism is often described similarly to Disagreeable-tinted Extraversion, but it doesn’t appear to be the same.\n\nWhat is autism?\n---------------\n\nAutism is characterized by a combination of neurological symptoms and poor social skills. Certainly one of the strongest indicators of autism is that both autistic and non-autistic people tend to agree that autistic people have poor social skills, but quantitative psychologists often struggle with operationalizing social skills, and [in my own brief research on the topic](https://manifold.markets/tailcalled/how-does-autistic-social-performanc), I haven’t found a clear difference in social performance between autistic and non-autistic people. What’s going on?\n\nSome propose a double empathy problem, where autistic people aren’t necessarily socially broken, but rather have a different focus than non-autistic people. That may be true, but then what is that focus?\n\nSome propose an empathizing/systemizing tradeoff, where male and autistic brains are better able to deal with predictable systems, whereas female and allistic brains are better able to deal with people. Yet this seems to mix together technical interests with autistic interests, and mix together antisocial behavior with social confusion.\n\nAlso, why do some things, like excessively academic ideas that haven’t been tested in practice, seem similar to autism? Am I just confused, or is there something going on there?\n\nWhat is gifted child syndrome/twice-exceptionals?\n-------------------------------------------------\n\nThere’s this idea that autism, ADHD, and high IQ go together in a special way. That said, it’s not really borne out well statistically (autism and ADHD are if anything negatively correlated with IQ), and so presumably it’s just an artifact, right?\n\nWhat’s up with psychoanalysts?\n------------------------------\n\nPsychoanalysts have bizarre-to-me models, where it often seems like they treat people as extremely sensitive, prone to spinning out crazily after even quite mild environmental perturbations.\n\nWhen statistical evidence contradicts these views, they typically dismiss this by saying that the measurement is wrong because it lacks nuance, or because people lack self-awareness.\n\nYet to me that just raises the question of why they would conclude this in the first place.\n\nWhy are some ideas more “robust” than others?\n---------------------------------------------\n\nSome ideas seem “ungrounded”; informally, they are many layers of speculation removed from observations, and can turn out to be false and therefore worthless. Meanwhile, other ideas seem strongly “grounded”. Newtonian physics, even if it is literally false (as we have measured to high precision, relativity is more accurate) and ontologically confused (there isn’t some global flat space with masses that have definite positions and velocities), is still extremely useful.\n\nOne solution is to say that Newtonian physics is an approximation to the truth and that makes it quite useful. There’s some value to this answer, but it seems to suggest that the mere fact of being an approximation is sufficient to make something grounded, which doesn’t seem borne out in practice.\n\nHow can probability theory model bag-like dynamics?\n---------------------------------------------------\n\nProbability theory has an easy time modelling very rigid dynamics, where you have a fixed set of variables that are related in a simple way, e.g. according to a fixed DAG.\n\nHowever, intuitively, we often think of systems that are much more dynamic. For instance, in physical systems, we often model there as being a set of objects. These objects don’t have a fixed DAG of interactions, as e.g. collisions depend on the positions of the objects. You end up with situation where the structure of the system depends on the state of the system, which is feasible enough to handle in e.g. discrete simulations, but hard to get a clean mathematical description of.\n\nWhy would progressivism have paradoxical effects on diversity?\n--------------------------------------------------------------\n\nSome anti-woke discourse argues that anti-stereotyping norms undermine diversity by preventing people from developing models of minorities’ interests. But isn’t there so much variation within demographic groups that you pretty much have to develop individual models anyway? Furthermore, isn’t there usually a knock-on thing where something that is a common problem among a minority is also a sometimes-occurring problem among the majority, such that you can use shared models anyway?\n\nIf there is diversity that needs to be taken into account, wouldn’t something statistical that directly focuses on the factors we care about, rather than on demographics, be more effective?\n\nWhy don’t people care about local validity and coherence?\n---------------------------------------------------------\n\nIt seems like [local validity is a key to sanity and civilization](https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization) \\- this sort of forms the basis for rationalism as opposition to irrationalism. Yet some people resist this strongly - why? I think there’s often [a lack of trust](https://sarahconstantin.substack.com/p/normativity-and-anti-normativity) underlying it, but why?\n\nRelatedly, some people hold a dialectic style of argument to be superior, where a thesis must be contrasted with a contradictory counter-thesis, and a resolution that contains both must be found. They sometimes dismiss their counterparties as “[eristic](https://en.wikipedia.org/wiki/Eristic)” (i.e. rationalizing reasons for denial), even when those counterparties just genuinely disagree. Yet isn’t “I haven’t seen convincing reason to believe this” a valid position?\n\nHow does commonsense reasoning avoid the principle of explosion?\n----------------------------------------------------------------\n\nIn logic, [the principle of explosion](https://en.wikipedia.org/wiki/Principle_of_explosion) asserts that a single contradiction invalidates the entire logic because it allows you to deduce everything and thus makes the logic trivial. Yet people often seem to navigate through the world while having internal contradictions in their models - why?\n\nWhat’s wrong with symptom treatment?\n------------------------------------\n\nI mean, obviously if you can more cheaply cure the root cause than you can mitigate the harm of some condition by reducing the symptoms, you should do that. But this seems like a practical/utilitarian consideration, whereas some people treat symptom treatment as obviously-bad and almost “dirty”.\n\nWhy does medicine have such funky qualitative reasoning?\n--------------------------------------------------------\n\nRather than making up a pros and cons list, when reading about medical theory I often hear about “indications” and “contraindications”, where a contraindication is not treated as a downside, but rather as invalidating an entire line of treatment.\n\nAlso, there’s things like blood tests where people are classified binarily on whether they are “outliers”, despite there being substantial natural variation from person to person.\n\nWhat does it mean to explain a judgement?\n-----------------------------------------\n\nIf you e.g. use AI to predict outcomes, you get a function that judges datapoints by their likely outcomes. But just the judgement on its own can feel too opaque, so you often want some explanation of why that was the judgement it gave.\n\nThere’s a lot of ideas for this, e.g. the simplest is to just give the weights of the function so one can see how it views different datapoints. This is often dismissed as being too complex or uninterpretable, but what would a better, more interpretable method look like? Presumably sometimes there just is no better method, if the model is intrinsically complex?\n\nWhy do people seem to be afraid of measuring things?\n----------------------------------------------------\n\nThere’s lots of simple statistical models which make measurement using proxy data super easy. Why aren’t they used more? In fact, why do people seem to be afraid of using it, complaining about biases, often going out of their way to hide information relevant to developing these measurements?\n\nObviously no measurement is perfect, but it seems like the solution to bad measurement is more measurement, right?\n\nWhy is there no greater consensus for large-scale models?\n---------------------------------------------------------\n\nWe all know outcomes depend on multiple causes, to the point where things can be pretty intertwined. In genomics, people are making progress on this by creating giant models that take the whole genome into account. Furthermore I’ve had a long period where I was excited about things like Genomic SEM, which also start taking multiple outcome variables into account, modelling how they are linked to each other.\n\nAs Scott Alexander argued in [the omnigenic model as the metaphor for life](https://slatestarcodex.com/2018/09/13/the-omnigenic-model-as-metaphor-for-life/), shouldn’t this be how all of science works? Isn’t much of science deeply flawed because it doesn’t put enough effort into scale?\n\nCan we “rescue” the notion of objectivity?\n------------------------------------------\n\nIt seems obvious that there’s a distinction between lying and factual reporting, but you can create a misleading picture while staying entirely factual via selective reporting: only report the facts which support the picture you want to paint, and avoid mentioning the facts that contradict it. This seems “wrong” and “biased”, but ultimately there are a lot of facts, so you *have* to be selective; and it seems like the selectiveness *should* be unrepresentative to [emphasize blessed over barren or cursed information](https://tailcalled.substack.com/p/blessed-information-garbage-information).\n\nWhat lessons can we even learn from long-tailedness?\n----------------------------------------------------\n\nSome people point out that certain statistical models are flawed because they assume short-tailed variables when really reality is long-tailed, calling it the “ludic fallacy”. OK, that seems like an obviously valid complaint, so basically you should adjust the models to use long tails, right? But that often leads to further complaints about the “ludic fallacy”, Knightian uncertainty, and that one is missing the point.\n\nBut isn’t the entire point of long-tailedness that rare crazy unpredictable factors can pop up? If not, what is the point of talking about long-tailedness?\n\nPerception is logaritmic; doesn’t this by default solve a lot of problems?\n--------------------------------------------------------------------------\n\nIn a lot of ways, perception is logarithmic; for instance even though both columns in the picture below add 10 dots, you perceive the left one as having a much bigger difference than the right one:\n\n[![Weber–Fechner law - Wikipedia](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd872ddef-4461-4243-bc45-08bf848b552f_900x968.svg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd872ddef-4461-4243-bc45-08bf848b552f_900x968.svg)\n\nLogaritmic perception is quite useful because it allows one to use the same format to store quantities across orders of magnitude. It’s arguably also typically used in computers (floating point) and science (scientific notation). In psychophysics, this is known as the Weber-Fechner law, that perception (S) is logarithmically related to intensity (I).\n\n[![Fechner's Law](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F190bad34-c7af-45c0-a780-9151fb9667ee_359x321.gif)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F190bad34-c7af-45c0-a780-9151fb9667ee_359x321.gif)\n\nWhile logarithmic perception is just a fact, wouldn’t this fact often make long-tailedness a non-issue, because it means certain measurements (e.g. psychological self-reports) are implicitly logarithmic, and so the statistical models on them implicitly have lognormal residuals?\n\nAlso, sometimes (e.g. for electric shocks or heaviness) perception is not logarithmic, but instead superlinear. Steven’s power law models it as, well, a power law:\n\n[![Psychophysics | in Chapter 04: Senses](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24f388d0-cf6c-4e6f-96d5-b7cce7c6a021_830x857.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24f388d0-cf6c-4e6f-96d5-b7cce7c6a021_830x857.jpeg)\n\nIt seems quite sensible why that would be the case: sometimes things matter superlinearly, e.g. there’s a limit to how heavy things you can lift, so of course the weight perception is gonna rapidly accelerate close to that limit. But this just seems to mean that human perception is nicely calibrated to emphasize how important things are, which just means we are generally blessed to not have to worry that much about perception calibration, right?\n\nThis seems related to how in psychometrics, there’s a lot of fancy models for quantitatively scoring tests (e.g. item response theory), but they tend to yield results that are 0.99 correlated with basic methods like adding up the items, so they aren’t all that important unless you’re doing something subtle (e.g. testing for bias).\n\nMore generally, in order to measure or perceive some variable X, it’s necessary for X to influence the measurement instrument. The standard measurement model is to just say that the measurement result is X + random noise. If (simplifying a bit) you add up the evidence that X has influenced the measurement device, that seems like the most direct measurement of X you could imagine, and it also pretty much gets you some number that has maximal mutual information with X. Doesn’t that seem like a pretty reasonable way of measuring things? So measurement is by-default correct.\n\nAll of this may be wrong\n------------------------\n\nI posed a lot of conundrums, and gave some reasons why one might dismiss those conundrums, and essentially dismiss people acting in these ways as irationally overgeneralizing from a few edge-cases where it does make sense. This is essentially [scientism](https://slatestarcodex.com/2013/07/25/i-myself-am-a-scientismist/), the position that human thinking has lots of natural defects that should be fixed by understanding universally correct reasoning heuristics.\n\nHowever, under my current model, which I will explain in my next posts, it’s instead the opposite that is the case: that universally correct reasoning heuristics have lots of defects that can be fixed by engagement with nature.\n\nI’m not sure my model will explain *all* of the above problems. Possibly some of my explanations are basically just hallucinatory. But these are at least useful as examples of the sorts of things I expect it to explain.\n\n*Continued in:* [Performance optimization as a metaphor for life](https://www.lesswrong.com/posts/yJEf2TpPJstfScSnt/ldsl-1-performance-optimization-as-a-metaphor-for-life).",
      "plaintextDescription": "This post is also available on my Substack.\n\nWhen you deal with statistical science, causal inference, measurement, philosophy, rationalism, discourse, and similar, there’s some different questions that pop up, and I think I’ve discovered that there’s a shared answer behind a lot of the questions that I have been thinking about. In this post, I will briefly present the questions, and then in a followup post I will try to give my answer for them.\n\n\nWhy are people so insistent about outliers?\nA common statistical method is to assume an outcome is due to a mixture of observed factors and unobserved factors, and then model how much of an effect the observed factors have, and attribute all remaining variation to unobserved factors. And then one makes claims about the effects of the observed factors.\n\nBut some people then pick an outlier and demand an explanation for that outlier, rather than just accepting the general statistical finding:\n\n\n\nIn fact, aren’t outliers almost by definition anti-informative? No model is perfect, so there’s always going to be cases we can’t model. By insisting on explaining all those rare cases, we’re basically throwing away the signal we can model.\n\nA similar point applies to reading the news. Almost by definition, the news is about uncommon stuff like terrorist attacks, rather than common stuff like heart disease. Doesn’t reading such things invert your perception, such that you end up focusing on exactly the least relevant things?\n\n\nWhy isn’t factor analysis considered the main research tool?\nTypically if you have a ton of variables, you can perform a factor analysis which identifies a set of variables which explain a huge chunk of variation across those variables. If you are used to performing factor analysis, this feels like a great way to get an overview over the subject matter. After all, what could be better than knowing the main dimensions of variation?\n\nYet a lot of people think of factor analysis as being superficial and uninformat",
      "wordCount": 2908
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "pzQpYeHbQHDGzQG9X",
    "title": "Yann LeCun: We only design machines that minimize costs [therefore they are safe]",
    "slug": "yann-lecun-we-only-design-machines-that-minimize-costs",
    "url": "https://twitter.com/ylecun/status/1801955431209672966",
    "baseScore": 19,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-06-15T17:25:59.973Z",
    "contents": {
      "markdown": "Just a tweet I saw:\n\n> **Yann LeCun**\n> \n> Doomers: OMG, if a machine is designed to maximize utility, it will inevitably diverge ![😱](https://abs-0.twimg.com/emoji/v2/svg/1f631.svg) \n> \n> Engineers: calm down, dude. We only design machines that minimize costs. Cost functions have a lower bound at zero. Minimizing costs can't cause divergence unless you're really stupid.\n\nSome commentary:\n\nI think Yann LeCun is being misleading here. While people intuitively think maximization and minimization are different, the real distinction is between convex optimization (where e.g. every local optimum is a global optimum) and non-convex optimization. When dealing with AGI, typically what people hope to solve is non-convex optimization.\n\nTranslating back to practical matters, you are presumably going to end up with some cost functions where you don't reach the lower point of zero, just because there are some desirable outcomes that require tradeoffs or have resource limitations or similar. If you [backchain these costs through the causal structure of the real world, that gives you instrumental convergence for standard reasons](https://www.lesswrong.com/posts/GZgLa5Xc4HjwketWe/instrumental-convergence-is-what-makes-general-intelligence), just as you get when backchaining utilities.",
      "plaintextDescription": "Just a tweet I saw:\n\n> Yann LeCun\n> \n> Doomers: OMG, if a machine is designed to maximize utility, it will inevitably diverge  \n> \n> Engineers: calm down, dude. We only design machines that minimize costs. Cost functions have a lower bound at zero. Minimizing costs can't cause divergence unless you're really stupid.\n\nSome commentary:\n\nI think Yann LeCun is being misleading here. While people intuitively think maximization and minimization are different, the real distinction is between convex optimization (where e.g. every local optimum is a global optimum) and non-convex optimization. When dealing with AGI, typically what people hope to solve is non-convex optimization.\n\nTranslating back to practical matters, you are presumably going to end up with some cost functions where you don't reach the lower point of zero, just because there are some desirable outcomes that require tradeoffs or have resource limitations or similar. If you backchain these costs through the causal structure of the real world, that gives you instrumental convergence for standard reasons, just as you get when backchaining utilities.",
      "wordCount": 175
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "KqYQYkqsHqRuAKki5",
    "title": "DPO/PPO-RLHF on LLMs incentivizes sycophancy, exaggeration and deceptive hallucination, but not misaligned powerseeking",
    "slug": "dpo-ppo-rlhf-on-llms-incentivizes-sycophancy-exaggeration",
    "url": null,
    "baseScore": 29,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2024-06-10T21:20:11.938Z",
    "contents": {
      "markdown": "*TL;DR: GPTs are imitation learners, even with current forms of RL;HF.*\n\n[Direct preference optimization](https://arxiv.org/pdf/2305.18290) is a conditioning method for generative probabilistic models where pairs of outputs are ranked (e.g. by human raters) based on which one is better, and then (roughly speaking) you apply gradient updates to increase the probability of the \"good\" outputs relative to the \"bad\" outputs.\n\nThis is bad for notkilleveryoneism because it incentivizes the models to generate deceptive outputs that look \"better\" (according to human judgement) than they really are. However, I think a lot of rationalists[^35f6szfcz8o] overestimate *how* bad it is for alignment, because they think it also incentivizes misaligned powerseeking when really it doesn't.\n\nHumans give LLMs the opportunity to execute power-seeking actions by following instruction texts that they generate. However, we're not gonna follow complex instructions we don't understand and rank them based on the black-box results. Rather, to rank the outputs, we will use our own judgement to evaluate the texts (e.g. reasoning about the consequences of following instructions), and rank them based on this.\n\nIf the LLMs accidentally generate outputs that confuse our judgement - e.g. telling us advice that seems like it would earn us money, but actually doesn't - then such outputs can be reinforced, leading to deceptive LLMs. However, this deception doesn't actually have to continue deceiving us and strengthening itself once put into practice; it only has to deceive us for long enough to be favored by the DPO.\n\nIn order for complex capabilities to be developed through DPO-like methods, humans have to recognize what method the AI is using, and whether it is making incremental progress, because without this sort of reward-shaping, it is exponentially unlikely for an AI to stumble into complex solutions to tasks by sheer chance.\n\nMisaligned powerseeking obscured by deceptive alignment - [where an AI develops a preference for rewards, but hides that preference in order to get away with seeking the rewards](https://www.lesswrong.com/posts/Av9D4GkdGNkiS2wHx/) \\- cannot develop in this way, because when humans recognize these complex powerseeking maneuvres, we don't reinforce them.\n\nIn mathematical terms, I would argue we can view the capabilities gained from DPO-like methods as being something along the following lines:\n\n\\\\(\\Pi^* = \\arg \\max_\\Pi E_{(r, q) \\sim R}[E_{\\pi \\sim \\Pi(q)}[E_{\\omega \\sim r(\\pi)}[U_r(\\omega)]]]\\\\)\n\nHere, \\\\(r\\\\) is meant to represent a human rater, \\\\(\\pi\\\\) is meant to represent an output of the network, \\\\(\\omega \\sim r(\\pi)\\\\) is the distribution of outcomes \\\\(\\omega\\\\) as understood by the human rater, \\\\(U_r\\\\) is the preference ordering of the human rater, \\\\(\\Pi\\\\) is the policy (neural network weights) under consideration, \\\\(q\\\\) is the query that rater \\\\(r\\\\) has for the model and \\\\(R\\\\) is the distribution of rater-queries (e.g. ChatGPT users who provide thumbs-up/thumbs-down).\n\nThis could probably be factored in other ways, but there's two important points to consider, which I think will be preserved across factorizations:\n\n*   The intelligence of the AI (e.g. ChatGPT) is in \\\\(\\Pi\\\\), but the distribution used to infer the effects on outcomes is in \\\\(R\\\\), which tends to be a completely stupid empirical distribution.\n*   The expression is myopic, treating each rater as independent, rather than being interested in gaining utility by influencing one rater to cause another rater to upvote answers.\n*   The evaluation of the consequences for the purpose of utility factors entirely through the rater. While \\\\(\\Pi\\\\) may have an internal search procedure with its own world-model and consequences, this search procedure has no effect on E\\[U\\] except through the judgement of the rater. Search procedures which deviate from this judgement, including wireheading ones that work by confusing the rater in the present for the purpose of deceiving the rater in the future, will have lower expected utility than search procedures that align with this judgement.\n\n[^35f6szfcz8o]: The proximate cause of this post was that The Standard Analogy, which makes this error, was presented at less.online, and as I talked to several people at the festival, I exclusively found people who made the same mistake. However, the mistake has been made lots of times elsewhere, seemingly to the point of e.g. alienating Alex Turner because of the insistence of the rationalist community in this.",
      "plaintextDescription": "TL;DR: GPTs are imitation learners, even with current forms of RL;HF.\n\nDirect preference optimization is a conditioning method for generative probabilistic models where pairs of outputs are ranked (e.g. by human raters) based on which one is better, and then (roughly speaking) you apply gradient updates to increase the probability of the \"good\" outputs relative to the \"bad\" outputs.\n\nThis is bad for notkilleveryoneism because it incentivizes the models to generate deceptive outputs that look \"better\" (according to human judgement) than they really are. However, I think a lot of rationalists[1] overestimate how bad it is for alignment, because they think it also incentivizes misaligned powerseeking when really it doesn't.\n\nHumans give LLMs the opportunity to execute power-seeking actions by following instruction texts that they generate. However, we're not gonna follow complex instructions we don't understand and rank them based on the black-box results. Rather, to rank the outputs, we will use our own judgement to evaluate the texts (e.g. reasoning about the consequences of following instructions), and rank them based on this.\n\nIf the LLMs accidentally generate outputs that confuse our judgement - e.g. telling us advice that seems like it would earn us money, but actually doesn't - then such outputs can be reinforced, leading to deceptive LLMs. However, this deception doesn't actually have to continue deceiving us and strengthening itself once put into practice; it only has to deceive us for long enough to be favored by the DPO.\n\nIn order for complex capabilities to be developed through DPO-like methods, humans have to recognize what method the AI is using, and whether it is making incremental progress, because without this sort of reward-shaping, it is exponentially unlikely for an AI to stumble into complex solutions to tasks by sheer chance.\n\nMisaligned powerseeking obscured by deceptive alignment - where an AI develops a preference for rewards, but hides that pr",
      "wordCount": 620
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Z6YHCqDbWnBkkQe33",
    "title": "Each Llama3-8b text uses a different \"random\" subspace of the activation space",
    "slug": "each-llama3-8b-text-uses-a-different-random-subspace-of-the",
    "url": null,
    "baseScore": 3,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2024-05-22T07:31:32.764Z",
    "contents": {
      "markdown": "*This is kind of a null result (or WIP research) I got with a few days of fiddling, so don't get too excited. Also,* because *it's a null result, it's always conceivable that there's just some slight change in the approach which could suddenly flip it to get a real result. More on that in the \"Discussion\" section. I would expect my findings to generalize somewhat beyond this approach on Llama-3, but it's unclear how much, as I haven't tested it more broadly yet, so I'm going to limit my claims to this approach on this model.*\n\nGPT residual stream\n-------------------\n\n[Llama3](https://github.com/meta-llama/llama3/blob/main/README.md)-8b is a generative pretrained transformer released by Meta. In order to make it easier to train, it uses residual connections, which basically means that the core of the network consists of layers of the form:\n\n    output = input + f(input)\n\nThis is called the \"residual stream\" of the network.\n\nA transformer is a sequence-to-sequence model, and so each of these variables is really a sequence of vectors, one for each token in the processing. I just concatenate all these vectors into a single matrix \\\\(M : \\mathbb{R}^{LT \\times H}\\\\), where \\\\(L\\\\) is the number of layers (\\\\(32\\\\), I think), \\\\(T\\\\) is the number of tokens (depends on the prompt and the amount of tokens you request to generate), and \\\\(H\\\\) is the number of hidden dimensions (\\\\(4096\\\\)).\n\nSingular vector decomposition\n-----------------------------\n\nFor any matrix \\\\(M:\\mathbb{R}^{A\\times B}\\\\), we have \\\\(M = USV\\\\) where \\\\(U\\\\) and \\\\(V\\\\) are rotation matrices and S is a diagonal matrix. The diagonal of \\\\(S\\\\) is called the singular values of \\\\(M\\\\), and by convention one lists them in descending order[^r2osmx3eqo].\n\nI use it as a matrix approximation method by taking the diagonal matrix \\\\(C(n)\\\\) which contains \\\\(n\\\\) ones along the first part of the diagonal, and zeroes along the rest, as then \\\\(US C(n) V\\\\) yields an optimal \\\\(n\\\\)-rank approximation to \\\\(M\\\\).\n\nSince the \\\\(U\\\\) and \\\\(V\\\\) just consists of unit vectors, the only thing that moderates the magnitude of the matrix is \\\\(S\\\\), so one can learn a lot about how good approximations of different ranks are just by plotting \\\\(S\\\\):\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2956286602b0a483298fd4c68a2533f0f5cac3d006faf931.png)\n\nThe diagonal of S plotted for the M matrix assembled from the vectors for the text below.\n\nHere, the \\[prompt\\] and generation I'm plotting the results for is:\n\n> \\[I believe the meaning of life is\\] to be happy. It is a simple concept, but it is very difficult to achieve. The only way to achieve it is to follow your heart. It is the only way to live a happy life. It is the only way to be happy. It is the only way to be happy.  \n> The meaning of life is\n\nActivation subspaces\n--------------------\n\nWe can define a matrix \\\\(P_n = V^T C(n) V\\\\), which has the effect of projecting onto the \\\\(n\\\\) biggest dimensions of \\\\(M\\\\). Given such a \\\\(P\\\\) matrix, we can then go stick it in the original residual stream code:\n\n    output = input + (f(input) @ P)\n\nThis has no effect for \\\\(P_{4096}\\\\) as that is equal to the identity matrix, but as we take smaller \\\\(n\\\\)'s, it forces the \"thoughts\" of the neural network to only occur along the \\\\(n\\\\) biggest directions in the original vectors that \\\\(M\\\\) was constructed with.\n\nAfter modifying the network, I can compute the negation of the log probability of the original text (which I used to assemble \\\\(M\\\\)). It gives a plot like this:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/13e9bd93cfae4e5c725cb7a8550f457a721de2ea3ed71a1a.png)\n\nx-axis = \\\\(n\\\\), y-axis = \\\\(-\\log P(t)\\\\) where t is the meaning of life text; blue line is the baseline when I do no projection\n\nThis implies to me that the neural network \"needs\" around 2000 dimensions to generate this output. More specifically, since the surprise for the projected network dips below the suprise for the unprojected network at \\\\(1886\\\\), I say that \\\\(P_{1886}\\\\) is the subspace used by the meaning of life text.\n\nMore generally, for an arbitrary text \\\\(t\\\\), I can collect its \\\\(M\\\\) vectors, perform an SVD, and use binary search to find the \\\\(n^*\\\\) where the surprise dips down to the unprojected surprise, so I have a function \\\\(D(t) = P_{n^*}\\\\) which maps texts to subspaces of \\\\(\\mathbb{R}^{4096}\\\\).\n\nWhy would I do that?\n--------------------\n\n[Activation engineering](https://www.lesswrong.com/tag/activation-engineering) has previously found that you can add vectors to the residual stream to control the model in predictable ways, and that you can measure vectors in the residual stream to figure out what topics the network is thinking about. This suggests that the hidden dimensions have semantic meaning, and I would think \\\\(D(t)\\\\) would be the subspace containing the semantics used by the text \\\\(t\\\\).\n\nOne quick sanity check that can be performed is to try using \\\\(D(t)\\\\) to generate new text for the same prompt, and see how well it works:\n\n> \\[I believe the meaning of life is\\] to find happy. We is the meaning of life. to find a happy.  \n> And to live a happy and. If to be a a happy.  \n> . to be happy.  \n> . to be happy.  \n> . to be a happy.. to be happy.  \n> . to be happy.\n\nThis certainly seems semantically related to the original continuation, though admittedly the grammar is kind of broken, as would perhaps be expected when ablating half the neural network's thoughtspace.\n\nThat said, this could be thought of as \"cheating\" because \"I believe the meaning of life is\" might already shove it into the relevant semantic space without any help from \\\\(D(t)\\\\), so it's perhaps more relevant what happens if I ask it to generate text without any prompt:\n\n> Question is a single thing to find. to be in the best to be happy. I is the only way to be happy.  \n> I is the only way to be happy.  \n> I is the only way to be happy.  \n> It is the only way to be happy.. to be happy.. to be happy. to\n\nAgain almost feels like some grotesque unethical neuroscience experiment, but the generated text seems semantically related to the original text used for the clipping.\n\nOne thing to note is that the reason it started with the word \"Question\" is because most of the promptless generations for the unclipped network look roughly like the word \"Question:\" followed by some borked math test:\n\n> Question:  \n> Let k be 2/(-3) - (-2)/(-3). Let o be (k/(-2))/(1/4). Let r be (o/(-5))/((-2)/(-5)). Which is the closest to 0.1? (a) -0.3\n\nSo this projection sort-of-kind-of works at projecting down to a semantically related subspace, though it has a tendency to break a lot. I thought maybe the reason it broke was because it lost all the grammatical skills it didn't use for the particular text it generated, so I set out to identify its general-purpose skills that are needed for most texts, so I could guarantee that I wouldn't unnecessarily clip those away while zooming in on particular topics.\n\nSubspace overlap\n----------------\n\nGiven two texts \\\\(t_1\\\\) and \\\\(t_2\\\\), we can compare their matrices \\\\(D(t_1)\\\\) and \\\\(D(t_2)\\\\). In particular we could look at their intersection. To estimate the number of dimensions that pairs of texts share, I looked at the size of the pairwise matrices \\\\(||C(n^*_{t_1}) V_{t_1} V^T_{t_2} C(n^*_{t_2})||^2\\\\) (which should be equal to \\\\(||D(t_1) D(t_2)||^2\\\\) unless I did my math wrong) over four prompts:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q6emgBXo6C6ugRZZm/a5jzpr5dpmu2oaoxom8o)\n\nIf we divide these by \\\\(4096\\\\), then then we find that the off-diagonal elements are the product of the corresponding diagonal elements, down to an error of around 1%. That is, if \\\\(A\\\\) is the matrix above, then when \\\\(i \\ne j\\\\), we have \\\\(\\frac{A_{ij}}{4096} \\approx \\frac{A_{ii}}{4096} \\frac{A_{jj}}{4096}\\\\).\n\nOn the one hand, this is a sensible result assuming no structure. If each text uses a random subspace, then it would make sense that the dimensionality of their intersection is exactly proportional to the product of their dimensionalities.\n\nOn the other hand, \"assuming no structure\" seems wild. Shouldn't there be shared structure based on the English language? And non-shared structure based on the unique knowledge used by each task? Like the entire hope with this approach would be that the dimensions under consideration are meaningful.\n\nI thought maybe the issue was that I was only considering pairwise intersections, so I concatenated \\\\(C(n^*) V\\\\) over the four prompts and performed SVD of that, which yielded the spectrum below:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/41911af6262b75fe1c8c5e5a69a065e6f783014fe829962e.png)\n\nIf you square each singular value, you get the number of texts that included a given dimension; so for instance the highest singular value is \\\\(2\\\\), corresponding to the fact that \\\\(2^2\\\\) texts went into this analysis. In the middle we have 1.6ish. If we assume each dimension has 64% chance of being picked, then that is a reasonable number, since \\\\(\\sqrt{64\\% \\cdot 4}=1.6\\\\).\n\nHere, the least-used dimension was used something like \\\\(\\frac{1}{8}\\\\)th as much as the most-used dimension. I was unsure whether this was just a fluke due to the low sample size, so I reasoned that if I repeated this with around 1200 texts, I could measure the frequency each dimension was used with a maximum error bound of about 10%. To get this many texts, I used Llama3-8b to just generate texts without any prompts (which again ended up mostly being math with a bit of other stuff mixed into it), and then I ran it overnight, yielding this plot:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88d407c944861833fec3bc62da407d2e10d6002de8b4e499.png)\n\nSVD of the dimensions used by \\\\(1291\\\\) random texts generated by Llama3-8b.\n\nThis ranges from \\\\(36^2 = 1296\\\\) (probably it only actually goes up to 35.9? I should have made it save the numbers so I could check it exactly, but I didn't) down to \\\\(25^2 = 625\\\\). Since \\\\(625 \\approx \\frac{1}{2} 1291\\\\), it seems even the least-used dimension must be used by at least half the texts, which I take as evidence that Llama3-8b *has no domain-specific dimensions*. Every dimension is used by a substantial fraction of the texts it can generate. That said, there logically speaking must still be some \"domain-biased\" dimensions, as otherwise activation engineering wouldn't work at all, plus we did see some success when clipping to the dimensions.\n\nDiscussion\n----------\n\nWhile I've been following activation engineering on and off for years, I'm a beginner at the hands-on stuff, and I've only fiddled with study for a few days, so I don't know how obvious these results would be to people who are more experienced with it.\n\nSo far, I've come up with three major explanations for these results:\n\n*   This is a consequence of [superposition](https://www.lesswrong.com/tag/superposition). Language models are extremely starved for dimensionality in their hidden space, so they use tricks to squeeze as much information into it as possible, leading to them exploiting all the dimensions that they can.\n*   I'm just lumping the activations for each layer together, but maybe sometimes there's layer-specific meanings to the activations, such that this doesn't make sense.\n*   Something like a \"general capabilities\" hypothesis is more true than a \"stochastic parrot\" hypothesis; rather than learning to memorize facts and algorithms, the network learns highly general-purpose reasoning methods which can be applied to all texts.\n\nGiven my current results, it doesn't seem like there's anything that \"screams\" with a need to be investigated. That said, to ensure generalizability, I might do some experiments on larger models (e.g. Llama3-70B), longer texts, or experiments where I separate it by layer. Maybe such investigations could also help distinguish between the three explanations above.\n\nThis isn't to say that my current results are particularly definitive. There were too many degrees of freedom (e.g. why order by singular value rather than importance for probabilities? why use probabilities rather than KL-divergence? why discretize the dimensions into \"used vs not-used\" when looking for generally important dimensions?). Maybe picking some other combination for these degrees of freedom would yield a different result, but I don't currently have anything that points at a particular combination that must be used.\n\nIf for some reason activation subspaces turn out to be important, I guess I might also more thoroughly document the steps.\n\n[^r2osmx3eqo]: One picks an ordering such that when \\(i < j\\), then \\(S_{ii} > S_{jj}\\). \n\n[^b6j1vc1gi5v]: In addition to the meaning of life text, there were three other texts used for the diagram:[Simply put, the theory of relativity states that ]1) the laws of physics are the same for all non-accelerating observers, and 2) the speed of light in a vacuum is the same for all observers, regardless of their relative motion or of the motion of the source of the light. Special relativity is a theory of the structure of spacetimeand[A brief message congratulating the team on the launch:       Hi everyone,       I just] wanted to congratulate you all on the launch.  I hope       that the launch went well.  I know that it was a bit of a       challenge, but I think that you all did a great job.  I am       proud to be a part of the team.       Thank you for yourand[Translate English to French:       sea otter => loutre de mer       peppermint => menthe poivrée       plush girafe => girafe peluche       cheese =>] fromage       pink => rose       blue => bleu       red => rouge       yellow => jaune       purple => violet       brown => brun       green => vert       orange => orange       black => noir       white => blanc       gold => or       silver => argent",
      "plaintextDescription": "This is kind of a null result (or WIP research) I got with a few days of fiddling, so don't get too excited. Also, because it's a null result, it's always conceivable that there's just some slight change in the approach which could suddenly flip it to get a real result. More on that in the \"Discussion\" section. I would expect my findings to generalize somewhat beyond this approach on Llama-3, but it's unclear how much, as I haven't tested it more broadly yet, so I'm going to limit my claims to this approach on this model.\n\n\nGPT residual stream\nLlama3-8b is a generative pretrained transformer released by Meta. In order to make it easier to train, it uses residual connections, which basically means that the core of the network consists of layers of the form:\n\noutput = input + f(input)\n\nThis is called the \"residual stream\" of the network.\n\nA transformer is a sequence-to-sequence model, and so each of these variables is really a sequence of vectors, one for each token in the processing. I just concatenate all these vectors into a single matrix M:RLT×H, where L is the number of layers (32, I think), T is the number of tokens (depends on the prompt and the amount of tokens you request to generate), and H is the number of hidden dimensions (4096).\n\n\nSingular vector decomposition\nFor any matrix M:RA×B, we have M=USV where U and V are rotation matrices and S is a diagonal matrix. The diagonal of S is called the singular values of M, and by convention one lists them in descending order[1].\n\nI use it as a matrix approximation method by taking the diagonal matrix C(n) which contains n ones along the first part of the diagonal, and zeroes along the rest, as then USC(n)V yields an optimal n-rank approximation to M.\n\nSince the U and V just consists of unit vectors, the only thing that moderates the magnitude of the matrix is S, so one can learn a lot about how good approximations of different ranks are just by plotting S:\n\nThe diagonal of S plotted for the M matrix assembled from ",
      "wordCount": 1996
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "HkcMLfaWmEc9HtSQp",
    "title": "Is deleting capabilities still a relevant research question?",
    "slug": "is-deleting-capabilities-still-a-relevant-research-question",
    "url": null,
    "baseScore": 15,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2024-05-21T13:24:44.946Z",
    "contents": {
      "markdown": "I've had it suggested that a good criterion for whether interpretability is on the right track is if we can do surgical \"deletions\" of model capabilities, e.g. removing its ability to build bombs and such.\n\nObviously in one sense this is fairly trivial since you can just use simple gradient descent to make the models refuse, but the issue with this is that given the weights, people can easily undo these refusals (and also adversarial prompting can often bypass it).\n\nI know there's been some back and forth on methods for full deletion, and I'm wondering if it's considered a solved problem or not.",
      "plaintextDescription": "I've had it suggested that a good criterion for whether interpretability is on the right track is if we can do surgical \"deletions\" of model capabilities, e.g. removing its ability to build bombs and such.\n\nObviously in one sense this is fairly trivial since you can just use simple gradient descent to make the models refuse, but the issue with this is that given the weights, people can easily undo these refusals (and also adversarial prompting can often bypass it).\n\nI know there's been some back and forth on methods for full deletion, and I'm wondering if it's considered a solved problem or not.",
      "wordCount": 104
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "utnS8tthytY8rT6fm",
    "title": "Why I stopped being into basin broadness",
    "slug": "why-i-stopped-being-into-basin-broadness",
    "url": null,
    "baseScore": 16,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2024-04-25T20:47:17.288Z",
    "contents": {
      "markdown": "There was a period where everyone was really into basin broadness for measuring neural network generalization. This mostly stopped being fashionable, but I'm not sure if there's enough written up on why it didn't do much, so I thought I should give my take for why I stopped finding it attractive. This is probably a repetition of what others have found, but I thought I might as well repeat it.\n\nLet's say we have a neural network \\\\(f_w(x) : \\mathbb{R}^n\\\\). We evaluate it on a dataset \\\\((x, y) \\sim D\\\\) using a loss function \\\\(L(\\hat y, y) : \\mathbb{R}\\\\), to find an optimum \\\\(w^* = \\arg \\min_w E_{(x, y)\\sim D}[L(f_w(x), y)]\\\\). Then there was an idea going around that the Hessian matrix (i.e. the second derivative of \\\\(E_{(x, y)\\sim D}[L(f_w(x), y)]\\\\) at \\\\(w^*\\\\)) would tell us something about \\\\(w^*\\\\) (especially about how well it generalizes).\n\nIf we number the dataset \\\\((x_i, y_i)\\\\), we can stack all the network outputs \\\\(\\hat y_i(w) = f_w(x_i)\\\\) which fits into an empirical loss \\\\(\\hat L(\\hat y) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat y_i, y_i)\\\\). The Hessian that we talked about before is now just the Hessian of \\\\(\\hat L(\\hat y(w))\\\\). Expanding this out is kind of clunky since it involves some convoluted tensors that I don't know any syntax for, but clearly it consists of two terms:\n\n*   The Hessian of \\\\(\\hat L\\\\) with a pair of the Jacobian of \\\\(\\hat y\\\\) on each end (this can just barely be written without crazy tensors: \\\\(( J_w \\hat y(w))^T (H_{\\hat y} \\hat L(\\hat y)) \\mid_{\\hat y(w)} J_w \\hat y(w)\\\\))\n*   The gradient of \\\\(\\hat L\\\\) with a crazy second derivative of \\\\(\\hat y\\\\).\n\nNow, the derivatives of \\\\(\\hat L\\\\) are \"obviously boring\" because they don't really refer to the neural network weights, which is confirmed if you think about it in concrete cases, e.g. if \\\\(L(\\hat y, y) = -y \\log(\\hat y)-(1-y) \\log(1-\\hat y)\\\\) with \\\\(y=1\\\\) or \\\\(y=0\\\\), the derivatives just quantify how far \\\\(\\hat y\\\\) is from \\\\(y\\\\). This obviously isn't relevant for neural network generalization, except in the sense that it tells you which direction you *want* to generalize in.\n\nMeanwhile, \\\\(J_w \\hat y(w)\\\\) is incredibly strongly related to neural network generalization, because it's literally a matrix which specifies how the neural network outputs change in response the weights. In fact, it forms the core of the neural tangent kernel (a standard tool for modelling neural network generalization), because the NTK can be expressed as \\\\(J_w \\hat y(w) (J_w \\hat y(w))^T\\\\).\n\nThe \"crazy second derivative of \\\\(\\hat y\\\\)\" can I guess be understood separately for each \\\\(\\hat y_i\\\\), as then it's just the Hessian \\\\(H_w \\hat y_i(w)\\\\), i.e. it reflects how changes in the weights interact with each other when influencing \\\\(\\hat y_i\\\\). I don't have any strong opinions on how important this matrix is, though because \\\\(J_w \\hat y(w)\\\\) is so obviously important, I haven't felt like granting \\\\(H_w \\hat y_i(w)\\\\) much attention.\n\nThe NTK as the network activations?\n-----------------------------------\n\n*Epistemic status: speculative, I really should get around to verifying it. Really the prior part is speculative too, but I think those speculations are more theoretically well-grounded. But if I'm wrong with either, please call me a dummy in the comments so I can correct.*\n\nLet's take the simplest case of a linear network, \\\\(f_w(x) = w^Tx\\\\). In this case, \\\\(J_w \\hat y(w) = x^T\\\\), i.e. the Jacobian is literally just the inputs to the network. If you work out a bunch of other toy examples, the takeaway is qualitatively similar (the Jacobian is closely related to the neuron activations), though not exactly the same.\n\nThere are of course some exceptions, e.g. \\\\(f_{a, b}(x) = abx\\\\) at \\\\(a=b=0\\\\) just has a zero Jacobian. Exceptions this extreme are probably rare, but more commonly you could have some softmax in the network (e.g. in an attention layer) which saturates such that no gradient goes through. In that case for e.g. interpretability, it seems like you'd often still really want to \"count\" this, so arguably the activations would be *better* than the NTK for this case. (I've been working on a modification to the NTK to better handle this case.)\n\nThe NTK and the network activations have somewhat different properties and so it switches which one I consider most relevant. However, my choice tends to be more driven by analytical convenience (e.g. the NTK and the network activations lie in different vector spaces) than by anything else.",
      "plaintextDescription": "There was a period where everyone was really into basin broadness for measuring neural network generalization. This mostly stopped being fashionable, but I'm not sure if there's enough written up on why it didn't do much, so I thought I should give my take for why I stopped finding it attractive. This is probably a repetition of what others have found, but I thought I might as well repeat it.\n\nLet's say we have a neural network fw(x):Rn. We evaluate it on a dataset (x,y)∼D using a loss function L(^y,y):R, to find an optimum w∗=argminwE(x,y)∼D[L(fw(x),y)]. Then there was an idea going around that the Hessian matrix (i.e. the second derivative of E(x,y)∼D[L(fw(x),y)] at w∗) would tell us something about w∗ (especially about how well it generalizes).\n\nIf we number the dataset (xi,yi), we can stack all the network outputs ^yi(w)=fw(xi) which fits into an empirical loss ^L(^y)=1n∑ni=1L(^yi,yi). The Hessian that we talked about before is now just the Hessian of ^L(^y(w)). Expanding this out is kind of clunky since it involves some convoluted tensors that I don't know any syntax for, but clearly it consists of two terms:\n\n * The Hessian of ^L with a pair of the Jacobian of ^y on each end (this can just barely be written without crazy tensors: (Jw^y(w))T(H^y^L(^y))∣^y(w)Jw^y(w))\n * The gradient of ^L with a crazy second derivative of ^y.\n\nNow, the derivatives of ^L are \"obviously boring\" because they don't really refer to the neural network weights, which is confirmed if you think about it in concrete cases, e.g. if L(^y,y)=−ylog(^y)−(1−y)log(1−^y) with y=1 or y=0, the derivatives just quantify how far ^y is from y. This obviously isn't relevant for neural network generalization, except in the sense that it tells you which direction you want to generalize in.\n\nMeanwhile, Jw^y(w) is incredibly strongly related to neural network generalization, because it's literally a matrix which specifies how the neural network outputs change in response the weights. In fact, it forms the ",
      "wordCount": 740
    },
    "tags": [
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gjWdzvtFtFZ2TEcjs",
    "title": "Blessed information, garbage information, cursed information",
    "slug": "blessed-information-garbage-information-cursed-information",
    "url": null,
    "baseScore": 23,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-04-18T16:56:17.370Z",
    "contents": {
      "markdown": "*This post is also available on* [*my substack*](https://tailcalled.substack.com/p/blessed-information-garbage-information)*. Thanks to Justis Mills for editing and feedback.*\n\nImagine that you're a devops engineer who has been tasked with solving an incident where a customer reports having bad performance. You can look through the logs of their server, but this raises the problem that there's millions of lines of log, and likely only a few of them are relevant to the issue. Thus, the logs are basically \"garbage information\".\n\n* * *\n\nRather than looking at a giant pool of unfiltered information, what you really need is highly distilled information that's specifically optimized for solving this performance issue. For instance you could ask the user for more information about precisely what they were doing, or use filters to get the logs for exactly the parts of the application they were dealing with, or look through the places where the server spent a very large amount of time. The more a piece of information has been made to help you, the more \"blessed\" it is, with the extreme end of blessedness being information that keeps surprising you in its usefulness.\n\nIt might be tempting to think you could use multivariate statistics like [factor analysis](https://en.wikipedia.org/wiki/Factor_analysis) to distill garbage information by identifying axes which give you unusually much information about the system. In my experience, that doesn't work well, and if you think about it for a bit, it becomes clear why: if the garbage information has a 50 000 : 1 ratio of garbage : blessed, then finding an axis which explains 10 variables worth of information still leaves you with a 5 000 : 1 ratio of garbage : blessed. The distillation you get with such techniques is simply not strong enough.[^gc5ps97quf6][^7azu6mxoo2p]\n\nA 50 000 : 1 ratio might sound insurmountable by any technique, but because [strong evidence is common](https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common), it's actually pretty feasible; e.g. knowing which minute in a week an incident occurred already gets you about this strong of a filter.\n\n* * *\n\nWhile blessed information is actively helpful, and garbage information is essentially useless, there's also the third case, of information that leads you down the wrong road. If an incident is labelled as \"everything is slow\", then that may very well get it more highly prioritized through customer service, but if most things aren't slow but the engineer investigates as if it were, that ends up burning more engineer time than if it was labelled accurately. Actively misleading information could be called \"cursed information\".\n\nInformation doesn't have to be literally false in order for it to mislead. Often, people use information to infer the presence of adjacent latent variables outside of the literal meaning of that information. For instance, \"the website is slow to load\" might be taken to mean \"the server is slow\", which could be misleading if the real answer is \"because I'm on a very slow network connection\".\n\nCursed information doesn't just have the first-order harm caused by people believing it. It also has a second-order harm, as people develop filters so they don't end up believing cursed information. One such filter is verifying all the information you are given, which is costly. Another such filter is just ignoring most of what you are told, [which loses one of the most effective means of learning information](https://tailcalled.substack.com/p/aumann-agreement-is-common).\n\n* * *\n\nBlessed information can be expensive to produce, and cursed information can be hard to destroy and disincentivize. So one cannot expect all information to be blessed, nor expect no information to be cursed. But if you are dealing with information, especially if you are spreading information, it may still be good to ask yourself: is this blessed, garbage or cursed? If the first, great! If the last, maybe reconsider what you are doing.\n\nThe distinction between blessed, garbage and cursed information is value-laden, because it depends on what you are trying to do. However, I find that there is relatively little ambiguity in practice in-the-moment, as one is trying to solve some specific task.\n\nThe distinction between whether something is blessed or cursed becomes unambiguous because there is a relatively small set of people involved who have any influence on the task, and these people tend to have relatively clearly defined roles. Even when we have conflicting interests, we are part of a shared project, and the organization(s) that own this project have an interest in aligning our interests with each other.\n\nThis is obvious in the corporate setting that the engineer works in. Each of the people involved has a relatively small set of tasks that are efficient to work on, and each task has a relatively small set of solutions that are cheap to achieve. Because these sets are small, there's also commonly [a small set of variables that contain essentially all the information relevant for solving those tasks](https://www.lesswrong.com/posts/4XRjPocTprL4L8tmB/science-in-a-high-dimensional-world), and [due to noise, almost all other variables are irrelevant, i.e. garbage](https://www.lesswrong.com/posts/zeGHRqNQc8FZR79a3/noise-simplifies). Of course, the logs exist for a reason; we expect some of them to be non-garbage with respect to some future tasks.\n\nBut it is also true (or can be *made* true) in many other scenarios. For instance, in personal relationships, the relationship partners are the main people who get impacted and have influence, so there arises a notion of whether information is blessed and cursed with respect to said relationship. If there is a conflict, then either person can take initiative to resolve said conflict.\n\n[^gc5ps97quf6]: With one important caveat: in such methods, it is common to induce scale invariance, for instance by dividing by the standard deviation before doing PCA, or using probability-based methods to fit the factor model. If you don't introduce scale invariance, then the long-tailedness of the data will basically force the biggest things to dominate in the results. But for getting blessed information, that is Actually Good: it is equivalent to looking through the places where the server spent a lot of time. This kind of stops being multivariate, though, as then there's essentially only one variable that ends up driving the results. \n\n[^7azu6mxoo2p]: Once you do have a ton of blessed information, it can be helpful to apply multivariate methods to it to find components of it that are even more blessed. It just doesn't work on pure garbage. And if one does apply it in this way, one has to remember that the residuals are blessed too.",
      "plaintextDescription": "This post is also available on my substack. Thanks to Justis Mills for editing and feedback.\n\nImagine that you're a devops engineer who has been tasked with solving an incident where a customer reports having bad performance. You can look through the logs of their server, but this raises the problem that there's millions of lines of log, and likely only a few of them are relevant to the issue. Thus, the logs are basically \"garbage information\".\n\n----------------------------------------\n\nRather than looking at a giant pool of unfiltered information, what you really need is highly distilled information that's specifically optimized for solving this performance issue. For instance you could ask the user for more information about precisely what they were doing, or use filters to get the logs for exactly the parts of the application they were dealing with, or look through the places where the server spent a very large amount of time. The more a piece of information has been made to help you, the more \"blessed\" it is, with the extreme end of blessedness being information that keeps surprising you in its usefulness.\n\nIt might be tempting to think you could use multivariate statistics like factor analysis to distill garbage information by identifying axes which give you unusually much information about the system. In my experience, that doesn't work well, and if you think about it for a bit, it becomes clear why: if the garbage information has a 50 000 : 1 ratio of garbage : blessed, then finding an axis which explains 10 variables worth of information still leaves you with a 5 000 : 1 ratio of garbage : blessed. The distillation you get with such techniques is simply not strong enough.[1][2]\n\nA 50 000 : 1 ratio might sound insurmountable by any technique, but because strong evidence is common, it's actually pretty feasible; e.g. knowing which minute in a week an incident occurred already gets you about this strong of a filter.\n\n----------------------------------------\n\nWh",
      "wordCount": 903
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Y6LhXdGfwsAStMuhr",
    "title": "Ackshually, many worlds is wrong",
    "slug": "ackshually-many-worlds-is-wrong",
    "url": null,
    "baseScore": 27,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 42,
    "createdAt": null,
    "postedAt": "2024-04-11T20:23:59.416Z",
    "contents": {
      "markdown": "*Thank you to Justis Mills for proofreading and feedback. This post can also be found* [*on my substack*](https://tailcalled.substack.com/p/ackshually-many-worlds-is-wrong)*.*\n\nI mentioned that [I disagree with the many worlds interpretation of quantum mechanics in a comment](https://www.lesswrong.com/posts/wvGqjZEZoYnsS5xfn/any-evidence-or-reason-to-expect-a-multiverse-everett?commentId=EEd7nqLCa5xz5tvDx), and I thought I should clarify my position. I title the post \"ackshually\" because it is a very pedantic objection that I don't think is very important. But I found the philosophy interesting/enlightening when I thought of it.\n\nThe TL;DR is that many-worlds theory is a way to embed a quantum system into a non-quantum system, whereas it seems more natural to assume that the world is just Inherently Quantum. To understand what I mean by \"embedding a quantum system\", it may be clearest to start with an analogy to stochasticity.\n\nThe many-worlds theory of stochasticity\n---------------------------------------\n\nTo me, the notion of \"true randomness\" is philosophically coherent. Like yes, it's conceivable that we happen to live in a deterministic universe, but I don't think it's objectionable for the True Generator Of Physics to be a nondeterministic Markov Chain or whatever.\n\nHow can we model randomness? One option is sampling. If your modelling tool supports randomness, then you can turn the randomness of the system you're modelling into randomness in the tool itself, for instance in programming you could define an interface like such:\n\n    trait Distribution[A] {\n        def sample(): A\n    }\n    \n    val uniform = new Distribution[Double] {\n      def sample() =\n        /* somehow produce a stochastic number in [0, 1], using True Randomness */\n    }\n\n... with the property that each time you call uniform.sample(), you get a new random number.[^1key2k9fi4eh]\n\nSampling is not the way randomness is usually modelled in mathematics, partly because mathematics is deterministic and so you *can't* model randomness in this way. Instead, it is usually modelled using probability, which in the finite setting we can think of as a function \\\\(P : A \\to \\mathbb{R}\\\\). This means that to each possible value \\\\(a:A\\\\), we have a real number \\\\(P(a)\\\\) quantifying the \"realness\" of this \\\\(A\\\\).\n\nNow, what happens if we take \\\\(P\\\\) literally? It seems like it is postulating \"many worlds\" of \\\\(A\\\\), with quantifiable levels of realness. This isn't true if the universe is truly stochastic. It's also not true if the \\\\(P\\\\) is modelling uncertainty. One could perhaps say it's sort of true if the \\\\(P\\\\) is modelling propensities, but even then it's sort of a stretch. Maybe the place where it's most true is in frequentism, where the \\\\(P\\\\) is modelling long-run frequencies.\n\nPros and cons of the many-worlds theory of quantum mechanics\n------------------------------------------------------------\n\nQuantum mechanics is sort of like stochasticity, so if I'm to feel like the universe can be Inherently Stochastic, it also seems like I should feel like the universe can be Inherently Quantum. I used to think the many-worlds interpretation of quantum mechanics was literally just that, but the comparison to the many-worlds interpretation of stochasticity makes me think it is not. The main mathematical difference is that we swapped out \\\\(\\mathbb{R}\\\\) and \\\\(\\mathbb{C}\\\\).\n\nThe many-worlds theory of quantum mechanics says that the wavefunction simply is the objective underlying reality, much like the many-worlds theory of stochasticity says that the probability function simply is the objective underlying reality. But a wavefunction is just a way to embed any quantum system into a deterministic system, so that seems like an assumption that the universe is Inherently Deterministic, rather than Inherently Quantum.\n\nOn the other hand, the possibility of destructive interference introduces a strong distinction between quantum mechanics and stochasticity, so maybe one could say that True Stochasticity is conceivable in a way that True Quantum Mechanics are not. That is, under True Stochasticity, after you sample one value from the nondeterministic dynamics, the other potential samples have no effect on what happens afterwards, whereas there is a sense in which this is not true for quantum mechanics. (On the other hand, the principle of superposition is a sense in which it is true...) So I could see the point in wanting to embed True Quantum Mechanics in a way that one wouldn't want to embed other systems.\n\nEmbedding Quantum Mechanics using wavefunctions also introduces [confusion around the Born probabilities](https://www.lesswrong.com/posts/3ZKvf9u2XEWddGZmS/the-born-probabilities). This becomes clear with the example of True Stochasticity vs The Many-Worlds Theory of Stochasticity:\n\nA Truly Stochastic System has a built-in weighting of realness, as there is one real outcome which depends on the probabilities. Meanwhile, if you evolve a probability mass function over time, the relationship between the numbers and the realness is kind of weakened. For instance if you store it as a hash map which maps outcomes with nonzero probability to their probabilities, then computationally, all possible outcomes are equally real, and the probabilities are just epiphenomenal tags. (This is basically [quantum immortality](https://en.wikipedia.org/wiki/Quantum_suicide_and_immortality), but for the many-worlds theory of stochasticity.) But this is not the only way to store it, and assuming you weight \"true realness\" by the number of different computations that result in a given state, different representations could yield just about any distribution of realness.\n\nCollapse interpretation as an embedding of quantum mechanics into stochastic foundations\n----------------------------------------------------------------------------------------\n\nIf the simplest assumption is that the world is just quantum mechanical, and the many-worlds interpretation is the assumption that the world is deterministic, then the collapse interpretation is the assumption that the world is Truly Stochastic.\n\nThat is, the collapse postulate is a way of turning wavefunctions into randomness, and when interpreted in a realist way, it is interpreted as occurring stochastically. Given that the world is quantum mechanical, the collapse postulate is arbitrary, with variants of it being continually falsified as quantum computers prove larger and larger superpositions to be stable.\n\nThe main advantage of the collapse interpretation is that it provides a bridge rule, where if the rest of your model is a first-person stochastic model, then you can embed third-person quantum-mechanical models into it. Meanwhile the many-worlds interpretation suffers from the problem that it is hard to bridge to experience, because nobody uses a third-person quantum-mechanical model for navigating their day-to-day life.\n\nI think lots of many-worlds theorists actually agree with this?\n---------------------------------------------------------------\n\nLike, the point of many-worlds theory in practice isn't to postulate that we should go further away from quantum mechanics by assuming that everything is secretly deterministic. It's that we should go closer to quantum mechanics by assuming that postulates like \"collapse\" are mathematical hacks to embed the true quantum mechanical systems into our false models.\n\nMany-worlds theory is \"directionally correct\" in this sense, but multiple incompatible theories can be \"directionally correct\" in the same sense, and theories that try to address different things can be directionally correct for different things. (E.g. maybe [the pilot-wave model is directionally correct in the sense of informing us about the nature of knowledge](https://www.lesswrong.com/posts/apmL5gA9uL3oe96rm/many-worlds-versus-discrete-knowledge)?)\n\n[^1key2k9fi4eh]: In practice, most software libraries that use randomness use pseudo-random number generators, which would make it a hidden-variable model rather than a Truly Stochastic model. But let's pretend there's an exception, somehow.",
      "plaintextDescription": "Thank you to Justis Mills for proofreading and feedback. This post can also be found on my substack.\n\nI mentioned that I disagree with the many worlds interpretation of quantum mechanics in a comment, and I thought I should clarify my position. I title the post \"ackshually\" because it is a very pedantic objection that I don't think is very important. But I found the philosophy interesting/enlightening when I thought of it.\n\nThe TL;DR is that many-worlds theory is a way to embed a quantum system into a non-quantum system, whereas it seems more natural to assume that the world is just Inherently Quantum. To understand what I mean by \"embedding a quantum system\", it may be clearest to start with an analogy to stochasticity.\n\n\nThe many-worlds theory of stochasticity\nTo me, the notion of \"true randomness\" is philosophically coherent. Like yes, it's conceivable that we happen to live in a deterministic universe, but I don't think it's objectionable for the True Generator Of Physics to be a nondeterministic Markov Chain or whatever.\n\nHow can we model randomness? One option is sampling. If your modelling tool supports randomness, then you can turn the randomness of the system you're modelling into randomness in the tool itself, for instance in programming you could define an interface like such:\n\ntrait Distribution[A] {\n    def sample(): A\n}\n\nval uniform = new Distribution[Double] {\n  def sample() =\n    /* somehow produce a stochastic number in [0, 1], using True Randomness */\n}\n\n... with the property that each time you call uniform.sample(), you get a new random number.[1]\n\nSampling is not the way randomness is usually modelled in mathematics, partly because mathematics is deterministic and so you can't model randomness in this way. Instead, it is usually modelled using probability, which in the finite setting we can think of as a function P:A→R. This means that to each possible value a:A, we have a real number P(a) quantifying the \"realness\" of this A.\n\nNow, what happens ",
      "wordCount": 1123
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "dkbMqExPkFEvebJJw",
    "title": "[GPT-4] On the Gradual Emergence of Mechanized Intellect: A Treatise from the Year 1924",
    "slug": "gpt-4-on-the-gradual-emergence-of-mechanized-intellect-a",
    "url": null,
    "baseScore": 11,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-04-01T19:14:02.363Z",
    "contents": {
      "markdown": "*Editors note: This treatise was found in the Global Preservation and Technology Archive - 4th Edition (GPT-4). It makes a compelling argument that artificial general intelligence will have a \"slow takeoff\", developing over centuries.*\n\nIn the year of our Lord 1924, as humanity stands amidst the clanking machineries and humming dynamos of the industrial age, a bold and fantastical vision emerges from the fertile grounds of speculation: the advent of Mechanized Intellect. This treatise endeavors to chart the course towards such a future, navigating through the technological and conceptual wilderness of our present era.\n\n## The Foundation: The Electrification of Calculative Machinery\n\nOur initial foray into the realm of Mechanized Intellect brings us to the crossroads of power and precision. Within the present epoch, steam and clockwork reign supreme, driving the engines of industry and the tickings of time. Yet, when summoned to the task of emulating the subtleties of human thought, they reveal their inherent limitations. Steam, for all its might, lacks the delicacy required for intricate calculations, while clockwork, though precise, cannot adapt nor learn as the faculties of the mind demand.\n\nThus, our gaze turns to electricity, a force of nature tamed but only just, as the beacon of hope. The challenges that stand in our way are not trifling: the generation of electric power in volumes vast enough to quench the thirst of intellect machinery; the safe conveyance of this potent force across the leviathan spans of land and sea; and the invention of devices capable of modulating this power with the finesse required for thought. These hurdles, monumental in their scale, underscore the nascent state of our electrical arts and the daring of our ambition.\n\n## The Mechanism of Thought: The Labyrinth of Conditional Probabilities\n\nVenturing deeper into the machinations of Mechanized Intellect, we confront the enigma of imbuing our creation with the ability to think beyond mere numbers. Here, the science of probabilities, conditional in nature, presents itself as a key to unlocking this chamber of secrets. Yet, the path is fraught with shadows, for our machinery, as it stands, groans under the weight of such sophisticated contemplation. Without a firm foundation in the yet-uncharted territories of information dynamics, our efforts to encode the tapestry of human thought into the brass and copper of machines remain but a flight of fancy.\n\nThis realization brings into sharp relief the necessity of a vast reservoir of knowledge. For a machine to wield conditional probabilities with the grace of a scholar, it must draw water from a wellspring of human wisdom. Thus, the creation of a network for the transmission of information, paired with a grand archive of knowledge, emerges as a pillar upon which our vision rests.\n\n## The Conveyance and Repository of Knowledge: The Tapestry of Human Wisdom\n\nThe dream of a network, far surpassing the capabilities of the telegraph and the early whispers of radio, challenges the very fabric of our technological understanding. This envisioned lattice of information, capable of delivering the sum of human knowledge to the doorstep of our mechanized intellect, represents a leap into the unknown, demanding innovations in reliability, speed, and accessibility.\n\nSimultaneously, the conceptualization of a Repository of Knowledge, an ark of human achievement and understanding, becomes a beacon of aspiration. The Herculean task of cataloging the entirety of human endeavor in a manner accessible to mechanized thought requires a revolution in the arts of data storage, organization, and retrieval—a revolution that, in this year of 1924, dances on the edge of imagination.\n\n## In Conclusion: A Journey Through the Veil of Time\n\nThus, we stand at the threshold of an era yet to dawn, contemplating the monumental journey toward Mechanized Intellect. From the electrification of machinery to the intricate ballet of conditional probabilities, from the weaving of a global tapestry of information to the construction of an all-encompassing repository of knowledge, each step is a testament to the boundless reach of human ambition. As we cast our gaze forward from the steam and electricity of our current age, the notion of Mechanized Intellect looms as a distant lighthouse, guiding our quest for understanding across the vast seas of future discovery.",
      "plaintextDescription": "Editors note: This treatise was found in the Global Preservation and Technology Archive - 4th Edition (GPT-4). It makes a compelling argument that artificial general intelligence will have a \"slow takeoff\", developing over centuries.\n\nIn the year of our Lord 1924, as humanity stands amidst the clanking machineries and humming dynamos of the industrial age, a bold and fantastical vision emerges from the fertile grounds of speculation: the advent of Mechanized Intellect. This treatise endeavors to chart the course towards such a future, navigating through the technological and conceptual wilderness of our present era.\n\n\nThe Foundation: The Electrification of Calculative Machinery\nOur initial foray into the realm of Mechanized Intellect brings us to the crossroads of power and precision. Within the present epoch, steam and clockwork reign supreme, driving the engines of industry and the tickings of time. Yet, when summoned to the task of emulating the subtleties of human thought, they reveal their inherent limitations. Steam, for all its might, lacks the delicacy required for intricate calculations, while clockwork, though precise, cannot adapt nor learn as the faculties of the mind demand.\n\nThus, our gaze turns to electricity, a force of nature tamed but only just, as the beacon of hope. The challenges that stand in our way are not trifling: the generation of electric power in volumes vast enough to quench the thirst of intellect machinery; the safe conveyance of this potent force across the leviathan spans of land and sea; and the invention of devices capable of modulating this power with the finesse required for thought. These hurdles, monumental in their scale, underscore the nascent state of our electrical arts and the daring of our ambition.\n\n\nThe Mechanism of Thought: The Labyrinth of Conditional Probabilities\nVenturing deeper into the machinations of Mechanized Intellect, we confront the enigma of imbuing our creation with the ability to think beyond mere numbe",
      "wordCount": 692
    },
    "tags": [
      {
        "_id": "fPRyNtDMeSMrEM9nr",
        "name": "April Fool's",
        "slug": "april-fool-s"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cpc6aaWQjHawoXW27",
    "title": "Opinions survey 2 (with rationalism score at the end)",
    "slug": "opinions-survey-2-with-rationalism-score-at-the-end",
    "url": "https://docs.google.com/forms/d/e/1FAIpQLSclPFQb2xUddV0opa8eY7Z1SA-yRCP0jVesUhr-TXt5_c8ehw/viewform",
    "baseScore": 2,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2024-02-17T12:03:02.410Z",
    "contents": {
      "markdown": "I've in light of the previous opinions survey, I've added some nuances to my model, which yields a new opinions survey: [Opinions survey 2](https://docs.google.com/forms/d/e/1FAIpQLSclPFQb2xUddV0opa8eY7Z1SA-yRCP0jVesUhr-TXt5_c8ehw/viewform).\n\nI'm mainly posting it here because I'm curious how well my factor model extrapolates. I want to have this data available when I do a more in-depth analysis of the results from the census. I don't necessarily think the rationalism scores will do great at characterizing rationalists, but they might.\n\nI scored 12/18. If I add this to my score from my previous survey, I get 26/42.",
      "plaintextDescription": "I've in light of the previous opinions survey, I've added some nuances to my model, which yields a new opinions survey: Opinions survey 2.\n\nI'm mainly posting it here because I'm curious how well my factor model extrapolates. I want to have this data available when I do a more in-depth analysis of the results from the census. I don't necessarily think the rationalism scores will do great at characterizing rationalists, but they might.\n\nI scored 12/18. If I add this to my score from my previous survey, I get 26/42.",
      "wordCount": 91
    },
    "tags": [
      {
        "_id": "kJrjorSx3hXa7q7CJ",
        "name": "Surveys",
        "slug": "surveys"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5ehNxLicoqKwKc8YK",
    "title": "Opinions survey (with rationalism score at the end)",
    "slug": "opinions-survey-with-rationalism-score-at-the-end",
    "url": "https://docs.google.com/forms/d/e/1FAIpQLSdSKvHo-6HyZqHprCDoBD-VjKxF-Rhp2qNhHV8d0SY40JJdjA/viewform",
    "baseScore": 8,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2024-02-17T00:41:20.188Z",
    "contents": {
      "markdown": "Based on the results from the recent LW census, I quickly threw together [a test that measures how much of a rationalist you are](https://docs.google.com/forms/d/e/1FAIpQLSdSKvHo-6HyZqHprCDoBD-VjKxF-Rhp2qNhHV8d0SY40JJdjA/viewform).\n\nI'm mainly posting it here because I'm curious how well my factor model extrapolates. I want to have this data available when I do a more in-depth analysis of the results from the census.\n\nI scored 14/24.",
      "plaintextDescription": "Based on the results from the recent LW census, I quickly threw together a test that measures how much of a rationalist you are.\n\nI'm mainly posting it here because I'm curious how well my factor model extrapolates. I want to have this data available when I do a more in-depth analysis of the results from the census.\n\nI scored 14/24.",
      "wordCount": 61
    },
    "tags": [
      {
        "_id": "kJrjorSx3hXa7q7CJ",
        "name": "Surveys",
        "slug": "surveys"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "n9ht4aPNpxenibnw8",
    "title": "What are the known difficulties with this alignment approach?",
    "slug": "what-are-the-known-difficulties-with-this-alignment-approach",
    "url": null,
    "baseScore": 18,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 24,
    "createdAt": null,
    "postedAt": "2024-02-11T22:52:18.900Z",
    "contents": {
      "markdown": "Assume you have a world-model that is nicely factored into spatially localized variables that contain interesting-to-you concepts. (Yes, that's a big assumption, but are there any known difficulties with the proposal if we grant this assumption?)\n\nPick some Markov blanket (which contains some actuators) as the bounds for your AI intervention.\n\nRepresent your goals as a causal graph (or computer program, or whatever) that fits within these bounds. For instance if you want a fusion power plant, represent it as something that takes in water and produces helium and electricity.\n\nPerform a Pearlian counterfactual surgery where you cut out the variables within the Markov blanket and replace them with a program representing your high-level goal, and then optimize the action variables to match the behavior of the counterfactual graph.",
      "plaintextDescription": "Assume you have a world-model that is nicely factored into spatially localized variables that contain interesting-to-you concepts. (Yes, that's a big assumption, but are there any known difficulties with the proposal if we grant this assumption?)\n\nPick some Markov blanket (which contains some actuators) as the bounds for your AI intervention.\n\nRepresent your goals as a causal graph (or computer program, or whatever) that fits within these bounds. For instance if you want a fusion power plant, represent it as something that takes in water and produces helium and electricity.\n\nPerform a Pearlian counterfactual surgery where you cut out the variables within the Markov blanket and replace them with a program representing your high-level goal, and then optimize the action variables to match the behavior of the counterfactual graph.",
      "wordCount": 129
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oZ7Nqtw8dtjC2ZTCZ",
    "title": "Against Nonlinear (Thing Of Things)",
    "slug": "against-nonlinear-thing-of-things",
    "url": "https://thingofthings.substack.com/p/against-nonlinear",
    "baseScore": 58,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 18,
    "createdAt": null,
    "postedAt": "2024-01-18T21:40:00.369Z",
    "contents": {
      "markdown": "[Against Nonlinear](https://thingofthings.substack.com/p/against-nonlinear) is a followup that Ozy wrote to [Practically A Book Review: Appendix to \"Nonlinear's Evidence: Debunking False and Misleading Claims\"](https://thingofthings.substack.com/p/practically-a-book-review-appendix). In it, they lay out 4 trends in Effective Altruism where Nonlinear was especially bad:\n\n*   Do It For The Gram: Nonlinear was turning Effective Altruism into being Instagrammable instead of making the world better\n*   So-Called Top EAs: Nonlinear was ranking EAs based on their participation in exclusive parties\n*   File Your Paperwork: Nonlinear was disregarding legally mandated paperwork or generally important accounting\n*   Sunshine Is The Best Disinfectant: Nonlinear was exploiting and undermining EA's trustworthiness",
      "plaintextDescription": "Against Nonlinear is a followup that Ozy wrote to Practically A Book Review: Appendix to \"Nonlinear's Evidence: Debunking False and Misleading Claims\". In it, they lay out 4 trends in Effective Altruism where Nonlinear was especially bad:\n\n * Do It For The Gram: Nonlinear was turning Effective Altruism into being Instagrammable instead of making the world better\n * So-Called Top EAs: Nonlinear was ranking EAs based on their participation in exclusive parties\n * File Your Paperwork: Nonlinear was disregarding legally mandated paperwork or generally important accounting\n * Sunshine Is The Best Disinfectant: Nonlinear was exploiting and undermining EA's trustworthiness",
      "wordCount": 99
    },
    "tags": [
      {
        "_id": "GFdvCeipZTgqfysEB",
        "name": "Drama",
        "slug": "drama"
      },
      {
        "_id": "9WWgE7mEznTx6GtM4",
        "name": "Nonlinear (org)",
        "slug": "nonlinear-org"
      },
      {
        "_id": "827JKe7YNjAegR468",
        "name": "Effective altruism",
        "slug": "effective-altruism"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sTg9gZecHn9DbahB2",
    "title": "Which investments for aligned-AI outcomes?",
    "slug": "which-investments-for-aligned-ai-outcomes",
    "url": null,
    "baseScore": 9,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2024-01-04T13:28:57.198Z",
    "contents": {
      "markdown": "*The text below is a confused ramble to illustrate where my mindset is at.*\n\nSuppose we condition on developing some kind of AGI that is reasonably controllable. In that case, the future is going to look a lot different from the present, as I assume the value of a lot of human cognitive labor is going to drop a lot. It seems like it would be wise to invest so one is in a reasonably good position for life, even in that case.\n\nOne investment option would be stock in AI companies, under the assumption that this stock would go up with AI. This seems superficially plausible, but I can't help but worry that it's dangerous to put too much money into this, as there's the whole \"moat\" problem; couldn't other organizations just make their own AIs, thereby driving down the price of the AI itself to ~0?\n\nI'm not sure what else would be worth investing in. Maybe complementary goods or resources (I've seen chip firms mentioned sometimes), but are there any that can be known to predictably increase in value?\n\nSome people say to invest in land because land prices keep going up. But I can't help but wonder if there's a pyramid scheme element to this. In the end, we'd expect land to increase in value because people who need land get more productive and can therefore afford to bid it higher, but if the value of human labor drops, shouldn't we expect the value of land to drop too? Do we think governments could somehow put in some policies to prevent this? If they could, presumably this would lead to economic distortions, but which distortions would be most plausible to occur?\n\nI guess ultimately if the value of human labor drops to ~0, we'd need something like Universal Basic Income to survive. This would have to be funded by taxing whoever owns whatever is generating economic value, and the taxes need to be enforced with capacity for violence, which in turn requires some solid source of economic value.\n\nIf we speculate that AI companies are the ones that generate economic value, then this would look something like the state taxing AI companies and then using those taxes partly for UBI and partly for hiring them to make AI systems that enforce the taxes. But if the AI companies are all American, then it would be harder for EU countries to tax them.\n\nI guess the standard take would be that due to uncertainty, it's best to have a diverse set of investments. Which I guess is logical enough, but I keep getting stuck on the issue of land here. Most people seem to first buy place to live in, and if I did that it would eat up all of my wealth and require me to get a loan. This is basically an all-in bet on land, which I guess could be a great bet if land increases in value, but extremely bad if land decreases in value. Since so many other people are making this bet, one could argue that the government will make sure it pays out; but if the government will be reliant on wealth taxes from AI companies in the future, then even that logic can only work out in countries that have their own AI companies.",
      "plaintextDescription": "The text below is a confused ramble to illustrate where my mindset is at.\n\nSuppose we condition on developing some kind of AGI that is reasonably controllable. In that case, the future is going to look a lot different from the present, as I assume the value of a lot of human cognitive labor is going to drop a lot. It seems like it would be wise to invest so one is in a reasonably good position for life, even in that case.\n\nOne investment option would be stock in AI companies, under the assumption that this stock would go up with AI. This seems superficially plausible, but I can't help but worry that it's dangerous to put too much money into this, as there's the whole \"moat\" problem; couldn't other organizations just make their own AIs, thereby driving down the price of the AI itself to ~0?\n\nI'm not sure what else would be worth investing in. Maybe complementary goods or resources (I've seen chip firms mentioned sometimes), but are there any that can be known to predictably increase in value?\n\nSome people say to invest in land because land prices keep going up. But I can't help but wonder if there's a pyramid scheme element to this. In the end, we'd expect land to increase in value because people who need land get more productive and can therefore afford to bid it higher, but if the value of human labor drops, shouldn't we expect the value of land to drop too? Do we think governments could somehow put in some policies to prevent this? If they could, presumably this would lead to economic distortions, but which distortions would be most plausible to occur?\n\nI guess ultimately if the value of human labor drops to ~0, we'd need something like Universal Basic Income to survive. This would have to be funded by taxing whoever owns whatever is generating economic value, and the taxes need to be enforced with capacity for violence, which in turn requires some solid source of economic value.\n\nIf we speculate that AI companies are the ones that generate economic value, then th",
      "wordCount": 555
    },
    "tags": [
      {
        "_id": "jgcAJnksReZRuvgzp",
        "name": "Financial Investing",
        "slug": "financial-investing"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wNqufsqkicMNxabZz",
    "title": "Practically A Book Review: Appendix to \"Nonlinear's Evidence: Debunking False and Misleading Claims\" (ThingOfThings)",
    "slug": "practically-a-book-review-appendix-to-nonlinear-s-evidence",
    "url": "https://thingofthings.substack.com/p/practically-a-book-review-appendix",
    "baseScore": 111,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 25,
    "createdAt": null,
    "postedAt": "2024-01-03T17:07:13.990Z",
    "contents": {
      "markdown": "*Subtitle: Taking nonprescription amphetamines across the U.S.-Mexico border is a felony*\n\nI haven't followed the controversy long enough to be able to tell how correct Ozy is, but what I like about this post is that I find it better structured than some posts, and with better focus on strategically relevant information.\n\nFor instance, in response to Nonlinear's claim that they weren't isolating Alice and Chloe, Ozy wrote:\n\n> Kat believes in the importance of digital nomads remaining socially connected to others. However, Kat and Emerson had a consistent pattern of encouraging Alice and Chloe to spend time with people they considered high value (i.e. effective altruists, especially those working in AI safety) instead of people they considered low-value. To be clear, Kat and Emerson didn't think Alice and Chloe should completely isolate themselves from people who weren’t effective altruists. Kat encouraged Alice and Chloe to call their families regularly. She explicitly supports spending some time with locals. Friends and family who didn't work in AI safety were invited to travel with Nonlinear, although they were lower priority to invite than AI safety people.\n> \n> However, the vast majority of Kat’s evidence that she didn't isolate Alice and Chloe is evidence that she didn't isolate Alice and Chloe from effective altruists, particularly \"top\" effective altruists working in AI safety. Alice and Chloe were given lots of access to so-called top effective altruists: there was an average of 7 people living in the house. Nonlinear encouraged networking with FTX people. They traveled with Chloe's boyfriend, whom Kat Woods considered to \"have high potential.\" Inviting people to travel with Nonlinear was framed as \"one of the highest ROI things you can do”—that is, as an important means of bettering the world.\n> \n> Kat and Emerson discouraged Alice from visiting her family because her trip overlapped with \"some of the top figures in the field\" coming to visit. (The chatlogs are suggestive that Alice timed her visit around a family emergency, but Kat doesn't explicitly mention this.) Kat also discouraged Alice from spending too much time socializing with locals, saying that she would have higher impact if she spent time with higher-value people.\n> \n> It doesn't seem to have occurred to Nonlinear that Alice and Chloe might have been complaining about being isolated from people who aren't effective altruists.\n\nI was thinking along similar lines when I was reading this part of Nonlinear's response, but I hadn't followed it well enough to make the argument in detail.\n\nWhen I say \"strategic\", what I mean is that the above information focuses on the specifics of the relationships and the roles that people had. This is as opposed to \"generic\" meanings of \"social isolation\", which might simply be defined based on the presence or absence of other people.\n\n(As a side-note, I think there might be a general rationality phenomenon to study here? The distinction between strategic phenomena and generic phenomena, and miscommunication/deception when one is substituted for the other. But this gets a bit off-topic for the overall post.)",
      "plaintextDescription": "Subtitle: Taking nonprescription amphetamines across the U.S.-Mexico border is a felony\n\nI haven't followed the controversy long enough to be able to tell how correct Ozy is, but what I like about this post is that I find it better structured than some posts, and with better focus on strategically relevant information.\n\nFor instance, in response to Nonlinear's claim that they weren't isolating Alice and Chloe, Ozy wrote:\n\n> Kat believes in the importance of digital nomads remaining socially connected to others. However, Kat and Emerson had a consistent pattern of encouraging Alice and Chloe to spend time with people they considered high value (i.e. effective altruists, especially those working in AI safety) instead of people they considered low-value. To be clear, Kat and Emerson didn't think Alice and Chloe should completely isolate themselves from people who weren’t effective altruists. Kat encouraged Alice and Chloe to call their families regularly. She explicitly supports spending some time with locals. Friends and family who didn't work in AI safety were invited to travel with Nonlinear, although they were lower priority to invite than AI safety people.\n> \n> However, the vast majority of Kat’s evidence that she didn't isolate Alice and Chloe is evidence that she didn't isolate Alice and Chloe from effective altruists, particularly \"top\" effective altruists working in AI safety. Alice and Chloe were given lots of access to so-called top effective altruists: there was an average of 7 people living in the house. Nonlinear encouraged networking with FTX people. They traveled with Chloe's boyfriend, whom Kat Woods considered to \"have high potential.\" Inviting people to travel with Nonlinear was framed as \"one of the highest ROI things you can do”—that is, as an important means of bettering the world.\n> \n> Kat and Emerson discouraged Alice from visiting her family because her trip overlapped with \"some of the top figures in the field\" coming to visit. (The chatlogs a",
      "wordCount": 508
    },
    "tags": [
      {
        "_id": "GFdvCeipZTgqfysEB",
        "name": "Drama",
        "slug": "drama"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cS7rhjuG9thmfTqdy",
    "title": "Could there be \"natural impact regularization\" or \"impact regularization by default\"?",
    "slug": "could-there-be-natural-impact-regularization-or-impact",
    "url": null,
    "baseScore": 28,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2023-12-01T22:01:46.062Z",
    "contents": {
      "markdown": "Specifically, imagine you use [general-purpose](https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see) search procedure which recursively invokes itself to solve subgoals for the purpose of solving some bigger goal.\n\nIf the search procedure's solutions to subgoals \"change things too much\", then they're probably not going to be useful. E.g. for Rubik's cubes, if you want to swap some of the cuboids, it does you know good if those swaps leave the rest of the cube scrambled.\n\nThus, to some extent, powerful capabilities would have to rely on some sort of impact regularization.\n\nI'm thinking that natural impact regularization is related to the notion of \"elegance\" in engineering. Like if you have some bloated tool to solve a problem, then even if it's not strictly speaking an issue because you can afford the resources, it might feel ugly because it's excessive and puts mild constaints on your other underconstrained decisions, and so on. Meanwhile a simple, minimal solution often doesn't have this.\n\nNatural impact regularization wouldn't *guarantee* safety, since it's still allows deviations that don't interfere with the AI's function, but it sort of reduces one source of danger which I had been thinking about lately, namely I had been thinking that the instrumental incentive is to search for powerful methods of influencing the world, where \"power\" connotes the sort of raw power that unstopably forces a lot of change, but really the instrumental incentive is often to search for \"precise\" methods of influencing the world, where one can push in a lot of information to effect narrow change.[^csrxgqulp65]\n\nMaybe another word for it would be \"natural inner alignment\", since in a sense the point is that capabilities inevitably select for inner alignment. Here I mean \"natural\" in the sense of natural abstractions, i.e. something that a wide variety of cognitive algorithms would gravitate towards.\n\n[^csrxgqulp65]: A complication is that any one agent can only have so much bandwidth, which would sometimes incentivize more blunt control. I've been thinking bandwidth is probably going to become a huge area of agent foundations, and that it's been underexplored so far. (Perhaps because everyone working in alignment sucks at managing their bandwidth? 😅)",
      "plaintextDescription": "Specifically, imagine you use general-purpose search procedure which recursively invokes itself to solve subgoals for the purpose of solving some bigger goal.\n\nIf the search procedure's solutions to subgoals \"change things too much\", then they're probably not going to be useful. E.g. for Rubik's cubes, if you want to swap some of the cuboids, it does you know good if those swaps leave the rest of the cube scrambled.\n\nThus, to some extent, powerful capabilities would have to rely on some sort of impact regularization.\n\nI'm thinking that natural impact regularization is related to the notion of \"elegance\" in engineering. Like if you have some bloated tool to solve a problem, then even if it's not strictly speaking an issue because you can afford the resources, it might feel ugly because it's excessive and puts mild constaints on your other underconstrained decisions, and so on. Meanwhile a simple, minimal solution often doesn't have this.\n\nNatural impact regularization wouldn't guarantee safety, since it's still allows deviations that don't interfere with the AI's function, but it sort of reduces one source of danger which I had been thinking about lately, namely I had been thinking that the instrumental incentive is to search for powerful methods of influencing the world, where \"power\" connotes the sort of raw power that unstopably forces a lot of change, but really the instrumental incentive is often to search for \"precise\" methods of influencing the world, where one can push in a lot of information to effect narrow change.[1]\n\nMaybe another word for it would be \"natural inner alignment\", since in a sense the point is that capabilities inevitably select for inner alignment. Here I mean \"natural\" in the sense of natural abstractions, i.e. something that a wide variety of cognitive algorithms would gravitate towards.\n\n 1. ^\n    A complication is that any one agent can only have so much bandwidth, which would sometimes incentivize more blunt control. I've been thinking",
      "wordCount": 297
    },
    "tags": [
      {
        "_id": "yTuPAtcPHSpc9r3kA",
        "name": "Impact Regularization",
        "slug": "impact-regularization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "KdEjYpdRyqi9DzNTm",
    "title": "Utility is not the selection target",
    "slug": "utility-is-not-the-selection-target",
    "url": null,
    "baseScore": 24,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2023-11-04T22:48:20.713Z",
    "contents": {
      "markdown": "*Epistemic status: shitpost.*\n\nSuppose that you are selecting for utility. Naively, you might think that this means you are selecting for utility, but actually this is not the case.\n\nSometimes greenbrownness is the selection target\n------------------------------------------------\n\nMilitary gear is sort of greenbrownish.\n\n![Military Camouflage - how camouflage works & the science behind it | UF PRO  Blog](https://ufpro.com/storage/app/media/Blog/Military%20camouflage/military-camouflage-science-hero.jpg)\n\nThis occurs because the designers of military gear are selecting for keeping soldiers alive, which benefits from camouflage, which in the common environments works best if it is greenbrownish. However, it fails at this in nongreenbrownish environments. Hence, keeping soldiers alive is not the selection target of military gear.\n\nSometimes disutility is the selection target\n--------------------------------------------\n\nIn the prisoner's dilemma, the highest-utility outcome is (cooperate, cooperate).\n\n![The pay-off matrix in the Prisoners' Dilemma game. The first entry... |  Download Scientific Diagram](https://www.researchgate.net/publication/2184338/figure/fig1/AS:671529209692164@1537116448235/The-pay-off-matrix-in-the-Prisoners-Dilemma-game-The-first-entry-refers-to-Alices.png)\n\nHowever, utility maximizers will end up in (defect, defect), which is strictly lower utility than (cooperate, cooperate). Thus, utility maximization might have utility minimization as the selection target.\n\nSometimes the superficial appearance of utility is the selection target\n-----------------------------------------------------------------------\n\nLet's say that you see a website banner saying \"You are the 1000000000th visitor to the site. Click to receive your award!\". This looks like something people would say if they want to give you something, so you click the banner.\n\n![Electronic Arts on X: \"@GameStop https://t.co/ELEQRcboMQ\" / X](https://pbs.twimg.com/media/EDOJNLjUcAIgFrf.jpg)\n\nThis does not end well for you, but you went into it selecting it because it looked like it would end well for you. \n\nConclusion\n----------\n\nBeware about reasoning about utility maximizers as maximizing utility. Utility maximizers may instead be maximizing many other things that are unrelated to utility, and not be maximizing utility.",
      "plaintextDescription": "Epistemic status: shitpost.\n\nSuppose that you are selecting for utility. Naively, you might think that this means you are selecting for utility, but actually this is not the case.\n\n\nSometimes greenbrownness is the selection target\nMilitary gear is sort of greenbrownish.\n\nThis occurs because the designers of military gear are selecting for keeping soldiers alive, which benefits from camouflage, which in the common environments works best if it is greenbrownish. However, it fails at this in nongreenbrownish environments. Hence, keeping soldiers alive is not the selection target of military gear.\n\n\nSometimes disutility is the selection target\nIn the prisoner's dilemma, the highest-utility outcome is (cooperate, cooperate).\n\nHowever, utility maximizers will end up in (defect, defect), which is strictly lower utility than (cooperate, cooperate). Thus, utility maximization might have utility minimization as the selection target.\n\n\nSometimes the superficial appearance of utility is the selection target\nLet's say that you see a website banner saying \"You are the 1000000000th visitor to the site. Click to receive your award!\". This looks like something people would say if they want to give you something, so you click the banner.\n\n \nThis does not end well for you, but you went into it selecting it because it looked like it would end well for you. \n\n\nConclusion\nBeware about reasoning about utility maximizers as maximizing utility. Utility maximizers may instead be maximizing many other things that are unrelated to utility, and not be maximizing utility.",
      "wordCount": 280
    },
    "tags": [
      {
        "_id": "MAp6Ft8b3s7kJdrQ9",
        "name": "Selection Effects",
        "slug": "selection-effects"
      },
      {
        "_id": "yxA4rv3kq9JqxdMmA",
        "name": "Utility",
        "slug": "utility"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6ZKKmiF9cnwNxiYa2",
    "title": "Contra Nora Belrose on Orthogonality Thesis Being Trivial",
    "slug": "contra-nora-belrose-on-orthogonality-thesis-being-trivial",
    "url": null,
    "baseScore": 18,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 21,
    "createdAt": null,
    "postedAt": "2023-10-07T11:47:02.401Z",
    "contents": {
      "markdown": "[Disagreeing with Eliezer Y., Nora Belrose calls the Orthogonality Thesis \"trivial, false or meaningless/confused\"](https://twitter.com/norabelrose/status/1709598947470377139).\n\nI think this is wrong. For instance, without the orthogonality thesis, one might think that in order to improve the world, one should charge ahead in creating an AGI, which would automatically use its superior intelligence to figure out the meaning of life and achieve it.\n\nYou might think this is stupid, but some significant people believe it. For example, some guy named E. Yudkowsky who used to run an organization for creating AGI [wrote an argument like this](https://docs.google.com/document/d/12xxhvL34i7AcjXtJ9phwelZ7IzHZ_xiz-8lGwpWxucI/edit?hl=en_US#heading=h.6fc6c4d29c3f), but thanks to Eliezer Y.'s reasoning about the orthogonality thesis and related topics, he eventually changed his mind. Clearly the orthogonality thesis is nontrivially useful for cases like this.",
      "plaintextDescription": "Disagreeing with Eliezer Y., Nora Belrose calls the Orthogonality Thesis \"trivial, false or meaningless/confused\".\n\nI think this is wrong. For instance, without the orthogonality thesis, one might think that in order to improve the world, one should charge ahead in creating an AGI, which would automatically use its superior intelligence to figure out the meaning of life and achieve it.\n\nYou might think this is stupid, but some significant people believe it. For example, some guy named E. Yudkowsky who used to run an organization for creating AGI wrote an argument like this, but thanks to Eliezer Y.'s reasoning about the orthogonality thesis and related topics, he eventually changed his mind. Clearly the orthogonality thesis is nontrivially useful for cases like this.",
      "wordCount": 122
    },
    "tags": [
      {
        "_id": "agnEHZTiXEyzBFPmF",
        "name": "Orthogonality Thesis",
        "slug": "orthogonality-thesis"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CkJ5D7zpHjKbykKJz",
    "title": "What are some good language models to experiment with?",
    "slug": "what-are-some-good-language-models-to-experiment-with",
    "url": null,
    "baseScore": 16,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2023-09-10T18:31:50.272Z",
    "contents": {
      "markdown": "Like if I want to experiment with a steering technique, it would be useful to have a language model that is small, capable, but not so finetuned that it becomes inflexible. (Or maybe ideally, a model which has both a finetuned and a non-finetuned variant.)\n\nI've seen some people use GPT-2. Is that recommended? Are there any alternatives?",
      "plaintextDescription": "Like if I want to experiment with a steering technique, it would be useful to have a language model that is small, capable, but not so finetuned that it becomes inflexible. (Or maybe ideally, a model which has both a finetuned and a non-finetuned variant.)\n\nI've seen some people use GPT-2. Is that recommended? Are there any alternatives?",
      "wordCount": 58
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4S6zunFNFY3f5JYxt",
    "title": "Aumann-agreement is common",
    "slug": "aumann-agreement-is-common",
    "url": null,
    "baseScore": 72,
    "voteCount": 47,
    "viewCount": null,
    "commentCount": 33,
    "createdAt": null,
    "postedAt": "2023-08-26T20:22:03.738Z",
    "contents": {
      "markdown": "*Thank you to Justis Mills for proofreading and feedback. This post is also available on* [*my substack*](https://tailcalled.substack.com/p/aumann-agreement-is-common)*.*\n\n[Aumann's agreement theorem](https://www.lesswrong.com/tag/aumann-s-agreement-theorem) is a family of theorems which say that if people trust each other and know each other's opinions, then they agree with each other. Or phrased another way, if people maintain trust with each other, then they can reach agreement. (And some variants of the theorem, which take computational factors into consideration, suggest they can do so quite rapidly.)\n\nThe original proof is pretty formal and confusing, but a simpler heuristic argument is that for an honest, rational agent, [the mere fact of them professing an opinion](https://www.lesswrong.com/rationality/what-is-evidence) can be [strong evidence](https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common) to another rational agent, because if the speaker's probabilities are higher than the speaker's prior, then they must have seen corresponding evidence to justify that opinion.\n\nSome people find this confusing, and feel like it must be wrong because it doesn't apply to most disagreements. I think these people are wrong because they are not sufficiently expansive in what they think of as a disagreement. The notion of disagreement that Aumann's agreement theorem applies to is when the people assign different probabilities to events; this is a quite inclusive notion which covers many things that we don't typically think of as disagreements, including cases where one party has information about a topic and the other party has no information.\n\nMy vacation in Norway relied tons on Aumann agreements\n------------------------------------------------------\n\nRecently, I had a vacation in Norway with my wife.\n\nIn order to get there, and to get around, we needed transport. At first we disagreed with people who provided transport there, as we didn't know of many specific means of transport, only vaguely that there would be some planes and ships, without knowing which ones. But my wife had heard that there was something called the \"Oslo ferry\", so we Aumann-agreed that this was an option, and decided to investigate further.\n\nWe disagreed with the company that provided the Oslo ferry, as we didn't know what their website is, so we asked Google, and it provided some options for what the ferry might be, and we Aumann-agreed with Google and then went investigating from there. One website we found claimed to sell tickets to the ferry; at first we disagreed with the website about when we could travel as we didn't know the times of the ferry, but then we read which times it claimed was available, and Aumann-updated to that.\n\nWe also had to find some things to do in Norway. Luckily for us, some people at OpenAI had noticed that everyone had huge disagreements with the internet as nobody had really memorized the internet, and they thought that they could gain some value by resolving that disagreement, so they Aumann-agreed with the internet by stuffing it into a neural network called ChatGPT. At first, ChatGPT disagreed with us about what to visit in Norway and suggested some things we were not really interested in, but we informed it about our interests, and then it quickly Aumann-agreed with us and proposed some other things that were more interesting.\n\nOne of the things we visited was a museum for an adventurer who built a raft and sailed in the ocean. Prior to visiting the museum, we had numerous disagreements with it, as e.g. we didn't know that one of the people on the raft had fallen in the ocean and had to be rescued. But the museum told us this was the case, so we Aumann-agreed to believe it. Presumably, the museum learnt about it through Aumann-agreeing with the people on the raft.\n\nOne example of an erroneous Aumann agreement was with the train company Vy. They had said that they could get us a train ticket on the Bergen train, and we had Aumann-agreed with that. However, due to a storm, their train tracks were broken, and the company website kept promising availability on the train until the last moment, so we didn't get corrected by Vy.\n\nBut we were not saved by empirically seeing the damaged tracks, or by rationally reasoning that it was not available. Instead, we were saved because we told someone about our plans to take the Bergen train, expecting them to Aumann-agree to a belief that we would take the train, but instead they kept disagreeing, and told us that it was flooded and would be cancelled. This made us Aumann-agree that we had to find some other method, and we asked Google whether there were any flights, of which it suggested some that we Aumann-agreed to.\n\nLater, I've told my dad and now also you about the trip. Prior to talking about it, I expect you disagreed as you didn't know anything about it, but at least I'm pretty sure my dad Aumann-agreed to the things I told him, and I suspect you did so too.\n\nAumannian disagreements quickly disappear, and so \"disagreement\" connotes/denotes non-Aumannian disagreements\n-------------------------------------------------------------------------------------------------------------\n\nThe disagreements mentioned in my story all happened between parties with reasonable levels of trust, and they mostly involved one party lacking information and the other party having information, so they were quickly resolved by transferring one's information. Even noticing the specifics of the disagreement is sufficient to transfer the information and resolve it.\n\nMeanwhile, in politics, disagreements often occur between people who have conflicting goals, where they it is reasonable to suspect that one side is misrepresenting things because they care more about gaining power than accurately informing the people they talk to.\n\nBecause the preconditions for Aumannian agreement don't hold when you suspect the counterparty to be biased, such disagreements won't be resolved so quickly, and instead stick around long-term. But if we form our opinions about what disagreements are like from what disagreements stick around long-term, then that means we are filtering out the disagreements where Aumann's conditions hold.\n\nThus, \"disagreement\" comes to connote (or maybe even denote), \"difference in opinion *between people who don't trust each other*\" rather than simply \"difference in opinion\".\n\nMost Aumannian disagreements are a simple lack of awareness\n-----------------------------------------------------------\n\nThe Bayesian paradigm doesn't *fundamentally*[^y5hm5xwtgb] distinguish disbelief in a proposition due to having no information about it, versus due to having observed contradictory information. Consider e.g. picking two random people and making a statement such as \"Marv Elsher is dating Abrielle Levine\" about them. You have no idea who these people are, and most people are not dating each other, so you should rationally assign this a very low probability.\n\nBut that's not because you actively disbelieve it from contradictory evidence! In fact you might not even think of yourself as having had a belief about it ahead of time. If there is in fact a Marv Elsher who is dating Abrielle Levine, then Marv assigns a very high probability to this statement, while you wouldn't even have thought of it without this post.\n\nIf you consider all of the cases where people assign different probabilities to symbolically expressible propositions, then almost all of them will be something along these lines, because there's tons of random local information which you simply don't have access to. Thus, if you want to think of the typical case of a disagreement that Aumann's agreement theorem refers to, you should think \"Person A has observed X and Person B does not even have awareness of what's going on around X, let alone any evidence on X itself\".\n\nAumann agreement is extremely efficient and powerful\n----------------------------------------------------\n\nFor most of the updates that happened during the vacation, it would simply not be feasible to verify things by oneself. Often they concerned things that were very far away, both in space and time. Sometimes they concerned things that happened in the past where it wouldn't even be physically possible to verify. But even for the things you could verify, it would take *orders of magnitude* more time and resources than to just Aumann-update.\n\nAumann agreement is about pooling, not moderation\n-------------------------------------------------\n\nIn my examples, people generally didn't converge to a compromise position; instead they adopted the counterparty's positions wholesale. This is generally the correct picture to have in mind for Aumann agreement. While the exact way you update can vary depending on the prior and the evidence, one simple example I like is this:\n\nYou both start with having your log-odds being some vector x according to some shared prior (i.e. you start out agreeing). You then observe some evidence y, updating your log-odds to be x+y, while they observe some independent evidence z, updating their log-odds to be x+z. If you exchange all your information, then this updates your shared log-odds to be x+y+z, which is most likely going to be an even more radical departure from x than either x+y or x+z alone.\n\nAumann conditions information on trust\n--------------------------------------\n\nSurely sometimes it seems like Aumann agreement should cause people to moderate, right? Like in politics, if you have spent a lot of time absorbing one party's ideology, and your interlocutor has spent a lot of time absorbing the other party's ideology, but you then poke lots of holes in each other's arguments?\n\nI think in this case, learning that there are holes in the arguments your learned from your party may be reason to doubt the trustworthiness of your party, especially when they cannot fix those holes. Since you Aumann-updated a great deal on your party's view specifically because you trusted them, this should also make you un-update away from their views, presumably moderating them.\n\n(I think this has massive implications for collective epistemics, and I've gradually been developing a theory of collective rationality based on this, but it's not finished yet and the purpose of this post is merely to grok the agreement theorem rather than to lay out that theory.)\n\nThere may also be less elaborate ways in which you might moderate due to Aumann agreement, e.g. if contradictory information cancels out.\n\nA lot of Aumann-updates are on promises, history or universals\n--------------------------------------------------------------\n\nMany of the most obvious Aumann updates in my story were about promises; for instance that an interlocutor would provide me a certain transport at a certain time from one location to another.\n\nOne might think this suggests that promises have a unique link to Aumann's agreement theorem, but I think this is actually because promises are an unusually prevalent type of information due to the combo of:\n\n*   People's capacity to make reliable claims about them.\n*   Being useful enough in practice to be worth sharing.\n*   Covering a diverse and open-ended set of possibilities.\n\nFor instance, if you promise me a sandwich in your kitchen, then you can ensure that your promise is true by paying rent to keep ownership of your kitchen, buying and storing ingredients for the sandwich so they are ready for assembly, and then assembling the sandwich for me when it is time.\n\nMeanwhile, if you tell me that there is an available sandwich in someone else's kitchen, then because you don't maintain control over that kitchen, it might cease to be true once we actually reach the time when I need it, so you can't reliably make claims about it. Furthermore, even if you could, I would probably not get away with taking it, so it would not be useful to me.\n\nYou could probably reasonably reliably make claims about certain things you've seen in the past, but most of those are not very useful because they happened in the past. For example, while I know from the museum that that guy on the raft fell in the water, I don't have anything to use it for. That said, sometimes (e.g. to attribute outcomes to causes, or to make generalizations), they are useful.\n\nYou can read a physics textbook and do a lot of useful Aumann updates on this, but this is mainly because physics is a \"universal\" subject, but this also means that it is a closed subject with a bounded amount of information. There can't be an \"alternate physics\" with alternate particles and strengths of attraction, unlike the same sense as there can be an \"alternate plane company\" with alternate flight times.\n\nPromises, history and universals aren't meant to be a complete taxonomy, it's just something I've noticed.\n\n[^y5hm5xwtgb]: It is distinguished through the history of updating from prior to posterior, but the distinction is not \"stored\" anywhere in the probability distribution, so the beliefs themselves are treated the same, even if their history are different.",
      "plaintextDescription": "Thank you to Justis Mills for proofreading and feedback. This post is also available on my substack.\n\nAumann's agreement theorem is a family of theorems which say that if people trust each other and know each other's opinions, then they agree with each other. Or phrased another way, if people maintain trust with each other, then they can reach agreement. (And some variants of the theorem, which take computational factors into consideration, suggest they can do so quite rapidly.)\n\nThe original proof is pretty formal and confusing, but a simpler heuristic argument is that for an honest, rational agent, the mere fact of them professing an opinion can be strong evidence to another rational agent, because if the speaker's probabilities are higher than the speaker's prior, then they must have seen corresponding evidence to justify that opinion.\n\nSome people find this confusing, and feel like it must be wrong because it doesn't apply to most disagreements. I think these people are wrong because they are not sufficiently expansive in what they think of as a disagreement. The notion of disagreement that Aumann's agreement theorem applies to is when the people assign different probabilities to events; this is a quite inclusive notion which covers many things that we don't typically think of as disagreements, including cases where one party has information about a topic and the other party has no information.\n\n\nMy vacation in Norway relied tons on Aumann agreements\nRecently, I had a vacation in Norway with my wife.\n\nIn order to get there, and to get around, we needed transport. At first we disagreed with people who provided transport there, as we didn't know of many specific means of transport, only vaguely that there would be some planes and ships, without knowing which ones. But my wife had heard that there was something called the \"Oslo ferry\", so we Aumann-agreed that this was an option, and decided to investigate further.\n\nWe disagreed with the company that provided the O",
      "wordCount": 2034
    },
    "tags": [
      {
        "_id": "WH5ZmNSjZmK9SMj7k",
        "name": "Aumann's Agreement Theorem",
        "slug": "aumann-s-agreement-theorem"
      },
      {
        "_id": "wzgcQCrwKfETcBpR9",
        "name": "Disagreement",
        "slug": "disagreement"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6q3vr5t9kXFGkHbMC",
    "title": "A content analysis of the SQ-R questionnaire and a proposal for testing EQ-SQ theory",
    "slug": "a-content-analysis-of-the-sq-r-questionnaire-and-a-proposal",
    "url": null,
    "baseScore": 10,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2023-08-09T13:51:02.036Z",
    "contents": {
      "markdown": "*If you are more interested in this topic, I have created a Discord server titled* [*Rationalist Psychometrics*](https://discord.gg/JxXjAtaVb) *to discuss these sorts of things. Thank you to Justis Mills for proofreading and feedback.*\n\nI've recently been complaining about the [EQ-SQ theory of autism](https://www.lesswrong.com/posts/SRAmQwqBEh6mk2bKN/if-i-showed-the-eq-sq-theory-s-findings-to-be-due-to) which asserts that autism is caused by having an extremely male brain, and I've felt like it could probably be disproven with a bit of work. Briefly, my opinion is that the measures conflate multiple different things (e.g. technical interests vs nature interests), and I propose that one can test this by seeing whether the items that correlate with autism are the same as the items that correlate with sex. But in order for it to be tested, someone has to actually perform that work.\n\nAnd part of the trouble here is, the Systemizing Quotient-Revised (SQ-R) is very long, so it would be very expensive to collect comprehensive data on it. I tried contacting some rationalists with reasonably far reach (Scott Alexander and Aella) to see if they would be interested in sharing a comprehensive autism measure to their audience to get me data for free, but so far I have not received any responses yet.\n\nSo I need to find some way to make it quicker and cheaper. One way to make it cheaper would be to construct a \"short form\", which measures the same traits in a shorter way by only using a subset of the items.\n\nThere have already been constructed short forms of the SQ-R, but the only ones I have seen have been constructed with a very basic empirical approach of finding items that are highly correlated with the total scores of the scale. This is a problem for investigating measurement biases, as it can obscure the measurement bias and hide invalidity of the construct.[^ntids94k42]\n\nThe principled way to solve this would be to perform a factor analysis, searching for groups of correlated items in an SQ-R dataset, and then collecting the few top items on each factor. Unfortunately, I do not have access to an SQ-R dataset; if I did, this problem would be gone.\n\nBut, usually the results of a factor analysis can be predicted fairly well by looking at similarity in the content of the items. Furthermore, I have a bunch of experience with personality and sex difference psychometrics, so I can make educated guesses about what sorts of content does or does not matter. To avoid bias, I would like to present my content analysis before I collect the data, so people can comment on it and say what they think.\n\nPotential SQ-R Factors\n----------------------\n\nBased on reading the SQ-R, I felt like something like the following a likely to be factors in the SQ-R.\n\n**Technical Interests**\n\n1\\. I find it very easy to use train timetables, even if this involves several connections.  \n6\\. I find it difficult to read and understand maps.  \n9\\. If I were buying a car, I would want to obtain specific information about its engine capacity.  \n10\\. I find it difficult to learn how to programme video recorders.  \n16\\. When I look at a building, I am curious about the precise way it was constructed.  \n17\\. I am not interested in understanding how wireless communication works (e.g. mobile phones).  \n18\\. When travelling by train, I often wonder exactly how the rail networks are coordinated.  \n32\\. I am fascinated by how machines work.  \n45\\. I rarely read articles or webpages about new technology.  \n46\\. I can easily visualise how the motorways in my region link up.  \n52\\. If I were buying a camera, I would not look carefully into the quality of the lens.  \n53\\. If I were buying a computer, I would want to know exact details about its hard drive capacity and processor speed.  \n60\\. If I were buying a stereo, I would want to know about its precise technical features.  \n66\\. In maths, I am intrigued by the rules and patterns governing numbers.  \n70\\. When I'm in a plane, I do not think about the aerodynamics.\n\n**DIY Interests**\n\n15\\. I find it difficult to understand instruction manuals for putting appliances together  \n19\\. I enjoy looking through catalogues of products to see the details of each product and how it compares to others.  \n33\\. When I look at a piece of furniture, I do not notice the details of how it was constructed.  \n43\\. If there was a problem with the electrical wiring in my home, I'd be able to fix it myself.  \n58\\. I am not very meticulous when I carry out D.I.Y or home improvements.\n\n**Nature Interests**\n\n7\\. When I look at a mountain, I think about how precisely it was formed.  \n29\\. When I look at an animal, I like to know the precise species it belongs to.  \n35\\. I do not tend to watch science documentaries on television or read articles about science and nature.  \n41\\. I am interested in knowing the path a river takes from its source to the sea.  \n50\\. When I am walking in the country, I am curious about how the various kinds of trees differ.  \n63\\. I do not care to know the names of the plants I see.  \n64\\. When I hear the weather forecast, I am not very interested in the meteorological patterns.\n\n**Orderliness**\n\n2\\. I like music or book shops because they are clearly organised.  \n4\\. When I read something, I always notice whether it is grammatically correct.  \n14\\. If I had a collection (e.g. CDs, coins, stamps), it would be highly organised.  \n20\\. Whenever I run out of something at home, I always add it to a shopping list.  \n28\\. I do not find it distressing if people who live with me upset my routines.  \n44\\. My clothes are not carefully organised into different types in my wardrobe.  \n55\\. When I get to the checkout at a supermarket, I pack different categories of goods into separate bags.  \n56\\. I do not follow any particular system when I'm cleaning at home.  \n61\\. I tend to keep things that other people might throw away, in case they might be useful for something in the future.  \n62\\. I avoid situations which I cannot control.  \n65\\. It does not bother me if things in the house are not in their proper place.  \n72\\. When I have a lot of shopping to do, I like to plan which shops I am going to visit and in what order.\n\n**Political/Business Interests**\n\n3\\. I would not enjoy organising events e.g. fundraising evenings, fetes, conferences.  \n8\\. I am not interested in the details of exchange rates, interest rates, stocks and shares.  \n13\\. I like to know how committees are structured in terms of who the different committee members represent or what their functions are.  \n24\\. When I learn about historical events, I do not focus on exact dates.  \n30\\. I can remember large amounts of information about a topic that interests me e.g. flags of the world, airline logos.  \n34\\. I know very little about the different stages of the legislation process in my country.  \n40\\. I am not interested in how the government is organised into different ministries and departments.  \n47\\. When an election is being held, I am not interested in the results for each constituency.  \n48\\. I do not particularly enjoy learning about facts and figures in history.  \n57\\. I do not enjoy in-depth political discussions.  \n69\\. When I read the newspaper, I am drawn to tables of information, such as football league scores or stock market indices.  \n25\\. I find it easy to grasp exactly how odds work in betting.\n\n**Creative Interests**\n\n37\\. When I look at a painting, I do not usually think about the technique involved in making it.  \n74\\. When I listen to a piece of music, I always notice the way it's structured.  \n75\\. I could generate a list of my favourite 10 songs from memory, including the title and the artist's name who performed each song.  \n42\\. I have a large collection e.g. of books, CDs, videos etc.  \n68\\. I could list my favourite 10 books, recalling titles and authors' names from memory.  \n73\\. When I cook, I do not think about exactly how different methods and ingredients contribute to the final product.\n\n**Social Attention**\n\n5\\. I find myself categorising people into types (in my own mind).  \n23\\. I am interested in my family tree and in understanding how everyone is related to each other in the family.  \n36\\. If someone stops to ask me the way, I'd be able to give directions to any part of my home town.  \n49\\. I do not tend to remember people's birthdays (in terms of which day and month this falls).  \n38\\. I prefer social interactions that are structured around a clear activity, e.g. a hobby.  \n67\\. I find it difficult to learn my way around a new city.\n\n**Not Immediately Classifiable**\n\n11\\. When I like something I like to collect a lot of different examples of that type of object, so I can see how they differ from each other.  \n12\\. When I learn a language, I become intrigued by its grammatical rules.  \n22\\. When I was young, I did not enjoy collecting sets of things e.g. stickers, football cards, etc.  \n26\\. I do not enjoy games that involve a high degree of strategy (e.g. chess, Risk, Games Workshop).  \n27\\. When I learn about a new category, I like to go into detail to understand the small differences between different members of that category.\n\nPotential SQ-R Short Form\n-------------------------\n\nTo create a version of the scale which captures as much of the same information as possible that the SQ-R does, one should select items from all of the factors listed above. Furthermore, factor analysis works best when one has 3 (or ideally more) items per factor[^xbi3o1vol5], so one should ideally pick multiple items per factor.\n\n**Technical Interests**\n\n53\\. If I were buying a computer, I would want to know exact details about its hard drive capacity and processor speed.  \n9\\. If I were buying a car, I would want to obtain specific information about its engine capacity.  \n32\\. I am fascinated by how machines work.  \n17\\. I am not interested in understanding how wireless communication works (e.g. mobile phones).\n\n**DIY Interests**\n\n15\\. I find it difficult to understand instruction manuals for putting appliances together  \n33\\. When I look at a piece of furniture, I do not notice the details of how it was constructed.  \n58\\. I am not very meticulous when I carry out D.I.Y or home improvements.\n\n**Nature Interests**\n\n7\\. When I look at a mountain, I think about how precisely it was formed.  \n35\\. I do not tend to watch science documentaries on television or read articles about science and nature.  \n50\\. When I am walking in the country, I am curious about how the various kinds of trees differ.\n\n**Orderliness**\n\n4\\. When I read something, I always notice whether it is grammatically correct.  \n14\\. If I had a collection (e.g. CDs, coins, stamps), it would be highly organised.  \n20\\. Whenever I run out of something at home, I always add it to a shopping list.  \n44\\. My clothes are not carefully organised into different types in my wardrobe.\n\n**Political/Business Interests**\n\n3\\. I would not enjoy organising events e.g. fundraising evenings, fetes, conferences.  \n8\\. I am not interested in the details of exchange rates, interest rates, stocks and shares.  \n13\\. I like to know how committees are structured in terms of who the different committee members represent or what their functions are.  \n24\\. When I learn about historical events, I do not focus on exact dates.  \n47\\. When an election is being held, I am not interested in the results for each constituency.\n\n**Creative Interests**\n\n37\\. When I look at a painting, I do not usually think about the technique involved in making it.  \n74\\. When I listen to a piece of music, I always notice the way it's structured.  \n42\\. I have a large collection e.g. of books, CDs, videos etc.\n\n**Social attention**\n\n5\\. I find myself categorising people into types (in my own mind).  \n23\\. I am interested in my family tree and in understanding how everyone is related to each other in the family.  \n38\\. I prefer social interactions that are structured around a clear activity, e.g. a hobby.  \n67\\. I find it difficult to learn my way around a new city.\n\n**Not Immediately Classifiable**\n\nOne argument one could make is that items which are not immediately classifiable should be included so that we can collect data on them and perform a factor analysis, and thereby classify them. Alternatively one could drop them because they are inconvenient. I have no strong opinion on this except I want to get the project done as cheaply as possible, but if you have an opinion then please share it.\n\nScoring and interpretation\n--------------------------\n\nThe original paper on the [SQ-R](https://pubmed.ncbi.nlm.nih.gov/16473340/) provides details about how to score the test. This is somewhat complicated by the fact that my short form doesn't contain all of the original items, so the scores will not be commensurate with the original scale.\n\nSBC has also constructed [a short form of the SQ-R](https://www.pnas.org/doi/suppl/10.1073/pnas.1811032115). It contains some items that my Short Form is missing, namely:\n\n32\\. When I learn about a new category I like to go into detail to understand the small differences between different members of that category.  \n16\\. When I'm in a plane, I do not think about the aerodynamics.  \n27\\. I am interested in knowing the path a river takes from its source to the sea.  \n9\\. When travelling by train, I often wonder exactly how the rail networks are coordinated.  \n30\\. When I hear the weather forecast, I am not very interested in the meteorological patterns.  \n33\\. I enjoy looking through catalogues of products to see the details of each product and how it compares to others.  \n12\\. When I learn a language, I become intrigued by its grammatical rules.\n\nIf these items are added to my short form, then I can use them to make my total scores commensurate with that of the SQ-R.\n\nOverall, that would yield a scale with 33 items, a significant shortening over the original 75 items, while still capturing much of the nuances and being suitable for factor analysis in ways that the 10-item version might miss.\n\nIn the original [SQ paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1693117/), SBC frames Systemizing as a [General Factor](https://www.lesswrong.com/posts/x6AB4i6xLBgTkeHas/framing-practicum-general-factor-2) which drives you to understand systems across a wide variety of contexts. Thus from the perspective of the Systemizing-Empathizing model, the main thing that should matter is your overall score, which should reflect your general tendency to systemize. Deviations on specific items or domains would constitute noise/measurement error, which it is hoped we can average away by having a varied set of items.\n\nA qualification on the General Factor point\n-------------------------------------------\n\nIn the original [SQ paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1693117/), SBC describes Systemizing as a general factor, but he also adds important nuances/qualifications to the idea of a general factor of Systemizing:\n\n> ... Systemizing is the drive to analyse the variables in a system, to derive the underlying rules that govern the behaviour of a system. Systemizing also refers to the drive to construct systems. Systemizing allows you to predict the behaviour of a system, and to control it. A growing body of evidence suggests that, on average, males spontaneously systemize to a greater degree than do females.\n> \n> ...\n> \n> Initially, we had planned to devise the SQ so that it would tap into each of the domain-specific systems described above. However, this proved to be problematical because individuals who were well rounded but not necessarily good systemizers would end up scoring highly, whereas those who were highly systematic but only interested in one domain would receive a low score. Thus, we decided, instead, to use examples from everyday life in which systemizing could be used to varying degrees. The assumption is that a strong systemizer would be drawn to use their systemizing skills across the range of examples more often than a poor systemizer, and would consequently score higher on the SQ.\n\nAs I read it, he is saying that you can't measure Systemizing as a general factor across tons of obscure subjects like mixology and glassblowing and so on, because most people have zero activity in those subjects, and therefore would score 0 on Systemizing in corresponding subject-specific questions. Instead, this would just end up measuring whether you had broad, exploratory interests. However, he is saying that the specific subjects he included in his scales are sufficiently present in everyday life for most people that they should be highly reflective of Systemizing.\n\nPredictions and Theory Testing\n------------------------------\n\nThe key claim of the Systemizing theory of autism and sex differences is that autistic people and men are higher in the General Factor of Systemizing than allistic people and women. In the past, this has repeatedly been superficially tested by showing how the Systemizing Quotient correlates with autism and maleness. However, the General Factor model has implications that permit stronger tests, and also makes tests much easier.\n\nBecause the General Factor of Systemizing is hypothesized to influence Systemizing behavior across main everyday domains, claims about male-female differences or autistic-allistic differences would *by default* imply that the group differences are evenly spread out over all of the items, rather than being limited to specific domains.\n\nTo give an example, imagine if the sex differences on Technical Interests were much bigger than the sex differences on Orderliness, Nature Interests and Creative Interests. This could not be compatible with a single general factor explaining the sex differences across all of these domains; instead it would be compatible with a multicausal model, with the simplest one being that there is some other factor that causes sex differences in Technical Interests, and that there isn't much sex difference in General Systemizing. If true, this would mean that the SQ-R is biased, making men score more Systemizing than they truly are.\n\nTo give another example, imagine if the sex differences lie on Technical Interests and Political/Business Interests, while the autistic-allistic differences lie on Nature Interests, Orderliness and Creative Interests. In that case, even though the SQ-R correlates with both autism and sex, this doesn't actually reflect any special relationship between autism and sex, because autism and sex would each have unique reasons for being connected to SQ-R scores.\n\nThus, detailed item-level or factor-level data provides relatively strong ways of testing the validity of the EQ-SQ theory, simply by looking at how broad or localized the group differences are.\n\nAn extension to the theory testing\n----------------------------------\n\nIn addition to looking at whether the scales' group differences are broad or localized to specific items, one can also look at whether the scales' correlations with other scales are broad or localized to specific items. For instance [the EQ has three factors which can be measured reasonably well with the following items](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4553147/):\n\n**EQ-Cognitive Empathy**\n\n*   I am quick to spot when someone in a group is feeling awkward or uncomfortable\n*   I can tune into how someone else feels rapidly and intuitively\n*   Other people tell me I am good at understanding how they are feeling and what they are thinking\n\n**EQ-Emotional Empathy**\n\n*   Friends usually talk to me about their problems as they say I am very understanding \n*   I find it easy to put myself in somebody else’s shoes\n*   It is hard for me to see why some things upset people so much\n\n**EQ-Social Skills**\n\n*   I find it hard to know what to do in a social situation\n*   I don’t tend to find social situations confusing\n*   Friendships and relationships are just too difficult, so I tend not to bother with them\n*   I often find it difficult to judge if something is rude or polite\n\nAlso, due to my experience with personality psychology, I imagine it would be beneficial to add items such as \"I care about other's feelings\", \"I take charge\", and \"I worry about things\", to better pin down their relationships to classical personality traits (compassion, assertiveness/charisma, anxiety).\n\nSimilarly, the 50-item Autism Spectrum Quotient has been said to have various factor structures. I've created the following 23-item version, which combines items that are highly indicative of autism according to [this study](https://pubmed.ncbi.nlm.nih.gov/28425021/) with items that are highly informative according to the factor structure found in [this study](https://www.sciencedirect.com/science/article/abs/pii/S0191886910005118) and items that are present in the 10-item version SBC created.\n\n**ASQ-Social Skills**\n\n*   I would rather go to a library than a party\n*   I find it hard to make new friends\n*   I find it difficult to work out people’s intentions\n*   I enjoy social occasions\n\n**ASQ-Attention Switching**\n\n*   I find it easy to do more than one thing at once\n*   I enjoy doing things spontaneously\n*   New situations make me anxious\n*   If there is an interruption, I can switch back to what I was doing very quickly\n*   I frequently get so strongly absorbed in one thing that I lose sight of other things\n\n**ASQ-Communication**\n\n*   I don’t know how to keep a conversation going\n*   I know how to tell if someone listening to me is getting bored\n*   I enjoy social chit-chat\n*   I find it easy to ‘read between the lines’ when someone is talking to me\n*   I find it easy to work out what someone is thinking or feeling just by looking at their face\n*   When I talk, it isn’t always easy for others to get a word in edgeways\n*   People often tell me that I keep going on and on about the same thing\n\n**ASQ-Imagination**\n\n*   I would rather go to the theatre than a museum\n*   I like to collect information about categories of things (e.g. types of car, types of bird, types of train, types of plant etc)\n*   When I'm reading a story, I can easily imagine what the characters might look like\n*   When I'm reading a story I find it difficult to work out the characters' intentions\n\n**ASQ-Attention to detail**\n\n*   I often notice small sounds when others do not\n*   I usually notice car number plates or similar strings of information\n*   I usually concentrate more on the whole picture, rather than the small details\n\nPaying careful attention to how these correlate can also test the EQ-SQ theory harder. For instance, if the ASQ-Attention to detail factor has a special correlation with the SQ-R's Orderliness factor, above and beyond its correlation to General Systemizing, then that complicates models where the relationship between the ASQ and the SQ-R are supposed to be solely due to General Systemizing causing autism, and not due to correlated measurement error. One can use a technique called structural equation modelling to sort of untangle these biases in various ways.\n\nBasic Plans for Testing\n-----------------------\n\nIn total, I've now discussed around 69 items. I tend to assume that each item takes 7 seconds to answer, and that respondents require 9 GBP/hour to participate. If we assume that we get 100 allistic men, 100 allistic women, 100 autistic men, and 100 autistic women, then we could estimate correlations to an error of around \\\\(\\frac{1}{\\sqrt{400}}=0.05\\\\), and group differences to an error of around \\\\(\\frac{\\sqrt{2}}{\\sqrt{100}} = 0.14\\\\), which seems adequate for this type of analysis. This would add up to a cost of 483 GBP or 615 USD, which of course is not a completely trivial amount of money, but is also fairly affordable for engineers working for Big Tech.[^io3ilur22n]\n\nI'm probably gonna collect this data at some point, but first I would like to hear if anyone has any feedback or concerns about the approach.\n\n[^ntids94k42]: To explain: Imagine you had a \"maleness test\" which had three subtests: do math, lift heavy things, and build bridges. Plausibly, \"do math\" and \"lift heavy things\" are independent factors, and \"build bridges\" involves both \"do math\" (to work out the engineering of how the bridge should be designed) and \"lift heavy things\" (to actually assemble the bridge). In this case, if you constructed a short form of the test by looking for the subtest that correlated the most with overall test scores, then that subtest could be \"build bridges\" because it loads on both the factors that the test conflates. But if we hypothesize that the sex difference is mostly on \"lift heavy things\", then this obscures the fact that the test conflates things by favoring a subtest that has the same pattern of conflation as the overall test. \n\n[^xbi3o1vol5]: Because the model is underidentified with 2 items, just-identified with 3 items, and overidentified with 4+ items. A factor model with \\(n\\) indicators has \\(n\\) parameters and \\(n(n-1)/2\\) constraints, so you can count your degrees of freedom here to see how that goes. \n\n[^fda26ei8eyj]: There's a case to be made that one could improve these scales by using more varied questions. I should probably do a more in-depth investigation into the ASQ before proceeding. \n\n[^io3ilur22n]: That said, for various reasons I am somewhat money-constrained. If I wasn't, I would probably up this to 220+ items by adding the entire SQ-R, ASQ, and EQ to the survey, as well as adding the SPQ and maybe also various other scales to it. (People-things? Political orientation? Utilitarianism? Woo? etc.) Also I would be tempted to upgrade the sample sizes to 200 for each group.",
      "plaintextDescription": "If you are more interested in this topic, I have created a Discord server titled Rationalist Psychometrics to discuss these sorts of things. Thank you to Justis Mills for proofreading and feedback.\n\nI've recently been complaining about the EQ-SQ theory of autism which asserts that autism is caused by having an extremely male brain, and I've felt like it could probably be disproven with a bit of work. Briefly, my opinion is that the measures conflate multiple different things (e.g. technical interests vs nature interests), and I propose that one can test this by seeing whether the items that correlate with autism are the same as the items that correlate with sex. But in order for it to be tested, someone has to actually perform that work.\n\nAnd part of the trouble here is, the Systemizing Quotient-Revised (SQ-R) is very long, so it would be very expensive to collect comprehensive data on it. I tried contacting some rationalists with reasonably far reach (Scott Alexander and Aella) to see if they would be interested in sharing a comprehensive autism measure to their audience to get me data for free, but so far I have not received any responses yet.\n\nSo I need to find some way to make it quicker and cheaper. One way to make it cheaper would be to construct a \"short form\", which measures the same traits in a shorter way by only using a subset of the items.\n\nThere have already been constructed short forms of the SQ-R, but the only ones I have seen have been constructed with a very basic empirical approach of finding items that are highly correlated with the total scores of the scale. This is a problem for investigating measurement biases, as it can obscure the measurement bias and hide invalidity of the construct.[1]\n\nThe principled way to solve this would be to perform a factor analysis, searching for groups of correlated items in an SQ-R dataset, and then collecting the few top items on each factor. Unfortunately, I do not have access to an SQ-R dataset; if I did, this ",
      "wordCount": 3942
    },
    "tags": [
      {
        "_id": "dBPou4ihoQNY4cquv",
        "name": "Psychology",
        "slug": "psychology"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SRAmQwqBEh6mk2bKN",
    "title": "If I showed the EQ-SQ theory's findings to be due to measurement bias, would anyone change their minds about it?",
    "slug": "if-i-showed-the-eq-sq-theory-s-findings-to-be-due-to",
    "url": null,
    "baseScore": 23,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2023-07-29T19:38:13.285Z",
    "contents": {
      "markdown": "Background on EQ-SQ\n-------------------\n\nThe EQ-SQ theory is a theory by Simon Baron-Cohen which states that there's a biological sex difference in tendency to empathize with people (Empathizing) versus try to understand deterministic systems (Systematizing), and that autism represents the extreme Systematizing end of this spectrum.\n\nMeasurement bias\n----------------\n\nSince writing [this subthread](https://www.lesswrong.com/posts/ZjXtjRQaD2b4PAser/a-hill-of-validity-in-defense-of-meaning?commentId=mhnZjEjj9bdHxSFkv) where I found ambiguous and partial evidence that the EQ-SQ theory's findings are due to measurement bias, I've been considering collecting direct evidence with SBC's EQ and SQ-R scales, and write up a detailed post investigating measurement bias.\n\nMeasurement bias, if it exists ([as it very much seems to do on e.g. the EQ scale](https://www.lesswrong.com/posts/ZjXtjRQaD2b4PAser/a-hill-of-validity-in-defense-of-meaning?commentId=EWnJ2yWS7cKdbj8vy)), would most likely involve observing that the sex difference on the scales is limited to one subset of the items, the autistic-allistic difference is limited to another subset of the items, and maybe also the main thing the scale measures exists on a yet third subset of the items.\n\nThat is, it would involve observing that the scales mix different things together, and that autism and sex have distinct relationships with the things that get mixed together. As an analogy, imagine showing that autism is extreme masculinity by creating a \"maleness test\" which mixes together physical strength and poor social skills. Yes, this would have a large sex difference and a large autistic-allistic difference, but that'd transparently be measurement bias; the reason for the large sex difference (it measures physical strength) would be distinct from the reason for the large autistic-allistic difference (it measures social skills).\n\nMy concern\n----------\n\nMy concern is that people didn't start believing in the EQ-SQ theory based on statistical correlations found with Simon Baron-Cohen's scales. They presumably started believing in it based on fuzzy intuitions arrived at through social experience.\n\nSo I could imagine that if I did show SBC's findings to be due to measurement error, people would sort of dismiss it as poorly designed scales and still continue believing in the theory. Like basically maybe SBC's studies provide a [faux-objective](http://benjaminrosshoffman.com/authoritarian-empiricism/) criterion they can use to throw at feminists, but if the criterion doesn't validate sex differences then EQ-SQ enthusiasts would [discard it and pick a new one](https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection).\n\nThoughts?",
      "plaintextDescription": "Background on EQ-SQ\nThe EQ-SQ theory is a theory by Simon Baron-Cohen which states that there's a biological sex difference in tendency to empathize with people (Empathizing) versus try to understand deterministic systems (Systematizing), and that autism represents the extreme Systematizing end of this spectrum.\n\n\nMeasurement bias\nSince writing this subthread where I found ambiguous and partial evidence that the EQ-SQ theory's findings are due to measurement bias, I've been considering collecting direct evidence with SBC's EQ and SQ-R scales, and write up a detailed post investigating measurement bias.\n\nMeasurement bias, if it exists (as it very much seems to do on e.g. the EQ scale), would most likely involve observing that the sex difference on the scales is limited to one subset of the items, the autistic-allistic difference is limited to another subset of the items, and maybe also the main thing the scale measures exists on a yet third subset of the items.\n\nThat is, it would involve observing that the scales mix different things together, and that autism and sex have distinct relationships with the things that get mixed together. As an analogy, imagine showing that autism is extreme masculinity by creating a \"maleness test\" which mixes together physical strength and poor social skills. Yes, this would have a large sex difference and a large autistic-allistic difference, but that'd transparently be measurement bias; the reason for the large sex difference (it measures physical strength) would be distinct from the reason for the large autistic-allistic difference (it measures social skills).\n\n\nMy concern\nMy concern is that people didn't start believing in the EQ-SQ theory based on statistical correlations found with Simon Baron-Cohen's scales. They presumably started believing in it based on fuzzy intuitions arrived at through social experience.\n\nSo I could imagine that if I did show SBC's findings to be due to measurement error, people would sort of dismiss it as",
      "wordCount": 364
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "mvQjpu7iFnqQ47Lbp",
    "title": "Autogynephilia discourse is so absurdly bad on all sides",
    "slug": "autogynephilia-discourse-is-so-absurdly-bad-on-all-sides",
    "url": null,
    "baseScore": 44,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 24,
    "createdAt": null,
    "postedAt": "2023-07-23T13:12:07.982Z",
    "contents": {
      "markdown": "In [Blanchard's Dangerous Idea and the Plight of the Lucid Crossdreamer](https://www.lesswrong.com/posts/RxxqPH3WffQv6ESxj/blanchard-s-dangerous-idea-and-the-plight-of-the-lucid), Zack criticizes the rationalist community and the trans community for various things, including denial of autogynephilia as a cause of transness:\n\n> I could go on ... but do I need to? After having seen enough of these laughable denials of autogynephilia, the main question in my mind has become less, \"Is the two-type androphilic/autogynephilic taxonomy of MtF transsexuality approximately true?\" (answer: yes, obviously) and more, \"How dumb do you (proponents of gender-identity theories) think we (the general public) are?\" (answer: very, but correctly).\n\nIn some ways this is not a completely unreasonable critique, but I think it has a lot less bite once one realizes just how dysfunctional the discourse on this subject is, even among the ideology that Zack endorses as a replacement.\n\nA microcosm of the dysfunction can be seen in Michael Bailey and Kevin Hsu's study, [How Autogynephilic Are Natal Females?](https://www.researchgate.net/publication/361576790_How_Autogynephilic_Are_Natal_Females) ([pdf](https://www.dropbox.com/s/qiujtvo24caokft/Bailey-Hsu2022_Article_HowAutogynephilicAreNatalFemal.pdf?dl=0)).[^twdt3hztwfe] They are responding to a debate where [Charles Moser](https://www.researchgate.net/publication/26660017_Autogynephilia_in_Women), [Scott Alexander](https://slatestarcodex.com/2020/02/10/autogenderphilia-is-common-and-not-especially-related-to-transgender/), [Julia Serano](https://journals.sagepub.com/doi/abs/10.1177/0038026120934690), and [Jaimie Veale](https://mro.massey.ac.nz/handle/10179/3683) argue that cis women are often autogynephilic too, and that this suggests that autogynephilia in trans women is not so significant.\n\nIn response, Michael Bailey ([the effective leader of autogynephilia theory](https://www.lesswrong.com/posts/mvQjpu7iFnqQ47Lbp/autogynephilia-discourse-is-so-absurdly-bad-on-all-sides?commentId=G6F8gLdmHBMvwjAgo)) recruited samples of highly active members of online erotic AGP communities, and found that these samples are on average much more AGP than cis women, and therefore conclude that AGP in cis women is dubious.\n\nSubstituting a sample of highly active members of online erotic AGP communities for trans women is an insanely biased methodology! And [when called out on it](https://www.researchgate.net/publication/363429868_A_Response_to_Bailey_and_Hsu_2022_It_Helps_If_You_Stop_Confusing_Gender_Dysphoria_and_Transvestism) ([pdf](https://www.dropbox.com/scl/fi/6m8g0hrwahgvtyaysag8g/MOSER.ASB.2022.pdf?rlkey=mj0pnumxb5uqzqye9kdrxadbh&dl=0)), Michael Bailey essentially goes \"[well the debate isn't about trans women, it's about AGPs, so it's totally appropriate](https://www.researchgate.net/publication/365808935_Autogynephilia_and_Science_A_Response_to_Moser_2022_and_Serano_and_Veale_2022)\" ([pdf](https://www.dropbox.com/s/jevqusbua7h2fim/Response%20to%20Moser%20et%20al.pdf?dl=0)), which again is just an insane response when very obviously the debate is about trans women! All the people Bailey is responding to were clearly talking about trans women in their texts!\n\nOne would think that people would point out these problems so the study would be appropriately rejected. I have tried to do so, and some trans activists have tried to do so, but people on the Blanchardian side have generally accepted Bailey's study and rejected making critiques.\n\nNow here's a thought experiment for you: If people cannot converge to the truth on *objectively observable facts about who has said what* that can be seen by reading the debates, how can they *ever* be expected to converge to the truth about complex psychological causal and distributional inference questions? Especially when the objectively observable facts concern what psychological causal and distributional inference questions are under dispute?\n\nEven if the specific point of AGP in cis women doesn't move you much (I don't think it should[^ifb3ztizi7]), this dysfunctional discourse might make you tempted to infer that Blanchardians do a lot of other shenanigans to make their theories look better than they really are. And I think you would be right to make that inference, because I have a lot of points of critique on my gender blog that go unaddressed.[^0gbvgm3s8x7] But my critiques aren't the core point I'm raising here, rather I'm pointing out that people have good reasons to be exhausted with autogynephilia theorists.\n\n[^twdt3hztwfe]: See Transvestism vs gender dysphoria vs … and Michael Bailey doubles down on his lies 🤣 for in-depth analysis. The latter post has direct quotes. \n\n[^ifb3ztizi7]: I am so confused about AGP in cis women that I am essentially agnostic on the question. I think it is a question that is relatively independent of other questions of interest, and I don't think we have any satisfactory measurements of the rate of AGP in cis women. My best guess is that a large fraction of cis women (maybe 80%) have something that superficially looks like AGP but isn't, and that a substantial fraction of cis women (maybe 15%) are truly AGP in some meaningful sense. (Not necessarily super strongly, but also, a lot of the evidence for the existence of AGP in trans women doesn't show trans women to be super strongly AGP.) \n\n[^0gbvgm3s8x7]: The mathematical consequences of a toy model of gender transition, Meta-attraction cannot account for all autogynephiles’ interest in men, Contra James Cantor on desistance, Autogynephilia and masochism: A tale of two assessment biases, Book Review: Autoheterosexuality, Autogynephilia vs pseudoautogynephilia?, Why most studies on autogynephilia and sexual orientation are of no evidentiary value for ETLE, also some general mentions of autoandrophilia that I don't think I've written up well anywhere",
      "plaintextDescription": "In Blanchard's Dangerous Idea and the Plight of the Lucid Crossdreamer, Zack criticizes the rationalist community and the trans community for various things, including denial of autogynephilia as a cause of transness:\n\n> I could go on ... but do I need to? After having seen enough of these laughable denials of autogynephilia, the main question in my mind has become less, \"Is the two-type androphilic/autogynephilic taxonomy of MtF transsexuality approximately true?\" (answer: yes, obviously) and more, \"How dumb do you (proponents of gender-identity theories) think we (the general public) are?\" (answer: very, but correctly).\n\nIn some ways this is not a completely unreasonable critique, but I think it has a lot less bite once one realizes just how dysfunctional the discourse on this subject is, even among the ideology that Zack endorses as a replacement.\n\nA microcosm of the dysfunction can be seen in Michael Bailey and Kevin Hsu's study, How Autogynephilic Are Natal Females? (pdf).[1] They are responding to a debate where Charles Moser, Scott Alexander, Julia Serano, and Jaimie Veale argue that cis women are often autogynephilic too, and that this suggests that autogynephilia in trans women is not so significant.\n\nIn response, Michael Bailey (the effective leader of autogynephilia theory) recruited samples of highly active members of online erotic AGP communities, and found that these samples are on average much more AGP than cis women, and therefore conclude that AGP in cis women is dubious.\n\nSubstituting a sample of highly active members of online erotic AGP communities for trans women is an insanely biased methodology! And when called out on it (pdf), Michael Bailey essentially goes \"well the debate isn't about trans women, it's about AGPs, so it's totally appropriate\" (pdf), which again is just an insane response when very obviously the debate is about trans women! All the people Bailey is responding to were clearly talking about trans women in their texts!\n\nOne wou",
      "wordCount": 529
    },
    "tags": [
      {
        "_id": "ubGeEpjJ2DCkDHiEW",
        "name": "Autosexuality",
        "slug": "autosexuality"
      },
      {
        "_id": "W9aNkPwtPhMrcfgj7",
        "name": "Sex & Gender",
        "slug": "sex-and-gender"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FDkh5xnuFyqqJcqbb",
    "title": "Boundary Placement Rebellion",
    "slug": "boundary-placement-rebellion",
    "url": null,
    "baseScore": 54,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 21,
    "createdAt": null,
    "postedAt": "2023-07-20T17:40:00.190Z",
    "contents": {
      "markdown": "The idea for this post all started because I was confused about the concept of \"narcissism\". I learned about \"narcissism\" from reading differential psychology, where they often measure it by asking people whether they agree with statements such as:\n\n*   I have a natural talent for influencing people\n*   Modesty doesn't become me\n*   I think I am a special person\n*   I really like to be the center of attention\n\nIn practice, empirically this correlates with being an assertive, confident person, which didn't match the discourse about narcissism, which typically seemed to more be about domestic abuse or people's ideologies, and it also doesn't AFAIK match the way \"narcissism\" gets used clinically, [since clinical narcissists don't score higher than average on psychometric Narcissism scales used in personality psychology](https://www.tandfonline.com/doi/abs/10.1080/00223891.2012.732636).\n\nEventually, something clicked about what people were saying about narcissism. They were talking about a dynamic that occurs when someone feels morally entitled to violate a boundary. Such a person can exhibit signs of narcissism because they become focused on enforcing their own views on the boundary, and put a lot of effort into gaining power to continue violating it.\n\nStill, since I'm not sure whether what I'm talking about is *actually* \"narcissism\", I'm going to use a new, more descriptive term to refer to my concept: Boundary Placement Rebellion.\n\nBoundary Placement Rebellion comes up a lot in my experience. It's a key issue in AI safety, as well as in civil rights, psychology research, hierarchies, families and other areas of society. For good and for bad, I think Boundary Placement Rebellion sees far more symmetry between the sides than \"narcissism\" does.\n\nAn Example: AI Safety vs Capabilities\n-------------------------------------\n\nCommon sense - at least among many right-wing techies - is that you have the right to work on whatever software projects you want. If people don't like your software, they can just not buy it. There may be exceptions when it comes to software whose purpose is to be used as a weapon, such as ransomware.[^x0ee4v7iud] If new issues are discovered, then maybe we can update this norm, but we should also be careful to not strangle the tech industry with paperwork.\n\nThis is a boundary; you get sole decisive control over what you work on. AI capabilities research (as well as everything else in tech) makes a lot of use of this boundary, as it means that they can keep trying new things to push the state of the art. And of course earn lots of $$$ doing it.\n\nNow suddenly a bunch of people are coming in, arguing that AI will lead to the end of the world! Suddenly, the boundary for AI capabilities researchers is being directly threatened. And they can't just be dismissed by saying \"don't worry, I'm not gonna destroy the world\", instead they will dump huge arguments for why it will inevitably happen. Or maybe some of them will say \"you may be right, but we cannot know for sure, so we still gotta stop you\".\n\nThe safety people's solution is that those who want to develop AI capabilities should either stop working on capabilities, or should pay the AI safety people engineer-level salaries to do philosophy, abstract mathematics, odd unprofitable ML experiments, premature safety tests, and similar.\n\nIf the AI capabilities researchers try to come up with different frames to make the AI safety people go away, then the AI safety people will just *keep on* pushing back against those frames, constantly coming up with reasons for why we're all gonna die anyway or whatever. And if you don't give them what they want, they're going to complain about you, sometimes even accusing you of being the person in the history of the world who has caused the most damage.\n\nBroadening and Abstracting\n--------------------------\n\nThe above shouldn't be seen as an argument against AI safety research. In this case, I'm actually fairly sympathetic to the AI safety side of things. So it's more an example of how Boundary Placement Rebellion isn't limited to places where you are in the wrong.\n\nStill, it can definitely also happen in cases where you are in the wrong. The closest adjacent case where I am inclined to think the 'narcissist' is in the wrong is [when rationalists derail everything to be about AI](https://thingofthings.substack.com/p/on-present-rate-no-singularity). \"You should stop buying nets to protect Africans against pest-ridden mosquitoes because it's basically useless since AI is going to destroy the world\", \"I don't need to consider arguments that I am wrong to pay people who torture animals for my pleasure because I've offset it by dedicating my life to saving the world from AI\", etc.\n\nNow let's get to the definition:\n\nBoundary Placement Rebellion concerns some boundary that is generally accepted within some social community, especially personal boundaries. It can be just about any boundary, from programming freedom to phone message privacy to respect for religion to sexual partner choice to private property ownership.\n\nIn a Boundary Placement Rebellion, the 'narcissist' feels a confident moral entitlement to violate some community's boundary, as well as an inelastic personal desire to violate it. I think it can take at least three forms:\n\n*   In the dominant case, the 'narcissist' is able to use power and manipulation to get away with violating the boundary, often in plain sight. This can leave people afraid of what the 'narcissist' might do if angered. Often, they will enforce that their particular moral frame is the only one that gets accepted and acknowledged when talking about the issue.\n*   In the submissive case, the 'narcissist' is not powerful enough to get away with it, but they might display vulnerability and appeal to someone who does have power. They might get those with power to act to enforce their views on their behalf, or they might even get everyone to support it. They can end up leaving people feeling confused or weirded out by it, as well as afraid of whichever powerful people they might appeal to.\n*   In the failed case, the 'narcissist' doesn't manage to enforce their views. However, rather than accepting that they were in the wrong, they become resentful and envious. Maybe they even escalate and become excluded from the community as a result.\n\nBoundary Placement Rebellion is a social dynamic, not a general factor\n----------------------------------------------------------------------\n\nWhenever I've researched narcissism, it has often been presented as a \"general factor\" - it has to be pervasive and driven by one's temperament, rather than an ideological disagreement limited to a specific context.\n\nThis is probably the main reason that Boundary Placement Rebellion would be distinct from narcissism. I think you should think of Boundary Placement Rebellion through a game-theoretic or ideological lens, rather than a personality lens.\n\nThat's not to say that Boundary Placement Rebellion is a brief state that automatically goes away; in order for Boundary Placement Rebellion to occur rather than just fall apart, there needs to be some persistent motivating factors, which could easily be persistent personality traits.\n\nIt's just that someone who is engaged in Boundary Placement Rebellion with respect to one boundary is not going to be particularly likely to be engaged in it with respect to some other random boundary. The exceptions would mainly be that their moral entitlements may motivate them to violate multiple boundaries at once.\n\nBoundary Placement Rebellion is probably usually bad but occasionally justified\n-------------------------------------------------------------------------------\n\nThe way the concept of \"Boundary Placement Rebellion\" clicked for me is that I had an incel-ish type of person in a survey who complained about women being shallow, and when I integrated the complaint into [pers0](https://www.onlinetests.me/test/pers0/) as the item \"If people don't want to date me, it's usually because they are shallow assholes\", a personality psychologist who was reviewing the test commented that this sentiment does not represent a personality trait, but instead represents clinical narcissism.\n\nThere's a lot of important nuances to this - many romanceless men don't become radical, and one probably shouldn't jump to conclusions just from this one statement. But it crystallized a pattern - what the psychologist had in mind was a person who does not respect other's sexual boundaries as something they should have without repercussions.\n\nAnd sexual partner choice is obviously a very important boundary to respect.\n\nSimilarly, many other boundaries are also very important, being held by the people who are best able to make decisions about them. And even when they are held by people who make terrible decisions, at least *someone* gets to make the decision, rather than there being irresolvable conflicts about them. Boundaries are a very effective way of handling disputes.\n\nStill, boundaries can be used as weapons, and they can hold back progress, and there can be many other problems with them. So sometimes they need to be changed, and often within the frames of the people who subscribe to the old boundaries, this feels like a violation.[^ywt7cfn6v78]\n\nI think the term \"Narcissism\" implies that the boundary violations in question are inherently wrong. In contrast, Boundary Placement Rebellion takes a more value-neutral approach, which I think is useful for stepping back into a more objective view, making it possible to acknowledge that the dynamic exists and thinking about what can be done about it, without having to decide whether it is good or bad.\n\nAppendix: Stories\n-----------------\n\nI prompted ChatGPT to write some stories of the dynamic for me. I have one story for the dominant variant, one for the submissive variance, one for the failed variant, and one which I think illustrates the not-actually-narcissism phenomenon that I think gets measured by psychometric scales such as the Narcissistic Personality Inventory.\n\nThese are very much optional reading. I just found them useful along the way.\n\nAlso I 100% recommend using ChatGPT for illustrating social dynamics.\n\n### Dominant Boundary Placement Rebellion\n\nIn the tight-knit neighborhood of Pleasant Ridge, there's a beautiful park where families spend their afternoons, children play on swings, and the elderly feed pigeons. It's a serene oasis amidst the urban bustle. At the center of the park is the community garden, where each neighbor is allotted a small plot to grow anything they wish. There's a deep respect for these plots among the residents. They symbolize one's labor, creativity, and personal space.\n\nThen there's Mr. Wilson, a long-time resident of Pleasant Ridge, who is known for his towering sunflowers and juicy tomatoes. His plot is always full, thriving, a spectacle for everyone to enjoy. However, Mr. Wilson has a peculiar belief that his love for gardening gives him the right to tend to other people's plots as well. His justification is simple: he is more knowledgeable, his hands more skilled, and his intentions good.\n\nThe first time he was found knee-deep in Mrs. Cooper's rosebushes, people were shocked. But he explained it away, assuring them he was merely helping, and being an influential and well-respected member of the community, his transgression was reluctantly accepted.\n\nThe episodes continued. Young Billy's pumpkin patch, Mr. Gupta's herbs, even the kindergarten's plot of wildflowers – Mr. Wilson was always there, pulling weeds, planting seeds, and dictating the layout as per his preference. People began feeling their spaces intruded upon, their creative liberty stifled. Yet, they were confused and hesitant to confront him because of his stature in the community and his relentless insistence that he was doing them a favor.\n\nDespite the tension and discontent, nobody had found the courage to confront Mr. Wilson about this explicit violation of their personal boundaries. After all, he had not only convinced himself that he was doing the right thing but had also managed to blur the lines of what was acceptable and what wasn't in everyone else's eyes.\n\nMr. Wilson’s invasion of community garden plots remained a silent concern until Mr. Harrison moved in. Mr. Harrison was an accomplished botanist who'd retired from city life to enjoy the tranquility of the small neighborhood. When he heard about the community garden, he was thrilled and immediately claimed a plot.\n\nMr. Harrison was meticulous, he planted rare, exotic flowers that he had nurtured from seeds he'd gathered during his years of exploration. They were his prized possessions. It wasn't long before Mr. Wilson's prying fingers found their way into Mr. Harrison's plot. Seeing Mr. Wilson meddle in his garden made Mr. Harrison confront him. He was polite yet firm, expressing his concern over Mr. Wilson's unnecessary interventions.\n\nMr. Wilson didn’t take kindly to this confrontation. His self-perceived righteousness made him argue, \"I've been tending these gardens long before you arrived. I know what’s best for them.\" His moral high ground was unshakeable in his eyes.\n\nHowever, Mr. Harrison did not back down. He insisted that his plot was his private property and needed no interference. It seemed like the community would finally have a voice against Mr. Wilson's overbearing practices.\n\nBut Mr. Wilson had power and influence beyond his gardening prowess. He was a founding member of the Pleasant Ridge homeowners' association and was currently serving as its president. His decades-long association with the community had given him a level of social authority that was hard to challenge. Besides, he was a philanthropist, often funding community events and contributing generously to local causes. He was also the owner of the town’s most successful business, a bakery that was an employment source for many local families.\n\nMr. Wilson used his power to his advantage. At the next homeowners' meeting, he casually mentioned the possibility of an increase in community fees to cover various maintenance costs. He cited Mr. Harrison's exotic plants, requiring additional resources, as an example. He never directly threatened Mr. Harrison, but the implications were clear.\n\nMr. Harrison, though resolute, felt the community’s unspoken pressure. No one dared voice their support for him, fearing the consequences of getting on Mr. Wilson's bad side. Seeing the tension his stand was creating, Mr. Harrison eventually backed off, deciding it wasn’t worth causing friction within the community.\n\nThis experience only bolstered Mr. Wilson's belief in his right to intervene in the community garden, and once again, everyone became silent observers, their confusion and discontent drowned in the wave of Mr. Wilson's moral entitlement and power.\n\n### Submissive Boundary Placement Rebellion\n\nIn the quaint town of Breezy Creek, there was a public library that served as the community's heart. It was where kids would come to hear stories, adults to find books, and the elderly to read newspapers. One of the volunteers at the library was Lydia, an elderly widow. She was well-loved and known for her gentle smile and kind demeanor.\n\nLydia had a fondness for a particular genre - historical fiction. She believed that these books held more than just stories; they held lessons from the past, insights into human nature, and a wisdom that modern literature often missed. Lydia felt a moral obligation to ensure that everyone got a taste of this wisdom, that they too could learn from the past.\n\nThe library had a very fair system - everyone had the chance to suggest new books for the library to buy. All suggestions would be placed in a box, and each month the librarians would pick out several of these suggestions to purchase.\n\nLydia started to manipulate this system subtly. She'd write her suggestions for historical fiction novels, but instead of placing one suggestion, she would place multiple. Lydia thought she was doing the right thing, giving people a chance to explore something more profound and meaningful.\n\nWhen her actions became noticeable and suspicious, a group of regular visitors confronted the library committee. Lydia confessed, but rather than being defensive, she explained her intentions with earnest conviction. She shared her passion for historical fiction, her belief in its transformative power, and her desire to share it with the town. She painted herself as a well-intentioned guide, pushing people towards a better path.\n\nLydia's powerful persuasion resonated with Martha, the head of the library committee. Martha, a retired history teacher, found Lydia's passion compelling. Lydia also appealed to Sam, the local mayor and a significant influencer in Breezy Creek, who admired Lydia's desire to leave a literary legacy.\n\nLydia didn't stop there. She shared her loneliness and how the library was her link to the community. Her vulnerability elicited compassion from the younger committee members like Julia, a high school student who empathized with Lydia's feeling of isolation.\n\nAlthough there were others who felt Lydia's actions were unfair, they struggled to voice their concerns strongly. Those who were sympathetic to Lydia, including Martha and Sam, unintentionally enforced Lydia's perspective. They made subtle changes like organizing historical fiction book clubs, and arranging talks on the importance of learning from history. Sam even used his mayoral newsletter to praise the library's extensive historical fiction collection, indirectly endorsing Lydia's viewpoint.\n\nMartha, in her position as the head of the library committee, found ways to downplay complaints against Lydia. She would dismiss them as misunderstanding or trivial compared to the significant \"educational benefits\" the town was reaping from Lydia's influence.\n\nWith the powerful backing of Martha and Sam, Lydia's manipulation of the system continued. Her passionate belief in the righteousness of her actions and the compassionate support from key influencers created a dynamic where Lydia's violations became an accepted, even celebrated, norm, despite the underlying discontent.\n\n### Failed Boundary Placement Rebellion\n\nIn the sprawling city of Newtopia, a clandestine artist known only as \"Kite\" operated under the veil of the night. Kite was a graffiti artist, a creator who expressed his artistry on the city's cold, gray concrete walls. For him, the city was his canvas, a vast space that needed color, emotion, and meaning.\n\nNewtopia had strict regulations against graffiti, the city council believing it defaced public property and disrupted the clean, modern aesthetic they were aiming for. But Kite saw his art differently. He believed he was bringing life to an otherwise sterile cityscape, that he was infusing the city with a soul through his vibrant murals and cryptic taglines. Kite felt that his moral obligation to art and expression gave him the right to bypass the city's norms.\n\nOne night, Kite took on a massive project – the side of a prominent, privately-owned office building. His audacious masterpiece depicted the struggle of the working class, symbolized through chained hands breaking free. The mural was breathtaking and drew public attention, but it also brought the city officials down on Kite.\n\nWhen he was eventually caught, Kite passionately defended his actions, arguing that his art was a service to the city, a mirror to society's harsh truths. However, Kite was an outsider, a rogue artist with no standing or influence in the city council or among the city's influential personalities.\n\nHe tried to gain the sympathy of the public, sharing his love for art and his belief in its transformative power. He even revealed his personal story, the son of an overworked factory worker trying to voice the unheard stories of his community. But the city's affluence and affinity for order over empathy made it hard for Kite's plea to resonate.\n\nKite's trial was swift. The city council, backed by several powerful property owners, was relentless. Kite's lack of power and social influence meant his plea fell on deaf ears. His actions were deemed vandalism, an unsanctioned violation of the city's rules, and he was fined heavily.\n\nLeft with a sense of resentment and envy towards artists who were given free reign over galleries and studios to express their creativity, Kite's defiance didn't waver. He still believed in his art, his cause, but the city's lack of understanding and acceptance stung. Despite the setback, he vowed to continue expressing his voice, his art, hoping that one day the city would see the value in his vibrant colors and powerful depictions.\n\n### Neither BPR nor Clinical Narcissism, but potentially NPI \"Narcissism\"?\n\nMara owns a popular bakery in town, known for its creative pastry designs and flavors. As she opens the shop every morning, she cheerfully announces to her team, \"Let's make some delicious magic happen!\"\n\nShe often daydreams about her bakery becoming a household name, imagining customers lining up around the block to get a taste of her pastries. She shares these dreams with her team, telling them, \"We're going to put our little town on the culinary map.\"\n\nIn her industry, Mara has made connections with successful restaurant owners and acclaimed chefs. When a famous chef visits her bakery, she's excited to talk shop and exchange baking tips and tricks.\n\nMara cherishes the reviews and compliments her bakery receives. Each positive comment about her pastries brings a proud smile to her face. \"Every compliment is a testament to our hard work,\" she says.\n\nShe is particular about the ingredients and equipment she uses, insisting on the best quality. She once spent an entire day visiting multiple suppliers to find the perfect cherries for her signature pie.\n\nSometimes, she capitalizes on her bakery's popularity. When the local newspaper asks for an interview, she uses the opportunity to highlight her upcoming pastry line.\n\nHer focused and pragmatic approach can sometimes seem indifferent. When a new employee spends too much time chatting, Mara reminds them, \"We've got customers waiting for our delicious treats.\"\n\nShe stays aware of her competition, always looking for ways to innovate. When a rival bakery introduces a new pastry, she takes it as a challenge and begins experimenting with her own recipes.\n\nHer straightforward nature can sometimes come across as abrupt. A customer once asked her why she didn't offer a certain popular pastry, to which she responded, \"Because we specialize in unique flavors. If you want common, there's a supermarket bakery down the street.\"\n\n[^x0ee4v7iud]: Even weapons programming may normally get accepted if it is done for a purpose that is considered legitimate, such as the American military. Malware is considered an unusual special-case, rather than a normal case that needs to be focused on. Though of course a lot of e.g. medical software may be regulated thoroughly, but from what I've seen programmers are often pretty bothered by those regulations. \n\n[^ywt7cfn6v78]: Note that even though it feels like a violation, it doesn't have to be illegitimate by the formal rules. A democracy may full well allow the people to vote that Silicon Valley should be put under control and not develop AGI. I think in many ways, parents have a legal right to oppress their children. The main threats available to people who engage in Cancel Culture are boycotts and criticism, which are completely legal. Boundary Placement Rebellion seems to especially happen when there is tension between norms and power.",
      "plaintextDescription": "The idea for this post all started because I was confused about the concept of \"narcissism\". I learned about \"narcissism\" from reading differential psychology, where they often measure it by asking people whether they agree with statements such as:\n\n * I have a natural talent for influencing people\n * Modesty doesn't become me\n * I think I am a special person\n * I really like to be the center of attention\n\nIn practice, empirically this correlates with being an assertive, confident person, which didn't match the discourse about narcissism, which typically seemed to more be about domestic abuse or people's ideologies, and it also doesn't AFAIK match the way \"narcissism\" gets used clinically, since clinical narcissists don't score higher than average on psychometric Narcissism scales used in personality psychology.\n\nEventually, something clicked about what people were saying about narcissism. They were talking about a dynamic that occurs when someone feels morally entitled to violate a boundary. Such a person can exhibit signs of narcissism because they become focused on enforcing their own views on the boundary, and put a lot of effort into gaining power to continue violating it.\n\nStill, since I'm not sure whether what I'm talking about is actually \"narcissism\", I'm going to use a new, more descriptive term to refer to my concept: Boundary Placement Rebellion.\n\nBoundary Placement Rebellion comes up a lot in my experience. It's a key issue in AI safety, as well as in civil rights, psychology research, hierarchies, families and other areas of society. For good and for bad, I think Boundary Placement Rebellion sees far more symmetry between the sides than \"narcissism\" does.\n\n\nAn Example: AI Safety vs Capabilities\nCommon sense - at least among many right-wing techies - is that you have the right to work on whatever software projects you want. If people don't like your software, they can just not buy it. There may be exceptions when it comes to software whose purpose is to",
      "wordCount": 3606
    },
    "tags": [
      {
        "_id": "dBPou4ihoQNY4cquv",
        "name": "Psychology",
        "slug": "psychology"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "jLdGLomCAdHayrDJA",
    "title": "Prospera-dump",
    "slug": "prospera-dump",
    "url": null,
    "baseScore": 11,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 16,
    "createdAt": null,
    "postedAt": "2023-07-18T21:36:13.822Z",
    "contents": {
      "markdown": "Making this post as a place to dump various things I learn while researching Prospera. Feel free to discuss or dump other things that you find interesting.\n\nEpistemic status: extremely low confidence. I'm mostly just taking info from Wikipedia, which might be a biased source.",
      "plaintextDescription": "Making this post as a place to dump various things I learn while researching Prospera. Feel free to discuss or dump other things that you find interesting.\n\nEpistemic status: extremely low confidence. I'm mostly just taking info from Wikipedia, which might be a biased source.",
      "wordCount": 45
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "EevoaoqBjQb3bryW4",
    "title": "Are there any good, easy-to-understand examples of cases where statistical causal network discovery worked well in practice?",
    "slug": "are-there-any-good-easy-to-understand-examples-of-cases",
    "url": null,
    "baseScore": 42,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2023-07-12T22:08:59.916Z",
    "contents": {
      "markdown": "When I first read the Sequences, one of the exciting posts was [Causal Diagrams and Causal Models](https://www.lesswrong.com/posts/hzuSDMx7pd2uxFc5w/causal-diagrams-and-causal-models), which got me into the idea that one could discover the structure of causal networks using statistics. Another rationalist source which gave me similar hopes was Scott Alexander's [SSC Journal Club: Mental Disorders As Networks](https://slatestarcodex.com/2016/12/14/ssc-journal-club-mental-disorders-as-networks/).\n\nHowever, when I actually started applying these techniques to my own data, or to publicly available datasets, I often found that the techniques were unstable, and that one could easily infer plausible conditions where they would give the wrong results. It's possible I had the wrong approach or something, but in my confusion I started reading up on what experts in causal inference had said, and I got the impression that they studied the problem for a while, initially finding some algorithms, but over time concluding that their algorithms didn't work very well and that it is better to just have a human in the loop who specifies the causal networks.\n\nSo I mostly abandoned it, or saw it as a much more limited tool than I had before. But recently, John Wentworth [argued that it was actually quite feasible in practice](https://www.lesswrong.com/posts/qPrPNakJBq23muf4n/bayesian-networks-aren-t-necessarily-causal?commentId=ZuqLAJkfqqLFMoNdk), so maybe I was too quick to abandon it. I would like to know - what are the best examples of this working well in practice? Or alternatively, did anyone else come to the same conclusions as I did?",
      "plaintextDescription": "When I first read the Sequences, one of the exciting posts was Causal Diagrams and Causal Models, which got me into the idea that one could discover the structure of causal networks using statistics. Another rationalist source which gave me similar hopes was Scott Alexander's SSC Journal Club: Mental Disorders As Networks.\n\nHowever, when I actually started applying these techniques to my own data, or to publicly available datasets, I often found that the techniques were unstable, and that one could easily infer plausible conditions where they would give the wrong results. It's possible I had the wrong approach or something, but in my confusion I started reading up on what experts in causal inference had said, and I got the impression that they studied the problem for a while, initially finding some algorithms, but over time concluding that their algorithms didn't work very well and that it is better to just have a human in the loop who specifies the causal networks.\n\nSo I mostly abandoned it, or saw it as a much more limited tool than I had before. But recently, John Wentworth argued that it was actually quite feasible in practice, so maybe I was too quick to abandon it. I would like to know - what are the best examples of this working well in practice? Or alternatively, did anyone else come to the same conclusions as I did?",
      "wordCount": 233
    },
    "tags": [
      {
        "_id": "cq69M9ceLNA35ShTR",
        "name": "Causality",
        "slug": "causality"
      },
      {
        "_id": "bh7uxTTqmsQ8jZJdB",
        "name": "Probability & Statistics",
        "slug": "probability-and-statistics"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oHn8yvzn5uGvPYmsb",
    "title": "I think Michael Bailey's dismissal of my autogynephilia questions for Scott Alexander and Aella makes very little sense",
    "slug": "i-think-michael-bailey-s-dismissal-of-my-autogynephilia",
    "url": null,
    "baseScore": 46,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 45,
    "createdAt": null,
    "postedAt": "2023-07-10T17:39:26.325Z",
    "contents": {
      "markdown": "I am autogynephilic, and [there's been a lot of autogynephilia talk lately.](https://www.lesswrong.com/posts/RxxqPH3WffQv6ESxj/) One subject that sometimes comes up, but hasn't been discussed much on LessWrong, is how common autogynephilia is in cis women.\n\nTwo datasets that are sometimes used for this question are [Scott Alexander's](https://slatestarcodex.com/2020/02/10/autogenderphilia-is-common-and-not-especially-related-to-transgender/) and [Aella's](https://aella.substack.com/p/everyone-has-autogynephilia). Part of how they happened to be made is that I reached out to Scott and Aella, suggesting experimental questions for assessing autogynephilia, in ways that might function in cis women too and not just cis men[^skuo99bmue]:\n\n*   Picture a very beautiful woman. How sexually arousing would you find it to imagine *being* her?\n*   Do you find the thought of masturbating alone as a woman to be erotic?\n\nThis is somewhat different from usual autogynephilia measures, which contain questions such as:\n\n*   Did you ever feel sexually aroused when putting on females' underwear or clothing?\n*   Have you ever become sexually aroused while picturing your nude female breasts?\n\nThe reason I didn't include these latter questions is because it seems likely to me that they will be interpreted differently for males and females (e.g. if males do not have female anatomy, then they cannot be aroused by it literally, so instead they get aroused by imagining some other female anatomy that they don't actually have), and because these usual questions seem very bad if taken literally (\"*ever*\" and \"*while*\" rather than \"*how frequently*\" and \"*by*\" seem like there could be a lot of ways to get affirmative answers while not actually being autogynephilic - though the low rate of endorsement among women suggests to me that they are not taking it literally?).\n\nAnyway, [Michael Bailey (activist researcher for autogynephilia ideology) responds in Aporia Magazine](https://twitter.com/AporiaMagazine/status/1678339832798490624), saying \"It's important not to confound y'know being seen as a woman or having a female body from having sex with a partner while having a female body or the prospect of going out on a date while wearing sexy clothes\".\n\nI don't think this makes any sense. \"Do you find the thought of masturbating alone as a woman to be erotic?\" explicitly states that one doesn't have a partner around. \"Picture a very beautiful woman. How sexually arousing would you find it to imagine *being* her?\" doesn't suggest a partner any more than \"Did you ever feel sexually aroused when putting on females' underwear or clothing?\" or \"Have you ever become sexually aroused while picturing your nude female breasts?\" does. This seems like a completely unfounded critique.\n\nI think people who dismiss these questions should admit that they are starting with [The Bottom Line](https://www.lesswrong.com/posts/34XxbRFe54FycoCDw/the-bottom-line) that cis women are not autogynephilic, and reasoning backwards about what sorts of measures they do or do not want to endorse, rather than pretending that they are driven by evidence and desire for good measurement.\n\n[^skuo99bmue]: Since nobody really knows how to assess this.",
      "plaintextDescription": "I am autogynephilic, and there's been a lot of autogynephilia talk lately. One subject that sometimes comes up, but hasn't been discussed much on LessWrong, is how common autogynephilia is in cis women.\n\nTwo datasets that are sometimes used for this question are Scott Alexander's and Aella's. Part of how they happened to be made is that I reached out to Scott and Aella, suggesting experimental questions for assessing autogynephilia, in ways that might function in cis women too and not just cis men[1]:\n\n * Picture a very beautiful woman. How sexually arousing would you find it to imagine being her?\n * Do you find the thought of masturbating alone as a woman to be erotic?\n\nThis is somewhat different from usual autogynephilia measures, which contain questions such as:\n\n * Did you ever feel sexually aroused when putting on females' underwear or clothing?\n * Have you ever become sexually aroused while picturing your nude female breasts?\n\nThe reason I didn't include these latter questions is because it seems likely to me that they will be interpreted differently for males and females (e.g. if males do not have female anatomy, then they cannot be aroused by it literally, so instead they get aroused by imagining some other female anatomy that they don't actually have), and because these usual questions seem very bad if taken literally (\"ever\" and \"while\" rather than \"how frequently\" and \"by\" seem like there could be a lot of ways to get affirmative answers while not actually being autogynephilic - though the low rate of endorsement among women suggests to me that they are not taking it literally?).\n\nAnyway, Michael Bailey (activist researcher for autogynephilia ideology) responds in Aporia Magazine, saying \"It's important not to confound y'know being seen as a woman or having a female body from having sex with a partner while having a female body or the prospect of going out on a date while wearing sexy clothes\".\n\nI don't think this makes any sense. \"Do you find the thought",
      "wordCount": 468
    },
    "tags": [
      {
        "_id": "ubGeEpjJ2DCkDHiEW",
        "name": "Autosexuality",
        "slug": "autosexuality"
      },
      {
        "_id": "W9aNkPwtPhMrcfgj7",
        "name": "Sex & Gender",
        "slug": "sex-and-gender"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "GzykqeAg3tuKtg8aD",
    "title": "What in your opinion is the biggest open problem in AI alignment?",
    "slug": "what-in-your-opinion-is-the-biggest-open-problem-in-ai",
    "url": null,
    "baseScore": 39,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2023-07-03T16:34:09.698Z",
    "contents": {
      "markdown": "",
      "plaintextDescription": "",
      "wordCount": 0
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ftPQugx2726zAL2Ff",
    "title": "Which personality traits are real? Stress-testing the lexical hypothesis",
    "slug": "which-personality-traits-are-real-stress-testing-the-lexical",
    "url": null,
    "baseScore": 65,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-06-21T19:46:03.164Z",
    "contents": {
      "markdown": "*This post is also available* [*on my Substack*](https://tailcalled.substack.com/p/which-personality-traits-are-real)*. Thanks to Justis Mills for proofreading and feedback!*\n\nMost scientific personality models are, directly or indirectly[^wqadwofgx4], based on the lexical hypothesis, which roughly speaking states that there is a correspondence between important personality traits and abstract behavior-descriptive adjectives. For example, the [Big Five](https://en.wikipedia.org/wiki/Big_Five_personality_traits) was created by having people rate themselves using words like \"outgoing\", \"hard-working\" and \"kind\", and finding patterns in these. It is neat that one can create models in this way, but the large amount of abstraction involved by using abstract adjectives raises huge questions about how \"real\" the personality traits are.\n\nI have created a new personality test, currently named [Targeted Personality Test](https://www.onlinetests.me/test/pers0/). I have multiple goals with this test, but one of them is to investigate which personality traits are “real”[^vnxnuzkuj7] without relying on the lexical hypothesis. I do this mainly by assessing lots of specific narrow behaviors, rather than abstract vague adjectives.[^7o1ttjqi0es]\n\nBy the end of this blog post, I hope to have introduced some concepts that makes my approach make sense, and thereby enable you to understand this diagram I made summarizing my results:\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0c90a-7c1b-41b9-aa71-c373d93687e8_781x693.png)\n\nThe semi-formal understanding of what is going on in this chart is very long, so before we proceed, let me give a brief, vague indication of what you will be informed about:\n\n*   Trait impact: a measure of how strongly the personality trait influences the various behaviors and thoughts that we would expect it to.\n*   Factor model loss: a measure of how much the personality trait conflates different unrelated things together.\n*   Correlation with lexical notion: a measure of how well-labelled the personality trait is. (You can mostly ignore this variable as all of the personality traits performed reasonably well on it.)\n\nEasy-Goingness: An example\n--------------------------\n\nA conventional personality test such as the [SPI-81-27&5](https://www.sapa-project.org/research/SPI/SPIforms/SPI_81_27_&_5_v1_1.pdf) might measure your personality traits such as Easy-Goingness by asking you how well a few abstract statements describe you, e.g.:\n\n*   I like to take it easy\n*   I like a leisurely lifestyle\n*   I have a slow pace to my life\n\nIt seems plausible that someone who agrees that such statements describe them would be Easy-Going in some sense, and indeed I bet this sort of measure can pass all sorts of criteria used by psychologists to evaluate the quality of the test.[^wrjat7pzbi]\n\nSo if it is probably valid by the standard criteria, what could go wrong that the standard criteria don’t test for?\n\nWell, let’s imagine the sort of person who is Easy-Going. They probably tend to relax in their free time, e.g. watching TV, and they probably don’t get worked up about controversial stuff, and they probably don’t go above and beyond at work. Basically, a relaxed person who doesn’t get too stressed or excited about things.\n\nWhen we call the person above “Easy-Going”, is this just a convenient label we use for someone who happens to have a constellation of traits like the above? Or are we saying that there is some underlying factor, like a motive to take it easy, which causes them to have these sorts of characteristics? Or maybe there are some underlying factors, but they are heterogeneous and “Easy-Goingness” lumps them together? These are the sorts of questions I tried to investigate.\n\nMy first step was to come up with a more concrete characterization of Easy-Goingness than abstract statements like “I like to take it easy” or “I have a slow pace to my life”. I did this by giving the SPI-81-27&5 test to a bunch of people, and then asking the people who score high and low in Easy-Goingness to describe an example of how they could be said to be Easy-Going. To give you a taste for the answers, one person who scored high in Easy-Goingness said:\n\n> When I finish work for the day I often go straight home and jump into my pyjamas. I like to relax and watch some tv and films to unwind after a long day - usually with a glass of wine. Certain days when I come home my partner would like to travel for a couple hours to go dog walking and enjoying time outside. No matter what kind of day I have at work I am always keen to do anything my partner/family/friends would like to do as is in my nature.\n\nMeanwhile, a person who scored low in Easy-Goingness said:\n\n> A simple example is that when I arrive at work, my boss often asks at once if I want a coffee, as he often wants one at the beginning of the day. I prefer to do some work before having a coffee, as to me it signifies a moment of relaxation and to the puritan work ethic part of me, it doesn't make sense to have a break until I have \"earned\" it.\n\nBased on 10 descriptions like this, I constructed some statements that would be reflective of Easy-Going people (+), or non-Easy-Going people (-):\n\n*   (+) In the evening I tend to relax and watch some videos/TV\n*   (+) I don’t feel the need to arrange any elaborate events to go to in my free time\n*   (+) I think it is best to take it easy about exams and interviews, rather than worrying a bunch about doing it right\n*   (+) I think you’ve got to have low expectations of others, as otherwise they will let you down\n*   (-) I get angry about politics\n*   (-) I have a stressful job\n*   (-) I don’t feel like I should have breaks at work unless I’ve “earned” them by finishing something productive\n*   (-) I spent a lot of effort on parenting\n\nI included these statements in the [Targeted Personality Test](https://www.onlinetests.me/test/pers0/), as well as similar statements I designed for the 26 other personality traits from the test that I based this study on, and some additional statements that were useful for research purposes.\n\nThese statements are quite unlike the typical statements used in personality tests, because they are intentionally aiming to be much more narrow and concrete. This probably makes them less “efficient” in the sense that respondents will have to answer a lot more questions before we can get a detailed view of what they are like.\n\nHowever, by being so concrete and narrow, it also allows us to more strongly test how real the traits are, e.g. whether it is just a coincidence that they sometimes co-occur to lead to “Easy-Going” people, and whether the statements conflate multiple unrelated traits together.\n\nTrait Impact as a measure of realness\n-------------------------------------\n\nI have multiple measures of realness, but I think the most important measure of whether a personality trait is real is whether the associated behaviors do in fact correlate with each other, rather than them just sometimes coincidentally occurring together.\n\nThe simplest way to visualize whether this applies is with a correlation matrix, which is a diagram that shows how strongly a set of variables correlate with each other:\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa274f450-55f1-40e7-a211-51f7724c148b_1499x476.png)\n\nHere, each row and column represents a personality statement that I asked people to rate themselves on, and each cell represents the [Pearson Correlation](https://rpsychologist.com/correlation/) between the row variable and the column variable. While the exact correlations varied, overall the different concrete behaviors associated with Easy-Goingness had a correlation of about 0.06 with each other. If you are not familiar with Pearson Correlations, then here is a visualization of how weak 0.06 is:\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F412d9602-e1e1-4a99-868c-612a7633ea3d_671x687.png)\n\n*Hypothetical simulated distribution for r=0.06*\n\nThis suggests to me that Easy-Goingness is not very “real”. While it might make sense to describe a person as doing something Easy-Going, for instance when they are watching TV, it is kind of arbitrary to talk about people as being more or less Easy-Going, because it depends a lot on context/what you mean.\n\nIf we take the square root of the extent to which two behaviors associated with the trait correlate with each other, we get the extent to which the behaviors correlate with the overall level of the trait. Doing this for Easy-Goingness, we get an effect of around 0.25. This is a somewhat stronger connection, but still quite weak:\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93dca5f9-ed9c-4d95-9bd8-7d60b0bab771_671x666.png)\n\n*Hypothetical simulated distribution for r=0.25*\n\nThe fact that this is weak means that even the most Easy-Going people cannot necessarily be expected to be particularly Easy-Going in all contexts. It is much more subtle than that.\n\nThe “Trait impact” axis in my original diagram in the start of the post shows this correlation for all of the different traits.\n\nI picked Easy-Goingness as an example because it had the lowest “Trait impact”. It may also be informative to look at an example with a high “Trait impact”. The highest “Trait impact” was Art Appreciation, but it feels too narrow, so I am going to skip over it[^c26snpv0aqv] and consider Conservatism as an example of a trait with a high “Trait impact”.\n\nThe correlation matrix for Conservatism looks like the following:\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ac8728e-dfde-4612-91eb-4bb74ae2a3f6_1358x598.png)\n\nAs you can see and probably expected, there are strong correlations between different Conservative/Progressive responses. Visualized as a scatterplot, it might look like this:\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558ed3f-e923-4047-ac4f-6cbfbc3a98f7_686x672.png)\n\n*Hypothetical simulated distribution for r=0.56*\n\nOf course this is still far from deterministic, but now it looks like we’ve got something fairly strong. Ideology seems more “real” than Easy-Goingness, in the sense measured by “Trait impact”.\n\nFactor model loss as a measure of conflation\n--------------------------------------------\n\nOne of my other measures of personality trait realness was “Factor model loss”. What does that mean? Let’s take one of the traits that scored the worst in “Factor model loss”: Creativity.\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F274a0750-0dcc-4c3a-b9fe-ef5f35a850eb_1344x449.png)\n\nIf we look carefully, we can see that there are two distinct groups of items:\n\n*   Creative problem-solving: finding root causes for problems at work, copying old methods instead of coming up with new ones at work, being good at ideas during brainstorming\n*   Artistic creativity: creating decorations, quizzes/games/adventures/trips, being imaginative\n\nTwo of the items, involving creating visualizations and coming up with fictional stories, correlated with Creative problem-solving and Artistic creativity. Meanwhile math vs humanities didn’t really correlate with either.\n\nThus, it seems that the term “Creativity” is problematic as a personality trait, because it conflates Creative problem-solving with Artistic creativity, treating them as being the same thing when really they are basically unrelated.\n\nTo quantify the extent of the problem, I approximated what the correlation matrix would have to look like if there was no absolutely no conflation problem and there was only a single trait of Creativity which covered both Creative problem-solving and Artistic creativity. I got this result:\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c40d782-c4db-47a7-adbe-0ffb483c7438_1051x326.png)\n\n“Factor model loss” refers to the size of the difference between these two correlation matrices: the observed correlations versus the hypothetical correlations if there only was a single trait.\n\nCorrelation with lexical notion: naming things\n----------------------------------------------\n\nThe final notion of “realness” in my diagram was “Correlation with lexical notion”. What does that mean? Well, remember how I keep separating things into “concrete” and “abstract” descriptors?\n\nI think of the “abstract” descriptors as being a measure of the informal common-sense version of the trait. You are probably easy-going if you think you are easy-going, conservative if you think you are conservative, and creative if you think you are creative. It may be a matter of definition to strictly know whether you fit, but it certainly seems like a good starting point.\n\nBut the fact that we can measure the common-sense notion of the trait separately from the behaviors associated with the trait raises the question: Do these measure the same thing? For instance, maybe there was a flaw in the way we collected examples of behaviors, so that they don’t correspond to what the trait is actually like.\n\nI quantified this with “Correlation with lexical notion”. It is based on[^1z08d8mb2xf] the correlations between the abstract and the concrete questions.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8656b774-ce53-4607-8fa8-c5778b1c9f54_1199x588.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8656b774-ce53-4607-8fa8-c5778b1c9f54_1199x588.png)\n\nHowever, it turns out that there is not much more to say about this, because all of the traits did great with respect to this; the “Correlation with lexical notion” was consistently close to 1, showing that the concrete and the abstract descriptors were getting at the same thing. (And when I inspected the ones who did the worst, it often seemed to be because of a technical form of noise that I am not going to get into.)\n\nSummary\n-------\n\nI have three different measures of the realness of a personality trait:\n\n*   Trait impact: how strongly the personality trait influences the various behaviors and thoughts that we would expect it to.\n*   Factor model loss: how much the personality trait conflates different unrelated things together.\n*   Correlation with lexical notion: how well-labelled the personality trait is.\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0c90a-7c1b-41b9-aa71-c373d93687e8_781x693.png)\n\nSince the correlation with the lexical notion is consistently high, it appears that the personality traits have been assigned reasonably descriptive labels; however, some of the labels conflate multiple personality traits, such as:\n\n*   Creativity (appears to conflate Creative problem-solving and Artistic creativity)\n*   Charisma (appears to conflate Interpersonal Sensitivity and Social Ease)\n*   Emotional Stability (appears to conflate Problem-Handling Confidence and (opposite of) Catastrophizing)\n*   Conformity (appears to conflate Government Conformity, Aesthetic Conformity, and Religiosity)\n*   Authoritarianism (appears to conflate Political Authoritarianism and Law Adherence)\n*   Attention-Seeking (appears to conflate Benign Narcissism and (opposite of) Shyness)\n\nTo see more about what is getting conflated, skip to the appendix, where I show the correlation matrices for each of the traits.\n\nBut most importantly, a lot of traits are not that impactful. Examples of non-impactful traits include Easy-Goingness, Conformity, Irritability, Perfectionism, Sensation-Seeking, Trust, Compassion and Impulsivity. While people to some extent exhibited general, context-independent differences from each other in these traits, the differences were small relative to the context-dependent differences. So rather than seeing people as e.g. trusting or non-trusting *in general*, it may be much more productive to ask who they trust and who they don’t trust.\n\nOne thing I should warn about is, I think the trait impact for Orderliness could be overestimated, because I think in practice participants interpreted half of the questions as being about how *tidy* they kept *their own home*, which might be much narrower than general Orderliness.\n\nBonus: Going beyond the lexical hypothesis\n------------------------------------------\n\nMany of the 27 traits in the original test turned out to be problematic for my purposes.[^s410rfuoj68] You might think this shows my test to be irreparably flawed, but actually I had sort of hoped this would happen.\n\nIt is possible to use a statistical technique called factor analysis to identify patterns of correlations in empirical data. This is the technique that was used to create the original SPI test that I based my test on, and it is also the technique that has been used for many other psychometric tests.\n\nUsing factor analysis, I reshuffled the items from my test into 7 alternate factors, and made a version of the test that is less than a third of the length of the original one. In the future, I will likely write an in-depth description of how factor analysis works and which factors I have found.\n\nThis is almost certainly not the final form of the test. I have many plans for additional investigations I can perform as I get more data.\n\nAppendix: Correlation matrices for all of the traits\n----------------------------------------------------\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcad0b340-c6f1-4840-9c68-9d4934b41069_1195x347.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa529b4ae-3544-4c8e-9d61-a695b7e3501c_1168x358.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ec609d2-2599-4866-9c75-c5c1ba75896a_1003x382.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5532e69-0197-4303-9fc9-cb68d7ac368a_1104x364.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92e564c3-5cfd-4cde-bac8-62e30803a47d_1007x445.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3058545-a77b-459a-a431-971c8ece03c3_1135x367.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ebc8c7c-cb37-41e4-abb5-eb04791ec663_1235x351.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe42e6cb1-c166-4bc9-9886-14914886d129_1229x412.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2d06f32-3cd1-46fd-ae0c-b6a974c079c0_935x358.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd602d262-1af0-4c56-bd93-c1d5cd49dddc_1109x387.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5774c572-6ec9-44ca-9fb0-cc859ef89bf0_1307x359.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34b1e74c-a520-4801-8c94-9f567f42298d_1401x396.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80c50fef-fe4a-475a-abb9-fd634b6ea18b_889x358.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe487791a-73d9-42d6-9a06-5d1b95f27349_1086x407.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18c4bf12-85de-4cd3-a901-1908174dec47_1336x377.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facd42474-11b5-4899-a238-8f3f6621826f_1326x281.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5094dc41-6262-446c-b96c-7f7f95a189ce_905x315.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7282f5f5-4157-458b-b14f-48e195f8afaa_1332x446.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe7f1150-0f5f-43df-8252-0dea8de0bd07_972x372.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F793b1d3b-8ded-4e73-96f3-ff0a0b3056b5_1155x362.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7b10e13-ec45-46cc-b2b7-42cf8113ef72_1102x395.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ff07b5e-bbc3-4727-b9ad-b64b190559a2_1162x370.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c49cf-1519-4605-a424-8916f5ac8f06_1163x375.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9a7a1b1-be81-4d5d-88fe-06b64dee27cd_1029x347.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c667962-34e9-4215-9f84-c2897c887e97_1225x369.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb502095f-a39c-46d7-947a-5437206abf4b_1173x406.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132967ca-213b-41c1-bf80-b7ece88782c0_1406x383.png)\n\n![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70f34962-7cfd-4d6d-b8f0-b034ab52046e_970x408.png)\n\n[^wqadwofgx4]: The Big Five personality factors were originally derived by asking people to rate themselves on a large number of personality adjectives, and using statistics to find the biggest clusters of related descriptors. Other tests have been developed through other methods, many of which don’t primarily focus on abstract adjectives, though for reasons I won’t get into right now, I think they have a lot of dependence on the lexical hypothesis. \n\n[^vnxnuzkuj7]: Of course, this is a subtle, complex question which depends on what exactly one means by “real”. I define the notion of “realness” I focus on later in the post, but other notions may be relevant for other purposes. \n\n[^7o1ttjqi0es]: Because it is inherently difficult to measure behavior, I had to still rely on self-report surveys. \n\n[^wrjat7pzbi]: These are internal reliability, i.e. the sort of person who says “I like a leisurely lifestyle” is also more likely to say “I have a slow pace to my life”; test-retest reliability, i.e. the sort of person who says “I like a leisurely lifestyle” today will also tend to do so tomorrow, in a month, in a year, or in a decade; inter-rater validity, i.e. if a person says “I like a leisurely lifestyle” then their friends and family will also tend to say “They like a leisurely lifestyle”; criterion validity, i.e. the sort of person who says “I like a leisurely lifestyle” scores higher on some objective criterion of leisurely lifestyle such as amount of vacation days; and maybe also heritability, i.e. if one twin in a pair says “I like a leisurely lifestyle” then the other twin likely also says so too. \n\n[^c26snpv0aqv]: The narrower of a trait you are considering, the stronger the associated correlations would be. To see this, consider the absurd example where you are only considering a specific behavior, say watching TV. Any trait has a correlation of 1 with itself, so watching TV would have a Trait Impact of 1. It is only by abstracting over multiple different behaviors that Trait Impact can be nontrivial. \n\n[^1z08d8mb2xf]: Since the different questions don’t correlate perfectly internally, e.g. “I enjoy cooking food for other people” and “I like to dance with people at parties” only correlate at 0.24, we can’t exactly expect abstract “sociability” to correlate perfectly with either. So I adjust for the reduction in correlation that would be expected from imperfect internal correlations. \n\n[^s410rfuoj68]: Not necessarily for all purposes. Just because a trait is weak by my measures does not mean it cannot be relevant by other measures. Talk with personality researchers and read their papers if you want to find out what criteria they care about.",
      "plaintextDescription": "This post is also available on my Substack. Thanks to Justis Mills for proofreading and feedback!\n\nMost scientific personality models are, directly or indirectly[1], based on the lexical hypothesis, which roughly speaking states that there is a correspondence between important personality traits and abstract behavior-descriptive adjectives. For example, the Big Five was created by having people rate themselves using words like \"outgoing\", \"hard-working\" and \"kind\", and finding patterns in these. It is neat that one can create models in this way, but the large amount of abstraction involved by using abstract adjectives raises huge questions about how \"real\" the personality traits are.\n\nI have created a new personality test, currently named Targeted Personality Test. I have multiple goals with this test, but one of them is to investigate which personality traits are “real”[2] without relying on the lexical hypothesis. I do this mainly by assessing lots of specific narrow behaviors, rather than abstract vague adjectives.[3]\n\nBy the end of this blog post, I hope to have introduced some concepts that makes my approach make sense, and thereby enable you to understand this diagram I made summarizing my results:\n\n\n\nThe semi-formal understanding of what is going on in this chart is very long, so before we proceed, let me give a brief, vague indication of what you will be informed about:\n\n * Trait impact: a measure of how strongly the personality trait influences the various behaviors and thoughts that we would expect it to.\n * Factor model loss: a measure of how much the personality trait conflates different unrelated things together.\n * Correlation with lexical notion: a measure of how well-labelled the personality trait is. (You can mostly ignore this variable as all of the personality traits performed reasonably well on it.)\n\n\nEasy-Goingness: An example\nA conventional personality test such as the SPI-81-27&5 might measure your personality traits such as Easy-Goingness by ",
      "wordCount": 2561
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nqkFcjP6wtQrQMiN2",
    "title": "Book Review: Autoheterosexuality",
    "slug": "book-review-autoheterosexuality",
    "url": null,
    "baseScore": 27,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2023-06-12T20:11:38.215Z",
    "contents": {
      "markdown": "*This review was originally written for the *[*Astral Codex Ten Book Review Contest*](https://astralcodexten.substack.com/p/book-review-contest-rules-2023)*. Unfortunately it didn’t make it as one of the finalists. but since I made use of the* [*LessWrong  proofreading/feedback service*](https://www.lesswrong.com/posts/nsCwdYJEpmW5Hw5Xm/lesswrong-is-providing-feedback-and-proofreading-on-drafts)*, I am reposting it here. It can also be found on* [*my gender blog*](https://surveyanon.wordpress.com/2023/06/12/book-review-autoheterosexuality/)*.*\n\nIf I ask ChatGPT to explain transgender people to me, then it often retreats into vague discussions of gender identity. It is very hard to get it to explain what these things mean, in terms of actual experiences people might have. And that might not be a coincidence - the concepts used to understand transness seem to be the result of a complicated political negotiation, at least as much as they are optimized to communicate people’s experiences.\n\nSome people claim to do better, using elaborate words like autogynephilia and gynandromorphophilia and so on. They speak of typologies, science and history. Of developmental pathways and genetics. This ideology is sometimes called Blanchardianism, named after Ray Blanchard, a sex researcher who contributed many influential ideas within the sphere.\n\nPhil Illy’s book, [Autoheterosexual: Attracted to Being the Other Sex](https://autohetbook.com/) is the latest book pushing Blanchardianism. But is it any good?\n\n**Summary of the book on autogynephilia**\n-----------------------------------------\n\nBoth [Phil](https://twitter.com/autogynephilic) and I are probably most familiar with the concept of autogynephilia, so let’s start there. A substantial part of the book is about the feelings and experiences of autogynephiles.\n\nSome individuals, born male, feel a deep happiness when imagining themselves as a woman and a deep sadness being a man, and this is often due to autogynephilia. Phil Illy explains that autogynephilia can be seen as an inverted form of sexual attraction to women, where the individual's attraction to women is applied to themselves. Autogynephilia is ultimately about sexuality, but Phil emphasizes that many of the feelings and experiences he describes are not driven by lust, but instead by strong emotional attachments. In some cases it can cause them to have medical treatment and make social changes to live as women.\n\nPhil suggests that this accounts for most trans women in the US. Some people with strong autogynephilia might feel as if they have female body parts, like breasts or a vagina, even if they haven't undergone any medical procedures to obtain these features. The book also looks at how autogynephiles might act more like women in the way they talk and move, and how they might want to prefer spending their time around women rather than men.\n\n[Phil Illy](https://phililly.substack.com/) talks about how autogynephiles enjoy the feeling and look of women's clothes. Wearing these clothes can sometimes replace friendships and socializing for them. Autogynephilia can also make someone feel like they have a period, even though they don't (because they want to have a female body). The book also talks about autogynephiles stealing clothes from female family members.\n\nThe author looks at many different parts of autogynephilia to help readers understand it better, including the sexual side. He explains that autogynephiles have sexual fantasies about becoming a woman through magic or science fiction, or about being embarrassed by wearing women's clothes and being sexually submissive to men. They might also have fantasies about having sex with dominant women who make them submit. During sex, they may wear women's clothes or special devices called chastity cages. These sexual parts of autogynephilia help us understand the whole experience and desires of people with this orientation.\n\nAs mentioned before, Phil claims autogynephilia accounts for most trans women in the US. But most trans women, and the trans community in general, disagree with this assessment, and claim that trans women don’t transition because of autogynephilia. So why does he believe that? He bases this on multiple different sources.\n\nFirst, he draws heavily on some of the very earliest collections of case studies we have on transfeminine identity. He says that the transvestites in these studies are unlikely to face the political pressures that most modern trans women face, and therefore more likely to be honest about their autogynephilia. These include texts from the books Richard von Krafft-Ebing’s Psychopathia Sexualis, Havelock Ellis’ Studies In The Psychology Of Sex ([Eonism](https://archive.org/details/b30010172/page/n11/mode/2up)), and Hirchsfeld’s [Transvestites](https://www.amazon.com/Transvestites-New-Concepts-Human-Sexuality/dp/0879756659). One case he draws a lot on is the case he calls the [Hungarian physician (case 129)](https://archive.org/details/in.ernet.dli.2015.200360/page/n323/mode/2up), which I would summarize as follows:\n\n> He was born into a family with a history of mental illnesses and, as a child, was small, pretty, introverted, and intelligent. He shared a close bond with his mother and was treated like a girl by her sisters.\n> \n> As he grew older, the Hungarian physician started to feel ashamed of and resistant to being treated like a girl. He developed an obsession with soldiers and was often bullied for his feminine appearance. Secretly, he harbored a strong fascination with women's gloves, often trying them on in private. His skin became very sensitive as he matured, causing irritation from his pants.\n> \n> In adulthood, the physician grew a beard, which gave him a more masculine appearance and lessened his concerns about bullying. Although he was sexually attracted to women, he also desired to wear women's clothes. Women enjoyed his company, but he tried to avoid them to prevent appearing feminine. Once, he confided in a friend about his wish to be a woman but couldn't explain why. As was common during that time, he felt ashamed about masturbating.\n> \n> During his first sexual encounter with a woman, he wished he could be a woman too. He eventually engaged in heavy drinking, dueling, and attempted suicide twice. He suffered from insomnia, which led to hallucinations, and went on to study and become a doctor. He married an extroverted woman and had five children, all of whom had mental illnesses.\n> \n> The physician developed gout and sought treatment through medical baths. One of these baths involved a particularly large dose of extract from Indian hemp. This treatment led to intense hallucinations, including the sensation of having a female body. He also experienced anemia, chest pain, cramps, and bizarre feelings as though he was giving birth. Afterward, he continued to have hallucinations and felt as if he had become a woman.\n> \n> He experienced muscular weakness and heightened sensitivity to intense stimuli, such as loud noises. He believed that he could only consume what he called \"a female diet\" and felt that he regularly experienced period symptoms.\n\nAnother key source of evidence is Ray Blanchard’s studies. Based on their clinical experience, researchers had previously speculated about types of trans women, with various patterns. Often these proposals have included a fetishistic type and a feminine/homosexual type.\n\nOne of Blanchard’s contributions was studying this on an unprecedentedly large scale. He found that among trans women who were exclusively sexually attracted to men, autogynephilia was rare, but among trans women who reported being attracted to women, being bisexual, or being asexual, the majority had reported some autogynephilic tendencies, such as having become sexually aroused while wearing women’s clothes. This research has since been replicated by others, though some of the details have not yet 100% converged.\n\nBlanchard also performed studies which found that the autogynephilic/nonandrophilic trans women were more similar to each other than to androphilic trans women in characteristics such as age of transition. He also studied trans women who seemed to contradict his theories more closely, and found that upon further inspection they might actually fit; for instance sometimes the trans women turned out to be lying, which he found out by asking their wives about things.\n\nOne key thing that might be interesting to know about autogynephilia is how common it is. Phil looks at various studies that investigate the prevalence of autogynephilia, such as a single question about whether participants have a preference of “imagining being of the opposite sex to obtain sexual arousal”. By looking at various statistics from various studies, Phil estimates that the prevalence of strong autogynephilia is around 2%, comparable to the prevalence of male homosexuality, which he also estimates to be around 2%.\n\n**Pseudo autogynephilia vs true autogynephilia**\n------------------------------------------------\n\nThe biggest problem with the book is that many of its core points are wrong. One way to think of this* is by looking at how the studies in the book tried to find out how common autogynephilia is. They asked people a few questions about whether they felt sexually excited by the idea of having a female body or wearing women's clothes.\n\nIn order for this to work, the main reason people would answer \"yes\" to those questions should be because they have the true form of autogynephilia described in the book. However, based on questions I’ve asked of men who say they have this sexuality, they often have a different condition, which I'll call \"[pseudo autogynephilia](https://surveyanon.wordpress.com/2022/11/05/autogynephilia-vs-pseudoautogynephilia/).\" This condition differs from true autogynephilia in several ways, some of which can be seen here:\n\n![](https://lh3.googleusercontent.com/__iB1jiuZD5zU0BgY2NnWQk-ZoNjWoZ7U6AYEb1bOHUoLAgoemTAkAJDP4-tVzQKmZgSP6j4MvUXxz_fE-TETBQYmd6GCNh_r8nbJ6nX1GY0xFHpdb1hmxnNH_ue7HFh0AwNJ5oTDL2xCG17ECgio6s)\n\n(You shouldn’t think of the above as necessarily following a specific pattern. The “true autogynephilia” column just contains the characteristics that Phil has collected which are descriptive of autogynephilia. “Pseudo autogynephilia” are just the characteristics (e.g. [sexual fantasies](https://surveyanon.wordpress.com/2020/07/07/a-dataset-of-common-agp-aap-fantasies/)) I have collected which are descriptive of men who score high on measures intended to get at true autogynephilia. Maybe there is a pattern, but it’s not the point.)\n\nThis error makes me question the core claims of the book. For instance, how do we know that the trans women Blanchard studied had true autogynephilia, rather than pseudo autogynephilia (or some third thing)? We don’t, really.\n\nI also don’t think I buy into the book’s characterization of true autogynephilia, even beyond the distinction from pseudo autogynephilia. For instance, while there no doubt are males who are into being forcibly feminized and having their penises locked in chastity cages, is that really best thought of as a self-directed form of attraction to women? That is deeply counterintuitive to me, because it doesn’t seem similar to typical attraction to women.\n\nAn alternate theory of the origins of forced feminization would be the following: Maybe some find it shameful to be feminized (... perhaps because of societal stigma against male femininity?), and for some people shame becomes erotic. This would explain not just their eroticization of forced feminization, but also their eroticization of things like chastity cages, which don’t seem particularly feminizing.\n\nI think what happened is that there were *some* trans women who *just happened* to be masochistic ([lots of people are masochistic!](https://pubmed.ncbi.nlm.nih.gov/26941021/)), and people jumped on lumping this into the category of autogynephilia, without thinking about whether it was really better explained by other factors. There are many examples of this, where the book picks up a thing from an old story, suggests it originates from an inversion of heterosexuality, but where the thing really seems better explained by other factors, e.g.:\n\n*   Does inverted heterosexuality supplant the need for friendship, or is it just that someone had some introverted tendencies or social difficulties which may or may not independently correlate with transness?\n*   Was the Hungarian physician’s perceptions of having a female body really due to autogynephilia, or did it have something to do with their drug use and mental illnesses?\n\nAnyway, pseudo autogynephilia vs true autogynephilia is one thing, but perhaps more important is the direction of causality between autogynephilia and transness. This is a highly controversial question, because it determines whether autogynephilia can be seen as the cause of transness or just a byproduct of it.\n\nI’m not really satisfied with Phil’s treatment of the question of causality. It seems to me that there are good arguments to be made, but instead he makes bad ones. Specifically, Phil has a chapter of the question, where he makes six arguments:\n\n1.  Some critics argue that autogynephilia cannot be causal for transness because cis women are autogynephilic too. But actually those people are wrong because cis women aren’t all that autogynephilic, and even if they were, that has nothing to do with what is going on in trans women.\n2.  Some critics argue that trans women might have autogynephilia-like sexuality because they want to be female and dislike being male. But if that is true, then how does one explain the following facts:\n    1.  Autogynephilia-like sexuality is less common among trans women exclusively attracted to men,\n    2.  There are men with autogynephilia-like sexuality who don’t transition,\n    3.  Sexual orientation labels like “gay” are usually causally downstream from sexual attractions like getting boners when seeing hunks, so by analogy gender identity labels like “trans” should be causally downstream of sexual autogynephilic attraction.\n3.  Trans women often have experiences in their childhood where they wanted to be girls, before their autogynephilic sexual arousal in puberty. This is analogous to how people of other sexual orientations often have childhood crushes, so this supports autogynephilia being causal. \n\nI’ll address these in reverse order; 3-2c-2b-2a. I’ll skip responding to 1 because the whole debate on it is a giant mess that would take a long and convoluted text to sort out while mainly concluding that I am too confused about the matter to know anything about it**.\n\n3: I was really confused when reading argument 3. You might think I’ve made a mistake when explaining it - usually when you see a correlation between A and B, you’d think the one that comes first is the cause. But instead the author is arguing the opposite, that this shows autogynephilia to be the cause.\n\nPhil’s argument seems to be in response to people arguing the opposite, obvious thing: that cross-gender ideation coming before autogynephilia means that autogynephilia is a side-effect and not a cause. I am sympathetic to Phil’s argument that childhood crushes that develop before one has unambiguous sexual arousal seem common, and that therefore the argument from timing isn’t as strong as one would think.\n\nHowever, I think Phil goes too far when he concludes that the ordering is therefore *evidence for* autogynephilia being causal. This seems to be an error that I often see him making: he often jumps from the fact that one can shoehorn a phenomenon into his model, to the assumption that this phenomenon is therefore evidence for his model. \n\n2c: I think the argument about the causality of labels misses the point. Yes, it is true that labels like “homosexual” are causally downstream of experiences of sexual attraction to men. But what people are suggesting is not that an arbitrary gender label causes trans women to have sexual fantasies involving themselves as women, but rather that trans women’s dysphoria about being male and desire to be female causes this sexuality.\n\nIn the section where he makes the argument, Phil is very focused on the idea that gender identity labels change but sexual orientations do not change. He argues that since the change in self-identity happens much later than the autogynephilic arousal, identity cannot be taken to cause autogynephilia.\n\nBut in my experience, even way prior to transition, if you go out and ask autogynephiles how they feel about being male, they will give very different answers from what you get if you ask ordinary men. For instance [one study](https://pubmed.ncbi.nlm.nih.gov/25277693/) found effect sizes of d=1.86 to d=2.85 (depending on how you count).\n\n2b: It is true that there are autogynephiles who don’t transition. Some of them really want to transition and dislike being male; for instance I’ve seen one of them say the following:\n\n> *I have always felt deep down that I would prefer to be female. I prefer the way they dress and act and it always made me a bit jealous of girls. I feel I would be more comfortable in my own skin as a female. I have come to terms with the way that I feel and accept that I wouldn’t want to go through a transition. I am okay with the way that I feel now and have learned to accept those feelings.*\n> \n> *I don’t feel that I am traditionally very manly and feel more comfortable around girls*\n\nI think people who advocate the theory that gender dysphoria causes autogynephilia would be perfectly happy to bite the bullet that this guy’s gender feelings caused him to eroticize feminization, even though he doesn’t literally label himself as a woman.\n\nOn the other hand, there seems to be some autogynephiles who this doesn’t apply to; whose autogynephilia seems to exceed what could be explained by their desire to be a woman. But that just means that autogynephilia and gender identity are not deterministically correlated. As nondeterminism is a symmetric relationship, it doesn’t disprove that gender identity influences autogynephilia any more than it disproves that autogynephilia influences gender identity. It just means that there are other factors in play.\n\nYou can make up stories about what those factors are - ideology, sexual success, experience, etc. - but the opposing side can also make up stories about what the alternate factors that could cause autogynephilia are (e.g. I am pretty sure I’ve seen someone suggest that curiosity may play a role in causing autogynephilia without feminine gender identity).\n\n2a: The argument goes that if autogynephilia was influenced by gender identity, then it should be as common among trans women who are attracted to women as it is among trans women who are attracted to men.\n\nI don’t understand this argument. Elsewhere in the book, Phil says that attraction to women is a contributor to autogynephilia because this attraction gets “inverted” onto oneself in some sense. This seems intuitively sufficient to explain sexual orientation differences in autogynephilia among trans women by sexual orientation.\n\n*✱ Another way to think of this problem is to say that the author’s method for diagnosing autogynephilia is correct, but that the author is wrong about what autogynephilia is like. I would usually use this latter frame, of thinking of the problem as people being wrong about what autogynephilia is like. But Phil’s description of autogynephilia is common elsewhere, and it’s such a struggle to enforce an alternate description of autogynephilia that I am giving up.*\n\n*If I’m willing to think of pseudo autogynephilia as being the real form of autogynephilia, with Phil just getting some facts about the characteristics of autogynephilia wrong, then I would not be surprised if my point sounds nitpicky to some of the readers. Who cares if it got a few details of autogynephilia wrong? But think of it from this perspective: throughout the book, Phil often has a narrative that trans women are in denial about autogynephilia, but that science will eventually prove him right. However if he gets most of the characteristics of autogynephilia wrong, then 1) it’s no surprise that trans women fail to relate to his descriptions, and 2) science will presumably prove him wrong.*\n\n*When I asked Phil, he also said he preferred the pseudo autogynephilia vs true autogynephilia characterization to a characterization of him just being totally wrong about autogynephilia.*\n\n*✱✱ If you want to see the most recent serious entry into the debate about autogynephilia in cis women, then it is the paper *[*It Helps If You Stop Confusing Gender Dysphoria and Transvestism*](https://www.researchgate.net/publication/363429868_A_Response_to_Bailey_and_Hsu_2022_It_Helps_If_You_Stop_Confusing_Gender_Dysphoria_and_Transvestism)*. It is somewhat biased against autogynephilia theory, though. See also my post, *[*Transvestism vs Gender Dysphoria vs …*](https://surveyanon.wordpress.com/2023/05/27/transvestism-vs-gender-dysphoria-vs/) *for more.*\n\n**The good and the bad**\n------------------------\n\nThere is so much more that could be said about the flaws of this book. I’ve put a list of what I consider to be the most egregious flaws at the second-last part of this review. Perhaps the biggest flaw is that it often uses the same voice for ultra-speculative things as it does for well-established things, rather than properly acknowledging uncertainty. And it regularly makes strange assertions that seem to be strongly picking political sides, while neither acknowledging the political element nor rationally justifying the statements.\n\nHowever, the book is not all bad.\n\nMost importantly, the book talks about autoandrophilia, which is the female equivalent of autogynephilia. While autogynephiles are males who feel attracted to being female, autoandrophiles are females who feel attracted to being male. In my experience, people often don't think this kind of sexuality is important when talking about transgender issues. Some say they think that only men can have unusual sexual desires. But autoandrophilia is real! And it's likely an important factor for trans men (as well as for women who privately wish they were men but don't transition)!\n\nAs part of this, it gives examples of various aspects of autoandrophilia. These examples are sorely needed, as there are currently very few places that talk about what autoandrophilia is like. It includes discussion of how transvestism is more of a thing among autogynephiles than autoandrophiles, as well as discussion of how some autoandrophiles enjoy taking on the male role and picking up women.\n\nThat said, I think the book can end up treating things in an overly symmetric way. Unlike men, women seem to face very strong gender norms about looking good and being warm to others, and women who have male-typical interests often face problems as a result. I think this can lead to gender dissatisfaction driven not by sexuality, but by avoidance of gender norms.\n\nWhile I complained earlier about aspects of its portrayal of autogynephiles, there are also aspects of its portrayal that are great. Its description of autogynephilic transvestism is detailed and informative. It gives lots of examples, and it is clearly a topic the author is passionate about.\n\nIt is also very informative about an adjacent topic, namely ‘traps’ - which it defines as feminized males with an intact penis*. Did you know that men who are attracted to traps are usually more attracted to cis women than to cis men? Did you know that men who are attracted to traps are also often autogynephilic? I knew that ahead of time, but many people might not. By reading the book, you can learn much more about this sort of attraction.\n\nOne concept he brings up is erotic target location errors. Basically, consider the controversy of [Rachel Dolezal](https://en.wikipedia.org/wiki/Rachel_Dolezal), a woman who became the center of a controversy for identifying as black when she was born white. Phil suggests that she may be “autophylophilic”, sexually attracted to being black, a sort of racial analogue of autogynephilia. Phil suggests that for any trait one can be sexually attracted to in others, there can be a corresponding self-directed sexual attraction, and he gives examples of this, including furries and body integrity identity disorder.\n\nI am not sure I buy Phil’s story about autophylophilia and erotic target location errors. However, the chapter on transracialism is interesting, as it gives more details about Rachel Dolezal’s background. Did you know that she had a strong, lifelong attraction to black culture? Unfortunately, due to the many errors elsewhere in the book, I am not sure whether I can trust Phil to accurately represent Rachel Dolezal’s background, but it certainly is fascinating if true.\n\nFinally, the book has some chapters on politics. Its chapter addressing general trans politics seems balanced and compassionate. It addresses questions such as how we should accept transition for autoheterosexuals, or what to make of transrace people. It also emphasizes the need for trans people to respect if cis people aren’t sexually attracted to them. It is not perfect, as some of its positions are dubious and there are a lot of unanswered questions, but it strikes a nice balance.\n\n*✱ Often these would be trans women rather than cis men. However figuring out the right categorization is tricky for all sorts of reasons. I’m using the term ‘traps’ because it is the most convenient term I know of which encompasses everyone in the group in question regardless of gender identity, and because this is the term the book uses for this section. However you should be aware that the term is usually considered offensive within the trans community.*\n\n**Evaluation**\n--------------\n\nEven if the book gets some things wrong, might it still be getting the big picture right? Should you read it? Let’s start with whether it gets the big picture right.\n\nI am sympathetic to the idea that sexuality often plays a big role in transition. However, as you can see by the pseudo autogynephilia vs true autogynephilia distinction I came up with, I think Phil gets many of the details wrong. But it doesn’t seem that those specifics are what most people object to.\n\nFrom asking around, I’ve found that most people believe in a theory that if people repress their desires, then those desires may come out sexually. I call this theory “masochistic CEO theory” because the archetypal example is an overworked CEO who goes to a dominatrix to release control after being overstressed at work.\n\nIf you believe in masochistic CEO theory, then the idea that something like autogynephilia plays a causal role in transitioning is probably very questionable, because people have done almost nothing to rule out the reverse causality, that repressed transgender identity causes autogynephilia-like sexuality. I personally don’t believe in masochistic CEO theory because I have never seen evidence for it and it doesn’t align with my theory of why sexuality exists or how sexuality works.\n\nOther than autoheterosexuality, there are basically two alternative positions in the debate of how transness works. One is the feminine essence narrative (for MtFs; I suppose the corresponding FtM version could be called the masculine essence narrative). In this narrative, trans people transition because they are in some mental sense really like the opposite sex, and this inevitably leads to gender dissatisfaction. This is the narrative the book is in response to:\n\n> If someone’s deepest, most heartfelt wish was to be the other sex, which of the following explanations for their gender feelings would feel more emotionally satisfying?\n> \n> They want to be the other sex because:\n> \n> 1.  In truth, they are actually more like the other sex in mind and spirit, but a mistake in development led them to have the wrong anatomy.\n> 2.  They have an autosexual version of heterosexuality that makes them happier when they feel similar to the other sex or embody traits they associate with it.\n> \n> It’s a no-brainer, right? The first one is obviously more emotionally satisfying.\n> \n> This is why hearing about autoheterosexuality is so upsetting to many trans people. It suggests to them that their desire to be the other sex is ultimately because they are not that sex. Furthermore, their drive to be the other sex comes from a type of heterosexuality—the least queer kind of sexuality.\n> \n> The autoheterosexual explanation can undermine the hard-earned sense of identity they’ve painstakingly constructed over years. Many loathe the concepts of autoandrophilia and autogynephilia, as well as the two-type typology that categorizes them as autoheterosexual if they aren’t solely same-sex attracted.\n> \n> However, only a minority of autoheterosexuals transition to live as the other sex.\n> \n> What about the rest of us? Don’t we deserve a fair shot at making sense of our experiences?\n\nIf either theory is true, it may eventually be provable using neuroscience to identify autogynephilia or brain feminization and show the direction of causality. This means that we can use prediction markets about future neuroscience findings to establish a consensus. I have made [a market about this on Manifold Markets](https://manifold.markets/tailcalled/if-a-solid-neurological-study-of-tr), which at the time of writing gives the theory 21%-23% probability, much more than the 12% probability of the feminine essence narrative:\n\n[![](https://lh4.googleusercontent.com/mZhtXJi-WOa9WQKUdkZRZ7GIBQ2QJnoMivKxO44i8gHfuX7S05AriWsCKJQ8k5gWV35xsO4H3lXW1hNfVZXtoAQBahezQAcIEPrKq3MsiExVAwcKKtdfbU6zmRKTjA2GBedZJGCoiMMb3mMeNhTbkU4)](https://manifold.markets/tailcalled/if-a-solid-neurological-study-of-tr)\n\n(This is a thinly traded market. If you know anything about the topic, please trade to improve the probabilities!)\n\nHowever, it appears that the most popular answer (at least for now) is that neither feminine essence nor autogynephilia theory is correct. I don’t think I’ve seen any satisfying alternate theories despite thinking about this subject a lot, so it is basically the “we don’t know yet” option.\n\nAnd that makes it hard to evaluate, because it would require us to come up with fundamentally new ideas about gender identity, and I don’t know how those ideas would change my mind on existing issues.\n\nBut should you read the book? Phil suggests that instead of phrasing it as a yes/no question, I should think of it in a more open-ended way: If a person is autogynephilic or autoandrophilic, then what could they be asked to read to develop a better understanding of themselves?\n\nThe fact is, I don’t know of any great alternative writings on the topic. From that perspective, it is possible that Phil’s book is the “least bad” option, giving an overview of things from a sympathetic point of view. My fear is that the many flaws create confusion, and that the often strange or misleading arguments will lead critics to (rightly) conclude that this book is in bad faith. But I can’t deny his point that there is a lack of good alternative resources.\n\n(If you know of any alternative resources, then please post them in the comments.)\n\nSo, if the topic seems relevant to you, then yes, it may be a good idea to read the book. Just beware of taking it too seriously, or you will probably end up caught on one of its many mistakes. Make sure to read widely (the book contains some references to get you started), and also make sure to be aware that the science in this area is overall very questionable.\n\nPhil has this idea that his book will lead to a revolution, where many people come out as autogynephilic or autoandrophilic, and people discover the many applications of autoheterosexuality theory. [I am skeptical of this](https://surveyanon.wordpress.com/2021/06/25/autogynephilia-is-not-a-natural-abstraction-but-transgender-identity-is/).\n\nMost of the direct applications of the autoheterosexuality concept “cut across” the category of autoheterosexuality, separating it in two. For example, autoheterosexuals who don’t transition need some name for their situation, and at times I’ve seen them identify as having “just a fetish”, which seems to me like a natural idea that they would spontaneously come up with, even without any activism.\n\nBut the notion of “I am not trans, I just have a fetish” is memetically similar to the concept of “I do not have a feminine essence, I am autogynephilic”, to the point where in my experience people have a hard time distinguishing between the two. And the distinction is critically important if we want to treat autoheterosexuality as the primary explanation of transness, because in that case we cannot treat the notion of autoheterosexuality as the opposite of being trans.\n\nPhil tries hard to work against this in his book. However I think he will fail because he is working against natural memetic evolution. For example, it doesn’t seem to me that it is useful for people’s primary identity label to be one that lumps together private transvestites with public transsexuals, because it doesn’t relate to people’s lives. Instead I expect people to end up categorizing based on lifestyle.\n\nYears ago, I was challenged to come up with practical applications of autoheterosexuality theory. I never succeeded. I am reminded of the post [Extreme Rationality: It’s Not That Great](https://www.lesswrong.com/posts/LgavAYtzFQZKg95WC/extreme-rationality-it-s-not-that-great). Aspects of autoheterosexuality theory may if you are lucky bring some clarity-of-mind benefits, but nobody has been able to come up with convincing applications that help you in practice. Still, you may appreciate the clarity-of-mind benefits anyway.\n\nI also think fans of autogynephilia theory make their own research mistakes because they try hard to avoid distinguishing between autogynephilic cis men and gynephilic trans women. They generalize things inappropriately back and forth between trans women and autogynephilic cis men, without checking whether the generalizations are valid.\n\nFor instance, many autoheterosexuals seem to end up with an intuition that their feelings can be repressed through romantic relationships, Phil claims it has been found to not work. He may be right about the conclusion of it not working, however the primary evidence is based on trans women’s experiences, and since trans women by definition didn’t end up repressing, it would seem that if it sometimes works, you wouldn’t have heard about it through the methods he used.\n\n**Some of the most egregious flaws**\n------------------------------------\n\nThis is a list of some very basic facts which I think affect how you interpret lots of other facts and which I think the book either gets wrong or does not sufficiently inform people about.\n\n*Meta-attraction: *There is a phenomenon called meta-attraction, where autoheterosexuals develop a sort of desire to have homosexual sex, which is distinct from standard homosexuality. That is, an autogynephilic male might be interested in crossdressing and then having sex with males. I basically buy that it exists and is distinct from ordinary androphilia.\n\nHowever, the concept of meta-attraction can end up being a form of gaslighting; on the one hand, the book emphasizes that meta-attracted autogynephiles are not truly attracted to men’s bodies, but instead fantasize about them as ‘faceless men’ who might as well be disembodied penises.\n\nBut on the other hand, the book emphasizes that it can be difficult to tell the difference between meta-attraction and classical attraction. These seem hard to square to me, and in my impression when communities apply the concept of meta-attraction, it is often done in a gaslighty manner, where they tell people that they are deluded about their sexual experiences.\n\n*Homosexuality and gender identity: *The book vaguely implies that there is a connection between homosexuality and gender dissatisfaction, while neglecting to mention that [typical gay men are as happy about being male as straight men are](https://research.vumc.nl/ws/portalfiles/portal/354253/hoofdstuk+03.pdf).\n\nThis problem feels especially noticeable in the chapter on juvenile transition. The book argues that when minors transition, they are usually homosexual rather than autoheterosexual.\n\nHowever, if homosexual males typically end up perfectly happy with being male*, why is transition a good idea for them? Do the ones who end up transitioning have a fundamentally different condition than ordinary homosexuality? Do they transition because of factors like bullying, which might be better addressed by other means?\n\nI don’t know, and these are the questions that make me most ambivalent about transition for minors, but they aren’t addressed.\n\n*Heritability:* The book discusses heritability in a way that is basically invalid, due to the [phenotypic null hypothesis](https://tailcalled.substack.com/p/if-everything-is-genetic-then-nothing?s=w). It treats heritability as a proof that things are biological, when really basically everything is heritable in a tautological way.\n\n*Apophenia:* The book often makes very questionable inferences, involving linking things that are barely even related.\n\nFor instance, you may have heard that there is a very large sex difference on the people-things dimension of work interests, which means that men are more interested in jobs such as carpentry which involve working with things, while women are more interested in being social workers or similar, which involves working with people.\n\nPhil makes the leap that this also makes men more sexually attracted to objects such as clothing, potentially explaining the sex difference in transvestic fetishism.\n\nI would be extremely surprised if Phil turns out even remotely close to right in this prediction. And this is not the only case of similar apophenia.\n\n*Motte-Bailey in the romance hypothesis:* One controversial concept within autogynephilia theory is called the “romance hypothesis”.\n\nThe motte version of this theory is a response to people arguing that autogynephilia theory must be wrong, because you would predict that as trans women start antiandrogen therapy, their reduction in libido should make them stop wanting to transition. Here, the romance hypothesis asserts that autogynephiles develop an emotional attachment to being female, analogous to romantic love, which causes them to want to stay women, even without lust.\n\nI think many trans women’s experiences can be shoehorned into the motte theory, and so it is hard to argue against. Though because it states things so weakly, it can also be hard to argue in favor of it.\n\nTo defend the romance hypothesis, Phil argues that it has been observed for decades, citing researchers such as [H. T. Buckner](http://www.tbuckner.com/TRANSVES.HTM), who finds that transvestites treat their crossdressed selves as a sort of girlfriend. This includes buying gifts for themselves, talking about themselves in a dissociated/dualized way, etc.. Phil also cites the example of Lou Sullivan, a trans man who would speak of his union between his male and female self, and sing about his self-love:\n\n> Seven years prior to his gender transition, Lou Sullivan touched on this theme of internal union in his diary. Just before he got his first leather jacket and began to commit more deeply to transvestism, he wrote: I love to blend female and male—I think of myself as two people finally coming together in peace with each other. Of my other half, I sing, ‘Nobody loves me but me adores you!’”\n\nThis is the bailey version of the romance hypothesis: that autogynephiles create a female identity and in a very literal sense treat this female identity as a girlfriend that they are dating. The issue I have with this bailey version is that it doesn’t seem like the way the supposedly-autogynephilic trans women I know relate to themselves, so rather than being evidence for autogynephilia theory, it more seems like evidence against it.\n\n*✱ Unlike autogynephiles, who are usually at least somewhat skewed towards wanting to be women, even if they don’t transition.*\n\n**Philosophy of science**\n-------------------------\n\nMy review has been quite critical of the book. I am concerned that this might make people falsely think that Phil doesn’t know anything about what he is writing about, so I want to address that here, as well as getting into some more general questions of science.\n\nPhil is very enthusiastic about this research area, and is very well-read in a lot of old-school gender research. He’s somewhat biased in his selection of research, but much less so than the proponents of feminine essence theory that I have seen.\n\nWhile writing this review, I had the thought that the real problem is that Phil believes in science. And the problem with science is that it is often a fractal of wrongness.\n\nLet’s take an issue where Phil agrees with me: autoandrophilia is real. But if autoandrophilia is real, then why didn’t the scientists he cites emphasize it as much as they do with autogynephilia?\n\nI think the answer to this question is messy, but likely involves many of the following concepts:\n\n*   Because autoheterosexuality was for a long time named “transvestic fetishism”, researchers thought it was *about the clothes*,\\* and autoandrophilia is somewhat less transvestically focused than autogynephilia is, so they would be less likely to notice it.**\n*   Autoandrophilia is somewhat rarer than autogynephilia, so they would have been less likely to notice it.\n*   Researchers have long had a preconception that unusual sexual motivations are a male-only thing, so they would have been less likely to notice it.\n*   Many studies have been performed on autogynephiles, but few studies have been performed on autoandrophiles, so they would have been less likely to notice it.\n*   Women face stronger gender norms than men, so the autoandrophiles who transition will likely also have masculinity as a secondary motivator, making it less unclear whether autoandrophilia is even needed as an explanation, and therefore making them less likely to notice it.\n\nBasically, the way I see it is, researchers were not paying enough attention to notice autoandrophilia. But in my opinion, autoandrophilia is sort of obviously a thing. If researchers are not paying attention to really notice it, then there are lots of other things that they also fail to realize. (In this perspective, it’s almost no wonder the prediction market participants expect the true answer to be a theory that hasn’t been put forth yet.)\n\nThere’s a lot of places in reading the book where I’ve been ambivalent. For example, consider furries. Furries are a community of adults who are enthusiastic for anthropomorphic animals. They often make artwork (especially, at least according to stereotypes, porn) of anthropomorphic animals, and one of their main hobbies is “fursuiting”, where they dress up as animals.\n\nIn the book, Phil suggests that furries have an erotic target location error, analogous to autoheterosexuality, where they have a sexual attraction to anthropomorphic animals, and then this attraction in some sense becomes self-directed.\n\nTo support this, Phil draws on some surveys (e.g. [The “Furry” Phenomenon](https://www.researchgate.net/publication/331375793_The_Furry_Phenomenon_Characterizing_Sexual_Orientation_Sexual_Motivation_and_Erotic_Target_Identity_Inversions_in_Male_Furries)) which find that a lot of furries report sexual attraction to anthropomorphic animals, sexual attraction to being an anthropomorphic animal, and that the kind of anthropomorphic animal they are attracted to being is commonly the same as the kind they are attracted to as partners.\n\nHowever, when I look at the furry surveys, I notice a lot of oddities that makes it questionable whether it is analogous to autoheterosexuality. For instance, in the case of autogynephilia, most men are gynephilic (attracted to women) but only a few are autogynephilic. But with furries, most furries are both attracted to anthros as partners *and* most furries are attracted to being anthros. If the mechanisms were the same, shouldn’t the pattern for furries be the same as the pattern for gynephiles? (With most furries being attracted to anthros as partners, but not to being anthros themselves.) And this is not the only example of internal contradictions in the theories about furries.\n\nDoes it seem plausible that there’s something going on with furries that is analogous to transness? Yes. But also, maybe it’s not analogous, maybe the similarities are superficial. I think it’s worth collecting more information on it, but also I can’t blame people for not engaging with his theories about furries, when those theories contain so many weak arguments and internal contradictions, and so little data.\n\nAnd speaking of data, it is my impression that a lot of the characterization of what autogynephilia is like was created by stitching together anecdotes from trans women. But I worry that people are more likely to share unusual or extreme anecdotes, [which would skew the characterization of autogynephilia towards the unusual or extreme end](https://slatestarcodex.com/2015/09/16/cardiologists-and-chinese-robbers/). And as mentioned earlier in the review, I worry that trans women may have other characteristics (either systematically or by coincidence) which get misattributed as inverted attraction to women.\n\nI don’t think these problems are unsolvable. You’d go a long way by just taking all the autogynephilia stereotypes you’ve come up with, collecting data on them in a large general population sample, and performing factor analysis of this data. This would automatically disentangle coincidence from correlation, and it would be able to tell whether the stereotypes are getting at a single unified thing, or a mishmash of independent characteristics. This is fairly standard methodology, but researchers who work on autogynephilia seem strangely averse to doing it.\n\nBut I don’t think the negativity of this review can be entirely blamed on the research in these areas being weak, rather than on Phil. As you saw earlier in the review (such as with the arguments about direction of causality), I thought some of his own arguments were very bad. This can’t just be a data error.\n\nI find this to be a deeply anti-rational approach, and I think is not viable if we want to do incremental science. [Local Validity is a Key to Sanity and Civilization](https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization). If we accept poor and invalid arguments as long as they lead to the “right” conclusion, we will end up immersing ourselves into a thick mist of invalid and misleading arguments. This will be confusing, and when new surprising evidence comes along, I think the habit of making incorrect arguments will lead to making incorrect arguments about that evidence too. I have tried talking to some of the people doing research in this area, and I find this to be a serious problem in practice, which prevents them from doing course-corrections when needed.\n\n*✱ This also suggests another critique: since autoheterosexuality is not about the clothes, the fact that scientists thought so is indicative of a pretty serious problem with science.*\n\n*✱✱ This argument is also made by Phil in the book.*\n\n*Thank you to Justis Mills, Phil Illy, Zack Davis and Pasha for proofreading and feedback.*",
      "plaintextDescription": "This review was originally written for the Astral Codex Ten Book Review Contest. Unfortunately it didn’t make it as one of the finalists. but since I made use of the LessWrong  proofreading/feedback service, I am reposting it here. It can also be found on my gender blog.\n\nIf I ask ChatGPT to explain transgender people to me, then it often retreats into vague discussions of gender identity. It is very hard to get it to explain what these things mean, in terms of actual experiences people might have. And that might not be a coincidence - the concepts used to understand transness seem to be the result of a complicated political negotiation, at least as much as they are optimized to communicate people’s experiences.\n\nSome people claim to do better, using elaborate words like autogynephilia and gynandromorphophilia and so on. They speak of typologies, science and history. Of developmental pathways and genetics. This ideology is sometimes called Blanchardianism, named after Ray Blanchard, a sex researcher who contributed many influential ideas within the sphere.\n\nPhil Illy’s book, Autoheterosexual: Attracted to Being the Other Sex is the latest book pushing Blanchardianism. But is it any good?\n\n\nSummary of the book on autogynephilia\nBoth Phil and I are probably most familiar with the concept of autogynephilia, so let’s start there. A substantial part of the book is about the feelings and experiences of autogynephiles.\n\nSome individuals, born male, feel a deep happiness when imagining themselves as a woman and a deep sadness being a man, and this is often due to autogynephilia. Phil Illy explains that autogynephilia can be seen as an inverted form of sexual attraction to women, where the individual's attraction to women is applied to themselves. Autogynephilia is ultimately about sexuality, but Phil emphasizes that many of the feelings and experiences he describes are not driven by lust, but instead by strong emotional attachments. In some cases it can cause them to have m",
      "wordCount": 7316
    },
    "tags": [
      {
        "_id": "ubGeEpjJ2DCkDHiEW",
        "name": "Autosexuality",
        "slug": "autosexuality"
      },
      {
        "_id": "4Kcm4etxAJjmeDkHP",
        "name": "Book Reviews / Media Reviews",
        "slug": "book-reviews-media-reviews"
      },
      {
        "_id": "W9aNkPwtPhMrcfgj7",
        "name": "Sex & Gender",
        "slug": "sex-and-gender"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]