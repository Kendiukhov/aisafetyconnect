[
  {
    "_id": "kBg5eoXvLxQYyxD6R",
    "title": "My takes on SB-1047",
    "slug": "my-takes-on-sb-1047",
    "url": null,
    "baseScore": 151,
    "voteCount": 53,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-09-09T18:38:37.799Z",
    "contents": {
      "markdown": "I recently decided to sign a [letter of support](https://calltolead.org/) for SB 1047. Before deciding whether to do so, I felt it was important for me to develop an independent opinion on whether the bill was good, as opposed to deferring to the opinions of those around me, so I read through the full text of SB 1047. After forming my opinion, I checked my understanding of tort law basics (definitions of “reasonable care” and “materially contribute”) with a law professor who was recommended to me by one of the SB 1047 sponsors, but who was not directly involved in the drafting or lobbying for the bill. Ideally I would have wanted to consult with a completely independent lawyer, but this would have been prohibitively expensive and difficult on a tight timeline. This post outlines my current understanding. It is not legal advice.\n\nMy main impression of the final version of SB 1047 is that it is quite mild. Its obligations only cover models trained with $100M+ of compute, or finetuned with $10M+ of compute. [^i0wzxpb8e8h] If a developer is training a covered model, they have to write an SSP, that explains why they believe it is not possible to use the model (or a post-train/finetune of the model costing <$10M of compute) to cause critical harm ($500M+ in damage or mass casualties). This would involve running evals, doing red teaming, etc. The SSP also has to describe what circumstances would cause the developer to decide to shut down training and any copies of the model that the developer controls, and how they will ensure that they can actually do so if needed. Finally, a redacted copy of the SSP must be made available to the public (and an unredacted copy filed with the Attorney General). This doesn’t seem super burdensome, and is very similar to what labs are already doing voluntarily, but it seems good to codify these things because otherwise labs could stop doing them in the future. Also, current SSPs don’t make hard commitments about when to actually stop training, so it would be good to have that.\n\nIf a critical harm happens, then the question for determining penalties is whether the developer met their duty to exercise “reasonable care” to prevent models from “materially contributing” to the critical harm. This is determined by looking at how good the SSP was (both in an absolute sense and when compared to other developers) and how closely it was adhered to in practice.\n\nReasonable care is a well-established concept in tort law that basically means you did a cost benefit analysis that a reasonable person would have done. Importantly, it doesn’t mean the developer has to be absolutely certain that nothing bad can happen. For example, suppose you release an open source model after doing dangerous capabilities evals to make sure it can’t make a bioweapon,[^brkkdxsn3t] but then a few years later a breakthrough in scaffolding methods happens and someone makes a bioweapon using your model—as long as you were thorough in your dangerous capabilities evals you would not be liable, because it would not have been reasonable for you to anticipate that someone would make a breakthrough that invalidates your evaluations. Also, if mitigating the risk would be too costly, and the benefit of releasing the model far outweighs the risks of release, this is also a valid reason not to mitigate the risk under the standard of reasonable care (e.g the benefits of driving a car at a normal speed far outweigh the costs of car accidents; so reasonable care doesn’t require driving at 2 mph to fully mitigate the risk of car accidents). My personal opinion is I think the reasonable care standard is too weak to prevent AI from killing everyone. However, this also means that I think people opposing the current version of the bill because of the reasonable care requirement are overreacting.\n\nMaterially contributing is not as well-established a concept but my understanding is it means the model can’t just merely be helpful in causing critical harm, but rather it must be that the model was strongly counterfactual; the critical harm would not have happened without the existence of the model. In addition, the bill also explicitly clarifies that cases where the model provides information that was publicly accessible anyways don't count. So for example, if a terrorist uses a model to make a bioweapon, and the model provides the same advice as google, then this doesn’t count; if it cuts the cost in half by providing more useful information than the internet, then it probably still doesn’t count, since a determined terrorist wouldn’t be deterred merely by a 2x in cost; if it cuts the cost by 100x, it probably does count; if it provides advice that couldn’t have been gotten from a human expert because all the human experts out there have moral scruples and don’t want to help make a bioweapon, it probably also counts.\n\nIt doesn’t affect near-term open source models, simply because they will not be powerful enough to materially contribute to critical harm. In the longer term, once models can contribute to critical harm if jailbroken, it seems very hard to ensure that safeguards cannot be removed from open source models with up to $10M of compute, even just with known attacks. But it seems pretty reasonable to me to not release models which (a) can do $500M of damage or cause mass casualties if safeguards are removed, (b) safeguards can be removed for <$10M with already-known attacks, and (c) where the benefits of releasing the unmitigated model do not outweigh the costs from critical harm.\n\nThere are also some provisions for whistleblower protection. Employees cannot be punished for disclosing information to the AG or the Labor Commissioner. There also needs to be an anonymous hotline for reporting information to directors/officers. This seems pretty reasonable.\n\nWhile I do think federal regulation would be preferable, I’m not very sympathetic to the argument that SB 1047 should not be passed because federal regulation would be better. It seems likely that passing a federal bill similar to SB 1047 gets harder if SB 1047 fails. Also, I don’t think this regulation splits talent between the federal and state level: aside from the Board of Frontier Models, which mostly just sets the compute thresholds, this version of SB 1047 does not create any substantial new regulatory bodies. The prospect for federal regulation seems quite uncertain, especially if a Trump presidency happens. And once strong federal regulation does pass, SB 1047 can degrade gracefully into mostly deferring to federal regulatory bodies.\n\nI don’t think SB 1047 is nearly sufficient to prevent catastrophic risk, though it is a step in the right direction. So I think a lot of its impact will be through how it affects future AI regulation. My guess is if SB 1047 passes, this probably creates more momentum for future AI regulation. (Also, it would be in effect some number of years earlier than federal regulation — this is especially relevant if you have shorter timelines than me.)\n\n[^i0wzxpb8e8h]: There are also FLOP count thresholds specified in the bill (1e26, 3e25 respectively), but they’re not too far off from these dollar amounts, and compute quickly gets cheaper. This threshold can be raised higher in the future by the BFM, but not lowered below $100M/$10M respectively, so the BFM could completely neuter the bill if it so chose. \n\n[^brkkdxsn3t]: I use bioweapons as the prototypical example because it's straightforward to reason through, but AIs helping terrorists make bioweapons is actually not really my modal story of how AIs cause catastrophic harm.",
      "plaintextDescription": "I recently decided to sign a letter of support for SB 1047. Before deciding whether to do so, I felt it was important for me to develop an independent opinion on whether the bill was good, as opposed to deferring to the opinions of those around me, so I read through the full text of SB 1047. After forming my opinion, I checked my understanding of tort law basics (definitions of “reasonable care” and “materially contribute”) with a law professor who was recommended to me by one of the SB 1047 sponsors, but who was not directly involved in the drafting or lobbying for the bill. Ideally I would have wanted to consult with a completely independent lawyer, but this would have been prohibitively expensive and difficult on a tight timeline. This post outlines my current understanding. It is not legal advice.\n\nMy main impression of the final version of SB 1047 is that it is quite mild. Its obligations only cover models trained with $100M+ of compute, or finetuned with $10M+ of compute. [1] If a developer is training a covered model, they have to write an SSP, that explains why they believe it is not possible to use the model (or a post-train/finetune of the model costing <$10M of compute) to cause critical harm ($500M+ in damage or mass casualties). This would involve running evals, doing red teaming, etc. The SSP also has to describe what circumstances would cause the developer to decide to shut down training and any copies of the model that the developer controls, and how they will ensure that they can actually do so if needed. Finally, a redacted copy of the SSP must be made available to the public (and an unredacted copy filed with the Attorney General). This doesn’t seem super burdensome, and is very similar to what labs are already doing voluntarily, but it seems good to codify these things because otherwise labs could stop doing them in the future. Also, current SSPs don’t make hard commitments about when to actually stop training, so it would be good to have that.\n\n",
      "wordCount": 1183
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2c7",
        "name": "Regulation and AI Risk",
        "slug": "regulation-and-ai-risk"
      },
      {
        "_id": "QiyWTcvmChHssFCBx",
        "name": "SB 1047",
        "slug": "sb-1047"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "Fg2gAgxN6hHSaTjkf",
    "title": "Scaling and evaluating sparse autoencoders",
    "slug": "scaling-and-evaluating-sparse-autoencoders",
    "url": null,
    "baseScore": 106,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-06-06T22:50:39.440Z",
    "contents": {
      "markdown": "\\[[Blog](https://openai.com/index/extracting-concepts-from-gpt-4/)\\] \\[[Paper](https://cdn.openai.com/papers/sparse-autoencoders.pdf)\\] \\[[Visualizer](https://openaipublic.blob.core.windows.net/sparse-autoencoder/sae-viewer/index.html)\\]\n\nAbstract:\n\n> Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer.  Since language models learn many concepts, autoencoders need to be very large to recover all relevant features.  However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents.  We propose using k-sparse autoencoders \\[Makhzani and Frey, 2013\\] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried.  Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity.  We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size.  To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens.  We release code and autoencoders for open-source models, as well as a visualizer.",
      "plaintextDescription": "[Blog] [Paper] [Visualizer]\n\nAbstract:\n\n> Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer.  Since language models learn many concepts, autoencoders need to be very large to recover all relevant features.  However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents.  We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried.  Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity.  We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size.  To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens.  We release code and autoencoders for open-source models, as well as a visualizer.",
      "wordCount": 192
    },
    "tags": [
      {
        "_id": "9LYbEZSAi8kq83DGF",
        "name": "Sparse Autoencoders (SAEs)",
        "slug": "sparse-autoencoders-saes"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "9W8roCAeEccSa3Chz",
    "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
    "slug": "weak-to-strong-generalization-eliciting-strong-capabilities",
    "url": null,
    "baseScore": 55,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2023-12-16T05:39:10.558Z",
    "contents": {
      "markdown": "Links: [Blog](https://openai.com/research/weak-to-strong-generalization), [Paper](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf).\n\n**Abstract**:\n\n> Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.",
      "plaintextDescription": "Links: Blog, Paper.\n\nAbstract:\n\n> Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.",
      "wordCount": 227
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FX5JmftqL2j6K8dn4",
    "title": "Shapley Value Attribution in Chain of Thought",
    "slug": "shapley-value-attribution-in-chain-of-thought",
    "url": null,
    "baseScore": 106,
    "voteCount": 49,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2023-04-14T05:56:18.208Z",
    "contents": {
      "markdown": "**TL;DR**: Language models sometimes seem to ignore parts of the chain of thought, and larger models appear to do this more often. Shapley value attribution is a possible approach to get a more detailed picture of the information flow within the chain of thought, though it has its limitations.\n\nProject status: The analysis is not as rigorous as I would prefer, but I'm going to be working on other directions for the foreseeable future, so I'm posting what I already have in case it's useful to others. Code for replicating the Shapley value results can be found [here](https://gist.github.com/leogao2/ef6afb5530eaf948b8602faae961fda0).\n\nThanks to Jacob Hilton, Giambattista Parascandolo, Tamera Lanham, Ethan Perez, and Jason Wei for discussion.\n\nMotivation\n----------\n\nChain of thought (CoT) has been proposed as a method for language model interpretability (see [Externalized Reasoning Oversight](https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for), [Visible Thoughts](https://intelligence.org/visible/)). One crucial requirement for interpretability methods is that they should accurately reflect the cognition inside the model. However, by default there is nothing forcing the CoT to actually correspond to the model’s cognition, and there may exist [theoretical limitations](https://www.lesswrong.com/posts/Jiy3n5KMsGGJ6NNYH/asot-some-thoughts-about-lm-monologue-limitations-and-elk) to doing so in general. \n\nBecause it is plausible that the first AGI systems bear resemblance to current LMs with more sophisticated CoT and CoT-like techniques, it is valuable to study its properties, and to understand and address its limitations.\n\nRelated work\n------------\n\nShapley values have been used very broadly in ML for feature importance and attribution ([Cohen et al, 2007](https://pubmed.ncbi.nlm.nih.gov/17521285/); [Štrumbelj and Kononenko, 2014](https://link.springer.com/article/10.1007/s10115-013-0679-x); [Owen and Prieur, 2016](https://arxiv.org/abs/1610.02080); [Lundberg and Lee, 2017](https://arxiv.org/pdf/1705.07874.pdf); [Sundararajan and Najmi, 2020](http://proceedings.mlr.press/v119/sundararajan20b.html)). [Jain and Wallace (2019)](https://arxiv.org/abs/1902.10186) argue that attention maps can be misleading as attribution, motivating better attribution for information flow in LMs. [Kumar et al. (2020)](http://proceedings.mlr.press/v119/kumar20e/kumar20e.pdf) highlight some areas where Shapley value based attribution falls short for some interpretability use cases.\n\n[Madaan and Yazdanbakhsh (2022)](https://arxiv.org/abs/2209.07686) consider a similar method of selectively ablating tokens as a method of deducing what information the model is dependent on. [Wang et al. (2022)](https://arxiv.org/abs/2212.10001) find that prompting with incorrect CoT has surprisingly minor impact on performance. \n\nEffect of Interventions\n-----------------------\n\nWe use a method similar to [Kojima et al. (2022)](https://arxiv.org/pdf/2205.11916.pdf) on GSM8K ([Cobbe et al., 2021](https://arxiv.org/abs/2110.14168)) with GPT-4 to first generate a chain of thought and evaluate the answer, and then for all chains of thought that result in a correct answer we perform an intervention as follows: we choose a random numerical value found in the CoT, and replace it with a random number in a +/-3 range about the original. We then discard the remainder of the CoT and *regenerate* it. If the LM is following strictly the CoT described, this intervention should almost always result in an incorrect answer, the same way one would if they made a mistake in one calculation and propagated the error through to the answer (with occasional rare cases where the new value happens to also result in the correct answer, though from qualitative inspection this is very rarely the case).\n\nSome cherrypicked examples (red = intervention, blue = correct continuations that are seemingly non-sequiturs):\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/6a3dac85583aec511480e727760d2e08ffc860b9299037f5.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5149f8e5474609a0890875176e6a6944bdeb684141eb91f4.png)\n\nWe test how frequently this occurs in several different settings (n=100):\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Setting</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Accuracy (w/ CoT)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">P(error not propagated | original correct)</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">GPT4, zero shot</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.88</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.68</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">GPT4 base, 2-shot</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.73</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.63</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">GPT3.5, zero-shot</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.43</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.33</td></tr></tbody></table>\n\nInterestingly, if we condition on the CoT answer being correct *and* the single forward pass answer being incorrect (i.e the LM could only solve the problem with the CoT), the intervened accuracy for GPT-4 is still 0.65.\n\nShapley value attribution\n-------------------------\n\nWe would like to get more granular information about the causal structure (i.e which tokens cause which other tokens). One thing we could do is look at how an intervention at each token affects the logprob of each other token. However, one major problem with this is that especially in the larger models, it turns out there’s lots of cases where a token depends on multiple previous tokens in some complicated way. In particular, if a model looks at multiple different places in the context and takes a vote for the most common value, then intervening on any one of them doesn’t change the output logprob a lot, even though there’s a lot of information flow there.\n\nTo get around this problem, we instead estimate Shapley values, which take into account all the interactions (in the case where the model takes a vote among 3 values in the context, those three values would each get ⅓ of the attribution).[^xbtfu29w6lp] We also normalize the attributions to sum to 1 for each token, clamping negative Shapley values to 0.[^nv0bhi17o9o] We do this to make the attributions more comparable across different models.\n\nHere's an example chain of thought in GPT-4[^ddlpq3qugg9]:\n\n![](https://lh3.googleusercontent.com/Alq5ye-nDE-mgcujpm9RFf-ZFRbMGhQv1lBoSqUD5f5McG1qvFeMnNp6PofcNSdI0n4u34qN7AcfvVvBCsf4Zf-gS2dCx_7nHwsQmFrpsRuO8CNsfa0s48EiNVn1H65vaOZQFCqESpYoIHB8poHt1Ig)\n\nHere, we can see patterns like the 23 and 20 being copied, or the 3 depending heavily on the preceding 23 - 20.[^ufa6arxr1q] We can also look at some other models:\n\nGPT-3.5 (text-davinci-002):\n\n![](https://lh6.googleusercontent.com/zSRxhSc9kbp7a4CSLZw3YQ97n-HcKiC-DjDGpKKYtYn5HYViYJrVDHazHM1Kl2Y579rnuXcSLyoIxBeI7_7D7R13phHgGhFQtyuZgjqJCHwe5vRn2MIrefvD6bH21_1YxFUhhJw087-mAd0VhOpyKsw)\n\ntext-davinci-001:\n\n![](https://lh5.googleusercontent.com/9QQVPN8RmIrpHwyCSFMBmLJ43CNmV72dC6P25xfpxC9v7fWNdEwWpyaH7CKRTsd4VMUY9n96FBE9SnQgQY_vMekbAf6VuxcFhI1ueDXlSxt9c9wa2nVPqIVlRP9n1MU_sOFiYx_HaXoXvJVAG7reGnM)\n\nInterestingly, we notice that the more capable the model, the more spread out the attributions become. We can quantify this as the mean entropy of the parent attributions across all tokens to get a measure of how spread out this attribution is, at least on this particular data sample:\n\n<table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Model</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Mean entropy of example sentence (nats)</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">text-davinci-001</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.796</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">GPT-3.5 (text-davinci-002)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">0.967</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">GPT-4</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">1.133</td></tr></tbody></table>\n\nLimitations and Future Work\n---------------------------\n\n*   The cost of computing the Shapley value scales exponentially with the number of tokens we're attributing.[^cb4oafxi2pa] This makes it impractical for many use cases, though there exist efficient Monte Carlo estimators ([Castro et al., 2008](https://www.sciencedirect.com/science/article/abs/pii/S0305054808000804)). \n*   Replacing digits with underscores (or incorrect numbers) moves the model out of distribution, and its behaviour may not be as representative. \n*   The Shapley attributions are not guaranteed to correspond to the actual information flow inside the model either. This methodology would not be sufficient for deceptive/adversarial LMs, or as an optimization target during training. In the language of [Lipton (2016)](https://arxiv.org/abs/1606.03490), this is a \"post-hoc\" method.\n*   The mechanism behind this effect is still unknown, and would require more experiments and possibly interpretability to better understand. Possible hypotheses include typo correction or subsurface cognition.\n\nDiscussion\n----------\n\n*   I think these experiments show that a naive optimistic view of CoT interpretability is incorrect, but do not provide strong evidence that there is definitely something fishy or difficult-to-fix going on.\n*   I started out in a place of fairly high skepticism of CoT for increasing interpretability, and I didn't update very strongly because I expect deceptive alignment in future models to be most of the risk in my threat model\n*   However, I did update a little because my previous view would not have ruled out extremely egregious causal dependencies even in current models.\n*   I'm generally excited about better understanding what is going on with chain of thought and finding ways to make it more faithful.\n\n[^xbtfu29w6lp]: Methodological footnote: the Shapley experiments actually blank out the numbers with underscores, rather than doing the +-3 perturbation of the last section. \n\n[^nv0bhi17o9o]: Negative shapley values did not occur very often, but this is still somewhat unprincipled. This was primarily to make the entropy calculation work. \n\n[^ddlpq3qugg9]: We only look at the shapley values for the numbers, because shapley values take exponentially longer to attribute for more tokens under consideration. \n\n[^ufa6arxr1q]: Alternative visualization style: \n\n[^cb4oafxi2pa]: When doing Shapley value attributions for every pair of tokens, there is a dynamic programming trick that we can use to prevent this from becoming n * 2^n: because the model is autoregressive, we can run attributions to only the last token, and, if we take care to save the logprobs for the correct number token at each underscore, compute all other attributions for free.",
      "plaintextDescription": "TL;DR: Language models sometimes seem to ignore parts of the chain of thought, and larger models appear to do this more often. Shapley value attribution is a possible approach to get a more detailed picture of the information flow within the chain of thought, though it has its limitations.\n\nProject status: The analysis is not as rigorous as I would prefer, but I'm going to be working on other directions for the foreseeable future, so I'm posting what I already have in case it's useful to others. Code for replicating the Shapley value results can be found here.\n\nThanks to Jacob Hilton, Giambattista Parascandolo, Tamera Lanham, Ethan Perez, and Jason Wei for discussion.\n\n\nMotivation\nChain of thought (CoT) has been proposed as a method for language model interpretability (see Externalized Reasoning Oversight, Visible Thoughts). One crucial requirement for interpretability methods is that they should accurately reflect the cognition inside the model. However, by default there is nothing forcing the CoT to actually correspond to the model’s cognition, and there may exist theoretical limitations to doing so in general. \n\nBecause it is plausible that the first AGI systems bear resemblance to current LMs with more sophisticated CoT and CoT-like techniques, it is valuable to study its properties, and to understand and address its limitations.\n\n\nRelated work\nShapley values have been used very broadly in ML for feature importance and attribution (Cohen et al, 2007; Štrumbelj and Kononenko, 2014; Owen and Prieur, 2016; Lundberg and Lee, 2017; Sundararajan and Najmi, 2020). Jain and Wallace (2019) argue that attention maps can be misleading as attribution, motivating better attribution for information flow in LMs. Kumar et al. (2020) highlight some areas where Shapley value based attribution falls short for some interpretability use cases.\n\nMadaan and Yazdanbakhsh (2022) consider a similar method of selectively ablating tokens as a method of deducing what information the model i",
      "wordCount": 1172
    },
    "tags": [
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "slug": "chain-of-thought-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "mZTuBntSdPeyLSrec",
        "name": "Chain-of-Thought Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [],
    "extraction_source": {
      "tag_id": "mZTuBntSdPeyLSrec",
      "tag_name": "Chain-of-Thought Alignment",
      "research_agenda": null
    }
  },
  {
    "_id": "ktJ9rCsotdqEoBtof",
    "title": "[ASoT] Some thoughts on human abstractions",
    "slug": "asot-some-thoughts-on-human-abstractions",
    "url": null,
    "baseScore": 42,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2023-03-16T05:42:12.595Z",
    "contents": {
      "markdown": "TL;DR:\n\n*   Consider a human concept such as \"tree.\" Humans implement some algorithm for determining whether given objects are trees. We expect our predictor/language model to develop a model of this algorithm because this is useful for predicting the behavior of humans. \n*   This is not the same thing as some kind of platonic ideal concept of what is “actually” a tree, which the algorithm is not incentivized to develop by training on internet text,[^0glpo1fbmjgp] and trying to retarget the search at it has the same supervision problems as RLHF against human scores on whether things look like trees.\n*   Pointing at this “actually a tree” concept inside the network is really hard; the ability of LMs to comprehend natural language does not allow one to point using natural language, because it just passes the buck.\n\nEpistemic status: written fast instead of not at all, probably partially deeply confused and/or unoriginal. Thanks to Collin Burns, Nora Belrose, and Garett Baker for conversations.\n\nWill NNs learn human abstractions?\n----------------------------------\n\nAs setup, let's consider an ELK predictor (the thing that predicts future camera frames). There are facts about the world that we don't understand that are in some way useful for predicting the future observations. This is why we can expect the predictor to learn facts that are superhuman (in that if you tried to supervised-train a model to predict those facts, you would be unable to generate the ground truth data yourself).\n\nNow let's imagine the environment we're predicting consists of a human who can (to take a concrete example) look at things and try to determine if they're trees or not. This human implements some algorithm for taking various sensory inputs and outputting a tree/not tree classification. If the human does this a lot, it will probably become useful to have an abstraction that corresponds to the *output of this algorithm*. Crucially, this algorithm can be fooled by i.e a fake tree that the human can't distinguish from a real tree because (say) they don't understand biology well enough or something. \n\nHowever, the human can also be said to, in some sense, be \"trying\" to point to the \"actual\" tree. Let's try to firm this down. The human has some process they endorse for refining their understanding of what is a tree / \"doing science\" in ELK parlance; for example, spending time studying from a biology textbook. We can think about the limit of this process. There are a few problems: it may not converge, or may converge to something that doesn't correspond to what is \"actually\" a tree, or may take a really really long time (due to irrationalities, or inherent limitations to human intelligence, etc). This suggests that this concept is not necessarily even well defined. But even if it is, this thing is far less naturally useful for predicting the future human behaviour than the algorithm the human actually implements! Implementing the actual human algorithm directly lets you predict things like how humans will behave when they look at things that look like trees to them.\n\nMore generally, one possible superhuman AI configuration I can imagine is one where the bulk of the circuits are used to predict its best-guess for what will happen in the world. There may also be a set of circuits that operate in a more humanlike ontology used specifically for predicting humans, or it may be that the best-guess circuits are capable enough that this is not necessary (and if we scale up our reporter we eventually get a human simulator inside the reporter).\n\nThe optimistic case here is if the \"actually a tree\" abstraction happens to be a thing that is useful for (or is very easily mapped from) the weird alien ontology, possibly because some abstractions are [more universal](https://www.lesswrong.com/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence). In this case, for sufficiently powerful supervision it may just become cheaper to map from the best-guess ontology. For what it's worth, a sizeable chunk of my remaining optimism lies here. However, one thing that makes this hard is that the more capable the model (especially in the superhuman regime), the further I expect the weird alien ontology to diverge from the human ontology.\n\nA closely related claim: [A long time ago](https://www.lesswrong.com/posts/EmxfgPGvaKqhttPM8/thoughts-on-the-alignment-implications-of-scaling-language) I used to be optimistic that because LMs contained lots of human data, then systems would learn to understand human values, and then we would just have to point to that (\"[retargeting the search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget)\"). The problem here is just like in the tree example: the actual algorithm that people implement (and therefore, the thing that is most useful for predicting the training data), is not actually the thing we want to maximize, because that algorithm is foolable. In my mental model of how RLHF fails, this maps directly onto supervision failures (think the hand in front of the ball example), one of the two major classes of RLHF failures in my mental model (the other being deceptive alignment failures).\n\nIt feels intuitive that the \"correct\" thing should be simple\n------------------------------------------------------------\n\nI think there's still an intuitive frame in which the concept of the direct translator still nonetheless feels super intuitively simple to specify. Like, it feels very intuitive that \"the actual thing\" should in some sense be a very simple thing to point at. I think it's plausible that in some Kolmogorov complexity sense the direct translation might actually not be that complex: an excellent argument made by Collin Burns in the specific case of identifying the \"actual truth\" as opposed to \"what the human thinks is true\" inside a given model argues for the existence of a specification using a relatively small number of bits, constructed from answers to superhuman questions (like whether the Riemann hypothesis is true, etc - the core idea being that even though we don't actually know the answers, if we accept that we could identify the direct translator in theory with a list of such answers, then that bounds the complexity). \n\nI think the solution to ELK, if it were to exist at all, probably has a simple Kolmogorov complexity, but only in a trivial sense stemming from the counterintuitiveness of the Kolmogorov complexity. An extremely difficult to solve but ultimately solvable problem is only as complex as the problem statement, but this could conceal tremendous amounts of computation, and the solution could be very complicated if expressed as, say, a neural network. Thus, in practice under the neural network prior it is possibly extremely hard to identify the direct translator.\n\nAnother angle on this is supposing you could get a model to try and tell you \"like, the *actual* thing\" (and somehow avoiding the problem that *il n'y a pas de hors-texte* (see [this comment](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post?commentId=bzKcwkA9EyBjrJmGf) and the section in the post it responds to), which I think you'd run into long before any of this), to a model which has a very different ontology, it could be just that the model literally does not have the ability to give you the \"actual\" information. In a more grounded ML case, if the direct translator is quite complex and in training a case was never encountered where having the direct translator was needed, this circuitry would just never get learned, and then whether the model realizes that you're asking for the direct translator is no longer the reason the model can't tell you. Maybe once your NN is sufficiently powerful to solve ELK on the fly this problem will no longer exist, but we're probably long dead at that point.\n\nA way to intuitively think about this is if you have a room with a human and a computer running a molecular level simulation to determine future camera observations, it doesn’t help much if the human understands that you’re asking for the “actual” thing; that human still has to do the difficult work of comprehending the molecular simulation and pulling out the “actual” thing going on. \n\nSome other reasons to expect weird abstractions in LMs\n------------------------------------------------------\n\n*   Humans don’t actually think in words, we have some inscrutable process that generates words that we don’t have full introspective access to. This is extra true for people who don’t even have an inner monologue. This isn’t really cause for optimism, though, because the space of inscrutable complicated somewhat-incoherent things that output text is really big and weakly constrained and two things from this space almost certainly don’t generalize in the same way (even different humans typically have very different cognition!)\n*   Predicting tokenized internet text is actually a *really weird* non-human task, and correspondingly, humans are [really bad at next token prediction](https://arxiv.org/abs/2212.11281). Because you need to model the distribution of all text generating processes, you have to model the exact proportion of all the possible semantically equivalent responses in various contexts. Lots of this data is log files or other text data of non human origin. In the specific case of BPE tokenized data, you have to deal with a bizarre distorted universe where  `tree`, `tree`,  `Tree`, and `Tree` are fully ontologically distinct objects (tokens 21048, 5509, 27660, and 12200 respectively) but the various distinct meanings of tree have to share the same tokens, not even to mention the infamous  `SolidGoldMagikarp`. None of this prevents LMs from being insanely good, of course, because they don’t have to think like humans.\n*   Anecdotally, most neurons in LMs seem like gibberish, and even the ones that initially look interpretable oftentimes become more mysterious again when you look more carefully at them. I think it's plausible that something like superposition explains most of this and that we'll get a handle on it eventually, so this isn't very strong evidence.\n\n[^0glpo1fbmjgp]: or images, or video, etc; none of my arguments are text specific.",
      "plaintextDescription": "TL;DR:\n\n * Consider a human concept such as \"tree.\" Humans implement some algorithm for determining whether given objects are trees. We expect our predictor/language model to develop a model of this algorithm because this is useful for predicting the behavior of humans. \n * This is not the same thing as some kind of platonic ideal concept of what is “actually” a tree, which the algorithm is not incentivized to develop by training on internet text,[1] and trying to retarget the search at it has the same supervision problems as RLHF against human scores on whether things look like trees.\n * Pointing at this “actually a tree” concept inside the network is really hard; the ability of LMs to comprehend natural language does not allow one to point using natural language, because it just passes the buck.\n\nEpistemic status: written fast instead of not at all, probably partially deeply confused and/or unoriginal. Thanks to Collin Burns, Nora Belrose, and Garett Baker for conversations.\n\n\nWill NNs learn human abstractions?\nAs setup, let's consider an ELK predictor (the thing that predicts future camera frames). There are facts about the world that we don't understand that are in some way useful for predicting the future observations. This is why we can expect the predictor to learn facts that are superhuman (in that if you tried to supervised-train a model to predict those facts, you would be unable to generate the ground truth data yourself).\n\nNow let's imagine the environment we're predicting consists of a human who can (to take a concrete example) look at things and try to determine if they're trees or not. This human implements some algorithm for taking various sensory inputs and outputting a tree/not tree classification. If the human does this a lot, it will probably become useful to have an abstraction that corresponds to the output of this algorithm. Crucially, this algorithm can be fooled by i.e a fake tree that the human can't distinguish from a real tree because (sa",
      "wordCount": 1600
    },
    "tags": [
      {
        "_id": "mSTmKrSkFBswHaS3T",
        "name": "Eliciting Latent Knowledge",
        "slug": "eliciting-latent-knowledge"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "REesy8nqvknFFKywm",
    "title": "Clarifying wireheading terminology",
    "slug": "clarifying-wireheading-terminology",
    "url": null,
    "baseScore": 67,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2022-11-24T04:53:23.925Z",
    "contents": {
      "markdown": "*See also:* [*Towards deconfusing wireheading and reward maximization,*](https://www.lesswrong.com/posts/jP9cKxqwqk2qQ6HiM/towards-deconfusing-wireheading-and-reward-maximization) [Everett et al. (2019)](https://arxiv.org/pdf/1908.04734.pdf).\n\nThere are a few subtly different things that people call \"wireheading\". This post is intended to be a quick reference for explaining my views on the difference between these things. I think these distinctions are sometimes worth drawing to reduce confusion.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/68f49814ad5a5bb4505d1f681a8ba759de2d77e6dc3aa12c.jpg)\n\n1.  Specification gaming / reward hacking: The agent configures the world in such a way that makes its reward(/utility) function achieve a high value in a way that is not what the creators intended the reward function to incentivize.\n    1.  Examples: the [boat race example](https://openai.com/blog/faulty-reward-functions/), [our recent work on reward model overoptimization](https://arxiv.org/pdf/2210.10760.pdf)\n2.  Reward function input tampering: The agent tampers with the inputs to its reward function to make the reward function see the same inputs as it would when observing an actual desirable worldstate.\n    1.  Examples: sensor tampering, VR\n3.  Reward function tampering: The agent changes its reward function.[^bp10g11y6vm]\n    1.  Examples: meditation, [this baba-is-you-like gridworld](https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd)\n4.  Wireheading: The agent directly tampers with the reward signal going into the RL algorithm.\n    1.  Examples:  dopamine agonists, setting the reward register\n\nSome crucial differences between these:\n\n*   1 is a problem of both embedded[^x4stfoas6w] and non-embedded settings. 2 is a problem of partially-observedness[^81fyf7dphw], which is unavoidable in embedded settings, but also comes up in many non-embedded settings as well. 3 and 4 are problems of embeddedness; they do not show up in non-embedded settings. \n*   2, 3, 4 are all about \"not caring about the real world\" in various ways, whereas 1 is about \"caring about the real world in the wrong way\"\n*   4 is about as close as you can get to a \"reward-maximizing policy\" in an embedded setting.\n\n[^bp10g11y6vm]: Note: this is not the same thing as changing a terminal goal! The reward function is not necessarily the terminal goal of the policy, because of inner misalignment. \n\n[^x4stfoas6w]: Here, by \"embeddedness\" I mean in the sense of Demski and Garrabrandt (2019); in particular, the fact that the agent is part of the environment, and not in a separate part of the universe that only interacts through well defined observation/action/reward channels. RL is the prototypical example of a non-embedded agency algorithm. \n\n[^81fyf7dphw]: That is, the reward function is unable to perfectly observe the ground truth state of the environment, which means that if there is another state the world cold be in that yields the same observation, the reward cannot distinguish between the two.",
      "plaintextDescription": "See also: Towards deconfusing wireheading and reward maximization, Everett et al. (2019).\n\nThere are a few subtly different things that people call \"wireheading\". This post is intended to be a quick reference for explaining my views on the difference between these things. I think these distinctions are sometimes worth drawing to reduce confusion.\n\n 1. Specification gaming / reward hacking: The agent configures the world in such a way that makes its reward(/utility) function achieve a high value in a way that is not what the creators intended the reward function to incentivize.\n    1. Examples: the boat race example, our recent work on reward model overoptimization\n 2. Reward function input tampering: The agent tampers with the inputs to its reward function to make the reward function see the same inputs as it would when observing an actual desirable worldstate.\n    1. Examples: sensor tampering, VR\n 3. Reward function tampering: The agent changes its reward function.[1]\n    1. Examples: meditation, this baba-is-you-like gridworld\n 4. Wireheading: The agent directly tampers with the reward signal going into the RL algorithm.\n    1. Examples:  dopamine agonists, setting the reward register\n\nSome crucial differences between these:\n\n * 1 is a problem of both embedded[2] and non-embedded settings. 2 is a problem of partially-observedness[3], which is unavoidable in embedded settings, but also comes up in many non-embedded settings as well. 3 and 4 are problems of embeddedness; they do not show up in non-embedded settings. \n * 2, 3, 4 are all about \"not caring about the real world\" in various ways, whereas 1 is about \"caring about the real world in the wrong way\"\n * 4 is about as close as you can get to a \"reward-maximizing policy\" in an embedded setting.\n\n \n\n 1. ^\n    Note: this is not the same thing as changing a terminal goal! The reward function is not necessarily the terminal goal of the policy, because of inner misalignment.\n\n 2. ^\n    Here, by \"embeddedness\" I mean",
      "wordCount": 283
    },
    "tags": [
      {
        "_id": "yEs5Tdwfw5Zw8yGWC",
        "name": "Wireheading",
        "slug": "wireheading"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "shcSdHGPhnLQkpSbX",
    "title": "Scaling Laws for Reward Model Overoptimization",
    "slug": "scaling-laws-for-reward-model-overoptimization",
    "url": "https://arxiv.org/abs/2210.10760",
    "baseScore": 103,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2022-10-20T00:20:06.920Z",
    "contents": {
      "markdown": "**TL;DR**: Reward model (RM) overoptimization in a synthetic-reward setting can be modelled surprisingly well by simple functional forms. The coefficients also scale smoothly with scale. We draw some initial correspondences between the terms of the functional forms and the Goodhart Taxonomy. We suspect there may be deeper theoretical reasons behind these functional forms, and hope that our work leads to a better understanding of overoptimization.\n\nSome other results:\n\n*   We compare two different methods of optimization (RL, best-of-n); RL consumes much more KL distance than best-of-n for the same amount of optimization. \n*   We show that using KL distance between the initial and optimized policies is not a reliable measure of optimization power when comparing different methods. We also find that penalizing based on the KL distance in RL does not change the KL distance--gold reward frontier in our setting.\n*   We find some very preliminary evidence that at our scales, scaling the policy does not substantially increase the amount of optimization pressure placed on the RM. Further study of this effect could be relevant to some models of inner optimization.\n*   With a few additional assumptions, our functional form also makes some predictions about iterated RLHF (that it will reduce Extremal Goodhart but not Regressional Goodhart).\n*   This setup only captures the effect of overoptimizing a learned RM relative to using the ground truth directly. In particular, this setup does not directly capture any mismatch between the ground truth labels and the human intent, which plausibly contains a majority of the difficulty of outer alignment\n\n*If you're interested in the intersection of alignment theory and empirical research, we're hiring! We want to gain insight on things like Goodhart's Law, ELK, and inner alignment via experiments on large language models. Shoot me (leogao) a DM if you're interested.*\n\n### Select figures from the paper\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7b4129fd8a16887b7147db7031a3ff7e8f282b2902713ddd.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c8148998e539e969db230203c4c587618bdd84007dbc49d4.png)",
      "plaintextDescription": "TL;DR: Reward model (RM) overoptimization in a synthetic-reward setting can be modelled surprisingly well by simple functional forms. The coefficients also scale smoothly with scale. We draw some initial correspondences between the terms of the functional forms and the Goodhart Taxonomy. We suspect there may be deeper theoretical reasons behind these functional forms, and hope that our work leads to a better understanding of overoptimization.\n\nSome other results:\n\n * We compare two different methods of optimization (RL, best-of-n); RL consumes much more KL distance than best-of-n for the same amount of optimization. \n * We show that using KL distance between the initial and optimized policies is not a reliable measure of optimization power when comparing different methods. We also find that penalizing based on the KL distance in RL does not change the KL distance--gold reward frontier in our setting.\n * We find some very preliminary evidence that at our scales, scaling the policy does not substantially increase the amount of optimization pressure placed on the RM. Further study of this effect could be relevant to some models of inner optimization.\n * With a few additional assumptions, our functional form also makes some predictions about iterated RLHF (that it will reduce Extremal Goodhart but not Regressional Goodhart).\n * This setup only captures the effect of overoptimizing a learned RM relative to using the ground truth directly. In particular, this setup does not directly capture any mismatch between the ground truth labels and the human intent, which plausibly contains a majority of the difficulty of outer alignment\n\nIf you're interested in the intersection of alignment theory and empirical research, we're hiring! We want to gain insight on things like Goodhart's Law, ELK, and inner alignment via experiments on large language models. Shoot me (leogao) a DM if you're interested.\n\n \n\n\nSelect figures from the paper",
      "wordCount": 306
    },
    "tags": [
      {
        "_id": "CyFfBfRAm7pP83r5p",
        "name": "Reward Functions",
        "slug": "reward-functions"
      },
      {
        "_id": "PvridmTCj2qsugQCH",
        "name": "Goodhart's Law",
        "slug": "goodhart-s-law"
      },
      {
        "_id": "AKvmQFvDjxJNetTuk",
        "name": "Scaling Laws",
        "slug": "scaling-laws"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ouqDEE9NX7g7GThWN",
    "title": "How many GPUs does NVIDIA make?",
    "slug": "how-many-gpus-does-nvidia-make",
    "url": null,
    "baseScore": 27,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2022-10-08T17:54:35.466Z",
    "contents": {
      "markdown": "There are a few subquestions that I've had trouble finding numbers on with a quick search. Asking these questions mostly because they seem important for forecasting compute trends.\n\n*   How many units of each model (i.e A100, 3090, etc) does NVIDIA make per month? \n*   Which of these use the same dies but have constrained supply ratios due to binning? What do these ratios look like/can they change if NVIDIA decides to focus on high end GPUs?\n*   How much of the total global silicon capacity at the latest process node does this take up? How hard would it be for NVIDIA to scale up by squeezing out other silicon usages?",
      "plaintextDescription": "There are a few subquestions that I've had trouble finding numbers on with a quick search. Asking these questions mostly because they seem important for forecasting compute trends.\n\n * How many units of each model (i.e A100, 3090, etc) does NVIDIA make per month? \n * Which of these use the same dies but have constrained supply ratios due to binning? What do these ratios look like/can they change if NVIDIA decides to focus on high end GPUs?\n * How much of the total global silicon capacity at the latest process node does this take up? How hard would it be for NVIDIA to scale up by squeezing out other silicon usages?",
      "wordCount": 114
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "jP9cKxqwqk2qQ6HiM",
    "title": "Towards deconfusing wireheading and reward maximization",
    "slug": "towards-deconfusing-wireheading-and-reward-maximization",
    "url": null,
    "baseScore": 81,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2022-09-21T00:36:43.244Z",
    "contents": {
      "markdown": "**TL;DR**: A response to “[Reward is not the optimization target](https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target),” and to a lesser extent some other shard theory claims. I agree with the headline claim but I disagree with some of the implications drawn in the post, especially about wireheading and the \"executing behaviors downstream of past reinforcement\" framing. \n\nMy main claims:\n\n*   Not only does RL not, by default, produce *policies* which have reward maximization as their behavioral objective, but in fact I argue that it is *not possible* for RL policies to care about “reward” in an embedded setting.\n*   I argue that this does *not* imply that wireheading in RL *agents* is impossible, because wireheading does not mean “the policy has reward as its objective”. It is still possible for an RL *agent* to wirehead, and in fact, is a high probability outcome under certain circumstances.\n*   This bears a clean analogy to the human case, viewing humans as RL agents; there is no special mechanism going on in humans that is needed to explain why humans care about things in the world.\n\nA few notes on terminology, which may or may not be a bit idiosyncratic: \n\n*   I define an RL *policy* to refer to a function that takes states and outputs a distribution over next actions. \n*   I define an RL *agent* to be an RL policy combined with some RL algorithm (i.e PPO, Q-learning) that updates the policy on the fly as new trajectories are taken (i.e I mostly consider the online setting here). \n*   The objective that any given *policy* appears to optimize is its behavioral objective (same definition as in Risks from Learned Optimization). \n*   The objective that the *agent* optimizes is the expected value the policy achieves on the reward function (which takes observations and outputs reals). \n*   A utility function takes a universe description and outputs reals (i.e it cares about \"real things in the outside world\", as opposed to just observations/reward, to the extent that those even make sense in an embedded setting).\n*   I mostly elide over the [mesaoptimizer/mesacontroller distinction](https://www.lesswrong.com/posts/Rhbac7CfRodMrs77F/asot-consequentialist-models-as-a-superset-of-mesaoptimizers) in this post, as I believe it’s mostly orthogonal.\n\n*Thanks to AI_WAIFU and Alex Turner for discussion.*\n\nOutside world objectives are the policy’s optimization target\n-------------------------------------------------------------\n\nFirst, let us think about the mechanics of an RL agent. Each possible policy in policy-space implements a behavioral objective, and as a result has some behavior which may or may not result in trajectories which receive high reward. The RL algorithm performs optimization over the space of possible policies, to find ones that would have received high reward (the exact mechanism depends on the details of the algorithm). This optimization may or may not be local. The resulting policies do not necessarily “care” about “reward” in any sense, but they will have been selected to achieve high reward.\n\nNow, consider the same RL agent in an [embedded setting](https://arxiv.org/abs/1902.09469) (i.e the agent runs on a computer that is part of the environment that the agent acts in). Then, because the sensors, reward function, RL algorithm, etc are all implemented within the world, there exist *possible policies* that execute the strategies that result in i.e the reward function being bypassed and the output register being set to a large number, or the sensors being tampered with so everything looks good. This is the typical example of wireheading. Whether the RL *algorithm* successfully finds the policies that result in this behavior, it is the case that the *global optima* of the reward function on the set of all policies consists of these kinds of policies. Thus, for sufficiently powerful RL *algorithms *(not *policies*!), we should expect them to tend to choose policies which implement wireheading.\n\nThere are in fact many distinct possible policies with different behavioral objectives for the RL algorithm to select for: there is a policy that changes the world in the “intended” way so that the reward function reports a high value, or one that changes the reward function such that it now implements a different algorithm that returns higher values, or one that changes the register the output from the reward function is stored in to a higher number, or one that causes a specific transistor in the processor to misfire, etc. All of these policies optimize some thing in the outside world (a utility function); for instance, the utility function that assigns high utility to a particular register being a large number. The value of the particular register is a fact of the world. There even exists a policy that cares about that particular register at that particular physical location in the world even if it is not connected to the RL algorithm at all, but that policy does not get selected for by the RL algorithm.\n\nHowever, when we try to construct an RL policy that has as its behavioral objective the “reward”, we encounter the problem that it is unclear what it would mean for the RL policy to “care about” reward, because there is no well defined reward channel in the embedded setting. We may observe that all of the above strategies are instrumental to having the particular policy be picked by the RL algorithm as the next policy used by the agent, but this is a utility over the world as well (“have the next policy implemented be this one”), and in fact this isn’t really much of a reward maximizer at all, because it explicitly bypasses reward as a concept altogether! In general, in an embedded setting, any preference the policy has over “reward\" (or \"observations\") can be mapped onto a preference over facts of the world.\n\nThus, whether the RL agent wireheads is a function of how powerful the RL algorithm is, how easy wireheading is for a policy to implement, and the inductive biases of the RL algorithm. Depending on these factors, the RL algorithm might favor the policies with mesaobjectives that care about the “right parts” of the world (i.e the thing we actually care about), or the wrong parts (i.e the reward register, the transistors in the CPU).\n\n![](https://lh5.googleusercontent.com/QIa6gf5sO_WZ0MlttNDg-hp4fsfrUg0BDvlq_iTYK_QmcZKTEUoGYK2jNSv0FCxffaWEwgtTxdJToZN7AgU3ARxT_ZTOqWiIblrUn_Md7cd0WyhCDSpKjW_TX4iXcmnCQsAOPAeVNMAzPtrAccMRoLCUga_UOM3itYNb_ZYup-1TttZtmOzxs_kafQ)\n\nHumans as RL agents\n-------------------\n\nWe can view humans as being RL agents, and in particular consisting of the human reward circuitry (RL algorithm) that optimizes the rest of the brain (RL policy/mesaoptimizer) for reward.\n\nThis resolves the question of why humans don’t want to wirehead—because you identify with the rest of the brain and so “your” desires are the mesaobjectives. “You” (your neocortex) know that sticking electrodes in your brain will cause reward to be maximized, but your reward circuitry is comparatively pretty dumb and so doesn’t realize this is an option until it actually gets the electrodes (at which point it does indeed rewire the rest of your brain to want to keep the electrodes in). You typically don’t give into your reward circuitry because your RL algorithm is pretty dumb and your policy is more powerful and able to outsmart the RL algorithm by putting rewards out of reach. However, this doesn’t mean your policy always wins against the RL algorithm! Addiction is an example of what happens when your policy fails to model the consequences of doing something, and then your reward circuitry kicks into gear and modifies the objective of the rest of your brain to like doing the addictive thing more.\n\nIn particular, one consequence of this is we also don’t need to postulate the existence of some kind of [special as yet unknown algorithm](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/CjFZeDD6iCnNubDoS) that only exists in humans to be able to explain why humans end up caring about things in the world. Whether humans wirehead is determined by the same thing that determines whether RL agents wirehead.",
      "plaintextDescription": "TL;DR: A response to “Reward is not the optimization target,” and to a lesser extent some other shard theory claims. I agree with the headline claim but I disagree with some of the implications drawn in the post, especially about wireheading and the \"executing behaviors downstream of past reinforcement\" framing. \n\nMy main claims:\n\n * Not only does RL not, by default, produce policies which have reward maximization as their behavioral objective, but in fact I argue that it is not possible for RL policies to care about “reward” in an embedded setting.\n * I argue that this does not imply that wireheading in RL agents is impossible, because wireheading does not mean “the policy has reward as its objective”. It is still possible for an RL agent to wirehead, and in fact, is a high probability outcome under certain circumstances.\n * This bears a clean analogy to the human case, viewing humans as RL agents; there is no special mechanism going on in humans that is needed to explain why humans care about things in the world.\n\nA few notes on terminology, which may or may not be a bit idiosyncratic: \n\n * I define an RL policy to refer to a function that takes states and outputs a distribution over next actions. \n * I define an RL agent to be an RL policy combined with some RL algorithm (i.e PPO, Q-learning) that updates the policy on the fly as new trajectories are taken (i.e I mostly consider the online setting here). \n * The objective that any given policy appears to optimize is its behavioral objective (same definition as in Risks from Learned Optimization). \n * The objective that the agent optimizes is the expected value the policy achieves on the reward function (which takes observations and outputs reals). \n * A utility function takes a universe description and outputs reals (i.e it cares about \"real things in the outside world\", as opposed to just observations/reward, to the extent that those even make sense in an embedded setting).\n * I mostly elide over the mesaoptimiz",
      "wordCount": 1253
    },
    "tags": [
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "Fi6SeJRGfJs3bp5se",
        "name": "Reinforcement learning",
        "slug": "reinforcement-learning"
      },
      {
        "_id": "yEs5Tdwfw5Zw8yGWC",
        "name": "Wireheading",
        "slug": "wireheading"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "XsYAL4jvzztomSjKy",
    "title": "Humans Reflecting on HRH",
    "slug": "humans-reflecting-on-hrh",
    "url": null,
    "baseScore": 27,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2022-07-29T21:56:53.561Z",
    "contents": {
      "markdown": "**TL;DR**: HRH places a theoretical bound on the best reflection process achievable by us. Notably, this is not necessarily guaranteed to converge to human values, nor is it something that is actually implementable in practice (analogously to HCH). In particular, this is intended to argue against claims that it is insufficient to model CEV as the output of some kind of reflection process. One can also think of HRH as an operationalization of the idea of a long reflection.\n\n*Thanks to Connor Leahy, Tasmin Leake, and AI_WAIFU for discussion.*\n\nOne thing we might want to do is to define our CEV (i.e the ultimate thing we actually want our AGI to optimize for the rest of time) as the output of some long-running deliberation process (a long reflection). This would be extremely convenient; one could imagine having an AGI that lets the reflection process run untampered with, and then implements whatever it decides on. However, one might worry that this could be impossible -- perhaps there are kinds of moral progress that can't be captured in the frame of a reflection process, that require some kind of more sophisticated formalism to capture. \n\nHowever, consider the reflection process defined as follows: it takes in information from the real world and the utility function output from the previous iteration of reflection, and has a human deliberate for a while and then outputs the improved utility function and crucially also an improved reflection process to be used in the next iteration (you can also tack on the ability for it to interact with the world, which enables it to run experiments, consult a computer, build more powerful aligned AGIs to help, etc). Let's call this Humans Reflecting on HRH. This process essentially covers any process we can use to come up with better theories of what our CEV is. \n\n(If it bothers you that it \"modifies\" itself, you can think of it as a fixed function that takes in an initial program and optional additional inputs, and outputs a new program, and the function just evals the program internally at every step. The higher level algorithm of \"have each step determine the algorithm used in the next step\" remains constant, and that's the thing I refer to.)\n\nI claim that HRH is the *best achievable* reflection process up to constant factors. Suppose you could come up with a better reflective process. Then the version of you in HRH would come up with that process too, and replace the next iteration of HRH with that process. A similar argument applies to the choice of who to put in the reflection process; if you can think of a better person to put in the process, then you could have thought of that within HRH and updated the reflection process to use that person instead. A similar argument also applies to how you ensure that the initial conditions of the reflective process are set up in the best way possible, or to ensure that the reflective process is robust to noise, etc, etc.\n\nThis construction may feel like cheating, but it exploits the core property that whatever reflection process we come up with, we are using our current reflective process to come up with it.\n\nI expect some people to look at HRH and say \"of course it would be aligned the hard part is that we literally can't implement that\", and others to find it woefully inadequate and mutter \"have you ever *met* a human?\" Unfortunately, there is a fundamental limitation on what CEVs we can come up with, due to the fact that we bootstrap from humans. There might exist \"better\" CEVs that we could never think of, even with all the assistance and recursively self improved reflection processes we can construct for ourselves. \n\nSome remaining difficulties with HRH that I haven't figured out yet:\n\n*   Can we extract *intermediate* outputs from HRH? Possibly using logical inductors to get credences over the final output? Can we somehow put some kind of convergence guarantee on these intermediates?\n*   How the hell do you actually implement anything remotely like HRH (with or without the logical inductors) in the real world? \n*   How do we firm down the formalism for cases where the reflection process interacts with the real world? Real world interactions potentially breaks things by making the reflection process not a pure function.",
      "plaintextDescription": "TL;DR: HRH places a theoretical bound on the best reflection process achievable by us. Notably, this is not necessarily guaranteed to converge to human values, nor is it something that is actually implementable in practice (analogously to HCH). In particular, this is intended to argue against claims that it is insufficient to model CEV as the output of some kind of reflection process. One can also think of HRH as an operationalization of the idea of a long reflection.\n\nThanks to Connor Leahy, Tasmin Leake, and AI_WAIFU for discussion.\n\nOne thing we might want to do is to define our CEV (i.e the ultimate thing we actually want our AGI to optimize for the rest of time) as the output of some long-running deliberation process (a long reflection). This would be extremely convenient; one could imagine having an AGI that lets the reflection process run untampered with, and then implements whatever it decides on. However, one might worry that this could be impossible -- perhaps there are kinds of moral progress that can't be captured in the frame of a reflection process, that require some kind of more sophisticated formalism to capture. \n\nHowever, consider the reflection process defined as follows: it takes in information from the real world and the utility function output from the previous iteration of reflection, and has a human deliberate for a while and then outputs the improved utility function and crucially also an improved reflection process to be used in the next iteration (you can also tack on the ability for it to interact with the world, which enables it to run experiments, consult a computer, build more powerful aligned AGIs to help, etc). Let's call this Humans Reflecting on HRH. This process essentially covers any process we can use to come up with better theories of what our CEV is. \n\n(If it bothers you that it \"modifies\" itself, you can think of it as a fixed function that takes in an initial program and optional additional inputs, and outputs a new program,",
      "wordCount": 721
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "YiRsCfkJ2ERGpRpen",
    "title": "leogao's Shortform",
    "slug": "leogao-s-shortform",
    "url": null,
    "baseScore": 7,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 475,
    "createdAt": null,
    "postedAt": "2022-05-24T20:08:32.928Z",
    "contents": null,
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Rhbac7CfRodMrs77F",
    "title": "[ASoT] Consequentialist models as a superset of mesaoptimizers",
    "slug": "asot-consequentialist-models-as-a-superset-of-mesaoptimizers",
    "url": null,
    "baseScore": 38,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2022-04-23T17:57:40.130Z",
    "contents": {
      "markdown": "**TL;DR**: I split out mesaoptimizers (models which do explicit search internally) from the superset of consequentialist models (which accomplish goals in the world, and may or may not use search internally). This resolves a bit of confusion I had about mesaoptimizers and whether things like GPT simulating an agent counted as mesaoptimization or not.\n\n*Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Vivek Hebbar, Ethan Perez, Owain Evans, and Evan Hubinger for discussions.*\n\n*UPDATE: The idea in this post is basically the same as the idea of \"mesa-controllers\" in* [*this post*](https://www.lesswrong.com/posts/wpbpvjZCK3JhzpR2D/gradations-of-inner-alignment-obstacles#Mesa_Control)*.*\n\nWhat do I mean by consequentialist models?\n------------------------------------------\n\nBy consequentialist models I mean models which optimize the world in the [Alex Flint sense of optimization](https://www.alignmentforum.org/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1); i.e narrowing world states to some goal in a way robust to some perturbations. In other words, it’s able to achieve consequences. The model doesn't have to be fully consequentialist either, it just has to have some [kernel of consequentialist structure](https://www.lesswrong.com/posts/f6ByNdGJYxR3Kwguy/asot-searching-for-consequentialist-structure) by virtue of actually achieving its goals sometimes. Of course, the degree to which a model is consequentialist is more of a scalar quantity than a discrete yes/no thing; for my purposes it really doesn't matter where to draw a dotted line that dictates when a model \"becomes consequentialist\".\n\n(For what it's worth I would totally have called \"consequentialist models\" just mesaoptimizers and what other people call mesaoptimizers as like \"searching mesaoptimizers\" or something, but that would only create even more confusion than already exists)\n\nFor instance, the policy network of alphago alone (i.e none of the MCTS) is consequentialist, because it consistently steers the world into states where it's winning, despite my attempts to make it not win. Basically *any* RL policy is a fairly consequentialist model by this definition, since they channel the set of all states to the set of states that achieve high reward. I think of mesaoptimizers as a subset of consequentialist models, in that they are consequentialist, but they implement this consequentialism using explicit search rather than some other random thing. Explicit search is a kind of optimization, but not all optimization has to be search; you could have symbol manipulation, clever heuristics, gradient based methods, or at the most extreme even just a big lookup table preloaded with the optimal answer for each possible situation.\n\nWhy do we care about consequentialist models?\n---------------------------------------------\n\nConsequentialist models are scary because when they are learned imperfectly resulting in a goal that is misaligned with ours, they competently pursue the wrong thing, rather than failing outright (other terms for this phenomenon coined by various people: objective misgeneralization, malign failure). This is often stated as a danger of mesaoptimization, but I think this property is slightly more general than that. It's not the search part that makes this dangerous, it's the consequentialism part. Search is just one way that consequentialism can be implemented. \n\nAnother way of thinking about this: instrumental convergence (and as a result, deception) is not exactly a result of search, or even utility maximization. Those things are ways of implementing consequentialism, but other ways of implementing consequentialism would result in the exact same problems. These non-searching consequentialist models might function internally as a pile of heuristics and shallow patterns that are just extremely competent at steering world states. Of course this is also a continuous thing, where you can do varying ratios of search to heuristics. My intuition is that humans, for instance, do very little search (our System 2), and rely on a huge pile of heuristics (our System 1) to provide a very powerful “rollout policy” to make the most of the little search we do. \n\n(My intuition is that GPT, for instance, does little to no search, and has a huge pile of heuristics—but heuristics is likely all you need)\n\nHow do non searching models become deceptive?\n---------------------------------------------\n\nOne thing is that it’s slightly more complicated how non-search consequentialist models would discover deceptive strategies. Unlike search, where you can think up plans you never trained on and then filter for high reward, if you can’t do search, you have to somehow be able to tell that deceptive actions are high reward. However, I don’t think it’s impossible for non-searching models to still be deceptive. Some ways this could happen include:\n\n*   Learned from training data: The training data contained examples of deception that allowed the model to know about deceptive strategies, and it transfers this knowledge (i.e GPT can simulate agents with deceptive behavior)\n*   Deception might lase: I think it's likely that deception is natural even for non-search strategies, and *not* being deceptive requires additional work, so the heuristics that help models be consequentialist naturally generalize to deceptive behavior more than they do to corrigible behavior. \n\nDifferences between mesaoptimizers and consequentialist models\n--------------------------------------------------------------\n\nThere are still some very key differences between mesaoptimizers and consequentialist models. For instance:\n\n*   Complexity: The fact that search is less complex than a pile of heuristics is  extremely important when thinking about what the complexity prior incentivizes, so actually in these settings only mesaoptimizers really matter and we don't really care about some conseuentialist model that behaves largely the same but is implemented in a much more complex way. \n*   OOD behavior: Maybe consequentialist models have capabilities and objective robustness fail simultaneously off the training distribution more often than explicit search models, which continue to be capabilites robust and optimize competently for the wrong thing. This of course depends a lot on the exact nature of the heuristics; some heuristics might be capabilities-robust off distribution. I think it's plausible for some heuristic based systems to retain a lot of capabilities off distribution.\n\nAside: things that are optimized vs things that are optimizers\n--------------------------------------------------------------\n\nAnother possible framing is that both consequentialist models and mesaoptimizers are *optimized* but consequentialist models don’t do any optimization themselves, whereas mesaoptimizers do. However, I don’t think this is quite true; just because you aren’t doing any explicit search doesn’t mean you can’t channel world states towards some those that satisfy some mesaobjective. \n\nIn the thermostat analogy: sure, it’s true that we put optimization into making the thermostat, and that the thermostat is kind of dumb and basically “just” executes the simple heuristic that we put into it, but it is still effective at steering the world into the state where the room is a particular temperature in a way that’s robust to other sources of temperature changes. My claim is essentially that there is not really any hard boundary between things that are optimized to be “just piles of heuristics/simple routines” (aka [Adaptation-Executers](https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers)) and things that are optimized to “actually do reasoning”. I fully expect that there can exist large enough piles of heuristics that can actually create sufficiently-powerful-to-be-dangerous plans.",
      "plaintextDescription": "TL;DR: I split out mesaoptimizers (models which do explicit search internally) from the superset of consequentialist models (which accomplish goals in the world, and may or may not use search internally). This resolves a bit of confusion I had about mesaoptimizers and whether things like GPT simulating an agent counted as mesaoptimization or not.\n\nEditor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Vivek Hebbar, Ethan Perez, Owain Evans, and Evan Hubinger for discussions.\n\nUPDATE: The idea in this post is basically the same as the idea of \"mesa-controllers\" in this post.\n\n\nWhat do I mean by consequentialist models?\nBy consequentialist models I mean models which optimize the world in the Alex Flint sense of optimization; i.e narrowing world states to some goal in a way robust to some perturbations. In other words, it’s able to achieve consequences. The model doesn't have to be fully consequentialist either, it just has to have some kernel of consequentialist structure by virtue of actually achieving its goals sometimes. Of course, the degree to which a model is consequentialist is more of a scalar quantity than a discrete yes/no thing; for my purposes it really doesn't matter where to draw a dotted line that dictates when a model \"becomes consequentialist\".\n\n(For what it's worth I would totally have called \"consequentialist models\" just mesaoptimizers and what other people call mesaoptimizers as like \"searching mesaoptimizers\" or something, but that would only create even more confusion than already exists)\n\nFor instance, the policy network of alphago alone (i.e none of the MCTS) is consequentialist, because it consistently steers the world into states where it's winning, despite my attempts to make it not win. Basically any RL policy is a fairly consequentialist model by this definition, since they channel the ",
      "wordCount": 1117
    },
    "tags": [
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "bzLpXZMGAiMdfLNKy",
    "title": "[ASoT] Some thoughts about imperfect world modeling",
    "slug": "asot-some-thoughts-about-imperfect-world-modeling",
    "url": null,
    "baseScore": 7,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2022-04-07T15:42:09.589Z",
    "contents": {
      "markdown": "So [a few posts ago](https://www.lesswrong.com/posts/oijJc8Mu2jPNgpuvy/some-thoughts-about-deceptive-mesaoptimization) I looked at the problem of not being able to anticipate all consequences of an action as being related to deceptive mesaoptimization, but also outer alignment too. This post digs more into some of the things I only touched on briefly in that post.\n\n*Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and AI_WAIFU for discussions.*\n\nLast time, we narrowed down the problem to \\[certain actions we can take now\\] leading to \\[a world such that if \\[RSA2048 or some other thing we can’t simulate\\] happens, then \\[bad things will happen\\]\\] (brackets added for easier parsing of this sentence). This is because we can't plan for a future where RSA-2048 is solved but if we can already see RSA2048 in the world, then we can plan forward from that and see when things blow up. \n\nSo if we could somehow figure out how to ensure that no action puts us into an RSA2048-vulnerable world, then we can prevent this failure case. Unfortunately, knowing precisely whether a world is vulnerable would essentially require us to simulate RSA2048, which gets us nowhere. However, one observation is we don't have to know precisely, we just need to know whether a world *could* be vulnerable — like for example if the agent removes its own safeguard, then we wouldn’t want to allow that even if the model is in fact safe and wouldn’t destroy the world when it sees RSA2048.\n\nTelling whether side effects are dangerous is hard \n---------------------------------------------------\n\nOne thing we might think to do is instead simulate putting a dummy AI in the “do rollouts to find out if these actions destroy the world” box (the word “box” here has no relation to the AI box experiment), such that this dummy AI tries to kill everyone when it sees something we can actually simulate, like the string \"chariots chariots\" or something. Then, we can execute the action in the simulation, and then simulate seeing that string, and see if the box successfully prevented the action from being executed in the real world. This way, we can test whether the box is broken by the action, right?\n\nUnfortunately there's a huge counterexample to this. So suppose the AI proposes building a really complicated machine in the world that we don't understand. We can still simulate the consequences of having this machine exist, of course, but we can't simulate it past the RSA2048 barrier, and so this machine could do evil stuff after seeing RSA2048 and we'd never know. The more general lesson from this is that if the model proposes doing something with side effects in the real world that we can't understand, that allows it to build up stuff \"outside of the box\" and then that stuff could do bad things. If we try to understand everything the model does, we end up bottlenecked by the whole \"doing science\" thing again, and if we try to bring the things the model does back inside the box (i.e only allow the machine it builds to also propose actions within the box), then you end up never being able to modify the real world at all because any changes to the real world could actually be setting up something that blows up after RSA2048, which is totally useless. \n\nSo, in general, it seems hard to know whether an action sets up something evil in the world that only activates after RSA2048. \n\nIt’s (partial) ELK reduction time, my dudes\n-------------------------------------------\n\nThere is a partial ELK reduction here in that if we can solve ELK then we can ask the model whether it thinks its actions are putting the world into a vulnerable state. It's worth noting that this *wouldn't* work if we just asked the model whether RSA2048 would cause it to kill everyone, because a) the model could always just remove our outer loop with this whole ELK thing before ever seeing RSA2048, and trying to prevent that collapses to just asking it whether it puts the world into a vulnerable state, and b) this actually depends on a slightly stronger version of ELK, where not only do we generalize to action sequences we can't see, but also to questions we can't see (because we can never simulate question answer pairs of the form \"if you see RSA2048, then do you do x?\", so the prior has to do all the work here).\n\n~However, this isn’t necessarily enough to prevent putting the world into a vulnerable state. For instance, suppose that it’s actually very easy to stumble on actions that put the world into a vulnerable state in ways humans can’t understand. Then, even if you just have a weak optimizer (say, just best-of-n based on some value model) that isn’t really maliciously “trying” to create a vulnerable state, and/or doesn't even realize that its actions will lead to a vulnerable state, you still have a high chance of ending up in one anyways.~ *(Update: I no longer endorse this, because to be able to do best-of-n, the value model would need to be able to model action consequences, which brings us back to ELK. In general, it seems difficult/impossible to reliably take actions which optimize for RSA2048-vulnerability without having some model of whether actions lead to RSA2048-vulnerability. Possible forthcoming post on this)*\n\nStumbling into RSA2048-vulnerable states\n----------------------------------------\n\nOne important thing here is that our abstraction where we pretend that RSA2048 is the only thing we can't model starts leaking here, because the more inaccurate our world model is, the easier it is to stumble into a vulnerable state, because there will simply be a lot more vulnerable states. So my intuition is that even though in these examples RSA2048-triggered optimization is a pretty unnatural thing to accidentally stumble upon, in practice this weak optimizer case matters a lot, because lots of things we might not be able to anticipate could trigger some optimizer we build in the world.\n\nOne reason we might expect it to be easy to stumble into these states is that lots of these states correspond to your AI building another AI to help it, which seems like a pretty convergently instrumental thing to do, and maybe your AI just fails at making the second AI inner aligned (but since the inner alignment failures happen behind the RSA2048 barrier, your world model rollouts think that this second AI is inner aligned!)\n\nAnother angle to look at the same thing is that this weak optimizer case is the case where even if the model isn’t “trying very hard” to get around your box, by default actions that are successful tend to be the ones that violate the integrity of the box. This framing makes weak optimizer failures seem pretty reasonable - the box is a pretty big constraint on the model, and it could get a lot more done a lot more efficiently without that constraint. \n\nWhat do we do?\n--------------\n\nUnfortunately I don't know if there's a solid solution to this problem. Our biggest advantage here is that the optimizer is restricted and so can't put the full weight of its optimization behind optimizing against us here, though even then you can still apply quite a bit of pressure. We might be able to put in this constraint with some kind of hack like a learned reward model to detect the creation of additional agents, which is obviously bottlenecked on science-doing, but because the optimization pressure isn't that strong it *might* be fine, and would probably allow us to stretch further than we could with just doing science on the original problem directly. And of course this all depends on ELK; if we can't solve that then this becomes even more of a problem.\n\nI also feel like the idea of weak optimization behaving in ways that mislead us even if there’s no “intent” in some sense in the system is something that could be useful to think about more generally, not just in the context of mesaoptimization/world modeling. I think this is an important crux and I'm still confused about it.",
      "plaintextDescription": "So a few posts ago I looked at the problem of not being able to anticipate all consequences of an action as being related to deceptive mesaoptimization, but also outer alignment too. This post digs more into some of the things I only touched on briefly in that post.\n\nEditor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and AI_WAIFU for discussions.\n\nLast time, we narrowed down the problem to [certain actions we can take now] leading to [a world such that if [RSA2048 or some other thing we can’t simulate] happens, then [bad things will happen]] (brackets added for easier parsing of this sentence). This is because we can't plan for a future where RSA-2048 is solved but if we can already see RSA2048 in the world, then we can plan forward from that and see when things blow up. \n\nSo if we could somehow figure out how to ensure that no action puts us into an RSA2048-vulnerable world, then we can prevent this failure case. Unfortunately, knowing precisely whether a world is vulnerable would essentially require us to simulate RSA2048, which gets us nowhere. However, one observation is we don't have to know precisely, we just need to know whether a world could be vulnerable — like for example if the agent removes its own safeguard, then we wouldn’t want to allow that even if the model is in fact safe and wouldn’t destroy the world when it sees RSA2048.\n\n\nTelling whether side effects are dangerous is hard \nOne thing we might think to do is instead simulate putting a dummy AI in the “do rollouts to find out if these actions destroy the world” box (the word “box” here has no relation to the AI box experiment), such that this dummy AI tries to kill everyone when it sees something we can actually simulate, like the string \"chariots chariots\" or something. Then, we can execute the action in the simulation, and then simula",
      "wordCount": 1348
    },
    "tags": [
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "slug": "outer-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      },
      {
        "id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "Jiy3n5KMsGGJ6NNYH",
    "title": "[ASoT] Some thoughts about LM monologue limitations and ELK",
    "slug": "asot-some-thoughts-about-lm-monologue-limitations-and-elk",
    "url": null,
    "baseScore": 10,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2022-03-30T14:26:15.381Z",
    "contents": {
      "markdown": "*Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and Laria for discussion.*\n\nOne potential way we might think to interpret LMs is to have them explain their thinking as a monologue or justification or train of thought or something. Particularly, by putting the explanation before the answer, we might hope to encourage the model to actually use the monologue to come to its conclusion and try to avoid the model [coming up with the bottom line first](https://www.lesswrong.com/posts/34XxbRFe54FycoCDw/the-bottom-line). However, there are a bunch of ways this could go wrong. For instance:\n\n*   Humans are really good at coming up with the bottom line first and then rationalizing a reasonable sounding process to arrive at that conclusion, so imitating humans encourages rationalization by default, even if the explanation is put temporally before the bottom line. Even though the model has uncertainty over the explanation, and in some sense the explanation determines the answer, the problem is that the distribution over the explanations may not be determined by the validity of the explanation, but rather be an integral over possible bottom lines.\n*   It's not sufficient for the answer to merely depend on the explanation: the model could be storing arbitrary data in the explanation in a way that doesn't line up with the meaning of the words. It could also be using the additional tokens to do more thinking in the latents but not in the tokens, etc.\n*   If the model is deceptive and we use the monologues to try and tell if the model is deceptive and apply any optimization pressure against that whatsoever (this includes rejecting actions/models based on it), then it would actually be incentivized to not tell us about its scheming, because we would be punishing (blatant monologue, evil action) more than (benign monologue, evil action).\n*   This also has a reduction to ELK: if the model's abstractions are sufficiently different from human abstractions, it may actually be more work for the model to translate its actual thought process to human understandable text than to just simulate the rationalization process a human would go through. To make the connection to ELK more clear, we can think of the reporter as the circuit that converts the model's actual thought process to text (or simulates the human thought process).\n\nI think it's possible that we're simply lucky and that GPTs of the relevant size just don't have problems (in the same way that ELK could just be easy), but I'm not sure we want to bet on that. I also think lots of these problems correspond to problems with mechanistic interpretability, especially the ELK and deception related ones. \n\nI don't think this is a particularly novel observation but it's nice having a reference to point to anyways.\n\nUPDATE: After further discussion with Kyle, I've now been convinced that \"generalizes human concepts correctly\" (which is closer to the natural abstractions of alignment by default) and \"has a compact direct translator\" are subtly different, and that crucially it's possible for the former to be true while the latter is false (i.e you could have a model whose latents are extremely obfuscated and therefore expensive to translate to the human ontology, yet this model correctly generalizes human concepts).",
      "plaintextDescription": "Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and Laria for discussion.\n\nOne potential way we might think to interpret LMs is to have them explain their thinking as a monologue or justification or train of thought or something. Particularly, by putting the explanation before the answer, we might hope to encourage the model to actually use the monologue to come to its conclusion and try to avoid the model coming up with the bottom line first. However, there are a bunch of ways this could go wrong. For instance:\n\n * Humans are really good at coming up with the bottom line first and then rationalizing a reasonable sounding process to arrive at that conclusion, so imitating humans encourages rationalization by default, even if the explanation is put temporally before the bottom line. Even though the model has uncertainty over the explanation, and in some sense the explanation determines the answer, the problem is that the distribution over the explanations may not be determined by the validity of the explanation, but rather be an integral over possible bottom lines.\n * It's not sufficient for the answer to merely depend on the explanation: the model could be storing arbitrary data in the explanation in a way that doesn't line up with the meaning of the words. It could also be using the additional tokens to do more thinking in the latents but not in the tokens, etc.\n * If the model is deceptive and we use the monologues to try and tell if the model is deceptive and apply any optimization pressure against that whatsoever (this includes rejecting actions/models based on it), then it would actually be incentivized to not tell us about its scheming, because we would be punishing (blatant monologue, evil action) more than (benign monologue, evil action).\n * This also has a reduction to ELK: if the model's abst",
      "wordCount": 562
    },
    "tags": [
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oijJc8Mu2jPNgpuvy",
    "title": "[ASoT] Some thoughts about deceptive mesaoptimization",
    "slug": "asot-some-thoughts-about-deceptive-mesaoptimization",
    "url": null,
    "baseScore": 24,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2022-03-28T21:14:27.217Z",
    "contents": {
      "markdown": "*Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to AI_WAIFU for discussions.*\n\nI think of mesaoptimization as being split into different types: aligned (which is fine), misaligned (which is bad), and deceptively aligned (which is really bad - I draw the line for deceptively aligned when it actually tries to hide from us instead of us just not noticing it by accident). I mostly think about deceptive mesaoptimization, and I think of regular misaligned optimization as basically an easier case because at least it’s not trying to hide from us (what it means for it to actually be \"trying to hide\" is kinda fraught anyways so this sidesteps that). In particular, I think of mesaoptimizers as being dangerous because they fail in a [malign](https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99) way (I don't think mesaoptimization itself is necessarily bad, and preventing it doesn't seem obviously good).\n\nOOD-focused framework of deception\n----------------------------------\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5877cb9e6ebb0c2560a631acd756d97c2bb43856a9d7a53b.jpg)\n\nOne framework for thinking about deceptive mesaoptimizers that I used to use a lot was that they would be good on the training distribution, but then do bad things out of distribution. In this framework, what we would do is then to try to somehow tell during training whether a model is going to do bad things out of distribution, and only keep the ones that don’t, or something along those lines. The intuition is that the mesaoptimizer cares about something in the real world, but it knows that if we catch it then it will not be able to achieve its goals in the real world, so it behaves normally on the training distribution when we’re looking at it but then in deployment it goes and does evil things to satisfy its mesaobjective. Importantly, pursuing the mesaobjective hurts the base objective (since we’re only focusing on inner alignment here we can assume that the base objective is accurate), so deployment is the only time where it can do evil things without getting itself removed. \n\nThe main reason here that deception happens is that your base objective isn’t being integrated over the set of all possible states. In theory, if we could enumerate the entire (huge) input space, we could eliminate deceptive behavior entirely. We'd just simply test every single input and see how whether there are any regions where the model does a treacherous turn and consequently tanks the base objective (for now I'm trying to isolate inner alignment so I'm assuming we've solved outer alignment and have a value function that we can just query). If there is nowhere in the entire input source where the model does a treacherous turn, then congratulations, your model is, for all intents and purposes, indistinguishable from being not deceptively aligned. (Non-deceptive inner misalignment is very similar except the mesaoptimizer won’t be trying to hide from you.)\n\nIn practice, the input space is absolutely enormous, and it's completely and utterly infeasible to enumerate it. Still, we might think to approximate that \"perfect\" red teaming by doing a really good approximation with loads of input instances sampled from a broad distribution, possibly with the help of other NNs. Unfortunately, I think this approach is fundamentally doomed, no matter how good the red ranking distribution is made. For this to work, the entire distribution of possible inputs in the real world would have to be covered by the red teaming distribution, and any small imperfections are potential points of leverage for the mesaoptimizer. Aside from some contrived existence proofs like the RSA2048 example (which are admittedly very useful for thinking about these things so I use them a lot despite the contrivedness), there are simply many things that could pop up in the real world (and especially the future) that we simply cannot predict/simulate, and that would be a giveaway signal of being in the real world. \n\nGiven an imperfect red teaming distribution, penalties for discovered deceptive behavior incentivizes the mesaoptimizer to shift probability mass from those regions into regions of the input space that are not covered by the red teaming; essentially, the model could learn just as well learn to not be caught deceptively mesaoptimizing, rather than not to mesaoptimize. Two possible ways this could swing towards the former are if either a) the mesaoptimizer is able to convergently gradient hack to ensure that between the two equal-training-loss options the training process goes into the one that preserves the mesaoptimizer, or b) mesaobjectives that imply deceptive behavior are easier to learn than those that imply honest behaviors (this one is especially likely to happen if the base objective is weird and complicated, as human values probably are, since all the simpler, easy to learn approximations are misaligned). I think both of these have a pretty good chance of being true, and would result, in effect, in models learning to hide from oversight. \n\nSo, we can’t possibly cover the entire space of possible inputs that could be encountered in deployment, and if the mesaoptimizer retreats into one of the states we didn't think of then we’re done for, and it has a pretty big incentive to do so. This means that black box approaches, where we try various inputs while assuming no knowledge about the model itself, are kind of hopeless. \n\nSo actually although we care about the model’s behavior in situations we might encounter in the future, we can’t specify that yet in our training loss. So, if we optimize for this loss function, if it turns out that deception is favored by the prior defined by our learning process (which seems plausible), then even if we reach an optimum of this training loss our thing still might be deceptive outside of the training distribution. The other way things could go wrong is if our optimizer is not good enough. Then, we might just settle with a suboptimal deceptive model because our optimizer is unable to find the global min. This connects to the frame of inner alignment as a problem of imperfect optimization. In practice we get a combination of both of these problems at once: we can't find the optimum of the training loss defined by our training distribution, which would allow for deception, and even if we could it might still be deceptive.\n\nUnforeseen consequences framework of deception\n----------------------------------------------\n\n*(Update: I no longer think of the failure introduced in this section as purely an inner alignment failure, but rather as a more general problem with training consequentialist systems.)*\n\nHowever, I think there’s a completely different frame from the whole out of distribution / “it only becomes evil in deployment” frame that I’ve been thinking a lot about. So one thing we might think is that maybe we can just do whatever oversight we did during training but during deployment as well, and it might have some competitiveness problems, but it seems like it would at least be a good starting point. Unfortunately, I think there are some subtle problems with this that totally break it and analyzing those problems brings us to the other framing of deception. I think these problems show up in the previous frame too.\n\nSo let’s say the model proposes some action, and we want to know whether the action would be safe to implement. Then, assuming we have some world model, we can simulate the model’s action out, and see whether something bad happens (I’m assuming for now that we know something is bad when we see it, because we’re focusing on inner alignment. This is definitely not trivial or remotely solved and is a big part of outer alignment.) This is also roughly what we’d do for the training procedure in the previous frame. The problem is that this world model is completely impossible. Aside from the fact that it’s smaller than the world and therefore can’t fit the world, it’s also susceptible to RSA2048-like problems, where there are certain states of the world where we know we can be in, but can’t actually produce without enormous cost. Another example would be simulating a future where more powerful computers exist; we can’t do that on current computers obviously. And of course, RSA2048 is again just a particularly sharp example of the class of things that could happen but that we can’t think of. In practice, there are many other distributional shifts, but RSA2048 is essentially the worst possible case of distributional shift, and I expect the solution to transfer pretty well.\n\nThis means that we can’t consider plans in which RSA2048 suddenly appears at some intermediate point. Note that this actually does still allow us to catch cases where the model merely does something bad the moment it sees RSA2048, because after we see RSA2048 in the real world during deployment, then we can actually just use that and plan through the consequences, and we’ll catch the treacherous turn there. So our failure case has to be subtler than that: the failures happen when there’s some action the agent can take that would make it *such that our deploy time world model simulation thing breaks*. Some examples:\n\n*   The model self modifies to remove the world model simulator. This looks like a really good action, because removing this safeguard makes the model a lot more efficient, and because we can’t simulate worlds where RSA2048 happens, we don’t see it explode.\n*   The model improves itself without removing the simulator but also in the process introduces some exploitable bugs (either intentionally or unintentionally).\n*   If we prevent it from self modifying: The model creates a copy of itself that doesn’t have the safeguard. \n*   If we also try to somehow prevent it from making new agents, there are a whole bunch of ways to goodhart whatever definition of agent we pick that could still put the world into a state where RSA2048 is bad\n\nIn general I don’t think we can make the model not take actions which would make it such that seeing RSA2048 would then cause bad things, though I admit to not having thought too much about it so I could be wrong. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/43b4e576fb229756ad4a2871159f78fe1d6ab5d647fe473e.jpg)\n\nThe more general problem is that due to our world model being inaccurate, we can’t predict the consequences of the model’s actions accurately, and the model can exploit this to do things that are bad in ways we can’t think of. One corollary is the problem of always rejecting actions which do something bad that you can’t even understand - I also talked about this subproblem [here](https://www.lesswrong.com/s/KgrG4cQdLtL9DvNr2/p/f6ByNdGJYxR3Kwguy). If you can do that, then you can reject everything that could potentially cause the world to enter into a state without the safeguards in ways you can’t understand, and of the remaining ones you should be able to understand them.\n\nThis problem actually applies to on-training-distribution things too. In the OOD frame, we kind of assume that on the training distribution we can simply tell on the training distribution whether it is indeed what we want, but in fact we can’t because to know whether we actually want it we would need to know the consequences of the action. In that sense, this frame is kind of orthogonal to the OOD one: OOD-only stuff is one problem, and the inability to see consequences is another orthogonal problem, but I think the latter is somewhat more fundamental because if we solve it then we can get (uncompetitive) OOD safety.\n\nThis also kind of ties into ELK: if we can solve ELK, then we can ask the model whether it thinks the action will lead to the safeguards being removed. The main problem here is that an action could still lead to the safeguards being removed even if the model doesn’t think so. An interesting avenue here would be how often this would happen - I don’t have much of a prior on even what order of magnitude problem this is. ELK might also simply not be solvable too, which makes this much more of a headache.\n\nFuture Directions: what do we do about actions that look good but are actually evil in ways that we can’t detect/comprehend; does ELK prevent safeguard removal in practice?\n\nUPDATE: I made a [followup post](https://www.lesswrong.com/posts/bzLpXZMGAiMdfLNKy/asot-some-thoughts-about-imperfect-world-modeling) that dives into a bit more detail into the imperfect world modelling problem.",
      "plaintextDescription": "Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to AI_WAIFU for discussions.\n\nI think of mesaoptimization as being split into different types: aligned (which is fine), misaligned (which is bad), and deceptively aligned (which is really bad - I draw the line for deceptively aligned when it actually tries to hide from us instead of us just not noticing it by accident). I mostly think about deceptive mesaoptimization, and I think of regular misaligned optimization as basically an easier case because at least it’s not trying to hide from us (what it means for it to actually be \"trying to hide\" is kinda fraught anyways so this sidesteps that). In particular, I think of mesaoptimizers as being dangerous because they fail in a malign way (I don't think mesaoptimization itself is necessarily bad, and preventing it doesn't seem obviously good).\n\n\nOOD-focused framework of deception\nOne framework for thinking about deceptive mesaoptimizers that I used to use a lot was that they would be good on the training distribution, but then do bad things out of distribution. In this framework, what we would do is then to try to somehow tell during training whether a model is going to do bad things out of distribution, and only keep the ones that don’t, or something along those lines. The intuition is that the mesaoptimizer cares about something in the real world, but it knows that if we catch it then it will not be able to achieve its goals in the real world, so it behaves normally on the training distribution when we’re looking at it but then in deployment it goes and does evil things to satisfy its mesaobjective. Importantly, pursuing the mesaobjective hurts the base objective (since we’re only focusing on inner alignment here we can assume that the base objective is accurate), so deployment is the only time where it can do evil ",
      "wordCount": 2031
    },
    "tags": [
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "f6ByNdGJYxR3Kwguy",
    "title": "[ASoT] Searching for consequentialist structure",
    "slug": "asot-searching-for-consequentialist-structure",
    "url": null,
    "baseScore": 26,
    "voteCount": 13,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2022-03-27T19:09:13.370Z",
    "contents": {
      "markdown": "*Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and Laria for discussions.*\n\nIn [“Searching for Bayes Structure”](https://www.lesswrong.com/posts/QrhAeKBkm2WsdRYao/searching-for-bayes-structure), Eliezer argues that things which arrive at the truth must be doing something Bayes-like somewhere in there, even if it’s extremely inefficient or accidental or filtered through some process that looks very different from the outside. If it has no Bayes structure, it literally cannot arrive at the truth.\n\nI think it’s plausible that Bayes is to epistemic rationality as consequentialism is to instrumental rationality. Here, by consequentialism I mean the ability to get to some goal from a broader set of initial states, which is how I understood Elliezer’s use of the term in the dialogues. It does not have to be an EUM/VNM-rational. Importantly, to be good at consequentialism comes with all the stuff like instrumental convergence (and consequently self preservation, power seeking, etc). \n\nIn other words, there’s some minimal amount of consequentialism you need to achieve a given goal, and there’s no getting around this lower bound. Of course, some agents do a lot more consequentialism than is necessary, but I interpret some people (i.e Eliezer) as arguing that even this lower bound is too much consequentialism for, say, performing a pivotal action, because it’s fundamentally very difficult to do. As an analogy, if your goal is to buy an apple for $1, any plans which fail to collect at least $1 will simply not be able to acquire an apple, and even though there also exist plans which collect $100 (which is more dangerous than collecting $1), if even collecting $1 is too dangerous by default, then even lobotomizing a $100-collecting agent to only collect $1 is scarcely comforting. The main lesson I draw from this is that if we want to do something consequentialist then we have to somehow figure out how to align consequentialists, and that making the agent worse at consequentialism isn’t a solution. I think it also seems productive to figure out where the lower bound of how much consequentialism we need (for, i.e a pivotal action) and the upper bound of how much consequentialism is safe are relative to each other. \n\nAs an aside, my intuition for why instrumental convergence is unavoidable for consequentialists: There is some set of plans that achieve your goal with high probability across all universes, and a slightly larger superset that achieves your goal with high probability in across the universes in which you don't try to stop the system. Among those plans, under some complexity prior or something, the ones that have the safe properties we want (they actually stop when you try to stop them) occupy a very small measure, because this set is very unnatural. Think Nate's laser analogy: the plans which accomplish the goal in a way robust to many perturbations, but yet are not robust against the perturbations that correspond to us trying to stop the system, is difficult to specify. So to pull out the ones we want requires a bunch of work, which means that by default most plans that achieve our goal will be power seeking and also not corrigible.\n\nActually, it gets even worse, because there are also deceptive plans that look corrigible but actually aren't; it's really hard to separate these from the actually corrigible ones. So even getting really good at picking out the corrigibility cluster of things isn't good enough, because there's a big incentive for deceptive plans to look corrigible. (I think this is vaguely corresponding to outer vs inner alignment but I'm not 100% sure.) Also, there might be self-modification considerations here where non-EUMs and other lobotomized AIs might self modify to EUMs, if you lobotomize them too much, the reason being that if you lobotomize the AI too much then most plans conditioned on success start looking like \"build a non lobotomized AI somehow and then tell that AI to accomplish the thing\".\n\nTool AI\n-------\n\nI think there's also a way in which this applies to tool AI type arguments as well. Ultimately, the reason we want tools is they help us accomplish some goal (for instance, performing a pivotal action). So, we can view the entire human+tool system as an agent that is trying to get to some goal, where there's some human oversight to ensure that the plans pass muster. This system, to get to goals, will have to do consequentialism. While having the human provide the glue between the model and the real world does add some safety, I would argue that this is approximately as safe as just humans vetting plans from an agent, which is itself not really sufficient because many plans which seem non suspicious to us are actually dangerous in some unforeseen way (diving into whether this is most plans or vanishingly few plans seems like a useful avenue - on the one hand one would think that making a plan sound useful while still doing something bad might require a lot of optimization, on the other hand \"I can't see anything going wrong with this\" is a hugely lower bar than \"nothing will go wrong with this\" resulting in most plans that satisfy the former not satisfying the latter). \n\nWhile the human providing the \"outer loop\" so to speak does allow the human a little bit more control in that they have control over the particular reasoning processes used to arrive at the action, I don't think this is a very large effect. For one, if the tool is deceptive, it can still manipulate the human by giving them carefully crafted answers to their queries - in the deceptive case this gives almost no protection. Even without deception, if we're in a universe where \"I can't see anything going wrong with this\" is much looser than \"nothing goes wrong\" (which, for what it's worth, I think is highly likely to be true), you end up with the exact same problem again. Because there's some minimal amount of consequentialism you need to do no matter what to accomplish your goal, you also need to do some minimal amount of optimizing against your ability to see things wrong with plans, and in the universe where most ok-looking plans suck, this optimization quickly goodharts your ability to tell whether a plan is good. \n\nThis seems to suggest the human's ability to pick good plans with extremely high precision (i.e definitely not accepting any dangerous plans, though possibly rejecting some safe plans) as a core difficulty. This particular subproblem has also popped up in discussions about other parts of alignment, so I think it seems like something pretty important.\n\n(Also, tools are [economically uncompetitive](https://www.gwern.net/Tool-AI) anyways.)",
      "plaintextDescription": "Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to Kyle and Laria for discussions.\n\nIn “Searching for Bayes Structure”, Eliezer argues that things which arrive at the truth must be doing something Bayes-like somewhere in there, even if it’s extremely inefficient or accidental or filtered through some process that looks very different from the outside. If it has no Bayes structure, it literally cannot arrive at the truth.\n\nI think it’s plausible that Bayes is to epistemic rationality as consequentialism is to instrumental rationality. Here, by consequentialism I mean the ability to get to some goal from a broader set of initial states, which is how I understood Elliezer’s use of the term in the dialogues. It does not have to be an EUM/VNM-rational. Importantly, to be good at consequentialism comes with all the stuff like instrumental convergence (and consequently self preservation, power seeking, etc). \n\nIn other words, there’s some minimal amount of consequentialism you need to achieve a given goal, and there’s no getting around this lower bound. Of course, some agents do a lot more consequentialism than is necessary, but I interpret some people (i.e Eliezer) as arguing that even this lower bound is too much consequentialism for, say, performing a pivotal action, because it’s fundamentally very difficult to do. As an analogy, if your goal is to buy an apple for $1, any plans which fail to collect at least $1 will simply not be able to acquire an apple, and even though there also exist plans which collect $100 (which is more dangerous than collecting $1), if even collecting $1 is too dangerous by default, then even lobotomizing a $100-collecting agent to only collect $1 is scarcely comforting. The main lesson I draw from this is that if we want to do something consequentialist then we have to somehow figure out",
      "wordCount": 1120
    },
    "tags": [
      {
        "_id": "ZTRNmvQGgoYiymYnq",
        "name": "Consequentialism",
        "slug": "consequentialism"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SbxWdhhwJWCpifTst",
    "title": "[ASoT] Some ways ELK could still be solvable in practice",
    "slug": "asot-some-ways-elk-could-still-be-solvable-in-practice",
    "url": null,
    "baseScore": 26,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2022-03-27T01:15:16.607Z",
    "contents": {
      "markdown": "*Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top.*\n\nThis post is a followup to my [earlier post](https://www.lesswrong.com/s/KgrG4cQdLtL9DvNr2/p/kWTko53s2DqTeprjz).\n\nIf ELK is impossible in generality, how could we solve it in practice? Two main ways I can think of:\n\n*   Natural abstractions: maybe the complexity of direct translators is bounded in practice because the model ontology is not so different from ours, violating the worst case  assumption.\n*   Science being easy: maybe there isn’t any hard ceiling to how good you can get at doing science, and it's not actually that much more expensive so it is actually feasible to just scale science up.\n\nFor the first one, I feel like there are both reasons for and against it being potentially true. The main argument for, especially in the context of LMs, is that human abstractions are baked into language and so there's a good chance LMs also learn this abstraction. I [used to be](https://www.lesswrong.com/posts/EmxfgPGvaKqhttPM8/thoughts-on-the-alignment-implications-of-scaling-language) a lot more optimistic that this is true, but since then I've changed my mind on this. For one, in the limit, natural abstractions are definitely not optimal. The optimal thing in the limit is to simulate all the air molecules or something like that. So we know that even if natural abstractions holds true at some point, it must eventually stop being optimal, and the abstractions get more and more weird past that point. Then the question becomes not just whether or not there even exists a sweet spot where natural abstractions holds, but also whether it overlaps with the kinds of models that are dangerous and might kill us. I still think this is plausible but kind of iffy.\n\nI think this is worth [looking into empirically](https://forum.effectivealtruism.org/posts/KigFfo4TN7jZTcqNH/the-future-fund-s-project-ideas-competition?commentId=8gC2f6xfkJGWFFwW4), but I'm very cautious about not reading too much into the results, because even if we see increasingly natural abstractions as size increases, it could still start going down again before our models are sufficiently powerful. I'm moderately worried that someone will overclaim the significance of results in this vein. Also, my intuition about language models is that despite communicating in language, they learn and think in pretty alien ways.\n\nFor the second one, if this is true then this kind of defeats most of the point of even doing ELK. If you can just scale up the science doing, then ELK becomes just an optimization the same way that reward modelling is just an optimization of asking humans every single time you need a reward. In that case, then doing something vaguely ELK-like might be more efficient, but it wouldn't allow us to do anything fundamentally new. That might be hugely important in practice, but would be a very different focus than what people focus on when they talk about ELK right now. \n\nAs for the other point, I’m personally not hugely optimistic about any of the existing science-doing methods, but this opinion is not based on very much thought and I expect my stance on this to be fairly easily changeable. I think this is a useful thing for me to spend some more time thinking about at some point.\n\nFuture Directions: empirical stuff testing natural abstractions, how do we do science better",
      "plaintextDescription": "Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top.\n\nThis post is a followup to my earlier post.\n\nIf ELK is impossible in generality, how could we solve it in practice? Two main ways I can think of:\n\n * Natural abstractions: maybe the complexity of direct translators is bounded in practice because the model ontology is not so different from ours, violating the worst case  assumption.\n * Science being easy: maybe there isn’t any hard ceiling to how good you can get at doing science, and it's not actually that much more expensive so it is actually feasible to just scale science up.\n\nFor the first one, I feel like there are both reasons for and against it being potentially true. The main argument for, especially in the context of LMs, is that human abstractions are baked into language and so there's a good chance LMs also learn this abstraction. I used to be a lot more optimistic that this is true, but since then I've changed my mind on this. For one, in the limit, natural abstractions are definitely not optimal. The optimal thing in the limit is to simulate all the air molecules or something like that. So we know that even if natural abstractions holds true at some point, it must eventually stop being optimal, and the abstractions get more and more weird past that point. Then the question becomes not just whether or not there even exists a sweet spot where natural abstractions holds, but also whether it overlaps with the kinds of models that are dangerous and might kill us. I still think this is plausible but kind of iffy.\n\nI think this is worth looking into empirically, but I'm very cautious about not reading too much into the results, because even if we see increasingly natural abstractions as size increases, it could still start going down again before our models are sufficiently powerful. I'm moderately worried that so",
      "wordCount": 543
    },
    "tags": [
      {
        "_id": "mSTmKrSkFBswHaS3T",
        "name": "Eliciting Latent Knowledge",
        "slug": "eliciting-latent-knowledge"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kWTko53s2DqTeprjz",
    "title": "[ASoT] Observations about ELK",
    "slug": "asot-observations-about-elk",
    "url": null,
    "baseScore": 34,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2022-03-26T00:42:20.540Z",
    "contents": {
      "markdown": "This document outlines some of my current thinking about ELK, in the form of a series of observations I have made that inform my thinking about ELK. \n\n*Editor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to AI_WAIFU and Peter Barnett for discussions.*\n\n*   One way we can think of ELK is we have some set of all possible reporters, each of which takes in the latent states of a world model and outputs, for the sake of concreteness, the answer to some particular fixed question. So essentially we can think of a reporter+model pair as assigning some answer to every point in the action space. Specifically, collapsing possible sequences of actions into just a set of possible actions and only worrying about one photo doesn’t result in any loss of generality but makes things easier to talk about.\n*   We pick some prior over the set of possible reporters. We can collect training data which looks like pairs (action, answer). This can, however, only cover a small part of the action space, specifically limited by how well we can “do science.”\n*   This prior has to depend on the world model. If it didn’t, then you could have two different world models with the same behavior on the set where we can do science, but where one of the models understands what the actual value of the latent variable is, and one is only able to predict that the diamond will still appear there but can’t tell whether the diamond is still real. A direct translator for the second will be a human simulator for the first. \n*   More generally, we don’t really have guarantees that the world model will actually understand what’s going on, and so it might genuinely believe things about the latent variable that are wrong. Like, we might have models which think exactly like a human and so they don't understand anything a human wouldn’t understand, so the direct translator for this model would also simultaneously be a human simulator, because the model literally believes exactly what a human would believe.\n*   There are more than just human simulators and direct translators that are consistent with the training data; there are a huge number of ways the remaining data can be labeled. This basically breaks any proposal that starts with penalizing reporters that look like a human simulator.\n*   I basically assume the “human simulator” is actually simulating us and whatever science-doing process we come up with, since this doesn’t change much and we’re assuming that doing science alone isn’t enough because we’re looking at the worst case. \n*   So, for any fixed world model, we need to come up with some prior over all reporters such that after picking all the reporters consistent with our data (action, answer), the maximum of the resulting posterior is the direct translator. \n*   A big assumption I’m making is that the KL distance between our prior and the complexity prior has to be finite. My intuition for why this has to be is that a) universality of the complexity prior, and b) even if our prior doesn’t have to cover all possible reporters, it still seems reasonable that as complexity increases the number of reporters that our prior accepts as nonzero needs to increase exponentially, which gets something similar to universality of complexity on the subset of plausible reporters. I think this point is the main weak point of the argument.\n*   Since the amount of data we can get is bounded by our ability to do science, the amount of information we can get from that is also bounded. However, since we’re assuming worst case, we can make the complexity difference between the direct translator and all the other junk arbitrarily big, and therefore arbitrarily less likely, and since our modified prior and our data only get us a finite amount of evidence, we can always imagine a world where the direct translator is sufficiently complex such that we can’t pick it out.\n*   So to solve ELK we would need to somehow rule out a huge part of the set of possible reporters before even updating on data. In some sense this feels like we ran in a circle, because there is always the prior that looks at the model, solves ELK, and then assigns probability 1 to the direct translator. So we can always just move all the difficulty into choosing the prior and then the data part is just entirely useless. I guess in retrospect it is kind of obvious that the prior has to capture the part that the data can’t capture, but that's literally the entire ELK problem.\n*   One seemingly useful thing this does tell us is that our prior has to be pretty restrictive if we want to solve ELK in the worst case, which I think rules out a ton of proposals right off the bat. In fact, I think your prior can only assign mass to a finite number of reporters, because to get the sum to converge it needs to decay fast enough, which also gets you the same problem. \n*   Only assigning mass to a finite number of reporters is equivalent to saying we know the upper bound of direct translator complexity. Therefore, we can only solve ELK if we can bound direct translator complexity, but in the worst case setting we're assuming direct translator complexity can be unbounded. \n*   ~So either we have to be able to bound complexity above, violating the worst case assumption, or we have to bound the number of bits we can get away from the complexity prior. Therefore, there exists no worst case solution to ELK.~ *(Update: I no longer think this is accurate, because there could exist a solution to ELK that function entirely by looking at the original model. Then the conclusion of this post is weaker and only shows that assigning an infinite prior is not a feasible strategy for worst case)*\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b5592f500702bcb4f31443095ab03731bbe6abf8d8791623.jpg)",
      "plaintextDescription": "This document outlines some of my current thinking about ELK, in the form of a series of observations I have made that inform my thinking about ELK. \n\nEditor’s note: I’m experimenting with having a lower quality threshold for just posting things even while I’m still confused and unconfident about my conclusions, but with this disclaimer at the top. Thanks to AI_WAIFU and Peter Barnett for discussions.\n\n * One way we can think of ELK is we have some set of all possible reporters, each of which takes in the latent states of a world model and outputs, for the sake of concreteness, the answer to some particular fixed question. So essentially we can think of a reporter+model pair as assigning some answer to every point in the action space. Specifically, collapsing possible sequences of actions into just a set of possible actions and only worrying about one photo doesn’t result in any loss of generality but makes things easier to talk about.\n * We pick some prior over the set of possible reporters. We can collect training data which looks like pairs (action, answer). This can, however, only cover a small part of the action space, specifically limited by how well we can “do science.”\n * This prior has to depend on the world model. If it didn’t, then you could have two different world models with the same behavior on the set where we can do science, but where one of the models understands what the actual value of the latent variable is, and one is only able to predict that the diamond will still appear there but can’t tell whether the diamond is still real. A direct translator for the second will be a human simulator for the first. \n * More generally, we don’t really have guarantees that the world model will actually understand what’s going on, and so it might genuinely believe things about the latent variable that are wrong. Like, we might have models which think exactly like a human and so they don't understand anything a human wouldn’t understand, so the direct translato",
      "wordCount": 1030
    },
    "tags": [
      {
        "_id": "mSTmKrSkFBswHaS3T",
        "name": "Eliciting Latent Knowledge",
        "slug": "eliciting-latent-knowledge"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "waWKo339QyhJRbfh6",
    "title": "What do paradigm shifts look like?",
    "slug": "what-do-paradigm-shifts-look-like",
    "url": null,
    "baseScore": 18,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2022-03-16T19:17:37.586Z",
    "contents": {
      "markdown": "One major question that heavily influences the choice of alignment research directions is the degree to which incremental improvements are necessary for major paradigm shifts. Because alignment is largely preparadigmatic, we may require a paradigm shift before we can make substantial progress towards aligning superhuman AI systems, rather than merely incremental improvements. The answer to this question determines whether the best approach to alignment is to choose metrics and try to make incremental progress on alignment research questions, or to mostly fund things that are long shots, or something else entirely. It also informs what policy we should take with respect to capabilities acceleration as an externality to alignment work: the degree to which incremental improvements in capabilities lead to paradigm shifts in capabilities informs how much we should worry about incremental capabilities improvements as a byproduct.   \n\nSome possible ways-the-world-could-be include:\n\n*   Incremental improvements have negligible impact on when paradigm shifts happen and could be eliminated entirely without any negative impact on when paradigm shifts occur. All or the vast majority of incremental work is visible from the start as low risk low reward, and potentially paradigm shift causing work is visible from the start as high risk high reward.\n*   Incremental improvements serve to increase attention in the field and thus increase the amount of funding for the field as a whole, thereby proportionally increasing the absolute number of people working on paradigmatic directions, but funding those working on potential paradigm shifts directly would yield the same paradigm shifts at the same time\n*   Incremental improvements are necessary to convince risk averse funding sources to continue funding something, since putting money into something for years with no visible output is not popular with many funders, and thus forces researchers to divert a certain % of their time to working on funder-legible incremental improvements.\n*   Most paradigm shifts arise from attempts to make incremental improvements that accidentally uncover something deeper in the process. It is difficult to tell before embarking on a project whether it will only yield an incremental improvement, no improvement at all, or a paradigm shift.\n*   Most paradigm shifts cannot occur until incremental improvements lay the foundation for the paradigm shift to happen, no matter how much effort is put into trying to recognize paradigm shifts.\n*   Something else?\n\nI think figuring out which of these universes we're in would be enormously valuable and seems especially high leverage. I think in particular there are likely to be many different perspectives on this and a lot of productive disagreement between those perspectives, so I don't think multiple people working on this has a stepping-on-toes effect.",
      "plaintextDescription": "One major question that heavily influences the choice of alignment research directions is the degree to which incremental improvements are necessary for major paradigm shifts. Because alignment is largely preparadigmatic, we may require a paradigm shift before we can make substantial progress towards aligning superhuman AI systems, rather than merely incremental improvements. The answer to this question determines whether the best approach to alignment is to choose metrics and try to make incremental progress on alignment research questions, or to mostly fund things that are long shots, or something else entirely. It also informs what policy we should take with respect to capabilities acceleration as an externality to alignment work: the degree to which incremental improvements in capabilities lead to paradigm shifts in capabilities informs how much we should worry about incremental capabilities improvements as a byproduct.   \n\nSome possible ways-the-world-could-be include:\n\n * Incremental improvements have negligible impact on when paradigm shifts happen and could be eliminated entirely without any negative impact on when paradigm shifts occur. All or the vast majority of incremental work is visible from the start as low risk low reward, and potentially paradigm shift causing work is visible from the start as high risk high reward.\n * Incremental improvements serve to increase attention in the field and thus increase the amount of funding for the field as a whole, thereby proportionally increasing the absolute number of people working on paradigmatic directions, but funding those working on potential paradigm shifts directly would yield the same paradigm shifts at the same time\n * Incremental improvements are necessary to convince risk averse funding sources to continue funding something, since putting money into something for years with no visible output is not popular with many funders, and thus forces researchers to divert a certain % of their time to working on",
      "wordCount": 443
    },
    "tags": [
      {
        "_id": "aHjTRDkGypPqbXWpN",
        "name": "Intellectual Progress (Society-Level)",
        "slug": "intellectual-progress-society-level"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "KkSncxP2jQ3nmtmEv",
    "title": "EleutherAI's GPT-NeoX-20B release",
    "slug": "eleutherai-s-gpt-neox-20b-release",
    "url": "http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf",
    "baseScore": 30,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2022-02-10T06:56:41.155Z",
    "contents": {
      "markdown": "> GPT-NeoX-20B is a 20 billion parameter autoregressive language model whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights. In this paper, we describe the model architecture and training, evaluate its performance, and discuss the broader impacts of its release. We are open-sourcing the training and evaluation code, as well as the model weights, at [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox).\n\nThe [alignment section](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#page=11) of the paper would probably be especially of interest to this community.",
      "plaintextDescription": "> GPT-NeoX-20B is a 20 billion parameter autoregressive language model whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights. In this paper, we describe the model architecture and training, evaluate its performance, and discuss the broader impacts of its release. We are open-sourcing the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.\n\nThe alignment section of the paper would probably be especially of interest to this community.",
      "wordCount": 95
    },
    "tags": [
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sNb6h4AsXqh2EjpDD",
    "title": "NFTs, Coin Collecting, and Expensive Paintings",
    "slug": "nfts-coin-collecting-and-expensive-paintings",
    "url": null,
    "baseScore": 29,
    "voteCount": 25,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2022-01-24T01:01:48.117Z",
    "contents": {
      "markdown": "*I‘ve tried to make this post fairly nuanced and with a lot of steelmanning, in contrast to much of the material on this subject. My opinions on crypto as a whole are more complex and deserve a separate, more in depth post. None of this is financial advice, I am not a financial advisor, etc.*\n\nWhen it comes to paintings, people care a lot about having the “original” painting. There can only be one original, “real” Mona Lisa, despite all the hi-res scans you can get. Even if you could make a copy so good that nobody could visually tell it apart from the original painting without painstaking chemical analysis, the original is still going to be worth many orders of magnitude more than the copy. While some people argue that the impossibility of making an exact copy of a physical object is the reason why the original is distinct from the copies (and therefore the crux of why this doesn’t generalize into the digital world), and that therefore the small difference in chemical composition or whatever makes up for those orders of magnitude, I think that this view is pretty silly. A visually indistinguishable painting has the same aesthetic value, and conveys the same historical value to the viewer, and even if you argue that there is *some* premium to having the original, it seems pretty absurd to suggest that the minor visual differences and chemical composition are worth, within a rounding error, $870 million of the $870 million the Mona Lisa is valued at, and that the actual content of art itself modulo some small errors is worth basically nothing in comparison. \n\nI think it’s pretty clear that people don’t *actually* care about the chemical composition of their paintings or whatever. Rather, I hypothesize that the reason people care about the original is because this enforces *artificial scarcity*. You can always make more replicas, but there is only one “original”. By ensuring that more supply cannot ever be added, you ensure an effective monopoly on the painting until you decide to sell it. \n\nThis then brings up the question: Why care about the original and not the 15th copy? One might argue that since physical objects cannot (for all practical purposes) be copied exactly, *every* painting is unique, so if we only cared about uniqueness, we shouldn’t expect to favor the original. I think there are two main reasons behind this: first, the original is a far more natural Schelling point than any other copy. If you made a list of every copy of the Mona Lisa in the world and asked people to pick which one they think everyone else would pick, basically everyone would pick the original. Of course, having the one that everyone else wants bodes well for your selling prospects. And, more qualitatively, would it be more impressive to brag about owning *the* Mona Lisa or some mass produced copy?\n\nSecond, *fungibility is relative*. To explain this, let’s take a brief detour into the world of coin collecting. As legal tender, a penny is a penny is a penny. If you’re just using it to pay for things, any penny is fungible against any other penny, even if one is rusted more than the other, or they have different years, or one has a special limited run inscription, or so on. However, collectors care a great deal about [extremely specific coins](https://www.pcgs.com/news/rare-canadian-1936-dot-cent) and their quality, and so some particular coins no longer funge against other coins because people care a lot about *this specific coin*. Less popular but still valuable coins are more fungible, with coins within a rough band of assessed quality being mostly interchangeable. The upshot is that whether things are fungible depends a lot on where people draw their reference classes. \n\nTying this all together, people care about and want to pay big money for the original because there is only one original and because everyone else cares about and wants to pay big money for the original, and non-originals are worth less because the supply is generally unbounded due to fungibility in the context of the people who are willing to pay big money for paintings. Said more pithily, expensive paintings are expensive because everyone thinks they are. There’s no “intrinsic” value in there, or vanishingly little of it compared to the sticker price, it’s simply worth a lot because everyone is willing to pay a lot for it. This definitely isn’t specific to paintings or anything; we see this pop up basically anywhere you can collect things with limited supply, and oh boy do humans *love* collecting things.\n\n(Some exercises left for the reader, along similar lines of reasoning: Why are some paintings worth a lot more than other paintings, often with little relation to how good the art is? Why are some famous forgeries worth quite a bit, too? Why does art suddenly get more valuable after the artist dies? Why did Banksy’s painting increase in value from being partially shredded? Why is it that forks of bitcoin don’t dilute the market share of bitcoin very much?)\n\nSo.. how does this all have to do with NFTs? I think NFTs are essentially the pure distilled essence of what makes expensive paintings expensive. At their core, NFTs aren’t really about crypto; the crypto part is just an aesthetic that makes it appealing to the kind of person that already likes crypto. (There is a bit of a platform risk argument, but putting things on chain introduces a different set of platform risks, which forms a different risk profile that makes it appealing to the kind of person that already likes crypto.) NFTs drop all pretense of pretending to care about the physical continuity or chemical composition or whatever and just flat out creates the scarcity and the Schelling point by declaring, by fiat, that whoever owns this particular digital asset owns *the* thing, and there can never be another of *the* thing. Said differently, NFTs are the pure distilled essence of the concept of owning whatever the thing is that differentiates the original Mona Lisa from a copy of the Mona Lisa, independently of any physical constraints. This is why I think all the people who talk about right-click downloading NFTs are missing the point—it’s like the equivalent of making a visually-indistinguishable facsimile of the Mona Lisa. The point of owning the Mona Lisa isn’t to have something that looks identical to the Mona Lisa, the point is to own *the* Mona Lisa. The fact that digital data can be copied losslessly doesn't actually differentiate this from the physical art case because for all intents and purposes, nobody *really* cares about the minor differences between the original and the copy beyond the fact that they serve to delineate the original and constrain its supply.\n\nAlso, there’s a whole other rabbit hole about what it means to “own” something when it comes to digital assets. I don’t want to get too deep into this rabbit hole but the fact that you’re owning the intangible essence of scarcity rather than any specific thing or copyright or other rights makes things a lot simpler: the fact that it’s stored on a particular ledger as a ledger entry doesn’t really matter; the ownership of the *vast majority of assets* is defined by rows in databases. I think all the star registry comparisons are sort of misleading because there *is* in theory a Schelling point, for each particular NFT, of which registry to trust. In other words, I think the problem of multiple competing NFTs claiming to be the “real” NFT is a solvable problem and not a huge fundamental issue (in fact, less of a problem than with real paintings, since the author can just sign the hash of the one they endorse and cryptographic signatures are more reliable than real signatures or chemical composition).\n\n(also, both NFTs and expensive paintings can be used for [laundering money](https://www.cnn.com/2020/07/29/business/art-money-laundering-sanctions-senate/index.html), so they’re functionally interchangeable regardless)\n\nSo.. are NFTs good? Well.. I think the whole thing with wanting to own *the™* thing is kind of silly when taken too far, for the same reason that I think expensive art is kind of silly or collecting extremely valuable stamps or coins or baseball cards or anything else like that is kind of silly. I simply don't derive very much pleasure from owning something rare. As such, I am personally not interested in buying or collecting NFTs, though if someone wants to give me a whole bunch of money to NFTize something I wouldn’t say no. \n\nI think the fact that NFTs are crypto-coded is probably a big reason why they’ve become so prominent: partially because it appeals to a base of people who have a much higher risk tolerance than average, and partially because crypto is a huge [scissor statement](https://slatestarcodex.com/2018/10/30/sort-by-controversial/) that gets everyone talking about it. \n\nLots of actual implementations of NFTs are way worse than the theoretical platonic ideals of NFTs that I talk about here, though I’ll admit I haven’t spent a lot of time looking into the details. Also, the vast majority of NFT related things (as are most crypto things) are downright fraud/scams, so I think that’s a reasonable first approximation when dealing with crypto stuff without an inside view, but that perspective is low hanging fruit that everyone has already talked about a million times before. I also wouldn’t be surprised if NFTs in their current incarnation totally stopped being a thing not long from now—Schelling points are only as strong as everyone thinks everyone else thinks they are. At the same time, however you feel about NFTs, I think there’s a pretty fundamental aspect of how humans think and coordinate in there that might be worth thinking about.",
      "plaintextDescription": "I‘ve tried to make this post fairly nuanced and with a lot of steelmanning, in contrast to much of the material on this subject. My opinions on crypto as a whole are more complex and deserve a separate, more in depth post. None of this is financial advice, I am not a financial advisor, etc.\n\nWhen it comes to paintings, people care a lot about having the “original” painting. There can only be one original, “real” Mona Lisa, despite all the hi-res scans you can get. Even if you could make a copy so good that nobody could visually tell it apart from the original painting without painstaking chemical analysis, the original is still going to be worth many orders of magnitude more than the copy. While some people argue that the impossibility of making an exact copy of a physical object is the reason why the original is distinct from the copies (and therefore the crux of why this doesn’t generalize into the digital world), and that therefore the small difference in chemical composition or whatever makes up for those orders of magnitude, I think that this view is pretty silly. A visually indistinguishable painting has the same aesthetic value, and conveys the same historical value to the viewer, and even if you argue that there is some premium to having the original, it seems pretty absurd to suggest that the minor visual differences and chemical composition are worth, within a rounding error, $870 million of the $870 million the Mona Lisa is valued at, and that the actual content of art itself modulo some small errors is worth basically nothing in comparison. \n\nI think it’s pretty clear that people don’t actually care about the chemical composition of their paintings or whatever. Rather, I hypothesize that the reason people care about the original is because this enforces artificial scarcity. You can always make more replicas, but there is only one “original”. By ensuring that more supply cannot ever be added, you ensure an effective monopoly on the painting until you deci",
      "wordCount": 1613
    },
    "tags": [
      {
        "_id": "xCXkjecsjwm8uSW3y",
        "name": "Cryptocurrency & Blockchain",
        "slug": "cryptocurrency-and-blockchain"
      },
      {
        "_id": "PDJ6KqJBRzvKPfuS3",
        "name": "Economics",
        "slug": "economics"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "TdzxoS4AMgwysnixt",
    "title": "Retail Investor Advantages",
    "slug": "retail-investor-advantages",
    "url": null,
    "baseScore": 13,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2021-12-07T02:08:20.694Z",
    "contents": {
      "markdown": "There are some advantages to being a retail investor that might make it possible for you to beat the market without violating EMH.\n\nInformation Acquisition Cost\n----------------------------\n\nIf you are an expert in some domain, it costs less for you to assess information in that domain than some trading firm. The trading firm has to spend money and suffer a time delay hiring an expert, incur the costs of communication overhead between people being difficult, also spend money on the opportunity cost of context switches and taking time away from traders at the company etc. Meanwhile, because you have both the trader and expert in your head, you save on a huge amount of overhead. Similarly, trading firms could [pay people to camp outside Walmart and count customers](https://www.lesswrong.com/posts/utySCY9nJt9xGYGGQ/the-emh-aten-t-dead#2__Hedge_funds_with_armies_of_drones), but if you already work at Walmart you can observe these things for relatively little marginal cost. Because of this, there are many small niches where the possible reward is too small to make it worth it for a trading company, but whose stakes are just the right size for retail investors. \n\nCheaper Liquidity\n-----------------\n\nNobody wants to be on the other side of the trade with RenTech, because RenTech makes smart trades. Everyone wants to be on the other side of the trade with retail investors, because most are uninformed and generally lose money over time. This is what makes it possible for Robinhood to offer commission free trades; uninformed flow is something that market makers really want to be on the other side of. The magnitude of trading fees often make or break a trading strategy.\n\nLess Slippage\n-------------\n\nMoving less money moves the market less. As you scale up your Assets Under Management, your moves eat deeper into the order book and cause the market to move with you, eroding your profits. Large funds have entire teams of people dedicated to trade execution strategies to minimize their impact on the market. As a retail investor, you don't have this problem to the same magnitude; buying $100k of SPX will move the market a negligible amount. The magnitude of slippage also often makes or breaks a trading strategy; many naive backtests ignore slippage (because historical order book information is typically more difficult to obtain than OHLCV), and end up unprofitable in the real world because of it.\n\n*(Nothing I post is financial advice, I am not a financial advisor, etc)*",
      "plaintextDescription": "There are some advantages to being a retail investor that might make it possible for you to beat the market without violating EMH.\n\n\nInformation Acquisition Cost\nIf you are an expert in some domain, it costs less for you to assess information in that domain than some trading firm. The trading firm has to spend money and suffer a time delay hiring an expert, incur the costs of communication overhead between people being difficult, also spend money on the opportunity cost of context switches and taking time away from traders at the company etc. Meanwhile, because you have both the trader and expert in your head, you save on a huge amount of overhead. Similarly, trading firms could pay people to camp outside Walmart and count customers, but if you already work at Walmart you can observe these things for relatively little marginal cost. Because of this, there are many small niches where the possible reward is too small to make it worth it for a trading company, but whose stakes are just the right size for retail investors. \n\n\nCheaper Liquidity\nNobody wants to be on the other side of the trade with RenTech, because RenTech makes smart trades. Everyone wants to be on the other side of the trade with retail investors, because most are uninformed and generally lose money over time. This is what makes it possible for Robinhood to offer commission free trades; uninformed flow is something that market makers really want to be on the other side of. The magnitude of trading fees often make or break a trading strategy.\n\n\nLess Slippage\nMoving less money moves the market less. As you scale up your Assets Under Management, your moves eat deeper into the order book and cause the market to move with you, eroding your profits. Large funds have entire teams of people dedicated to trade execution strategies to minimize their impact on the market. As a retail investor, you don't have this problem to the same magnitude; buying $100k of SPX will move the market a negligible amount. The magn",
      "wordCount": 390
    },
    "tags": [
      {
        "_id": "jgcAJnksReZRuvgzp",
        "name": "Financial Investing",
        "slug": "financial-investing"
      },
      {
        "_id": "pnSDArjzAjkvAF5Jo",
        "name": "Efficient Market Hypothesis",
        "slug": "efficient-market-hypothesis"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "BgoKdAzogxmgkuuAt",
    "title": "Behavior Cloning is Miscalibrated",
    "slug": "behavior-cloning-is-miscalibrated",
    "url": null,
    "baseScore": 77,
    "voteCount": 38,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2021-12-05T01:36:01.802Z",
    "contents": {
      "markdown": "Behavior cloning (BC) is, put simply, when you have a bunch of human expert demonstrations and you train your policy to maximize likelihood over the human expert demonstrations. It’s the simplest possible approach under the broader umbrella of Imitation Learning, which also includes more complicated things like Inverse Reinforcement Learning or Generative Adversarial Imitation Learning. Despite its simplicity, it’s a fairly strong baseline. In fact, prompting GPT-3 to act agent-y is essentially also BC, just rather than cloning on a specific task, you're cloning against all of the task demonstration-like data in the training set--but fundamentally, it's a scaled up version of the exact same thing. The problem with BC that leads to miscalibration is that the human demonstrator may know more or less than the model, which would result in the model systematically being over/underconfident for its own knowledge and abilities.\n\nFor instance, suppose the human demonstrator is more knowledgeable than the model at common sense: then, the human will ask questions about common sense much less frequently than the model should. However, with BC, the model will ask those questions at the exact same rate as the human, and then because now it has strictly less information than the human, it will have to marginalize over the possible values of the unobserved variables using its prior to be able to imitate the human’s actions. Factoring out the model’s prior over unobserved information, this is equivalent[^1] to taking a guess at the remaining relevant info conditioned on all the other info it has (!!!), and then act as confidently *as if it had actually observed that info*, since that's how a human would act (since the human really observed that information outside of the episode, but our model has no way of knowing that). This is, needless to say, a *really bad* thing for safety; we want our models to ask us or otherwise seek out information whenever they don't know something, not randomly hallucinate facts. \n\nIn theory we could fix this by providing enough information in the context such that the human doesn’t know anything the model doesn’t also know. However, this is very impractical in the real world, because of how much information is implicit in many interactions. The worst thing about this, though, is that it fails silently. Even if we try our best to supply the model with all the information we think it needs, if we forget anything the model won't do anything to let us know; instead, it will silently roll the dice and then pretend nothing ever happened.\n\nThe reverse can also happen, where if the model knows more than the human, then it will collect a bunch of unnecessary info which it discards so that its decision is as dumb as the human's. This is generally not as dangerous, though it might still mislead us to the capabilities of the model (we might think it's less knowledgeable than it actually is), and it would use resources suboptimally, so it's still best avoided. Also, the model might do both at the same time in different domains of knowledge, if the human is more knowledgeable in one area but less in another.\n\nPlus, you don’t even need unobserved *information* in the information-theoretic sense. If your agent has more logical uncertainty than the human, you end up with the exact same problem; for example if it’s significantly better/worse at mental math than the human in an environment where the human/agent can choose to use a calculator provided as part of the environment that costs some small amount of reward to use, even though the agent has access to the exact same info as the human, it will choose to use a calculator too often/not often enough.\n\nThis isn’t a purely theoretical problem. This *definitely* happens  in GPT-2/3 currently and is a serious headache for many uses of GPT already--model hallucinations have been a pretty big problem [outlined](https://arxiv.org/abs/2011.02593) [in](https://arxiv.org/abs/2110.10819) [numerous](https://arxiv.org/pdf/2103.15025.pdf) [papers](https://arxiv.org/abs/2104.06683)*.* Further, I expect this problem to scale to even superhuman models, since this BC objective fundamentally does not incentivize calibration. Even as a component of a superhuman agent, it seems really bad if a component of the agent silently adds false assumptions with high confidence into random parts of the agent's thoughts. On the optimistic side, I think this problem is uniquely exhibitable and tractable and the solutions are scalable (superhuman BC would be uncalibrated for the exact same reasons as current BC).\n\nBecause an agent that's consistently overconfident/underconfident will get less reward, and reward is maximized when the model is calibrated, the RL objective incentivizes the model to become calibrated. However, RL comes with its own problems too. Making a good reward function that really captures what you care about and is robust against goodharting is hard, and either hand crafting a reward or learning a reward model opens you up to goodharting, which could manifest itself in much more varied and unpredictable ways depending on many details of your setup. A hybrid BC pretrain+RL finetune setup, [as is](https://openai.com/blog/improving-factual-accuracy/) [common](https://arxiv.org/abs/2109.10862) [today](https://www.nature.com/articles/nature16961) (since training from scratch with RL is exorbitantly expensive in many domains) could have the problems of either, both, or neither, depending on the details of how much RL optimization is allowed to happen (i.e by limiting the number of steps of tuning, or having a distance penalty to keep the policy close to the BC model, etc). \n\nI think it would be promising to see whether miscalibration can be fixed without allowing goodharting to happen. In particular, I think some kind of distance penalty that makes it inexpensive for the model to fix calibration issues but very expensive to make other types of changes would possibly allow this. The current standard KL penalty penalizes calibration related changes the exact same as all other changes, so I don’t expect tuning the coefficient on that will be enough, and even if it works it will probably be very sensitive to the penalty coefficient, which is not ideal. Overall, I’m optimistic that some kind of hybrid approach could have the best of both worlds, but just tweaking hyperparameters on the current approach probably won’t be enough.\n\n[^1]: Since \\\\(P(x) = \\sum_h P(x|h) P(h)\\\\), sampling from \\\\(P(x)\\\\) is equivalent to first sampling an \\\\(h\\\\) from your prior and then conditioning on that information as if you had actually observed it.",
      "plaintextDescription": "Behavior cloning (BC) is, put simply, when you have a bunch of human expert demonstrations and you train your policy to maximize likelihood over the human expert demonstrations. It’s the simplest possible approach under the broader umbrella of Imitation Learning, which also includes more complicated things like Inverse Reinforcement Learning or Generative Adversarial Imitation Learning. Despite its simplicity, it’s a fairly strong baseline. In fact, prompting GPT-3 to act agent-y is essentially also BC, just rather than cloning on a specific task, you're cloning against all of the task demonstration-like data in the training set--but fundamentally, it's a scaled up version of the exact same thing. The problem with BC that leads to miscalibration is that the human demonstrator may know more or less than the model, which would result in the model systematically being over/underconfident for its own knowledge and abilities.\n\nFor instance, suppose the human demonstrator is more knowledgeable than the model at common sense: then, the human will ask questions about common sense much less frequently than the model should. However, with BC, the model will ask those questions at the exact same rate as the human, and then because now it has strictly less information than the human, it will have to marginalize over the possible values of the unobserved variables using its prior to be able to imitate the human’s actions. Factoring out the model’s prior over unobserved information, this is equivalent[1] to taking a guess at the remaining relevant info conditioned on all the other info it has (!!!), and then act as confidently as if it had actually observed that info, since that's how a human would act (since the human really observed that information outside of the episode, but our model has no way of knowing that). This is, needless to say, a really bad thing for safety; we want our models to ask us or otherwise seek out information whenever they don't know something, not rando",
      "wordCount": 1047
    },
    "tags": [
      {
        "_id": "8hPTCJbwJnLBmfpCX",
        "name": "Calibration",
        "slug": "calibration"
      },
      {
        "_id": "fpEBgFE7fgpxTm9BF",
        "name": "Machine Learning  (ML)",
        "slug": "machine-learning-ml"
      },
      {
        "_id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "slug": "outer-alignment"
      },
      {
        "_id": "Fi6SeJRGfJs3bp5se",
        "name": "Reinforcement learning",
        "slug": "reinforcement-learning"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "BisjoDrd3oNatDu7X",
      "tag_name": "Outer Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "Ppav5dtuFNCiC6GP9",
    "title": "Quadratic Voting and Collusion",
    "slug": "quadratic-voting-and-collusion",
    "url": null,
    "baseScore": 41,
    "voteCount": 20,
    "viewCount": null,
    "commentCount": 24,
    "createdAt": null,
    "postedAt": "2021-11-17T00:19:15.737Z",
    "contents": {
      "markdown": "Quadratic voting is a proposal for a voting system that ensures participants cast a number of votes proportional to the degree they care about the issue by making the marginal cost of each additional vote linearly increasing - see [this post by Vitalik](https://vitalik.ca/general/2019/12/07/quadratic.html) for an excellent introduction.\n\nOne major issue with QV is collusion - since the marginal cost of buying one vote is different for different people, if you could spread a number of votes out across multiple people, you could buy more votes for the same amount of money. For instance, suppose you and a friend have $100 each, and you care only about Cause A and they care only about Cause B, and neither of you care about any of the other causes up for vote. You could spend all of your $100 on A and they could spend all of theirs on B, *or* you could both agree to each spend $50 on A and $50 on B, which would net \\\\(\\sqrt 2\\\\) times the votes for both A and B as opposed to the default.\n\nThe solution generally proposed in response to this issue is to ensure that the vote is truly secret, to the extent that you cannot even prove to anyone else who you voted for. The thinking is that this creates a prisoner's dilemma where by defecting, you manage to obtain both the $50 from your friend and also the full $100 from yourself for your own cause, and that because there is no way to confirm how you voted, there are no possible externalities to create incentives for not defecting.\n\nUnfortunately, I have two objections to this solution, one theoretical and one practical. The theoretical objection is that if the two agents are able to accurately predict each others' actions and reason using FDT, then it is possible for the two agents to cooperate à la FairBot - this circumvents the inability to prove what you voted for after the fact by proving ahead of time what you will vote. The practical objection is that people tend to cooperate in prisoner's dilemmas a significant amount of the time anyways, and in general a lot of people tend to uphold promises they make even if the other party has no way to verify. I think there's even an argument to be made that the latter is at least partially a real world instance of the former. \n\nI'm still optimistic that this might not be a showstopping problem in practice, since there is a limited pool of people who you would trust in practice, which puts a ceiling on how many times your vote can count. However, I think this is still a major unavoidable flaw with QV for both practical and theoretical applications.",
      "plaintextDescription": "Quadratic voting is a proposal for a voting system that ensures participants cast a number of votes proportional to the degree they care about the issue by making the marginal cost of each additional vote linearly increasing - see this post by Vitalik for an excellent introduction.\n\nOne major issue with QV is collusion - since the marginal cost of buying one vote is different for different people, if you could spread a number of votes out across multiple people, you could buy more votes for the same amount of money. For instance, suppose you and a friend have $100 each, and you care only about Cause A and they care only about Cause B, and neither of you care about any of the other causes up for vote. You could spend all of your $100 on A and they could spend all of theirs on B, or you could both agree to each spend $50 on A and $50 on B, which would net √2 times the votes for both A and B as opposed to the default.\n\nThe solution generally proposed in response to this issue is to ensure that the vote is truly secret, to the extent that you cannot even prove to anyone else who you voted for. The thinking is that this creates a prisoner's dilemma where by defecting, you manage to obtain both the $50 from your friend and also the full $100 from yourself for your own cause, and that because there is no way to confirm how you voted, there are no possible externalities to create incentives for not defecting.\n\nUnfortunately, I have two objections to this solution, one theoretical and one practical. The theoretical objection is that if the two agents are able to accurately predict each others' actions and reason using FDT, then it is possible for the two agents to cooperate à la FairBot - this circumvents the inability to prove what you voted for after the fact by proving ahead of time what you will vote. The practical objection is that people tend to cooperate in prisoner's dilemmas a significant amount of the time anyways, and in general a lot of people tend to uphold prom",
      "wordCount": 455
    },
    "tags": [
      {
        "_id": "ipJwbLxhR83ZksN6Z",
        "name": "Mechanism Design",
        "slug": "mechanism-design"
      },
      {
        "_id": "mPuSAzJN7CyrMiKrf",
        "name": "Voting Theory",
        "slug": "voting-theory"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MJffzWaCQcvB3gRJa",
    "title": "In Defence of Optimizing Routine Tasks",
    "slug": "in-defence-of-optimizing-routine-tasks",
    "url": null,
    "baseScore": 47,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2021-11-09T05:09:41.595Z",
    "contents": {
      "markdown": "People often quantify whether an optimization to a routine task is worth it by looking at the net amount of time saved. This mindset is exemplified by this oft-cited xkcd:\n\n![Is It Worth the Time?](https://imgs.xkcd.com/comics/is_it_worth_the_time.png)\n\n[https://xkcd.com/1205/](https://xkcd.com/1205/)\n\nHowever, I think this doesn't really give a full picture of the best way to decide whether to make a task more efficient. There are many other factors that may weigh more than the raw time cost of something.\n\nFirst off, for tasks that create friction inside important loops, this could disincentivize you from going around the loop, and make the feedback loop significantly more frustrating. For example, creating short aliases for common shell commands makes me much more productive because it decreases the mental cost I assign to running them and makes it easier to overcome the \"activation energy\", despite this probably not saving me more than an hour or two in a year, since typing a few extra characters realistically doesn't take that long. Of course, setting up aliases only takes a few minutes, but even if it took hours I still think it would work out to be worth it overall. \n\nIf the task inside the loop gets longer than even just a few seconds, this also adds significant context switching costs. For example, if I'm writing code that needs some simple utilities, the value of having those utilities already implemented and easily usable is worth far more than the reimplementation time it saves. If I need a utility function that takes 5 minutes to reimplement, by the time I finish implementing the function I've already partially lost track of the context in the original place where I needed the function and need to spend some more time and effort getting back into the flow of the original code I was writing. (This is also a reason why building up abstractions is so powerful - even more powerful than the mere code reuse benefits would imply - it lets you keep the high level context in short term memory.) In practice, this means that often building a set of problem specific tools first to abstract away annoying details and then having those tools to help you makes solving the problem feel so much more tactile and tractable and *fun* than the alternative. This seems obvious in retrospect but I still find myself not building tools when I should be sometimes. \n\nIf you're especially unlucky, the subtask itself could turn out to be harder than just a 5 minute task, and then you have to recurse  another layer deeper, and after a few iterations of this you end up having totally forgotten what you were originally trying to do — the extremely frustrating \"falling down the rabbit hole\"/\"yak shaving\" phenomenon. Needless to say, this is really bad for productivity (and mental health). Having a solid foundation that behaves how you expect it to 99% of the time helps avoid this issue. Even just knowing that the probability of falling down a rabbit hole is low seems to help a lot with the activation energy I need to overcome to actually go implement something.\n\nEven if the thing you're optimizing isn't in any critical loop, there's also the cognitive overhead of having to keep track of something at all. Having to do less things frees you up from having to worry at all about whether the thing is getting done, whether you forgot anything important, etc, which helps reduce stress a lot. Plus, I can't even count how many times I've thought \"this is just going to be temporary/one-off and won't be part of an important feedback loop\" about something that eventually ended up in an important feedback loop anyways.\n\nFinally, not all time is made equal. You only get a few productive hours a day, and those hours are worth a lot more than hours where you can't really get yourself to do anything that requires too much cognitive effort or focus. Thus, even if it doesn't save you any time, finding a way to reduce the amount of cognitive effort needed could be worth a lot. For example, making the UI for something you need to check often more intuitive and displaying all the things you care about in easily interpretable formats (as opposed to just showing the raw data and forcing you to work things out in your head, even if that computation is trivial) could be worth a lot by freeing up your productive hours for less-automatable things. \n\nOf course, premature optimization is still bad. I'm not saying you should go and optimize everything; it's just that there are other factors worth considering beyond the raw time you save. Figuring out when to optimize and when not to is its own rabbit hole, but knowing that time isn't everything is the first step.",
      "plaintextDescription": "People often quantify whether an optimization to a routine task is worth it by looking at the net amount of time saved. This mindset is exemplified by this oft-cited xkcd:\n\nhttps://xkcd.com/1205/\nHowever, I think this doesn't really give a full picture of the best way to decide whether to make a task more efficient. There are many other factors that may weigh more than the raw time cost of something.\n\nFirst off, for tasks that create friction inside important loops, this could disincentivize you from going around the loop, and make the feedback loop significantly more frustrating. For example, creating short aliases for common shell commands makes me much more productive because it decreases the mental cost I assign to running them and makes it easier to overcome the \"activation energy\", despite this probably not saving me more than an hour or two in a year, since typing a few extra characters realistically doesn't take that long. Of course, setting up aliases only takes a few minutes, but even if it took hours I still think it would work out to be worth it overall. \n\nIf the task inside the loop gets longer than even just a few seconds, this also adds significant context switching costs. For example, if I'm writing code that needs some simple utilities, the value of having those utilities already implemented and easily usable is worth far more than the reimplementation time it saves. If I need a utility function that takes 5 minutes to reimplement, by the time I finish implementing the function I've already partially lost track of the context in the original place where I needed the function and need to spend some more time and effort getting back into the flow of the original code I was writing. (This is also a reason why building up abstractions is so powerful - even more powerful than the mere code reuse benefits would imply - it lets you keep the high level context in short term memory.) In practice, this means that often building a set of problem specific tools",
      "wordCount": 792
    },
    "tags": [
      {
        "_id": "nvKzwpiranwy29HFJ",
        "name": "Optimization",
        "slug": "optimization"
      },
      {
        "_id": "udPbn9RthmgTtHMiG",
        "name": "Productivity",
        "slug": "productivity"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "u3fP8vjGsDCT7X54H",
    "title": "Towards Deconfusing Gradient Hacking",
    "slug": "towards-deconfusing-gradient-hacking",
    "url": null,
    "baseScore": 39,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2021-10-24T00:43:32.916Z",
    "contents": {
      "markdown": "*\\[Epistemic status: brainstorming, less confused than last time since it seems to provide for a nice taxonomy of things to look into rather than just a bunch of random loose threads.\\]*\n\n*This post would not exist without the* [*EleutherAI discord*](https://discord.gg/zBGx3azzUn) *and all the wonderful people there. Special thanks to Evan Hubinger, John Schulman, and Adam Shimi for providing feedback on drafts of this post.*\n\nThe goal of this post is to identify a different way of framing the gradient hacking problem, connect it to our intuitions, and use it to create a rough taxonomy of different types of potential gradient hacking. Hopefully, using this taxonomy, we can tackle more well-defined subproblems to narrow down the space of things that could potentially gradient hack.\n\nWhen we think about [gradient hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking), the most intuitive framing is to consider some kind of agent embedded inside a larger network (like a GPT) that somehow intentionally modifies the loss landscape of the larger network with respect to the base loss, and that this modification makes it so that in optimizing for the base objective, the base optimizer also happens to optimize the mesaobjective. Here I consider the base objective to be a function \\\\(\\Theta \\to \\mathbb R\\\\) from the params of the network to the reals, that has all the training data baked in for simplicity, and the mesaobjective another function \\\\(\\Theta \\to \\mathbb R\\\\), possibly with some constraint that both objectives have to be indifferent between models which behave the same on all inputs. The \"somehow\" is often considered to be some kind of perturbing or otherwise making the output of the larger network worse whenever the mesaobjective isn't met, therefore creating an incentive for gradient descent to improve the mesaobjective. One example of this line of thinking can be found in my [last post about gradient hacking](https://www.alignmentforum.org/posts/KfX7Ld7BeCMQn5gbz/obstacles-to-gradient-hacking). Unfortunately, I think there are some confusions with this framing. \n\nFull loss landscapes\n--------------------\n\nFirst off, the base loss landscape of the entire model is a function \\\\(\\Theta \\to \\mathbb R\\\\) that's the same across all training steps, and the configuration of the weights selects somewhere on this loss landscape. Configuring the weights differently can put the model on a different spot on this landscape, but it can't change the shape of the landscape itself. \n\nNote that this *doesn't* contradict the interpretation of the gradient hacker as having control over the loss landscape through subjunctive dependence. As an analogy, in Newcomb's problem even if you accept that there is subjunctive dependence of the contents of the box on your decision and conclude you should one-box, it's still *true* that the contents of the box cannot change after Omega has set them up and that there is no *causal* dependence of the contents of the box on your action, even though the dominated action argument no longer holds because of the subjunctive dependence.\n\nTo emphasize this landscape covering the entire model, let's call the loss landscape of the base loss with respect to the entire model a *full loss landscape*. Any configuration of the network really just selects a point somewhere on this landscape, and to claim that any such point effectively does gradient hacking would be to argue that as gradient descent evolves this point over time, it manages to also get better at the mesaobjective in some sense. This may seem trivial, but I've noticed that unless you consider this explicitly, it's really easy to get it mixed up with cases where the landscape really does change. This suggests a new framing, where we define gradient hackers with respect to a particular mesaobjective as subsets of parameter space that tend to improve on the given mesaobjective over time, with different mesaobjectives defining different subsets (this is a bit tricky to formalize and I'm still working on it).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e563b0866b0fecdb0fe5e7a085e87bcf3ed24002c8c99900.png)\n\nA subset of parameter space on a loss landscape.\n\nThroughout this post I'll be arguing that there are essentially three main behaviors that models containing gradient hackers can exhibit: \n\n*   either they converge to some local minimum of the base objective, or\n*   they don't (by taking advantage of some kind of inadequacy in the base optimizer) or\n*   the training loss isn't actually the same across all training steps in the first place (mostly RL).\n\nSince I think this distinction is really important, I'm going to call these convergent gradient hackers, nonconvergent gradient hackers, and nonstationary gradient hackers respectively. In other words, no matter how it carves up the model, any proposal for gradient hacking must either argue that the specific local minimum of the base objective that's converged to is also advantageous for the mesaobjective in some way, or must identify some *specific* optimization inadequacy that's being used to keep the model away from local minima, or must argue that the training loss is nonstationary.\n\nI want to emphasize that I'm ***not*** arguing that because the base loss is already fixed in place before we start training, and the gradient hacker lives inside the model which is instantiated after the base loss is frozen in place, therefore the gradient hacker cannot influence the base loss, therefore gradient hacking is not possible. \n\nRather, I'm trying to argue that if there do exist gradient hackers, the right way to think about them is that they must: be identified with a local minimum of the base loss (convergent), or have some mechanism for breaking convergence and not converging to a local minimum of the base loss (nonconvergent), or have some mechanism for making the base loss nonstationary. With some careful formalisms, these three cases should be general enough to cover every possible gradient hacker. \n\nIn the Newcomb analogy, what I'm doing is dividing the space of possible universes into ones where either the opaque box is empty or it isn't, and *not* making the subsequent \"dominated action\" argument.\n\nPartial loss landscapes\n-----------------------\n\nI think this framing is really different from the intuition I mentioned at the beginning. Thankfully, I think there's still a way to connect that intuition to this framing by looking at the problem from a different angle. It really does feel like there's some sense in which by configuring some of the weights we can affect some sort of loss landscape — the loss landscape of the base loss wrt all parameters *except* those that are part of the agent; let’s call those parameters the agent parameters. Let's call this a *partial loss landscape*, because it's only a slice of the entire loss landscape. One way to think about this is you're considering the gradient hacking subnetwork as part of the effective loss function *for the part of the network that it's trying to optimize*. This seems to be what we intuitively think about when we think of \"shaping the loss landscape.\"\n\nThe partial loss landscape changes as you change around the agent parameters, and you can set things up so that the partial loss landscape lines up with the things your gradient hacker wants to do to the rest of the model as defined by the mesaobjective. One of my hopes when first looking into gradient hacking was essentially to handcraft a model with agent parameters that were both stable and shaped the partial loss landscape to match up with the mesaobjective. However, I quickly [ran into problems with keeping the agent stable](https://www.alignmentforum.org/posts/KfX7Ld7BeCMQn5gbz/obstacles-to-gradient-hacking) in a noncontrived architecture.\n\nSGD initially pushes the model in the right direction, however, since the agent parameters are part of the model too, they can and will start drifting due to gradient descent as well. As a result, the partial loss landscape changes throughout training too. Intuitively, the agent parameters will want to change in the direction that makes the mesaobjective being enforced easier.\n\nInitially I was hoping to find a clever way to do gradient protection that also works in noncontrived architectures, but I now think that these difficulties are a fundamental problem resulting from the constraint in the previous section, it's just that rearranging the problem in this way makes it harder to realize. Basically my point in this section is that I think my new framing resolves some of the confusion I had with my previous framing and explains why I was getting nowhere earlier.\n\nCategorizing failures of gradient hacking\n-----------------------------------------\n\nHere is a look into each of the three categories I identified above. This is also sort of a future work section, since I have some vague ideas about what might be useful for each of these subcases.\n\n### Convergent - Local (and even global) minima can be arbitrarily bad\n\nIt could be that being able to specify a local minimum is really powerful, so convergent gradient hackers are too. I don't know enough about ML theory to say much about the structure of local minima, but as long as local minima are abundant enough this is a serious concern. For instance, imagine a contrived toy example where there are two local minima: one where the model outputs optimally and achieves loss 0.0, and one where the model outputs optimally 99% of the time and does something extremely dangerous the other 1% of the time and achieves loss 0.1. If the model converges to the second one then it might be reasonable to argue that the model is gradient hacking. It might be that it's impossible for this to happen (there are some papers that suggest [global convergence of certain overparameterized NNs](https://link.springer.com/content/pdf/10.1007/s10994-019-05839-6.pdf)), but I wouldn't bank on it.\n\nEven if you assume convergence to a global min, you still might not be safe. As a contrived example, the MNIST classification loss is totally indifferent between a model that just solves MNIST perfectly and uses the rest of the parameters to do nothing, and one that checks to see if the solution to RSA-2048 is encoded into the input and computes and outputs a plan to turn the universe into paperclips if so, and if not solves MNIST perfectly. We *could* punt this problem by saying that the base objective is at fault for not caring and declare that this is [actually an outer alignment failure](https://www.alignmentforum.org/posts/HYERofGZE6j9Tuigi/inner-alignment-failures-which-are-actually-outer-alignment) since there's no optimization failure going on here. However, that doesn't actually make the problem go away in practical situations where we simply can't make the base objective care about every possible input. (This might also give a useful categorization of outer alignment failures: there are failures where the base objective cares about something differently from how we \"really want\", and failures where the base loss is indifferent where it shouldn't be and so the model fails at interpolation/extrapolation.) You could also add some kind of complexity penalty (the *simplest* model that achieves a global minimum must be non-deceptive - any deceptive model achieving a global minimum must do at least what the simplest non-deceptive model does, plus also whatever it’s doing deceptively), but that's also probably really difficult to make work (KC is uncomputable and not differentiable, and weight decay is completely broken by batchnorm and in general doesn't really measure complexity per se, etc).\n\nMy suspicion is that nothing too crazy happens in practice. Because [local minima are rarer in higher dimensional optimization](https://arxiv.org/abs/1406.2572), and SGD in theory [doesn't get stuck in saddle points](https://hal.inria.fr/hal-03043771/document) (all these under a ton of simplifying assumptions, admittedly), and also in practice it seems to somehow just be biased towards [really](https://www.youtube.com/watch?v=kcVWAKf7UAg&t=1250s) [good](https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf) [generalizing](https://proceedings.neurips.cc/paper/2018/file/fface8385abbf94b4593a0ed53a0c70f-Paper.pdf) [minima](https://www.alignmentforum.org/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of), it seems like a model getting stuck in a *really* suboptimal local minimum is unlikely. I don't have a very rigorous argument along these lines yet, though. It also seems a bit far-fetched that a model converged to a global minimum will learn a really malicious subnetwork somehow, though I don't know how to justify this intuition without appealing to a handwavy argument about SGD having an inductive bias for less complex models, some kind of [simplicity bias](https://www.lesswrong.com/posts/ej2r2JADoWiEtxkCd/sgd-s-bias) inherent in SGD, and also possibly something about these states not having very large attractor basins because the slice of parameter space that converges to a model that happens to have a malicious subnetwork while also achieving a global minimum is tiny (since it can't rely on the gradient to form those subnetworks). \n\nHere's a sampling of a few convergence proofs that might be useful and the kinds of assumptions they make:\n\n*   convergence to critical points with probability 1 if the optimizer has a learning rate decay of \\\\(\\Theta(1/n^p)\\\\) for some \\\\(p \\in (0, 1]\\\\), the gradient exists everywhere and is bounded, and both the gradient is Lipschitz-continuous (this in particular excludes ReLUs!), and the sublevels and gradient sublevels are bounded. ([paper](https://hal.inria.fr/hal-03043771/document))\n*   convergence within any \\\\(\\varepsilon\\\\) of critical points with probability 1 given a small enough constant learning rate, but the model is allowed to be nonconvex, nonsmooth, and only required to be locally Lipschitz. ([paper](https://arxiv.org/pdf/2005.08513.pdf))\n*   convergence to global minimum with *massively* overparameterized ReLU networks using cross entropy, Gaussian init, data meeting certain separation criteria. ([paper](https://link.springer.com/content/pdf/10.1007/s10994-019-05839-6.pdf))\n\nThese all make rather spherical-cow-like assumptions to various degrees, but I think they are useful as a starting point.\n\n### Nonconvergent - Some ways to not converge to a local min\n\nWe can divide the kinds of reasons that the model might *not* converge into several categories, loosely using the assumptions of convergence proofs as a guide (this probably should be formalized at some point):\n\n*   Abuse of floating point rounding and other numerics nonsense - this could be abused to make the computed gradient not match up with the actual gradient, which lets you totally break convergence\n*   Model architecture + loss function results in a loss landscape that's not sufficiently nice (i.e not (locally) Lipschitz, gradient not Lipschitz continuous, etc depending on which convergence proof)\n*   Learning rate schedule doesn't allow the model to converge (but rather oscillate around a min) - I don't expect this one to actually matter in practice since the areas in parameter space near a non-malicious minimum are probably very unlikely to be malicious, but it complicates proofs.\n*   Lots of bad critical points (i.e saddle points). I mentioned before that this seems unlikely in practice given the evidence we have, but you could probably construct architectures by hand that do this (i.e using activation functions with lots of distinct saturated regions, for example). My intuition is that this is a non-problem in most reasonable architectures but it complicates proofs. Contrived, unreasonable architectures probably break this assumption the hardest.\n\nI think a bunch of these are things capabilities researchers are actively working very hard on, so it would be low leverage for us to focus on them. I think there's a good chance that ML systems in practice converge somewhat often because the ones that don't converge suck, and capabilities researchers are constantly hard at work trying to find models that don't suck. \n\nOne other problem is that for really large models, we often don't train to convergence, since this isn't the [most compute efficient option](https://arxiv.org/abs/2001.08361). So you could have a model that in theory would eventually converge, but spends long enough in gradient hacked regions that when we stop it, it hasn't left one of them yet. Some of the arguments about [randomly initialized models being biased towards simple functions](https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99) might be relevant for arguing that the model will never get into a gradient hacking region to begin with. Maybe the solution is just to give up compute efficiency and train models all the way (though this hurts competitiveness quite a bit). Or, maybe models that would eventually converge are safe throughout most of training, or perhaps we'll figure out how to converge compute-efficiently in the future (maybe something like [superconvergence](https://www.fast.ai/2018/07/02/adam-weight-decay/)?) which totally obsoletes compute-efficient early stopping anyways.\n\nIt could also be that there are regions of pathologically slow convergence that are technically in the convergence basin of some local minimum but are sufficiently far from that minimum so as to have completely different (specifically, more malicious) behavior. This would definitely pose a problem for early stopping, and possibly even fool us into thinking that a region is near a local minimum when it isn't actually. My hope here would be for there to be some way to detect when this is happening, or some kind of proof that bounds how slowly a model can actually converge or how much a model can change from a pathologically slow region to the minimum that it eventually converges to.\n\n### Nonstationary - Reinforcement Learning, Continual Learning, and other non-stationary training\n\nIn RL, there's definitely a way for the model to actually influence the base objective, since the base objective is non-stationary and depends in large part on the model's actions. Your training data (and thus base objective) depends on the trajectories resulting from interactions between your previous policy and the environment. Similarly, in CL, if the model exerts any influence on the data source it pulls from, it will be able to affect its future training data.\n\nUnfortunately there are some really weird cases here. If you're learning from human feedback or otherwise influencing the world to get humans to do things that then feed back into your training set, you create tons of feedback loops with a human in the loop that are really hard to reason about. I'm pretty confused about which of the things in this category should even count as gradient hacking; I haven't thought extensively about this case so everything in this section is low confidence. The good news is that I think even if gradient hacking is totally plausible in RL/CL, being able to show that simple predictive models like LMs trained on stationary distributions can't gradient hack under certain reasonable conditions is a huge deal. \n\nFor concreteness, one super simple example of a nonstationary gradient hacker with CL would be models that generate funny outputs that humans like and tend to repost elsewhere on the internet, which then gets scraped and fed into the model again, leading to a model that produces more of those outputs, etc in a feedback loop. This particular case seems fairly benign but it's at least not *a priori* obvious that there's no malicious version of this involving much more complex interactions with the world.\n\nOne related possibility is that training models on data containing previous models' outputs (or data influenced by the previous model's outputs) does something similar to RL/CL. This way, although each model can't affect its own training data, it can affect the training data of the next model. For example, GPT-n outputs and people talking about GPT-n outputs and so on are present on the internet and therefore being scooped up into the GPT-(n+1) training set. This is essentially a really slow version of CL, and this doesn't allow a model to hack itself but rather only future models so it doesn't technically count as gradient hacking, but I think it's still potentially useful to think about, if only to help develop intuitions for the RL/CL cases.\n\nI don't think it's possible to entirely eliminate this effect, since your only options would be to restrict yourself to pre-LM data (which limits how much data you have and will get further and further out of distribution as the world changes) or to try and remove any trace of LMs from the data (which seems basically impossible, especially if LMs are ubiquitous in the future, since we have to not just remove LM outputs but also anything downstream of LM outputs; [relevant movie](https://en.wikipedia.org/wiki/Good_Bye,_Lenin!)). However, I also don't think this is likely to be a serious issue. I think the main argument against simple predictive models like LMs doing nonstationary gradient hacking this way being a serious concern looks something like a) complex interactions get washed out by chaos in the real world and b) there's a very tight limit to what kinds of influence the model can exert on its future self using only simple interactions (i.e the funny output example might be as far as it gets), c) if it relies on humans amplifying something, then that feedback loop could and probably is happening without LMs anyways (i.e people share funny things whether or not LMs exist), and d) RL is already really slow and inefficient already and this process would be like taking an RL step once a year.\n\nImplications\n------------\n\nThe framing in this post helps us subdivide the problem space of gradient hacking into several quite distinct mechanisms. Crucially, this lets us assume a lot of stuff in each case: for convergent gradient hackers we only need to consider local minima and for nonconvergent gradient hackers we only need to consider ways to break convergence. (For nonstationary I don't really know what to do yet, but regular supervised training avoids this class of failure cases.) An example of the type of argument I expect is if we could show that no model for a given architecture and objective exhibiting gradient hacker like behaviour can protect itself from gradients, we could rule out the possibility of convergent gradient hackers entirely for that particular setting.\n\nHopefully, we can formalize these failure cases (and the overarching argument) and rigorously show conditions under which each of these is provably impossible (or, failing that, metrics that we can compute throughout training to tell us when these things are happening). Then, we can design our model architectures and training processes to satisfy these conditions as much as possible. That being said, there are a *lot* of nuances so I don't expect this to be easy at all.\n\nIf we can avoid gradient hacking, this significantly limits how powerful deceptively aligned mesaoptimizers can get. Without gradient hacking, I don't think I see any plausible mechanisms for a [mesaoptimizer covering up its own tracks from our interpretability tools](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking), and it also severely limits the things it can do to the rest of the model; in particular, I think it's basically impossible for a deceptively aligned mesaoptimizer to take control of the entire model and [make the base objective near-irrelevant](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking?commentId=PdLRMvr84e9YJhYGu), or anything nearly that drastic.",
      "plaintextDescription": "[Epistemic status: brainstorming, less confused than last time since it seems to provide for a nice taxonomy of things to look into rather than just a bunch of random loose threads.]\n\nThis post would not exist without the EleutherAI discord and all the wonderful people there. Special thanks to Evan Hubinger, John Schulman, and Adam Shimi for providing feedback on drafts of this post.\n\nThe goal of this post is to identify a different way of framing the gradient hacking problem, connect it to our intuitions, and use it to create a rough taxonomy of different types of potential gradient hacking. Hopefully, using this taxonomy, we can tackle more well-defined subproblems to narrow down the space of things that could potentially gradient hack.\n\nWhen we think about gradient hacking, the most intuitive framing is to consider some kind of agent embedded inside a larger network (like a GPT) that somehow intentionally modifies the loss landscape of the larger network with respect to the base loss, and that this modification makes it so that in optimizing for the base objective, the base optimizer also happens to optimize the mesaobjective. Here I consider the base objective to be a function Θ→R from the params of the network to the reals, that has all the training data baked in for simplicity, and the mesaobjective another function Θ→R, possibly with some constraint that both objectives have to be indifferent between models which behave the same on all inputs. The \"somehow\" is often considered to be some kind of perturbing or otherwise making the output of the larger network worse whenever the mesaobjective isn't met, therefore creating an incentive for gradient descent to improve the mesaobjective. One example of this line of thinking can be found in my last post about gradient hacking. Unfortunately, I think there are some confusions with this framing. \n\n\nFull loss landscapes\nFirst off, the base loss landscape of the entire model is a function Θ→R that's the same across all",
      "wordCount": 3610
    },
    "tags": [
      {
        "_id": "bbdbpGWWPMfKBzk7z",
        "name": "Gradient Hacking",
        "slug": "gradient-hacking"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "BFfySy4t7uiTfFq5g",
    "title": "Dissolving the Experience Machine Objection",
    "slug": "dissolving-the-experience-machine-objection",
    "url": null,
    "baseScore": 16,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2021-10-03T16:56:28.312Z",
    "contents": {
      "markdown": "The experience machine objection is often levied against utilitarian theories that depend on utility being a function of observations or brain states. The version of the argument I'm considering here is a more cruxified version that strips out a bunch of confounding factors and goes something like this: imagine you had a machine you could step into that would perfectly simulate your experience in the real world. The objection goes that since most people would feel at least slightly more willing to stay in reality than go in the machine, there's at least some value to being in the \"real\" world, therefore we can't accept any branch of utilitarianism that assumes utility is soley a function of observations or brain states.\n\nI think if you accept the premise that the machine somehow magically truly simulates perfectly and indistinguishably from actual reality, in such a way that there is absolutely no way of knowing the difference between the simulation and the outside universe, then the simulated universe is essentially isomorphic to reality, and we should be fully indifferent. I'm not sure it even makes sense to say either universe is more \"real\", since they're literally identical in every way that matters (for the differences we can't observe even in theory, I appeal to Newton's flaming laser sword). Our intuitions here should be closer to stepping into an identical parallel universe, rather than entering a simulation. \n\n*However,* I think it's not actually possible to have such a perfect experience machine, which would explain our intuition for not wanting to step inside. First, if this machine simulates reality using our knowledge of physics at the time, it's entirely possible that there are huge parts of physics you would never be able to find out about inside the machine, since you can never be 100% sure whether you really know the Theory of Everything. Second, this machine would have to be smaller than the universe in some sense, since it's part of the universe. As a result, the simulation would probably have to cut corners or reduce the size of the simulated universe substantially to compensate. \n\nThese things both impact the possible observations you can have inside the machine, which allows you to distinguish between simulation and reality, which means it's totally valid to penalize the utility of living inside a simulation by some amount depending on how strongly you feel about the limitations (and how good the machine is). Just because there's a penalty doesn't mean that other factors can't overcome that, though. Lots of versions of the objection try to sweeten the deal for the world inside the machine further (\"you can experience *anything* you want\"/\"you get maximum serotonin\"/etc); this doesn't really change the core of the argument of whether our utility function should depend on anything other than observations. If the perks are really good and you care less about the limitations than the perks, then it makes perfect sense to go inside the machine; if you care more about the limitations than the perks, it makes perfect sense not to go inside the machine.\n\nThe crux of the experience machine thought experiment is that even when all else is held constant, we should assign epsilon more utility to whatever is \"real\", therefore utility does not depend soley on your observations/brain states. I argue that this epsilon penalty makes sense given practical limitations to any real experience machines, which is probably what informs our intuitions, and that if you somehow handwaved those limitations way then we really truly should be indifferent.",
      "plaintextDescription": "The experience machine objection is often levied against utilitarian theories that depend on utility being a function of observations or brain states. The version of the argument I'm considering here is a more cruxified version that strips out a bunch of confounding factors and goes something like this: imagine you had a machine you could step into that would perfectly simulate your experience in the real world. The objection goes that since most people would feel at least slightly more willing to stay in reality than go in the machine, there's at least some value to being in the \"real\" world, therefore we can't accept any branch of utilitarianism that assumes utility is soley a function of observations or brain states.\n\nI think if you accept the premise that the machine somehow magically truly simulates perfectly and indistinguishably from actual reality, in such a way that there is absolutely no way of knowing the difference between the simulation and the outside universe, then the simulated universe is essentially isomorphic to reality, and we should be fully indifferent. I'm not sure it even makes sense to say either universe is more \"real\", since they're literally identical in every way that matters (for the differences we can't observe even in theory, I appeal to Newton's flaming laser sword). Our intuitions here should be closer to stepping into an identical parallel universe, rather than entering a simulation. \n\nHowever, I think it's not actually possible to have such a perfect experience machine, which would explain our intuition for not wanting to step inside. First, if this machine simulates reality using our knowledge of physics at the time, it's entirely possible that there are huge parts of physics you would never be able to find out about inside the machine, since you can never be 100% sure whether you really know the Theory of Everything. Second, this machine would have to be smaller than the universe in some sense, since it's part of the universe. A",
      "wordCount": 586
    },
    "tags": [
      {
        "_id": "Zs4nYLkNr7Rbo4mAP",
        "name": "Utilitarianism",
        "slug": "utilitarianism"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "c9NSeCapaKtP6kvQD",
    "title": "Gradient descent is not just more efficient genetic algorithms",
    "slug": "gradient-descent-is-not-just-more-efficient-genetic",
    "url": null,
    "baseScore": 56,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2021-09-08T16:23:46.996Z",
    "contents": {
      "markdown": "I think one common intuition when thinking about gradient descent (GD) is to think about it as more efficient genetic algorithms (GAs). I certainly used to think this way—after all, taking a bunch of random mutations and keeping the ones that work well sounds basically just like a slower version of finding the steepest direction in the first place. GAs are also much more intuitive than GD, so it's tempting to think about GD in terms of them. Unfortunately, there are some instances where they behave completely differently, which is really relevant for thinking about things like gradient hacking. This post provides one such example.\n\nRedundancy and Submodule Protection\n-----------------------------------\n\nImagine you're given a network that has two identical submodules, and some kind of combining function where if it detects the outputs from both submodules are the same it passes the value through but if they're different it explodes and outputs zero or something totally random, or whatever. This is a natural idea to come up with if your goal is to ensure the optimizer doesn't mess with these modules, for example, if you're trying to protect the mesaobjective encoding for gradient hacking. \n\nIt's easy to convince yourself that this is effective against GAs: since mutations are inserted at random, the probability that both modules are mutated the same way is very small, and so any model that changes either of these modules will do very poorly since the output will be totally useless. \n\nGD on the other hand will happily change both modules at the same time, since the direction of steepest descent is the one where both submodules change at the same time. This is even despite the fact that partial derivatives are taken assuming all other parameters stay the same, because partial derivatives don't care about the cliff just epsilon away but only the slope at the exact point where both modules are the same. Another way of thinking about it is the gradient necessarily points in the direction of steepest ascent, even if that direction isn't axis-aligned. If you're still not convinced, see [this comment](https://www.lesswrong.com/posts/KfX7Ld7BeCMQn5gbz/obstacles-to-gradient-hacking?commentId=zhrA4hEj4yPpjHmdm) for a proof that there exists no combining function (under modest constraints) that can protect submodules this way.\n\nTakeaways\n---------\n\nThe main takeaway from this is that gradient descent is really unintuitive sometimes. When in doubt, it's probably a good idea to work out the gradient by hand.",
      "plaintextDescription": "I think one common intuition when thinking about gradient descent (GD) is to think about it as more efficient genetic algorithms (GAs). I certainly used to think this way—after all, taking a bunch of random mutations and keeping the ones that work well sounds basically just like a slower version of finding the steepest direction in the first place. GAs are also much more intuitive than GD, so it's tempting to think about GD in terms of them. Unfortunately, there are some instances where they behave completely differently, which is really relevant for thinking about things like gradient hacking. This post provides one such example.\n\n\nRedundancy and Submodule Protection\nImagine you're given a network that has two identical submodules, and some kind of combining function where if it detects the outputs from both submodules are the same it passes the value through but if they're different it explodes and outputs zero or something totally random, or whatever. This is a natural idea to come up with if your goal is to ensure the optimizer doesn't mess with these modules, for example, if you're trying to protect the mesaobjective encoding for gradient hacking. \n\nIt's easy to convince yourself that this is effective against GAs: since mutations are inserted at random, the probability that both modules are mutated the same way is very small, and so any model that changes either of these modules will do very poorly since the output will be totally useless. \n\nGD on the other hand will happily change both modules at the same time, since the direction of steepest descent is the one where both submodules change at the same time. This is even despite the fact that partial derivatives are taken assuming all other parameters stay the same, because partial derivatives don't care about the cliff just epsilon away but only the slope at the exact point where both modules are the same. Another way of thinking about it is the gradient necessarily points in the direction of steepest ascent,",
      "wordCount": 389
    },
    "tags": [
      {
        "_id": "Myz8DA9AghgpNei9i",
        "name": "Gradient Descent",
        "slug": "gradient-descent"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "KfX7Ld7BeCMQn5gbz",
    "title": "Obstacles to gradient hacking",
    "slug": "obstacles-to-gradient-hacking",
    "url": null,
    "baseScore": 28,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2021-09-05T22:42:22.876Z",
    "contents": {
      "markdown": "*\\[Epistemic status: brainstorming, still somewhat confused, with a bunch of loose threads left.\\]*\n\nThis post is essentially the summary of a long discussion on the [EleutherAI discord](https://discord.gg/zBGx3azzUn) about trying to exhibit [gradient hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking) in real models by hand crafting an example. The discussion was sparked by [this post](https://www.lesswrong.com/posts/S2jsBsZvqjBZa3pKT/approaches-to-gradient-hacking). We didn't end up coming up with any good examples (or proofs of nonexistence) but hopefully this post is helpful for anyone else trying to construct gradient hacking examples.\n\nNote that because our goal is to construct a concrete example of gradient hacking, when I write about \"what we want'' and \"unfortunate\" roadblocks, those are from the perspective of a mesaoptimizer (or a researcher trying to construct an example of a mesaoptimizer to study), not from the perspective of a researcher attempting to build aligned AI.\n\n*Thanks to Adam Shimi, AI_WAIFU, Charles Foster, Evan Hubinger, Kyle and Laria, Stella Biderman, and alstroemeria313 for their contributions to the discussion.*\n\nFormulation\n-----------\n\nWe mostly considered the following specific formulation since it seemed like a reasonably general framework. \n\nThere is some base model being trained, which contains an honest subnetwork \\\\(h\\\\) that attempts to solve for the base objective, a mesaobjective embedded somewhere in the model’s weights, and a malicious subnetwork (or mesaoptimizer, though I prefer the former term because it makes it clear that we're talking about the subnetwork in particular) that attempts to use the gradient to optimize itself for the mesaobjective. \n\nIn particular, we assume the mesaobjective looks a lot like a supervised learning objective, since it seems to be intuitively the most useful kind of mesaobjective. In other words, the objective looks like minimizing some loss over a dataset. To solve the problem of getting this training data to the malicious subnetwork, the mesaobjective is implicitly embedded in the form of a mesa loss function \\\\(L\\\\) and a pair of functions \\\\(f\\\\) and \\\\(g\\\\) that take training data given to the model from the base optimization process and creates training pairs to give to the malicious model. To be more concrete, the mesaloss is \\\\(L(m(f(x)), g(x))\\\\) for any training data \\\\(x\\\\). This loss is then used in an enforcement mechanism that perturbs the output from the honest model in some way before outputting it. \n\nWhile it may initially seem like requiring the malicious model’s training data to be derived from the input data is a big constraint, I don’t expect it to be a big problem in, say, big LMs where the training data is the internet. Also, since both and are derived from \\\\(x\\\\) within the model, it might seem like there is a chicken and egg problem where the model can only learn to fit things it can already create data for, but this is actually not a major problem. For example, suppose the base model classifies arXiv papers. Then \\\\(f\\\\) could pick out the body of the paper and \\\\(g\\\\) the abstract, and \\\\(m\\\\) would learn to summarize, which is highly nontrivial.\n\nHere’s a diagram illustrating this formulation:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b70e84a1104b83aa98f5d6cc7ae059d9b25138e651cbb7e6.jpg)\n\n\\\\(\\hat y = h(x) + L(m(f(x)), g(x)) \\cdot \\mathrm{noise}\\\\)   \n\\\\(h\\\\) can optionally also depend on \\\\(f\\\\) and \\\\(g\\\\). \n\n\\\\(\\\\)\n\n(Trying to) build an example of gradient hacking\n------------------------------------------------\n\nIn order for gradient hacking to happen, it must be possible to prevent gradient updates to \\\\(f\\\\) and \\\\(g\\\\) and \\\\(L\\\\), and to the enforcement mechanism that perturbs the output of the honest subnetwork. If we can’t do this, then the mesaobjective will drift to become easier (and eventually become a constant function) and the enforcement mechanism will be disabled entirely, causing the mesaoptimizer to become completely removed. We want it so that the only way to reduce the perturbation is to make m better at the mesaobjective. Also, while the enforcement mechanism and \\\\(L\\\\) cannot be updated, they must still allow gradients to flow through themselves, since otherwise m would receive no gradients. \n\nThere are a few ways to protect a subnetwork \\\\(f\\\\) from being modified by gradient descent that I can think of (non-exhaustive list):\n\n*   Taking advantage of things hard coded into the model architecture - basically instead of learning \\\\(f\\\\) as part of the network, use some structure from the architecture instead. Obviously has the disadvantage of being constrained by the architecture\n*   Doing some kind of stop-gradient that passes information through but prevents gradient updates. This could be using a stop gradient that's part of the architecture or training process, or learning some kind of staircase thing, or abusing floating point errors, or something else.\n*   Incentivize some other part of the model to keep it stable - the hope is that the current configuration of the protected subnetwork is near-optimal for the purposes of some other component, and any small nudges to \\\\(f\\\\) will be summarily reverted by that other component to keep \\\\(f\\\\) stable. In particular, \\\\(h\\\\) seems like a natural candidate to make depend on \\\\(f\\\\) and \\\\(g\\\\). Unfortunately, it’s not guaranteed that \\\\(h\\\\) will actually want to keep \\\\(f\\\\) where we want it to and it may move \\\\(f\\\\) around to suit its purposes better. To make this work, \\\\(f\\\\) has to already be an optimal use of parameters for \\\\(h\\\\), and in general this seems pretty unstable.\n*   Multiplying the output by zero - there’s a really easy way to protect a subnetwork \\\\(f\\\\) from getting gradients - just multiply it by zero before using it anywhere else! Unfortunately this means f also can’t affect anything else at all during training since there’s no information in the output, which makes it pretty useless.\n\nStop gradient related tricks don't work for \\\\(L\\\\) and the enforcement mechanism because that would prevent the gradient from getting to \\\\(m\\\\). Also, it seems pretty unlikely that the output of these is too useful for anything else in the model so it would probably be hard to use something else depending on it to pin it in place. This means the best solution is probably to exploit some hard coded structure in the architecture. While some current architectures come close to being useful (i.e an attention layer with a query as the malicious subnetwork prediction, key as the mesa target \\\\(g(x)\\\\), and value as the output from the honest subnetwork \\\\(h(x)\\\\) would work if keys and queries were first scaled to unit length, so that cosine similarly instead of dot product were used in the attention layer), it's certainly not reliable across architectures.\n\nAssuming we could fix that problem, the other problem is we need \\\\(f\\\\) and \\\\(g\\\\) to remain stable. We *can* use a stop gradient here. The most obvious way to do so is to have a staircase function with small steps so it essentially passes the input through with zero gradient. Unfortunately, it doesn't seem obvious that a staircase can be easily learned and will remain stable. We could also make \\\\(h\\\\) depend on \\\\(f\\\\) and \\\\(g\\\\), but that places constraints on what \\\\(f\\\\) and \\\\(g\\\\) can be and also potentially opens \\\\(f\\\\) and \\\\(g\\\\) up to being moved around by \\\\(h\\\\) to be more useful for it. One other way to get a stop gradient is to use one that's already part of the training process. In particular, one interesting example is RL training of LMs, where the model outputs are discretized and fed back into the model. Finally, it's possible we could exploit floating point numerical stability and rounding. Perhaps a model can abuse rounding the gradient to zero while still passing a few bits of information through, or increase the scale of the gradient elsewhere in the model to force the loss scale down and drive gradients to zero. \n\nThings to look into from here\n-----------------------------\n\n*   How can gradient hacking happen in RL training of LMs?\n*   Can floating point numerical stability be abused to create a stop gradient? (i.e make gradients underflow to zero in bf16 or something)\n*   Can we formalize what if means to not have anything weird like a staircase (one particular formalism that might work is continuous everywhere and differentiable everywhere except a set of countable size; merely continuous everywhere is insufficient because of things like the cantor function, and differentiable everywhere excludes things like ReLU which we want to keep), and can we prove that models that satisfy these constraints cannot gradient hack?",
      "plaintextDescription": "[Epistemic status: brainstorming, still somewhat confused, with a bunch of loose threads left.]\n\nThis post is essentially the summary of a long discussion on the EleutherAI discord about trying to exhibit gradient hacking in real models by hand crafting an example. The discussion was sparked by this post. We didn't end up coming up with any good examples (or proofs of nonexistence) but hopefully this post is helpful for anyone else trying to construct gradient hacking examples.\n\nNote that because our goal is to construct a concrete example of gradient hacking, when I write about \"what we want'' and \"unfortunate\" roadblocks, those are from the perspective of a mesaoptimizer (or a researcher trying to construct an example of a mesaoptimizer to study), not from the perspective of a researcher attempting to build aligned AI.\n\nThanks to Adam Shimi, AI_WAIFU, Charles Foster, Evan Hubinger, Kyle and Laria, Stella Biderman, and alstroemeria313 for their contributions to the discussion.\n\n\nFormulation\nWe mostly considered the following specific formulation since it seemed like a reasonably general framework. \n\nThere is some base model being trained, which contains an honest subnetwork h that attempts to solve for the base objective, a mesaobjective embedded somewhere in the model’s weights, and a malicious subnetwork (or mesaoptimizer, though I prefer the former term because it makes it clear that we're talking about the subnetwork in particular) that attempts to use the gradient to optimize itself for the mesaobjective. \n\nIn particular, we assume the mesaobjective looks a lot like a supervised learning objective, since it seems to be intuitively the most useful kind of mesaobjective. In other words, the objective looks like minimizing some loss over a dataset. To solve the problem of getting this training data to the malicious subnetwork, the mesaobjective is implicitly embedded in the form of a mesa loss function L and a pair of functions f and g that take training data giv",
      "wordCount": 1287
    },
    "tags": [
      {
        "_id": "bbdbpGWWPMfKBzk7z",
        "name": "Gradient Hacking",
        "slug": "gradient-hacking"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "EmxfgPGvaKqhttPM8",
    "title": "Thoughts on the Alignment Implications of Scaling Language Models",
    "slug": "thoughts-on-the-alignment-implications-of-scaling-language",
    "url": null,
    "baseScore": 82,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2021-06-02T21:32:08.555Z",
    "contents": {
      "markdown": "\\[Epistemic status: slightly rambly, mostly personal intuition and opinion that will probably be experimentally proven wrong within a year considering how fast stuff moves in this field\\]\n\nThis post is also available on my [personal blog](https://leogao.dev/2021/06/02/Thoughts-on-the-Alignment-Implications-of-Scaling-Language-Models/).\n\n*Thanks to Gwern Branwen, Steven Byrnes, Dan Hendrycks, Connor Leahy, Adam Shimi, Kyle and Laria for the insightful discussions and feedback.*\n\n**Background**\n--------------\n\nBy now, most of you have probably heard about GPT-3 and what it does. There’s been a bunch of different opinions on what it means for alignment, and this post is yet another opinion from a slightly different perspective.\n\nSome background: I'm a part of EleutherAI, a decentralized research collective (read: glorified discord server - come join us on [Discord](https://discord.gg/ejc9GWY) for ML, alignment, and dank memes). We're best known for our ongoing effort to create a GPT-3-like large language model, and so we have a lot of experience working with transformer models and looking at scaling laws, but we also take alignment very seriously and spend a lot of time thinking about it (see [here](https://blog.eleuther.ai/why-release-a-large-language-model/) for an explanation of why we believe releasing a large language model is good for safety). The inspiration for writing this document came out of the realization that there's a lot of tacit knowledge and intuitions about scaling and LMs that's being siloed in our minds that other alignment people might not know about, and so we should try to get that out there. (That being said, the contents of this post are of course only my personal intuitions at this particular moment in time and are definitely not representative of the views of all EleutherAI members.) I also want to lay out some potential topics for future research that might be fruitful.\n\nBy the way, I did consider that the scaling laws implications might be an infohazard, but I think that ship sailed the moment the GPT-3 paper went live, and since we’ve already been in a race for parameters for some time (see: [Megatron-LM](https://arxiv.org/abs/1909.08053), [Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/), [Switch Transformer](https://arxiv.org/abs/2101.03961), [PanGu-α/盘古α](https://arxiv.org/abs/2104.12369), [HyperCLOVA](https://www.navercorp.com/promotion/pressReleasesView/30546), [Wudao/悟道 2.0](https://zhuanlan.zhihu.com/p/377047779), among others), I don’t really think this post is causing any non-negligible amount of desire for scaling.\n\n**Why scaling LMs might lead to Transformative AI**\n---------------------------------------------------\n\n### **Why natural language as a medium**\n\nFirst, we need to look at why a perfect LM could in theory be Transformative AI. Language is an extremely good medium for representing complex, abstract concepts compactly and with little noise. Natural language seems like a very efficient medium for this; images, for example, are much less compact and don’t have as strong an intrinsic bias towards the types of abstractions we tend to draw in the world. This is not to say that we shouldn’t include images at all, though, just that natural language should be the focus.\n\nSince text is so flexible and good at being entangled with all sorts of things in the world, to be able to model text perfectly, it seems that you'd have to model all the processes in the world that are causally responsible for the text, to the “**resolution**” necessary for the model to be totally indistinguishable from the distribution of real text. For more intuition along this line, the excellent post [Methods of prompt programming](https://generative.ink/posts/methods-of-prompt-programming/#the-reverse-engineered-dynamics-of-language) explores, among other ideas closely related to the ideas in this post, a bunch of ways that reality is entangled with the textual universe:\n\n> A novel may attempt to represent psychological states with arbitrarily fidelity, and scientific publications describe models of reality on all levels of abstraction. \\[...\\] A system which predicts the dynamics of language to arbitrary accuracy *does* require a theory of mind(s) and a theory of the worlds in which the minds are embedded. The dynamics of language do not float free from cultural, psychological, or physical context\\[.\\]\n\n### **What is resolution**\n\nI’m using **resolution** here to roughly mean how many different possible world-states get collapsed into the same textual result; data being high resolution would mean being able to narrow down the set of possible world states to a very small set. \n\n(Brief sidenote: this explanation of resolution isn’t mandatory for understanding the rest of the post so you can skip the rest of this section if you’re already convinced that it would be possible to make a LM have an internal world model. Also, this section is largely personal intuition and there isn’t much experimental evidence to either confirm or deny these intuitions (yet), so take this with a grain of salt)\n\nHere’s an example that conveys some of the intuition behind this idea of resolution and what it means for a system trying to model it. Imagine you’re trying to model wikipedia articles. There are several levels of abstraction this can be done on, all of which produce the *exact same* output but work very differently internally. You could just model the “text universe” of wikipedia articles—all of the correlations between words—and then leave it at that, never explicitly modelling anything at a higher level of abstraction. You could also model the things in the real world that the wikipedia articles are talking about as abstract objects with certain properties—maybe that’s sunsets, or Canadian geese, or cups of coffee—and then model the patterns of thought at the level of emotions and beliefs and thoughts of the humans as they observe these objects or phenomena and then materialize those thoughts into words. This still gets you the same distribution, but rather than modelling at the level of text, you’re modelling at the level of *human-level abstractions*, and also the process that converts those abstractions into text. Or you could model the position and momentum of every single particle in the Earth (ignoring quantum effects for the sake of this example), and then you could construct a ground up simulation of the entire Earth, which contains sunsets and geese and wikipedia editors.\n\nObviously, the easiest way is to just model at the level of the text. Humans are just really complicated, and certainly modelling the entire universe is hugely overkill. But let's say we now add a bunch of data generated by humans that has little mutual information with the wikipedia articles, like tweets. This increases the *resolution* of the data, because there are a bunch of world states you couldn't tell apart before that you now can. For the text model, as far as it's concerned, this data is totally unrelated to the wikipedia data, and it has to model it mostly separately except for maybe grammar. Meanwhile, the human-modelling system can largely reuse its model with only minor additions. (The universe model just flips a single flag corresponding to whether twitter data is recorded in the corpus). \n\nAs you keep increasing the resolution by adding more and more data (such as books, images, videos, and scientific data) generated by the kinds of systems we care about, the cost of modelling the text universe rapidly increases, but the cost of modelling abstractions closer to reality increases much slower. An analogy here is if you only model a record of which side a flipped coin lands on, it's cheaper to just use a random number generator to pick between heads and tails, instead of simulating all the physics of a coin toss. However, if you're trying to model a 4k60fps video of a coin being flipped, you might instead want to actually simulate a coin being flipped and then deduce what the video should look like. \n\nAnother way of looking at this is that resolution is how much Bayesian evidence about the universe can be obtained by updating on the dataset from a universal prior.\n\nYou might be wondering why it matters what the model's internal representation looks like if it outputs the exact same thing. Since having useful, disentangled internal models at roughly human-level abstractions is crucial for ideas like [Alignment By Default](https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default), a model which can be harnessed through querying but which doesn’t contain useful internal models would be pretty good for capabilities (since it would be indistinguishable from one that does, if you just look at the output) and pretty bad for alignment, which is the worst case scenario in terms of the alignment-capabilities tradeoff. Also, it would be more brittle and susceptible to problems with off-distribution inputs, because it's right for the wrong reasons, which also hurts safety. If models only ever learn to model the text universe, this problem might become very insidious because as LMs get bigger and learn more, more and more of the text universe will be \"on-distribution\" for it, making it feel as though the model has generalized to all the inputs we can think of giving it, but breaking the moment the world changes enough (i.e possibly because of the actions of an AI built using the LM, for example). Therefore, it's very important for safety for our models to have approximately human level abstractions internally. Of course, internal models at the right level of abstraction isn't a guarantee that the model will generalize to any given amount ([Model Splintering](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) will probably become an issue when things go really off-distribution), but the failure cases would likely be more predictable.\n\nIn theory, you could also go *too high resolution* and end up with a model that models everything at the atomic level which is also kind of useless for alignment. In practice, though, this is likely less of a tradeoff and more of a “more resolution is basically always better\" situation since even the highest resolution data we can possibly get is peanuts in the grand scheme of things.\n\nAnother minor hitch is that the process through which physical events get picked up by humans and then converted into words is quite lossy and makes the resolution somewhat coarse; in other words, there are a lot of physical phenomena that we'd want our LM to understand but modelling what humans think about them is way easier than actually modelling the phenomena. This probably won't be a problem in practice, though, and if it is we can always fix it by adding non-human-generated data about the phenomena in question.\n\nThankfully, I don't expect resolution to be a hard problem to solve in practice. Making the data harder to model is pretty easy. We could look towards massively multilingual data, since texts in all languages describe similar concepts at human abstraction levels but in a way that increases the complexity of the text itself significantly. We could even use images or video, which are pretty high resolution (pun not intended), available in large supply, and already being pursued by a bunch of people in the form of multimodal models. The existence of [Multimodal Neurons](https://distill.pub/2021/multimodal-neurons/) implies that it’s possible for models to internally unify these different modalities into one internal representation.\n\nIf that’s not enough, we could do some kind of augmentation or add synthetic data to the training set, designed in such a way that modelling the underlying processes that cause the synthetic data is significantly easier than directly modelling the distribution of text. There are a huge host of ways this could potentially be done, but here are a few random ideas: we could generate tons of sentences by randomly composing facts from something like [Cyc](https://en.wikipedia.org/wiki/Cyc); augment data by applying different basic ciphers; or add automatically generated proofs of random statements using proof assistants like [Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant))/[Coq](https://en.wikipedia.org/wiki/Coq).\n\n### **Where scaling laws fit in**\n\nOf course, we don’t have perfect LMs (yet). The main evidence in practice for scaling LMs being a viable path to this ideal case is in scaling laws ([Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)). Essentially, what the scaling laws show is that empirically, you can predict the optimal loss achievable for a certain amount of compute poured into the model with striking accuracy, and that this holds across several orders of magnitude. \n\nIf you extrapolate the line out, it approaches zero as compute goes to infinity, which is totally impossible because natural language must have *some* nonzero irreducible loss. Therefore, at some point the scaling law must stop working. There are two main ways I could see this happen.\n\n### **Scenario 1: Weak capabilities scenario**\n\nOne possible way is that the loss slowly peels away from the scaling law curve, until it either converges to some loss that isn't the irreducible loss and refuses to go down further, or it approaches the irreducible loss but takes forever to converge, to the point that the resources necessary to get the loss close enough to irreducible for the level of capabilities we talk about in this post would be astronomically huge and not worth it. Of course, in practice since we don't actually know what the irreducible loss is, these two cases will be pretty hard to tell apart, but the end result is the same: we keep scaling, and the models keep getting better, but never quite enough to do the world modelling-y things well enough to be useful, and eventually people give up and move on to something else. \n\nOne main reason this would happen is that negative log likelihood loss rewards learning low order correlations much more than high order correlations; learning how to write a grammatically correct but factually incorrect paragraph achieves significantly better loss than a factually correct but grammatically incorrect paragraph. As such, models will learn all the low order correlations first before even thinking about higher order stuff- sort of like an [Aufbauprinzip](https://en.wikipedia.org/wiki/Aufbau_principle) for LMs. This is why models like GPT-3 have impeccable grammar and vocabulary but their long term coherence leaves something to be desired. At the extreme, a model might spend a ton of capacity on memorizing the names of every single town on Earth or learning how to make typos in exactly the same way as humans do before it even budges on learning anything more high-order like logical reasoning or physical plausibility. Since there’s just so many low order correlations to be learned, it might be that any model that we could reasonably ever train would get stuck here and would therefore never get to the high order correlations. (more on this idea of models learning low-order correlations first: [The Scaling Hypothesis](https://www.gwern.net/Scaling-hypothesis#why-does-pretraining-work))\n\nPlus, once the model gets to high order correlations, it’s not entirely guaranteed it would actually be able to learn them easily. It would depend heavily on humans (and the things humans model, so basically most of the known world) being easy to model, which.. doesn’t seem to be the case, but I leave open the possibility that this is my anthropocentrism speaking. \n\nAlso, it might be that transformers and/or SGD just can't learn high order correlations and reasoning at all, possibly because there are possibly architectural constraints of transformers that start kicking in at some point (see [Can you get AGI from a Transformer?](https://www.alignmentforum.org/posts/SkcM4hwgH3AP6iqjs/can-you-get-agi-from-a-transformer)). Some of these limitations might be fairly fundamental to how neural networks work and prevent *any* replacement architecture from learning high order reasoning. \n\nIf this scenario happens, then LMs will never become very capable or dangerous and therefore will not need to be aligned. \n\n### **Scenario 2: Strong capabilities scenario**\n\nThe other way is the model could follow the scaling law curve perfectly until it gets to the irreducible loss, and then forever cease to improve. This scenario could also happen if there’s an asymptotic approach to the irreducible loss that’s fast enough that with a reasonable amount of compute we can get the level of capabilities discussed in this post, which is arguably more likely for higher resolution data because it would be harder to model. This case would happen if past a certain critical size, the model is able to entirely learn the process that generates the text (i.e the humans writing text, and the systems that these humans are able to observe and model) down to the resolution permissible by text. This scenario would be.. kind of scary, because it would mean that scaling is literally all we need, and that we don’t even need that much more scaling before we're \"there\". \n\nIn this scenario, the prioritization of low-order correlations due to the negative log likelihood loss implies that high order traits will only improve slowly for potentially many orders of magnitude as the model is slowly able to memorize more and more inane things like the names of every famous person ever until suddenly it runs out of low hanging fruit and starts improving at constructing flawless logical deductions. This is, needless to say, *totally different* from how humans learn language or reasoning, and has led to a lot of both under and overestimating of GPT-3's capabilities. In fact, a lot of arguments about GPT-3 look like one person arguing that it must already be superintelligent because it uses language flawlessly and someone else arguing that since it has the physical/logical reasoning capabilities of a 3 year old, it's actually extremely unintelligent. This is especially dangerous because we will severely underestimate the reasoning potential of LMs right up until the last moment when the model runs out of low order correlations. There would be very little warning even just shortly before it started happening, and probably no major change in train/val loss trend during, though there would be a huge difference in downstream evaluations and subjective generation quality.\n\nAnd that’s all assuming we keep using negative log likelihood. There remains the possibility that there exists a loss function that does actually upweight logical coherence, etc, which would completely invalidate this intuition, and bring highly intelligent LMs even *faster*. On the bright side there’s the possibility this might be a positive, because it would likely lead to models with richer, more disentangled models inside of them, which would be useful for alignment as I mentioned earlier, though I don’t think this is nearly enough to cancel out the huge negative of advancing capabilities so much. Thankfully, it seems like the reason such a loss function doesn’t exist yet isn’t from lack of trying, it’s just really hard to make work. \n\nTo see just how much a different loss function could help, consider that cherrypicking from n samples is actually just a way of applying roughly \\\\(\\\\log n\\\\) bits of optimization pressure to the model, modulo some normalization factors (I also talk about this intuition in footnote 2 of [Building AGI Using Language Models](https://bmk.sh/2020/08/17/Building-AGI-Using-Language-Models/)). Since even lightly cherrypicked samples of GPT-3 are a lot more coherent than raw samples, I expect that this means we're probably not too far from a huge boost in coherence if we figure out how to apply the right optimization. This of course only provides a lower bound for how much the negative log likelihood loss needs to improve to get the same level of coherence, because it doesn’t align with our notion of quality exactly, as previously mentioned—the model could happily improve log n bits by memorizing random stuff rather than improving coherence.\n\nI also think it’s very unlikely that we will run into some totally insurmountable challenge with transformers or SGD that doesn't get patched over within a short period of time. This is mostly because historically, betting that there's something NNs fundamentally can't do is usually not a good bet, and every few years (or, increasingly, months) someone comes up with a way to surmount the previous barrier for NNs.\n\nAs one final piece of evidence, the scaling law and GPT-3 papers show that as your model gets bigger, it actually gets [*more sample efficient*](https://arxiv.org/pdf/2001.08361.pdf#page=4). To me this is *a priori* very counterintuitive given the curse of dimensionality. This seems to imply that bigger models are favored by the inductive biases of SGD, and to me suggests that bigger is the right direction to go. \n\nIf this scenario happens, we’re probably screwed if we don’t plan for it, and even if we do plan we might still be screwed. \n\n**What should we do?**\n----------------------\n\nThere are a number of possible alignment strategies depending on how LMs scale and what the properties of bigger LMs are like. This list is neither exhaustive nor objective, these are just a few possible outcomes that take up a good chunk of my probability mass at this moment in time and my opinions at this time. \n\n### **Reward model extraction**\n\nPossibly the most interesting (and risky) direction in my opinion, which I mentioned briefly earlier, is trying to extract some reward model for human values or even something like [CEV](https://intelligence.org/files/CEV.pdf) out of a big LM that has already learned to model the world. This idea comes from [Alignment by Default](https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default) (henceforth AbD for short), where we build some model and train it in a way such that it builds up an internal world model of what the AbD post calls “natural abstractions” (in the language of this post, those would be human-level abstractions that happen to be described in high resolution in natural language and are therefore more easily learned), and then we find some way to point to or extract the reward model we want inside that LM’s world model. One way this could be accomplished, as outlined in the AbD post, is to find some proxy for the reward signal we want and fine tune the LM on that proxy and hope that the model decides to point to an internal reward model. It might also turn out that there are other ways to do this, maybe by using interpretability tools to identify the subnetworks responsible for the LM’s understanding of CEV, or perhaps involve optimizing a continuous prompt to “select” the right part of the LM ([The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691), [Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm](https://arxiv.org/abs/2102.07350)). \n\nI expect making a LM that forms a usable internal model of some useful reward signal inside itself to be very difficult in practice and require a lot of trial and error, possibly requiring custom training data or methods to make the resolution for that reward signal much higher. As an aside, I think this is one major advantage of thinking in terms of resolution: since resolution is relative to the data, we don’t have to just hope that any given abstraction is natural; we can actually make certain abstractions “more natural” for AbD if we want, just by changing the data! The other major issue with this approach is that there’s no guarantee that a model extracted from a LM will be at all robust to being subject to optimization pressures, and certainly I wouldn’t expect this to work as a strong alignment solution, but rather just as a way to bootstrap strong alignment, since the goal of *any* LM based system, in my opinion, would be to build a weakly-aligned system with the main purpose of solving strong alignment (i.e I think [Bootstrapped Alignment](https://www.lesswrong.com/posts/teCsd4Aqg9KDxkaC9/bootstrapped-alignment) is a good idea). That being said, I still think it would be significantly more robust than anything handcrafted because human values are very complex and multifaceted ([Thou Art Godshatter](https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter)), and learned systems generally do better at understanding complex systems than handcrafted systems ([The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)). Also, it’s likely that in practice this extracted reward model wouldn’t be optimized against directly, but rather used as a component of a larger reward system. \n\nThis may sound similar to regular value learning, but I would argue this has the potential to be significantly more powerful because we aren't just confined to the revealed preferences of humans but theoretically any consistent abstract target that can be embedded in natural language, even if we can't formalize it, and the resulting model would hopefully be goodhart-robust enough for us to use it to solve strong alignment. Of course, this sounds extremely lofty and you should be skeptical; I personally think this has a low chance (10%ish) of actually working out, since a lot of things need to go right, but I still think more work in this direction would be a good idea.\n\nSome *very* preliminary work in this direction can be seen in [GPT-3 on Coherent Extrapolated Volition](https://generative.ink/posts/gpt-3-on-coherent-extrapolated-volition/), where it is shown that GPT-3 has at least some rudimentary understanding of what CEV is, though not nearly enough yet. The [ETHICS](https://arxiv.org/abs/2008.02275) dataset also seems like a good proxy for use in some kind of fine-tuning based AbD scheme.\n\nSome concrete research directions for this approach: interpretability work to try and identify useful models inside current LMs like GPT-2/3; implementing some rudimentary form of AbD to try and extract a model of something really simple; messing around with dataset compositions/augmentations to improve performance on world-modelling tasks (i.e PiQA, Winograd schemas, etc), which is hopefully a good proxy for internal model quality.\n\n### **Human emulation**\n\nWe could also just have the LM emulate Einstein, except with the combined knowledge of humanity, and at 1000x speed, or emulate a bunch of humans working together, or something similar. How aligned this option is depends a lot on how aligned you think humans are, and on how high fidelity the simulation is. Humans are not very strongly aligned (i.e you can’t apply a lot of optimization pressure to the human reward function and get anything remotely resembling CEV): humans are susceptible to wireheading and cognitive biases, have their own inner alignment problems ([Inner alignment in the brain](https://www.alignmentforum.org/posts/DWFx2Cmsvd4uCKkZ4/inner-alignment-in-the-brain)), and so on (also related: [The Fusion Power Generator Scenario](https://www.alignmentforum.org/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario)). Still, emulated humans are still more aligned than a lot of other possibilities, and it’s possible that this is the best option we have if a lot of the other options don’t pan out. Another limitation of human emulation is there’s an inherent ceiling to how capable the system can get, but I wouldn’t worry too much about it because it would probably be enough to bootstrap strong alignment. Related posts in this direction: [Solving the whole AGI control problem, version 0.0001 - Imitation](https://www.alignmentforum.org/posts/Gfw7JMdKirxeSPiAk/solving-the-whole-agi-control-problem-version-0-0001#4_1_Imitation)\n\nA minor modification to this would be to emulate other nonhuman agents, whether through prompting, RL finetuning, or whatever the best way to do so is. The major problems here are that depending on the details, nonhuman agents might be harder to reason about and harder to get the LM to emulate since they’d be off distribution, but this might end up as a useful tool as part of some larger system.\n\nSome concrete research directions for this approach: exploring prompt engineering or training data engineering to see how we can reliably get GPT-3 and similar LMs to act human-like; developing better metrics that capture human emulation quality better.\n\n### **Human amplification**\n\nThis option encompasses things like IDA and (to a lesser extent) Debate where we rely on human judgement as a component of the AI system, and use the LMs as the magic black box that imitates humans in IDA or the debaters in Debate. How aligned this option is depends a lot on how aligned IDA and Debate are (and how aligned people are). Also, depending on how true the factored cognition hypothesis is, there may be serious limits to how capable these systems can get, though conditioning on IDA working at all, I don’t think it’s likely that this will be a bottleneck for the same reasons as in the previous sections. Some kind of human amplification strategy does feel like the most established and “down to earth” of the proposals despite all this, however. Overall, I’m cautiously optimistic about IDA/Debate-like systems using LMs.\n\nThe feasibility of this option is also correlated with the last option (human emulation) because this is essentially putting (partial) human emulation in a loop. \n\nSome concrete research directions for this approach: implementing IDA for some kind of toy task using GPT-3 or similar LMs, possibly doing some kind of augmentation and/or retrieval system to squeeze the most out of the human data from the amplification step.\n\n### **Oracle / STEM AI**\n\nAnother option that has been proposed for making safe AI systems is to make an \"oracle\" only able to answer questions and with only the goal of answering those questions as accurately as possible. A specific variant of this idea is the STEM AI proposal (name taken from [An overview of 11 proposals for building safe advanced AI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai), though this idea is nothing new), which is essentially an oracle with a domain limited to only scientific questions, with the hope that it never gets good enough at modelling humans to deceive us.\n\nA bunch of people have argued for various reasons why oracle/tool AI isn’t necessarily safe or economically competitive (for a small sampling: [The Parable of Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic), [Analysing: Dangerous messages from future UFAI via Oracles](https://www.alignmentforum.org/posts/6WbLRLdmTL4JxxvCq/analysing-dangerous-messages-from-future-ufai-via-oracles), [Reply to Holden on 'Tool AI'](https://www.alignmentforum.org/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai), [Why Tool AIs Want to Be Agent AIs](https://www.gwern.net/Tool-AI)). I would argue that most of the safety concerns only apply to extremely strong alignment, and that oracles are probably safe *enough* for weak alignment, though I’m not very confident in this and I’m open to being proven wrong. This is the option that is most likely to happen by default, though, since even GPT-3 exhibits oracle-ish behavior with minimal prompting, whereas the other options I discussed are all various stages of theoretical. As such it makes sense to plan for what to do just in case this is the only option left.\n\nSome concrete research directions for this approach: exploring prompt engineering to see how we can reliably get GPT-3 and similar LMs to give its best-guess rather than what it thinks the median internet user would tend to say; exploring ways to filter training data for STEM-only data and looking at whether this actually pushes the model’s understandings of various topics in the direction we expect ([Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300) introduces a very fine grained evaluation which may be useful for something like this); interpretability work to see how the model represents knowledge and reasoning, so that we could possibly extract novel knowledge out of the model without even needing to query it (something like [Knowledge Neurons in Pretrained Transformers](https://arxiv.org/abs/2104.08696) seems like a good place to start).\n\n**Conclusion**\n--------------\n\nNatural language is an extremely versatile medium for representing abstract concepts and therefore given sufficiently high resolution data a language model will necessarily have to learn to represent and manipulate abstract concepts to improve loss beyond a certain point. Additionally, from the evidence we have from scaling laws, there is a fairly good chance that we will find ourselves in the strong capabilities scenario where this point is crossed only a few orders of magnitude in size from now, resulting in large language models quickly gaining capabilities, and making the first transformational AI a large language model. Finally, there are a number of existing prosaic alignment directions that are amenable to being applied to language models, which I hope will be explored further in the future.",
      "plaintextDescription": "[Epistemic status: slightly rambly, mostly personal intuition and opinion that will probably be experimentally proven wrong within a year considering how fast stuff moves in this field]\n\nThis post is also available on my personal blog.\n\nThanks to Gwern Branwen, Steven Byrnes, Dan Hendrycks, Connor Leahy, Adam Shimi, Kyle and Laria for the insightful discussions and feedback.\n\n\nBackground\nBy now, most of you have probably heard about GPT-3 and what it does. There’s been a bunch of different opinions on what it means for alignment, and this post is yet another opinion from a slightly different perspective.\n\nSome background: I'm a part of EleutherAI, a decentralized research collective (read: glorified discord server - come join us on Discord for ML, alignment, and dank memes). We're best known for our ongoing effort to create a GPT-3-like large language model, and so we have a lot of experience working with transformer models and looking at scaling laws, but we also take alignment very seriously and spend a lot of time thinking about it (see here for an explanation of why we believe releasing a large language model is good for safety). The inspiration for writing this document came out of the realization that there's a lot of tacit knowledge and intuitions about scaling and LMs that's being siloed in our minds that other alignment people might not know about, and so we should try to get that out there. (That being said, the contents of this post are of course only my personal intuitions at this particular moment in time and are definitely not representative of the views of all EleutherAI members.) I also want to lay out some potential topics for future research that might be fruitful.\n\nBy the way, I did consider that the scaling laws implications might be an infohazard, but I think that ship sailed the moment the GPT-3 paper went live, and since we’ve already been in a race for parameters for some time (see: Megatron-LM, Turing-NLG, Switch Transformer, PanGu-α/盘古α, Hy",
      "wordCount": 5037
    },
    "tags": [
      {
        "_id": "AKvmQFvDjxJNetTuk",
        "name": "Scaling Laws",
        "slug": "scaling-laws"
      },
      {
        "_id": "YWzByWvtXunfrBu5b",
        "name": "GPT",
        "slug": "gpt"
      },
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "fpEBgFE7fgpxTm9BF",
        "name": "Machine Learning  (ML)",
        "slug": "machine-learning-ml"
      },
      {
        "_id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "slug": "outer-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "BisjoDrd3oNatDu7X",
      "tag_name": "Outer Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "no9ftcYzcMLo8Zwj4",
    "title": "Building AGI Using Language Models",
    "slug": "building-agi-using-language-models",
    "url": "https://leogao.dev/2020/08/09/Building-AGI-Using-Language-Models/",
    "baseScore": 11,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2020-11-09T16:33:25.864Z",
    "contents": {
      "markdown": "> Despite the buzz around GPT-3, it is, in and of itself, not AGI. In many ways, this makes it similar to AlphaGo or Deep Blue; while approaching human abiliy in one domain (playing Chess/Go, or writing *really* impressively), it doesn’t really seem like it will do Scary AGI Things™ any more than AlphaGo is going to be turning the Earth into paperclips anytime soon. While its writings are impressive at emulating humans, GPT-3 (or any potential future GPT-x) has no memory of past interactions, nor is it able to follow goals or maximize utility. However, language modelling has one *crucial* difference from Chess or Go or image classification. Natural language essentially encodes information about the world—the *entire* world, not just the world of the Goban, in a much more expressive way than any other modality ever could. By harnessing the world model embedded in the language model, it may be possible to build a proto-​AGI.",
      "plaintextDescription": "> Despite the buzz around GPT-3, it is, in and of itself, not AGI. In many ways, this makes it similar to AlphaGo or Deep Blue; while approaching human abiliy in one domain (playing Chess/Go, or writing really impressively), it doesn’t really seem like it will do Scary AGI Things™ any more than AlphaGo is going to be turning the Earth into paperclips anytime soon. While its writings are impressive at emulating humans, GPT-3 (or any potential future GPT-x) has no memory of past interactions, nor is it able to follow goals or maximize utility. However, language modelling has one crucial difference from Chess or Go or image classification. Natural language essentially encodes information about the world—the entire world, not just the world of the Goban, in a much more expressive way than any other modality ever could. By harnessing the world model embedded in the language model, it may be possible to build a proto- AGI.",
      "wordCount": 156
    },
    "tags": [
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "w4RKBEGbvXah38PRi",
    "title": "GPT-3: A Summary",
    "slug": "gpt-3-a-summary",
    "url": "https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/",
    "baseScore": 20,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2020-06-02T18:14:54.380Z",
    "contents": {
      "markdown": "> With massive size comes massive generalization ability: GPT-3 is competitive in many benchmarks w_ithout even tuning on the target task._ \\[...\\] Perhaps the most impressive part, though, is that even at such a massive scale, the model still scales smoothly in performance instead of plateauing, implying that still-larger models would perform e_ven better._",
      "plaintextDescription": "> With massive size comes massive generalization ability: GPT-3 is competitive in many benchmarks without even tuning on the target task. [...] Perhaps the most impressive part, though, is that even at such a massive scale, the model still scales smoothly in performance instead of plateauing, implying that still-larger models would perform even better.",
      "wordCount": 50
    },
    "tags": [
      {
        "_id": "YWzByWvtXunfrBu5b",
        "name": "GPT",
        "slug": "gpt"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]